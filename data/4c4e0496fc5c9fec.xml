<?xml version="1.0" encoding="UTF-8" standalone="no"?><?xml-stylesheet href="http://www.blogger.com/styles/atom.css" type="text/css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:gd="http://schemas.google.com/g/2005" xmlns:georss="http://www.georss.org/georss" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:thr="http://purl.org/syndication/thread/1.0"><id>tag:blogger.com,1999:blog-8474926331452026626</id><updated>2022-10-29T18:07:25.438-07:00</updated><category term="Machine Learning"/><category term="Deep Learning"/><category term="Computer Vision"/><category term="Google Brain"/><category term="Natural Language Processing"/><category term="open source"/><category term="Publications"/><category term="Research"/><category term="TensorFlow"/><category term="Machine Perception"/><category term="Education"/><category term="Natural Language Understanding"/><category term="conference"/><category term="datasets"/><category term="Neural Networks"/><category term="University Relations"/><category term="conferences"/><category term="Reinforcement Learning"/><category term="Health"/><category term="Robotics"/><category term="AI"/><category term="NLP"/><category term="CVPR"/><category term="Research Awards"/><category term="Algorithms"/><category term="Computational Photography"/><category term="Speech"/><category term="Computer Science"/><category term="MOOC"/><category term="Machine Intelligence"/><category term="Quantum Computing"/><category term="On-device Learning"/><category term="Machine Translation"/><category term="AI for Social Good"/><category term="ICLR"/><category term="Image Classification"/><category term="Multimodal Learning"/><category term="Pixel"/><category term="Visualization"/><category term="YouTube"/><category term="HCI"/><category term="Hardware"/><category term="Security and Privacy"/><category term="Self-Supervised Learning"/><category term="accessibility"/><category term="optimization"/><category term="AutoML"/><category term="Android"/><category term="Audio"/><category term="Awards"/><category term="Quantum AI"/><category term="TPU"/><category term="ACL"/><category term="EMNLP"/><category term="Information Retrieval"/><category term="NeurIPS"/><category term="ICML"/><category term="Image Processing"/><category term="ML"/><category term="Physics"/><category term="Structured Data"/><category term="TTS"/><category term="Google Accelerated Science"/><category term="ML Fairness"/><category term="Search"/><category term="Speech Recognition"/><category term="Google Translate"/><category term="Graph Mining"/><category term="User Experience"/><category term="distributed systems"/><category term="video"/><category term="ACM"/><category term="Automatic Speech Recognition"/><category term="Earth Engine"/><category term="Google Maps"/><category term="K-12"/><category term="Video Analysis"/><category term="statistics"/><category term="Chemistry"/><category term="Collaboration"/><category term="DeepMind"/><category term="Diversity"/><category term="Vision Research"/><category term="Voice Search"/><category term="ph.d. fellowship"/><category term="Cloud Computing"/><category term="Environment"/><category term="Interspeech"/><category term="NIPS"/><category term="Software"/><category term="Supervised Learning"/><category term="UI"/><category term="data science"/><category term="grants"/><category term="market algorithms"/><category term="Compression"/><category term="Faculty Summit"/><category term="Google Cloud Platform"/><category term="Google Genomics"/><category term="ICCV"/><category term="Machine Hearing"/><category term="Semi-supervised Learning"/><category term="Translate"/><category term="crowd-sourcing"/><category term="Acoustic Modeling"/><category term="Art"/><category term="Augmented Reality"/><category term="Course Builder"/><category term="Google Photos"/><category term="Google+"/><category term="PhD Fellowship"/><category term="Recommender Systems"/><category term="Systems"/><category term="Unsupervised Learning"/><category term="WWW"/><category term="renewable energy"/><category term="Computational Imaging"/><category term="Data Discovery"/><category term="Europe"/><category term="Expander"/><category term="Fusion Tables"/><category term="Google Books"/><category term="Moore's Law"/><category term="Ngram"/><category term="Semantic Models"/><category term="Social Networks"/><category term="Year in Review"/><category term="schema.org"/><category term="API"/><category term="App Engine"/><category term="Gmail"/><category term="Google Play Apps"/><category term="High Dynamic Range Imaging"/><category term="Image Annotation"/><category term="India"/><category term="Internet of Things"/><category term="Kaggle"/><category term="NAACL"/><category term="Networks"/><category term="Optical Character Recognition"/><category term="Virtual Reality"/><category term="ads"/><category term="economics"/><category term="internationalization"/><category term="publication"/><category term="resource optimization"/><category term="search ads"/><category term="wikipedia"/><category term="Adaptive Data Analysis"/><category term="Africa"/><category term="App Inventor"/><category term="China"/><category term="DeepDream"/><category term="EMEA"/><category term="Exacycle"/><category term="Gboard"/><category term="Google Docs"/><category term="Google Drive"/><category term="Google Science Fair"/><category term="Google Sheets"/><category term="Graph"/><category term="Inbox"/><category term="KDD"/><category term="Keyboard Input"/><category term="Labs"/><category term="Low-Light Photography"/><category term="MapReduce"/><category term="Policy"/><category term="Proposals"/><category term="Responsible AI"/><category term="Style Transfer"/><category term="TensorBoard"/><category term="VLDB"/><category term="electronics"/><category term="osdi"/><category term="patents"/><category term="trends"/><category term="Android Wear"/><category term="April Fools"/><category term="Australia"/><category term="BigQuery"/><category term="Biology"/><category term="Cantonese"/><category term="Chrome"/><category term="Conservation"/><category term="Data Center"/><category term="ECCV"/><category term="Electronic Commerce and Algorithms"/><category term="Encryption"/><category term="Entity Salience"/><category term="Faculty Institute"/><category term="Flu Trends"/><category term="Google Trips"/><category term="Google Voice Search"/><category term="Government"/><category term="ICSE"/><category term="IPython"/><category term="Journalism"/><category term="Klingon"/><category term="Korean"/><category term="Linear Optimization"/><category term="Magenta"/><category term="Market Research"/><category term="Mixed Reality"/><category term="Network Management"/><category term="Nexus"/><category term="Peer Review"/><category term="PhotoScan"/><category term="PiLab"/><category term="Professional Development"/><category term="Public Data Explorer"/><category term="SIGCOMM"/><category term="SIGMOD"/><category term="Site Reliability Engineering"/><category term="Sound Search"/><category term="TV"/><category term="UNIX"/><category term="Visiting Faculty"/><category term="Wiki"/><category term="adsense"/><category term="adwords"/><category term="correlate"/><category term="entities"/><category term="gamification"/><category term="jsm"/><category term="jsm2011"/><category term="localization"/><category term="materials science"/><category term="operating systems"/><category term="osdi10"/><title type="text">Google AI Blog</title><subtitle type="html">The latest news from Google AI.</subtitle><link href="http://ai.googleblog.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;redirect=false" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/><author><name>ewood</name><uri>http://www.blogger.com/profile/12341551220176883769</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><generator uri="http://www.blogger.com" version="7.00">Blogger</generator><openSearch:totalResults>1141</openSearch:totalResults><openSearch:startIndex>1</openSearch:startIndex><openSearch:itemsPerPage>25</openSearch:itemsPerPage><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7123003530389096578</id><published>2022-10-26T10:18:00.001-07:00</published><updated>2022-10-27T13:07:25.856-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="Education"/><category scheme="http://www.blogger.com/atom/ns#" term="ML"/><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Understanding"/><title type="text">Natural Language Assessment: A New Framework to Promote Education</title><content type="html">&lt;span class="byline-author"&gt;Posted by Kedem Snir, Software Engineer, and Gal Elidan, Senior Staff Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Whether it's a professional honing their skills or a child learning to read, coaches and educators play a key role in assessing the learner's answer to a question in a given context and guiding them towards a goal. These interactions have unique characteristics that set them apart from other forms of dialogue, yet are not available when learners practice alone at home. In the field of natural language processing, this type of capability has not received much attention and is technologically challenging. We set out to explore how we can use machine learning to assess answers in a way that facilitates learning. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In this blog, we introduce an important natural language understanding (NLU) capability called Natural Language Assessment (NLA), and discuss how it can be helpful in the context of education. While typical NLU tasks focus on the user's intent, NLA allows for the assessment of an answer from multiple perspectives. In situations where a user wants to know how good their answer is, NLA can offer an analysis of how close the answer is to what is expected. In situations where there may not be a “correct” answer, NLA can offer subtle insights that include topicality, relevance, verbosity, and beyond. We formulate the scope of NLA, present a practical model for carrying out topicality NLA, and showcase how NLA has been used to help job seekers practice answering interview questions with Google's new interview prep tool, &lt;a href="https://blog.google/outreach-initiatives/grow-with-google/interview-warmup/"&gt;Interview Warmup&lt;/a&gt;. &lt;/p&gt;  &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Overview of Natural Language Assessment (NLA)&lt;/h2&gt;&lt;p&gt;The goal of NLA is to evaluate the user's answer against a set of expectations. Consider the following components for an NLA system interacting with students: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;A question&lt;/em&gt; presented to the student  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Expectations&lt;/em&gt; that define what we expect to find in the answer (e.g., a concrete textual answer, a set of topics we expect the answer to cover, conciseness)  &lt;/li&gt;&lt;li&gt;&lt;em&gt;An answer&lt;/em&gt; provided by the student  &lt;/li&gt;&lt;li&gt;&lt;em&gt;An assessment output&lt;/em&gt; (e.g., correctness, missing information, too specific or general, stylistic feedback, pronunciation, etc.)  &lt;/li&gt;&lt;li&gt;[Optional] &lt;em&gt;A context&lt;/em&gt; (e.g., a chapter in a book or an article) &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;With NLA, both the expectations about the answer and the assessment of the answer can be very broad. This enables teacher-student interactions that are more expressive and subtle. Here are two examples: &lt;/p&gt;&lt;ol&gt; &lt;li&gt;&lt;em&gt;A question with a concrete correct answer&lt;/em&gt;: Even in situations where there is a clear correct answer, it can be helpful to assess the answer more subtly than simply correct or incorrect. Consider the following:  &lt;blockquote style="border: none; margin: -10px 0px 0px 40px; padding: 0px; text-align: left;"&gt;&lt;p&gt;   &lt;b&gt;Context&lt;/b&gt;: &lt;em&gt;Harry Potter and the Philosopher's Stone&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Question&lt;/b&gt;: &lt;em&gt;“What is Hogwarts?”&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Expectation&lt;/b&gt;: &lt;em&gt;“Hogwarts is a school of Witchcraft and Wizardry” [expectation is given as text]&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Answer&lt;/b&gt;: &lt;em&gt;“I am not exactly sure, but I think it is a school.”&lt;/em&gt;  &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The answer may be missing salient details but labeling it as incorrect wouldn’t be entirely true or useful to a user. NLA can offer a more subtle understanding by, for example, identifying that the student’s answer is too general, and also that the student is uncertain. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNwF8YrXFz2iqbVrKP5LS0lJPNDVAsqozOzz-Ut8075ogDQhWd1bXaVBTfpWLmzjcyB2XxeXzEpXB1eO5RA0Q8iO8j1zC3Cpsn8PDj8YNLywLFD2THrJovvowrUTlrOvslYMwFsTyApSvWo3JOkd_KysHV8Chg7sk3PrAr4aMERjEBWttxjez89Wnszw/s1200/image21.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="614" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNwF8YrXFz2iqbVrKP5LS0lJPNDVAsqozOzz-Ut8075ogDQhWd1bXaVBTfpWLmzjcyB2XxeXzEpXB1eO5RA0Q8iO8j1zC3Cpsn8PDj8YNLywLFD2THrJovvowrUTlrOvslYMwFsTyApSvWo3JOkd_KysHV8Chg7sk3PrAr4aMERjEBWttxjez89Wnszw/s16000/image21.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Illustration of the NLA process from input question, answer and expectation to assessment output.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;This kind of subtle assessment, along with noting the uncertainty the student expressed, can be important in helping students build skills in conversational settings. &lt;/p&gt; &lt;/li&gt;&lt;li&gt;&lt;em&gt;Topicality expectations&lt;/em&gt;: There are many situations in which a concrete answer is not expected. For example, if a student is asked an opinion question, there is no concrete textual expectation. Instead, there's an expectation of relevance and opinionation, and perhaps some level of succinctness and fluency. Consider the following interview practice setup: &lt;blockquote style="border: none; margin: -10px 0px 0px 40px; padding: 0px; text-align: left;"&gt;&lt;p&gt;         &lt;b&gt;Question&lt;/b&gt;: &lt;em&gt;“Tell me a little about yourself?”&lt;/em&gt;&lt;br /&gt;  &lt;b&gt;Expectations&lt;/b&gt;: &lt;em&gt;{ “Education”, “Experience”, “Interests” } (a set of topics)&lt;/em&gt;&lt;br /&gt;        &lt;b&gt;Answer&lt;/b&gt;: &lt;em&gt;“Let’s see. I grew up in the Salinas valley in California and went to Stanford where I majored in economics but then got excited about technology so next I ….”&lt;/em&gt;  &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In this case, a useful assessment output would map the user’s answer to a subset of the topics covered, possibly along with a markup of which parts of the text relate to which topic. This can be challenging from an NLP perspective as answers can be long, topics can be mixed, and each topic on its own can be multi-faceted.    &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;  &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;A Topicality NLA Model&lt;/h2&gt;&lt;p&gt;In principle, topicality NLA is a standard multi-class task for which one can readily train a classifier using standard techniques. However, training data for such scenarios is scarce and it would be costly and time consuming to collect for each question and topic. Our solution is to break each topic into granular components that can be identified using large language models (LLMs) with a straightforward generic tuning.  &lt;/p&gt;&lt;p&gt;We map each topic to a list of underlying questions and define that if the sentence contains an answer to one of those underlying questions, then it covers that topic. For the topic “Experience” we might choose underlying questions such as: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;Where did you work?  &lt;/li&gt;&lt;li&gt;What did you study?  &lt;/li&gt;&lt;li&gt;… &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While for the topic “Interests” we might choose underlying questions such as: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;What are you interested in?  &lt;/li&gt;&lt;li&gt;What do you enjoy doing?  &lt;/li&gt;&lt;li&gt;… &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These underlying questions are designed through an iterative manual process. Importantly, since these questions are sufficiently granular, current language models (see details below) can capture their semantics. This allows us to offer a zero-shot setting for the NLA topicality task: once trained (more on the model below), it is easy to add new questions and new topics, or adapt existing topics by modifying their underlying content expectation without the need to collect topic specific data. See below the model’s predictions for the sentence “&lt;em&gt;I’ve worked in retail for 3 years”&lt;/em&gt; for the two topics described above: &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhOjKY5O1qBOyFYNcboWvDaDnZj9CpBhZNGkODUMh-oF7F01FcUlaVtZJ9vOfkmgRJKBuRVJk0wk1VNl-VWVLfkfkWAIKALH3dkFmeiJgd6AuCULJT-WDTfRN8qLV6z4mpvCUGTsDJIalOx5zEbpuMvosCGEOiuXSGQ2B55nB03R1K0bFJdLLnhkid_w/s1680/Diagram%2010%20(1).png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="660" data-original-width="1680" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhOjKY5O1qBOyFYNcboWvDaDnZj9CpBhZNGkODUMh-oF7F01FcUlaVtZJ9vOfkmgRJKBuRVJk0wk1VNl-VWVLfkfkWAIKALH3dkFmeiJgd6AuCULJT-WDTfRN8qLV6z4mpvCUGTsDJIalOx5zEbpuMvosCGEOiuXSGQ2B55nB03R1K0bFJdLLnhkid_w/s16000/Diagram%2010%20(1).png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A diagram of how the model uses underlying questions to predict the topic most likely to be covered by the user’s answer.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Since an underlying question for the topic “Experience” was matched, the sentence would be classified as “Experience”. &lt;/p&gt; &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Application: Helping Job Seekers Prepare for Interviews&lt;/h2&gt;  &lt;p&gt;&lt;a href="https://blog.google/outreach-initiatives/grow-with-google/interview-warmup/"&gt;Interview Warmup&lt;/a&gt; is a new tool developed in collaboration with job seekers to help them prepare for interviews in fast-growing fields of employment such as IT Support and UX Design. It allows job seekers to practice answering questions selected by industry experts and to become more confident and comfortable with interviewing. As we worked with job seekers to understand their challenges in preparing for interviews and how an interview practice tool could be most useful, it inspired our research and the application of topicality NLA. &lt;/p&gt;&lt;p&gt;We build the topicality NLA model (once for all questions and topics) as follows: we train an encoder-only T5 model (&lt;a href="https://arxiv.org/pdf/2110.08426.pdf"&gt;EncT5&lt;/a&gt; architecture) with 350 million parameters on Question-Answers data to predict the compatibility of an &lt;code&gt;&amp;lt;underlying question, answer&amp;gt;&lt;/code&gt; pair. We rely on data from &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;SQuAD 2.0&lt;/a&gt; which was processed to produce &lt;code&gt;&amp;lt;question, answer, label&amp;gt;&lt;/code&gt; triplets. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirInL-Tue-1LTGLBbt_o_uXoDMr8Hl8ERvuxYUt9hrT95-rptv-VORYi6ZZq4jLnGsGk2wEGLF7bnT8Vqgo74uO4ElEqSU9atwQn2-FqFKMRS1302FrLib7xCz4IEEHPrs49MAvKKYnB8glHUdCb2ECg3Te7fB4H8oZW38er5WV1UV0Ku7W3buEZmEHA/s1069/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="668" data-original-width="1069" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirInL-Tue-1LTGLBbt_o_uXoDMr8Hl8ERvuxYUt9hrT95-rptv-VORYi6ZZq4jLnGsGk2wEGLF7bnT8Vqgo74uO4ElEqSU9atwQn2-FqFKMRS1302FrLib7xCz4IEEHPrs49MAvKKYnB8glHUdCb2ECg3Te7fB4H8oZW38er5WV1UV0Ku7W3buEZmEHA/s16000/image4.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;In the Interview Warmup tool, users can switch between talking points to see which ones were detected in their answer.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The tool does not grade or judge answers. Instead it enables users to practice and identify ways to improve on their own. After a user replies to an interview question, their answer is parsed sentence-by-sentence with the Topicality NLA model. They can then switch between different talking points to see which ones were detected in their answer. We know that there are many potential pitfalls in signaling to a user that their response is “good”, especially as we only detect a limited set of topics.  Instead, we keep the control in the user’s hands and only use ML to help users make &lt;em&gt;their own discoveries&lt;/em&gt; about how to improve.  &lt;/p&gt;&lt;p&gt;So far, the tool has had great results helping job seekers around the world, including in the US, and we have recently expanded it to &lt;a href="https://blog.google/around-the-globe/google-africa/how-ai-is-helping-african-communities-and-businesses/"&gt;Africa&lt;/a&gt;. We plan to continue working with job seekers to iterate and make the tool even more helpful to the millions of people searching for new jobs.&lt;br /&gt;&lt;/p&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" frameborder="0" height="360" rel="0&amp;amp;" src="https://www.youtube.com/embed/KKfAuQrwzTY" width="640" youtube-src-id="KKfAuQrwzTY"&gt;&lt;/iframe&gt;&lt;/div&gt;     &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A short film showing how Interview Warmup and its NLA capabilities were developed in collaboration with job seekers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Natural Language Assessment (NLA) is a technologically challenging and interesting research area. It paves the way for new conversational applications that promote learning by enabling the nuanced assessment and analysis of answers from multiple perspectives. Working together with communities, from job seekers and businesses to classroom teachers and students, we can identify situations where NLA has the potential to help people learn, engage, and develop skills across an array of subjects, and we can build applications in a responsible way that empower users to assess their own abilities and discover ways to improve. &lt;/p&gt;  &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work is made possible through a collaboration spanning several teams across Google. We’d like to acknowledge contributions from Google Research Israel, Google Creative Lab, and Grow with Google teams among others. &lt;/em&gt;&lt;/p&gt;     </content><link href="http://ai.googleblog.com/feeds/7123003530389096578/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/natural-language-assessment-new.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7123003530389096578" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7123003530389096578" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/natural-language-assessment-new.html" rel="alternate" title="Natural Language Assessment: A New Framework to Promote Education" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNwF8YrXFz2iqbVrKP5LS0lJPNDVAsqozOzz-Ut8075ogDQhWd1bXaVBTfpWLmzjcyB2XxeXzEpXB1eO5RA0Q8iO8j1zC3Cpsn8PDj8YNLywLFD2THrJovvowrUTlrOvslYMwFsTyApSvWo3JOkd_KysHV8Chg7sk3PrAr4aMERjEBWttxjez89Wnszw/s72-c/image21.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-3606260649484710280</id><published>2022-10-25T12:52:00.000-07:00</published><updated>2022-10-25T12:52:17.444-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/><title type="text">Open Images V7 — Now Featuring Point Labels</title><content type="html">&lt;span class="byline-author"&gt;Posted by Rodrigo Benenson, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;&lt;a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html"&gt;Open Images&lt;/a&gt; is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. Researchers around the world use Open Images to train and evaluate computer vision models. Since the &lt;a href="https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html"&gt;initial release&lt;/a&gt; of Open Images in 2016, which included image-level labels covering 6k categories, we have provided multiple updates to enrich annotations and expand the potential use cases of the dataset. Through several releases, we have added &lt;a href="https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html"&gt;image-level labels for over 20k categories&lt;/a&gt; on all images and &lt;a href="https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html"&gt;bounding box annotations&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html"&gt;visual relations&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html"&gt;instance segmentations&lt;/a&gt;, and &lt;a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html"&gt;localized narratives&lt;/a&gt; (synchronized voice, mouse trace, and text caption) on a subset of 1.9M images.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Today, we are happy to announce the release of &lt;a href="https://storage.googleapis.com/openimages/web/index.html"&gt;Open Images V7&lt;/a&gt;, which expands the Open Images dataset even further with a new annotation type called &lt;em&gt;point-level labels&lt;/em&gt; and includes a new all-in-one visualization tool that allows a better exploration of the rich data available. &lt;/p&gt; &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Point Labels&lt;/h2&gt;&lt;p&gt;The main strategy used to collect the new point-level label annotations leveraged suggestions from a machine learning (ML) model and human verification. First, the ML model selected points of interest and asked a yes or no question, e.g., “is this point on a pumpkin?”. Then, human annotators spent an average of 1.1 seconds answering the yes or no questions. We aggregated the answers from different annotators over the same question and assigned a final “yes”, “no”, or “unsure” label to each annotated point. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s474/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="396" data-original-width="474" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Illustration of the annotations interface. &lt;br /&gt;(&lt;a href="https://c2.staticflickr.com/9/8329/8140854650_2a93ae7105_z.jpg"&gt;Image&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/lenore-m/"&gt;Lenore Edman&lt;/a&gt;, under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license&lt;/a&gt;)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For each annotated image, we provide a collection of points, each with a “yes” or “no” label for a given class. These points provide sparse information that can be used for the semantic segmentation task. We collected a total of 38.6M new point annotations (12.4M with “yes” labels) that cover 5.8 thousand classes and 1.4M images. &lt;/p&gt; &lt;p&gt;By focusing on point labels, we expanded the number of images annotated and categories covered. We also concentrated the efforts of our annotators on efficiently collecting useful information. Compared to our instance segmentation, the new points include 16x more classes and cover more images. The new points also cover 9x more classes than our box annotations. Compared to existing segmentation datasets, like &lt;a href="https://paperswithcode.com/dataset/pascal-voc"&gt;PASCAL VOC&lt;/a&gt;, &lt;a href="https://cocodataset.org/#home"&gt;COCO&lt;/a&gt;, &lt;a href="https://www.cityscapes-dataset.com/"&gt;Cityscapes&lt;/a&gt;, &lt;a href="https://www.lvisdataset.org/"&gt;LVIS&lt;/a&gt;, or &lt;a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/"&gt;ADE20K&lt;/a&gt;, our annotations cover more classes and more images than previous work. The new point label annotations are the first type of annotation in Open Images that provides localization information for both things (countable objects, like cars, cats, and catamarans), and stuff categories (uncountable objects like grass, granite, and gravel). Overall, the newly collected data is roughly equivalent to two years of human annotation effort. &lt;/p&gt; &lt;p&gt;Our initial experiments show that this type of sparse data is suitable for both training and evaluating segmentation models. Training a model directly on sparse data allows us to reach comparable quality to training on dense annotations. Similarly, we show that one can directly compute the traditional semantic segmentation &lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;intersection-over-union&lt;/a&gt; (IoU) metric over sparse data. The ranking across different methods is preserved, and the sparse IoU values are an accurate estimate of its dense version. See our &lt;a href="https://storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf"&gt;paper&lt;/a&gt; for more details.  &lt;/p&gt; &lt;p&gt;Below, we show four example images with their point-level labels, illustrating the rich and diverse information these annotations provide. Circles ⭘ are “yes” labels, and squares &lt;b&gt;☐&lt;/b&gt; are “no” labels. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s1338/image1.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1309" data-original-width="1338" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s16000/image1.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s1527/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1252" data-original-width="1527" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s1650/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1650" data-original-width="1544" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s16000/image6.png" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s1684/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1650" data-original-width="1684" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Four example images with point-level labels. &lt;br /&gt;Images by &lt;a href="https://c7.staticflickr.com/5/4088/5070100626_57e898dfdf_z.jpg"&gt;Richie&lt;/a&gt; &lt;a href="https://www.flickr.com/people/puroticorico/"&gt;Diesterheft&lt;/a&gt;, &lt;a href="https://c3.staticflickr.com/4/3928/15385724218_0a4b86d2f9_o.jpg"&gt;John AM&lt;/a&gt; &lt;a href="https://www.flickr.com/people/nuevajam/"&gt;Nueva&lt;/a&gt;, &lt;a href="https://farm8.staticflickr.com/4073/4793785065_bd8509a087_o.jpg"&gt;Sarah&lt;/a&gt; &lt;a href="https://www.flickr.com/people/sackerman519/"&gt;Ackerman&lt;/a&gt;, and &lt;a href="https://c5.staticflickr.com/5/4101/4908677641_458e9a060f_o.jpg"&gt;C&lt;/a&gt; &lt;a href="https://www.flickr.com/people/madmarlin_/"&gt;Thomas&lt;/a&gt;, all under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license.&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;New Visualizers&lt;/h2&gt;&lt;p&gt;In addition to the new data release, we also expanded &lt;a href="https://storage.googleapis.com/openimages/web/visualizer/index.html"&gt;the available visualizations&lt;/a&gt; of the Open Images annotations. The Open Images website now includes dedicated visualizers to explore the localized narratives annotations, the new point-level annotations, and a new all-in-one view. This new all-in-one view is available for the subset of 1.9M densely annotated images and allows one to explore the rich annotations that Open Images has accumulated over seven releases. On average these images have annotations for 6.7 image-labels (classes), 8.3 boxes, 1.7 relations, 1.5 masks, 0.4 localized narratives and 34.8 point-labels per image. &lt;/p&gt; &lt;p&gt;Below, we show two example images with various annotations in the all-in-one visualizer. The figures show the image-level labels, bounding boxes, box relations, instance masks, localized narrative mouse trace and caption, and point-level labels. The &lt;b&gt;+&lt;/b&gt; classes have positive annotations (of any kind), while &lt;b&gt;–&lt;/b&gt; classes have only negative annotations (image-level or point-level).  &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s1571/image9.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1439" data-original-width="1571" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s16000/image9.png" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s1433/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1433" data-original-width="1132" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Two example images with various annotations in the all-in-one visualizer. &lt;br /&gt;Images by &lt;a href="https://c1.staticflickr.com/4/3835/15014236762_04033666d5_z.jpg"&gt;Jason&lt;/a&gt; &lt;a href="https://www.flickr.com/people/jasonparis/"&gt;Paris&lt;/a&gt;, and &lt;a href="https://c7.staticflickr.com/5/4015/4359099851_701ccfd762_z.jpg"&gt;Rubén&lt;/a&gt; &lt;a href="https://www.flickr.com/people/vike/"&gt;Vique&lt;/a&gt;, all under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license.&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We hope that this new data release will enable computer vision research to cover ever more diverse and challenging scenarios. As the quality of automated semantic segmentation models improves over common classes, we want to move towards the long tail of visual concepts, and sparse point annotations are a step in that direction. More and more works are exploring how to use such sparse annotations (e.g., as &lt;a href="https://openreview.net/forum?id=wt6QxYgddsl"&gt;supervision&lt;/a&gt; for &lt;a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.html"&gt;instance&lt;/a&gt; &lt;a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880599.pdf"&gt;segmentation&lt;/a&gt; or &lt;a href="https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.html"&gt;semantic segmentation&lt;/a&gt;), and Open Images V7 contributes to this research direction. We are looking forward to seeing what you will build next. &lt;/p&gt; &lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;Thanks to &lt;a href="https://sites.google.com/corp/view/vittoferrari"&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href="https://jponttuset.cat/"&gt;Jordi Pont-Tuset&lt;/a&gt;, &lt;a href="https://akuznetso.github.io/"&gt;Alina Kuznetsova&lt;/a&gt;, Ashlesha Sadras, and the annotators team for their support creating this new data release.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/3606260649484710280/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/open-images-v7-now-featuring-point.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3606260649484710280" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3606260649484710280" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/open-images-v7-now-featuring-point.html" rel="alternate" title="Open Images V7 — Now Featuring Point Labels" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s72-c/image3.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4407074621491257938</id><published>2022-10-22T20:30:00.004-07:00</published><updated>2022-10-24T08:40:18.077-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="conference"/><category scheme="http://www.blogger.com/atom/ns#" term="ECCV"/><title type="text">Google at ECCV 2022 </title><content type="html">&lt;span class="byline-author"&gt;Posted by Shaina Mehta, Program Manager, Google&lt;/span&gt; &lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWmLScVcpCyi8puS5FYC4xNN88CWeK0-77JiMlowYPlxCPym5T4zX7iKIuzIjwtDnUubUJ0yM8Xa6Mt83N_-YoBgyrZH_dwSL6YgoxUEgGf1tNuR5fSWeRY61Ut0TOhlzUuTJlKs6fyBF3JTVzAlbEOUgtGb_5gdgFBIXWFCJ0utT8kuUh3rKIjw4L1g/s2880/about_hero.png" style="display: none;" /&gt; &lt;p&gt;Google is proud to be a &lt;a href="https://eccv2022.ecva.net/sponsors/"&gt;Platinum Sponsor&lt;/a&gt; of the &lt;a href="https://eccv2022.ecva.net/"&gt;European Conference on Computer Vision&lt;/a&gt; (ECCV 2022), a premier forum for the dissemination of research in computer vision and machine learning (ML). This year, ECCV 2022 will be held as a hybrid event, in person in Tel Aviv, Israel with virtual attendance as an option. Google has a strong presence at this year’s conference with over 60 accepted publications and active involvement in a number of workshops and tutorials. We look forward to sharing some of our extensive research and expanding our partnership with the broader ML research community.&amp;nbsp;&lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Registered for ECCV 2022? We hope you’ll visit our on-site or virtual booths to learn more about the research we’re presenting at ECCV 2022, including several demos and opportunities to connect with our researchers. Learn more about Google's research being presented at ECCV 2022 below (Google affiliations in &lt;b&gt;bold&lt;/b&gt;). &lt;/p&gt;&lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Organizing Committee&lt;/h2&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;Program Chairs include:&lt;b&gt; &lt;i&gt;Moustapha Cissé&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;Awards Paper Committee:&lt;b&gt; &lt;i&gt;Todd Zickler&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;Area Chairs include:&lt;b&gt; &lt;i&gt;Ayan Chakrabarti&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alireza Fathi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;David Fleet&lt;/i&gt;, &lt;i&gt;Dilip Krishnan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; T&lt;i&gt;odd Zickler&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Accepted Publications&lt;/h2&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.11911.pdf"&gt;NeuMesh: Learning Disentangled Neural Mesh-Based Implicit Field for Geometry and Texture Editing&lt;/a&gt;&lt;br /&gt;Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;, Zhaopeng Cui, Guofeng Zhang &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2109.09023.pdf"&gt;Anti-Neuron Watermarking: Protecting Personal Data Against Unauthorized Neural Networks&lt;/a&gt;&lt;br /&gt;Zihang Zou, &lt;b&gt;&lt;i&gt;Boqing Gong&lt;/i&gt;&lt;/b&gt;, Liqiang Wang &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.08954.pdf"&gt;Exploiting Unlabeled Data with Vision and Language Models for Object Detection&lt;/a&gt;&lt;br /&gt;Shiyu Zhao, Zhixing Zhang, Samuel Schulter, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, Vijay Kumar B G, Anastasis Stathopoulos, Manmohan Chandraker, Dimitris N. Metaxas &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2206.07704.pdf"&gt;Waymo Open Dataset: Panoramic Video Panoptic Segmentation&lt;/a&gt;&lt;br /&gt;Jieru Mei, Alex Zhu, Xinchen Yan, Hang Yan, &lt;b&gt;&lt;i&gt;Siyuan Qiao&lt;/i&gt;&lt;/b&gt;, Yukun Zhu, &lt;b&gt;&lt;i&gt;Liang-Chieh Chen&lt;/i&gt;&lt;/b&gt;, Henrik Kretzschmar &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.06143.pdf"&gt;PRIF: Primary Ray-Based Implicit Function&lt;/a&gt;&lt;br /&gt;Brandon Yushan Feng, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Danhang Tang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ruofei Du&lt;/i&gt;&lt;/b&gt;, Amitabh Varshney &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.08622.pdf"&gt;LoRD: Local 4D Implicit Representation for High-Fidelity Dynamic Human Modeling&lt;/a&gt;&lt;br /&gt;Boyan Jiang, Xinlin Ren, &lt;b&gt;&lt;i&gt;Mingsong Dou&lt;/i&gt;&lt;/b&gt;, Xiangyang Xue, Yanwei Fu, &lt;b&gt;&lt;i&gt;Yinda Zhang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.04044.pdf"&gt;k-Means Mask Transformer&lt;/a&gt; (see &lt;a href="https://ai.googleblog.com/2022/07/revisiting-mask-transformer-from.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;Qihang Yu&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Siyuan Qiao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Maxwell D Collins&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yukun Zhu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, Alan Yuille, &lt;b&gt;&lt;i&gt;Liang-Chieh Chen&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.01697.pdf"&gt;MaxViT: Multi-Axis Vision Transformer&lt;/a&gt; (see &lt;a href="https://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Zhengzhong Tu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hossein Talebi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Feng Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Peyman Milanfar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alan Bovik&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Yinxiao Li&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10008.pdf"&gt;E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs&lt;/a&gt;&lt;br /&gt;Yanyan Li, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.00237.pdf"&gt;RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation&lt;/a&gt;&lt;br /&gt;Ruida Zhang, Yan Di, Zhiqiang Lou, &lt;b&gt;&lt;i&gt;Fabian Manhardt&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;, Xiangyang Ji &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10158.pdf"&gt;GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning&lt;/a&gt;&lt;br /&gt;Huseyin Coskun, Alireza Zareian, Joshua L Moore, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;, Chen Wang &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.12143.pdf"&gt;Scaling Open-Vocabulary Image Segmentation with Image-Level Labels&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Golnaz Ghiasi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiuye Gu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yin Cui&lt;/i&gt;&lt;/b&gt;, Tsung-Yi Lin&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.12175.pdf"&gt;Adaptive Transformers for Robust Few-Shot Cross-Domain Face Anti-spoofing&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Hsin-Ping Huang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yaojie Liu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Wen-Sheng Chu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Taihong Xiao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jinwei Yuan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.04799.pdf"&gt;DualPrompt: Complementary Prompting for Rehearsal-Free Continual Learning&lt;/a&gt;&lt;br /&gt;Zifeng Wang&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Zizhao Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Sayna Ebrahimi&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ruoxi Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen-Yu Lee&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaoqi Ren&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Guolong Su&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vincent Perot&lt;/i&gt;&lt;/b&gt;, Jennifer Dy, &lt;b&gt;&lt;i&gt;Tomas Pfister&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.05112.pdf"&gt;BLT: Bidirectional Layout Transformer for Controllable Layout Generation&lt;/a&gt;&lt;br /&gt;Xiang Kong, &lt;b&gt;&lt;i&gt;Lu Jiang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huiwen Chang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yuan Hao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Haifeng Gong&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Irfan Essa&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.10638.pdf"&gt;V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer&lt;/a&gt;&lt;br /&gt;Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;, Jiaqi Ma &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.10652.pdf"&gt;Learning Visibility for Robust Dense Human Body Estimation&lt;/a&gt;&lt;br /&gt;Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.10659.pdf"&gt;Are Vision Transformers Robust to Patch Perturbations?&lt;/a&gt;&lt;br /&gt;Jindong Gu, Volker Tresp, &lt;b&gt;Yao Qin&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910542.pdf"&gt;PseudoAugment: Learning to Use Unlabeled Data for Data Augmentation in Point Clouds&lt;/a&gt;&lt;br /&gt;Zhaoqi Leng, Shuyang Cheng, &lt;b&gt;&lt;i&gt;Ben Caine&lt;/i&gt;&lt;/b&gt;, Weiyue Wang, Xiao Zhang, &lt;b&gt;&lt;i&gt;Jonathon Shlens&lt;/i&gt;&lt;/b&gt;, Mingxing Tan, Dragomir Anguelov &lt;/p&gt;  &lt;p&gt;&lt;a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930020.pdf"&gt;Structure and Motion from Casual Videos&lt;/a&gt;&lt;br /&gt;Zhoutong Zhang, &lt;b&gt;&lt;i&gt;Forrester Cole&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Zhengqi Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Noah Snavely&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;William T. Freeman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.10435.pdf"&gt;PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map&lt;/a&gt;&lt;br /&gt;Chenfeng Xu, Tian Li, Chen Tang, Lingfeng Sun, Kurt Keutzer, Masayoshi Tomizuka, &lt;b&gt;&lt;i&gt;Alireza Fathi&lt;/i&gt;&lt;/b&gt;, Wei Zhan &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10659.pdf"&gt;Novel Class Discovery Without Forgetting&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Joseph K J&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Sujoy Paul&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Gaurav Aggarwal&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Soma Biswas&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Piyush Rai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Kai Han&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vineeth N Balasubramanian&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.09644.pdf"&gt;Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning&lt;/a&gt;&lt;br /&gt;Yuxiao Chen, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, Jianbo Yuan, Yu Tian, Zhaoyang Xia, Shijie Geng, Ligong Han, Dimitris N. Metaxas &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.05126.pdf"&gt;PACTran: PAC-Bayesian Metrics for Estimating the Transferability of Pretrained Models to Classification Tasks&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Nan Ding&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xi Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tomer Levinboim&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Soravit Changpinyo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Radu Soricut&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://infinite-nature-zero.github.io/"&gt;InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Zhengqi Li&lt;/i&gt;&lt;/b&gt;, Qianqian Wang&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Noah Snavely&lt;/i&gt;&lt;/b&gt;, Angjoo Kanazawa&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;  &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10662.pdf"&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt; (see &lt;a href="https://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;Mohammed Suhail&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Carlos Esteves&lt;/i&gt;&lt;/b&gt;, Leonid Sigal, &lt;b&gt;&lt;i&gt;Ameesh Makadia&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://cseweb.ucsd.edu/~mil070/projects/ECCV2022/paper.pdf"&gt;LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds&lt;/a&gt;&lt;br /&gt;Minghua Liu, Yin Zhou, Charles R. Qi, &lt;b&gt;&lt;i&gt;Boqing Gong&lt;/i&gt;&lt;/b&gt;, Hao Su, Dragomir Anguelov &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2206.04453.pdf"&gt;The Missing Link: Finding Label Relations Across Datasets&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Mensink&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.16530.pdf"&gt;Learning Instance-Specific Adaptation for Cross-Domain Segmentation&lt;/a&gt;&lt;br /&gt;Yuliang Zou, &lt;b&gt;&lt;i&gt;Zizhao Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chun-Liang Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Han Zhang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tomas Pfister&lt;/i&gt;&lt;/b&gt;, Jia-Bin Huang &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.00679v1.pdf"&gt;Learning Audio-Video Modalities from Image Captions&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Paul Hongsuck Seo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Bryan Seybold&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anja Hauth&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;Santiago Manen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.06773.pdf"&gt;TL;DW? Summarizing Instructional Videos with Task Relevance &amp;amp; Cross-Modal Saliency&lt;/a&gt;&lt;br /&gt;Medhini Narasimhan&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chen Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;, Trevor Darrell, Anna Rohrbach, &lt;b&gt;&lt;i&gt;Cordelia Schmid&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10225.pdf"&gt;On Label Granularity and Object Localization&lt;/a&gt;&lt;br /&gt;Elijah Cole, &lt;b&gt;&lt;i&gt;Kimberly Wilber&lt;/i&gt;&lt;/b&gt;, Grant Van Horn, &lt;b&gt;&lt;i&gt;Xuan Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Marco Fornoni&lt;/i&gt;&lt;/b&gt;, Pietro Perona, Serge Belongie, &lt;b&gt;&lt;i&gt;Andrew Howard&lt;/i&gt;&lt;/b&gt;, Oisin Mac Aodha &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.10712.pdf"&gt;Disentangling Architecture and Training for Optical Flow&lt;/a&gt;&lt;br /&gt;  &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Charles Herrmann&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael Rubinstein&lt;/i&gt;&lt;/b&gt;,&lt;b&gt; &lt;i&gt;David J. Fleet&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;William T. Freeman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.13061.pdf"&gt;NewsStories: Illustrating Articles with Visual Summaries&lt;/a&gt;&lt;br /&gt;Reuben Tan, Bryan Plummer, Kate Saenko, &lt;b&gt;&lt;i&gt;J.P. Lewis&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Avneesh Sud&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Leung&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.09932.pdf"&gt;Improving GANs for Long-Tailed Data Through Group Spectral Regularization&lt;/a&gt;&lt;br /&gt;Harsh Rangwani, Naman Jaswani, &lt;b&gt;&lt;i&gt;Tejan Karmali&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Venkatesh Babu Radhakrishnan &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.10235.pdf"&gt;Planes vs. Chairs: Category-Guided 3D Shape Learning Without Any 3D Cues&lt;/a&gt;&lt;br /&gt;Zixuan Huang, Stefan Stojanov, Anh Thai, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, James Rehg &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.03354.pdf"&gt;A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch&lt;/a&gt;&lt;br /&gt;Patsorn Sangkloy, &lt;b&gt;&lt;i&gt;Wittawat Jitkrittum&lt;/i&gt;&lt;/b&gt;, Diyi Yang, James Hays &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.09171.pdf"&gt;Learned Monocular Depth Priors in Visual-Inertial Initialization&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Yunwen Zhou&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Abhishek Kar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eric L. Turner&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Adarsh Kowdle&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Chao Guo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ryan DuToit&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Konstantine Tsotsos&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.01403.pdf"&gt;How Stable are Transferability Metrics Evaluations?&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Andrea Agostinelli&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michal Pandy&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jasper Uijlings&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Mensink&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.02086.pdf"&gt;Data-Free Neural Architecture Search via Recursive Label Calibration&lt;/a&gt;&lt;br /&gt;Zechun Liu&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, Zhiqiang Shen, &lt;b&gt;&lt;i&gt;Yun Long&lt;/i&gt;&lt;/b&gt;, Eric Xing, Kwang-Ting Cheng, &lt;b&gt;&lt;i&gt;Chas H. Leichner&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2201.00392.pdf"&gt;Fast and High Quality Image Denoising via Malleable Convolution&lt;/a&gt;&lt;br /&gt;Yifan Jiang&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Bartlomiej Wronski&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jonathan T. Barron&lt;/i&gt;&lt;/b&gt;, Zhangyang Wang, &lt;b&gt;&lt;i&gt;Tianfan Xue&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.13247.pdf"&gt;Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation&lt;/a&gt;&lt;br /&gt;Jogendra Nath Kundu, Suvaansh Bhambri, Akshay R Kulkarni, Hiran Sarkar, &lt;br /&gt;&lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Venkatesh Babu Radhakrishnan &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.03353.pdf"&gt;Learning Online Multi-Sensor Depth Fusion&lt;/a&gt;&lt;br /&gt;Erik Sandström, Martin R. Oswald, Suryansh Kumar, Silvan Weder, Fisher Yu, &lt;b&gt;&lt;i&gt;Cristian Sminchisescu&lt;/i&gt;&lt;/b&gt;, Luc Van Gool &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.03764.pdf"&gt;Hierarchical Semantic Regularization of Latent Spaces in StyleGANs&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Tejan Karmali&lt;/i&gt;&lt;/b&gt;, Rishubh Parihar, Susmit Agrawal, Harsh Rangwani, &lt;b&gt;&lt;i&gt;Varun Jampani&lt;/i&gt;&lt;/b&gt;, Maneesh K Singh, Venkatesh Babu Radhakrishnan &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.13296.pdf"&gt;RayTran: 3D Pose Estimation and Shape Reconstruction of Multiple Objects from Videos with Ray-Traced Transformers&lt;/a&gt;&lt;br /&gt;Michał J Tyszkiewicz, &lt;b&gt;&lt;i&gt;Kevis-Kokitsi Maninis&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Stefan Popov&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Vittorio Ferrari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2107.12038.pdf"&gt;Neural Video Compression Using GANs for Detail Synthesis and Propagation&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Fabian Mentzer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eirikur Agustsson&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Johannes Ballé&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;David Minnen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Nick Johnston&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;George Toderici&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10664.pdf"&gt;Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset&lt;/a&gt;&lt;br /&gt;Grant Van Horn, Rui Qian, &lt;b&gt;&lt;i&gt;Kimberly Wilber&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Hartwig Adam&lt;/i&gt;&lt;/b&gt;, Oisin Mac Aodha, Serge Belongie &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.04267.pdf"&gt;Implicit Neural Representations for Image Compression&lt;/a&gt;&lt;br /&gt;Yannick Strümpler, Janis Postels, Ren Yang, Luc Van Gool, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.14673.pdf"&gt;3D Compositional Zero-Shot Learning with DeCompositional Consensus&lt;/a&gt;&lt;br /&gt;Muhammad Ferjad Naeem, Evin Pınar Örnek, Yongqin Xian, Luc Van Gool, &lt;b&gt;&lt;i&gt;Federico Tombari&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.17273.pdf"&gt;FindIt: Generalized Localization with Natural Language Queries&lt;/a&gt; (see &lt;a href="https://ai.googleblog.com/2022/09/findit-generalized-object-localization.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Weicheng Kuo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fred Bertsch&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Wei Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;AJ Piergiovanni&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Mohammad Saffar&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anelia Angelova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.09747.pdf"&gt;A Simple Single-Scale Vision Transformer for Object Detection and Instance Segmentation&lt;/a&gt;&lt;br /&gt;Wuyang Chen&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Xianzhi Du&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fan Yang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Lucas Beyer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaohua Zhai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tsung-Yi Lin&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huizhong Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jing Li&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaodan Song&lt;/i&gt;&lt;/b&gt;, Zhangyang Wang, &lt;b&gt;&lt;i&gt;Denny Zhou&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2209.04439.pdf"&gt;Improved Masked Image Generation with Token-Critic&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Jose Lezama&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Huiwen Chang&lt;/i&gt;&lt;/b&gt;, &lt;i&gt;&lt;b&gt;Lu Jiang&lt;/b&gt;&lt;/i&gt;, &lt;b&gt;&lt;i&gt;Irfan Essa&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.13876.pdf"&gt;Learning Discriminative Shrinkage Deep Networks for Image Deconvolution&lt;/a&gt;&lt;br /&gt;Pin-Hung Kuo, Jinshan Pan, Shao-Yi Chien, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.10141.pdf"&gt;AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation&lt;/a&gt;&lt;br /&gt;Efthymios Tzinis&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Scott Wisdom&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tal Remez&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;John Hershey&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2205.06230.pdf"&gt;Simple Open-Vocabulary Object Detection with Vision Transformers&lt;/a&gt;&lt;br /&gt;&lt;b&gt;&lt;i&gt;Matthias Minderer&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexey Gritsenko&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Austin C Stone&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Maxim Neumann&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Dirk Weißenborn&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexey Dosovitskiy&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Aravindh Mahendran&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anurag Arnab&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Mostafa Dehghani&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Zhuoran Shen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiao Wang&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Xiaohua Zhai&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Kipf&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Neil Houlsby&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;   &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2112.05892.pdf"&gt;COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality&lt;/a&gt;&lt;br /&gt;Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng, Farley Lai, &lt;b&gt;&lt;i&gt;Long Zhao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ting Liu&lt;/i&gt;&lt;/b&gt;, Mubbasir Kapadia, Hans Peter Graf &lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.00934.pdf"&gt;Video Question Answering with Iterative Video-Text Co-tokenization&lt;/a&gt; (see &lt;a href="https://ai.googleblog.com/2022/08/efficient-video-text-learning-with.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;AJ Piergiovanni&lt;/i&gt;&lt;/b&gt;, Kairo Morton&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;&lt;i&gt;Weicheng Kuo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Michael S. Ryoo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Anelia Angelova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.11430.pdf"&gt;Class-Agnostic Object Detection with Multi-modal Transformer&lt;/a&gt;&lt;br /&gt;Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2202.04901.pdf"&gt;FILM: Frame Interpolation for Large Motion&lt;/a&gt;&lt;b&gt; &lt;/b&gt;(see &lt;a href="https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;&lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Janne Kontkanen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Eric Tabellion&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Caroline Pantofaru&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Brian Curless&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.12824.pdf"&gt;Compositional Human-Scene Interaction Synthesis with Semantic Control&lt;/a&gt;&lt;br /&gt;Kaifeng Zhao, Shaofei Wang, Yan Zhang, &lt;b&gt;&lt;i&gt;Thabo Beeler,&lt;/i&gt;&lt;/b&gt; Siyu Tang &lt;/p&gt;&lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Workshops&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.latinxinai.org/eccv-2022"&gt;LatinX in AI&lt;/a&gt;&lt;br /&gt;Mentors include: &lt;i&gt;&lt;b&gt;José Lezama&lt;/b&gt;&lt;/i&gt;&lt;br /&gt;Keynote Speakers include: &lt;b&gt;&lt;i&gt;Andre Araujo&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cveu.github.io/"&gt;AI for Creative Video Editing and Understanding&lt;/a&gt;&lt;br /&gt;Keynote Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Negar Rostamzadeh&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://l2id.github.io/l2id2022/"&gt;Learning With Limited and Imperfect Data (L2ID)&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Xiuye Gu&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Sadeep Jayasumana&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://campworkshop.org/"&gt;International Challenge on Compositional and Multimodal Perception (CAMP)&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;Edward Vendrow&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sslwin.org/"&gt;Self-Supervised Learning: What is Next?&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Mathilde Caron&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;   Organizers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://eccv22-arow.github.io/"&gt;3rd Workshop on Adversarial Robustness In the Real World&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ekin Dogus Cubuk&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Xinyun Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Alexander Robey&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Nataniel Ruiz&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yutong Bai&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;       &lt;p&gt;&lt;a href="https://av4d.org/#speakers"&gt;AV4D: Visual Learning of Sounds in Spaces&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;John Hershey&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="http://mipi-challenge.org/index.html"&gt;Challenge on Mobile Intelligent Photography and Imaging (MIPI)&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Peyman Milanfar&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="http://www.robustvision.net/"&gt;Robust Vision Challenge 2022&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Alina Kuznetsova&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://computer-vision-in-the-wild.github.io/eccv-2022/"&gt;Computer Vision in the Wild&lt;/a&gt;&lt;br /&gt;Challenge Organizers include: &lt;b&gt;&lt;i&gt;Yi-Ting Chen&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ye Xia&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Yin Cui&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Yongqin Xian&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Neil Houlsby&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://sslad2022.github.io/pages/organizers.html"&gt;Self-Supervised Learning for Next-Generation Industry-Level Autonomous Driving (SSLAD)&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/view/rcv-at-eccv-2022/home"&gt;Responsible Computer Vision&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Been Kim&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Emily Denton&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://cross-modal-human-robot-interaction.github.io/"&gt;Cross-Modal Human-Robot Interaction&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Peter Anderson&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://workshop2022.isic-archive.com/#invited_speakers"&gt;ISIC Skin Image Analysis&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Steering Committee includes: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Dale Webster&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Yuan Liu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://sites.google.com/corp/view/hands2022/home"&gt;Observing and Understanding Hands in Action&lt;/a&gt;&lt;br /&gt;Sponsored by Google &lt;/p&gt; &lt;p&gt;&lt;a href="https://avvision.xyz/eccv22/"&gt;Autonomous Vehicle Vision (AVVision)&lt;/a&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://jrdb.erc.monash.edu/workshops/eccv2022"&gt;Visual Perception for Navigation in Human Environments: The JackRabbot Human Body Pose Dataset and Benchmark&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;i&gt;&lt;b&gt;Edward Vendrow&lt;/b&gt;&lt;/i&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://languagefor3dscenes.github.io/ECCV2022/"&gt;Language for 3D Scenes&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Jason Baldridge&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Leonidas Guibas&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://computerperception.github.io/"&gt;Designing and Evaluating Computer Perception Systems (CoPe)&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://learn3dg.github.io/"&gt;Learning To Generate 3D Shapes and Scenes&lt;/a&gt;&lt;br /&gt;Panelists include: &lt;b&gt;&lt;i&gt;Pete Florence&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://data.vision.ee.ethz.ch/cvl/aim22/"&gt;Advances in Image Manipulation&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;George Toderici&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Ming-Hsuan Yang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/view/tie-eccv2022"&gt;TiE: Text in Everything&lt;/a&gt;&lt;br /&gt;Challenge Organizers include: &lt;b&gt;&lt;i&gt;Shangbang Long&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Siyang Qin&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Aishwarya Agrawal&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ilr-workshop.github.io/ECCVW2022/"&gt;Instance-Level Recognition&lt;/a&gt;&lt;br /&gt;Organizing Committee: &lt;b&gt;&lt;i&gt;Andre Araujo&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Bingyi Cao&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Tobias Weyand&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Mathilde Caron&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://what-is-motion-for.github.io/"&gt;What Is Motion For?&lt;/a&gt;&lt;br /&gt;Organizing Committee: &lt;b&gt;&lt;i&gt;Deqing Sun&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fitsum Reda&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Charles Herrmann&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Tali Dekel&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://ngr-co3d.github.io/"&gt;Neural Geometry and Rendering: Advances and the Common Objects in 3D Challenge&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://geometry.stanford.edu/voli/"&gt;Visual Object-Oriented Learning Meets Interaction: Discovery, Representations, and Applications&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Klaus Greff&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Thomas Kipf&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Leonidas Guibas&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://wvbsd.github.io/2022/index.html"&gt;Vision with Biased or Scarce Data (VBSD)&lt;/a&gt;&lt;br /&gt;Program Committee includes: &lt;b&gt;&lt;i&gt;Yizhou Wang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://motcomplex.github.io/"&gt;Multiple Object Tracking and Segmentation in Complex Environments&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Xingyi Zhou&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Fisher Yu&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://vipriors.github.io/"&gt;3rd Visual Inductive Priors for Data-Efficient Deep Learning Workshop&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Ekin Dogus Cubuk&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://deeperaction.github.io/"&gt;DeeperAction: Detailed Video Action Understanding and Anomaly Recognition&lt;/a&gt;&lt;br /&gt;Advisors include: &lt;b&gt;&lt;i&gt;Rahul Sukthankar&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://signlanguageworkshop.github.io/"&gt;Sign Language Understanding Workshop and Sign Language Recognition, Translation &amp;amp; Production Challenge&lt;/a&gt;&lt;br /&gt;Organizing Committee includes: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Andrew Zisserman&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://ego4d-data.org/workshops/eccv22/"&gt;Ego4D: First-Person Multi-Modal Video Understanding&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Michal Irani&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://vcmi.inesctec.pt/aimia_eccv/"&gt;AI-Enabled Medical Image Analysis: Digital Pathology &amp;amp; Radiology/COVID19&lt;/a&gt;&lt;br /&gt;Program Chairs include: &lt;b&gt;&lt;i&gt;Po-Hsuan Cameron Chen&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Workshop Partner: &lt;b&gt;&lt;i&gt;Google Health&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://www.votchallenge.net/vot2022/index.html"&gt;Visual Object Tracking Challenge (VOT 2022)&lt;/a&gt;&lt;br /&gt;Technical Committee includes: &lt;b&gt;&lt;i&gt;Christoph Mayer&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://iplab.dmi.unict.it/acvr2022/"&gt;Assistive Computer Vision and Robotics&lt;/a&gt;&lt;br /&gt;Technical Committee includes: &lt;b&gt;&lt;i&gt;Maja Mataric&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/view/egocentric-hand-body-activity"&gt;Human Body, Hands, and Activities from Egocentric and Multi-View Cameras&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Francis Engelmann&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/view/mono3d-eccv-workshop"&gt;Frontiers of Monocular 3D Perception: Implicit x Explicit&lt;/a&gt;&lt;br /&gt;Panelists include: &lt;b&gt;&lt;i&gt;Pete Florence&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;div style="line-height: 40%;"&gt;    &lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Tutorials&lt;/h2&gt;&lt;p&gt;&lt;a href="https://feichtenhofer.github.io/eccv2022-ssl-tutorial/"&gt;Self-Supervised Representation Learning in Computer Vision&lt;/a&gt;&lt;br /&gt;Invited Speakers include: &lt;b&gt;&lt;i&gt;Ting Chen&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/berkeley.edu/nerf-tutorial/home"&gt;Neural Volumetric Rendering for Computer Vision&lt;/a&gt;&lt;br /&gt;Organizers include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Pratul Srinivasan&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Jon Barron&lt;/i&gt;&lt;/b&gt;&lt;br /&gt;Presenters include: &lt;b&gt;&lt;i&gt;Ben Mildenhall&lt;/i&gt;&lt;/b&gt;, &lt;b&gt;&lt;i&gt;Pratul Srinivasan&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://sites.google.com/corp/g.ucla.edu/eccv2022-nas/home"&gt;New Frontiers in Efficient Neural Architecture Search!&lt;/a&gt;&lt;br /&gt;Speakers include: &lt;b&gt;&lt;i&gt;Ruochen Wang&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;!--Footnotes themselves at the bottom.--&gt;&lt;hr width="80%" /&gt;&lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;br /&gt;  &lt;a name="1"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;Work done while at Google.&lt;a href="#top1"&gt; &amp;nbsp;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;p&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4407074621491257938/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/google-at-eccv-2022.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4407074621491257938" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4407074621491257938" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/google-at-eccv-2022.html" rel="alternate" title="Google at ECCV 2022 " type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWmLScVcpCyi8puS5FYC4xNN88CWeK0-77JiMlowYPlxCPym5T4zX7iKIuzIjwtDnUubUJ0yM8Xa6Mt83N_-YoBgyrZH_dwSL6YgoxUEgGf1tNuR5fSWeRY61Ut0TOhlzUuTJlKs6fyBF3JTVzAlbEOUgtGb_5gdgFBIXWFCJ0utT8kuUh3rKIjw4L1g/s72-c/about_hero.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-553665404692333245</id><published>2022-10-20T13:39:00.015-07:00</published><updated>2022-10-21T09:27:14.310-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations</title><content type="html">&lt;span class="byline-author"&gt;Posted by Wenhao Yu, Research Scientist, Robotics at Google, and Kuang-Huei Lee, Research Engineer, Google Research, Brain team&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Evolution_strategy"&gt;Evolution strategy&lt;/a&gt; (ES) is a family of optimization techniques inspired by the ideas of &lt;a href="https://en.wikipedia.org/wiki/Natural_selection"&gt;natural selection&lt;/a&gt;: a population of candidate solutions are usually evolved over generations to better adapt to an optimization objective. ES has been applied to a variety of challenging decision making problems, such as &lt;a href="https://openreview.net/forum?id=NDYbXf-DvwZ"&gt;legged locomotion&lt;/a&gt;, &lt;a href="https://ieeexplore.ieee.org/document/9307102"&gt;quadcopter control&lt;/a&gt;, and even &lt;a href="https://ieeexplore.ieee.org/abstract/document/9477182"&gt;power system control&lt;/a&gt;.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Compared to gradient-based &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;reinforcement learning&lt;/a&gt; (RL) methods like &lt;a href="https://arxiv.org/abs/1707.06347"&gt;proximal policy optimization&lt;/a&gt; (PPO) and &lt;a href="https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html"&gt;soft actor-critic&lt;/a&gt; (SAC), ES has several advantages. First, ES directly explores in the space of controller parameters, while gradient-based methods often explore within a limited action space, which indirectly influences the controller parameters. More direct exploration has been shown to &lt;a href="https://openai.com/blog/better-exploration-with-parameter-noise/"&gt;boost learning performance&lt;/a&gt; and enable large scale data collection with parallel computation. Second, a major challenge in RL is long-horizon credit assignment, e.g., when a robot accomplishes a task in the end, determining which actions it performed in the past were the most critical and should be assigned a greater reward. Since ES directly considers the total reward, it relieves researchers from needing to explicitly handle credit assignment. In addition, because ES does not rely on gradient information, it can naturally handle highly non-smooth objectives or controller architectures where gradient computation is non-trivial, such as &lt;a href="https://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html"&gt;meta–reinforcement learning&lt;/a&gt;. However, a major weakness of ES-based algorithms is their difficulty in scaling to problems that require high-dimensional sensory inputs to encode the environment dynamics, such as training robots with complex vision inputs. &lt;/p&gt;&lt;p&gt;In this work, we propose “&lt;a href="https://arxiv.org/abs/2207.13224"&gt;PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations&lt;/a&gt;”, a learning algorithm that combines &lt;a href="https://en.wikipedia.org/wiki/Feature_learning"&gt;representation learning&lt;/a&gt; and ES to effectively solve high dimensional problems in a scalable way. The core idea is to leverage &lt;a href="https://arxiv.org/abs/cond-mat/9902341"&gt;predictive information&lt;/a&gt;, a representation learning objective, to obtain a compact representation of the high-dimensional environment dynamics, and then apply &lt;a href="https://arxiv.org/abs/1803.07055"&gt;Augmented Random Search&lt;/a&gt; (ARS), a popular ES algorithm, to transform the learned compact representation into robot actions. We tested PI-ARS on the challenging problem of visual-locomotion for legged robots. PI-ARS enables fast training of performant vision-based locomotion controllers that can traverse a variety of difficult environments. Furthermore, the controllers trained in simulated environments successfully transfer to a real quadruped robot. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s480/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="270" data-original-width="480" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;PI-ARS trains reliable visual-locomotion policies that are transferable to the real world.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Predictive Information&lt;/h2&gt;&lt;p&gt;A good representation for policy learning should be both &lt;em&gt;compressive&lt;/em&gt;, so that ES can focus on solving a much lower dimensional problem than learning from raw observations would entail, and &lt;em&gt;task-critical&lt;/em&gt;, so the learned controller has all the necessary information needed to learn the optimal behavior. For robotic control problems with high-dimensional input space, it is critical for the policy to understand the environment, including the dynamic information of both the robot itself and its surrounding objects.  &lt;/p&gt;&lt;p&gt;As such, we propose an observation encoder that preserves information from the raw input observations that allows the policy to predict the future states of the environment, thus the name &lt;em&gt;predictive information&lt;/em&gt; (PI). More specifically, we optimize the encoder such that the encoded version of what the robot has seen and planned in the past can accurately predict what the robot might see and be rewarded in the future. One mathematical tool to describe such a property is that of &lt;a href="https://en.wikipedia.org/wiki/Mutual_information"&gt;mutual information&lt;/a&gt;, which measures the amount of information we obtain about one random variable &lt;em&gt;X&lt;/em&gt; by observing another random variable &lt;em&gt;Y&lt;/em&gt;. In our case, &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt; would be what the robot saw and planned in the past, and what the robot sees and is rewarded in the future. Directly optimizing the mutual information objective is &lt;a href="https://arxiv.org/pdf/1905.06922.pdf"&gt;a challenging problem&lt;/a&gt; because we usually only have access to samples of the random variables, but not their underlying distributions. In this work we follow &lt;a href="https://arxiv.org/abs/2007.12401"&gt;a previous approach&lt;/a&gt; that uses &lt;a href="https://arxiv.org/abs/1807.03748"&gt;InfoNCE&lt;/a&gt;, a contrastive variational bound on mutual information to optimize the objective. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s1988/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="946" data-original-width="1988" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left&lt;/b&gt;: We use representation learning to encode PI of the environment. &lt;b&gt;Right&lt;/b&gt;: We train the representation by replaying trajectories from the replay buffer and maximize the predictability between the observation and motion plan in the past and the observation and reward in the future of the trajectory.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Predictive Information with Augmented Random Search&lt;/h2&gt;&lt;p&gt;Next, we combine PI with &lt;a href="https://arxiv.org/abs/1803.07055"&gt;Augmented Random Search&lt;/a&gt; (ARS), an algorithm that has shown excellent optimization performance for challenging decision-making tasks. At each iteration of ARS, it samples a population of perturbed controller parameters, evaluates their performance in the testing environment, and then computes a gradient that moves the controller towards the ones that performed better.  &lt;/p&gt;&lt;p&gt;We use the learned compact representation from PI to connect PI and ARS, which we call PI-ARS. More specifically, ARS optimizes a controller that takes as input the learned compact representation PI and predicts appropriate robot commands to achieve the task. By optimizing a controller with smaller input space, it allows ARS to find the optimal solution more efficiently. Meanwhile, we use the data collected during ARS optimization to further improve the learned representation, which is then fed into the ARS controller in the next iteration. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s1100/image1.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="750" data-original-width="1100" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s16000/image1.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An overview of the PI-ARS data flow. Our algorithm interleaves between two steps: 1) optimizing the PI objective that updates the policy, which is the weights for the neural network that extracts the learned representation; and 2) sampling new trajectories and updating the controller parameters using ARS.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Visual-Locomotion for Legged Robots&lt;/h2&gt;&lt;p&gt;We evaluate PI-ARS on the problem of visual-locomotion for legged robots. We chose this problem for two reasons: visual-locomotion is a key bottleneck for legged robots to be applied in real-world applications, and the high-dimensional vision-input to the policy and the complex dynamics in legged robots make it an ideal test-case to demonstrate the effectiveness of the PI-ARS algorithm. A demonstration of our task setup in simulation can be seen below. Policies are first trained in simulated environments, and then transferred to hardware. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s480/image2.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="270" data-original-width="480" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s16000/image2.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An illustration of the visual-locomotion task setup. The robot is equipped with two cameras to observe the environment (illustrated by the transparent pyramids). The observations and robot state are sent to the policy to generate a high-level motion plan, such as feet landing location and desired moving speed. The high-level motion plan is then achieved by a low-level Motion Predictive Control (MPC) controller.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Experiment Results&lt;/h2&gt;&lt;p&gt;We first evaluate the PI-ARS algorithm on four challenging simulated tasks: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;Uneven stepping stones&lt;/em&gt;: The robot needs to walk over uneven terrain while avoiding gaps.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Quincuncial piles&lt;/em&gt;: The robot needs to avoid gaps both in front and sideways.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Moving platforms&lt;/em&gt;: The robot needs to walk over stepping stones that are randomly moving horizontally or vertically. This task illustrates the flexibility of learning a vision-based policy in comparison to explicitly reconstructing the environment.  &lt;/li&gt;&lt;li&gt;&lt;em&gt;Indoor navigation&lt;/em&gt;: The robot needs to navigate to a random location while avoiding obstacles in an indoor environment. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As shown below, PI-ARS is able to significantly outperform ARS in all four tasks in terms of the total task reward it can obtain (by 30-50%). &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s1417/image11.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1417" data-original-width="900" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s16000/image11.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left: &lt;/b&gt;Visualization of PI-ARS policy performance in simulation. &lt;b&gt;Right: &lt;/b&gt;Total task reward (i.e., episode return) for PI-ARS (&lt;b&gt;green line&lt;/b&gt;) and ARS (&lt;b&gt;red line&lt;/b&gt;). The PI-ARS algorithm significantly outperforms ARS on four challenging visual-locomotion tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We further deploy the trained policies to a real &lt;a href="http://www.unitree.cc/e/action/ShowInfo.php?classid=6&amp;amp;id=1"&gt;Laikago&lt;/a&gt; robot on two tasks: &lt;a href="https://youtu.be/59vMC3fTsuQ?t=20"&gt;random stepping stone&lt;/a&gt; and &lt;a href="https://youtu.be/59vMC3fTsuQ?t=43"&gt;indoor navigation&lt;/a&gt;. We demonstrate that our trained policies can successfully handle real-world tasks. Notably, the success rate of the random stepping stone task improved from 40% in &lt;a href="https://openreview.net/forum?id=NDYbXf-DvwZ"&gt;the prior work&lt;/a&gt; to 100%. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s480/image1.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="270" data-original-width="480" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s16000/image1.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;PI-ARS trained policy enables a real Laikago robot to navigate around obstacles.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this work, we present a new learning algorithm, PI-ARS, that combines gradient-based representation learning with gradient-free evolutionary strategy algorithms to leverage the advantages of both. PI-ARS enjoys the effectiveness, simplicity, and parallelizability of gradient-free algorithms, while relieving a key bottleneck of ES algorithms on handling high-dimensional problems by optimizing a low-dimensional representation. We apply PI-ARS to a set of challenging visual-locomotion tasks, among which PI-ARS significantly outperforms the state of the art. Furthermore, we validate the policy learned by PI-ARS on a real quadruped robot. It enables the robot to walk over randomly-placed stepping stones and navigate in an indoor space with obstacles. Our method opens the possibility of incorporating modern large neural network models and large-scale data into the field of evolutionary strategy for robotics control. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;  &lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We would like to thank our paper co-authors: Ofir Nachum, Tingnan Zhang, Sergio Guadarrama, and Jie Tan. We would also like to thank Ian Fischer and John Canny for valuable feedback.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/553665404692333245/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/553665404692333245" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/553665404692333245" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html" rel="alternate" title="PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2594486859371625986</id><published>2022-10-20T09:58:00.010-07:00</published><updated>2022-10-21T09:42:42.455-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">MUSIQ: Assessing Image Aesthetic and Technical Quality with Multi-scale Transformers</title><content type="html">&lt;span class="byline-author"&gt;Posted by Junjie Ke, Senior Software Engineer, and Feng Yang, Senior Staff Software Engineer, Google Research&lt;/span&gt;&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s637/MUSIQ.png" style="display: none;" /&gt;&lt;p&gt;Understanding the aesthetic and technical quality of images is important for providing a better user visual experience. &lt;a href="https://en.wikipedia.org/wiki/Image_quality"&gt;Image quality assessment&lt;/a&gt; (IQA) uses models to build a bridge between an image and a user's subjective perception of its quality. In the deep learning era, many IQA approaches, such as &lt;a href="https://ai.googleblog.com/2022/08/uvq-measuring-youtubes-perceptual-video.html"&gt;NIMA&lt;/a&gt;, have achieved success by leveraging the power of &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;convolutional neural networks&lt;/a&gt; (CNNs). However, CNN-based IQA models are often constrained by the fixed-size input requirement in batch training, i.e.,&lt;em&gt; &lt;/em&gt;the input images need to be resized or cropped to a fixed size shape. This preprocessing is problematic for IQA because images can have very different aspect ratios and resolutions. Resizing and cropping can impact image composition or introduce distortions, thus changing the quality of the image.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8GOrjNuB2EyMjRI_LdmmVBHWSR1DCD06iyBk-8p870oUgMOYeybTiuHEFFn5POZHEhNxc9oqPiAgbzqJWM9a809dVxhmnuJFmtYfdrR8y6AKBBK3W0WRz7mCIQda12i8QMCOO2NtF0tJUBihIbMZI7hFehuprkRfGI_3l2ToVbSyeKoz8-r69yTrjeA/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="511" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8GOrjNuB2EyMjRI_LdmmVBHWSR1DCD06iyBk-8p870oUgMOYeybTiuHEFFn5POZHEhNxc9oqPiAgbzqJWM9a809dVxhmnuJFmtYfdrR8y6AKBBK3W0WRz7mCIQda12i8QMCOO2NtF0tJUBihIbMZI7hFehuprkRfGI_3l2ToVbSyeKoz8-r69yTrjeA/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;In CNN-based models, images need to be resized or cropped to a fixed shape for batch training. However, such preprocessing can alter the image aspect ratio and composition, thus impacting image quality. &lt;a href="https://www.flickr.com/photos/ddebold/3052189192"&gt;Original image&lt;/a&gt; used under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ke_MUSIQ_Multi-Scale_Image_Quality_Transformer_ICCV_2021_paper.pdf"&gt;MUSIQ: Multi-scale Image Quality Transformer&lt;/a&gt;”, published at &lt;a href="https://iccv2021.thecvf.com/home"&gt;ICCV 2021&lt;/a&gt;, we propose a patch-based multi-scale image quality &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;transformer&lt;/a&gt; (MUSIQ) to bypass the CNN constraints on fixed input size and predict the image quality effectively on native-resolution images. The MUSIQ model supports the processing of full-size image inputs with varying aspect ratios and resolutions and allows multi-scale feature extraction to capture image quality at different granularities. To support positional encoding in the multi-scale representation, we propose a novel hash-based 2D spatial embedding combined with an embedding that captures the image scaling. We apply MUSIQ on four large-scale IQA datasets, demonstrating consistent state-of-the-art results across three technical quality datasets (&lt;a href="https://baidut.github.io/PaQ-2-PiQ/"&gt;PaQ-2-PiQ&lt;/a&gt;, &lt;a href="http://database.mmsp-kn.de/koniq-10k-database.html"&gt;KonIQ-10k&lt;/a&gt;, and &lt;a href="https://github.com/h4nwei/SPAQ"&gt;SPAQ&lt;/a&gt;) and comparable performance to that of state-of-the-art models on the aesthetic quality dataset &lt;a href="http://refbase.cvc.uab.es/files/MMP2012a.pdf"&gt;AVA&lt;/a&gt;. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s637/MUSIQ.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="530" data-original-width="637" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s16000/MUSIQ.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The patch-based MUSIQ model can process the full-size image and extract multi-scale features, which better aligns with a person’s typical visual response.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In the following figure, we show a sample of images, their MUSIQ score, and their mean opinion score (MOS) from multiple human raters in the brackets. The range of the score is from 0 to 100, with 100 being the highest perceived quality. As we can see from the figure, MUSIQ predicts high scores for images with high aesthetic quality and high technical quality, and it predicts low scores for images that are not aesthetically pleasing (low aesthetic quality) or that contain visible distortions (low technical quality). &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td rowspan="2" style="text-align: left;"&gt;High quality&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiztsZ31x6DTJz03BbHk3jlNbqutJWQIvbSInQYJuAQaMxVynSMlcaKzdRFOvR_6bWp1UujU-Rzx8qffWIArNT1hyTHrk136OkGF1XXPW2HYwNNVwNZrqaxZJDB-ScT9G1hD7xns8BYHxPFC0OORytBsPSAcvs5KH0S6YEkqp0N1TN6d6IR-y6VHTdqg/s1024/image10.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiztsZ31x6DTJz03BbHk3jlNbqutJWQIvbSInQYJuAQaMxVynSMlcaKzdRFOvR_6bWp1UujU-Rzx8qffWIArNT1hyTHrk136OkGF1XXPW2HYwNNVwNZrqaxZJDB-ScT9G1hD7xns8BYHxPFC0OORytBsPSAcvs5KH0S6YEkqp0N1TN6d6IR-y6VHTdqg/w200-h150/image10.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr0DKaAoO6qTrJo3hXP8UM3D4AB8gQeNI22Q2QphBVGgn-5v84tjhH3ZWTlGtlUoPdlcx54dM93Qi04MuN7eBbj9WlT8Qxy6B2Us4kcn_53FH28MnTtGCzMPhjCVGIgXRL8ZEMeO-7iue7sNEGxBtgx2bI-eKDQAondM8Dfjb1FaybFgUQji4UU9-0vQ/s1024/image9.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr0DKaAoO6qTrJo3hXP8UM3D4AB8gQeNI22Q2QphBVGgn-5v84tjhH3ZWTlGtlUoPdlcx54dM93Qi04MuN7eBbj9WlT8Qxy6B2Us4kcn_53FH28MnTtGCzMPhjCVGIgXRL8ZEMeO-7iue7sNEGxBtgx2bI-eKDQAondM8Dfjb1FaybFgUQji4UU9-0vQ/w200-h150/image9.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="font-size: small; text-align: center;"&gt;76.10 [74.36]&lt;/td&gt;    &lt;td style="font-size: small; text-align: center;"&gt;69.29 [70.92]&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td rowspan="2" style="text-align: left;"&gt;Low aesthetics quality&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAmXa-tOH5bOTKSqfUq0vPszEMxbCsC3dHhclyFaTwx5hvkBhc6uZzwRLcVDCPmP5KXN0PVt0hvIKrgpAaw1Qd1ujuBFL-gBAmM4BZJQ5fR3K-_ItIiUKz-UgEOAPMBeTEVbEQkKx8Zm8KKQjaGHWIE6LIXSQMM6tfGur89uUU7O8HURJF9QfYWU3h6Q/s1024/image5.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAmXa-tOH5bOTKSqfUq0vPszEMxbCsC3dHhclyFaTwx5hvkBhc6uZzwRLcVDCPmP5KXN0PVt0hvIKrgpAaw1Qd1ujuBFL-gBAmM4BZJQ5fR3K-_ItIiUKz-UgEOAPMBeTEVbEQkKx8Zm8KKQjaGHWIE6LIXSQMM6tfGur89uUU7O8HURJF9QfYWU3h6Q/w200-h150/image5.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0-h8-32jlpKIrqt_HYtFEUX8PwZgmd1ig1_yUUpIRQOruypWk_AdywrmMPeISTGTxHfSy4WZy5xwQBCT6EweDT5jC4I9_48R2Aqs-x3-JvQM1lSPxmDfvjo6V8MuZjI3tAU4R9onsJQDSk4BoTSOeziNMN5A0N9trpq2PllpP3GiVW2pBQKyCb4kDqw/s1024/image11.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh0-h8-32jlpKIrqt_HYtFEUX8PwZgmd1ig1_yUUpIRQOruypWk_AdywrmMPeISTGTxHfSy4WZy5xwQBCT6EweDT5jC4I9_48R2Aqs-x3-JvQM1lSPxmDfvjo6V8MuZjI3tAU4R9onsJQDSk4BoTSOeziNMN5A0N9trpq2PllpP3GiVW2pBQKyCb4kDqw/w200-h150/image11.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="font-size: small; text-align: center;"&gt;55.37 [53.18]&lt;/td&gt;    &lt;td style="font-size: small; text-align: center;"&gt;32.50 [35.47]&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td rowspan="2" style="text-align: left;"&gt;Low technical quality&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwCvrl2K6kVpYBy7SWwmTFfg7Xv09im0bnS_UYHF05O7IQTlFg5kktUF-LYCfklepbSKncood20CB6UroQhE8hpV_7xm1EKhKElTD5EOIOx8gL1psXQbdTsTe1HE43YBzBfhjMCDBe2NI7eq8fKrXJrw36QwmwoUQueQQp2IIVqWuWGKsBdi8ET57e5g/s1024/image2.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwCvrl2K6kVpYBy7SWwmTFfg7Xv09im0bnS_UYHF05O7IQTlFg5kktUF-LYCfklepbSKncood20CB6UroQhE8hpV_7xm1EKhKElTD5EOIOx8gL1psXQbdTsTe1HE43YBzBfhjMCDBe2NI7eq8fKrXJrw36QwmwoUQueQQp2IIVqWuWGKsBdi8ET57e5g/w200-h150/image2.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh--6ooAFsWYmPRh9OlTO9mA8GE0yGoy_e-mIAe2MT1XVuFMNABUw3XE6npSoyfe0pN4QXEl4GHxs_RzBx-E8EK5ge3QUBi2ZLRrNjEsevYoIRQSLPmiJkR80GT1glU-DB7xwxrU7zAMfDXnNt7_-6fLNaKI_qfxn1foBdx8ZRMv_PeJDiTZbZ1DYp6Ow/s1024/image12.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="768" data-original-width="1024" height="150" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh--6ooAFsWYmPRh9OlTO9mA8GE0yGoy_e-mIAe2MT1XVuFMNABUw3XE6npSoyfe0pN4QXEl4GHxs_RzBx-E8EK5ge3QUBi2ZLRrNjEsevYoIRQSLPmiJkR80GT1glU-DB7xwxrU7zAMfDXnNt7_-6fLNaKI_qfxn1foBdx8ZRMv_PeJDiTZbZ1DYp6Ow/w200-h150/image12.png" width="200" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="font-size: small; text-align: center;"&gt;14.93 [14.38]&lt;/td&gt;    &lt;td style="font-size: small; text-align: center;"&gt;15.24 [11.86]&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Predicted MUSIQ score (and ground truth) on images from the &lt;a href="http://database.mmsp-kn.de/koniq-10k-database.html"&gt;KonIQ-10k&lt;/a&gt; dataset. &lt;b&gt;Top: &lt;/b&gt;MUSIQ predicts high scores for high quality images. &lt;b&gt;Middle: &lt;/b&gt;MUSIQ predicts low scores for images with low aesthetic quality, such as images with poor composition or lighting. &lt;b&gt;Bottom: &lt;/b&gt;MUSIQ predicts low scores for images with low technical quality, such as images with visible distortion artifacts (e.g., blurry, noisy).&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;The Multi-scale Image Quality Transformer&lt;/h2&gt;&lt;p&gt;MUSIQ tackles the challenge of learning IQA on full-size images. Unlike CNN-models that are often constrained to fixed resolution, MUSIQ can handle inputs with arbitrary aspect ratios and resolutions. &lt;/p&gt;&lt;p&gt;To accomplish this, we first make a multi-scale representation of the input image, containing the native resolution image and its resized variants. To preserve the image composition, we maintain its aspect ratio during resizing. After obtaining the pyramid of images, we then partition the images at different scales into fixed-size patches that are fed into the model.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBa-dfw5Y5O_KldCjZw3c6XM2u96fM4Tr5CV21fpE2fzS-tibqIzRKb834UySVwXeoxu2Yn4RfQM40M8YJPpWWJOIiIwdnnH8RoXJB82RIESYNFYEACv2r1YCKyghVtl9WLW4qFrVKDzyvr25oQGhcfKiIC31Csf0izXpCFpQ2f0Ym2W-LRe4-KKs7vA/s1344/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="708" data-original-width="1344" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhBa-dfw5Y5O_KldCjZw3c6XM2u96fM4Tr5CV21fpE2fzS-tibqIzRKb834UySVwXeoxu2Yn4RfQM40M8YJPpWWJOIiIwdnnH8RoXJB82RIESYNFYEACv2r1YCKyghVtl9WLW4qFrVKDzyvr25oQGhcfKiIC31Csf0izXpCFpQ2f0Ym2W-LRe4-KKs7vA/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Illustration of the multi-scale image representation in MUSIQ.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Since patches are from images of varying resolutions, we need to effectively encode the multi-aspect-ratio multi-scale input into a sequence of tokens, capturing both the pixel, spatial, and scale information. To achieve this, we design three encoding components in MUSIQ, including: 1) a patch encoding module to encode patches extracted from the multi-scale representation; 2) a novel hash-based spatial embedding module to encode the 2D spatial position for each patch; and 3) a learnable scale embedding to encode different scales. In this way, we can effectively encode the multi-scale input as a sequence of tokens, serving as the input to the Transformer encoder. &lt;/p&gt;&lt;p&gt;To predict the final image quality score, we use the standard approach of prepending an additional learnable “&lt;a href="https://aclanthology.org/N19-1423/"&gt;classification token&lt;/a&gt;” (CLS). The CLS token state at the output of the Transformer encoder serves as the final image representation. We then add a fully connected layer on top to predict the IQS. The figure below provides an overview of the MUSIQ model. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbR9OlJ3OqPoS9WsMVVIu4QQJBsM6ylGupmX0KKpQ0NulLa3IlH1u7hGwDY_EMZemhiC6nSE-5LQJeCRXDleG_Ko5NyAeRzoXqBBElywfhq1QoL8AXHTUTKnuVf_JjgR2phf-BOxEWR98Yx2_dRRbCJ8NIC3Z1mjfDPeYAu-vM7iHT6iSUZZyJvsm0jg/s1999/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1158" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbR9OlJ3OqPoS9WsMVVIu4QQJBsM6ylGupmX0KKpQ0NulLa3IlH1u7hGwDY_EMZemhiC6nSE-5LQJeCRXDleG_Ko5NyAeRzoXqBBElywfhq1QoL8AXHTUTKnuVf_JjgR2phf-BOxEWR98Yx2_dRRbCJ8NIC3Z1mjfDPeYAu-vM7iHT6iSUZZyJvsm0jg/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Overview of MUSIQ. The multi-scale multi-resolution input will be encoded by three components: the scale embedding (SCE), the hash-based 2D spatial embedding (HSE), and the multi-scale patch embedding (MPE).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Since MUSIQ only changes the input encoding, it is compatible with any Transformer variants. To demonstrate the effectiveness of the proposed method, in our experiments we use the classic Transformer with a relatively lightweight setting so that the model size is comparable to &lt;a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf"&gt;ResNet-50&lt;/a&gt;. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Benchmark and Evaluation&lt;/h2&gt;&lt;p&gt;To evaluate MUSIQ, we run experiments on multiple large-scale IQA datasets. On each dataset, we report the &lt;a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"&gt;Spearman’s rank correlation coefficient&lt;/a&gt; (SRCC) and &lt;a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"&gt;Pearson linear correlation coefficient&lt;/a&gt; (PLCC) between our model prediction and the human evaluators’ mean opinion score. SRCC and PLCC are correlation metrics ranging from -1 to 1. Higher PLCC and SRCC means better alignment between model prediction and human evaluation. The graph below shows that MUSIQ outperforms other methods on &lt;a href="https://baidut.github.io/PaQ-2-PiQ/"&gt;PaQ-2-PiQ&lt;/a&gt;, &lt;a href="http://database.mmsp-kn.de/koniq-10k-database.html"&gt;KonIQ-10k&lt;/a&gt;, and &lt;a href="https://github.com/h4nwei/SPAQ"&gt;SPAQ&lt;/a&gt;.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz8OWuZJCzDHkEJo8ezsscPcFNDKcp0cWulefa4S2P0P9UiH4BglcOfak5zdnUckxGTaiF5yI85vC1JIH4ZBLQN4UEp-4CK5RJJIoX2hXM97oQ6F1KH3tpdDa773RB8kT45UePSuiNHX6OxEr9lRLwYU8ROuHsVMHkMDYjQE7kHiXp6hmWG4gL02h2sg/s1192/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="906" data-original-width="1192" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhz8OWuZJCzDHkEJo8ezsscPcFNDKcp0cWulefa4S2P0P9UiH4BglcOfak5zdnUckxGTaiF5yI85vC1JIH4ZBLQN4UEp-4CK5RJJIoX2hXM97oQ6F1KH3tpdDa773RB8kT45UePSuiNHX6OxEr9lRLwYU8ROuHsVMHkMDYjQE7kHiXp6hmWG4gL02h2sg/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Performance comparison of MUSIQ and previous state-of-the-art (SOTA) methods on four large-scale IQA datasets. On each dataset we compare the Spearman’s rank correlation coefficient (SRCC) and Pearson linear correlation coefficient (PLCC) of model prediction and ground truth.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Notably, the &lt;a href="https://baidut.github.io/PaQ-2-PiQ/"&gt;PaQ-2-PiQ&lt;/a&gt; test set is entirely composed of large pictures having at least one dimension exceeding 640 pixels. This is very challenging for traditional deep learning approaches, which require resizing. MUSIQ can outperform previous methods by a large margin on the full-size test set, which verifies its robustness and effectiveness. &lt;/p&gt;&lt;p&gt;It is also worth mentioning that previous CNN-based methods often required sampling as many as 20 crops for each image during testing. This kind of multi-crop ensemble is a way to mitigate the fixed shape constraint in the CNN models. But since each crop is only a sub-view of the whole image, the ensemble is still an approximate approach. Moreover, CNN-based methods both add additional inference cost for every crop and, because they sample different crops, they can introduce randomness in the result. In contrast, because MUSIQ takes the full-size image as input, it can directly learn the best aggregation of information across the full image and it only needs to run the inference once. &lt;/p&gt;&lt;p&gt;To further verify that the MUSIQ model captures different information at different scales, we visualize the attention weights on each image at different scales.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_NxfcTrlUG2pHSP4KwPJnfykJuon-5yBH4irHlfDMv0lGYmGLK6VUnbXnUS2phWcnJ4QabsiNt_fV_AlJYJ3bGOPEPNFZjnxpPaxrjATVBj-PC-aIzr1rk5UcV5GsZgw7Mv85PPJdJYJJ3T1t9HqW22s1dPjitfaBLmZ2X124GOA91lP2PE-xWdJ4wg/s637/Attention.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="498" data-original-width="637" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_NxfcTrlUG2pHSP4KwPJnfykJuon-5yBH4irHlfDMv0lGYmGLK6VUnbXnUS2phWcnJ4QabsiNt_fV_AlJYJ3bGOPEPNFZjnxpPaxrjATVBj-PC-aIzr1rk5UcV5GsZgw7Mv85PPJdJYJJ3T1t9HqW22s1dPjitfaBLmZ2X124GOA91lP2PE-xWdJ4wg/s16000/Attention.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Attention visualization from the output tokens to the multi-scale representation, including the original resolution image and two proportionally resized images. Brighter areas indicate higher attention, which means that those areas are more important for the model output. Images for illustration are taken from the &lt;a href="http://refbase.cvc.uab.es/files/MMP2012a.pdf"&gt;AVA&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We observe that MUSIQ tends to focus on more detailed areas in the full, high-resolution images and on more global areas on the resized ones. For example, for the flower photo above, the model’s attention on the original image is focusing on the pedal details, and the attention shifts to the buds at lower resolutions. This shows that the model learns to capture image quality at different granularities. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We propose a multi-scale image quality transformer (MUSIQ), which can handle full-size image input with varying resolutions and aspect ratios. By transforming the input image to a multi-scale representation with both global and local views, the model can capture the image quality at different granularities. Although MUSIQ is designed for IQA, it can be applied to other scenarios where task labels are sensitive to image resolution and aspect ratio. The MUSIQ model and checkpoints are available at our &lt;a href="https://github.com/google-research/google-research/tree/master/musiq"&gt;GitHub repository&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This work is made possible through a collaboration spanning several teams across Google. We’d like to acknowledge contributions from Qifei Wang, Yilin Wang and Peyman Milanfar.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/2594486859371625986/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/musiq-assessing-image-aesthetic-and.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2594486859371625986" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2594486859371625986" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/musiq-assessing-image-aesthetic-and.html" rel="alternate" title="MUSIQ: Assessing Image Aesthetic and Technical Quality with Multi-scale Transformers" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCIbYQlkOE0n8G7IFh_Uo4KhEafpAKdpnx9swpUA06IV4kFaqN2bTjd22zaSmhVUDmzpFOAqTH73AQNtXvffeaW3UZwlI54w427v7dsDsI8_8UQEvY198hGgTCu9f0upoDn33AYsq0fqUbwu12rwb9_FVzrSTTHNewaiJyMIpEDzMURKzFdzpZOubazQ/s72-c/MUSIQ.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7866944510509103804</id><published>2022-10-19T10:57:00.005-07:00</published><updated>2022-10-21T09:44:54.408-07:00</updated><title type="text">Do Modern ImageNet Classifiers Accurately Predict Perceptual Similarity?</title><content type="html">&lt;span class="byline-author"&gt;Posted by Manoj Kumar, Research Engineer, and Ekin Dogus Cubuk, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;The task of determining the similarity between images is an open problem in computer vision and is crucial for evaluating the realism of machine-generated images. Though there are a number of straightforward methods of estimating image similarity (e.g., low-level metrics that measure pixel differences, such as &lt;a href="https://ieeexplore.ieee.org/document/5705575"&gt;FSIM&lt;/a&gt; and &lt;a href="https://ieeexplore.ieee.org/document/1284395"&gt;SSIM&lt;/a&gt;), in many cases, the measured similarity differences do not match the differences perceived by a person. However, &lt;a href="https://arxiv.org/abs/1801.03924"&gt;more recent work&lt;/a&gt; has demonstrated that intermediate representations of neural network classifiers, such as &lt;a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"&gt;AlexNet&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1409.1556"&gt;VGG&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1602.07360"&gt;SqueezeNet&lt;/a&gt; trained on &lt;a href="https://image-net.org/index.php"&gt;ImageNet&lt;/a&gt;, exhibit perceptual similarity as an emergent property. That is, &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance"&gt;Euclidean distances&lt;/a&gt; between encoded representations of images by ImageNet-trained models correlate much better with a person’s judgment of differences between images than estimating perceptual similarity directly from image pixels. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s1379/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="471" data-original-width="1379" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Two sets of sample images from the &lt;a href="https://github.com/richzhang/PerceptualSimilarity"&gt;BAPPS dataset&lt;/a&gt;. Trained networks agree more with human judgements as compared to low-level metrics (&lt;a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"&gt;PSNR&lt;/a&gt;, SSIM, FSIM). Image source: &lt;a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html"&gt;Zhang et al. (2018)&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href="https://openreview.net/forum?id=qrGKGZZvH0"&gt;Do better ImageNet classifiers assess perceptual similarity better?&lt;/a&gt;” published in &lt;em&gt;&lt;a href="https://www.jmlr.org/tmlr/"&gt;Transactions on Machine Learning Research&lt;/a&gt;&lt;/em&gt;, we contribute an extensive experimental study on the relationship between the accuracy of ImageNet classifiers and their emergent ability to capture perceptual similarity. To evaluate this emergent ability, we follow &lt;a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html"&gt;previous work&lt;/a&gt; in measuring the perceptual scores (PS), which is roughly the correlation between human preferences to that of a model for image similarity on the &lt;a href="https://github.com/richzhang/PerceptualSimilarity"&gt;BAPPS dataset&lt;/a&gt;. While prior work studied the first generation of ImageNet classifiers, such as AlexNet, SqueezeNet and VGG, we significantly increase the scope of the analysis incorporating modern classifiers, such as &lt;a href="https://en.wikipedia.org/wiki/Residual_neural_network"&gt;ResNets&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"&gt;Vision Transformers&lt;/a&gt; (ViTs), across a wide range of hyper-parameters. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Relationship Between Accuracy and Perceptual Similarity&lt;/h2&gt;&lt;p&gt;It is well established that features learned via training on ImageNet transfer well to a number of &lt;a href="https://arxiv.org/abs/1910.04867"&gt;downstream&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1705.07750"&gt;tasks&lt;/a&gt;, making ImageNet pre-training a standard recipe. Further, better accuracy on ImageNet usually implies better performance on a diverse set of downstream tasks, such as &lt;a href="https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html"&gt;robustness to common corruptions&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1902.10811"&gt;out-of-distribution&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2107.04649"&gt;generalization&lt;/a&gt; and transfer learning on &lt;a href="https://arxiv.org/abs/1805.08974"&gt;smaller classification datasets&lt;/a&gt;. Contrary to prevailing evidence that suggests models with high validation accuracies on ImageNet are likely to transfer better to other tasks, surprisingly, we find that representations from underfit ImageNet models with modest validation accuracies achieve the best perceptual scores. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/s800/image6.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="506" data-original-width="800" height="253" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/w400-h253/image6.gif" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Plot of perceptual scores (PS) on the 64 × 64 &lt;a href="https://github.com/richzhang/PerceptualSimilarity"&gt;BAPPS dataset&lt;/a&gt; (y-axis) against the &lt;a href="https://image-net.org/index.php"&gt;ImageNet&lt;/a&gt; 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/s530/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="344" data-original-width="530" height="260" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/w400-h260/image6.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Plot of perceptual scores (PS) on the 64 × 64 &lt;a href="https://github.com/richzhang/PerceptualSimilarity"&gt;BAPPS Dataset&lt;/a&gt; (y-axis) against the &lt;a href="https://image-net.org/index.php"&gt;ImageNet&lt;/a&gt; 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;  &lt;p&gt;We study the variation of perceptual scores as a function of neural network hyperparameters: width, depth, number of training steps, &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2"&gt;weight decay&lt;/a&gt;,&lt;a href="https://ieeexplore.ieee.org/document/7780677"&gt; label smoothing&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Dilution_(neural_networks)"&gt;dropout&lt;/a&gt;. For each hyperparameter, there exists an optimal accuracy up to which improving accuracy improves PS. This optimum is fairly low and is attained quite early in the hyperparameter sweep. Beyond this point, improved classifier accuracy corresponds to worse PS. &lt;/p&gt;&lt;p&gt;As illustration, we present the variation of PS with respect to two hyperparameters: training steps in ResNets and width in ViTs. The PS of ResNet-50 and ResNet-200 peak very early at the first few epochs of training. After the peak, PS of better classifiers decrease more drastically. ResNets are trained with a learning rate schedule that causes a stepwise increase in accuracy as a function of training steps. Interestingly, after the peak, they also exhibit a step-wise decrease in PS that matches this step-wise accuracy increase.&lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/s423/image8.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="276" data-original-width="423" height="261" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/w400-h261/image8.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/s423/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="276" data-original-width="423" height="261" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/w400-h261/image3.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Early-stopped ResNets attain the best PS across different depths of 6, 50 and 200.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;ViTs consist of a stack of transformer blocks applied to the input image. The width of a ViT model is the number of output neurons of a single transformer block. Increasing its width is an effective way to improve its accuracy. Here, we vary the width of two ViT variants, B/8 and L/4 (i.e., Base and Large ViT models with patch sizes 4 and 8 respectively), and evaluate both the accuracy and PS. Similar to our observations with early-stopped ResNets, narrower ViTs with lower accuracies perform better than the default widths. Surprisingly, the optimal width of ViT-B/8 and ViT-L/4 are 6 and 12% of their default widths.&amp;nbsp;For a more comprehensive list of experiments involving other hyperparameters such as width, depth, number of training steps, weight decay, label smoothing and dropout across both ResNets and ViTs, check out &lt;a href="https://openreview.net/pdf?id=qrGKGZZvH0"&gt;our paper&lt;/a&gt;.&lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/s423/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="276" data-original-width="423" height="261" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/w400-h261/image7.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/s423/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="276" data-original-width="423" height="261" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/w400-h261/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Narrow ViTs attain the best PS.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Scaling Down Models Improves Perceptual Scores&lt;/h2&gt;&lt;p&gt;Our results prescribe a simple strategy to improve an architecture’s PS: scale down the model to reduce its accuracy until it attains the optimal perceptual score. The table below summarizes the improvements in PS obtained by scaling down each model across every hyperparameter. Except for ViT-L/4, early stopping yields the highest improvement in PS, regardless of architecture. In addition, early stopping is the most efficient strategy as there is no need for an expensive grid search. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 11%; margin-right: 11%;"&gt;      &lt;colgroup&gt;     &lt;col style="width: 15%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;     &lt;col style="width: 9%;"&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;b&gt;&lt;em&gt;Model&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Default&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Width&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Depth&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Weight&lt;br /&gt;Decay&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Central&lt;br /&gt;Crop&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Train&lt;br /&gt;Steps&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;Best&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;ResNet-6&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;69.1    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+0.3    &lt;/td&gt;   &lt;td&gt;0.0    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+0.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.6    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;ResNet-50&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;68.2    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+0.7    &lt;/td&gt;   &lt;td&gt;+0.7    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.7    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;ResNet-200&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.6    &lt;/td&gt;   &lt;td&gt;+0.2    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;+1.3    &lt;/td&gt;   &lt;td&gt;+1.2    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.9&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;69.5    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;ViT B/8&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.6    &lt;/td&gt;   &lt;td&gt;+1.1    &lt;/td&gt;   &lt;td&gt;+1.0    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+1.3&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;+0.9    &lt;/td&gt;   &lt;td&gt;+1.1    &lt;/td&gt;   &lt;td&gt;68.9    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;ViT L/4&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;67.9    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;+0.4    &lt;/td&gt;   &lt;td&gt;-0.1    &lt;/td&gt;   &lt;td&gt;-1.1    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;+0.5&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;68.4    &lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 5%; margin-right: 5%;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Perceptual Score improves by scaling down ImageNet models. Each value denotes the improvement obtained by scaling down a model across a given hyperparameter over the model with default hyperparameters.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Global Perceptual Functions&lt;/h2&gt;&lt;p&gt;In &lt;a href="https://arxiv.org/abs/1801.03924"&gt;prior work&lt;/a&gt;, the perceptual similarity function was computed using Euclidean distances across the spatial dimensions of the image. This assumes a direct correspondence between pixels, which may not hold for warped, translated or rotated images. Instead, we adopt two perceptual functions that rely on global representations of images, namely the style-loss function from the &lt;a href="https://arxiv.org/abs/1508.06576"&gt;Neural Style Transfer&lt;/a&gt; work that captures stylistic similarity between two images, and a normalized &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers"&gt;mean pool&lt;/a&gt; distance function. The style-loss function compares the inter-channel cross-correlation matrix between two images while the mean pool function compares the spatially averaged global representations. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/s423/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="277" data-original-width="423" height="263" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/w400-h263/image2.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/s423/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="277" data-original-width="423" height="263" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/w400-h263/image4.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Global perceptual functions consistently improve PS across both networks trained with default hyperparameters (&lt;b&gt;top&lt;/b&gt;) and ResNet-200 as a function of train epochs (&lt;b&gt;bottom&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We probe a number of hypotheses to explain the relationship between accuracy and PS and come away with a few additional insights. For example, the accuracy of models without commonly used skip-connections also inversely correlate with PS, and layers close to the input on average have lower PS as compared to layers close to the output. For further exploration involving distortion sensitivity, ImageNet class granularity, and spatial frequency sensitivity, check out &lt;a href="https://openreview.net/pdf?id=qrGKGZZvH0"&gt;our paper&lt;/a&gt;. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this paper, we explore the question of whether improving classification accuracy yields better perceptual metrics. We study the relationship between accuracy and PS on ResNets and ViTs across many different hyperparameters and observe that PS exhibits an inverse-U relationship with accuracy, where accuracy correlates with PS up to a certain point, and then exhibits an inverse-correlation. Finally, in our paper, we discuss in detail a number of explanations for the  observed relationship between accuracy and PS, involving skip connections, global similarity functions, distortion sensitivity, layerwise perceptual scores, spatial frequency sensitivity and ImageNet class granularity. While the exact explanation for the observed tradeoff between ImageNet accuracy and perceptual similarity is a mystery, we are excited that our paper opens the door for further research in this area. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;This is joint work with Neil Houlsby and Nal Kalchbrenner. We would additionally like to thank Basil Mustafa, Kevin Swersky, Simon Kornblith, Johannes Balle, Mike Mozer, Mohammad Norouzi and Jascha Sohl-Dickstein for useful discussions.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7866944510509103804/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/do-modern-imagenet-classifiers.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7866944510509103804" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7866944510509103804" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/do-modern-imagenet-classifiers.html" rel="alternate" title="Do Modern ImageNet Classifiers Accurately Predict Perceptual Similarity?" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s72-c/image5.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7982911050269775769</id><published>2022-10-18T10:50:00.024-07:00</published><updated>2022-10-21T10:07:54.413-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">Table Tennis: A Research Platform for Agile Robotics</title><content type="html">&lt;span class="byline-author"&gt;Posted by Avi Singh, Research Scientist, and Laura Graesser, Research Engineer, Robotics at Google&lt;/span&gt;&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh563V_fA2AfwFfnKgijiPz38oX40zeziDIAGd6chEHZiNJRLpo1N3IXjqYRhF9acNGgkwrrBvbDMKIeTxov5QJHtTx3FO36jdOVlvMpRWdjrASBWSrGLNWP8gIPoS_qEk4z5fdJiNhLKwYWiMQDsjiFwvE5iABgFNyoREW37VzHjJ102qWxpNP7aWBvg/s775/i-S2R.gif" style="display: none;" /&gt;&lt;p&gt;Robot learning has been applied to a wide range of challenging real world tasks, including &lt;a href="https://arxiv.org/abs/1910.07113"&gt;dexterous manipulation&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2004.00784"&gt;legged locomotion&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/1806.10293"&gt;grasping&lt;/a&gt;. It is less common to see robot learning applied to dynamic, high-acceleration tasks requiring tight-loop human-robot interactions, such as table tennis. There are two complementary properties of the table tennis task that make it interesting for robotic learning research. First, the task requires both speed and precision, which puts significant demands on a learning algorithm. At the same time, the problem is highly-structured (with a fixed, predictable environment) and naturally multi-agent (the robot can play with humans or another robot), making it a desirable testbed to investigate questions about human-robot interaction and &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;reinforcement learning&lt;/a&gt;. These properties have led to several research groups developing table tennis research platforms [&lt;a href="https://www.ias.informatik.tu-darmstadt.de/Research/LearningToPlayPing-pong"&gt;1&lt;/a&gt;, &lt;a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/kognitive-systeme/projects/table-tennis-robot/"&gt;2&lt;/a&gt;, &lt;a href="https://www.omron.com/global/en/technology/omrontechnics/vol51/016.html"&gt;3&lt;/a&gt;, &lt;a href="https://core-robotics.gatech.edu/"&gt;4&lt;/a&gt;].  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;The &lt;a href="https://research.google/teams/robotics/"&gt;Robotics&lt;/a&gt; team at Google has built such a platform to study problems that arise from robotic learning in a multi-player, dynamic and interactive setting. In the rest of this post we introduce two projects, &lt;a href="https://sites.google.com/view/is2r"&gt;Iterative-Sim2Real&lt;/a&gt; (to be presented at &lt;a href="https://corl2022.org/"&gt;CoRL 2022&lt;/a&gt;) and &lt;a href="https://sites.google.com/view/goals-eye"&gt;GoalsEye&lt;/a&gt; (&lt;a href="https://iros2022.org/"&gt;IROS 2022&lt;/a&gt;), which illustrate the problems we have been investigating so far. Iterative-Sim2Real enables a robot to hold rallies of over 300 hits with a human player, while GoalsEye enables learning goal-conditioned policies that match the precision of amateur humans. &lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;video autoplay="" loop="" muted="" playsinline="" width="80%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/i-S2R_highlights.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt; &lt;video autoplay="" loop="" muted="" playsinline="" width="80%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/goalseye-highlights-1x-4x-1x_compressed.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;&lt;/div&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Iterative-Sim2Real policies playing cooperatively with humans (&lt;b&gt;top&lt;/b&gt;) and a GoalsEye policy returning balls to different locations (&lt;b&gt;bottom&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Iterative-Sim2Real: Leveraging a Simulator to Play Cooperatively with Humans&lt;/h2&gt;&lt;p&gt;In this project, the goal for the robot is &lt;em&gt;cooperative&lt;/em&gt; in nature: to carry out a rally with a human for as long as possible. Since it would be tedious and time-consuming to train directly against a human player in the real world, we adopt a simulation-based (i.e., &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;sim-to-real&lt;/a&gt;) approach. However, because it is difficult to simulate human behavior accurately, applying sim-to-real learning to tasks that require tight, close-loop interaction with a human participant is difficult. &lt;/p&gt;&lt;p&gt;In &lt;a href="https://sites.google.com/view/is2r"&gt;Iterative-Sim2Real&lt;/a&gt;, (i.e., i-S2R), we present a method for learning human behavior models for human-robot interaction tasks, and instantiate it on our robotic table tennis platform. We have built a system that can achieve rallies of up to 340 hits with an amateur human player (shown below). &lt;/p&gt;   &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" frameborder="0" height="360" src="https://www.youtube.com/embed/Fh7VK0WPvU4?rel=0&amp;amp;" width="640" youtube-src-id="Fh7VK0WPvU4"&gt;&lt;/iframe&gt;&lt;/div&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A 340-hit rally lasting over 4 minutes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Learning Human Behavior Models: a Chicken and Egg Problem&lt;/h2&gt;&lt;p&gt;The central problem in learning accurate human behavior models for robotics is the following: if we do not have a good-enough robot policy to begin with, then we cannot collect high-quality data on how a person might interact with the robot. But without a human behavior model, we cannot obtain robot policies in the first place. An alternative would be to train a robot policy directly in the real world, but this is often slow, cost-prohibitive, and poses safety-related challenges, which are further exacerbated when people are involved. i-S2R, visualized below, is a solution to this chicken and egg problem. It uses a simple model of human behavior as an approximate starting point and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s1360/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="785" data-original-width="1360" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s16000/image4.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;i-S2R Methodology.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;h2&gt;Results&lt;/h2&gt;&lt;p&gt;To evaluate i-S2R, we repeated the training process five times with five different human opponents and compared it with a baseline approach of ordinary sim-to-real plus fine-tuning (&lt;a href="https://arxiv.org/abs/2110.05457"&gt;S2R+FT&lt;/a&gt;). When aggregated across all players, the i-S2R rally length is higher than S2R+FT by about 9% (below on the left). The histogram of rally lengths for i-S2R and S2R+FT (below on the right) shows that a large fraction of the rallies for S2R+FT are shorter (i.e., less than 5), while i-S2R achieves longer rallies more frequently.  &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="935" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Summary of i-S2R results. &lt;/b&gt;Boxplot details: The white circle is the mean, the horizontal line is the median, box bounds are the 25th and 75th percentiles.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;       &lt;p&gt;We also break down the results based on player type: beginner (40% players), intermediate (40% of players) and advanced (20% players). We see that i-S2R significantly outperforms S2R+FT for both beginner and intermediate players (80% of players). &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="645" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;i-S2R Results by player type.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;More details on i-S2R can be found on our &lt;a href="https://arxiv.org/abs/2207.06572"&gt;preprint&lt;/a&gt;, &lt;a href="https://sites.google.com/view/is2r"&gt;website&lt;/a&gt;, and also in the following summary video. &lt;/p&gt;  &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" frameborder="0" height="360" src="https://www.youtube.com/embed/vtVUFXV9qR4?rel=0&amp;amp;" width="640" youtube-src-id="vtVUFXV9qR4"&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;div style="line-height:120%;"&gt;  &lt;br&gt;  &lt;br&gt;&lt;/div&gt; &lt;h2&gt;GoalsEye: Learning to Return Balls Precisely on a Physical Robot&lt;/h2&gt;&lt;p&gt;While we focused on sim-to-real learning in i-S2R, it is sometimes desirable to learn using only real-world data — closing the sim-to-real gap in this case is unnecessary. &lt;a href="https://en.wikipedia.org/wiki/Imitative_learning"&gt;Imitation learning&lt;/a&gt; (IL) provides a simple and stable approach to learning in the real world, but it requires access to demonstrations and cannot exceed the performance of the teacher. Collecting expert human demonstrations of precise goal-targeting in high speed settings is challenging and sometimes impossible (due to limited precision in human movements). While reinforcement learning (RL) is well-suited to such high-speed, high-precision tasks, it faces a difficult exploration problem (especially at the start), and can be very sample inefficient. In &lt;a href="https://sites.google.com/view/goals-eye"&gt;GoalsEye&lt;/a&gt;, we demonstrate an approach that combines recent behavior cloning techniques [&lt;a href="https://arxiv.org/abs/1903.01973"&gt;5&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1912.06088"&gt;6&lt;/a&gt;] to learn a precise goal-targeting policy, starting from a small, weakly-structured, non-targeting dataset. &lt;/p&gt;&lt;p&gt;Here we consider a different table tennis task with an emphasis on precision. We want the robot to return the ball to an arbitrary goal location on the table, e.g. “hit the back left corner" or ''land the ball just over the net on the right side" (see left video below). Further, we wanted to find a method that can be applied &lt;em&gt;directly&lt;/em&gt; on our real world table tennis environment with no simulation involved. We found that the synthesis of two existing imitation learning techniques, &lt;a href="https://arxiv.org/abs/1903.01973"&gt;Learning from Play&lt;/a&gt; (LFP) and &lt;a href="https://arxiv.org/abs/1912.06088"&gt;Goal-Conditioned Supervised Learning&lt;/a&gt; (GCSL), scales to this setting. It is safe and &lt;a href="https://en.wikipedia.org/wiki/Sample_complexity"&gt;sample efficient&lt;/a&gt; enough to train a policy on a physical robot which is as accurate as amateur humans at the task of returning balls to specific goals on the table. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;     &lt;td style="text-align: center;"&gt;    &lt;video autoplay="" loop="" muted="" playsinline="" width="100%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-robot-5-goals-compressed.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt; &lt;td style="text-align: center;"&gt; &amp;nbsp;   &lt;/td&gt; &lt;td style="text-align: center;"&gt;    &lt;video autoplay="" loop="" muted="" playsinline="" width="100%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-human-5-goals-compressed-unblurred.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;GoalsEye policy aiming at a 20cm diameter goal (&lt;b&gt;left&lt;/b&gt;). Human player aiming at the same goal (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;The essential ingredients of success are:  &lt;/p&gt;&lt;ol&gt; &lt;li&gt;&lt;em&gt;A minimal, but non-goal-directed “bootstrap” dataset&lt;strong&gt; &lt;/strong&gt;&lt;/em&gt;of the robot hitting the ball to overcome an initial difficult exploration problem.   &lt;/li&gt;&lt;li&gt;&lt;em&gt;Hindsight relabeled goal conditioned behavioral cloning&lt;/em&gt; (GCBC) to train a goal-directed policy to reach any goal in the dataset.   &lt;/li&gt;&lt;li&gt;&lt;em&gt;Iterative self-supervised goal reaching.&lt;/em&gt; The agent improves continuously by setting random goals and attempting to reach them using the current policy. All attempts are relabeled and added into a continuously expanding training set. This &lt;em&gt;self-practice&lt;/em&gt;, in which the robot expands the training data by setting and attempting to reach goals, is repeated iteratively. &lt;/li&gt;&lt;/ol&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s1600/image5.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="468" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s16000/image5.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;GoalsEye methodology.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;h2&gt;Demonstrations and Self-Improvement Through Practice Are Key&lt;/h2&gt;&lt;p&gt;The synthesis of techniques is crucial. The policy’s objective is to return a &lt;em&gt;variety &lt;/em&gt;of incoming balls to &lt;em&gt;any &lt;/em&gt;location on the opponent’s side of the table. A policy trained on the initial 2,480 demonstrations only accurately reaches within 30 cm of the goal 9% of the time. However, after a policy has self-practiced for ~13,500 attempts, goal-reaching accuracy rises to 43% (below on the right). This improvement is clearly visible as shown in the videos below. Yet if a policy only self-practices, training fails completely in this setting. Interestingly, the number of demonstrations improves the efficiency of subsequent self-practice, albeit with diminishing returns. This indicates that demonstration data and self-practice could be substituted depending on the relative time and cost to gather demonstration data compared with self-practice. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s1441/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="541" data-original-width="1441" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Self-practice substantially improves accuracy. &lt;b&gt;Left&lt;/b&gt;: simulated training. &lt;b&gt;Right&lt;/b&gt;: real robot training. The demonstration datasets contain ~2,500 episodes, both in simulation and the real world.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;     &lt;td style="text-align: center;"&gt;    &lt;video autoplay="" loop="" muted="" playsinline="" width="100%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt_5_goal_E_no_audio_4x_compressed.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt; &lt;td style="text-align: center;"&gt; &amp;nbsp;   &lt;/td&gt; &lt;td style="text-align: center;"&gt;    &lt;video autoplay="" loop="" muted="" playsinline="" width="100%"&gt; &lt;source src="https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt-300-goal-E-no-audio-4x_compressed.mp4?raw=true" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;  &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Visualizing the benefits of self-practice. &lt;b&gt;Left&lt;/b&gt;: policy trained on initial 2,480 demonstrations. &lt;b&gt;Right&lt;/b&gt;: policy after an additional 13,500 self-practice attempts.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;More details on GoalsEye can be found in the &lt;a href="https://arxiv.org/abs/2210.03662"&gt;preprint&lt;/a&gt; and on our &lt;a href="https://sites.google.com/view/goals-eye"&gt;website&lt;/a&gt;. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Conclusion and Future Work&lt;/h2&gt;&lt;p&gt;We have presented two complementary projects using our robotic table tennis research platform. i-S2R learns RL policies that are able to interact with humans, while GoalsEye demonstrates that learning from real-world unstructured data combined with self-supervised practice is effective for learning goal-conditioned policies in a precise, dynamic setting. &lt;/p&gt;&lt;p&gt;One interesting research direction to pursue on the table tennis platform would be to build a robot “coach” that could adapt its play style according to the skill level of the human participant to keep things challenging and exciting.  &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We thank our co-authors, Saminda Abeyruwan, Alex Bewley, Krzysztof Choromanski, David B. D’Ambrosio, Tianli Ding, Deepali Jain, Corey Lynch, Pannag R. Sanketi, Pierre Sermanet and Anish Shankar. We are also grateful for the support of many members of the Robotics Team who are listed in the acknowledgement sections of the papers.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7982911050269775769/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7982911050269775769" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7982911050269775769" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/table-tennis-research-platform-for.html" rel="alternate" title="Table Tennis: A Research Platform for Agile Robotics" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh563V_fA2AfwFfnKgijiPz38oX40zeziDIAGd6chEHZiNJRLpo1N3IXjqYRhF9acNGgkwrrBvbDMKIeTxov5QJHtTx3FO36jdOVlvMpRWdjrASBWSrGLNWP8gIPoS_qEk4z5fdJiNhLKwYWiMQDsjiFwvE5iABgFNyoREW37VzHjJ102qWxpNP7aWBvg/s72-c/i-S2R.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1370943694206193273</id><published>2022-10-14T11:47:00.009-07:00</published><updated>2022-10-21T10:24:04.673-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="NLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Publications"/><category scheme="http://www.blogger.com/atom/ns#" term="Research"/><title type="text">UL2 20B: An Open Source Unified Language Learner</title><content type="html">&lt;span class="byline-author"&gt;Posted by Yi Tay and Mostafa Dehghani, Research Scientists, Google Research, Brain Team&lt;/span&gt;&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif" style="display: none;" /&gt;&lt;p&gt;Building models that understand and generate natural language well is one the grand goals of machine learning (ML) research and has a direct impact on building smart systems for everyday applications. Improving the quality of language models is a key target for researchers to make progress toward such a goal.   &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Most common paradigms to build and train language models use either autoregressive decoder-only architectures (e.g., &lt;a href="https://arxiv.org/abs/2204.02311"&gt;PaLM&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/2005.14165"&gt;GPT-3&lt;/a&gt;), where the model is trained to predict the next word for a given prefix phrase, or span corruption-based encoder-decoder architectures (e.g., &lt;a href="https://arxiv.org/abs/1910.10683"&gt;T5&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2202.08906"&gt;ST-MoE&lt;/a&gt;), where the training objective is to recover the subset of words masked out of the input. On the one hand, T5-like models perform well on &lt;a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"&gt;supervised fine-tuning&lt;/a&gt; tasks, but struggle with few-shot in-context learning. On the other hand, autoregressive language models are great for &lt;a href="https://arxiv.org/abs/2005.14165"&gt;open-ended generation&lt;/a&gt; (e.g., dialog generation with&lt;a href="https://arxiv.org/abs/2201.08239"&gt; LaMDA&lt;/a&gt;) and prompt-based learning (e.g., in-context learning with PaLM), but may perform suboptimally on fine-tuning tasks. Thus, there remains an opportunity to create an effective unified framework for pre-training models. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2205.05131"&gt;Unifying Language Learning Paradigms&lt;/a&gt;”, we present a novel language pre-training paradigm called Unified Language Learner (UL2) that improves the performance of language models universally across datasets and setups. UL2 frames different objective functions for training language models as &lt;a href="https://arxiv.org/abs/1910.10683"&gt;denoising&lt;/a&gt; tasks, where the model has to recover missing sub-sequences of a given input. During pre-training it uses a novel &lt;em&gt;mixture-of-denoisers&lt;/em&gt; that samples from a varied set of such objectives, each with different configurations. We demonstrate that models trained using the UL2 framework perform well in a variety of language domains, including prompt-based few-shot learning and models fine-tuned for down-stream tasks. Additionally, we show that UL2 excels in &lt;a href="https://en.wikipedia.org/wiki/Natural_language_generation"&gt;generation&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Natural-language_understanding"&gt;language understanding&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Information_retrieval"&gt;retrieval&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)"&gt;long-text understanding&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Question_answering"&gt;question answering&lt;/a&gt; tasks. Finally, we are excited to publicly release the &lt;a href="https://github.com/google-research/google-research/tree/master/ul2"&gt;checkpoints&lt;/a&gt; for our best performing UL2 20 billion parameter model. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Background: Language Modeling Objectives and Architectures&lt;/h2&gt;&lt;p&gt;Common objective functions for training language models can mostly be framed as learning data transformations that map inputs to targets. The model is conditioned on different forms of input to predict target tokens. To this end, different objectives utilize different properties of the inputs. &lt;/p&gt;&lt;p&gt;The standard &lt;a href="https://arxiv.org/abs/2005.14165"&gt;Causal Language&lt;/a&gt; modeling objective (CausalLM) is trained to predict full sequence lengths and so, only recognizes tokens in the target output. The &lt;a href="https://arxiv.org/abs/1910.10683"&gt;prefix language modeling&lt;/a&gt; objective (PrefixLM) modifies this process by randomly sampling a contiguous span of &lt;em&gt;k&lt;/em&gt; tokens from the given tokenized text to form the input of the model, referred to as the “prefix”. The &lt;a href="https://arxiv.org/abs/1910.10683"&gt;span corruption&lt;/a&gt; objective masks contiguous spans from the inputs and trains the model to predict these masked spans.  &lt;/p&gt;&lt;p&gt;In the table below, we list the common objectives on which state-of-the-art language models are trained along with different characteristics of the input, i.e., how it is presented to the model. Moreover, we characterize the example efficiency of each objective in terms of the ability of the model for exploiting supervision signals from a single input, e.g., how much of the input tokens contribute to the calculation of the loss. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 5%; margin-right: 5%;"&gt;      &lt;colgroup&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;  &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style="text-align: left;"&gt;&lt;em&gt;&lt;b&gt;Objective&lt;br /&gt;Function&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Inputs&lt;br /&gt;(Bi-directional)&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Targets&lt;br /&gt;(Causal)&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;Input&lt;br /&gt;Properties&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;td&gt;&lt;em&gt;&lt;b&gt;Example&lt;br /&gt;Efficiency&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;CausalLM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;none    &lt;/td&gt;   &lt;td&gt;text    &lt;/td&gt;   &lt;td&gt;N/A    &lt;/td&gt;   &lt;td&gt;full seq_len    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;PrefixLM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;text&lt;br&gt;(up to position &lt;em&gt;k&lt;/em&gt;)    &lt;/td&gt;   &lt;td&gt;text&lt;br&gt;(after position &lt;em&gt;k&lt;/em&gt;)    &lt;/td&gt;   &lt;td&gt;contiguous    &lt;/td&gt;   &lt;td&gt;seq_len - &lt;em&gt;k&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;Span corruption&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;masked text    &lt;/td&gt;   &lt;td&gt;masked_tokens    &lt;/td&gt;   &lt;td&gt;&lt;span style="font-size: small;"&gt;non-contiguous, may be bi-directional&lt;/span&gt;&lt;/td&gt;   &lt;td&gt;&lt;span style="font-size: small;"&gt;typically lower than others&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Common objectives used in today’s language models. Throughout, “text” indicates tokenized text.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;UL2 leverages the strengths of each of these objective functions through a framework that generalizes over each of them, which enables the ability to reason and unify common pre-training objectives. Based on this framework, the main task for training a language model is to learn the transformation of a sequence of input tokens to a sequence of target tokens. Then all the objective functions introduced above can be simply reduced to different ways of generating input and target tokens. For instance, the PrefixLM objective can be viewed as a transformation that moves a segment of &lt;em&gt;k&lt;/em&gt; contiguous tokens from the inputs to the targets. Meanwhile, the span corruption objective is a data transformation that corrupts spans (a subsequence of tokens in the input), replacing them with mask tokens that are shifted to the targets. &lt;/p&gt;&lt;p&gt;It is worth noting that one can decouple the model architecture and the objective function with which it’s trained. Thus, it is possible to train different architectures, such as the common single stack decoder-only and two-stack encoder-decoder models, with any of these objectives. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Mixture of Denoisers&lt;/h2&gt;&lt;p&gt;The UL2 framework can be used to train a model on a mixture of pre-training objectives and supply it with capabilities and inductive bias benefits from different pre-training tasks. Training on the mixture helps the model leverage the strengths of different tasks and mitigates the weaknesses of others. For instance, the mixture-of-denoisers objective can strongly improve the prompt-based learning capability of the model as opposed to a span corruption-only T5 model.  &lt;/p&gt;&lt;p&gt;UL2 is trained using a mixture of three denoising tasks: (1) &lt;em&gt;R-denoising&lt;/em&gt; (or regular span corruption), which emulates the standard T5 span corruption objective; (2) &lt;em&gt;X-denoising&lt;/em&gt; (or extreme span corruption); and (3) &lt;em&gt;S-denoising&lt;/em&gt; (or sequential PrefixLM). During pre-training, we sample from the available denoising tasks based on user-specified ratios (i.e., different combinations of the R, X, and S-denoisers) and prepare the input and target appropriately. Then, a paradigm token is appended to the input (one of &lt;code&gt;[R]&lt;/code&gt;, &lt;code&gt;[X]&lt;/code&gt;, or &lt;code&gt;[S]&lt;/code&gt;) indicating the denoising task at hand. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="500" data-original-width="1300" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An overview of the denoising objectives used in UL2’s mixture-of-denoisers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Improving Trade-Offs Across Learning Paradigms&lt;/h2&gt;&lt;p&gt;Many existing commonly used language learning paradigms typically excel at one type of task or application, such as fine-tuning performance or prompt-based in-context learning. In the plot below, we show baseline objective functions on different tasks compared to UL2: CausalLM (&lt;em&gt;referred to as GPT-like&lt;/em&gt;), PrefixLM, Span Corrupt (&lt;em&gt;also referred to as T5 in the plot&lt;/em&gt;), and a baseline objective function proposed by &lt;a href="https://arxiv.org/abs/1905.03197"&gt;UniLM&lt;/a&gt;. We use these objectives for training decoder only architectures (green) and encoder-decoder architectures (blue) and evaluate different combinations of objective functions and architectures on two main sets of tasks:  &lt;/p&gt;&lt;ol&gt;&lt;li&gt;Fine-tuning, by measuring performance on &lt;a href="https://super.gluebenchmark.com/"&gt;SuperGLUE&lt;/a&gt; (y-axis of the plot below) &lt;/li&gt;  &lt;li&gt;In-context learning, by measuring performance of the model on a suite of 1-shot &lt;a href="https://gem-benchmark.com/"&gt;GEM tasks&lt;/a&gt; (e.g., &lt;a href="https://arxiv.org/abs/1808.08745"&gt;XSUM&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1909.05855"&gt;SGD or Schema guided dialog&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2004.14373"&gt;TOTTO&lt;/a&gt;) (x-axis of the plot below).  &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For most of the existing language learning paradigms, there is a trade-off between the quality of the model on these two sets of tasks. We show that UL2 bridges this trade-off across in-context learning and fine-tuning.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/s1004/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="868" data-original-width="1004" height="346" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/w400-h346/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;In both decoder-only and encoder-decoder setups, UL2 strikes a significantly improved balance in performance between fine-tuned discriminative tasks and prompt-based 1-shot open-ended text generation compared to previous methods. (All models are comparable in terms of computational costs, i.e., FLOPs (EncDec models are 300M and Dec models are 150M parameters).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;UL2 for Few-Shot Prompting and Chain-of-Thought Reasoning&lt;/h2&gt;&lt;p&gt;We scale up UL2 and train a 20 billion parameter encoder-decoder model on the public &lt;a href="https://www.tensorflow.org/datasets/catalog/c4"&gt;C4 corpus&lt;/a&gt; and demonstrate some impressive capabilities of the UL2 20B model.  &lt;/p&gt;&lt;p&gt;UL2 is a powerful in-context learner that excels at both few-shot and &lt;a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"&gt;chain-of-thought&lt;/a&gt; (CoT) prompting. In the table below, we compare UL2 with other state-of-the-art models (e.g, &lt;a href="https://arxiv.org/abs/1910.10683"&gt;T5 XXL&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2204.02311"&gt;PaLM&lt;/a&gt;) for few-shot prompting on the XSUM summarization dataset. Our results show that UL2 20B outperforms PaLM and T5, both of which are in the same ballpark of compute cost. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 17%; margin-right: 17%;"&gt;  &lt;colgroup&gt;     &lt;col style="width: 18%;"&gt;&lt;/col&gt;     &lt;col style="width: 16%;"&gt;&lt;/col&gt;     &lt;col style="width: 16%;"&gt;&lt;/col&gt;     &lt;col style="width: 16%;"&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;&lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-1&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-2&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;&lt;b&gt;ROUGE-L&lt;/b&gt;&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;LaMDA 137B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;5.4    &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;PaLM 62B&lt;/em&gt;     &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;11.2    &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;PaLM 540B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;12.2&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;–    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;PaLM 8B&lt;/em&gt;     &lt;/td&gt;   &lt;td&gt;–     &lt;/td&gt;   &lt;td&gt;4.5    &lt;/td&gt;   &lt;td&gt;–     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;T5 XXL 11B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.6    &lt;/td&gt;   &lt;td&gt;0.1    &lt;/td&gt;   &lt;td&gt;0.6     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;T5 XXL 11B + LM&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;13.3    &lt;/td&gt;   &lt;td&gt; 2.3    &lt;/td&gt;   &lt;td&gt; 10.7     &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;&lt;em&gt;UL2 20B&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;25.5&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;8.6&lt;/strong&gt;   &lt;/td&gt;   &lt;td&gt;&lt;strong&gt;19.8&lt;/strong&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of UL2 with &lt;a href="https://arxiv.org/abs/1910.10683"&gt;T5 XXL&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2204.02311"&gt;PaLM&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2201.08239"&gt;LamDA 137B&lt;/a&gt; on 1-shot summarization (&lt;a href="https://arxiv.org/abs/1808.08745"&gt;XSUM&lt;/a&gt;) in terms of &lt;a href="https://arxiv.org/abs/1808.08745"&gt;ROUGE-1/2/L&lt;/a&gt; (higher is better), which captures the quality by comparing the generated summaries with the gold summaries as reference.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Most CoT prompting results have been obtained using much larger language models, such as GPT-3 175B, PaLM 540B, or LaMDA 137B. We show that reasoning via CoT prompting can be achieved with UL2 20B, which is both publicly available and several times smaller than prior models that leverage chain-of-thought prompting. This enables an open avenue for researchers to conduct research on CoT prompting and reasoning at an accessible scale. In the table below, we show that for UL2, CoT prompting outperforms standard prompting on math word problems with a range of difficulties (&lt;a href="http://go/arxiv/2110.14168"&gt;GSM8K&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2103.07191"&gt;SVAMP&lt;/a&gt;, &lt;a href="https://aclanthology.org/2020.acl-main.92/"&gt;ASDiv&lt;/a&gt;, &lt;a href="https://aclanthology.org/P17-1015"&gt;AQuA&lt;/a&gt;, and &lt;a href="https://aclanthology.org/N16-1136/"&gt;MAWPS&lt;/a&gt;). We also show that &lt;a href="https://arxiv.org/abs/2203.11171"&gt;self-consistency&lt;/a&gt; further improves performance. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s1760/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="874" data-original-width="1760" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Chain-of-thought (CoT) prompting and &lt;a href="https://arxiv.org/abs/2203.11171"&gt;self-consistency&lt;/a&gt; (SC) results on five arithmetic reasoning benchmarks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Conclusion and Future Directions&lt;/h2&gt;&lt;p&gt;UL2 demonstrates superior performance on a plethora of fine-tuning and few-shot tasks. We &lt;a href="https://github.com/google-research/google-research/tree/master/ul2"&gt;publicly release&lt;/a&gt; checkpoints of our best performing UL2 model with 20 billion parameters, which we hope will inspire faster progress in developing better language models in the machine learning community as a whole.  &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt; &lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;It was an honor and privilege to work on this with Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby and Donald Metzler. We further acknowledge Alexey Gritsenko, Andrew M. Dai, Jacob Devlin, Jai Gupta, William Fedus, Orhan Firat, Sebastian Gerhmann, Nan Du, Dave Uthus, Siamak Shakeri, Slav Petrov and Quoc Le for support and discussions. We thank the Jax and T5X team for building such wonderful infrastructure that made this research possible. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1370943694206193273/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1370943694206193273" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1370943694206193273" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html" rel="alternate" title="UL2 20B: An Open Source Unified Language Learner" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4616939676419842373</id><published>2022-10-13T09:53:00.014-07:00</published><updated>2022-10-21T10:44:33.146-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="EMNLP"/><category scheme="http://www.blogger.com/atom/ns#" term="Multimodal Learning"/><title type="text">Crossmodal-3600 — Multilingual Reference Captions for Geographically Diverse Images</title><content type="html">&lt;span class="byline-author"&gt;Posted by Ashish Thapliyal, Software Engineer, and Jordi Pont-Tuset, Research Scientist, Google Research&lt;/span&gt;&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA3qznWJUsCY3Jh7Z5CPA5U3dJIBWD28I3OOEXklBBY4TseAh5iap2pmDdYi-s4PbBZZu7lgMut_pW9WpbEr9FLyRK0WBnzejaerkRjJfNAc4tKhOvbnxhTY70akSiIrEir7NUxXhgbFyA1DopoQdqY0feRtoiTRkuyh6Goiqs2m73Yz9bFeBvkhekSg/s600/xm3600_animation_long.gif" style="display: none;" /&gt;&lt;p&gt;&lt;a href="https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html"&gt;Image captioning&lt;/a&gt; is the machine learning task of automatically generating a fluent natural language description for a given image. This task is important for &lt;a href="https://blog.google/outreach-initiatives/accessibility/more-accessible-web-images-arrive-10-new-languages/"&gt;improving accessibility&lt;/a&gt; for visually impaired users and is a core task in multimodal research encompassing both vision and language modeling.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt; &lt;p&gt;However, datasets for image captioning are primarily available in English. Beyond that, there are only a few datasets covering a limited number of languages that represent just a small fraction of the world’s population. Further, these datasets feature images that severely under-represent the richness and diversity of cultures from across the globe. These aspects have hindered research on image captioning for a wide variety of languages, and directly hamper the deployment of accessibility solutions for a large potential audience around the world.  &lt;/p&gt;&lt;p&gt;Today we present and make publicly available the &lt;a href="https://google.github.io/crossmodal-3600/"&gt;Crossmodal 3600&lt;/a&gt; (XM3600) image captioning evaluation dataset as a robust benchmark for multilingual image captioning that enables researchers to reliably compare research contributions in this emerging field. XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images. We show that the captions are of high quality and the style is consistent across languages. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;  &lt;video align="center" autoplay="" loop="" muted="" playsinline="" syle="margin-left:20%" width="80%"&gt;&lt;source src="https://google.github.io/crossmodal-3600/web-data/xm3600_animation.mp4" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;&lt;/div&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The Crossmodal 3600 dataset includes reference captions in 36 languages for each of a geographically diverse set of 3600 images. All images used with permission under the &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC-BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Overview of the Crossmodal 3600 Dataset&lt;/h2&gt;&lt;p&gt;Creating large training and evaluation datasets in multiple languages is a resource-intensive endeavor. &lt;a href="https://aclanthology.org/2020.acl-main.16/"&gt;Recent work&lt;/a&gt; has shown that it is feasible to build multilingual image captioning models trained on machine-translated data with English captions as the starting point. However, some of the most reliable automatic metrics for image captioning are much less effective when applied to evaluation sets with translated image captions, resulting in poorer agreement with human evaluations compared to the English case. As such, trustworthy model evaluation at present can only be based on extensive human evaluation. Unfortunately, such evaluations usually cannot be replicated across different research efforts, and therefore do not offer a fast and reliable mechanism to automatically evaluate multiple model parameters and configurations (e.g., model &lt;a href="https://en.wikipedia.org/wiki/Hill_climbing"&gt;hill climbing&lt;/a&gt;) or to compare multiple lines of research. &lt;/p&gt;&lt;p&gt;XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images from the &lt;a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html"&gt;Open Images&lt;/a&gt; dataset. We measure the quality of generated captions by comparing them to the manually provided captions using the &lt;a href="https://arxiv.org/abs/1411.5726"&gt;CIDEr&lt;/a&gt; metric, which ranges from 0 (unrelated to the reference captions) to 10 (perfectly matching the reference captions). When comparing pairs of models, we observed strong correlations between the differences in the CIDEr scores of the model outputs, and side-by-side human evaluations comparing the model outputs. , making XM3600 is a reliable tool for high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Language Selection&lt;/h2&gt;&lt;p&gt;We chose 30 languages beyond English, roughly based on their percentage of web content. In addition, we chose an additional five languages that include under-resourced languages that have many native speakers or major native languages from continents that would not be covered otherwise. Finally, we also included English as a baseline, thus resulting in a total of 36 languages, as listed in the table below. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 9%; margin-right: 9%;"&gt;    &lt;colgroup&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 12%;"&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;&lt;td style="text-align: left;"&gt;Arabic     &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Bengali*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Chinese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Croatian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: left;"&gt;&lt;span style="font-size: small;"&gt;Cusco&lt;br /&gt;Quechua*&lt;/span&gt;   &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Czech    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Danish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Dutch    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;English    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Filipino    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Finnish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;French    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;German    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Greek    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Hebrew    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Hindi    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Hungarian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Indonesian    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Italian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Japanese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Korean    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Maori*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Norwegian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Persian    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Polish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Portuguese    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Romanian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Russian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Spanish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Swahili*    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Swedish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Telugu*    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Thai    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Turkish    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Ukrainian    &lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: left;"&gt;Vietnamese &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;List of languages used in XM3600. &amp;nbsp; *Low-resource languages with many native speakers, or major native languages from continents that would not be covered otherwise.&lt;/td&gt;&lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Image Selection&lt;/h2&gt;&lt;p&gt;The images were selected from among those in the &lt;a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html"&gt;Open Images&lt;/a&gt; dataset that have location metadata. Since there are many regions where more than one language is spoken, and some areas are not well covered by these images, we designed an algorithm to maximize the correspondence between selected images and the regions where the targeted languages are spoken. The algorithm starts with the selection of images with geo-data corresponding to the languages for which we have the smallest pool (e.g., Persian) and processes them in increasing order of their candidate image pool size. If there aren't enough images in an area where a language is spoken, then we gradually expand the geographic selection radius to: (i) a country where the language is spoken; (ii) a continent where the language is spoken; and, as last resort, (iii) from anywhere in the world. This strategy succeeded in providing our target number of 100 images from an appropriate region for most of the 36 languages, except for Persian (where 14 continent-level images are used) and Hindi (where all 100 images are at the global level, because the in-region images were assigned to Bengali and Telugu). &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;English&lt;br&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s640/1-1-car.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s320/1-1-car.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/lodekka/5072748008"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/lodekka/"&gt;Chris Sampson&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;Swahili&lt;br&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s640/1-2-giraffe.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s320/1-2-giraffe.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/henrikpalm/7555980588"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/henrikpalm/"&gt;Henrik Palm&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;Telugu&lt;br&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s640/1-3-statue.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s320/1-3-statue.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/14675798@N06/6901350577"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/14675798@N06/"&gt;rojypala&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;td&gt;&lt;br /&gt;&lt;/td&gt; &lt;/tr&gt;    &lt;tr&gt;&lt;td style="text-align: center;"&gt;Cusco Quechua&lt;br /&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s640/2-1-tree.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s320/2-1-tree.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/mckaysavage/8296819497"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/mckaysavage/"&gt;McKay Savage&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;Filipino&lt;br /&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s640/2-2-boats.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s320/2-2-boats.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/schoeters/2683433042"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/schoeters/"&gt;Simon Schoeters&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: center;"&gt;Chinese&lt;br /&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s640/2-3-carving.jpg" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="450" data-original-width="640" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s320/2-3-carving.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/rapidtravelchai/8639346056"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/rapidtravelchai/"&gt;Stefan Krasowski&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;br&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Sample images showcasing the geographical diversity of the annotated images. Images used under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;h2&gt;Caption Generation&lt;/h2&gt;&lt;p&gt;In total, all 3600 images (100 images per language) are annotated in all 36 languages, each with an average of two annotations per language, yielding a total of 261,375 captions. &lt;/p&gt;&lt;p&gt;Annotators work in batches of 15 images. The first screen shows all 15 images with their captions in English as generated by a captioning model trained to output a consistent style of the form "&amp;lt;main salient objects&amp;gt; doing &amp;lt;activities&amp;gt; in the &amp;lt;environment&amp;gt;", often with object attributes, such as a "smiling" person, "red" car, etc. The annotators are asked to rate the caption quality given guidelines for a 4-point scale from "excellent" to "bad", plus an option for "not_enough_information". This step forces the annotators to carefully assess caption quality and it primes them to internalize the style of the captions. The following screens show the images again but individually and without the English captions, and the annotators are asked to produce descriptive captions in the target language for each image.  &lt;/p&gt;&lt;p&gt;The image batch size of 15 was chosen so that the annotators would internalize the style without remembering the exact captions. Thus, we expect the raters to generate captions based on the image content only and lacking translation artifacts. For example in the example shown below, the Spanish caption mentions “number 42” and the Thai caption mentions “convertibles”, none of which are mentioned in the English captions. The annotators were also provided with a protocol to use when creating the captions, thus achieving style consistency across languages. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;  &lt;colgroup&gt;     &lt;col style="width: 30%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 10%;"&gt;&lt;/col&gt;     &lt;col style="width: 2%;"&gt;&lt;/col&gt;     &lt;col style="width: 58%;"&gt;&lt;/col&gt;  &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;&lt;td rowspan="8" style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s641/image3.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="479" data-original-width="641" height="239" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s320/image3.jpg" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&lt;em&gt;&lt;a href="https://www.flickr.com/photos/briansolis/5129089526"&gt;Photo&lt;/a&gt; by &lt;a href="https://www.flickr.com/people/briansolis/"&gt;Brian Solis&lt;/a&gt;&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan="8"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td rowspan="2" style="text-align: left;"&gt;&lt;span style="font-size: small;"&gt;English&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan="2"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;A vintage sports car in a showroom with many other vintage sports cars&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;The branded classic cars in a row at display&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;    &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td rowspan="2" style="text-align: left;"&gt;&lt;span style="font-size: small;"&gt;Spanish&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan="2"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;Automóvil clásico deportivo en exhibición de automóviles de galería — &lt;em&gt;(Classic sports car in gallery car show)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;Coche pequeño de carreras color plateado con el número 42 en una exhibición de coches — &lt;em&gt;(Small silver racing car with the number 42 at a car show)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;    &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td rowspan="2" style="text-align: left;"&gt;&lt;span style="font-size: small;"&gt;Thai&lt;/span&gt;&lt;/td&gt;    &lt;td rowspan="2"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;รถเปิดประทุนหลายสีจอดเรียงกันในที่จัดแสดง — &lt;em&gt;(Multicolored convertibles line up in the exhibit)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: left;"&gt;• &lt;span style="font-size: small;"&gt;รถแข่งวินเทจจอดเรียงกันหลายคันในงานจัดแสดง — &lt;em&gt;(Several vintage racing cars line up at the show.)&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;br&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;  &lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Sample captions in three different languages (out of 36 — see full list of captions in Appendix A of the &lt;a href="https://arxiv.org/abs/2205.12522"&gt;Crossmodal-3600 paper&lt;/a&gt;), showcasing the creation of annotations that are consistent in style across languages, while being free of direct-translation artifacts (e.g., the Spanish “number 42” or the Thai “convertibles” would not be possible when directly translating from the English versions). Image used under &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC BY 2.0 license&lt;/a&gt;.&lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;  &lt;h2&gt;Caption Quality and Statistics&lt;/h2&gt;&lt;p&gt;We ran two to five pilot studies per language to troubleshoot the caption generation process and to ensure high quality captions. We then manually evaluated a random subset of captions. First we randomly selected a sample of 600 images. Then, to measure the quality of captions in a particular language, for each image, we selected for evaluation one of the manually generated captions. We found that: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;For 25 out of 36 languages, the percentage of captions rated as “Good” or “Excellent” is above 90%, and the rest are all above 70%.&lt;/li&gt;  &lt;li&gt;For 26 out of 36 languages, the percentage of captions rated as “Bad” is below 2%, and the rest are all below 5%.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For languages that use spaces to separate words, the number of words per caption can be as low as 5 or 6 for some &lt;a href="https://en.wikipedia.org/wiki/Agglutinative_language"&gt;agglutinative languages&lt;/a&gt; like Cusco Quechua and Czech, and as high as 18 for an &lt;a href="https://en.wikipedia.org/wiki/Analytic_language"&gt;analytic language&lt;/a&gt; like Vietnamese. The number of characters per caption also varies drastically — from mid-20s for Korean to mid-90s for Indonesian — depending on the alphabet and the script of the language. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Empirical Evaluation and Results&lt;/h2&gt;&lt;p&gt;We empirically measured the ability of the XM3600 annotations to rank image captioning model variations by training four variations of a multilingual image captioning model and comparing the CIDEr differences of the models’ outputs over the XM3600 dataset for 30+ languages, to side-by-side human evaluations. We observed strong correlations between the CIDEr differences and the human evaluations. These results support the use of the XM3600 references as a means to achieve high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Recent Uses&lt;/h2&gt;&lt;p&gt;Recently &lt;a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html"&gt;PaLI&lt;/a&gt; used XM3600 to evaluate model performance beyond English for image captioning, image-to-text retrieval and text-to-image retrieval. The key takeaways they found when evaluating on XM3600 were that multilingual captioning greatly benefits from scaling the PaLI models, especially for low-resource languages. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;We would like to acknowledge the coauthors of this work: Xi Chen and Radu Soricut.&lt;/em&gt;   &lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4616939676419842373/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/crossmodal-3600-multilingual-reference.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4616939676419842373" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4616939676419842373" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/crossmodal-3600-multilingual-reference.html" rel="alternate" title="Crossmodal-3600 — Multilingual Reference Captions for Geographically Diverse Images" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA3qznWJUsCY3Jh7Z5CPA5U3dJIBWD28I3OOEXklBBY4TseAh5iap2pmDdYi-s4PbBZZu7lgMut_pW9WpbEr9FLyRK0WBnzejaerkRjJfNAc4tKhOvbnxhTY70akSiIrEir7NUxXhgbFyA1DopoQdqY0feRtoiTRkuyh6Goiqs2m73Yz9bFeBvkhekSg/s72-c/xm3600_animation_long.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-8280632452472362334</id><published>2022-10-06T13:05:00.002-07:00</published><updated>2022-10-22T19:48:00.054-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Acoustic Modeling"/><category scheme="http://www.blogger.com/atom/ns#" term="Audio"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">AudioLM: a Language Modeling Approach to Audio Generation</title><content type="html">&lt;span class="byline-author"&gt;Posted by Zalán Borsos, Research Software Engineer, and Neil Zeghidour, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it &lt;a href="https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html"&gt;text transcripts for speech synthesis&lt;/a&gt; or &lt;a href="https://magenta.tensorflow.org/music-transformer"&gt;MIDI representations for piano&lt;/a&gt;. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to &lt;a href="https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html"&gt;help people with speech impairments recover their voice&lt;/a&gt;, or stylistic components of a piano performance. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2209.03143"&gt;AudioLM: a Language Modeling Approach to Audio Generation&lt;/a&gt;”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our &lt;a href="http://ai.google/principles"&gt;AI Principles&lt;/a&gt;, we've also developed a model to identify synthetic audio generated by AudioLM. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;From Text to Audio Language Models&lt;/h2&gt;&lt;p&gt;In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from &lt;a href="https://blog.google/technology/ai/lamda/"&gt;open-ended dialogue&lt;/a&gt; to &lt;a href="https://ai.googleblog.com/2022/05/24-new-languages-google-translate.html"&gt;machine translation&lt;/a&gt; or even &lt;a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"&gt;common-sense reasoning&lt;/a&gt;. They have further shown their capacity to model other signals than texts, such &lt;a href="https://parti.research.google/"&gt;as natural images&lt;/a&gt;. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.  &lt;/p&gt;&lt;p&gt;However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio &lt;a href="https://en.wikipedia.org/wiki/Waveform"&gt;waveform&lt;/a&gt; typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.  &lt;/p&gt;&lt;p&gt;To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, &lt;em&gt;semantic tokens&lt;/em&gt; are extracted from &lt;a href="https://arxiv.org/abs/2108.06209"&gt;w2v-BERT&lt;/a&gt;, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.   &lt;/p&gt;&lt;p&gt;However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on &lt;em&gt;acoustic tokens&lt;/em&gt; produced by &lt;a href="https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html"&gt;a SoundStream neural codec&lt;/a&gt;, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.  &lt;/p&gt;&lt;div style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s960/image2.png"&gt;&lt;img border="0" data-original-height="330" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;h2&gt;Training an Audio-Only Language Model&lt;/h2&gt;&lt;p&gt;AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer&lt;/a&gt; models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLXFSPJXdgbOkxanFQ4TChCGNF6dRkntaSTtsoFChIR4Tp97RJesNvMhwVlgHLcI7j_D6Vkur2u5GtTVOVrswAsoNK93VtOEU0xBFqgS6SPrnYCl5O8cA_9qNCRADnty2_seXrB5fYsYoLjRyKAFdHlRfwJvHl-iDTW7V07EQJUQeBAen3jfktwLn/s958/image6.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="184" data-original-width="958" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyLXFSPJXdgbOkxanFQ4TChCGNF6dRkntaSTtsoFChIR4Tp97RJesNvMhwVlgHLcI7j_D6Vkur2u5GtTVOVrswAsoNK93VtOEU0xBFqgS6SPrnYCl5O8cA_9qNCRADnty2_seXrB5fYsYoLjRyKAFdHlRfwJvHl-iDTW7V07EQJUQeBAen3jfktwLn/s16000/image6.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwPPMHQ8s-D3GVIeGIxaRmwVqcb4sh7eZHs7El7gwP7pvoXk3Cv3qSjBhETkMzJzt7_3eBV8t4FkdAExVCVV-i7RrWo2HZJoai3w4Nj1R7mf4tatvPa8jOOO-iqZWmMYyg-o9we-MLi81QgCp4pG5AGDI-ifHgrw1Oc1-6kb2ZMflnEsqjdnonLOuM/s960/image1.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="166" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwPPMHQ8s-D3GVIeGIxaRmwVqcb4sh7eZHs7El7gwP7pvoXk3Cv3qSjBhETkMzJzt7_3eBV8t4FkdAExVCVV-i7RrWo2HZJoai3w4Nj1R7mf4tatvPa8jOOO-iqZWmMYyg-o9we-MLi81QgCp4pG5AGDI-ifHgrw1Oc1-6kb2ZMflnEsqjdnonLOuM/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform. &lt;/p&gt;&lt;div style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdFRqUq5R0CwA7eaE0qAXStL2AAZw3LNOxus3NYoz_JkbWSXz1ydyb839s0Z5qPk_wTGgAYL4B_XZBQRXaSsYtb9RYZZpf8kB8UjhwbGMZBhqswxse110R2OaVc4szGTvcSqZLm4hSCQ3howGlmEMoJxhvonK3MWkp49RquIhciqCJ349fCv6KxvUl/s960/image4.png"&gt;&lt;img border="0" data-original-height="403" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdFRqUq5R0CwA7eaE0qAXStL2AAZw3LNOxus3NYoz_JkbWSXz1ydyb839s0Z5qPk_wTGgAYL4B_XZBQRXaSsYtb9RYZZpf8kB8UjhwbGMZBhqswxse110R2OaVc4szGTvcSqZLm4hSCQ3howGlmEMoJxhvonK3MWkp49RquIhciqCJ349fCv6KxvUl/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Speech continuation&lt;/em&gt;, where the model is expected to retain the speaker characteristics, &lt;a href="https://en.wikipedia.org/wiki/Prosody_(linguistics)"&gt;prosody&lt;/a&gt; and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent. &lt;/li&gt;&lt;li&gt;&lt;em&gt;Piano continuation&lt;/em&gt;, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on &lt;a href="https://google-research.github.io/seanet/audiolm/examples/"&gt;this webpage&lt;/a&gt;. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" frameborder="0" height="360" src="https://www.youtube.com/embed/_xkZwJ0H9IU" width="640" youtube-src-id="_xkZwJ0H9IU"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;p&gt;To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener. &lt;/p&gt;&lt;p&gt;Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our &lt;a href="http://ai.google/principles"&gt;AI Principles&lt;/a&gt;, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;em&gt;The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/8280632452472362334/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8280632452472362334" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8280632452472362334" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html" rel="alternate" title="AudioLM: a Language Modeling Approach to Audio Generation" type="text/html"/><author><name>Andrew Helton</name><uri>http://www.blogger.com/profile/12510940840342590054</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX0MyO_IXk730mCbbJX7LXxBRIxJk2K41Y4leuEk4WQRjz0kgIp9CGHFwLePaKt3qEcCK8fvhAxjJ7J_sXH05q7xnsMbjZZFDDLPIlVyaKr3yYo77oT2KBqe9gw4MFuUZnUfxFprP67ExPzr2RNxduB0SruUGjJXghihHSoxvMtlG3YNtHHesZOJzY/s72-c/image2.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4364275298594755861</id><published>2022-10-04T11:43:00.005-07:00</published><updated>2022-10-22T19:51:19.210-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="video"/><title type="text">Large Motion Frame Interpolation</title><content type="html">&lt;span class="byline-author"&gt;Posted by Fitsum Reda and Janne Kontkanen, Google Research&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Motion_interpolation"&gt;Frame interpolation&lt;/a&gt; is the process of synthesizing in-between images from a given set of images. The technique is often used for &lt;a href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Frame_rate%23Frame_rate_up-conversion&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1664322285856563&amp;amp;usg=AOvVaw1YyWoW7xD9EqEtOSAjIKSD"&gt;temporal up-sampling&lt;/a&gt; to increase the refresh rate of videos or to create slow motion effects. Nowadays, with digital cameras and smartphones, we often take several photos within a few seconds to capture the best picture. Interpolating between these “near-duplicate” photos can lead to engaging videos that reveal scene motion, often delivering an even more pleasing sense of the moment than the original photos.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;Frame interpolation between consecutive video frames, which often have small motion, has been studied extensively. Unlike videos, however, the temporal spacing between near-duplicate photos can be several seconds, with commensurately large in-between motion, which is a major failing point of existing frame interpolation methods. Recent methods attempt to handle large motion by training on datasets with &lt;a href="https://arxiv.org/abs/2103.16206"&gt;extreme motion&lt;/a&gt;, albeit with limited effectiveness on &lt;a href="https://arxiv.org/abs/2108.06815"&gt;smaller motions&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;In “&lt;a href="https://arxiv.org/pdf/2202.04901.pdf"&gt;FILM: Frame Interpolation for Large Motion&lt;/a&gt;”, published at &lt;a href="https://eccv2022.ecva.net/"&gt;ECCV 2022&lt;/a&gt;, we present a method to create high quality slow-motion videos from near-duplicate photos. FILM is a new neural network architecture that achieves state-of-the-art results in large motion, while also handling smaller motions well. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s1920/image3.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1080" data-original-width="1920" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FILM interpolating between two near-duplicate photos to create a slow motion video.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;FILM Model Overview&lt;/h2&gt;&lt;p&gt;The FILM model takes two images as input and outputs a middle image. At inference time, we recursively invoke the model to output in-between images. FILM has three components: (1) A feature extractor that summarizes each input image with deep multi-scale (&lt;a href="https://arxiv.org/abs/1612.03144"&gt;pyramid&lt;/a&gt;) features; (2) a bi-directional motion estimator that computes pixel-wise motion (i.e., flows) at each pyramid level; and (3) a fusion module that outputs the final interpolated image. We train FILM on regular video frame triplets, with the middle frame serving as the ground-truth for supervision. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s530/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="530" data-original-width="319" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A standard&lt;a href="https://arxiv.org/abs/1612.03144"&gt; feature pyramid&lt;/a&gt; extraction on two input images. Features are processed at each level by a series of convolutions, which are then downsampled to half the spatial resolution and passed as input to the deeper level.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Scale-Agnostic Feature Extraction &lt;/h2&gt;&lt;p&gt;Large motion is typically handled with hierarchical motion estimation using multi-resolution feature pyramids (shown above). However, this method struggles with small and fast-moving objects because they can disappear at the deepest pyramid levels. In addition, there are far fewer available pixels to derive supervision at the deepest level. &lt;/p&gt; &lt;p&gt;To overcome these limitations, we adopt a &lt;a href="https://augmentedperception.github.io/pixelfusion/"&gt;feature extractor&lt;/a&gt; that shares weights across scales to create a “scale-agnostic” feature pyramid. This feature extractor (1) allows the use of a shared motion estimator across pyramid levels (next section) by equating large motion at shallow levels with small motion at deeper levels, and (2) creates a compact network with fewer weights.  &lt;/p&gt; &lt;p&gt;Specifically, given two input images, we first create an image pyramid by successively downsampling each image. Next, we use a shared &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1"&gt;U-Net&lt;/a&gt; convolutional encoder to extract a smaller feature pyramid from each image pyramid level (columns in the figure below). As the third and final step, we construct a scale-agnostic feature pyramid by horizontally concatenating features from different convolution layers that have the same spatial dimensions. Note that from the third level onwards, the feature stack is constructed with the same set of shared convolution weights (shown in the same color). This ensures that all features are similar, which allows us to continue to share weights in the subsequent motion estimator. The figure below depicts this process using four pyramid levels, but in practice, we use seven. &lt;/p&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Bi-directional Flow Estimation &lt;/h2&gt;&lt;p&gt;After feature extraction, FILM performs pyramid-based residual flow estimation to compute the flows from the yet-to-be-predicted middle image to the two inputs. The flow estimation is done once for each input, starting from the deepest level, using a stack of convolutions. We estimate the flow at a given level by adding a residual correction to the upsampled estimate from the next deeper level. This approach takes the following as its input: (1) the features from the first input at that level, and (2) the features of the second input after it is warped with the upsampled estimate. The same convolution weights are shared across all levels, except for the two finest levels. &lt;/p&gt; &lt;p&gt;Shared weights allow the interpretation of small motions at deeper levels to be the same as large motions at shallow levels, boosting the number of pixels available for large motion supervision. Additionally, shared weights not only enable the training of powerful models that may reach a higher &lt;a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"&gt;peak signal-to-noise ratio&lt;/a&gt; (PSNR), but are also needed to enable models to fit into GPU memory for practical applications.&lt;/p&gt;  &lt;table align="center" cellpadding="2" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s768/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="768" data-original-width="579" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;  &lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s768/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="768" data-original-width="571" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The impact of weight sharing on image quality. &lt;b&gt;Left&lt;/b&gt;: no sharing, &lt;b&gt;Right&lt;/b&gt;: sharing. For this ablation we used a smaller version of our model (called FILM-med in the &lt;a href="https://arxiv.org/pdf/2202.04901.pdf"&gt;paper&lt;/a&gt;) because the full model without weight sharing would diverge as the regularization benefit of weight sharing was lost.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Fusion and Frame Generation&lt;/h2&gt;&lt;p&gt;Once the bi-directional flows are estimated, we warp the two feature pyramids into alignment. We obtain a concatenated feature pyramid by stacking, at each pyramid level, the two aligned feature maps, the bi-directional flows and the input images. Finally, a &lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1"&gt;U-Net&lt;/a&gt; decoder synthesizes the interpolated output image from the aligned and stacked feature pyramid.  &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s1600/image9.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1000" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s16000/image9.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FILM Architecture. &lt;b&gt;FEATURE EXTRACTION&lt;/b&gt;: we extract scale-agnostic features. The features with matching colors are extracted using shared weights. &lt;b&gt;FLOW ESTIMATION&lt;/b&gt;: we compute bi-directional flows using shared weights across the deeper pyramid levels and warp the features into alignment. &lt;b&gt;FUSION&lt;/b&gt;: A U-Net decoder outputs the final interpolated frame.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Loss Functions &lt;/h2&gt;&lt;p&gt;During training, we supervise FILM by combining three losses. First, we use the &lt;a href="https://en.wikipedia.org/wiki/Least_absolute_deviations"&gt;absolute L1&lt;/a&gt; difference between the predicted and ground-truth frames to capture the motion between input images. However, this produces blurry images when used alone. Second, we use &lt;a href="https://arxiv.org/abs/1603.08155"&gt;perceptual loss&lt;/a&gt; to improve image fidelity. This minimizes the L1 difference between the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; pre-trained &lt;a href="https://arxiv.org/abs/1409.1556"&gt;VGG-19&lt;/a&gt; features extracted from the predicted and ground truth frames. Third, we use &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf"&gt;Style loss&lt;/a&gt; to minimize the L2 difference between the &lt;a href="https://en.wikipedia.org/wiki/Gram_matrix"&gt;Gram matrix&lt;/a&gt; of the ImageNet pre-trained VGG-19 features. The Style loss enables the network to produce sharp images and realistic &lt;a href="https://en.wikipedia.org/wiki/Inpainting"&gt;inpaintings&lt;/a&gt; of large pre-occluded regions. Finally, the losses are combined with weights empirically selected such that each loss contributes equally to the total loss. &lt;/p&gt;  &lt;p&gt;Shown below, the combined loss greatly improves sharpness and image fidelity when compared to training FILM with L1 loss and VGG losses. The combined loss maintains the sharpness of the tree leaves.  &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s1629/image8.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="640" data-original-width="1629" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s16000/image8.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FILM’s combined loss functions. L1 loss (&lt;b&gt;left&lt;/b&gt;), L1 plus VGG loss (&lt;b&gt;middle&lt;/b&gt;), and Style loss (&lt;b&gt;right&lt;/b&gt;), showing significant sharpness improvements (green box).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Image and Video Results&lt;/h2&gt;&lt;p&gt;We evaluate FILM on an internal near-duplicate photos dataset that exhibits large scene motion. Additionally, we compare FILM to recent frame interpolation methods: &lt;a href="https://arxiv.org/abs/2003.05534"&gt;SoftSplat&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2108.06815"&gt;ABME&lt;/a&gt;. FILM performs favorably when interpolating across large motion. Even in the presence of motion as large as 100 pixels, FILM generates sharp images consistent with the inputs.&lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s1897/image6.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="909" data-original-width="1897" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s16000/image6.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Frame interpolation with SoftSplat (&lt;b&gt;left&lt;/b&gt;), ABME (&lt;b&gt;middle&lt;/b&gt;) and FILM (&lt;b&gt;right&lt;/b&gt;) showing favorable image quality and temporal consistency. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s1896/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="909" data-original-width="1896" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s1174/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="756" data-original-width="1174" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Large motion interpolation. &lt;b&gt;Top&lt;/b&gt;: 64x slow motion video. &lt;b&gt;Bottom&lt;/b&gt; (left to right): The two input images blended, SoftSplat interpolation, ABME interpolation, and FILM interpolation. FILM captures the dog’s face while maintaining the background details.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;We introduce FILM, a large motion frame interpolation neural network. At its core, FILM adopts a scale-agnostic feature pyramid that shares weights across scales, which allows us to build a “scale-agnostic” bi-directional motion estimator that learns from frames with normal motion and generalizes well to frames with large motion. To handle wide disocclusions caused by large scene motion, we supervise FILM by matching the Gram matrix of ImageNet pre-trained VGG-19 features, which results in realistic inpainting and crisp images. FILM performs favorably on large motion, while also handling small and medium motions well, and generates temporally smooth high quality videos. &lt;/p&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Try It Out Yourself&lt;/h2&gt;&lt;p&gt;You can try out FILM on your photos using the &lt;a href="https://film-net.github.io"&gt;source code&lt;/a&gt;, which is now publicly available. &lt;/p&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;i&gt;We would like to thank Eric Tabellion, Deqing Sun, Caroline Pantofaru, Brian Curless for their contributions. We thank Marc Comino Trinidad for his contributions on the scale-agnostic feature extractor, Orly Liba and Charles Herrmann for feedback on the text, Jamie Aspinall for the imagery in the paper, Dominik Kaeser, Yael Pritch, Michael Nechyba, William T. Freeman, David Salesin, Catherine Wah, and Ira Kemelmacher-Shlizerman for support. Thanks to Tom Small for creating the animated diagram in this post.&lt;/i&gt;  &lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4364275298594755861/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4364275298594755861" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4364275298594755861" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html" rel="alternate" title="Large Motion Frame Interpolation" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s72-c/image3.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5097682856425549750</id><published>2022-09-27T10:32:00.002-07:00</published><updated>2022-10-22T19:53:27.922-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="resource optimization"/><title type="text">Quantization for Fast and Environmentally Sustainable Reinforcement Learning</title><content type="html">&lt;span class="byline-author"&gt;Posted by Srivatsan Krishnan, Student Researcher, and Aleksandra Faust, Senior Staff Research Scientist, Google Research, Brain Team &lt;/span&gt;  &lt;p&gt;Deep &lt;a href="https://en.wikipedia.org/wiki/Q-learning"&gt;reinforcement learning&lt;/a&gt; (RL) continues to make great strides in solving real-world sequential decision-making problems such as &lt;a href="https://ai.googleblog.com/2022/02/the-balloon-learning-environment.html"&gt;balloon navigation&lt;/a&gt;, &lt;a href="https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control"&gt;nuclear physics&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html"&gt;robotics&lt;/a&gt;, and &lt;a href="https://ai.googleblog.com/2019/06/introducing-google-research-football.html"&gt;games&lt;/a&gt;. Despite its promise, one of its limiting factors is long training times. While the current approach to &lt;a href="https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html"&gt;speed up RL training&lt;/a&gt; on complex and difficult tasks leverages &lt;a href="https://github.com/deepmind/acme"&gt;distributed training&lt;/a&gt; scaling up to hundreds or even thousands of computing nodes, it still requires the use of significant hardware resources which makes RL training expensive, while increasing its environmental impact. However, recent work [&lt;a href="https://blog.google/technology/ai/minimizing-carbon-footprint/"&gt;1&lt;/a&gt;, &lt;a href="https://proceedings.mlsys.org/paper/2022/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf"&gt;2&lt;/a&gt;] indicates that performance optimizations on existing hardware can reduce the &lt;a href="https://en.wikipedia.org/wiki/Carbon_footprint"&gt;carbon footprint&lt;/a&gt; (i.e., total &lt;a href="https://en.wikipedia.org/wiki/Greenhouse_gas_emissions"&gt;greenhouse gas&lt;/a&gt; emissions) of training and inference.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;  &lt;p&gt;RL can also benefit from similar system optimization techniques that can reduce training time, improve hardware utilization and reduce carbon dioxide (CO&lt;sub&gt;2&lt;/sub&gt;) emissions. One such technique is quantization, a process that converts full-precision floating point (&lt;a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format"&gt;FP32&lt;/a&gt;) numbers to lower precision (&lt;a href="https://www.gnu.org/software/libc/manual/html_node/Integers.html"&gt;int8&lt;/a&gt;) numbers and then performs computation using the lower precision numbers. Quantization can save memory storage cost and bandwidth for faster and more energy-efficient computation. Quantization has been successfully applied to supervised learning to &lt;a href="https://blog.tensorflow.org/2021/06/how-tensorflow-helps-edge-impulse-make-ml-accessible.html"&gt;enable edge deployments&lt;/a&gt; of machine learning (ML) models and achieve &lt;a href="https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/"&gt;faster training&lt;/a&gt;. However, there remains an opportunity to apply quantization to RL training.  &lt;/p&gt;  &lt;p&gt;To that end, we present “&lt;a href="https://openreview.net/pdf?id=xwWsiFmUEs"&gt;QuaRL: Quantization for Fast and Environmentally Sustainable&lt;/a&gt;&lt;a href="https://openreview.net/pdf?id=xwWsiFmUEs"&gt;Reinforcement Learning&lt;/a&gt;”, published in the &lt;i&gt;&lt;a href="https://jmlr.org/tmlr/"&gt;Transactions of Machine Learning Research&lt;/a&gt;&lt;/i&gt; journal, which introduces&lt;i&gt; &lt;/i&gt;a new paradigm called &lt;i&gt;ActorQ &lt;/i&gt;that applies quantization to speed up RL training by 1.5-5.4x while maintaining performance. Additionally, we demonstrate that compared to training in full-precision, the carbon footprint is also significantly reduced by a factor of 1.9-3.8x. &lt;/p&gt;  &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Applying Quantization to RL Training&lt;/h2&gt;&lt;p&gt;In traditional RL training, a &lt;i&gt;learner &lt;/i&gt;policy is applied to an &lt;i&gt;actor&lt;/i&gt;, which uses the policy to explore the environment and collect data samples. The samples collected by the &lt;i&gt;actor&lt;/i&gt; are then used by the &lt;i&gt;learner&lt;/i&gt; to continuously refine the initial policy. Periodically, the policy trained on the learner side is used to update the &lt;i&gt;actor's&lt;/i&gt; policy. To apply quantization to RL training, we develop the ActorQ paradigm. ActorQ performs the same sequence described above, with one key difference being that the policy update from learner to actors is quantized, and the actor explores the environment using the int8 quantized policy to collect samples. &lt;/p&gt; &lt;p&gt;Applying quantization to RL training in this fashion has two key benefits. First, it reduces the &lt;a href="https://en.wikipedia.org/wiki/Memory_footprint"&gt;memory footprint&lt;/a&gt; of the policy. For the same peak bandwidth, less data is transferred between learners and actors, which reduces the communication cost for policy updates from learners to actors. Second, the actors perform inference on the quantized policy to generate actions for a given environment state. The quantized inference process is much faster when compared to performing inference in full precision.  &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s1600/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="322" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s16000/image4.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An overview of traditional RL training (&lt;b&gt;left&lt;/b&gt;) and ActorQ RL training (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In ActorQ, we use the &lt;a href="https://github.com/deepmind/acme"&gt;ACME&lt;/a&gt; distributed RL framework. The quantizer block performs uniform quantization that converts the FP32 policy to int8. The actor performs inference using optimized int8 computations. Though we use uniform quantization when designing the quantizer block, we believe that other &lt;a href="https://arxiv.org/abs/1806.08342"&gt;quantization techniques&lt;/a&gt; can replace uniform quantization and produce similar results. The samples collected by the actors are used by the learner to train a neural network policy. Periodically the learned policy is quantized by the quantizer block and broadcasted to the actors. &lt;/p&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Quantization Improves RL Training Time and Performance&lt;/h2&gt;&lt;p&gt;We evaluate ActorQ in a range of environments, including the &lt;a href="https://www.deepmind.com/open-source/deepmind-control-suite"&gt;Deepmind Control Suite&lt;/a&gt; and the &lt;a href="https://openai.com/blog/openai-gym-beta/"&gt;OpenAI Gym&lt;/a&gt;. We demonstrate the speed-up and improved performance of &lt;a href="https://arxiv.org/abs/1804.08617"&gt;D4PG&lt;/a&gt; and &lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"&gt;DQN&lt;/a&gt;. We chose D4PG as it was the best learning algorithm in &lt;a href="https://arxiv.org/abs/2006.00979"&gt;ACME&lt;/a&gt; for Deepmind Control Suite tasks, and DQN is a widely used and standard RL algorithm.  &lt;/p&gt; &lt;p&gt;We observe a significant speedup (between 1.5x and 5.41x) in training RL policies. More importantly, performance is maintained even when actors perform int8 quantized inference. The figures below demonstrate this for the D4PG and DQN agents for Deepmind Control Suite and OpenAI Gym tasks. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_9JtTGkLC_0r_leaxnELxfZ8ZxbKDwaMi-wunwlbllT4SihT7gyDrGnk2rXC3eO5N9UshGSAUqyYge5DfU_at-hxyN8h1nnaNhMu4BArs_KxXarLdHmskQ5QqZHxjWVvxoTKJXzFQJIORP78gdXcAXgI_zod1Ef9RK683LQcWTNVn6DGj3rI-2ty-0g/s957/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="957" data-original-width="566" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_9JtTGkLC_0r_leaxnELxfZ8ZxbKDwaMi-wunwlbllT4SihT7gyDrGnk2rXC3eO5N9UshGSAUqyYge5DfU_at-hxyN8h1nnaNhMu4BArs_KxXarLdHmskQ5QqZHxjWVvxoTKJXzFQJIORP78gdXcAXgI_zod1Ef9RK683LQcWTNVn6DGj3rI-2ty-0g/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A comparison of RL training using the FP32 policy (q=32) and the quantized int8 policy (q=8) for D4PG agents on various Deepmind Control Suite tasks. Quantization achieves speed-ups of 1.5x to 3.06x.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyF643UhZYXbEb90Ue5asz0D5SL_OKcn_qY-fYvTsmUh7k5Yf5Jfua_LSMmEEHu_nTRU1TITAlH7Dwxmgmz9HnSmBlItTd6o24aFXv9V-i2m9940IdCpPl2uAcBcAme4GuBJ5O1AD88LK4np_KRwWCmraqPnxxT7ryhu3qsPnMgK8QkomODbUTnGA7CA/s1244/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="904" data-original-width="1244" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhyF643UhZYXbEb90Ue5asz0D5SL_OKcn_qY-fYvTsmUh7k5Yf5Jfua_LSMmEEHu_nTRU1TITAlH7Dwxmgmz9HnSmBlItTd6o24aFXv9V-i2m9940IdCpPl2uAcBcAme4GuBJ5O1AD88LK4np_KRwWCmraqPnxxT7ryhu3qsPnMgK8QkomODbUTnGA7CA/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A comparison of RL training using the FP32 policy (q=32) and the quantized int8 policy (q=8) for DQN agents in the OpenAI Gym environment. Quantization achieves a speed-up of 2.2x to 5.41x.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Quantization Reduces Carbon Emission&lt;/h2&gt;&lt;p&gt;Applying quantization in RL using ActorQ improves training time without affecting performance. The direct consequence of using the hardware more efficiently is a smaller carbon footprint. We measure the carbon footprint improvement by taking the ratio of carbon emission when using the FP32 policy during training over the carbon emission when using the int8 policy during training. &lt;/p&gt; &lt;p&gt;In order to measure the carbon emission for the RL training experiment, we use the &lt;a href="https://github.com/Breakend/experiment-impact-tracker"&gt;experiment-impact-tracker&lt;/a&gt; proposed in &lt;a href="https://jmlr.org/papers/volume21/20-312/20-312.pdf"&gt;prior work&lt;/a&gt;. We instrument the ActorQ system with carbon monitor APIs to measure the energy and carbon emissions for each training experiment. &lt;/p&gt; &lt;p&gt;Compared to the carbon emission when running in full precision (FP32), we observe that the quantization of policies reduces the carbon emissions anywhere from 1.9x to 3.76x, depending on the task. As RL systems are scaled to run on thousands of distributed hardware cores and accelerators, we believe that the absolute carbon reduction (measured in kilograms of CO&lt;sub&gt;2&lt;/sub&gt;) can be quite significant. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGC7inzPO6k_D-ZacfPL32YNfnSXt4AheMAFeRuGJisa8uTKac1kwOhepphdeXqx7KnrMFqV1ldHkxvmg2hzEBIGbhgdceEaNnxwWUwCXtHZ61TfjwQi0Ujdwqi-GcRNxZCYLFUXE2_oNr0ZdHG1mMyHv1xB0l5Nj0y31ebzv5QhNvYadQlGg1eZnetw/s1999/image8.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="939" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGC7inzPO6k_D-ZacfPL32YNfnSXt4AheMAFeRuGJisa8uTKac1kwOhepphdeXqx7KnrMFqV1ldHkxvmg2hzEBIGbhgdceEaNnxwWUwCXtHZ61TfjwQi0Ujdwqi-GcRNxZCYLFUXE2_oNr0ZdHG1mMyHv1xB0l5Nj0y31ebzv5QhNvYadQlGg1eZnetw/s16000/image8.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Carbon emission comparison between training using a FP32 policy and an int8 policy. The X-axis scale is normalized to the carbon emissions of the FP32 policy. Shown by the red bars greater than 1, ActorQ reduces carbon emissions.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Conclusion and Future Directions&lt;/h2&gt;&lt;p&gt;We introduce ActorQ, a novel paradigm that applies quantization to RL training and achieves speed-up improvements of 1.5-5.4x while maintaining performance. Additionally, we demonstrate that ActorQ can reduce RL training’s carbon footprint by a factor of 1.9-3.8x compared to training in full-precision without quantization. &lt;/p&gt; &lt;p&gt;ActorQ demonstrates that quantization can be effectively applied to many aspects of RL, from obtaining high-quality and efficient quantized policies to reducing training times and carbon emissions. As RL continues to make great strides in solving real-world problems, we believe that making RL training sustainable will be critical for adoption. As we scale RL training to thousands of cores and GPUs, even a 50% improvement (as we have experimentally demonstrated) will generate significant savings in absolute dollar cost, energy, and carbon emissions. Our work is the first step toward applying quantization to RL training to achieve efficient and environmentally sustainable training.  &lt;/p&gt; &lt;p&gt;While our design of the quantizer in ActorQ relied on simple uniform quantization, we believe that other forms of quantization, compression and sparsity can be applied (e.g., distillation, sparsification, etc.). We hope that future work will consider applying more aggressive quantization and compression methods, which may yield additional benefits to the performance and accuracy tradeoff obtained by the trained RL policies. &lt;/p&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;i&gt;We would like to thank our co-authors Max Lam, Sharad Chitlangia, Zishen Wan, and Vijay Janapa Reddi (Harvard University), and Gabriel Barth-Maron (DeepMind), for their contribution to this work. We also thank the Google Cloud team for providing research credits to seed this work.&lt;/i&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/5097682856425549750/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/quantization-for-fast-and.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5097682856425549750" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5097682856425549750" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/quantization-for-fast-and.html" rel="alternate" title="Quantization for Fast and Environmentally Sustainable Reinforcement Learning" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF_4o81i0gPf7bU-_E8Fqhy_Ih-AdGI6s_CNhcfVv6wirCsGJlMdwX7DQamF3VYTUbNNJn1E2omMJ3tziscCnTxrxMslApGiPfrg67-3H7WpKCcxXJg5ihqaSrKlOLClmgTIxy8yDKMIFZIQzZL1tNamHlGGvtXZW6zWDAHHlN8kRCZAjYGcuFuDaOYw/s72-c/image4.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-4863623362813999190</id><published>2022-09-22T14:33:00.009-07:00</published><updated>2022-10-22T20:01:27.187-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Cloud Computing"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="open source"/><title type="text">TensorStore for High-Performance, Scalable Array Storage</title><content type="html">&lt;span class="byline-author"&gt;Posted by Jeremy Maitin-Shepard and Laramie Leavitt, Software Engineers, Connectomics at Google&lt;/span&gt;&lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s800/image1.gif" style="display: none;" /&gt;&lt;p&gt;Many exciting contemporary applications of computer science and machine learning (ML) manipulate multidimensional datasets that span a single large coordinate system, for example, &lt;a href="https://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html"&gt;weather modeling&lt;/a&gt; from atmospheric measurements over a spatial grid or &lt;a href="https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html"&gt;medical imaging&lt;/a&gt; predictions from multi-channel image intensity values in a 2d or 3d scan. In these settings, even a single dataset may require terabytes or petabytes of data storage. Such datasets are also challenging to work with as users may read and write data at irregular intervals and varying scales, and are often interested in performing analyses using numerous machines working in parallel.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Today we are introducing &lt;a href="https://github.com/google/tensorstore"&gt;TensorStore&lt;/a&gt;, an open-source C++ and Python software library designed for storage and manipulation of &lt;em&gt;n-&lt;/em&gt;dimensional data that: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Provides a uniform API for reading and writing multiple array formats, including &lt;a href="https://zarr.readthedocs.io/en/stable/"&gt;zarr&lt;/a&gt; and &lt;a href="https://github.com/saalfeldlab/n5"&gt;N5&lt;/a&gt;.  &lt;/li&gt;  &lt;li&gt;Natively supports &lt;a href="https://google.github.io/tensorstore/kvstore/index.html"&gt;multiple storage systems&lt;/a&gt;, including &lt;a href="https://google.github.io/tensorstore/kvstore/gcs/index.html"&gt;Google Cloud Storage&lt;/a&gt;, &lt;a href="https://google.github.io/tensorstore/kvstore/file/index.html"&gt;local and network filesystems&lt;/a&gt;, &lt;a href="https://google.github.io/tensorstore/kvstore/http/index.html"&gt;HTTP servers&lt;/a&gt;, and &lt;a href="https://google.github.io/tensorstore/kvstore/memory/index.html"&gt;in-memory storage&lt;/a&gt;. &lt;/li&gt;  &lt;li&gt;Supports read/writeback caching and transactions, with strong &lt;a href="https://en.wikipedia.org/wiki/ACID"&gt;atomicity, isolation, consistency, and durability&lt;/a&gt; (ACID) guarantees. &lt;/li&gt;  &lt;li&gt;Supports safe, efficient access from multiple processes and machines via optimistic concurrency. &lt;/li&gt;  &lt;li&gt;Offers an asynchronous API to enable high-throughput access even to high-latency remote storage. &lt;/li&gt;  &lt;li&gt;Provides advanced, fully composable &lt;a href="https://google.github.io/tensorstore/python/indexing.html"&gt;indexing&lt;/a&gt; operations and virtual views. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TensorStore has already been used to solve key engineering challenges in scientific computing (e.g., management and processing of large datasets in neuroscience, such as &lt;a href="https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html"&gt;peta-scale 3d electron microscopy data&lt;/a&gt; and “4d” &lt;a href="https://www.youtube.com/watch?v=Nxa19uWC_oA"&gt;videos of neuronal activity&lt;/a&gt;). TensorStore has also been used in the creation of large-scale machine learning models such as &lt;a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"&gt;PaLM&lt;/a&gt; by addressing the problem of managing model parameters (checkpoints) during distributed training.  &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Familiar API for Data Access and Manipulation&lt;/h2&gt;&lt;p&gt;TensorStore provides a simple Python API for loading and manipulating large array data. In the following example, we create a TensorStore object that represents a 56 trillion voxel &lt;a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html"&gt;3d image of a fly brain&lt;/a&gt; and access a small 100x100 patch of the data as a &lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt; array: &lt;/p&gt;&lt;span style="font-size: small;"&gt;  &lt;pre class="prettyprint" style="margin-left: 40px; margin-right: 40px; white-space: pre-wrap;"&gt;&amp;gt;&amp;gt;&amp;gt; import tensorstore as ts&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np&lt;br /&gt;&lt;br /&gt;# Create a TensorStore object to work with fly brain data.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset = ts.open({&lt;br /&gt;...     'driver':&lt;br /&gt;...         'neuroglancer_precomputed',&lt;br /&gt;...     'kvstore':&lt;br /&gt;...         'gs://neuroglancer-janelia-flyem-hemibrain/' + &lt;br /&gt;...         'v1.1/segmentation/',&lt;br /&gt;... }).result()&lt;br /&gt;&lt;br /&gt;# Create a 3-d view (remove singleton 'channel' dimension):&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset_3d = dataset[ts.d['channel'][0]]&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset_3d.domain&lt;br /&gt;{ "x": [0, 34432), "y": [0, 39552), "z": [0, 41408) }&lt;br /&gt;&lt;br /&gt;# Convert a 100x100x1 slice of the data to a numpy ndarray&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; slice = np.array(dataset_3d[15000:15100, 15000:15100, 20000])&lt;/pre&gt;&lt;/span&gt; &lt;p&gt;Crucially, no actual data is accessed or stored in memory until the specific 100x100 slice is requested; hence arbitrarily large underlying datasets can be loaded and manipulated without having to store the entire dataset in memory, using indexing and manipulation syntax largely identical to standard NumPy operations. TensorStore also provides extensive support for &lt;a href="https://google.github.io/tensorstore/python/indexing.html"&gt;advanced indexing features&lt;/a&gt;, including transforms, &lt;a href="https://google.github.io/tensorstore/index_space.html#alignment-and-broadcasting"&gt;alignment, broadcasting&lt;/a&gt;, and virtual views (&lt;a href="https://google.github.io/tensorstore/python/api/tensorstore.cast.html#tensorstore.cast"&gt;data type conversion&lt;/a&gt;, &lt;a href="https://google.github.io/tensorstore/python/api/tensorstore.downsample-store.html"&gt;downsampling&lt;/a&gt;, &lt;a href="https://google.github.io/tensorstore/python/api/tensorstore.virtual_chunked.html"&gt;lazily on-the-fly generated arrays&lt;/a&gt;).  &lt;/p&gt;&lt;p&gt;The following example demonstrates how TensorStore can be used to create a zarr array, and how its asynchronous API enables higher throughput: &lt;/p&gt;&lt;span style="font-size: small;"&gt;&lt;pre class="prettyprint" style="margin-left: 40px; margin-right: 40px; white-space: pre-wrap;"&gt;&amp;gt;&amp;gt;&amp;gt; import tensorstore as ts&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Create a zarr array on the local filesystem&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; dataset = ts.open({&lt;br /&gt;...     'driver': 'zarr',&lt;br /&gt;...     'kvstore': 'file:///tmp/my_dataset/',&lt;br /&gt;... },&lt;br /&gt;... dtype=ts.uint32,&lt;br /&gt;... chunk_layout=ts.ChunkLayout(chunk_shape=[256, 256, 1]),&lt;br /&gt;... create=True,&lt;br /&gt;... shape=[5000, 6000, 7000]).result()&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Create two numpy arrays with example data to write.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; a = np.arange(100*200*300, dtype=np.uint32).reshape((100, 200, 300))&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; b = np.arange(200*300*400, dtype=np.uint32).reshape((200, 300, 400))&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Initiate two asynchronous writes, to be performed concurrently.&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_a = dataset[1000:1100, 2000:2200, 3000:3300].write(a)&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_b = dataset[3000:3200, 4000:4300, 5000:5400].write(b)&lt;br /&gt;&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; # Wait for the asynchronous writes to complete&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_a.result()&lt;br /&gt;&amp;gt;&amp;gt;&amp;gt; future_b.result()&lt;/pre&gt;&lt;/span&gt; &lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Safe and Performant Scaling &lt;/h2&gt;&lt;p&gt;Processing and analyzing large numerical datasets requires significant computational resources. This is typically achieved through parallelization across numerous CPU or accelerator cores spread across many machines. Therefore a fundamental goal of TensorStore has been to enable parallel processing of individual datasets that is both safe (i.e., avoids corruption or inconsistencies arising from parallel access patterns) and high performance (i.e., reading and writing to TensorStore is not a bottleneck during computation). In fact, in a test within Google’s datacenters, we found nearly linear scaling of read and write performance as the number of CPUs was increased: &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8b07lDLRkn-9O0urUMRv4nU96Yd-m3chrVmIAgw5tlceIU3dRKtpvK0IZAUS-hiqxtT-U1oe-AZBLdri1V-PJmTMw8EoVMlSO0BqzQRmmJN0oadFab5OyP4CieM1m9S1nNFOCWGNZ7obq5Jx9N5AXhf6zjChzLFnM7YECO3mJ6YEqQpc0o8JagSVX/s1999/image2.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1066" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8b07lDLRkn-9O0urUMRv4nU96Yd-m3chrVmIAgw5tlceIU3dRKtpvK0IZAUS-hiqxtT-U1oe-AZBLdri1V-PJmTMw8EoVMlSO0BqzQRmmJN0oadFab5OyP4CieM1m9S1nNFOCWGNZ7obq5Jx9N5AXhf6zjChzLFnM7YECO3mJ6YEqQpc0o8JagSVX/s16000/image2.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Read and write performance for a TensorStore dataset in &lt;a href="https://google.github.io/tensorstore/driver/zarr/index.html"&gt;zarr format&lt;/a&gt; residing on Google Cloud Storage (GCS) accessed concurrently using a variable number of single-core compute tasks in Google data centers. Both read and write performance scales nearly linearly with the number of compute tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Performance is achieved by implementing core operations in C++, extensive use of multithreading for operations such as encoding/decoding and network I/O, and partitioning large datasets into much smaller units through chunking to enable efficiently reading and writing subsets of the entire dataset. TensorStore also provides configurable in-memory caching (which reduces slower storage system interactions for frequently accessed data) and an asynchronous API that enables a read or write operation to continue in the background while a program completes other work.  &lt;/p&gt;&lt;p&gt;Safety of parallel operations when many machines are accessing the same dataset is achieved through the use of &lt;a href="https://en.wikipedia.org/wiki/Optimistic_concurrency_control"&gt;optimistic concurrency&lt;/a&gt;, which maintains compatibility with diverse underlying storage layers (including Cloud storage platforms, such as &lt;a href="https://cloud.google.com/storage/?gclid=CjwKCAjwlqOXBhBqEiwA-hhitER-4ub5UNrYCHC4rdBFwwNEl7PDYPvu9maM9G1d1wM1eY5QrzJ4NhoCme4QAvD_BwE&amp;amp;gclsrc=aw.ds"&gt;GCS&lt;/a&gt;, as well as local filesystems) without significantly impacting performance. TensorStore also provides strong ACID guarantees for all individual operations executing within a single runtime.  &lt;/p&gt;&lt;p&gt;To make distributed computing with TensorStore compatible with many existing data processing workflows, we have also integrated TensorStore with parallel computing libraries such as &lt;a href="https://beam.apache.org/"&gt;Apache Beam&lt;/a&gt; (&lt;a href="https://github.com/google/tensorstore/tree/master/tensorstore/examples/python/beam"&gt;example code&lt;/a&gt;) and &lt;a href="https://www.dask.org/"&gt;Dask&lt;/a&gt; (&lt;a href="https://github.com/google-research/connectomics"&gt;example code&lt;/a&gt;).  &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Use Case: Language Models &lt;/h2&gt;&lt;p&gt;An exciting recent development in ML is the emergence of more advanced language models such as &lt;a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"&gt;PaLM&lt;/a&gt;. These neural networks contain hundreds of billions of parameters and exhibit &lt;a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"&gt;some surprising capabilities&lt;/a&gt; in natural language understanding and generation. These models also push the limits of computational infrastructure; in particular, training a language model such as PaLM requires thousands of TPUs working in parallel.  &lt;/p&gt;&lt;p&gt;One challenge that arises during this training process is efficiently reading and writing the model parameters. Training is distributed across many separate machines, but parameters must be regularly saved to a single object (“checkpoint”) on a permanent storage system without slowing down the overall training process. Individual training jobs must also be able to read just the specific set of parameters they are concerned with in order to avoid the overhead that would be required to load the entire set of model parameters (which could be hundreds of gigabytes).  &lt;/p&gt;&lt;p&gt;TensorStore has already been used to address these challenges. It has been applied to manage checkpoints associated with large-scale (“&lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/googles-scalable-supercomputers-for-machine-learning-cloud-tpu-pods-are-now-publicly-available-in-beta"&gt;multipod&lt;/a&gt;”) models trained with &lt;a href="https://github.com/google/jax"&gt;JAX&lt;/a&gt;&amp;nbsp;(&lt;a href="https://github.com/google/jax/blob/640e15fe070a887197143b76a19a3dced816c8df/jax/experimental/gda_serialization/serialization.py"&gt;code example&lt;/a&gt;) and has been integrated with frameworks such as &lt;a href="https://arxiv.org/pdf/2203.17189.pdf"&gt;T5X&lt;/a&gt;&amp;nbsp;(&lt;a href="https://github.com/google-research/t5x/blob/d34fff62ddc71c86b680300f8dca198f9db7b246/t5x/checkpoints.py"&gt;code example&lt;/a&gt;) and &lt;a href="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/"&gt;Pathways&lt;/a&gt;. Model parallelism is used to partition the full set of parameters, which can occupy more than a terabyte of memory, over hundreds of TPUs. Checkpoints are stored in zarr format using TensorStore, with a chunk structure chosen to allow the partition for each TPU to be read and written independently in parallel. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s800/image1.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="746" data-original-width="800" height="373" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/w400-h373/image1.gif" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;When saving a checkpoint, each model parameter is written using TensorStore in zarr format using a chunk grid that further subdivides the grid used to partition the parameter over TPUs. The host machines write in parallel the zarr chunks for each of the partitions assigned to TPUs attached to that host. Using TensorStore's asynchronous API, training proceeds even while the data is still being written to persistent storage. When resuming from a checkpoint, each host reads only the chunks that make up the partitions assigned to that host.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Use Case: 3D Brain Mapping&lt;/h2&gt;&lt;p&gt;The field of synapse-resolution &lt;a href="https://en.wikipedia.org/wiki/Connectomics"&gt;connectomics&lt;/a&gt; aims to map the wiring of &lt;a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html"&gt;animal&lt;/a&gt; and &lt;a href="http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html"&gt;human&lt;/a&gt; brains at the detailed level of individual synaptic connections. This requires imaging the brain at extremely high resolution (nanometers) over fields of view of up to millimeters or more, which yields datasets that can span petabytes in size. In the future these datasets may extend to exabytes as scientists contemplate mapping &lt;a href="https://www.sciencedirect.com/science/article/pii/S0092867420310011"&gt;entire mouse&lt;/a&gt; or primate brains. However, even current datasets pose significant challenges related to storage, manipulation, and processing; in particular, even a single brain sample may require millions of gigabytes with a coordinate system (pixel space) of hundreds of thousands pixels in each dimension.  &lt;/p&gt;&lt;p&gt;We have used TensorStore to solve computational challenges associated with large-scale connectomic datasets. Specifically, TensorStore has managed some of the largest and most widely accessed connectomic datasets, with Google Cloud Storage as the underlying object storage system. For example, it has been applied to the &lt;a href="https://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html"&gt;human cortex “h01” dataset&lt;/a&gt;, which is a 3d nanometer-resolution image of human brain tissue. The raw imaging data is 1.4 petabytes (roughly 500,000 * 350,000 * 5,000 pixels large, and is further associated with additional content such as 3d segmentations and annotations that reside in the same coordinate system. The raw data is subdivided into individual chunks 128x128x16 pixels large and stored in the “&lt;a href="https://google.github.io/tensorstore/driver/neuroglancer_precomputed/index.html"&gt;Neuroglancer precomputed&lt;/a&gt;” format, which is optimized for &lt;a href="https://github.com/google/neuroglancer"&gt;web-based interactive viewing&lt;/a&gt; and can be easily &lt;a href="https://colab.research.google.com/gist/jbms/1ec1192c34ec816c2c517a3b51a8ed6c/h01_data_access.ipynb"&gt;manipulated from TensorStore&lt;/a&gt;.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM2p80P4faZAfq1_1m0GtjZP1-pFR1idRhN_z8XKijvrnvCYmmY_8bpF3qzloedAy444kxa8VqgAGSHLaWehjgLmLUn_-ZEZlAz3W_0rvIqk86DJhdoxg8Hp3eHcVqhYfrvpcpT7glU88g0GvbTqA5ho2okVK2emPi3OQuhvd65Vg14KKdq6O7AdHz/s640/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="360" data-original-width="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhM2p80P4faZAfq1_1m0GtjZP1-pFR1idRhN_z8XKijvrnvCYmmY_8bpF3qzloedAy444kxa8VqgAGSHLaWehjgLmLUn_-ZEZlAz3W_0rvIqk86DJhdoxg8Hp3eHcVqhYfrvpcpT7glU88g0GvbTqA5ho2okVK2emPi3OQuhvd65Vg14KKdq6O7AdHz/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A fly brain &lt;a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html"&gt;reconstruction&lt;/a&gt; for which the underlying data can be easily &lt;a href="https://colab.research.google.com/gist/jbms/10b7a91c8b2f8ecbf30a869a1c50defb/flyem-hemibrain-data-access.ipynb"&gt;accessed and manipulated using TensorStore&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Getting Started&lt;/h2&gt;&lt;p&gt;To get started using the TensorStore Python API, you can install the &lt;a href="https://pypi.org/project/tensorstore/"&gt;tensorstore PyPI package&lt;/a&gt; using: &lt;/p&gt;&lt;code style="margin-left: 40px; margin-right: 40px;"&gt;pip install tensorstore &lt;/code&gt;&lt;p&gt;Refer to the &lt;a href="https://google.github.io/tensorstore/python/tutorial.html"&gt;tutorials&lt;/a&gt; and &lt;a href="https://google.github.io/tensorstore/python/api/index.html"&gt;API documentation&lt;/a&gt; for usage details. For other installation options and for using the C++ API, refer to &lt;a href="https://google.github.io/tensorstore/installation.html"&gt;installation instructions&lt;/a&gt;. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href="https://research.google/people/105450/"&gt;Tim Blakely&lt;/a&gt;, &lt;a href="https://research.google/people/VirenJain/"&gt;Viren Jain&lt;/a&gt;, &lt;a href="https://github.com/yashk2810"&gt;Yash Katariya&lt;/a&gt;, &lt;a href="https://www.jan-matthis.de/"&gt;Jan-Matthis Luckmann&lt;/a&gt;, &lt;a href="https://research.google/people/MichalJanuszewski/"&gt;Michał Januszewski&lt;/a&gt;, &lt;a href="https://research.google/people/PeterLi/"&gt;Peter Li&lt;/a&gt;, &lt;a href="https://research.google/people/104881/"&gt;Adam Roberts&lt;/a&gt;, &lt;a href="https://research.google/people/107327/"&gt;Brain Williams&lt;/a&gt;, and &lt;a href="https://research.google/people/HectorYee/"&gt;Hector Yee&lt;/a&gt; from Google Research, and &lt;a href="https://www.janelia.org/people/davis-bennett"&gt;Davis Bennet&lt;/a&gt;, &lt;a href="https://www.janelia.org/people/stuart-berg"&gt;Stuart Berg&lt;/a&gt;, &lt;a href="https://www.yikes.com/~eric/"&gt;Eric Perlman&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=Pc1TkM0AAAAJ&amp;amp;hl=en"&gt;Stephen Plaza&lt;/a&gt;, and &lt;a href="https://research.monash.edu/en/persons/juan-nunez-iglesias"&gt;Juan Nunez-Iglesias&lt;/a&gt; from the broader scientific community for valuable feedback on the design, early testing and debugging.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/4863623362813999190/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4863623362813999190" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4863623362813999190" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html" rel="alternate" title="TensorStore for High-Performance, Scalable Array Storage" type="text/html"/><author><name>Andrew Helton</name><uri>http://www.blogger.com/profile/12510940840342590054</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX9DAdPcFpMfSPteAKTplWpi2h3okSemN9o1J3B6K45LHBjkp2OiSbNimITgluVBLLyyVwXIhe0tvhwDIG01lWXXPCW-Lh0HqOg6qRBE96ON4ndUBmb4kdDmQ0cMsk2dZcsHkTNU2quF7UMttmX8-GbfMLGmXfKePb1sGIa0qSODsf6oV4dXPZhoo-/s72-c/image1.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6099606360064831644</id><published>2022-09-21T12:27:00.003-07:00</published><updated>2022-10-22T20:05:08.679-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">View Synthesis with Transformers</title><content type="html">&lt;span class="byline-author"&gt;Posted by Carlos Esteves and Ameesh Makadia, Research Scientists, Google Research&lt;/span&gt; &lt;p&gt;A long-standing problem in the intersection of computer vision and computer graphics, &lt;a href="https://en.wikipedia.org/wiki/View_synthesis"&gt;view synthesis&lt;/a&gt; is the task of creating new views of a scene from multiple pictures of that scene. This has received increased attention [&lt;a href="https://dellaert.github.io/NeRF/"&gt;1&lt;/a&gt;, &lt;a href="https://dellaert.github.io/NeRF21/"&gt;2&lt;/a&gt;, &lt;a href="https://dellaert.github.io/NeRF22/"&gt;3&lt;/a&gt;] since &lt;a href="https://www.matthewtancik.com/nerf"&gt;the introduction of neural radiance fields&lt;/a&gt; (NeRF). The problem is challenging because to accurately synthesize new views of a scene, a model needs to capture many types of information — its detailed 3D structure, materials, and illumination —  from a small set of reference images.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In this post, we present recently published deep learning models for view synthesis. In “&lt;a href="https://light-field-neural-rendering.github.io/"&gt;Light Field Neural Rendering&lt;/a&gt;” (LFNR), presented at &lt;a href="https://cvpr2022.thecvf.com/"&gt;CVPR 2022&lt;/a&gt;, we address the challenge of accurately reproducing view-dependent effects by using &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;transformers&lt;/a&gt; that learn to combine reference pixel colors. Then in “&lt;a href="https://mohammedsuhail.net/gen_patch_neural_rendering/"&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt;” (GPNR), to be presented at &lt;a href="https://eccv2022.ecva.net/"&gt;ECCV 2022&lt;/a&gt;, we address the challenge of generalizing to unseen scenes by using a sequence of transformers with canonicalized positional encoding that can be trained on a set of scenes to synthesize views of new scenes. These models have some unique features. They perform image-based rendering, combining colors and features from the reference images to render novel views. They are purely transformer-based, operating on sets of image patches, and they leverage a &lt;a href="https://en.wikipedia.org/wiki/Light_field#The_4D_light_field"&gt;4D light field&lt;/a&gt; representation for positional encoding, which helps to model view-dependent effects. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s960/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="444" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s16000/image6.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6wR1MX9K5D3v-kOvSAzBwV4x7Ex3q_nqTIXgrrspTcgez4gGqdeR5MeQYZn2h6FX8WPet_zxok41c4qEtTnFvZzOpufJcO8PZARqeRpBWxoTpTuOmgTPtiefzceefdvu52VxRC9AxUtlxbQEMnxUa_IS4raPaXH8uyB3APcTiiWKH2cEinApucd2Y4A/s504/image5.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="288" data-original-width="504" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6wR1MX9K5D3v-kOvSAzBwV4x7Ex3q_nqTIXgrrspTcgez4gGqdeR5MeQYZn2h6FX8WPet_zxok41c4qEtTnFvZzOpufJcO8PZARqeRpBWxoTpTuOmgTPtiefzceefdvu52VxRC9AxUtlxbQEMnxUa_IS4raPaXH8uyB3APcTiiWKH2cEinApucd2Y4A/s16000/image5.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;We train deep learning models that are able to produce new views of a scene given a few images of it. These models are particularly effective when handling view-dependent effects like the refractions and translucency on the test tubes. This animation is compressed; see the original-quality renderings &lt;a href="https://light-field-neural-rendering.github.io/static/videos/shiny_lab.mp4"&gt;here&lt;/a&gt;. Source: Lab scene from the &lt;a href="https://nex-mpi.github.io/"&gt;NeX/Shiny&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Overview&lt;/h2&gt;&lt;p&gt;The input to the models consists of a set of reference images and their camera parameters (focal length, position, and orientation in space), along with the coordinates of the target &lt;a href="https://en.wikipedia.org/wiki/Line_(geometry)#Ray"&gt;ray&lt;/a&gt; whose color we want to determine. To produce a new image, we start from the camera parameters of the input images, obtain the coordinates of the target rays (each corresponding to a pixel), and query the model for each.  &lt;/p&gt;&lt;p&gt;Instead of processing each reference image completely, we look only at the regions that are likely to influence the target pixel. These regions are determined via &lt;a href="https://en.wikipedia.org/wiki/Epipolar_geometry"&gt;epipolar geometry&lt;/a&gt;, which maps each target pixel to a line on each reference frame. For robustness, we take small regions around a number of points on the epipolar line, resulting in the set of patches that will actually be processed by the model. The transformers then act on this set of patches to obtain the color of the target pixel.  &lt;/p&gt;&lt;p&gt;Transformers are especially useful in this setting since their self-attention mechanism naturally takes sets as inputs, and the attention weights themselves can be used to combine reference view colors and features to predict the output pixel colors. These transformers follow the architecture introduced in &lt;a href="https://github.com/google-research/vision_transformer"&gt;ViT&lt;/a&gt;. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjGFUDAhjY-yJytdfcPFABdtwKgq_BwD7rSib6T_3Y6IY8jOm6S7ZLOdr_9gX8W7oj6bFKBvL8H7h9QVCJ1Kdw75UPv2Ekubh-TpiGhqOCf7e4fXuC0sQscp8KNn6hCuHNFd7e1ih-OpyXp2mf338pS-h2Qw5zqRIo4ZJYvgrVJ4PLC-umaEVDgIUZVg/s1050/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="417" data-original-width="1050" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjGFUDAhjY-yJytdfcPFABdtwKgq_BwD7rSib6T_3Y6IY8jOm6S7ZLOdr_9gX8W7oj6bFKBvL8H7h9QVCJ1Kdw75UPv2Ekubh-TpiGhqOCf7e4fXuC0sQscp8KNn6hCuHNFd7e1ih-OpyXp2mf338pS-h2Qw5zqRIo4ZJYvgrVJ4PLC-umaEVDgIUZVg/s16000/image4.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;To predict the color of one pixel, the models take a set of patches extracted around the epipolar line of each reference view. Image source: &lt;a href="https://bmild.github.io/llff/index.html"&gt;LLFF&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Light Field Neural Rendering&lt;/h2&gt;&lt;p&gt;In &lt;a href="https://light-field-neural-rendering.github.io/"&gt;Light Field Neural Rendering&lt;/a&gt; (LFNR), we use a sequence of two transformers to map the set of patches to the target pixel color. The first transformer aggregates information along each epipolar line, and the second along each reference image. We can interpret the first transformer as finding potential correspondences of the target pixel on each reference frame, and the second as reasoning about occlusion and view-dependent effects, which are common challenges of image-based rendering. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6stcn8aiJ2QXdr4PLJo68S11D5ycKYVf9CJ9yOtjoZbAcl5crn_7SmKNOl3f_zsa04pXF3Q8VXyGaS6vVk8ZmHn1-gWjOaLlJ8MlS6wnZm58sSee-uyCTzN6t2gjG0roaoWBbPIdDC8tuSGBD9zMKNVpKq04-iRSVfslAz2HD7RXrqttKM8Chf28zSw/s1999/image8.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="584" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6stcn8aiJ2QXdr4PLJo68S11D5ycKYVf9CJ9yOtjoZbAcl5crn_7SmKNOl3f_zsa04pXF3Q8VXyGaS6vVk8ZmHn1-gWjOaLlJ8MlS6wnZm58sSee-uyCTzN6t2gjG0roaoWBbPIdDC8tuSGBD9zMKNVpKq04-iRSVfslAz2HD7RXrqttKM8Chf28zSw/s16000/image8.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;LFNR uses a sequence of two transformers to map a set of patches extracted along epipolar lines to the target pixel color.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;LFNR improved the state-of-the-art on the most popular view synthesis benchmarks (Blender and Real Forward-Facing scenes from &lt;a href="https://www.matthewtancik.com/nerf"&gt;NeRF&lt;/a&gt; and Shiny from &lt;a href="https://nex-mpi.github.io/"&gt;NeX&lt;/a&gt;) with margins as large as 5dB &lt;a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"&gt;peak signal-to-noise ratio&lt;/a&gt; (PSNR). This corresponds to a reduction of the pixel-wise error by a factor of 1.8x. We show qualitative results on challenging scenes from the &lt;a href="https://nex-mpi.github.io/"&gt;Shiny&lt;/a&gt; dataset below: &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJ4SRa5sepHJhmbySIqPmalNVkIBwrGTxaoIPVFiOXt_muv4Q8ZDjd8fnnTaEw0iqAu2pb4HzPKFElNytafFAdXlCdFtaDq2WK3fbXrYbKK7YxAGKZcVx6AwUTsJgFxWlC95SQ-v6FowCxYsI-pK34zq-xqq8xgokz1K1jAKl4JNeGvp4sBG2PVOazfw/s504/image9.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="288" data-original-width="504" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJ4SRa5sepHJhmbySIqPmalNVkIBwrGTxaoIPVFiOXt_muv4Q8ZDjd8fnnTaEw0iqAu2pb4HzPKFElNytafFAdXlCdFtaDq2WK3fbXrYbKK7YxAGKZcVx6AwUTsJgFxWlC95SQ-v6FowCxYsI-pK34zq-xqq8xgokz1K1jAKl4JNeGvp4sBG2PVOazfw/s16000/image9.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;LFNR reproduces challenging view-dependent effects like the rainbow and reflections on the CD, reflections, refractions and translucency on the bottles. This animation is compressed; see the original quality renderings &lt;a href="https://light-field-neural-rendering.github.io/static/videos/shiny_cd.mp4"&gt;here&lt;/a&gt;. Source: CD scene from the &lt;a href="https://nex-mpi.github.io/"&gt;NeX/Shiny&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheYJQqMchzPTvb4mtom8VmoYtE91IB7Q47qygzupMAHvhp5lDyUEsyKlTx5567vlT32M4EV2JiZ9ERt_A-InkESptwHeteVcapgSjoPC8dTKpGtURFvk51UxjfiI_q5fdBGVNPBB1Kl2i333JU-nBLjJhsuij9q7c0saaXN_DGTbkbYL_3xbypGTQ3tA/s1402/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="422" data-original-width="1402" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheYJQqMchzPTvb4mtom8VmoYtE91IB7Q47qygzupMAHvhp5lDyUEsyKlTx5567vlT32M4EV2JiZ9ERt_A-InkESptwHeteVcapgSjoPC8dTKpGtURFvk51UxjfiI_q5fdBGVNPBB1Kl2i333JU-nBLjJhsuij9q7c0saaXN_DGTbkbYL_3xbypGTQ3tA/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Prior methods such as &lt;a href="https://nex-mpi.github.io/"&gt;NeX&lt;/a&gt; and &lt;a href="https://www.matthewtancik.com/nerf"&gt;NeRF&lt;/a&gt; fail to reproduce view-dependent effects like the translucency and refractions in the test tubes on the Lab scene from the &lt;a href="https://nex-mpi.github.io/"&gt;NeX/Shiny&lt;/a&gt; dataset. See also our video of this scene at the top of the post and the original quality outputs &lt;a href="https://light-field-neural-rendering.github.io/static/videos/shiny_lab.mp4"&gt;here&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Generalizing to New Scenes&lt;/h2&gt;&lt;p&gt;One limitation of LFNR is that the first transformer collapses the information along each epipolar line independently for each reference image. This means that it decides which information to preserve based only on the output ray coordinates and patches from each reference image, which works well when training on a single scene (as most neural rendering methods do), but it does not generalize across scenes. Generalizable methods are important because they can be applied to new scenes without needing to retrain. &lt;/p&gt;&lt;p&gt;We overcome this limitation of LFNR in &lt;a href="https://mohammedsuhail.net/gen_patch_neural_rendering/"&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt; (GPNR). We add a transformer that runs before the other two and exchanges information between points at the same depth over all reference images. For example, this first transformer looks at the columns of the patches from the park bench shown above and can use cues like the flower that appears at corresponding depths in two views, which indicates a potential match. Another key idea of this work is to canonicalize the positional encoding based on the target ray, because to generalize across scenes, it is necessary to represent quantities in relative and not absolute frames of reference. The animation below shows an overview of the model.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl_5a5VpgxxOOSrFjTOv4IIFMgSTCnzxda-l9XVzac4sXBWdIUrmi921QTlj0ZQggGxNW7kpYbFulLie1Hy7umjVmaxQoi9YtZnIPL_XnFqvQF9OU_wT7WyYyJLcNCbYmTu1TEZe7D5dzAOb_WFpz2KIoENs_Wv_loRm6LK68mHm-TNO76JsrXJeydlQ/s960/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="540" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgl_5a5VpgxxOOSrFjTOv4IIFMgSTCnzxda-l9XVzac4sXBWdIUrmi921QTlj0ZQggGxNW7kpYbFulLie1Hy7umjVmaxQoi9YtZnIPL_XnFqvQF9OU_wT7WyYyJLcNCbYmTu1TEZe7D5dzAOb_WFpz2KIoENs_Wv_loRm6LK68mHm-TNO76JsrXJeydlQ/s16000/image3.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;GPNR consists of a sequence of three transformers that map a set of patches extracted along epipolar lines to a pixel color. Image patches are mapped via the linear projection layer to initial features (shown as blue and green boxes). Then those features are successively refined and aggregated by the model, resulting in the final feature/color represented by the gray rectangle. Park bench image source: &lt;a href="https://bmild.github.io/llff/index.html"&gt;LLFF&lt;/a&gt; dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;To evaluate the generalization performance, we train GPNR on a set of scenes and test it on new scenes. GPNR improved the state-of-the-art on several benchmarks (following &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt; and &lt;a href="https://apchenstu.github.io/mvsnerf/"&gt;MVSNeRF&lt;/a&gt; protocols) by 0.5–1.0 dB on average. On the &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt; benchmark, GPNR outperforms the baselines while using only 11% of the training scenes. The results below show new views of unseen scenes rendered with no fine-tuning.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9puTw4NAnf8DUJcCCv2LSvAQu6kmIMsOK-bkCSKtDV0WeqKpwoJmvXolP1ibzHQ8u_VqVVikaOz1S2Z2PBO5SpLowKSRdDvxf3_QN1JD1pTHLXnGcepNeflOvsofeLMGWoF3iIMfupMcDvplslsRU1G-JrHGZHLSgbRTbDwq5ZGZpRGLx27jFqB1z8Q/s576/image2.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="437" data-original-width="576" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9puTw4NAnf8DUJcCCv2LSvAQu6kmIMsOK-bkCSKtDV0WeqKpwoJmvXolP1ibzHQ8u_VqVVikaOz1S2Z2PBO5SpLowKSRdDvxf3_QN1JD1pTHLXnGcepNeflOvsofeLMGWoF3iIMfupMcDvplslsRU1G-JrHGZHLSgbRTbDwq5ZGZpRGLx27jFqB1z8Q/s16000/image2.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;GPNR-generated views of held-out scenes, without any fine tuning. This animation is compressed; see the original quality renderings &lt;a href="https://mohammedsuhail.net/gen_patch_neural_rendering/img/combined_results.mp4"&gt;here&lt;/a&gt;. Source: &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt; collected dataset. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKN1tnJDaafkhry535hVj4XZr4QVgcwebkox-YLImsrYSBcharQlzjgpp81SGwTTprQR6t4W2rJa4AvuRixFk5MYQavaXGmI0xBgQjKOOfJ1bIon1xEfzyjrt8FA5NL14pGU5o5M61XNvBHxIBoiL2jBdh9t1MMnHqcYTU1DocsH6HACg0TOnuktUp2Q/s1772/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="363" data-original-width="1772" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKN1tnJDaafkhry535hVj4XZr4QVgcwebkox-YLImsrYSBcharQlzjgpp81SGwTTprQR6t4W2rJa4AvuRixFk5MYQavaXGmI0xBgQjKOOfJ1bIon1xEfzyjrt8FA5NL14pGU5o5M61XNvBHxIBoiL2jBdh9t1MMnHqcYTU1DocsH6HACg0TOnuktUp2Q/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Details of GPNR-generated views on held-out scenes from &lt;a href="https://nex-mpi.github.io/"&gt;NeX/Shiny&lt;/a&gt; (&lt;b&gt;left&lt;/b&gt;) and &lt;a href="https://bmild.github.io/llff/index.html"&gt;LLFF&lt;/a&gt; (&lt;b&gt;right&lt;/b&gt;), without any fine tuning. GPNR reproduces more accurately the details on the leaf and the refractions through the lens when compared against &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Future Work&lt;/h2&gt;&lt;p&gt;One limitation of most neural rendering methods, including ours, is that they require camera poses for each input image. Poses are not easy to obtain and typically come from offline optimization methods that can be slow, limiting possible applications, such as those on mobile devices. Research on jointly learning view synthesis and input poses is a promising future direction. Another limitation of our models is that they are computationally expensive to train. There is an active line of research on faster transformers which might help improve our models’ efficiency. For the papers, more results, and open-source code, you can check out the projects pages for "&lt;a href="https://light-field-neural-rendering.github.io/"&gt;Light Field Neural Rendering&lt;/a&gt;" and "&lt;a href="https://mohammedsuhail.net/gen_patch_neural_rendering/"&gt;Generalizable Patch-Based Neural Rendering&lt;/a&gt;". &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Potential Misuse&lt;/h2&gt;&lt;p&gt;In our research, we aim to accurately reproduce an existing scene using images from that scene, so there is little room to generate fake or non-existing scenes. Our models assume static scenes, so synthesizing moving objects, such as people, will not work. &lt;/p&gt;&lt;div style="line-height:40%;"&gt;    &lt;br&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;&lt;em&gt;All the hard work was done by our amazing intern – &lt;a href="https://mohammedsuhail.net/"&gt;Mohammed Suhail&lt;/a&gt; – a PhD student at UBC, in collaboration with &lt;a href="https://machc.github.io/"&gt;Carlos Esteves&lt;/a&gt; and &lt;a href="http://www.ameeshmakadia.com/"&gt;Ameesh Makadia&lt;/a&gt; from Google Research, and &lt;a href="https://www.cs.ubc.ca/~lsigal/"&gt;Leonid Sigal&lt;/a&gt; from UBC. We are thankful to Corinna Cortes for supporting and encouraging this project.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Our work is inspired by &lt;a href="https://www.matthewtancik.com/nerf"&gt;NeRF&lt;/a&gt;, which sparked the recent interest in view synthesis, and &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt;, which first considered generalization to new scenes. Our light ray positional encoding is inspired by the seminal paper &lt;a href="https://graphics.stanford.edu/papers/light/"&gt;Light Field Rendering&lt;/a&gt; and our use of transformers follow &lt;a href="https://github.com/google-research/vision_transformer"&gt;ViT&lt;/a&gt;.  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Video results are from scenes from &lt;a href="https://bmild.github.io/llff/index.html"&gt;LLFF&lt;/a&gt;, &lt;a href="https://nex-mpi.github.io/"&gt;Shiny&lt;/a&gt;, and &lt;a href="https://ibrnet.github.io/"&gt;IBRNet&lt;/a&gt; collected datasets.&lt;/em&gt;&lt;/p&gt; </content><link href="http://ai.googleblog.com/feeds/6099606360064831644/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6099606360064831644" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6099606360064831644" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html" rel="alternate" title="View Synthesis with Transformers" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkJzGICK0DKkQBjC-O0ts_iqk-W3qo90RcUbJl9xOA4NIpzqRHiDx1C8C3KV0ylyBR3cPwgvoW1I9s3y2OW1VdGVArKtWDaIDaxmLFSRoOo66YcMVA9Df7Tj9_zaR2fZplryCBphBKgTtDRegu6POYDiyMy_waqgL3_37KTFeMGjCD7R-U7jT0boYong/s72-c/image6.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-6639900871004963241</id><published>2022-09-20T10:00:00.009-07:00</published><updated>2022-10-21T06:31:17.420-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Google Brain"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">FindIt: Generalized Object Localization with Natural Language Queries</title><content type="html">&lt;span class="byline-author"&gt;Posted by Weicheng Kuo and Anelia Angelova, Research Scientists, Google Research, Brain Team&lt;/span&gt; &lt;img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s1600/image4.gif" style="display: none;" /&gt; &lt;p&gt;Natural language enables flexible descriptive queries about images. The interaction between text queries and images grounds linguistic meaning in the visual world, facilitating a better understanding of object relationships, human intentions towards objects, and interactions with the environment. The research community has studied object-level visual grounding through a range of tasks, including &lt;a href="https://paperswithcode.com/task/referring-expression-comprehension"&gt;referring expression comprehension&lt;/a&gt;, &lt;a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf"&gt;text-based localization&lt;/a&gt;, and more broadly &lt;a href="https://paperswithcode.com/task/object-detection"&gt;object detection&lt;/a&gt;, each of which require different skills in a model. For example, object detection seeks to find all objects from a predefined set of classes, which requires accurate localization and classification, while referring expression comprehension localizes an object from a referring text and often requires complex reasoning on prominent objects. At the intersection of the two is text-based localization, in which a simple category-based text query prompts the model to detect the objects of interest. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;Due to their dissimilar task properties, referring expression comprehension, detection, and text-based localization are mostly studied through separate benchmarks with most models only dedicated to one task. As a result, existing models have not adequately synthesized information from the three tasks to achieve a more holistic visual and linguistic understanding. Referring expression comprehension models, for instance, are trained to predict one object per image, and often struggle to localize multiple objects, reject negative queries, or detect novel categories. In addition, detection models are unable to process text inputs, and text-based localization models often struggle to process complex queries that refer to one object instance, such as “Left half sandwich.” Lastly, none of the models can generalize sufficiently well beyond their training data and categories. &lt;/p&gt;&lt;p&gt;To address these limitations, we are presenting “&lt;a href="https://arxiv.org/abs/2203.17273"&gt;FindIt: Generalized Localization with Natural Language Queries&lt;/a&gt;” at &lt;a href="https://eccv2022.ecva.net/"&gt;ECCV 2022&lt;/a&gt;. Here we propose a unified, general-purpose and multitask visual grounding model, called FindIt, that can flexibly answer different types of grounding and detection queries. Key to this architecture is a multi-level cross-modality fusion module that can perform complex reasoning for referring expression comprehension and simultaneously recognize small and challenging objects for text-based localization and detection. In addition, we discover that a standard object detector and detection losses are sufficient and surprisingly effective for all three tasks without the need for task-specific design and losses common in existing works. FindIt is simple, efficient, and outperforms alternative state-of-the-art models on the referring expression comprehension and text-based localization benchmarks, while being competitive on the detection benchmark. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPIIB2rIV0acedyIsoK8cqvf_ZNc6ZY55KXI_L6Cv8LuDHY3OZTJ3d4MQlMq4WCGu73Cp41NOM8J07yCMEHTrUViWdCqdntnhKw86c34datYAJZTaNrbxaof2xw5GAdWhXX2bX6xd5avmM7nH1acHnsCp8zxRJe11yzwuR5VViFiAUrmwwPqFV-W8TyQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1085" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjPIIB2rIV0acedyIsoK8cqvf_ZNc6ZY55KXI_L6Cv8LuDHY3OZTJ3d4MQlMq4WCGu73Cp41NOM8J07yCMEHTrUViWdCqdntnhKw86c34datYAJZTaNrbxaof2xw5GAdWhXX2bX6xd5avmM7nH1acHnsCp8zxRJe11yzwuR5VViFiAUrmwwPqFV-W8TyQ/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FindIt is a unified model for referring expression comprehension (col. 1), text-based localization (col. 2), and the object detection task (col. 3). FindIt can respond accurately when tested on object types/classes not known during training, e.g. “Find the desk” (col. 4). Compared to existing baselines (&lt;a href="https://arxiv.org/abs/1801.08186"&gt;MattNet&lt;/a&gt; and &lt;a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf"&gt;GPV&lt;/a&gt;), FindIt can perform these tasks well and in a single model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Multi-level Image-Text Fusion&lt;/b&gt;&lt;br /&gt;Different localization tasks are created with different semantic understanding objectives. For example, because the referring expression task primarily references prominent objects in the image rather than small, occluded or faraway objects, low resolution images generally suffice. In contrast, the detection task aims to detect objects with various sizes and occlusion levels in higher resolution images. Apart from these benchmarks, the general visual grounding problem is inherently multiscale, as natural queries can refer to objects of any size. This motivates the need for a multi-level image-text fusion model for efficient processing of higher resolution images over different localization tasks.  &lt;/p&gt;&lt;p&gt;The premise of FindIt is to fuse the higher level semantic features using more expressive &lt;a href="https://arxiv.org/abs/1706.03762v5"&gt;transformer&lt;/a&gt; layers, which can capture all-pair interactions between image and text. For the lower-level and higher-resolution features, we use a cheaper &lt;a href="https://en.wikipedia.org/wiki/Dot_product"&gt;dot-product&lt;/a&gt; fusion to save computation and memory cost. We attach a detector head (e.g., &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN&lt;/a&gt;) on top of the fused feature maps to predict the boxes and their classes. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s1600/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="575" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s16000/image4.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FindIt accepts an image and a query text as inputs, and processes them separately in image/text backbones before applying the multi-level fusion. We feed the fused features to Faster R-CNN to predict the boxes referred to by the text. The feature fusion uses more expressive &lt;a href="https://arxiv.org/abs/1706.03762v5"&gt;transformers&lt;/a&gt; at higher levels and cheaper dot-product at the lower levels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Multitask Learning&lt;/b&gt;&lt;br /&gt;Apart from the multi-level fusion described above, we adapt the text-based localization and detection tasks to take the same inputs as the referring expression comprehension task. For the text-based localization task, we generate a set of queries over the categories present in the image. For any present category, the text query takes the form “Find the [&lt;code&gt;object&lt;/code&gt;],” where [&lt;code&gt;object&lt;/code&gt;] is the category name. The objects corresponding to that category are labeled as foreground and the other objects as background. Instead of using the aforementioned prompt, we use a static prompt for the detection task, such as “Find all the objects.”. We found that the specific choice of prompts is not important for text-based localization and detection tasks.  &lt;/p&gt;&lt;p&gt;After adaptation, all tasks in consideration share the same inputs and outputs — an image input, a text query, and a set of output bounding boxes and classes. We then combine the datasets and train on the mixture. Finally, we use the standard object detection losses for all tasks, which we found to be surprisingly simple and effective. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Evaluation&lt;/b&gt;&lt;br /&gt;We apply FindIt to the popular &lt;a href="https://paperswithcode.com/dataset/refcoco"&gt;RefCOCO&lt;/a&gt; benchmark for referring expression comprehension tasks. When only the &lt;a href="https://cocodataset.org/#home"&gt;COCO&lt;/a&gt; and RefCOCO dataset is available, FindIt outperforms the state-of-the-art-model on all tasks. In the settings where external datasets are allowed, FindIt sets a new state of the art by using COCO and all RefCOCO splits together (no other datasets). On the challenging &lt;a href="https://github.com/lichengunc/refer"&gt;Google and UMD splits&lt;/a&gt;, FindIt outperforms the state of the art by a 10% margin, which, taken together, demonstrate the benefits of multitask learning. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUY4nOj2imEyW4rV-mBEfdA4v5W_MIJ_fDr9pxjXaOolm_YcYqsoqKj9GsHepuXyZxfuxNQk6re_SCfX6qmVg4RvNUsxsb3cRvFaJWLWttP06RYfifP_-wPV20_Y2GiNAsKW4jeGX6VGQWTkK_t6d-zhFoHsJ6h6GxGkXU6IakldIAtD6Wu1UNG29kdg/s1324/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="886" data-original-width="1324" height="428" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUY4nOj2imEyW4rV-mBEfdA4v5W_MIJ_fDr9pxjXaOolm_YcYqsoqKj9GsHepuXyZxfuxNQk6re_SCfX6qmVg4RvNUsxsb3cRvFaJWLWttP06RYfifP_-wPV20_Y2GiNAsKW4jeGX6VGQWTkK_t6d-zhFoHsJ6h6GxGkXU6IakldIAtD6Wu1UNG29kdg/w640-h428/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison with the state of the art on the popular referring expression benchmark. FindIt is superior on both the &lt;a href="https://cocodataset.org/#home"&gt;COCO&lt;/a&gt; and unconstrained settings (additional training data allowed).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;On the text-based localization benchmark, FindIt achieves 79.7%, higher than the &lt;a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_Towards_General_Purpose_Vision_Systems_An_End-to-End_Task-Agnostic_Vision-Language_Architecture_CVPR_2022_paper.pdf"&gt;GPV&lt;/a&gt; (73.0%), and &lt;a href="https://arxiv.org/abs/1506.01497"&gt;Faster R-CNN&lt;/a&gt; baselines (75.2%). Please refer to the &lt;a href="https://arxiv.org/abs/2203.17273"&gt;paper&lt;/a&gt; for more quantitative evaluation. &lt;/p&gt;&lt;p&gt;We further observe that FindIt generalizes better to novel categories and super-categories in the text-based localization task compared to competitive single-task baselines on the popular &lt;a href="https://cocodataset.org/#home"&gt;COCO&lt;/a&gt; and &lt;a href="https://www.objects365.org/overview.html"&gt;Objects365&lt;/a&gt; datasets, shown in the figure below. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr1Z0xl-mLE30eKpa69jWj6_BYASXXd6O0p_kpfwDhTAndm4mnER3QS37UkWG1mQTdyfo215knHHLVtw89EKrPYBVPNCVVhruF-n7MSNHvLPJv4y_GDnxz0XTnLJdGjIiSh0ymMcogMRpx1M0Nj4jBCRpYmPgej2USGOfGjB6dcqY6LQYTRFFw3TByTA/s1375/Screenshot%202022-09-20%209.40.47%20AM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="411" data-original-width="1375" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgr1Z0xl-mLE30eKpa69jWj6_BYASXXd6O0p_kpfwDhTAndm4mnER3QS37UkWG1mQTdyfo215knHHLVtw89EKrPYBVPNCVVhruF-n7MSNHvLPJv4y_GDnxz0XTnLJdGjIiSh0ymMcogMRpx1M0Nj4jBCRpYmPgej2USGOfGjB6dcqY6LQYTRFFw3TByTA/s16000/Screenshot%202022-09-20%209.40.47%20AM.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;FindIt on novel and super categories. &lt;b&gt;Left:&lt;/b&gt; FindIt outperforms the single-task baselines especially on the novel categories. &lt;b&gt;Right:&lt;/b&gt; FindIt outperforms the single-task baselines on the unseen super categories. “Rec-Single” is the Referring expression comprehension single task model and “Loc-Single” is the text-based localization single task model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Efficiency&lt;/b&gt;&lt;br /&gt;We also benchmark the inference times on the referring expression comprehension task (see Table below). FindIt is efficient and comparable with existing one-stage approaches while achieving higher accuracy. For fair comparison, all running times are measured on one &lt;a href="https://www.nvidia.com/en-us/geforce/10-series/"&gt;GTX 1080Ti&lt;/a&gt; GPU. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;  &lt;td style="text-align: left;"&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;b&gt;Image Size&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;b&gt;Backbone&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;b&gt;Runtime (ms)&lt;/b&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://arxiv.org/abs/1801.08186"&gt;MattNet&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;1000&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;a href="https://arxiv.org/abs/1512.03385"&gt;R101&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;378&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.pdf"&gt;FAOA&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;256&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;a href="https://arxiv.org/abs/1804.02767"&gt;DarkNet53&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;39&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://arxiv.org/abs/2003.08813"&gt;MCN&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;416&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;a href="https://arxiv.org/abs/1804.02767"&gt;DarkNet53&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;56&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://arxiv.org/abs/2104.08541"&gt;TransVG&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;640&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;a href="https://arxiv.org/abs/1512.03385"&gt;R50&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;62&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;FindIt (Ours)&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;640&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;R50&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;107&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;FindIt (Ours)&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;384&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;R50&lt;/td&gt;&lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td style="text-align: center;"&gt;57&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We present Findit, which unifies referring expression comprehension, text-based localization, and object detection tasks. We propose multi-scale cross-attention to unify the diverse localization requirements of these tasks. Without any task-specific design, FindIt surpasses the state of the art on referring expression and text-based localization, shows competitive performance on detection, and generalizes better to out-of-distribution data and novel classes. All of these are accomplished in a single, unified, and efficient model. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work is conducted by Weicheng Kuo, Fred Bertsch, Wei Li, AJ Piergiovanni, Mohammad Saffar, and Anelia Angelova. We would like to thank Ashish Vaswani, Prajit Ramachandran, Niki Parmar, David Luan, Tsung-Yi Lin, and other colleagues at Google Research for their advice and helpful discussions. We would like to thank Tom Small for preparing the animation.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/6639900871004963241/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/findit-generalized-object-localization.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6639900871004963241" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6639900871004963241" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/findit-generalized-object-localization.html" rel="alternate" title="FindIt: Generalized Object Localization with Natural Language Queries" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRaPsdl4AdqQprPVoN9xRVQpw4oyc4e9wXl3QDbM6hu9u3K1o1A0nwVNxA-JnCIg2YyMc1NVYPLCkjqRQUHInynLpgdo5E13kdffkV9XyatQpnZl7xR_5CXSW-uSX7r4H934VAgGHpuAFDOmQf-rcUdnDVMquinbeP6emaXqg7cu9aiGHbXLXQ5wLOoA/s72-c/image4.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5836135189460527312</id><published>2022-09-17T19:00:00.003-07:00</published><updated>2022-10-21T06:31:19.010-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="conference"/><category scheme="http://www.blogger.com/atom/ns#" term="Interspeech"/><category scheme="http://www.blogger.com/atom/ns#" term="Speech"/><title type="text">Google at Interspeech 2022</title><content type="html">&lt;span class="byline-author"&gt;Posted by Cat Armato, Program Manager, Google&lt;/span&gt; &lt;p&gt;This week, the 23rd Annual &lt;a href="https://www.interspeech2022.org/"&gt;Conference of the International Speech Communication Association&lt;/a&gt; (INTERSPEECH 2022) is being held in Incheon, South Korea, representing one of the world’s most extensive conferences on research and technology of spoken language understanding and processing. Over 2,000 experts in speech-related research fields gather to take part in oral presentations and poster sessions and to collaborate with streamed events across the globe. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;We are excited to be a Diamond Sponsor of INTERSPEECH 2022, where we will be showcasing nearly 50 research publications and supporting a number of workshops, special sessions and tutorials. We welcome in-person attendees to drop by the Google booth to meet our researchers and participate in Q&amp;amp;As and demonstrations of some of our latest speech technologies, which help to improve accessibility and provide convenience in communication for billions of users. In addition, online attendees are encouraged to visit our virtual booth in &lt;a href="https://spotvirtual.com/invite/lifeskilz-69dZxjOOvd"&gt;GatherTown&lt;/a&gt; where you can get up-to-date information on research and opportunities at Google. You can also learn more about the Google research being presented at INTERSPEECH 2022 below (Google affiliations in &lt;b&gt;bold&lt;/b&gt;). &lt;/p&gt;&lt;br /&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration: underline;"&gt;Organizing Committee&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Industry Liaisons include: &lt;b&gt;&lt;i&gt;Bhuvana Ramabahdran&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Area Chairs include: &lt;i&gt;&lt;b&gt;John Hershey&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, &lt;b&gt;Shrikanth Narayanan&lt;/b&gt;, &lt;b&gt;Bastiaan Kleijn&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration: underline;"&gt;ISCA Fellows&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Include: &lt;i&gt;&lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration: underline;"&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.06322.pdf"&gt;Production Federated Keyword Spotting via Distillation, Filtering, and Joint Federated-Centralized Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Andrew Hard&lt;/b&gt;, &lt;b&gt;Kurt Partridge&lt;/b&gt;, &lt;b&gt;Neng Chen&lt;/b&gt;, &lt;b&gt;Sean Augenstein&lt;/b&gt;, &lt;b&gt;Aishanee Shah&lt;/b&gt;, &lt;b&gt;Hyun Jin Park&lt;/b&gt;, &lt;b&gt;Alex Park&lt;/b&gt;, &lt;b&gt;Sara Ng&lt;/b&gt;, &lt;b&gt;Jessica Nguyen&lt;/b&gt;, &lt;b&gt;Ignacio Lopez Moreno&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Françoise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.13339.pdf"&gt;Leveraging Unsupervised and Weakly-Supervised Data to Improve Direct Speech-to-Speech Translation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Yifan Ding&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Nobu Morioka&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.05008.pdf"&gt;Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Ruoming Pang&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Shankar Kumar&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.00706.pdf"&gt;UserLibri: A Dataset for ASR Personalization Using Only Text&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Theresa Breiner&lt;/b&gt;, &lt;b&gt;Swaroop Ramaswamy&lt;/b&gt;, &lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Shefali Garg&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Kilol Gupta&lt;/b&gt;, &lt;b&gt;Mingqing Chen&lt;/b&gt;, &lt;b&gt;Lara McConnaughey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.00764.pdf"&gt;SNRi Target Training for Joint Speech Enhancement and Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Yuma Koizumi&lt;/b&gt;, &lt;b&gt;Shigeki Karita&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Sankaran Panchapagesan&lt;/b&gt;, &lt;b&gt;Michiel Bacchiani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.13321.pdf"&gt;Turn-Taking Prediction for Natural Conversational Speech&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Chao Zhang&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.13322.pdf"&gt;Streaming Intended Query Detection Using E2E Modeling for Continued Conversation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Guru Prakash&lt;/b&gt;, &lt;b&gt;Zelin Wu&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Adam Stambler&lt;/b&gt;, &lt;b&gt;Shyam Upadhyay&lt;/b&gt;, &lt;b&gt;Manaal Faruqui&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.16104.pdf"&gt;Improving Distortion Robustness of Self-Supervised Speech Processing Tasks with Domain Adaptation&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Kuan Po Huang, Yu-Kuan Fu,&lt;b&gt; Yu Zhang&lt;/b&gt;, Hung-yi Lee &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2111.09296.pdf"&gt;XLS-R: Self-Supervised Cross-Lingual Speech Representation Learning at Scale&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, &lt;b&gt;Alexis Conneau&lt;/b&gt;, Michael Auli &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.08345.pdf"&gt;Extracting Targeted Training Data from ASR Models, and How to Mitigate It&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ehsan Amid&lt;/b&gt;, &lt;b&gt;Om Thakkar&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;, &lt;b&gt;Françoise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2204.09606"&gt;Detecting Unintended Memorization in Language-Model-Fused ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Steve Chien&lt;/b&gt;, &lt;b&gt;Om Thakkar&lt;/b&gt;, &lt;b&gt;Rajiv Mathews&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2206.07684.pdf"&gt;AVATAR: Unconstrained Audiovisual Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Valentin Gabeur&lt;/b&gt;, &lt;b&gt;Paul Hongsuck Seo&lt;/b&gt;, &lt;b&gt;Arsha Nagrani&lt;/b&gt;, &lt;b&gt;Chen Sun&lt;/b&gt;, Karteek Alahari, &lt;b&gt;Cordelia Schmid&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.00652.pdf"&gt;End-to-End Multi-talker Audio-Visual ASR Using an Active Speaker Attention Module&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Richard Rose&lt;/b&gt;, &lt;b&gt;Olivier Siohan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2201.10439.pdf"&gt;Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Multi-person Video&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Dmitriy Serdyuk&lt;/b&gt;, &lt;b&gt;Otavio Braga&lt;/b&gt;, &lt;b&gt;Olivier Siohan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.01981.pdf"&gt;Unsupervised Data Selection via Discrete Speech Representation for ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zhiyun Lu&lt;/b&gt;, &lt;b&gt;Yongqiang Wang&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Wei Han&lt;/b&gt;, &lt;b&gt;Zhehuai Chen&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2209.06987"&gt;Non-parallel Voice Conversion for ASR Augmentation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Gary Wang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Fadi Biadsy&lt;/b&gt;, &lt;b&gt;Jesse Emond&lt;/b&gt;, &lt;b&gt;Yinghui Huang&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.02262.pdf"&gt;Ultra-Low-Bitrate Speech Coding with Pre-trained Transformers&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Ali Siahkoohi, &lt;b&gt;Michael Chinen&lt;/b&gt;, &lt;b&gt;Tom Denton&lt;/b&gt;, &lt;b&gt;W. Bastiaan Kleijn&lt;/b&gt;, &lt;b&gt;Jan Skoglund&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1EOLTniiA30yenTHyM3dgXcL2whthz1hc/view?usp=sharing&amp;amp;resourcekey=0-xmtb_udWK0467lOOMA42IA"&gt;Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Chao Zhang&lt;/b&gt;, &lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2206.14716.pdf"&gt;Improving Deliberation by Text-Only and Semi-supervised Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ke Hu, Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Weiran Wang&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.10749.pdf"&gt;E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Shuo-yiin Chang&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Tara N. Sainath&lt;/b&gt;, &lt;b&gt;Cyril Allauzen&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Zhiyun Lu&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.15652.pdf"&gt;CycleGAN-Based Unpaired Speech Dereverberation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Min Ma&lt;/b&gt;, Patrick von Platen, Anton Lozhkov, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Clara Rivera&lt;/b&gt;, &lt;b&gt;Mihir Kale&lt;/b&gt;, &lt;b&gt;Daan van Esch&lt;/b&gt;, &lt;b&gt;Vera Axelrod&lt;/b&gt;, &lt;b&gt;Simran Khanuja&lt;/b&gt;, &lt;b&gt;Jonathan Clark&lt;/b&gt;, &lt;b&gt;Orhan Firat&lt;/b&gt;, Michael Auli, &lt;b&gt;Sebastian Ruder&lt;/b&gt;, &lt;b&gt;Jason Riesa, Melvin Johnson&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://aps.arxiv.org/pdf/2203.00236.pdf"&gt;TRILLsson: Distilled Universal Paralinguistic Speech Representations&lt;/a&gt;&lt;b&gt; &lt;/b&gt;(see &lt;a href="https://ai.googleblog.com/2022/03/trillsson-small-universal-speech.html"&gt;blog post&lt;/a&gt;) &lt;br /&gt;  &lt;i&gt;&lt;b&gt;Joel Shor&lt;/b&gt;, &lt;b&gt;Subhashini Venugopalan&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.15519.pdf"&gt;Learning Neural Audio Features Without Supervision&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Sarthak Yadav, &lt;b&gt;Neil Zeghidour&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2202.07273.pdf"&gt;SpeechPainter: Text-Conditioned Speech Inpainting&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zalan Borsos&lt;/b&gt;, &lt;b&gt;Matthew Sharifi&lt;/b&gt;, &lt;b&gt;Marco Tagliasacchi&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.16749.pdf"&gt;SpecGrad: Diffusion Probabilistic Model-Based Neural Vocoder with Adaptive Noise Spectral Shaping&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Yuma Koizumi&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, Kohei Yatabe, &lt;b&gt;Nanxin Chen&lt;/b&gt;, &lt;b&gt;Michiel Bacchiani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.00562.pdf"&gt;Distance-Based Sound Separation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Katharine Patterson&lt;/b&gt;, &lt;b&gt;Kevin Wilson&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;John R. Hershey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2209.06096.pdf"&gt;Analysis of Self-Attention Head Diversity for Conformer-Based Automatic Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Kartik Audhkhasi&lt;/b&gt;, &lt;b&gt;Yinghui Huang&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.07553.pdf"&gt;Improving Rare Word Recognition with LM-Aware MWER Training&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Wang Weiran&lt;/b&gt;, &lt;b&gt;Tongzhou Chen&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, &lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;W. Ronny Huang&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Neeraj Gaur&lt;/b&gt;, &lt;b&gt;Sepand Mavandadi&lt;/b&gt;, &lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.03409.pdf"&gt;MAESTRO: Matched Speech Text Representations Through Modality Matching&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Zhehuai Chen&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Pedro J. Moreno&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.12668.pdf"&gt;Pseudo Label is Better Than Human Label&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Zhouyuan Huo&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;On the Optimal Interpolation Weights for Hybrid Autoregressive Transducer Model &lt;br /&gt;  &lt;i&gt;&lt;b&gt;Ehsan Variani&lt;/b&gt;, &lt;b&gt;Michael Riley&lt;/b&gt;, &lt;b&gt;David Rybach&lt;/b&gt;, &lt;b&gt;Cyril Allauzen&lt;/b&gt;, &lt;b&gt;Tongzhou Chen&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.07556.pdf"&gt;Streaming Align-Refine for Non-autoregressive Deliberation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Wang Weiran&lt;/b&gt;, &lt;b&gt;Ke Hu&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2209.06359.pdf"&gt;Federated Pruning: Improving Neural Network Efficiency with Federated Learning&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Rongmei Lin&lt;a href="#1" name="top1"&gt;&lt;span class="Apple-style-span" style="font-size: small;"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/a&gt;, &lt;b&gt;Yonghui Xiao&lt;/b&gt;, &lt;b&gt;Tien-Ju Yang&lt;/b&gt;, &lt;b&gt;Ding Zhao&lt;/b&gt;, Li Xiong, &lt;b&gt;Giovanni Motta&lt;/b&gt;, &lt;b&gt;Fran&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;ç&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;oise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.06164.pdf"&gt;A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Weiran Wang&lt;/b&gt;, &lt;b&gt;Ding Zhao&lt;/b&gt;, &lt;b&gt;Tara N Sainath&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Robert David&lt;/b&gt;, &lt;b&gt;Rami Botros&lt;/b&gt;, &lt;b&gt;Xin Wang&lt;/b&gt;, &lt;b&gt;Rina Panigrahy&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Ian McGraw&lt;/b&gt;, &lt;b&gt;Rohit Prabhavalkar&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.15952.pdf"&gt;4-Bit Conformer with Native Quantization Aware Training for Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Phoenix Meadowlark&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Lukasz Lew&lt;/b&gt;, &lt;b&gt;Shivani Agrawal&lt;/b&gt;, &lt;b&gt;Oleg Rybakov&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2207.07935.pdf"&gt;Visually-Aware Acoustic Event Detection Using Heterogeneous Graphs&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Amir Shirian, &lt;b&gt;Krishna Somandepalli&lt;/b&gt;, Victor Sanchez, Tanaya Guha &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2205.03481.pdf"&gt;A Conformer-Based Waveform-Domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Sankaran Panchapagesan&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Turaj Zakizadeh Shabestary&lt;/b&gt;, &lt;b&gt;Shuai Shao&lt;/b&gt;, &lt;b&gt;Nathan Howard&lt;/b&gt;, &lt;b&gt;Alex Park&lt;/b&gt;, &lt;b&gt;James Walker&lt;/b&gt;, &lt;b&gt;Alexander Gruenstein&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://research.google/pubs/pub51530/"&gt;Reducing Domain Mismatch in Self-Supervised Speech Pre-training&lt;/a&gt;&lt;br /&gt;&lt;i&gt;  Murali Karthick Baskar, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Bhuvana Ramabhadran&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Nicolás Serrano&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1vCrHH2lfWgVzyxH0R6i9H-wPtb0GYTmx/view?usp=sharing"&gt;On-the-Fly ASR Corrections with Audio Exemplars&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Golan Pundak&lt;/b&gt;, &lt;b&gt;Tsendsuren Munkhdalai&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.13916.pdf"&gt;A Language Agnostic Multilingual Streaming On-Device ASR System&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Bo Li&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, Ruoming Pang*, &lt;b&gt;Shuo-Yiin Chang&lt;/b&gt;, &lt;b&gt;Qiumin Xu&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Vince Chen&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Heguang Liu&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Parisa Haghani&lt;/b&gt;, &lt;b&gt;Sameer Bidichandani&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.10752.pdf"&gt;XTREME-S: Evaluating Cross-Lingual Speech Representations&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Alexis Conneau&lt;/b&gt;, &lt;b&gt;Ankur Bapna&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Min Ma&lt;/b&gt;, Patrick von Platen, Anton Lozhkov, &lt;b&gt;Colin Cherry&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Clara Rivera&lt;/b&gt;, &lt;b&gt;Mihir Kale&lt;/b&gt;, &lt;b&gt;Daan van Esch&lt;/b&gt;, &lt;b&gt;Vera Axelrod&lt;/b&gt;, &lt;b&gt;Simran Khanuja&lt;/b&gt;, &lt;b&gt;Jonathan Clark&lt;/b&gt;, &lt;b&gt;Orhan Firat&lt;/b&gt;, Michael Auli, &lt;b&gt;Sebastian Ruder&lt;/b&gt;, &lt;b&gt;Jason Riesa&lt;/b&gt;, &lt;b&gt;Melvin Johnson&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.13191.pdf"&gt;Towards Disentangled Speech Representations&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Cal Peyser&lt;/b&gt;, &lt;b&gt;Ronny Huang&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Tara Sainath&lt;/b&gt;, Michael Picheny, Kyunghyun Cho &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.03793.pdf"&gt;Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Shaojin Ding&lt;/b&gt;, &lt;b&gt;Rajeev Rikhye&lt;/b&gt;, &lt;b&gt;Qiao Liang&lt;/b&gt;, &lt;b&gt;Yanzhang He&lt;/b&gt;, &lt;b&gt;Quan Wang&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Tom O'Malley&lt;/b&gt;, &lt;b&gt;Ian McGraw&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2209.06410.pdf"&gt;A Universally-Deployable ASR Frontend for Joint Acoustic Echo Cancellation, Speech Enhancement, and Voice Separation&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Tom O’Malley&lt;/b&gt;, &lt;b&gt;Arun Narayanan&lt;/b&gt;, &lt;b&gt;Quan Wang&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2208.13183"&gt;Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Lev Finkelstein&lt;/b&gt;, &lt;b&gt;Heiga Zen&lt;/b&gt;, Norman Casagrande, &lt;b&gt;Chun-an Chan&lt;/b&gt;, &lt;b&gt;Ye Jia&lt;/b&gt;, &lt;b&gt;Tom Kenter&lt;/b&gt;, &lt;b&gt;Alex Petelin&lt;/b&gt;, Jonathan Shen*,&lt;b&gt; Vincent Wan&lt;/b&gt;, &lt;b&gt;Yu Zhang&lt;/b&gt;, &lt;b&gt;Yonghui Wu&lt;/b&gt;, &lt;b&gt;Robert Clark&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.12559.pdf"&gt;A Scalable Model Specialization Framework for Training and Inference Using Submodels and Its Application to Speech Model Personalization&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Fadi Biadsy&lt;/b&gt;, &lt;b&gt;Youzheng Chen&lt;/b&gt;, &lt;b&gt;Xia Zhang&lt;/b&gt;, &lt;b&gt;Oleg Rybakov&lt;/b&gt;, &lt;b&gt;Andrew Rosenberg&lt;/b&gt;, &lt;b&gt;Pedro Moreno&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2204.05738.pdf"&gt;Text-Driven Separation of Arbitrary Sounds&lt;/a&gt;&lt;br /&gt;  &lt;i&gt;&lt;b&gt;Kevin Kilgour&lt;/b&gt;, &lt;b&gt;Beat Gfeller&lt;/b&gt;, &lt;b&gt;Qingqing Huang&lt;/b&gt;, &lt;b&gt;Aren Jansen&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;Marco Tagliasacchi&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;br /&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration: underline;"&gt;Workshops, Tutorials &amp;amp; Special  Sessions&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc"&gt;The VoxCeleb Speaker Recognition Challenge 2022 (VoxSRC-22)&lt;/a&gt;&lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Arsha Nagrani&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Self-Supervised Representation Learning for Speech Processing &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Tara Sainath&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Learning from Weak Labels &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Ankit Shah&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2203.03543.pdf"&gt;RNN Transducers for Named Entity Recognition with Constraints on Alignment for Understanding Medical Conversations&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Hagen Soltau&lt;/b&gt;, &lt;b&gt;Izhak Shafran&lt;/b&gt;, &lt;b&gt;Mingqiu Wang&lt;/b&gt;, &lt;b&gt;Laurent El Shafey&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://claritychallenge.org/clarity2021-workshop/papers/Clarity_2021_paper_yang.pdf"&gt;Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Samuel Yang&lt;/b&gt;, &lt;b&gt;Scott Wisdom&lt;/b&gt;, &lt;b&gt;Chet Gnegy&lt;/b&gt;, &lt;b&gt;Richard F. Lyon&lt;/b&gt;, &lt;b&gt;Sagar Savla&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2209.06358.pdf"&gt;Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Michael Chinen&lt;/b&gt;, &lt;b&gt;Jan Skoglund&lt;/b&gt;, &lt;b&gt;Chandan K. A. Reddy&lt;/b&gt;, Alessandro Ragano, Andrew Hines &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2110.00155.pdf"&gt;Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device&lt;/a&gt;&lt;br /&gt;  Authors: &lt;i&gt;&lt;b&gt;Zhouyuan Huo&lt;/b&gt;, &lt;b&gt;Dongseong Hwang&lt;/b&gt;, &lt;b&gt;Khe Chai Sim&lt;/b&gt;, &lt;b&gt;Shefali Garg&lt;/b&gt;, &lt;b&gt;Ananya Misra&lt;/b&gt;, &lt;b&gt;Nikhil Siddhartha&lt;/b&gt;, &lt;b&gt;Trevor Strohman&lt;/b&gt;, &lt;b&gt;Fran&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;ç&lt;/b&gt;&lt;/i&gt;&lt;i&gt;&lt;b&gt;oise Beaufays&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Trustworthy Speech Processing &lt;br /&gt;  Organizers include: &lt;b&gt;&lt;i&gt;Shrikanth Narayanan&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; &lt;!--Footnotes themselves at the bottom.--&gt; &lt;hr width="80%" /&gt;&lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;br /&gt;  &lt;a name="1"&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;Work done while at Google.&lt;a href="#top1"&gt; &amp;nbsp;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;  </content><link href="http://ai.googleblog.com/feeds/5836135189460527312/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/google-at-interspeech-2022.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5836135189460527312" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5836135189460527312" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/google-at-interspeech-2022.html" rel="alternate" title="Google at Interspeech 2022" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2426489791409654175</id><published>2022-09-16T13:35:00.001-07:00</published><updated>2022-10-21T06:31:20.348-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="ads"/><category scheme="http://www.blogger.com/atom/ns#" term="Algorithms"/><category scheme="http://www.blogger.com/atom/ns#" term="market algorithms"/><title type="text">Robust Online Allocation with Dual Mirror Descent</title><content type="html">&lt;span class="byline-author"&gt;Posted by Santiago Balseiro, Staff Research Scientist, Google Research, and Associate Professor at Columbia University, and Vahab Mirrokni, Distinguished Scientist, Google Research&lt;/span&gt; &lt;p&gt;The emergence of digital technologies has transformed decision making across commercial sectors such as airlines, online retailing, and internet advertising. Today, real-time decisions need to be repeatedly made in highly uncertain and rapidly changing environments. Moreover, organizations usually have limited resources, which need to be efficiently allocated across decisions. Such problems are referred to as &lt;em&gt;online allocation problems with resource constraints&lt;/em&gt;, and applications abound. Some examples include: &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;ul&gt; &lt;li&gt;&lt;b&gt;Bidding with Budget Constraints: &lt;/b&gt;Advertisers increasingly purchase ad slots using auction-based marketplaces such as search engines and ad exchanges. A typical advertiser can participate in a large number of auctions in a given month. Because the supply in these marketplaces is uncertain, advertisers set budgets to control their total spend. Therefore, advertisers need to determine how to optimally place bids while limiting total spend and maximizing conversions.  &lt;/li&gt;&lt;li&gt;&lt;b&gt;Dynamic Ad Allocation: &lt;/b&gt;Publishers can monetize their websites by signing deals with advertisers guaranteeing a number of impressions or by auctioning off slots in the open market. To make this choice, publishers need to trade off, in real-time, the short-term revenue from selling slots in the open market and the long-term benefits of delivering good quality spots to reservation ads.  &lt;/li&gt;&lt;li&gt;&lt;b&gt;Airline Revenue Management:&lt;/b&gt; Planes have a limited number of seats that need to be filled up as much as possible before a flight’s departure. But demand for flights changes over time and airlines would like to sell airline tickets to the customers who are willing to pay the most. Thus, airlines have increasingly adopted sophisticated automated systems to manage the pricing and availability of airline tickets.   &lt;/li&gt;&lt;li&gt;&lt;b&gt;Personalized Retailing with Limited Inventories:&lt;/b&gt; Online retailers can use real-time data to personalize their offerings to customers who visit their store. Because product inventory is limited and cannot be easily replenished, retailers need to dynamically decide which products to offer and at what price to maximize their revenue while satisfying their inventory constraints. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The common feature of these problems is the presence of resource constraints (budgets, contractual obligations, seats, or inventory, respectively in the examples above) and the need to make dynamic decisions in environments &lt;a href="https://en.wikipedia.org/wiki/Knightian_uncertainty"&gt;with uncertainty&lt;/a&gt;. Resource constraints are challenging because they link decisions across time — e.g., in the bidding problem, bidding too high early can leave advertisers with no budget, and thus missed opportunities later. Conversely, bidding too conservatively can result in a low number of conversions or clicks. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s1273/image10.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="664" data-original-width="1273" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s16000/image10.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Two central resource allocation problems faced by advertisers and publishers in internet advertising markets.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;         &lt;p&gt;In this post, we discuss state-of-the-art algorithms that can help maximize goals in dynamic, resource-constrained environments. In particular, we have recently developed a new class of algorithms for online allocation problems, called &lt;em&gt;dual mirror descent&lt;/em&gt;, that are simple, robust, and flexible. Our papers have appeared in &lt;a href="https://arxiv.org/abs/2011.10124"&gt;Operations Research&lt;/a&gt;, &lt;a href="https://proceedings.mlr.press/v119/balseiro20a.html"&gt;ICML’20&lt;/a&gt;, and &lt;a href="https://proceedings.mlr.press/v139/balseiro21a.html"&gt;ICML’21&lt;/a&gt;, and we have &lt;a href="https://arxiv.org/abs/2202.06152"&gt;ongoing work&lt;/a&gt; to continue progress in this space. Compared to existing approaches, dual mirror descent is faster as it does not require solving auxiliary optimization problems, is more flexible because it can handle many applications across different sectors with minimal modifications, and is more robust as it enjoys remarkable performance under different environments. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Online Allocation Problems&lt;/b&gt;&lt;br /&gt;In an online allocation problem, a decision maker has a limited amount of total resources (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;B&lt;/em&gt;&lt;/span&gt;) and receives a certain number of requests over time (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt;). At any point in time (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;), the decision maker receives a reward function (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;f&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;)  and resource consumption function (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;b&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;), and takes an action (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;). The reward and resource consumption functions change over time and the objective is to maximize the total reward within the resource constraints. If all the requests were &lt;em&gt;known in advance&lt;/em&gt;, then an &lt;em&gt;optimal&lt;/em&gt; allocation could be obtained by solving an &lt;em&gt;offline&lt;/em&gt; optimization problem for how to maximize the reward function over time within the resource constraints&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;&lt;span style="font-size: x-small;"&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;p&gt;The optimal &lt;em&gt;offline&lt;/em&gt; allocation cannot be implemented in practice because it requires knowing future requests. However, this is still useful for framing the goal of &lt;em&gt;online&lt;/em&gt; allocation problems: to design an algorithm whose performance is as close to optimal as possible &lt;em&gt;without&lt;/em&gt; knowing future requests. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Achieving the Best of Many Worlds with Dual Mirror Descent&lt;/b&gt;&lt;br /&gt;A simple, yet powerful idea to handle resource constraints is introducing “prices” for the resources, which enables accounting for the &lt;a href="https://en.wikipedia.org/wiki/Opportunity_cost"&gt;opportunity cost&lt;/a&gt; of consuming resources when making decisions. For example, selling a seat on a plane today means it can’t be sold tomorrow. These prices are useful as an internal accounting system of the algorithm. They serve the purpose of coordinating decisions at different moments in time and allow decomposing&lt;a href="https://en.wikipedia.org/wiki/Lagrangian_relaxation"&gt; a complex problem&lt;/a&gt; with resource constraints into simpler subproblems: one per time period with no resource constraints. For example, in a bidding problem, the prices capture an advertiser’s opportunity cost of consuming one unit of budget and allow the advertiser to handle each auction as an independent bidding problem. &lt;/p&gt;&lt;p&gt;This reframes the online allocation problem as a problem of pricing resources to enable optimal decision making. The key innovation of our algorithm is using machine learning to predict optimal prices in an online fashion: we choose prices dynamically using &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Mirror_descent"&gt;mirror descent&lt;/a&gt;&lt;/em&gt;, a popular optimization algorithm for training machine learning predictive models. Because prices for resources are referred to as "&lt;a href="https://en.wikipedia.org/wiki/Duality_(optimization)"&gt;dual variables&lt;/a&gt;" in the field of optimization, we call the resulting algorithm &lt;a href="https://arxiv.org/abs/2011.10124"&gt;dual mirror descent&lt;/a&gt;.   &lt;/p&gt;&lt;p&gt;The algorithm works sequentially by assuming uniform resource consumption over time is optimal and updating the dual variables after each action. It starts at a moment in time (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;) by taking an action (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;x&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;) that maximizes the reward minus the opportunity cost of consuming resources (shown in the top gray box below). The action (e.g., how much to bid or which ad to show) is implemented if there are enough resources available. Then, the algorithm computes the error in the resource consumption (&lt;span style="font-family: Times New Roman;"&gt;&lt;em&gt;g&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt;&lt;/span&gt;), which is the difference between uniform consumption over time and the actual resource consumption (below in the third gray box). A new dual variable for the next time period is computed using mirror descent based on the error, which then informs the next action. Mirror descent seeks to make the error as close as possible to zero, improving the accuracy of its estimate of the dual variable, so that resources are consumed uniformly over time. While the assumption of uniform resource consumption may be surprising, it helps avoid missing good opportunities and often aligns with commercial goals so is effective. Mirror descent also allows a variety of update rules; more details are in the paper. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxmJQO-O-g_9ueC0lFFhTzjHEfy_tj68ams9V4INm_8bQrRSvzKOyKtVcE33LbvVwS4TEy0u_h3AoXGTh4k7_u6KXf-Z6gLJYSDw-FZQdfbzBnrSP9OIK4klcN3e9EA7jYoFW0Jrv3YhOXn7MyxldWm2KVu34E4pVqy4ci3fdKn1dm8UpYYfdwFEsfg/s1273/image20.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1139" data-original-width="1273" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFxmJQO-O-g_9ueC0lFFhTzjHEfy_tj68ams9V4INm_8bQrRSvzKOyKtVcE33LbvVwS4TEy0u_h3AoXGTh4k7_u6KXf-Z6gLJYSDw-FZQdfbzBnrSP9OIK4klcN3e9EA7jYoFW0Jrv3YhOXn7MyxldWm2KVu34E4pVqy4ci3fdKn1dm8UpYYfdwFEsfg/s16000/image20.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An overview of the dual mirror descent algorithm.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;         &lt;p&gt;By design, dual mirror descent has a self-correcting feature that prevents depleting resources too early or waiting too long to consume resources and missing good opportunities. When a request consumes more or less resources than the target, the corresponding dual variable is increased or decreased. When resources are then priced higher or lower, future actions are chosen to consume resources more conservatively or aggressively.  &lt;/p&gt;&lt;p&gt;This algorithm is easy to implement, fast, and enjoys remarkable performance under different environments. These are some salient features of our algorithm: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;Existing methods require periodically &lt;a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.2014.1289"&gt;solving large auxiliary optimization problems&lt;/a&gt; using past data. In contrast, this algorithm does not need to solve any auxiliary optimization problem and has a very simple rule to update the dual variables, which, in many cases, can be run in &lt;a href="https://en.wikipedia.org/wiki/Time_complexity"&gt;linear time complexity&lt;/a&gt;. Thus, it is appealing for many real-time applications that require fast decisions.  &lt;/li&gt;&lt;li&gt;There are minimal requirements on the structure of the problem. Such flexibility allows dual mirror descent to handle many applications across different sectors with minimal modifications. Moreover, our algorithms are flexible since they accommodate different objectives, constraints, or regularizers. By &lt;a href="https://proceedings.mlr.press/v139/balseiro21a.html"&gt;incorporating regularizers&lt;/a&gt;, decision makers can include important objectives beyond economic efficiency, such as fairness.  &lt;/li&gt;&lt;li&gt;Existing algorithms for online allocation problems are tailored for either adversarial or stochastic input data. Algorithms for &lt;a href="https://ieeexplore.ieee.org/document/1530720"&gt;adversarial inputs&lt;/a&gt; are robust as they make almost no assumptions on the structure of the data but, in turn, obtain performance guarantees that are too pessimistic in practice. On the other hand, algorithms for &lt;a href="https://dl.acm.org/doi/abs/10.1145/1566374.1566384"&gt;stochastic inputs&lt;/a&gt; enjoy better performance guarantees by exploiting statistical patterns in the data but can perform poorly when the model is &lt;a href="https://en.wikipedia.org/wiki/Statistical_model_specification"&gt;misspecified&lt;/a&gt;. Dual mirror descent, however, attains performance close to optimal in both stochastic and adversarial input models while being oblivious to the structure of the input model. Compared to existing work on simultaneous approximation algorithms, our method &lt;a href="https://epubs.siam.org/doi/10.1137/1.9781611973099.134"&gt;is more general&lt;/a&gt;, applies to a wide range of problems, and requires &lt;a href="https://dl.acm.org/doi/10.1145/2764468.2764536"&gt;no forecasts&lt;/a&gt;.  Below is a comparison of our algorithm to other state-of-the-art methods. Results are based on synthetic data for an &lt;a href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2014.2017"&gt;ad allocation problem&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;      &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFbs6yYYocSijnXw-NrM1isG8h-PxhAyS7o8L_vJY3gG43YDoldz2Q6nvNkd4h0EuOku9t17V4nVUm-HKI7eRyAfaciWzMhzBFcdrsauJRQcDb2pf3cwE7AAcVQuY0QDjgwSR5lqmXGob5OpssT1d4VZ89s5va8p2SarAYcJtDrm8SlQ9jRg0QYqEqRQ/s1200/image12.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="742" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFbs6yYYocSijnXw-NrM1isG8h-PxhAyS7o8L_vJY3gG43YDoldz2Q6nvNkd4h0EuOku9t17V4nVUm-HKI7eRyAfaciWzMhzBFcdrsauJRQcDb2pf3cwE7AAcVQuY0QDjgwSR5lqmXGob5OpssT1d4VZ89s5va8p2SarAYcJtDrm8SlQ9jRg0QYqEqRQ/s16000/image12.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Performance of dual mirror descent, &lt;a href="https://dl.acm.org/doi/10.1145/1566374.1566384"&gt;a training based method,&lt;/a&gt; and &lt;a href="https://ieeexplore.ieee.org/document/1530720"&gt;an adversarial method&lt;/a&gt; relative to the optimal offline solution. Lower values indicate performance closer to the optimal offline allocation. Results are generated using synthetic experiments based on &lt;a href="https://pubsonline.informs.org/doi/10.1287/mnsc.2014.2017"&gt;public data&lt;/a&gt; for an ad allocation problem.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;In this post we introduced dual mirror descent, an algorithm for online allocation problems that is simple, robust, and flexible. It is particularly notable that after a long line of work in online allocation algorithms, dual mirror descent provides a way to analyze a wider range of algorithms with superior robustness priorities compared to previous techniques.  Dual mirror descent has a wide range of applications across several commercial sectors and has been used over time at Google to help advertisers capture more value through better algorithmic decision making. We are also exploring &lt;a href="https://arxiv.org/abs/2202.06152"&gt;further work&lt;/a&gt; related to mirror descent and its connections to &lt;a href="https://en.wikipedia.org/wiki/PID_controller"&gt;PI controllers&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to thank our co-authors Haihao Lu and Balu Sivan, and Kshipra Bhawalkar for their exceptional support and contributions. We would also like to thank our collaborators in the ad quality team and market algorithm research.&lt;/em&gt;&lt;/p&gt;    &lt;!--Footnotes--&gt;&lt;hr width="80%" /&gt;&lt;p&gt;&lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;sup&gt;&lt;a name="fn1"&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;Formalized in the equation below:&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; &lt;table align="left" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/s2220/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="349" data-original-width="2220" height="50" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwlilkKvmPKrZhHmes0ianEnwF-jUTHBRB25lPXRbZLR0VWTEPVPmgfY416f64hox-v0aAHrsPl5HAGcA0CbkIZprG-NHCWTFErz6YwGI54WEZO7Af523tm32J0fIp_N3t9UzMziOIDPaEtyrsFrHtfnYrYUo2dJjOh23RkdypGfK9Ax49SCBgWg238Q/w320-h50/Screen%20Shot%202022-09-16%20at%201.02.10%20PM.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  </content><link href="http://ai.googleblog.com/feeds/2426489791409654175/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/robust-online-allocation-with-dual.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2426489791409654175" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2426489791409654175" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/robust-online-allocation-with-dual.html" rel="alternate" title="Robust Online Allocation with Dual Mirror Descent" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeCOTNurjAHYOaGD6AgsHuh7Q2HDcJMVY7TkzCCZ1zebUJbkWDEG1glO-4C5UAxS3B_QvOBcaHgjY74DCu9Gt7woFDMLhi0oYChKj9bntmdyI9lDAl_HlBYG5n8wm1IjTrgUc9f3iAD_CEiYP9ezYFacEf8dXQlEiR5VxqxDu1ySlnH3BpL-dDD9dmVg/s72-c/image10.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1528557374592447782</id><published>2022-09-15T12:16:00.004-07:00</published><updated>2022-10-21T06:31:21.320-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Multimodal Learning"/><title type="text">PaLI: Scaling Language-Image Learning in 100+ Languages</title><content type="html">&lt;span class="byline-author"&gt;Posted by Xi Chen and Xiao Wang, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;Advanced language models (e.g., &lt;a href="https://arxiv.org/abs/2005.14165"&gt;GPT&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"&gt;GLaM&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"&gt;PaLM&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html"&gt;T5&lt;/a&gt;) have demonstrated diverse capabilities and achieved impressive results across tasks and languages by scaling up their number of parameters. Vision-language (VL) models can benefit from similar scaling to address many tasks, such as &lt;a href="https://en.wikipedia.org/wiki/Natural_language_generation#Image_Captioning"&gt;image captioning&lt;/a&gt;, visual &lt;a href="https://en.wikipedia.org/wiki/Question_answering"&gt;question answering&lt;/a&gt; (VQA), &lt;a href="https://en.wikipedia.org/wiki/Outline_of_object_recognition"&gt;object recognition&lt;/a&gt;, and in-context &lt;a href="https://en.wikipedia.org/wiki/Optical_character_recognition"&gt;optical-character-recognition&lt;/a&gt; (OCR). Increasing the success rates for these practical tasks is important for everyday interactions and applications. Furthermore, for a truly universal system, vision-language models should be able to operate in many languages, not just one. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2209.06794"&gt;PaLI: A Jointly-Scaled Multilingual Language-Image Model&lt;/a&gt;”, we introduce a unified language-image model trained to perform many tasks and in over 100 languages. These tasks span vision, language, and multimodal image and language applications, such as &lt;a href="https://arxiv.org/abs/1505.00468"&gt;visual question answering&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Natural_language_generation#Image_Captioning"&gt;image captioning&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Object_detection"&gt;object detection&lt;/a&gt;, &lt;a href="https://paperswithcode.com/task/image-classification"&gt;image classification&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Optical_character_recognition"&gt;OCR&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning"&gt;text reasoning&lt;/a&gt;, and others. Furthermore, we use a collection of public images that includes automatically collected annotations in 109 languages, which we call the WebLI dataset. The PaLI model pre-trained on WebLI achieves state-of-the-art performance on challenging image and language benchmarks, such as &lt;a href="https://arxiv.org/abs/1504.00325"&gt;COCO-Captions&lt;/a&gt;,&amp;nbsp;&lt;a href="https://textvqa.org/textcaps/"&gt;TextCaps&lt;/a&gt;, &lt;a href="https://visualqa.org/"&gt;VQAv2&lt;/a&gt;, &lt;a href="https://okvqa.allenai.org/"&gt;OK-VQA&lt;/a&gt;, &lt;a href="https://textvqa.org/"&gt;TextVQA&lt;/a&gt; and others. It also outperforms prior models’ multilingual visual captioning and visual question answering benchmarks. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;br /&gt;One goal of this project is to examine how language and vision models interact at scale and specifically the scalability of language-image models. We explore both per-modality scaling and the resulting cross-modal interactions of scaling. We train our largest model to 17 billion (17B) parameters, where the visual component is scaled up to 4B parameters and the language model to 13B.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The PaLI model architecture is simple, reusable and scalable. It consists of a &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer&lt;/a&gt; encoder that processes the input text, and an auto-regressive Transformer decoder that generates the output text. To process images, the input to the Transformer encoder also includes "visual words" that represent an image processed by a &lt;a href="https://arxiv.org/abs/2010.11929"&gt;Vision Transformer&lt;/a&gt; (ViT). A key component of the PaLI model is reuse, in which we seed the model with weights from previously-trained uni-modal vision and language models, such as &lt;a href="https://arxiv.org/abs/2010.11934"&gt;mT5-XXL&lt;/a&gt; and large &lt;a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"&gt;ViTs&lt;/a&gt;. This reuse not only enables the transfer of capabilities from uni-modal training, but also saves computational cost.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s1600/LILM%20%20PaLI%2006.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="454" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s16000/LILM%20%20PaLI%2006.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The PaLI model addresses a wide range of tasks in the language-image, language-only and image-only domain using the same API (e.g., visual-question answering, image captioning, scene-text understanding, etc.). The model is trained to support over 100 languages and tuned to perform multilingually for multiple language-image tasks.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Dataset: Language-Image Understanding in 100+ Languages&lt;/b&gt;&lt;br /&gt;Scaling studies for deep learning show that larger models require larger datasets to train effectively. To unlock the potential of language-image pretraining, we construct WebLI, a multilingual language-image dataset built from images and text available on the public web. &lt;/p&gt;&lt;p&gt;WebLI scales up the text language from English-only datasets to 109 languages, which enables us to perform downstream tasks in many languages. The data collection process is similar to that employed by other datasets, e.g. &lt;a href="https://arxiv.org/abs/2102.05918"&gt;ALIGN&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2111.07991"&gt;LiT&lt;/a&gt;, and enabled us to scale the WebLI dataset to 10 billion images and 12 billion alt-texts.  &lt;/p&gt;&lt;p&gt;In addition to annotation with web text, we apply the &lt;a href="https://cloud.google.com/vision"&gt;Cloud Vision API&lt;/a&gt; to perform OCR on the images, leading to 29 billion image-OCR pairs. We perform near-deduplication of the images against the train, validation and test splits of 68 common vision and vision-language datasets, to avoid leaking data from downstream evaluation tasks, as is standard in the literature. To further improve the data quality, we score image and alt-text pairs based on their cross-modal similarity, and tune the threshold to keep only 10% of the images, for a total of 1 billion images used for training PaLI. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigSmeBge-LPNYi7dcLEgt3DKUPHeq4uPWd4WUCozQzAmhs3-pEVuZav3vxz8qH0Kid8IL5DqTC5n_FO3LWpIVT_vlSFp6mqOh-0HMw8RrSs6FNbEBitPN7_ih-2I7HS85BNJdf2mxLliZokGa75EyJB2uG-2wIsx4GCjB5zUfkBYFZ-D8G8TrRrwSicg/s1999/image2.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="691" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigSmeBge-LPNYi7dcLEgt3DKUPHeq4uPWd4WUCozQzAmhs3-pEVuZav3vxz8qH0Kid8IL5DqTC5n_FO3LWpIVT_vlSFp6mqOh-0HMw8RrSs6FNbEBitPN7_ih-2I7HS85BNJdf2mxLliZokGa75EyJB2uG-2wIsx4GCjB5zUfkBYFZ-D8G8TrRrwSicg/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Sampled images from WebLI associated with multilingual alt-text and OCR. The second image is by jopradier (&lt;a href="https://www.geneanet.org/cartes-postales/view/6573969#0"&gt;original&lt;/a&gt;), used under the&lt;a href="https://www.creativecommons.org/licenses/by-nc-sa/2.0/fr/deed.en"&gt; CC BY-NC-SA 2.0 license&lt;/a&gt;. Remaining images are also used with permission.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8WYm-cmOCCHfoTOd2NK51zUXhW8qN9SwaXFBM477IQl6t4hAJ5BT0Oa089szfVlbMOw2_shK5zqdX8HAxP5ALKpn9LGzi4Fu7HNI28TgBhEbJcu9YhYMADzS4vqjjkP-WE2G5gw4_iz_4v-pdZe92Lwk0i17JGnosqG3Jezw5XKrafZPwpaFwerd0A/s1999/image3.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="626" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgx8WYm-cmOCCHfoTOd2NK51zUXhW8qN9SwaXFBM477IQl6t4hAJ5BT0Oa089szfVlbMOw2_shK5zqdX8HAxP5ALKpn9LGzi4Fu7HNI28TgBhEbJcu9YhYMADzS4vqjjkP-WE2G5gw4_iz_4v-pdZe92Lwk0i17JGnosqG3Jezw5XKrafZPwpaFwerd0A/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Statistics of recognized languages from alt-text and OCR in WebLI.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbKpPjtzNzu7HuitGdREtnSheTGYlSPYOhgvVCiCIrc9enygtEewRkz3DeOg4wiiQQ0TE3lPJCVhc4zLPtpRrs2RfL7fh2EEEALOJWV-PVjyTRtF0atTuuoMGo3J1EUjjvSYTnPV3xELnWq0gHJz7evyaL5O9lR1Fc1Y6-wcCrjtF2_HM4h2wKDf8Wcg/s954/image5.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="686" data-original-width="954" height="288" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbKpPjtzNzu7HuitGdREtnSheTGYlSPYOhgvVCiCIrc9enygtEewRkz3DeOg4wiiQQ0TE3lPJCVhc4zLPtpRrs2RfL7fh2EEEALOJWV-PVjyTRtF0atTuuoMGo3J1EUjjvSYTnPV3xELnWq0gHJz7evyaL5O9lR1Fc1Y6-wcCrjtF2_HM4h2wKDf8Wcg/w400-h288/image5.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Image-text pair counts of WebLI and other large-scale vision-language datasets, &lt;a href="https://openai.com/blog/clip/"&gt;CLIP&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2102.05918"&gt;ALIGN&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2111.07991"&gt;LiT&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Training Large Language-Image Models&lt;/b&gt;&lt;br /&gt;Vision-language tasks require different capabilities and sometimes have diverging goals. Some tasks inherently require localization of objects to solve the task accurately, whereas some other tasks might need a more global view. Similarly, different tasks might require either long or compact answers. To address all of these objectives, we leverage the richness of the WebLI pre-training data and introduce a mixture of pre-training tasks, which prepare the model for a variety of downstream applications. To accomplish the goal of solving a wide variety of tasks, we enable knowledge-sharing between multiple image and language tasks by casting all tasks into a single generalized API (input: image + text; output: text), which is also shared with the pretraining setup. The objectives used for pre-training are cast into the same API as a weighted mixture aimed at both maintaining the ability of the reused model components and training the model to perform new tasks (e.g., split-captioning for image description, OCR prediction for scene-text comprehension, VQG and VQA prediction). &lt;/p&gt;&lt;p&gt;The model is trained in &lt;a href="https://github.com/google/jax"&gt;JAX&lt;/a&gt; with &lt;a href="https://github.com/google/flax"&gt;Flax&lt;/a&gt; using the open-sourced &lt;a href="https://github.com/google-research/t5x"&gt;T5X&lt;/a&gt; and &lt;a href="https://github.com/google/flaxformer"&gt;Flaxformer&lt;/a&gt; framework. For the visual component, we introduce and train a large &lt;a href="https://arxiv.org/abs/2010.11929"&gt;ViT&lt;/a&gt; architecture, named ViT-e, with 4B parameters using the open-sourced &lt;a href="https://github.com/google-research/big_vision"&gt;BigVision&lt;/a&gt; framework. ViT-e follows the same recipe as the &lt;a href="https://arxiv.org/abs/2106.04560"&gt;ViT-G&lt;/a&gt; architecture (which has 2B parameters). For the language component, we concatenate the dense token embeddings with the patch embeddings produced by the visual component, together as the input to the multimodal encoder-decoder, which is initialized from mT5-XXL. During the training of PaLI, the weights of this visual component are frozen, and only the weights of the multimodal encoder-decoder are updated.&lt;/p&gt;&lt;p&gt; &lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;We compare PaLI on common vision-language benchmarks that are varied and challenging. The PaLI model achieves state-of-the-art results on these tasks, even outperforming very large models in the literature. For example, it outperforms the &lt;a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model"&gt;Flamingo&lt;/a&gt; model, which is several times larger (80B parameters), on several VQA and image-captioning tasks, and it also sustains performance on challenging language-only and vision-only tasks, which were not the main training objective. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf8R7SdVR3WqhSPWvKyWsVh9mVgHhxtwUaFjB4WuqVocEZtmqd7pOfAcucfzPBO---KdLKRNbEJ-j-Mp1sG-QzQTJS0pduBf_AfIX9wQeGqnStzsPKQz-caYkCC8GN9dQbmKHZwZx_dXdtqSnyBWAqdDvJFLx9Oe-JhqeYPk_U-T8_dvCppdtfZeOJdw/s1999/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="840" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjf8R7SdVR3WqhSPWvKyWsVh9mVgHhxtwUaFjB4WuqVocEZtmqd7pOfAcucfzPBO---KdLKRNbEJ-j-Mp1sG-QzQTJS0pduBf_AfIX9wQeGqnStzsPKQz-caYkCC8GN9dQbmKHZwZx_dXdtqSnyBWAqdDvJFLx9Oe-JhqeYPk_U-T8_dvCppdtfZeOJdw/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;PaLI (17B parameters) outperforms the state-of-the-art approaches (including &lt;a href="https://arxiv.org/abs/2108.10904"&gt;SimVLM&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2205.01917"&gt;CoCa&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2205.14100.pdf"&gt;GIT2&lt;/a&gt;, &lt;a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model"&gt;Flamingo&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2208.10442"&gt;BEiT3&lt;/a&gt;) on multiple vision-and-language tasks. In this plot we show the absolute score differences compared with the previous best model to highlight the relative improvements of PaLI. Comparison is on the official test splits when available. &lt;a href="https://arxiv.org/abs/1411.5726"&gt;CIDEr&lt;/a&gt; score is used for evaluation of the image captioning tasks, whereas VQA tasks are evaluated by VQA Accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMKBLTuvtTQQLBB3nEqGjXD8-wH7uHIAGnmRmaA04JJmr1QI7LsNP9Dj-iWrpCNiztMzhurkVw62-6TSFCgiuNyaD6iOsX29u5jbzmOvwNnPsyRGyaWB_2Pgpn2WISAOYIHK2MRpBLM-mHpHKe4u80oicplJ0SodGyHznIGFvvNGR4-43CVCIi3G4Y/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="845" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMKBLTuvtTQQLBB3nEqGjXD8-wH7uHIAGnmRmaA04JJmr1QI7LsNP9Dj-iWrpCNiztMzhurkVw62-6TSFCgiuNyaD6iOsX29u5jbzmOvwNnPsyRGyaWB_2Pgpn2WISAOYIHK2MRpBLM-mHpHKe4u80oicplJ0SodGyHznIGFvvNGR4-43CVCIi3G4Y/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;PaLI (17B parameters) outperforms the state-of-the-art approaches (including &lt;a href="https://arxiv.org/abs/2108.10904"&gt;SimVLM&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2205.01917"&gt;CoCa&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2205.14100.pdf"&gt;GIT2&lt;/a&gt;, &lt;a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model"&gt;Flamingo&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2208.10442"&gt;BEiT3&lt;/a&gt;) on multiple vision-and-language tasks. In this plot we show the absolute score differences compared with the previous best model to highlight the relative improvements of PaLI. Comparison is on the official test splits when available. &lt;a href="https://arxiv.org/abs/1411.5726"&gt;CIDEr&lt;/a&gt; score is used for evaluation of the image captioning tasks, whereas VQA tasks are evaluated by VQA Accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;  &lt;b&gt;Model Scaling Results&lt;/b&gt;&lt;br /&gt;We examine how the image and language model components interact with each other with regards to model scaling and where the model yields the most gains. We conclude that scaling both components jointly results in the best performance, and specifically, scaling the visual component, which requires relatively few parameters, is most essential. Scaling is also critical for better performance across multilingual tasks. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wQNoCEBG5hbvM8OYjFwlcTdG51euqx4pkhBMOU93J16ZAhWf7n5knnCL97H5-x8U-JqPKIvtrA6bT0ESuVGyCsZfcjLtJ3qhWa7oZQm1JUiCOTLNfv1zq6aqLCowh4RfvnoqSRyi1htt0sqSb0A-PYMav-BJ1cOEWffi4LUd8Fecjd2eLJJR9kFx/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1007" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj9wQNoCEBG5hbvM8OYjFwlcTdG51euqx4pkhBMOU93J16ZAhWf7n5knnCL97H5-x8U-JqPKIvtrA6bT0ESuVGyCsZfcjLtJ3qhWa7oZQm1JUiCOTLNfv1zq6aqLCowh4RfvnoqSRyi1htt0sqSb0A-PYMav-BJ1cOEWffi4LUd8Fecjd2eLJJR9kFx/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Scaling both the language and the visual components of the PaLI model contribute to improved performance. The plot shows the score differences compared to the PaLI-3B model: &lt;a href="https://arxiv.org/abs/1411.5726"&gt;CIDEr&lt;/a&gt; score is used for evaluation of the image captioning tasks, whereas VQA tasks are evaluated by VQA Accuracy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVOj5hGOaPKkUpgzeaUJMEBOA91dvGz9vGgbmP3QerXumL1qvQEVfL3cgAp4XhltbEL9DCTTmhdJyZNKb31XehcQn7bpTWxx5PPWQ_Mgb39eLXxUFuHFtm32drQdMbomNV708oe_rBDzd4tXFVqY39VtP3g6cTEZjaRSXNpgGC89w4k1L0ikBdBrBh/s1574/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="732" data-original-width="1574" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVOj5hGOaPKkUpgzeaUJMEBOA91dvGz9vGgbmP3QerXumL1qvQEVfL3cgAp4XhltbEL9DCTTmhdJyZNKb31XehcQn7bpTWxx5PPWQ_Mgb39eLXxUFuHFtm32drQdMbomNV708oe_rBDzd4tXFVqY39VtP3g6cTEZjaRSXNpgGC89w4k1L0ikBdBrBh/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Multilingual captioning greatly benefits from scaling the PaLI models. We evaluate PaLI on a 35-language benchmark Crossmodal-3600. Here we present the average score over all 35 languages and the individual score for seven diverse languages.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Model Introspection: Model Fairness, Biases, and Other Potential Issues&lt;/b&gt;&lt;br /&gt;To avoid creating or reinforcing unfair bias within large language and image models, important first steps are to (1) be transparent about the data that were used and how the model used those data, and (2) test for model fairness and conduct responsible data analyses. To address (1), our &lt;a href="http://arxiv.org/abs/2209.06794"&gt;paper&lt;/a&gt; includes a &lt;a href="https://arxiv.org/abs/2204.01075"&gt;data card&lt;/a&gt; and &lt;a href="https://modelcards.withgoogle.com/about"&gt;model card&lt;/a&gt;. To address (2), the paper includes results of demographic analyses of the dataset. We consider this a first step and know that it will be important to continue to measure and mitigate potential biases as we apply our model to new tasks, in alignment with our &lt;a href="http://ai.google/principles"&gt;AI Principles&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;  &lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We presented PaLI, a scalable multi-modal and multilingual model designed for solving a variety of vision-language tasks. We demonstrate improved performance across visual-, language- and vision-language tasks. Our work illustrates the importance of scale in both the visual and language parts of the model and the interplay between the two. We see that accomplishing vision and language tasks, especially in multiple languages, actually requires large scale models and data, and will potentially benefit from further scaling. We hope this work inspires further research in multi-modal and multilingual models.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We thank all the authors who conducted this research Soravit (Beer) Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut. We also thank Claire Cui, Slav Petrov, Tania Bedrax-Weiss, Joelle Barral, Tom Duerig, Paul Natsev, Fernando Pereira, Jeff Dean, Jeremiah Harmsen, Zoubin Ghahramani, Erica Moreira, Victor Gomes, Sarah Laszlo, Kathy Meier-Hellstern, Susanna Ricco, Rich Lee, Austin Tarango, Emily Denton, Bo Pang, Wei Li, Jihyung Kil, Tomer Levinboim, Julien Amelot, Zhenhai Zhu, Xiangning Chen, Liang Chen, Filip Pavetic, Daniel Keysers, Matthias Minderer, Josip Djolonga, Ibrahim Alabdulmohsin, Mostafa Dehghani, Yi Tay, Elizabeth Adkison, James Cockerille, Eric Ni, Anna Davies, and Maysam Moussalem for their suggestions, improvements and support. We thank Tom Small for providing visualizations for the blogpost. &lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1528557374592447782/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1528557374592447782" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1528557374592447782" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" rel="alternate" title="PaLI: Scaling Language-Image Learning in 100+ Languages" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-Pn9OEBEctQryii0Xw5Uj5Dqq6GUBAu5YcVqcv9zObFj3-VcYYy4w58MEKFHPorcr0kDUoMaldwUdwDr4eEL-0y-79hSf-6TUuS59dgEsvCarxv8U4gI3VvsxJ7We73J794BalwvaK7ao8o3FXDwlynySsmBRzCcwPhUbRptT36QUm17ahjAydIzbnw/s72-c/LILM%20%20PaLI%2006.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2508264082683271000</id><published>2022-09-13T14:18:00.001-07:00</published><updated>2022-10-21T06:31:22.191-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Responsible AI"/><title type="text">LOLNeRF: Learn from One Look</title><content type="html">&lt;span class="byline-author"&gt;Posted by Daniel Rebain, Student Researcher, and Mark Matthews, Senior Software Engineer, Google Research, Perception Team&lt;/span&gt; &lt;p&gt;An important aspect of human vision is our ability to comprehend 3D shape from the 2D images we observe. Achieving this kind of understanding with computer vision systems has been a fundamental challenge in the field. Many successful approaches rely on &lt;em&gt;multi-view&lt;/em&gt; data, where two or more images of the same scene are available from different perspectives, which makes it much easier to infer the 3D shape of objects in the images. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;There are, however, many situations where it would be useful to know 3D structure from a single image, but this problem is generally difficult or impossible to solve. For example, it isn’t necessarily possible to tell the difference between an image of an &lt;em&gt;actual&lt;/em&gt; beach and an image of a flat poster of the same beach. However it is possible to estimate 3D structure based on what kind of 3D objects occur commonly and what similar structures look like from different perspectives. &lt;/p&gt;&lt;p&gt;In &lt;a href="https://ubc-vision.github.io/lolnerf/"&gt;“LOLNeRF: Learn from One Look”&lt;/a&gt;, presented at &lt;a href="https://cvpr2022.thecvf.com/"&gt;CVPR 2022&lt;/a&gt;, we propose a framework that learns to model 3D structure and appearance from collections of &lt;em&gt;single-view&lt;/em&gt; images. LOLNeRF learns the typical 3D structure of a class of objects, such as cars, human faces or cats, but only from &lt;em&gt;single views&lt;/em&gt; of any one object, never the same object twice. We build our approach by combining &lt;em&gt;&lt;a href="https://arxiv.org/abs/1707.05776"&gt;Generative Latent Optimization&lt;/a&gt;&lt;/em&gt; (GLO) and &lt;em&gt;&lt;a href="https://arxiv.org/abs/2003.08934"&gt;neural radiance fields&lt;/a&gt;&lt;/em&gt; (NeRF) to achieve state-of-the-art results for novel view synthesis and competitive results for depth estimation. &lt;/p&gt;     &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_SG6q9CCEhSwhjXb7eBXXBiBBZNs6o2nE7paWjh5ThWpwSkhbANHxYfgx2w6c1C93C_p_GerQNgQXvbfTo9f6-vu0wcdyy0-EL2vdntANkpLQIKAw2A0Yw6ZLCoGHk36AXT1Oy9XliJG-Y8g0DFswTzy-90GtuI7r2G9aKe-9FVvXJKzhhMwBlI2OTA/s1280/image3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="422" data-original-width="1280" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_SG6q9CCEhSwhjXb7eBXXBiBBZNs6o2nE7paWjh5ThWpwSkhbANHxYfgx2w6c1C93C_p_GerQNgQXvbfTo9f6-vu0wcdyy0-EL2vdntANkpLQIKAw2A0Yw6ZLCoGHk36AXT1Oy9XliJG-Y8g0DFswTzy-90GtuI7r2G9aKe-9FVvXJKzhhMwBlI2OTA/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;We learn a 3D object model by reconstructing a large collection of single-view images using a neural network conditioned on latent vectors, &lt;em&gt;z&lt;/em&gt; (&lt;b&gt;left&lt;/b&gt;). This allows for a 3D model to be lifted from the image, and rendered from novel viewpoints. Holding the camera fixed, we can interpolate or sample novel identities (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Combining GLO and NeRF&lt;/b&gt;&lt;br/&gt;GLO is a general method that learns to reconstruct a dataset (such as a set of 2D images) by co-learning a neural network (decoder) and table of codes (latents) that is also an input to the decoder. Each of these latent codes re-creates a single element (such as an image) from the dataset. Because the latent codes have fewer dimensions than the data elements themselves, the network is forced to &lt;em&gt;generalize&lt;/em&gt;, learning common structure in the data (such as the general shape of dog snouts). &lt;/p&gt;&lt;p&gt;NeRF is a technique that is very good at reconstructing a static 3D object from 2D images. It represents an object with a neural network that outputs color and density for each point in 3D space. Color and density values are accumulated along &lt;a href="https://en.wikipedia.org/wiki/Line_(geometry)#Ray"&gt;rays&lt;/a&gt;, one ray for each pixel in a 2D image. &lt;a href="https://www.youtube.com/watch?v=JuH79E8rdKc&amp;amp;t=8s"&gt;These are then combined&lt;/a&gt; using standard computer graphics volume rendering to compute a final pixel color. Importantly, all these operations are &lt;a href="https://en.wikipedia.org/wiki/Differentiable_function"&gt;differentiable&lt;/a&gt;, allowing for end-to-end supervision. By enforcing that each rendered pixel (of the 3D representation) matches the color of ground truth (2D) pixels, the neural network creates a 3D representation that can be rendered from any viewpoint.  &lt;/p&gt;&lt;p&gt;We combine NeRF with GLO by assigning each object a latent code and concatenating it with standard NeRF inputs, giving it the ability to reconstruct &lt;em&gt;multiple&lt;/em&gt; objects. Following GLO, we co-optimize these latent codes along with network weights during training to reconstruct the input images. Unlike standard NeRF, which requires multiple views of the same object, we supervise our method with only &lt;em&gt;single views&lt;/em&gt; of any one object (but multiple examples of that type of object). Because NeRF is inherently 3D, we can then render the object from arbitrary viewpoints. Combining NeRF with GLO gives it the ability to learn common 3D structure &lt;em&gt;across instances&lt;/em&gt; from only single views while still retaining the ability to recreate specific instances of the dataset. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Camera Estimation&lt;/b&gt;&lt;br/&gt;In order for NeRF to work, it needs to know the exact camera location, relative to the object, for each image. Unless this was measured when the image was taken, it is generally unknown. Instead, we use the &lt;a href="https://google.github.io/mediapipe/solutions/face_mesh.html"&gt;MediaPipe Face Mesh&lt;/a&gt; to extract five landmark locations from the images. Each of these 2D predictions correspond to a semantically consistent point on the object (e.g., the tip of the nose or corners of the eyes). We can then derive a set of canonical 3D locations for the semantic points, along with estimates of the camera poses for each image, such that the projection of the canonical points into the images is as consistent as possible with the 2D landmarks. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLCIFNh7CJaU9hs0zwHA74_TVh7V8p-8XqA_3GLFtsc0aoCuCpwJuXLQNGmMwNQJJJ3LzjA9xwjEOtTsLRfePql74lcRruDbNA9_ZZ_c7oUbhir5BPcsooSootQWZSwFRjU-JT7sT88o0he59oQVQE7PWhR8phi7b3Ue6bwOFWjSEUFH9FM2Q61M2WBQ/s1280/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="488" data-original-width="1280" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLCIFNh7CJaU9hs0zwHA74_TVh7V8p-8XqA_3GLFtsc0aoCuCpwJuXLQNGmMwNQJJJ3LzjA9xwjEOtTsLRfePql74lcRruDbNA9_ZZ_c7oUbhir5BPcsooSootQWZSwFRjU-JT7sT88o0he59oQVQE7PWhR8phi7b3Ue6bwOFWjSEUFH9FM2Q61M2WBQ/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;We train a per-image table of latent codes alongside a NeRF model. Output is subject to per-ray RGB, mask and hardness losses. Cameras are derived from a fit of predicted landmarks to canonical 3D keypoints.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLec8X8kyx4B9n4jxlKctxlFRTfIARw4RBLAEb2pPQFiAINSppY_iv8mHtQTpBJPxS-cbgOvGF0RelF_8UTnHkW8MGv5PDRG6tjVl3aWFQQEXxpN4TcbqKo1g_VXeZt9NXC9NoR9otrlbPySVxmKqK6tg7w8uNWuQN53mhhc2F0OXsCRFCoEeCAuIxhg/s1999/image4.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="500" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLec8X8kyx4B9n4jxlKctxlFRTfIARw4RBLAEb2pPQFiAINSppY_iv8mHtQTpBJPxS-cbgOvGF0RelF_8UTnHkW8MGv5PDRG6tjVl3aWFQQEXxpN4TcbqKo1g_VXeZt9NXC9NoR9otrlbPySVxmKqK6tg7w8uNWuQN53mhhc2F0OXsCRFCoEeCAuIxhg/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Example MediaPipe landmarks and &lt;a href="https://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html"&gt;segmentation masks&lt;/a&gt; (images from &lt;a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"&gt;CelebA&lt;/a&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;        &lt;p&gt;&lt;b&gt;Hard Surface and Mask Losses&lt;/b&gt;&lt;br/&gt;Standard NeRF is effective for accurately reproducing the images, but in our single-view case, it tends to produce images that look blurry when viewed off-axis. To address this, we introduce a novel &lt;em&gt;hard surface&lt;/em&gt; loss, which encourages the density to adopt sharp transitions from exterior to interior regions, reducing blurring. This essentially tells the network to create “solid” surfaces, and not semi-transparent ones like clouds.  &lt;/p&gt;&lt;p&gt;We also obtained better results by splitting the network into separate foreground and background networks. We supervised this separation with a mask from the &lt;a href="https://google.github.io/mediapipe/solutions/selfie_segmentation.html"&gt;MediaPipe Selfie Segmenter&lt;/a&gt; and a loss to encourage network specialization. This allows the foreground network to specialize only on the object of interest, and not get “distracted” by the background, increasing its quality. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br/&gt;We surprisingly found that fitting only five key points gave accurate enough camera estimates to train a model for cats, dogs, or human faces. This means that given only a single view of your beloved cats Schnitzel, Widget and friends, you can create a new image from any other angle. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjby12a56iNW74Rhgi3Ezl3d4XS5IzkTWqwb2ChbW6dKjrmPHue9pt6Cfv6FfydeAHGz4MqpSSe5zU4kitKtg6Tcz3WM_StsPqnHhCmgrxWC6wzO72W8yWSq3h9w_6AXw33qJbrXB8DtDBjBB4dcNAVWWHzw1EyvvPoX1Lk9ghScesusOrQrFhQ0NTGng/s1024/image2.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="512" data-original-width="1024" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjby12a56iNW74Rhgi3Ezl3d4XS5IzkTWqwb2ChbW6dKjrmPHue9pt6Cfv6FfydeAHGz4MqpSSe5zU4kitKtg6Tcz3WM_StsPqnHhCmgrxWC6wzO72W8yWSq3h9w_6AXw33qJbrXB8DtDBjBB4dcNAVWWHzw1EyvvPoX1Lk9ghScesusOrQrFhQ0NTGng/s16000/image2.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Top&lt;/b&gt;: example cat images from &lt;a href="https://github.com/clovaai/stargan-v2"&gt;AFHQ&lt;/a&gt;. &lt;b&gt;Bottom&lt;/b&gt;: A synthesis of novel 3D views created by LOLNeRF.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;We’ve developed a technique that is effective at discovering 3D structure from single 2D images. We see great potential in LOLNeRF for a variety of applications and are currently investigating potential use-cases.  &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQaRH3ZtCh7_1YSriewIh5xGGz4ZjXqxoI4S77F5zkJ_Fy4vlREUOt88l3Bug9JvagFO1OjpoB3mf2sFhc5wMmzDSrj4-0_v5SUUnUJuBmYNBeG-dYRhC3m1qTe0AVtDHRQw0uEiSqVtmYkoeTbyJ8wWTHcOrw41hs7jveVnFZu8SVQ4eKu8GTMUCShw/s1024/image5.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="256" data-original-width="1024" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQaRH3ZtCh7_1YSriewIh5xGGz4ZjXqxoI4S77F5zkJ_Fy4vlREUOt88l3Bug9JvagFO1OjpoB3mf2sFhc5wMmzDSrj4-0_v5SUUnUJuBmYNBeG-dYRhC3m1qTe0AVtDHRQw0uEiSqVtmYkoeTbyJ8wWTHcOrw41hs7jveVnFZu8SVQ4eKu8GTMUCShw/s16000/image5.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Interpolation of feline identities from linear interpolation of learned latent codes for different examples in &lt;a href="https://github.com/clovaai/stargan-v2"&gt;AFHQ&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Code Release&lt;/b&gt;&lt;br/&gt;We acknowledge the potential for misuse and importance of acting responsibly. To that end, we will only release the code for reproducibility purposes, but will not release any trained generative models. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;We would like to thank Andrea Tagliasacchi, Kwang Moo Yi, Viral Carpenter, David Fleet, Danica Matthews, Florian Schroff, Hartwig Adam and Dmitry Lagun for continuous help in building this technology.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/2508264082683271000/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/lolnerf-learn-from-one-look.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2508264082683271000" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2508264082683271000" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/lolnerf-learn-from-one-look.html" rel="alternate" title="LOLNeRF: Learn from One Look" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_SG6q9CCEhSwhjXb7eBXXBiBBZNs6o2nE7paWjh5ThWpwSkhbANHxYfgx2w6c1C93C_p_GerQNgQXvbfTo9f6-vu0wcdyy0-EL2vdntANkpLQIKAw2A0Yw6ZLCoGHk36AXT1Oy9XliJG-Y8g0DFswTzy-90GtuI7r2G9aKe-9FVvXJKzhhMwBlI2OTA/s72-c/image3.png" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-368748851875317388</id><published>2022-09-09T12:25:00.001-07:00</published><updated>2022-10-21T06:31:23.519-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/><category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/><title type="text">Learning to Walk in the Wild from Terrain Semantics</title><content type="html">&lt;span class="byline-author"&gt;Posted by Yuxiang Yang, Student Researcher, Robotics at Google&lt;/span&gt; &lt;p&gt;An important promise for quadrupedal robots is their potential to operate in complex outdoor environments that are difficult or inaccessible for humans. Whether it’s to find natural resources deep in the mountains, or to search for life signals in heavily-damaged earthquake sites, a robust and versatile quadrupedal robot could be very helpful. To achieve that, a robot needs to perceive the environment, understand its locomotion challenges, and adapt its locomotion skill accordingly. While recent &lt;a href="https://proceedings.mlr.press/v164/yu22a.html"&gt;advances&lt;/a&gt; in perceptive locomotion have greatly enhanced the capability of quadrupedal robots, most works focus on indoor or urban environments, thus they cannot effectively handle the complexity of off-road terrains. In these environments, the robot needs to understand not only the terrain &lt;em&gt;shape&lt;/em&gt; (e.g., slope angle, smoothness), but also its &lt;em&gt;contact properties&lt;/em&gt; (e.g., friction, restitution, deformability), which are important for a robot to decide its locomotion skills. As existing perceptive locomotion systems mostly focus on the use of depth cameras or &lt;a href="https://en.wikipedia.org/wiki/Lidar"&gt;LiDARs&lt;/a&gt;, it can be difficult for these systems to estimate such terrain properties accurately. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2206.13631"&gt;Learning Semantics-Aware Locomotion Skills from Human Demonstrations&lt;/a&gt;”, we design a hierarchical learning framework to improve a robot’s ability to traverse complex, off-road environments. Unlike previous approaches that focus on environment &lt;em&gt;geometry&lt;/em&gt;, such as terrain shape and obstacle locations, we focus on environment &lt;em&gt;semantics&lt;/em&gt;, such as terrain type (grass, mud, etc.) and contact properties, which provide a complementary set of information useful for off-road environments. As the robot walks, the framework decides the locomotion skill, including the &lt;em&gt;speed&lt;/em&gt; and &lt;em&gt;gait&lt;/em&gt; (i.e., shape and timing of the legs’ movement) of the robot based on the perceived semantics, which allows the robot to walk robustly on a variety of off-road terrains, including rocks, pebbles, deep grass, mud, and more. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijyXAvjHybEWgkqkqJwshfCIH9RN_62HOw2dlALF4_qmjbk5PqRxxtaKG52O2rKM_V2gwZzhSLh4N3w_B7OT8yity2QOlZYDnh8OUymoXka6WiBfRxVPy7ov4CMk9I7GFez3IssfQWePcBWlI4Iu_VTe6DKFDwt1Pl91mw72H29Ydfr2of7Ve8TcRyxw/s600/image1.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="338" data-original-width="600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijyXAvjHybEWgkqkqJwshfCIH9RN_62HOw2dlALF4_qmjbk5PqRxxtaKG52O2rKM_V2gwZzhSLh4N3w_B7OT8yity2QOlZYDnh8OUymoXka6WiBfRxVPy7ov4CMk9I7GFez3IssfQWePcBWlI4Iu_VTe6DKFDwt1Pl91mw72H29Ydfr2of7Ve8TcRyxw/s16000/image1.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Our framework selects skills (gait and speed) of the robot from the camera RGB image. We first compute the speed from terrain semantics, and then select a gait based on the speed.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;br /&gt;The hierarchical framework consists of a high-level &lt;em&gt;skill policy&lt;/em&gt; and a low level &lt;em&gt;motor controller&lt;/em&gt;. The &lt;em&gt;skill policy &lt;/em&gt;selects a locomotion skill based on camera images, and the &lt;em&gt;motor controller&lt;/em&gt; converts the selected skill into motor commands. The high-level &lt;em&gt;skill policy &lt;/em&gt;is further decomposed into a learned &lt;em&gt;speed policy &lt;/em&gt;and a heuristic-based &lt;em&gt;gait selector&lt;/em&gt;. To decide a skill, the &lt;em&gt;speed policy&lt;/em&gt; first computes the desired forward speed, based on the semantic information from the onboard &lt;a href="https://en.wikipedia.org/wiki/RGB_color_model"&gt;RGB&lt;/a&gt; camera. For energy efficiency and robustness, quadrupedal robots usually &lt;a href="https://arxiv.org/pdf/2104.04644.pdf"&gt;select a different gait for each speed&lt;/a&gt;, so we designed the gait selector to compute a desired gait based on the forward speed. Lastly, a low-level convex &lt;a href="https://en.wikipedia.org/wiki/Model_predictive_control"&gt;model-predictive controller&lt;/a&gt; (MPC) converts the desired locomotion skill into motor &lt;a href="https://en.wikipedia.org/wiki/Torque"&gt;torque&lt;/a&gt; commands, and executes them on the real hardware. We train the speed policy directly in the real world using &lt;a href="https://arxiv.org/pdf/1811.06711.pdf"&gt;imitation learning&lt;/a&gt; because it requires fewer training data compared to standard &lt;a href="https://en.wikipedia.org/wiki/Q-learning"&gt;reinforcement learning&lt;/a&gt; algorithms. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqVdIFdPFrpMF3uHieeK0--h99M_9XFAqe_oWCiMXjr_FL_h90IdU0Pcl-amBI6aOVCesOE3YlWBb3MBF_ZQyTaAxlwqN5dRcFHmwuhdsyCIvGw5lkeg_68n99uhfq-88sS8n_ZfoYF6zpMSjCnLP-0gvikXfMW9ySI1bW0QuRspPs3J24dTGYBGEt8g/s1600/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="905" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqVdIFdPFrpMF3uHieeK0--h99M_9XFAqe_oWCiMXjr_FL_h90IdU0Pcl-amBI6aOVCesOE3YlWBb3MBF_ZQyTaAxlwqN5dRcFHmwuhdsyCIvGw5lkeg_68n99uhfq-88sS8n_ZfoYF6zpMSjCnLP-0gvikXfMW9ySI1bW0QuRspPs3J24dTGYBGEt8g/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The framework consists of a high-level skill policy and a low-level motor controller.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Learning Speed Command from Human Demonstrations&lt;/b&gt;&lt;br /&gt;As the central component in our pipeline, the &lt;em&gt;speed policy&lt;/em&gt; outputs the desired forward speed of the robot based on the RGB image from the onboard camera. Although many robot learning tasks can leverage &lt;a href="https://ai.googleblog.com/2021/07/speeding-up-reinforcement-learning-with.html"&gt;simulation&lt;/a&gt; as a source of lower-cost data collection, we train the speed policy in the real world because accurate simulation of complex and diverse off-road environments is not yet available. As policy learning in the real world is &lt;a href="https://arxiv.org/abs/2002.08550"&gt;time-consuming&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2022/05/learning-locomotion-skills-safely-in.html"&gt;potentially unsafe&lt;/a&gt;, we make two key design choices to improve the data efficiency and safety of our system. &lt;/p&gt;&lt;p&gt;The first is learning from human demonstrations. Standard reinforcement learning algorithms typically learn by &lt;em&gt;exploration&lt;/em&gt;, where the agent attempts different actions in an environment and builds preferences based on the rewards received. However, such explorations can be potentially unsafe, especially in off-road environments, since any robot failures can damage both the robot hardware and the surrounding environment. To ensure safety, we train the speed policy using imitation learning from human demonstrations. We first ask a human operator to teleoperate the robot on a variety of off-road terrains, where the operator controls the speed and heading of the robot using a remote joystick. Next, we collect the training data by storing (&lt;span style="font-family: Courier New"&gt;image, forward_speed&lt;/span&gt;) pairs. We then train the speed policy using standard supervised learning to predict the human operator’s speed command. As it turns out, the human demonstration is both safe and high-quality, and allows the robot to learn a proper speed choice for different terrains. &lt;/p&gt;&lt;p&gt;The second key design choice is the training method. Deep neural networks, especially those involving high-dimensional visual inputs, typically require lots of data to train. To reduce the amount of real-world training data required, we first pre-train a &lt;a href="https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html"&gt;semantic segmentation&lt;/a&gt; model on &lt;a href="http://rugd.vision/"&gt;RUGD&lt;/a&gt; (an off-road driving dataset where the images look similar to those captured by the robot’s onboard camera), where the model predicts the semantic class (grass, mud, etc.) for every pixel in the camera image. We then extract a &lt;em&gt;semantic embedding&lt;/em&gt; from the model’s intermediate layers and use that as the feature for on-robot training. With the pre-trained semantic embedding, we can train the speed policy effectively using less than 30 minutes of real-world data, which greatly reduces the amount of effort required. &lt;/p&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ0-v9xmVu_brD7wsuyA3uhPyXKRihExXM0KPK2WIrlfqcRr4ImZ1cWKfjeSvcBNfSDDuKYvZOWIc1IMJFGJrdplaxOkCeHM8POTM2a63-Nal2xNo57wGoZzDfYI2fdhQu_n4WdV2mfCTHdeHgY9yqyyXKvxhmo65eLsu8sH020TR9_e3ZYljMSLBRPQ/s1600/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="638" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZ0-v9xmVu_brD7wsuyA3uhPyXKRihExXM0KPK2WIrlfqcRr4ImZ1cWKfjeSvcBNfSDDuKYvZOWIc1IMJFGJrdplaxOkCeHM8POTM2a63-Nal2xNo57wGoZzDfYI2fdhQu_n4WdV2mfCTHdeHgY9yqyyXKvxhmo65eLsu8sH020TR9_e3ZYljMSLBRPQ/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;We pre-train a semantic segmentation model and extract a semantic embedding to be fine-tuned on robot data.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Gait Selection and Motor Control&lt;/b&gt;&lt;br /&gt;The next component in the pipeline, the &lt;em&gt;gait selector&lt;/em&gt;, computes the appropriate gait based on the speed command from the &lt;em&gt;speed policy&lt;/em&gt;. The &lt;a href="https://en.wikipedia.org/wiki/Horse_gait"&gt;gait&lt;/a&gt; of a robot, including its stepping frequency, swing height, and base height, can greatly affect the robot’s ability to traverse different terrains. &lt;/p&gt;&lt;p&gt;&lt;a href="https://www.nature.com/articles/292239a0"&gt;Scientific studies&lt;/a&gt; have shown that animals switch between different gaits at different speeds, and this result is further &lt;a href="https://arxiv.org/pdf/2104.04644.pdf"&gt;validated in quadrupedal robots&lt;/a&gt;, so we designed the gait selector to compute a robust gait for each speed. Compared to using a fixed gait across all speeds, we find that the gait selector further enhances the robot’s navigation performance on off-road terrains (more details in the paper). &lt;/p&gt;&lt;p&gt;The last component of the pipeline is a &lt;em&gt;motor controller&lt;/em&gt;, which converts the speed and gait commands into motor torques. Similar to &lt;a href="https://dspace.mit.edu/bitstream/handle/1721.1/138000/convex_mpc_2fix.pdf?sequence=2&amp;amp;isAllowed=y"&gt;previous work&lt;/a&gt;, we use separate control strategies for swing and stance legs. By separating the task of skill learning and motor control, the skill policy only needs to output the desired speed, and does not need to learn low-level locomotion controls, which greatly simplifies the learning process. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Experiment Results&lt;/b&gt;&lt;br /&gt;We implemented our framework on an &lt;a href="https://m.unitree.com/products/a1"&gt;A1 quadrupedal robot&lt;/a&gt; and tested it on an outdoor trail with multiple terrain types, including grass, gravel, and asphalt, which pose varying degrees of difficulty for the robot. For example, while the robot needs to walk slowly with high foot swings in deep grass to prevent its foot from getting stuck, on asphalt it can walk much faster with lower foot swings for better energy efficiency. Our framework captures such differences and selects an appropriate skill for each terrain type: slow speed (0.5m/s) on deep grass, medium speed (1m/s) on gravel, and high speed (1.4m/s) on asphalt. It completes the 460m-long trail in 9.6 minutes with an average speed of 0.8m/s (i.e., that’s 1.8 miles or 2.9 kilometers per hour). In contrast, non-adaptive policies either cannot complete the trail safely or walk significantly slower (0.5m/s), illustrating the importance of adapting locomotion skills based on the perceived environments. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzh6bSiilppYYmpPGC9P1nxhC8DSHfJIXJTvf2gF8Z17A3Lj-ecDVlCBpn3a4SCSVaIPK5Nt2uNE9HKmflPy5f6A0w87ARrrTXhUor7-MMv_1n9WBWtwft9DuUlYyQ_LnN2zxPh9omG_UKoI37BWsYKVHCKLVMUMJ_Fi3nTwZnVo6xx1wyY2X6ryqTPA/s1600/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="731" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzh6bSiilppYYmpPGC9P1nxhC8DSHfJIXJTvf2gF8Z17A3Lj-ecDVlCBpn3a4SCSVaIPK5Nt2uNE9HKmflPy5f6A0w87ARrrTXhUor7-MMv_1n9WBWtwft9DuUlYyQ_LnN2zxPh9omG_UKoI37BWsYKVHCKLVMUMJ_Fi3nTwZnVo6xx1wyY2X6ryqTPA/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The framework selects different speeds based on conditions of the trail.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To test generalizability, we also deployed the robot to a number of trails that are not seen during training. The robot traverses through all of them without failure, and adjusts its locomotion skills based on terrain semantics. In general, the skill policy selects a faster skill on rigid and flat terrains and a slower speed on deformable or uneven terrain. At the time of writing, the robot has traversed over 6km of outdoor trails without failure. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgte0hfMF3KCP7hKWEdTKEzyQfqslS4Mw4QqAy0Betl4kd_cH3LSlX9zvZ8Zz6vVYMk1Y_2Nz4LzbD_ht7CgpJQ1u--lSOgEI04KM09htPxZYc3GoKqV60ZpXz2GvtDQq_tlOppz9yMXN3CJM97mL_AjagdSjuATJHGjldm-Zm5sKlcSLx7SYjKAhc24A/s1600/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="394" data-original-width="1600" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgte0hfMF3KCP7hKWEdTKEzyQfqslS4Mw4QqAy0Betl4kd_cH3LSlX9zvZ8Zz6vVYMk1Y_2Nz4LzbD_ht7CgpJQ1u--lSOgEI04KM09htPxZYc3GoKqV60ZpXz2GvtDQq_tlOppz9yMXN3CJM97mL_AjagdSjuATJHGjldm-Zm5sKlcSLx7SYjKAhc24A/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikB_yft_Q0_WX9NTnQEuIJY2_ws-hVYiN6aRx_O2PN0zUpwSH8f_mbA_8hLLOi5dNrnfTjIhavVFa924b0Sczof99A0n9FvI175R7LfzEEzSwb9JxYlqH3ZtbVCuywteDAB6S8IL_NMz3S-6-HUUUgwZHZgydUx9D-cuBiQS2u68X6BUIIwrKGrT2CFw/s1972/image13.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="402" data-original-width="1972" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikB_yft_Q0_WX9NTnQEuIJY2_ws-hVYiN6aRx_O2PN0zUpwSH8f_mbA_8hLLOi5dNrnfTjIhavVFa924b0Sczof99A0n9FvI175R7LfzEEzSwb9JxYlqH3ZtbVCuywteDAB6S8IL_NMz3S-6-HUUUgwZHZgydUx9D-cuBiQS2u68X6BUIIwrKGrT2CFw/s16000/image13.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;With the framework, the robot walks safely on a variety of outdoor terrains not seen during training.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;In this work, we present a hierarchical framework to learn semantic-aware locomotion skills for off-road locomotion. Using less than 30 minutes of human demonstration data, the framework learns to adjust the speed and gait of the robot based on the perceived semantics of the environment. The robot can walk safely and efficiently on a wide variety of off-road terrains. One limitation of our framework is that it only adjusts locomotion skills for standard walking and does not support more agile behaviors such as jumping, which can be essential for traversing more difficult terrains with gaps or hurdles. Another limitation is that our framework currently requires manual steering commands to follow a desired path and reach the goal. In future work, we plan to look into a deeper integration of high-level skill policy with the low-level controller for more agile behaviors, and incorporate navigation and path planning into the framework so that the robot can operate fully autonomously in challenging off-road environments. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to thank our paper co-authors: Xiangyun Meng, Wenhao Yu, Tingnan Zhang, Jie Tan, &lt;/em&gt;and&lt;em&gt; Byron Boots. We would also like to thank the team members of Robotics at Google for discussions and feedback.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/368748851875317388/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/learning-to-walk-in-wild-from-terrain.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/368748851875317388" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/368748851875317388" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/learning-to-walk-in-wild-from-terrain.html" rel="alternate" title="Learning to Walk in the Wild from Terrain Semantics" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijyXAvjHybEWgkqkqJwshfCIH9RN_62HOw2dlALF4_qmjbk5PqRxxtaKG52O2rKM_V2gwZzhSLh4N3w_B7OT8yity2QOlZYDnh8OUymoXka6WiBfRxVPy7ov4CMk9I7GFez3IssfQWePcBWlI4Iu_VTe6DKFDwt1Pl91mw72H29Ydfr2of7Ve8TcRyxw/s72-c/image1.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-1486873953874941813</id><published>2022-09-08T10:43:00.001-07:00</published><updated>2022-10-21T06:31:24.866-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><title type="text">A Multi-Axis Approach for Vision Transformer and MLP Models</title><content type="html">&lt;span class="byline-author"&gt;Posted by Zhengzhong Tu and Yinxiao Li, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;Convolutional neural networks&lt;/a&gt; have been the dominant machine learning architecture for computer vision since the introduction of &lt;a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"&gt;AlexNet&lt;/a&gt; in 2012. Recently, inspired by the evolution of &lt;a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"&gt;Transformers&lt;/a&gt; in &lt;a href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;natural language processing&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)"&gt;attention&lt;/a&gt; mechanisms have been prominently incorporated into vision models. These attention methods boost some parts of the input data while minimizing other parts so that the network can focus on small but important parts of the data. The &lt;a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"&gt;Vision Transformer&lt;/a&gt; (ViT) has created a new landscape of model designs for computer vision that is completely free of convolution. ViT regards image patches as a sequence of words, and applies a Transformer encoder on top. When trained on &lt;a href="https://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html"&gt;sufficiently large datasets&lt;/a&gt;, ViT demonstrates compelling performance on image recognition.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;While convolutions and attention are both sufficient for good performance, neither of them are necessary. For example, &lt;a href="https://arxiv.org/pdf/2105.01601.pdf"&gt;MLP-Mixer&lt;/a&gt; adopts a simple &lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;multi-layer perceptron&lt;/a&gt; (MLP) to mix image patches across all the spatial locations, resulting in an all-MLP architecture. It is a competitive alternative to existing state-of-the-art vision models in terms of the trade-off between accuracy and computation required for training and inference. However, both ViT and the MLP models struggle to scale to higher input resolution because the computational complexity increases quadratically with respect to the image size.  &lt;/p&gt;&lt;p&gt;Today we present a new multi-axis approach that is simple and effective, improves on the original ViT and MLP models, can better adapt to high-resolution, dense prediction tasks, and can naturally adapt to different input sizes with high flexibility and low complexity. Based on this approach, we have built two backbone models for high-level and low-level vision tasks. We describe the first in “&lt;a href="https://arxiv.org/pdf/2204.01697.pdf"&gt;MaxViT: Multi-Axis Vision Transformer&lt;/a&gt;”, to be presented in &lt;a href="https://eccv2022.ecva.net/"&gt;ECCV 2022&lt;/a&gt;, and show it significantly improves the state of the art for high-level tasks, such as image classification, object detection, segmentation, quality assessment, and generation. The second, presented in “&lt;a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.pdf"&gt;MAXIM: Multi-Axis MLP for Image Processing&lt;/a&gt;” at &lt;a href="https://cvpr2022.thecvf.com/"&gt;CVPR 2022&lt;/a&gt;, is based on a &lt;a href="https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf"&gt;UNet&lt;/a&gt;-like architecture and achieves competitive performance on low-level imaging tasks including denoising, deblurring, dehazing, deraining, and low-light enhancement. To facilitate further research on efficient Transformer and MLP models, we have open-sourced the code and models for both &lt;a href="https://github.com/google-research/maxvit"&gt;MaxViT&lt;/a&gt; and &lt;a href="https://github.com/google-research/maxim"&gt;MAXIM&lt;/a&gt;.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgTOeCdzkEPhjPrjeWU0UfHqbdz-5_ZR_BC_H7MWebdU_ZIrVQUFOqJzkEsOWY0GJSJO7H3UkmoqbQPlbpnFZa-1R8Fk72ZbFY8c_cRpHLS-d8qD7AIV-WQVMJ7k8R7b_UbR4Uwmax4w-gWwHWMwvWZ_e-Lwho9VG-V0kcxtxM3DqBH8DfSO10X1NewRg/s640/image2.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="360" data-original-width="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgTOeCdzkEPhjPrjeWU0UfHqbdz-5_ZR_BC_H7MWebdU_ZIrVQUFOqJzkEsOWY0GJSJO7H3UkmoqbQPlbpnFZa-1R8Fk72ZbFY8c_cRpHLS-d8qD7AIV-WQVMJ7k8R7b_UbR4Uwmax4w-gWwHWMwvWZ_e-Lwho9VG-V0kcxtxM3DqBH8DfSO10X1NewRg/s16000/image2.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A demo of image deblurring using MAXIM frame by frame.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;br /&gt;Our new approach is based on multi-axis attention, which decomposes the full-size attention (each pixel attends to all the pixels) used in ViT into two sparse forms — local and (sparse) global. As shown in the figure below, the multi-axis attention contains a sequential stack of block attention and grid attention. The block attention works within non-overlapping windows (small patches in intermediate feature maps) to capture local patterns, while the grid attention works on a sparsely sampled uniform grid for long-range (global) interactions. The window sizes of grid and block attentions can be fully controlled as hyperparameters to ensure a linear computational complexity to the input size.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgy3hoIJicaOPinAtGT3d7EKKf5gH_T16QFe-MR0TbIMAtwiKHOUFSX_jcwTF3CqXLOqTFy_MY5N7yQIBrr7rfbVM78NdCFEs-Wk49P3zO48BmLpM7OucY7HSPl7io9-JWIIj-oulNKttjkbSAyoawEeDDSSlnS-MYtEyWc23K_EDrzbdmnfEs4aP7amw/s999/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="237" data-original-width="999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgy3hoIJicaOPinAtGT3d7EKKf5gH_T16QFe-MR0TbIMAtwiKHOUFSX_jcwTF3CqXLOqTFy_MY5N7yQIBrr7rfbVM78NdCFEs-Wk49P3zO48BmLpM7OucY7HSPl7io9-JWIIj-oulNKttjkbSAyoawEeDDSSlnS-MYtEyWc23K_EDrzbdmnfEs4aP7amw/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The proposed multi-axis attention conducts blocked local and dilated global attention sequentially followed by a &lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"&gt;FFN&lt;/a&gt;, with only a linear complexity. The pixels in the same colors are attended together.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Such low-complexity attention can significantly improve its wide applicability to many vision tasks, especially for high-resolution visual predictions, demonstrating greater generality than the original attention used in ViT. We build two backbone instantiations out of this multi-axis attention approach – MaxViT and MAXIM, for high-level and low-level tasks, respectively. &lt;/p&gt;&lt;p&gt;&lt;b&gt;MaxViT&lt;/b&gt;&lt;br /&gt;In MaxViT, we first build a single MaxViT block (shown below) by concatenating MBConv (proposed by &lt;a href="https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html"&gt;EfficientNet&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2104.00298.pdf"&gt;V2&lt;/a&gt;) with the multi-axis attention. This single block can encode local and global visual information regardless of input resolution. We then simply stack repeated blocks composed of attention and convolutions in a hierarchical architecture (similar to &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf"&gt;ResNet&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2106.04803.pdf"&gt;CoAtNet&lt;/a&gt;), yielding our homogenous MaxViT architecture. Notably, MaxViT is distinguished from previous hierarchical approaches as it can “see” globally throughout the entire network, even in earlier, high-resolution stages, demonstrating stronger model capacity on various tasks.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRJZAcZ1K5REHwhtTptulU2I2DO9x7p-0RJnEQcmEpSN8gSprcxz8ieHM5yKxZ3U1YlO_23UW5rRfhZKmN6NWB94XafW3X25nrOtCxfZOVaIK0dsnwxuMXAuDOJFGSAw52wqy0_2l5gCDMS-ZFBmGoSzpEXPPxF4uSEMlTyuDBGxQtKRXvQIrU8U4UXQ/s1818/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="806" data-original-width="1818" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRJZAcZ1K5REHwhtTptulU2I2DO9x7p-0RJnEQcmEpSN8gSprcxz8ieHM5yKxZ3U1YlO_23UW5rRfhZKmN6NWB94XafW3X25nrOtCxfZOVaIK0dsnwxuMXAuDOJFGSAw52wqy0_2l5gCDMS-ZFBmGoSzpEXPPxF4uSEMlTyuDBGxQtKRXvQIrU8U4UXQ/s16000/image6.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The meta-architecture of MaxViT.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;MAXIM&lt;/b&gt;&lt;br /&gt;Our second backbone, &lt;a href="https://arxiv.org/pdf/2201.02973.pdf"&gt;MAXIM&lt;/a&gt;, is a generic &lt;a href="https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf"&gt;UNet&lt;/a&gt;-like architecture tailored for low-level image-to-image prediction tasks. MAXIM explores parallel designs of the local and global approaches using the &lt;a href="https://arxiv.org/pdf/2105.08050.pdf"&gt;gated multi-layer perceptron&lt;/a&gt; (gMLP) network (patching-mixing MLP with a &lt;a href="http://proceedings.mlr.press/v70/dauphin17a/dauphin17a.pdf"&gt;gating mechanism&lt;/a&gt;). Another contribution of MAXIM is the cross-gating block that can be used to apply interactions between two different input signals. This block can serve as an efficient alternative to the &lt;a href="https://arxiv.org/abs/1706.03762"&gt;cross-attention&lt;/a&gt; module as it only employs the cheap gated MLP operators to interact with various inputs without relying on the computationally heavy cross-attention. Moreover, all the proposed components including the gated MLP and cross-gating blocks in MAXIM enjoy linear complexity to image size, making it even more efficient when processing high-resolution pictures. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;We demonstrate the effectiveness of MaxViT on a broad range of vision tasks. On image classification, MaxViT achieves state-of-the-art results under various settings: with only &lt;a href="https://www.image-net.org/"&gt;ImageNet-1K&lt;/a&gt; training, MaxViT attains 86.5% top-1 accuracy; with &lt;a href="https://www.image-net.org/"&gt;ImageNet-21K&lt;/a&gt; (14M images, 21k classes) pre-training, MaxViT achieves 88.7% top-1 accuracy; and with &lt;a href="https://arxiv.org/abs/1707.02968"&gt;JFT&lt;/a&gt; (300M images, 18k classes) pre-training, our largest model MaxViT-XL achieves a high accuracy of 89.5% with 475M parameters.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEio_MxdnD_XsqJ1JU-OssjoNkLHGzShDwwT6g1udUkR7vhLXAv17qwiUIRSGmNwWThGzYNqONyNtG_rjAqjhqHInK1ea4cqZy8BYMAHrMNRGBZEsZCQUcFPMxuOnNcHx1fBrXMMIOSQtpZzEEMC0oe6iS62VLJTu2lxrE5f4X2WcViYbZINYqQ7mTx2Pg/s818/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="635" data-original-width="818" height="310" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEio_MxdnD_XsqJ1JU-OssjoNkLHGzShDwwT6g1udUkR7vhLXAv17qwiUIRSGmNwWThGzYNqONyNtG_rjAqjhqHInK1ea4cqZy8BYMAHrMNRGBZEsZCQUcFPMxuOnNcHx1fBrXMMIOSQtpZzEEMC0oe6iS62VLJTu2lxrE5f4X2WcViYbZINYqQ7mTx2Pg/w400-h310/image5.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheIg7kEQNDd0Bq87HjzZbAIgssxBHZlF2SX3muy4DdwMeHKRRzeCRmCgjfGQMSJJkOophDmFBroj1RC9I_eT_x5VFdRpjAga73GknJu7XrQ9E0ptxiHrtonSZeWjmmElUHWCNKQuahKIYLykWqKhkab5gRJ7rpN-lVw_PeoemfNd04Z4LnoNY95xxMhw/s820/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="633" data-original-width="820" height="309" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheIg7kEQNDd0Bq87HjzZbAIgssxBHZlF2SX3muy4DdwMeHKRRzeCRmCgjfGQMSJJkOophDmFBroj1RC9I_eT_x5VFdRpjAga73GknJu7XrQ9E0ptxiHrtonSZeWjmmElUHWCNKQuahKIYLykWqKhkab5gRJ7rpN-lVw_PeoemfNd04Z4LnoNY95xxMhw/w400-h309/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Performance comparison of MaxViT with state-of-the-art models on ImageNet-1K. &lt;b&gt;Top&lt;/b&gt;: Accuracy vs. FLOPs performance scaling with 224x224 image resolution. &lt;b&gt;Bottom&lt;/b&gt;: Accuracy vs. parameters scaling curve under ImageNet-1K fine-tuning setting.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For downstream tasks, MaxViT as a backbone delivers favorable performance on a broad spectrum of tasks. For object detection and segmentation on the &lt;a href="https://cocodataset.org/#home"&gt;COCO&lt;/a&gt; dataset, the MaxViT backbone achieves 53.4 &lt;a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)"&gt;AP&lt;/a&gt;, outperforming other base-level models while requiring only about 60% the computational cost. For image aesthetics assessment, the MaxViT model advances the state-of-the-art &lt;a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ke_MUSIQ_Multi-Scale_Image_Quality_Transformer_ICCV_2021_paper.pdf"&gt;MUSIQ&lt;/a&gt; model by 3.5% in terms of linear correlation with human opinion scores. The standalone MaxViT building block also demonstrates effective performance on image generation, achieving better &lt;a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance"&gt;FID&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Inception_score"&gt;IS&lt;/a&gt; scores on the ImageNet-1K unconditional generation task with a significantly lower number of parameters than the state-of-the-art model, &lt;a href="https://arxiv.org/pdf/2106.07631.pdf"&gt;HiT&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The UNet-like MAXIM backbone, customized for image processing tasks, has also demonstrated state-of-the-art results on 15 out of 20 tested datasets, including denoising, deblurring, deraining, dehazing, and low-light enhancement, while requiring fewer or comparable number of parameters and FLOPs than competitive models. Images restored by MAXIM show more recovered details with less visual artifacts. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjx2dIN8Tp5n9s40k-HGnI8_KNygWKnT_0O8lCJ-Sc39PPFiix1fUMBTVgb_8avoiIyX71Ox6L-cIj0ZejxbWGv5xmtpCkb4NtuPfLULLddW3WBYVReXc5Q341zFZQS2EaTSGQI-NmCTAIgY1wLKWXDOPtU-x1E77xwWjLMFseFhQMPnennECauuTyZog/s1292/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="630" data-original-width="1292" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjx2dIN8Tp5n9s40k-HGnI8_KNygWKnT_0O8lCJ-Sc39PPFiix1fUMBTVgb_8avoiIyX71Ox6L-cIj0ZejxbWGv5xmtpCkb4NtuPfLULLddW3WBYVReXc5Q341zFZQS2EaTSGQI-NmCTAIgY1wLKWXDOPtU-x1E77xwWjLMFseFhQMPnennECauuTyZog/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Visual results of MAXIM for image deblurring, deraining, and low-light enhancement.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Summary&lt;/b&gt;&lt;br /&gt;Recent works in the last two or so years have shown that ConvNets and Vision Transformers can achieve similar performance. Our work presents a unified design that takes advantage of the best of both worlds — efficient convolution and sparse attention — and demonstrates that a model built on top, namely MaxViT, can achieve state-of-the-art performance on a variety of vision tasks. More importantly, MaxViT scales well to very large data sizes. We also show that an alternative multi-axis design using MLP operators, MAXIM, achieves state-of-the-art performance on a broad range of low-level vision tasks.  &lt;/p&gt;&lt;p&gt;Even though we present our models in the context of vision tasks, the proposed multi-axis approach can easily extend to language modeling to capture both local and global dependencies in linear time. Motivated by the work here, we expect that it is worthwhile to study other forms of sparse attention in higher-dimensional or multimodal signals such as videos, point clouds, and vision-language models. &lt;/p&gt;&lt;p&gt;We have open-sourced the code and models of &lt;a href="https://github.com/google-research/maxim"&gt;MAXIM&lt;/a&gt; and &lt;a href="https://github.com/google-research/maxvit"&gt;MaxViT&lt;/a&gt; to facilitate future research on efficient attention and MLP models. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to thank our co-authors: Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, and Alan Bovik. We would also like to acknowledge the valuable discussion and support from Xianzhi Du, Long Zhao, Wuyang Chen, Hanxiao Liu, Zihang Dai, Anurag Arnab, Sungjoon Choi, Junjie Ke, Mauricio Delbracio, Irene Zhu, Innfarn Yoo, Huiwen Chang, and Ce Liu.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/1486873953874941813/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1486873953874941813" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/1486873953874941813" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html" rel="alternate" title="A Multi-Axis Approach for Vision Transformer and MLP Models" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgTOeCdzkEPhjPrjeWU0UfHqbdz-5_ZR_BC_H7MWebdU_ZIrVQUFOqJzkEsOWY0GJSJO7H3UkmoqbQPlbpnFZa-1R8Fk72ZbFY8c_cRpHLS-d8qD7AIV-WQVMJ7k8R7b_UbR4Uwmax4w-gWwHWMwvWZ_e-Lwho9VG-V0kcxtxM3DqBH8DfSO10X1NewRg/s72-c/image2.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-7891790943858040956</id><published>2022-09-06T10:35:00.006-07:00</published><updated>2022-10-21T06:31:25.826-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="Biology"/><category scheme="http://www.blogger.com/atom/ns#" term="Chemistry"/><title type="text">Digitizing Smell: Using Molecular Maps to Understand Odor</title><content type="html">&lt;span class="byline-author"&gt;Posted by Richard C. Gerkin, Google Research, and Alexander B. Wiltschko, Google&lt;/span&gt; &lt;p&gt;&lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="font-size: small; margin-left: auto; margin-right: auto; text-align: center; width: 90%;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: left;"&gt;&lt;em&gt;Did you ever try to measure a smell? …Until you can measure their likenesses and differences you can have no science of odor. If you are ambitious to found a new science, measure a smell.&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td style="text-align: right;"&gt;— Alexander Graham Bell, 1914.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;How &lt;em&gt;can&lt;/em&gt; we measure a smell? Smells are produced by molecules that waft through the air, enter our noses, and bind to sensory receptors. Potentially &lt;a href="https://doi.org/10.1073/pnas.2116576119"&gt;billions of molecules&lt;/a&gt; can produce a smell, so figuring out which ones produce which smells is difficult to catalog or predict. Sensory &lt;em&gt;maps&lt;/em&gt; can help us solve this problem.  Color vision has the most familiar examples of these maps, from the color wheel we each learn in primary school to more sophisticated variants used to perform color correction in video production. While these maps have existed for centuries, useful maps for smell have been missing, because smell is a harder problem to crack: molecules vary in many more ways than photons do; data collection requires physical proximity between the smeller and smell (we don’t have good smell “cameras” and smell “monitors”); and the human eye only has three sensory receptors for color while the human nose has &amp;gt; 300 for odor. As a result, previous efforts to produce odor maps have failed to gain traction. &lt;/p&gt;&lt;p&gt;In 2019, &lt;a href="https://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html"&gt;we developed&lt;/a&gt; a graph neural network (GNN) &lt;a href="https://arxiv.org/abs/1910.10685"&gt;model&lt;/a&gt; that began to explore thousands of examples of distinct molecules paired with the smell labels that they evoke, e.g., “beefy”, “floral”, or “minty”, to learn the relationship between a molecule’s structure and the probability that such a molecule would have each smell label.  The embedding space of this model contains a representation of each molecule as a fixed-length vector describing that molecule in terms of its odor, much as the RGB value of a visual stimulus describes its color. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIKw_aiWx5tcRb73eI1fHs1oe2GSiZJVH2kV5r36E_TvpdDA09nWAiLuKPQs7jzM5OXtkDreuJQYgYnEp7EtRFVCGmack-ueAhkZZJTW3OTlfgQE6Hjdy-NW_bCGBlB9ssewPYDZ8E1bydgZMfRxW6sjByxiyWN9QxagjcSTSFQPLEP9hVQt6dxij0Xw/s1999/image5.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1061" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIKw_aiWx5tcRb73eI1fHs1oe2GSiZJVH2kV5r36E_TvpdDA09nWAiLuKPQs7jzM5OXtkDreuJQYgYnEp7EtRFVCGmack-ueAhkZZJTW3OTlfgQE6Hjdy-NW_bCGBlB9ssewPYDZ8E1bydgZMfRxW6sjByxiyWN9QxagjcSTSFQPLEP9hVQt6dxij0Xw/s16000/image5.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left:&lt;/b&gt; An example of a color map (&lt;a href="https://en.wikipedia.org/wiki/CIE_1931_color_space"&gt;CIE 1931&lt;/a&gt;) in which coordinates can be directly translated into values for hue and saturation. Similar colors lie near each other, and specific wavelengths of light (and combinations thereof) can be identified with positions on the map. &lt;b&gt;Right:&lt;/b&gt; Odors in the Principal Odor Map operate similarly. Individual molecules correspond to points (grey), and the locations of these points reflect predictions of their odor character.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Today we introduce the “Principal Odor Map” (POM), which identifies the vector representation of each odorous molecule in the model’s embedding space as a single point in a high-dimensional space. The POM has the properties of a sensory map: first, pairs of perceptually similar odors correspond to two nearby points in the POM (by analogy, red is nearer to orange than to green on the color wheel). Second, the POM enables us to predict and discover new odors and the molecules that produce them. In a series of papers, we demonstrate that the map can be used to prospectively predict the odor properties of molecules, understand these properties in terms of fundamental biology, and tackle pressing global health problems. We discuss each of these promising applications of the POM and how we test them below. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Test 1: Challenging the Model with Molecules Never Smelled Before&lt;/b&gt;&lt;br /&gt;First, we asked if the underlying model could correctly predict the odors of &lt;em&gt;new&lt;/em&gt; molecules that no one had ever smelled before and that were very &lt;em&gt;different&lt;/em&gt; from molecules used during model development. This is an important test — many models perform well on data that looks similar to what the model has seen before, but break down when tested on novel cases. &lt;/p&gt;&lt;p&gt;To test this, we collected the largest ever dataset of odor descriptions for novel molecules. Our partners at the &lt;a href="https://monell.org/"&gt;Monell Center&lt;/a&gt; trained panelists to rate the smell of each of 400 molecules using 55 distinct labels (e.g., “minty”) that were selected to cover the space of possible smells while being neither redundant nor too sparse. Unsurprisingly, we found that different people had different characterizations of the same molecule. This is why sensory research typically uses panels of dozens or hundreds of people and highlights why smell is a hard problem to solve. Rather than see if the model could match any one person, we asked how close it was to the consensus: the average across all of the panelists. &lt;a href="http://doi.org/10.1101/2022.09.01.504602"&gt;We found&lt;/a&gt;&amp;nbsp;that the predictions of the model were closer to the consensus than the average panelist was. In other words, the model demonstrated an exceptional ability to predict odor from a molecule’s structure. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 10%; margin-right: 15%;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilxKA31YxHf0DHxp3K_P-coF-Ko_lhh1TYNk1yK1NERdKdO2jquFGDEoqdEt8RWDA1zWS-aGuot4eikOnMdc4vtrTP1gVP51MgqK0FmmjHUfrBpGyGnW7-GTT47h-Vxayrk4-rDGdfrdnhjj0CklZP4RGIAII3__SwPA4tLu1Y8k68pJ24IWi7nBcK2w/s4404/Figure%202a%20VSP%20upper.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="4404" data-original-width="3426" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilxKA31YxHf0DHxp3K_P-coF-Ko_lhh1TYNk1yK1NERdKdO2jquFGDEoqdEt8RWDA1zWS-aGuot4eikOnMdc4vtrTP1gVP51MgqK0FmmjHUfrBpGyGnW7-GTT47h-Vxayrk4-rDGdfrdnhjj0CklZP4RGIAII3__SwPA4tLu1Y8k68pJ24IWi7nBcK2w/s16000/Figure%202a%20VSP%20upper.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Predictions made by two models, our GNN model (orange) and a baseline chemoinformatic random forest (RF) model (blue), compared with the mean ratings given by trained panelists (green) for the molecule 2,3-dihydrobenzofuran-5-carboxaldehyde. Each bar corresponds to one odor character label (with only the top 17 of 55 shown for clarity). The top five are indicated in color; our model correctly identifies four of the top five, with high confidence, vs. only three of five, with low confidence, for the RF model. The correlation (R) to the full set of 55 labels is also higher in our model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUAv6dkHdKPQxv0n6R-hF9a_ysr3T86wzyCteeX_9ma6j3r_-38PTiAScvXeyfIYmi9_XT_Ir7Ny6lz4aeghig2D6-MjZrxaMyFMII9cE0SE6niaRS7E3bWPvcgptEiqjTlemq7oY4xeCmsluSxyJq5Ii8Tfk2tueC2xLzMMTN_kdA5WNnvDf5oRhCaA/s1227/image4.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1227" data-original-width="921" height="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUAv6dkHdKPQxv0n6R-hF9a_ysr3T86wzyCteeX_9ma6j3r_-38PTiAScvXeyfIYmi9_XT_Ir7Ny6lz4aeghig2D6-MjZrxaMyFMII9cE0SE6niaRS7E3bWPvcgptEiqjTlemq7oY4xeCmsluSxyJq5Ii8Tfk2tueC2xLzMMTN_kdA5WNnvDf5oRhCaA/w480-h640/image4.jpg" width="480" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Predictions made by two models, our GNN model (orange) and a baseline chemoinformatic random forest (RF) model (blue), compared with the mean ratings given by trained panelists (green) for the molecule 2,3-dihydrobenzofuran-5-carboxaldehyde. Each bar corresponds to one odor character label (with only the top 17 of 55 shown for clarity). The top five are indicated in color; our model correctly identifies four of the top five, with high confidence, vs. only three of five, with low confidence, for the RF model. The correlation (R) to the full set of 55 labels is also higher in our model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: 10%; margin-right: 10%;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCa2WrJ9xL_L3ClTD8VFAsENbrUH1IH6tUPBfV2Db38fhq8yl6sGJsEQBrQIpmgMZZ9jHvzB7yoHYIEDhQPAGEG6bYPzII1kQKMEtqeo6gUQokyUVdjNpkouX8nXFkh750qbzHVuQ_71543DqOflhf5oX8iXbEQHqqPEF0pdmlA4ssbGbmkFG-UMfTQg/s1494/Blog%20Post%20Figure%202b_%20VSP%20Paper%20Lower-crop.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="1174" data-original-width="1494" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiCa2WrJ9xL_L3ClTD8VFAsENbrUH1IH6tUPBfV2Db38fhq8yl6sGJsEQBrQIpmgMZZ9jHvzB7yoHYIEDhQPAGEG6bYPzII1kQKMEtqeo6gUQokyUVdjNpkouX8nXFkh750qbzHVuQ_71543DqOflhf5oX8iXbEQHqqPEF0pdmlA4ssbGbmkFG-UMfTQg/s16000/Blog%20Post%20Figure%202b_%20VSP%20Paper%20Lower-crop.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Unlike alternative benchmark models (RF and nearest-neighbor models trained on various sets of chemoinformatic features), our GNN model outperforms the median human panelist at predicting the panel mean rating.  In other words, our GNN model better reflects the panel consensus than the typical panelist.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicX0ATUWPq0ztp12BEuB4I60Ij7Thfbkwyic6pdpzPc0INDwy8X8Rx12OIdVIbhvW96kx9ddMGEvLzcfFhO7spyitYUmbRUWDDzBWF720V8Ht321YaGH3ftP-hdMaAcRXc-7tRncoVqzxZShjDWkWDwrJx9VE917QaDvMQDcM9V60jmwgIcKrsJ3_ohw/s921/image2.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="750" data-original-width="921" height="326" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicX0ATUWPq0ztp12BEuB4I60Ij7Thfbkwyic6pdpzPc0INDwy8X8Rx12OIdVIbhvW96kx9ddMGEvLzcfFhO7spyitYUmbRUWDDzBWF720V8Ht321YaGH3ftP-hdMaAcRXc-7tRncoVqzxZShjDWkWDwrJx9VE917QaDvMQDcM9V60jmwgIcKrsJ3_ohw/w400-h326/image2.jpg" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Unlike alternative benchmark models (RF and nearest-neighbor models trained on various sets of chemoinformatic features), our GNN model outperforms the median human panelist at predicting the panel mean rating.  In other words, our GNN model better reflects the panel consensus than the typical panelist.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;  &lt;p&gt;The POM also exhibited state-of-the-art performance on alternative human olfaction tasks like detecting the strength of a smell or the similarity of different smells. Thus, with the POM, it should be possible to predict the odor qualities of any of billions of as-yet-unknown odorous molecules, with broad applications to flavor and fragrance. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Test 2: Linking Odor Quality Back to Fundamental Biology&lt;/b&gt;&lt;br /&gt;Because the Principal Odor Map was useful in predicting human odor perception, we asked whether it could also predict odor perception in animals, and the brain activity that underlies it. &lt;a href="https://www.biorxiv.org/content/10.1101/2022.07.21.500995"&gt;We found&lt;/a&gt; that the map could successfully predict the activity of sensory receptors, neurons, and behavior in most animals that olfactory neuroscientists have studied, including mice and insects.   &lt;/p&gt;&lt;p&gt;What common feature of the natural world makes this map applicable to species separated by hundreds of millions of years of evolution? We realized that the common purpose of the ability to smell might be to detect and discriminate between metabolic states, i.e., to sense when something is ripe vs. rotten, nutritious vs. inert, or healthy vs. sick. We gathered data about metabolic reactions in dozens of species across the kingdoms of life and found that the map corresponds closely to metabolism itself. When two molecules are far apart in odor, according to the map, a long series of metabolic reactions is required to convert one to the other; by contrast, similarly smelling molecules are separated by just one or a few reactions. Even long reaction pathways containing many steps trace smooth paths through the map. And molecules that co-occur in the same natural substances (e.g., an orange) are often very tightly clustered on the map. The POM shows that olfaction is linked to our natural world through the structure of metabolism and, perhaps surprisingly, captures fundamental principles of biology. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEityR5nHqNm_JKdpe21epV_vjktTvZNuCerCvfZx55JuyJbOLIMbn9BlFn-LvluNwhP6OCqQAzJMLC0SL8pBn3u0IqO_l4sxRlz-2Casj7gZ23VULJzyj4civnYtf837_wykGpUlNwp60nSYgq7pzmO3_EIDaCQoeRnD5_0Qldm7Z-oZTlJ-5iCnIAM_A/s2284/metabolic.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1146" data-original-width="2284" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEityR5nHqNm_JKdpe21epV_vjktTvZNuCerCvfZx55JuyJbOLIMbn9BlFn-LvluNwhP6OCqQAzJMLC0SL8pBn3u0IqO_l4sxRlz-2Casj7gZ23VULJzyj4civnYtf837_wykGpUlNwp60nSYgq7pzmO3_EIDaCQoeRnD5_0Qldm7Z-oZTlJ-5iCnIAM_A/s16000/metabolic.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left:&lt;/b&gt; We aggregated metabolic reactions found in 17 species across 4 kingdoms to construct a metabolic graph. In this illustration, each circle is a distinct metabolite molecule and an arrow indicates that there is a metabolic reaction that converts one molecule to another. Some metabolites have an odor (color) and others do not (gray), and the metabolic distance between two odorous metabolites is the minimum number of reactions necessary to convert one into the other. In the path shown in bold, the distance is 3. &lt;b&gt;Right:&lt;/b&gt; Metabolic distance was highly correlated with distance in the POM, an estimate of perceived odor dissimilarity.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKd1jaSm3rrkDcGUsRqHgMJZvftyHeti4gNKstYvwKL8-8iFsT7_y4a4LgnVhxkaayktKpp7SzddFFRrx9x5VUX81fZf25Efczhp58lPkkqlxfRrAViGj5c_TtEZxRakNt5tgcv6tHd2XqUYijZCPWn7iFeK6mzf4FJWm3E8I9VQgYlIsLKp3ZFY1AeQ/s1999/image3.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="970" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKd1jaSm3rrkDcGUsRqHgMJZvftyHeti4gNKstYvwKL8-8iFsT7_y4a4LgnVhxkaayktKpp7SzddFFRrx9x5VUX81fZf25Efczhp58lPkkqlxfRrAViGj5c_TtEZxRakNt5tgcv6tHd2XqUYijZCPWn7iFeK6mzf4FJWm3E8I9VQgYlIsLKp3ZFY1AeQ/s16000/image3.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left:&lt;/b&gt; We aggregated metabolic reactions found in 17 species across 4 kingdoms to construct a metabolic graph. In this illustration, each circle is a distinct metabolite molecule and an arrow indicates that there is a metabolic reaction that converts one molecule to another. Some metabolites have an odor (color) and others do not (gray), and the metabolic distance between two odorous metabolites is the minimum number of reactions necessary to convert one into the other. In the path shown in bold, the distance is 3. &lt;b&gt;Right:&lt;/b&gt; Metabolic distance was highly correlated with distance in the POM, an estimate of perceived odor dissimilarity.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;&lt;b&gt;Test 3: Extending the Model to Tackle a Global Health Challenge&lt;/b&gt;&lt;br /&gt;A map of odor that is tightly connected to perception and biology across the animal kingdom opens new doors. Mosquitos and other insect pests are drawn to humans in part by &lt;em&gt;their&lt;/em&gt; odor perception. Since the POM can be used to predict animal olfaction &lt;a href="https://www.biorxiv.org/content/10.1101/2022.07.21.500995"&gt;generally&lt;/a&gt;, we retrained it to tackle one of humanity’s biggest problems, the scourge of diseases transmitted by mosquitoes and ticks, which kill hundreds of thousands of people each year.   &lt;/p&gt;&lt;p&gt;For this purpose, we improved our original model with two new sources of data: (1) a &lt;a href="https://www.google.com/books/edition/Results_of_Screening_Tests_with_Material/cgxDAAAAYAAJ?hl=en&amp;amp;gbpv=0"&gt;long-forgotten set of experiments&lt;/a&gt; conducted by the &lt;a href="https://www.nal.usda.gov/legacy/topics/insects-and-entomology"&gt;USDA&lt;/a&gt; on human volunteers beginning 80 years ago and recently made discoverable by &lt;a href="https://books.google.com"&gt;Google Books&lt;/a&gt;, which we subsequently made machine-readable; and (2) a new dataset collected by our partners at &lt;a href="https://tropiq.nl/"&gt;TropIQ&lt;/a&gt;, using their high-throughput laboratory mosquito assay. Both datasets measure how well a given molecule keeps mosquitos away. Together, &lt;a href="http://doi.org/10.1101/2022.09.01.504601"&gt;the resulting model&lt;/a&gt; can predict the mosquito repellency of nearly any molecule, enabling a virtual screen over huge swaths of molecular space. We validated this screen experimentally using entirely new molecules and found over a dozen of them with repellency at least as high as &lt;a href="https://en.wikipedia.org/wiki/DEET"&gt;DEET&lt;/a&gt;, the active ingredient in most insect repellents. Less expensive, longer lasting, and safer repellents can reduce the worldwide incidence of diseases like malaria, potentially saving countless lives. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSLGgcc-x9986CdJEGhKp3ONZ6Yit88BdkoMWxUph-0cObgR8wnjXthp3WR_eAf2dKMpVr6G9bvkoVj31zU8aj6-Ug9KT1cf7SvCDt2qSnhCgxkCOp_v5T5BpkVfXIacFhB9_2iIl-CxHsdwN1vzSqx-DCVFPtEImG_gsYJPnUOvByh2NMKGEQiRW6vQ/s1488/image6.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="952" data-original-width="1488" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSLGgcc-x9986CdJEGhKp3ONZ6Yit88BdkoMWxUph-0cObgR8wnjXthp3WR_eAf2dKMpVr6G9bvkoVj31zU8aj6-Ug9KT1cf7SvCDt2qSnhCgxkCOp_v5T5BpkVfXIacFhB9_2iIl-CxHsdwN1vzSqx-DCVFPtEImG_gsYJPnUOvByh2NMKGEQiRW6vQ/s16000/image6.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;We digitized USDA mosquito repellency data for thousands of molecules previously scanned by Google Books, and used it to refine the learned representation (the map) at the heart of the model. We added additional layers, specifically to predict repellency in a mosquito feeder assay, and iteratively trained the model to improve assay predictions while running computational screens for candidate repellents.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxDxAry57CWi2gHHpp4O_bLevEd_epr4RJyATCjU8_Q5ORZ2LnqU0cAAPFRVAI2cZtiYsnty72NlQ4o14UreUhgu2WhF6a2fcSDyjjCnuIl4RDG46aIYGJ0F4meWk_sH9hGrcxNDe6IZiGvm9Ylsd38k501Y2wIETQoEVJgON1a1PVnOcRmYcktT_isg/s1488/image1.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="956" data-original-width="1488" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxDxAry57CWi2gHHpp4O_bLevEd_epr4RJyATCjU8_Q5ORZ2LnqU0cAAPFRVAI2cZtiYsnty72NlQ4o14UreUhgu2WhF6a2fcSDyjjCnuIl4RDG46aIYGJ0F4meWk_sH9hGrcxNDe6IZiGvm9Ylsd38k501Y2wIETQoEVJgON1a1PVnOcRmYcktT_isg/s16000/image1.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Many molecules showing mosquito repellency in the laboratory assay also showed repellency when applied to humans. Several showed repellency greater than the most common repellents used today (DEET and &lt;a href="https://en.wikipedia.org/wiki/Icaridin"&gt;picaridin&lt;/a&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;The Road Ahead&lt;/b&gt;&lt;br /&gt;We discovered that our modeling approach to smell prediction could be used to draw a Principal Odor Map for tackling odor-related problems more generally. This map was the key to measuring smell: it answered a range of questions about novel smells and the molecules that produce them, it connected smells back to their origins in evolution and the natural world, and it is helping us tackle important human-health challenges that affect millions of people. Going forward, we hope that this approach can be used to find new solutions to problems in food and fragrance formulation, environmental quality monitoring, and the detection of human and animal diseases. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work was performed by the ML olfaction research team, including Benjamin Sanchez-Lengeling, Brian K. Lee, Jennifer N. Wei, Wesley W. Qian, and Jake Yasonik (the latter two were partly supported by the Google Student Researcher program) and our external partners including Emily Mayhew and Joel D. Mainland from the Monell Center, and Koen Dechering and Marnix Vlot from TropIQ.&amp;nbsp;&lt;/em&gt;&lt;i&gt;The Google Books team brought the USDA dataset online.&lt;/i&gt;&lt;em&gt;&amp;nbsp;Richard C. Gerkin was supported by the Google Visiting Faculty Researcher program and is also an Associate Research Professor at Arizona State University.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/7891790943858040956/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7891790943858040956" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7891790943858040956" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/09/digitizing-smell-using-molecular-maps.html" rel="alternate" title="Digitizing Smell: Using Molecular Maps to Understand Odor" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgIKw_aiWx5tcRb73eI1fHs1oe2GSiZJVH2kV5r36E_TvpdDA09nWAiLuKPQs7jzM5OXtkDreuJQYgYnEp7EtRFVCGmack-ueAhkZZJTW3OTlfgQE6Hjdy-NW_bCGBlB9ssewPYDZ8E1bydgZMfRxW6sjByxiyWN9QxagjcSTSFQPLEP9hVQt6dxij0Xw/s72-c/image5.jpg" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-3533757912595814663</id><published>2022-08-31T10:03:00.001-07:00</published><updated>2022-10-21T06:31:27.116-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI"/><category scheme="http://www.blogger.com/atom/ns#" term="datasets"/><category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Processing"/><category scheme="http://www.blogger.com/atom/ns#" term="patents"/><title type="text">Announcing the Patent Phrase Similarity Dataset</title><content type="html">&lt;span class="byline-author"&gt;Posted Grigor Aslanyan, Software Engineer, Google&lt;/span&gt; &lt;p&gt;Patent documents typically use legal and highly technical language, with context-dependent terms that may have meanings quite different from colloquial usage and even between different documents. The process of using &lt;a href="https://www.sciencedirect.com/science/article/pii/S017221901630103X"&gt;traditional patent search methods&lt;/a&gt; (e.g., keyword searching) to search through the corpus of over one hundred million patent documents can be tedious and result in many missed results due to the broad and non-standard language used. For example, a "soccer ball" may be described as a "spherical recreation device", "inflatable sportsball" or "ball for ball game". Additionally, the language used in some patent documents may obfuscate terms to their advantage, so more powerful &lt;a href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;natural language processing&lt;/a&gt; (NLP) and &lt;a href="https://en.wikipedia.org/wiki/Semantic_similarity"&gt;semantic similarity&lt;/a&gt; understanding can give everyone access to do a thorough search.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;   &lt;p&gt;The patent domain (and more general technical literature like scientific publications) poses unique challenges for NLP modeling due to its use of legal and technical terms. While there are multiple commonly used general-purpose &lt;a href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html"&gt;semantic textual similarity&lt;/a&gt; (STS) benchmark datasets (e.g., &lt;a href="https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark"&gt;STS-B&lt;/a&gt;, &lt;a href="https://marcobaroni.org/composes/sick.html"&gt;SICK&lt;/a&gt;, &lt;a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398"&gt;MRPC&lt;/a&gt;, &lt;a href="https://github.com/cocoxu/SemEval-PIT2015"&gt;PIT&lt;/a&gt;), to the best of our knowledge, there are currently no datasets focused on technical concepts found in patents and scientific publications (the somewhat related &lt;a href="http://bioasq.org/"&gt;BioASQ challenge&lt;/a&gt; contains a biomedical question answering task). Moreover, with the continuing &lt;a href="https://patents.google.com/coverage"&gt;growth in size of the patent corpus&lt;/a&gt; (millions of new patents are issued worldwide every year), there is a need to develop more useful NLP models for this domain.  &lt;/p&gt;   &lt;p&gt;Today, we announce the release of the &lt;a href="https://www.kaggle.com/datasets/google/google-patent-phrase-similarity-dataset"&gt;Patent Phrase Similarity&lt;/a&gt; dataset, a new human-rated contextual phrase-to-phrase semantic matching dataset, and the accompanying &lt;a href="https://arxiv.org/pdf/2208.01171.pdf"&gt;paper&lt;/a&gt;, presented at the &lt;a href="http://ifs.tuwien.ac.at/patentsemtech/index.html"&gt;SIGIR PatentSemTech Workshop&lt;/a&gt;, which focuses on technical terms from patents. The Patent Phrase Similarity dataset contains ~50,000 rated phrase pairs, each with a &lt;a href="https://worldwide.espacenet.com/classification?locale=en_EP"&gt;Cooperative Patent Classification&lt;/a&gt; (CPC) class as context. In addition to similarity scores that are typically included in other benchmark datasets, we include granular rating classes similar to &lt;a href="https://wordnet.princeton.edu/"&gt;WordNet&lt;/a&gt;, such as synonym, antonym, &lt;a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy#Hyponyms_and_hypernyms"&gt;hypernym&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy#Hyponyms_and_hypernyms"&gt;hyponym&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Meronymy_and_holonymy"&gt;holonym&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Meronymy_and_holonymy"&gt;meronym&lt;/a&gt;, and domain related. This dataset (distributed under the &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International license&lt;/a&gt;) was used by &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; and &lt;a href="https://www.uspto.gov/"&gt;USPTO&lt;/a&gt; as the benchmark dataset in the &lt;a href="https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching"&gt;U.S. Patent Phrase to Phrase Matching&lt;/a&gt; competition to draw more attention to the performance of machine learning models on technical text. Initial results show that models fine-tuned on this new dataset perform substantially better than general pre-trained models without fine-tuning. &lt;/p&gt;   &lt;p&gt;&lt;b&gt;The Patent Phrase Similarity Dataset&lt;/b&gt;&lt;br&gt;To better train the next generation of state-of-the-art models, we created the Patent Phrase Similarity dataset, which includes many examples to address the following problems: (1) phrase disambiguation, (2) adversarial keyword matching, and (3) hard negative keywords (i.e., keywords that are unrelated but received a high score for similarity from other models ). Some keywords and phrases can have multiple meanings (e.g., the phrase "mouse" may refer to an animal or a computer input device), so we disambiguate the phrases by including CPC classes with each pair of phrases. Also, many NLP models (e.g., &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model"&gt;bag of words models)&lt;/a&gt; will not do well on data with phrases that have matching keywords but are otherwise unrelated (adversarial keywords, e.g., “container section” → “kitchen container”, “offset table” → “table fan”). The Patent Phrase Similarity dataset is designed to include many examples of matching keywords that are unrelated through adversarial keyword match, enabling NLP models to improve their performance.   &lt;/p&gt;   &lt;p&gt;Each entry in the Patent Phrase Similarity dataset contains two phrases, an anchor and target, a context CPC class, a rating class, and a similarity score. The dataset contains 48,548 entries with 973 unique anchors, split into training (75%), validation (5%), and test (20%) sets. When splitting the data, all of the entries with the same anchor are kept together in the same set. There are 106 different context CPC classes and all of them are represented in the training set.  &lt;/p&gt;    &lt;table align="center" style="margin-left: auto; margin-right: auto; text-align: center; font-size: small; width: 90%;"&gt;  &lt;tr&gt;   &lt;td&gt;&lt;b&gt;Anchor&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;Target&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;Context&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;Rating&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;Score&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;acid absorption    &lt;/td&gt;   &lt;td&gt;absorption of acid    &lt;/td&gt;   &lt;td&gt;B08    &lt;/td&gt;   &lt;td&gt;exact    &lt;/td&gt;   &lt;td&gt;1.0    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;acid absorption    &lt;/td&gt;   &lt;td&gt;acid immersion    &lt;/td&gt;   &lt;td&gt;B08    &lt;/td&gt;   &lt;td&gt;synonym    &lt;/td&gt;   &lt;td&gt;0.75    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;acid absorption    &lt;/td&gt;   &lt;td&gt;chemically soaked    &lt;/td&gt;   &lt;td&gt;B08    &lt;/td&gt;   &lt;td&gt;domain related    &lt;/td&gt;   &lt;td&gt;0.25    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;acid absorption    &lt;/td&gt;   &lt;td&gt;acid reflux    &lt;/td&gt;   &lt;td&gt;B08    &lt;/td&gt;   &lt;td&gt;not related    &lt;/td&gt;   &lt;td&gt;0.0    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;gasoline blend    &lt;/td&gt;   &lt;td&gt;petrol blend    &lt;/td&gt;   &lt;td&gt;C10    &lt;/td&gt;   &lt;td&gt;synonym    &lt;/td&gt;   &lt;td&gt;0.75    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;gasoline blend    &lt;/td&gt;   &lt;td&gt;fuel blend    &lt;/td&gt;   &lt;td&gt;C10    &lt;/td&gt;   &lt;td&gt;hypernym    &lt;/td&gt;   &lt;td&gt;0.5    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;gasoline blend    &lt;/td&gt;   &lt;td&gt;fruit blend    &lt;/td&gt;   &lt;td&gt;C10    &lt;/td&gt;   &lt;td&gt;not related    &lt;/td&gt;   &lt;td&gt;0.0    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;faucet assembly    &lt;/td&gt;   &lt;td&gt;water tap    &lt;/td&gt;   &lt;td&gt;A22    &lt;/td&gt;   &lt;td&gt;hyponym    &lt;/td&gt;   &lt;td&gt;0.5    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;faucet assembly    &lt;/td&gt;   &lt;td&gt;water supply    &lt;/td&gt;   &lt;td&gt;A22    &lt;/td&gt;   &lt;td&gt;holonym    &lt;/td&gt;   &lt;td&gt;0.25    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;faucet assembly    &lt;/td&gt;   &lt;td&gt;school assembly    &lt;/td&gt;   &lt;td&gt;A22    &lt;/td&gt;   &lt;td&gt;not related    &lt;/td&gt;   &lt;td&gt;0.0    &lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A small sample of the dataset with anchor and target phrases, context CPC class (B08: Cleaning, C10: Petroleum, gas, fuel, lubricants, A22: Butchering, processing meat/poultry/fish), a rating class, and a similarity score. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;&lt;b&gt;Generating the Dataset&lt;/b&gt;&lt;br&gt;To generate the Patent Phrase Similarity data, we first process the ~140 million patent documents in the Google Patent's &lt;a href="https://patents.google.com/coverage"&gt;corpus&lt;/a&gt; and automatically extract important English phrases, which are typically noun phrases (e.g., “fastener”, “lifting assembly”) and functional phrases (e.g., “food processing”, “ink printing”). Next, we filter and keep phrases that appear in at least 100 patents and randomly sample around 1,000 of these filtered phrases, which we call anchor phrases. For each anchor phrase, we find all of the matching patents and all of the CPC classes for those patents. We then randomly sample up to four matching CPC classes, which become the context CPC classes for the specific anchor phrase.  &lt;/p&gt;   &lt;p&gt;We use two different methods for pre-generating target phrases: (1) partial matching and (2) a &lt;a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"&gt;masked language model&lt;/a&gt; (MLM). For partial matching, we randomly select phrases from the entire corpus that partially match with the anchor phrase (e.g., “abatement” → “noise abatement”, “material formation” → “formation material”). For MLM, we select sentences from the patents that contain a given anchor phrase, mask them out, and use the &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/how-ai-improves-patent-analysis"&gt;Patent-BERT&lt;/a&gt; model to predict candidates for the masked portion of the text. Then, all of the phrases are cleaned up, which includes lowercasing and the removal of punctuation and certain stopwords (e.g., "and", "or", "said"), and sent to expert raters for review. Each phrase pair is rated independently by two raters skilled in the technology area. Each rater also generates new target phrases with different ratings. Specifically, they are asked to generate some low-similarity and unrelated targets that partially match with the original anchor and/or some high-similarity targets. Finally, the raters meet to discuss their ratings and come up with final ratings. &lt;/p&gt;   &lt;p&gt;&lt;b&gt;Dataset Evaluation&lt;/b&gt;&lt;br&gt;To evaluate its performance, the Patent Phrase Similarity dataset was used in the &lt;a href="https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching"&gt;U.S. Patent Phrase to Phrase Matching Kaggle competition&lt;/a&gt;. The competition was very popular, drawing about 2,000 competitors from around the world. A variety of approaches were successfully used by the top scoring teams, including ensemble models of &lt;a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"&gt;BERT&lt;/a&gt; variants and prompting (see the full &lt;a href="https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion"&gt;discussion&lt;/a&gt; for more details). The table below shows the best results from the competition, as well as several off-the-shelf baselines from our &lt;a href="https://arxiv.org/abs/2208.01171"&gt;paper&lt;/a&gt;. The &lt;a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"&gt;Pearson correlation&lt;/a&gt; metric was used to measure the linear correlation between the predicted and true scores, which is a helpful metric to target for downstream models so they can distinguish between different similarity ratings.  &lt;/p&gt;   &lt;p&gt;The baselines in the paper can be considered zero-shot in the sense that they use off-the-shelf models without any further fine-tuning on the new dataset (we use these models to embed the anchor and target phrases separately and compute the cosine similarity between them). The Kaggle competition results demonstrate that by using our training data, one can achieve significant improvements compared with existing NLP models. We have also estimated human performance on this task by comparing a single rater’s scores to the combined score of both raters. The results indicate that this is not a particularly easy task, even for human experts. &lt;/p&gt;    &lt;table align="center" style="margin-left: auto; margin-right: auto; width: 80%;"&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;b&gt;Model&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;Training&lt;/b&gt;   &lt;/td&gt;   &lt;td style="text-align: center;"&gt;&lt;b&gt;Pearson correlation&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://tfhub.dev/google/Wiki-words-250/2"&gt;word2vec&lt;/a&gt;   &lt;/td&gt;   &lt;td&gt;Zero-shot    &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.44    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://services.google.com/fh/files/blogs/bert_for_patents_white_paper.pdf"&gt;Patent-BERT&lt;/a&gt;   &lt;/td&gt;   &lt;td&gt;Zero-shot    &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.53    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;a href="https://www.sbert.net/docs/pretrained_models.html"&gt;Sentence-BERT&lt;/a&gt;   &lt;/td&gt;   &lt;td&gt;Zero-shot    &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.60    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Kaggle 1st place single    &lt;/td&gt;   &lt;td&gt;Fine-tuned    &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.87    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Kaggle 1st place ensemble    &lt;/td&gt;   &lt;td&gt;Fine-tuned    &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.88    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;Human    &lt;/td&gt;   &lt;td&gt;   &lt;/td&gt;   &lt;td style="text-align: center;"&gt;0.93    &lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Performance of popular models with no fine-tuning (zero-shot), models fine-tuned on the Patent Phrase Similarity dataset as part of the Kaggle competition, and single human performance. &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion and Future Work&lt;/b&gt;&lt;br&gt;We present the &lt;a href="https://www.kaggle.com/datasets/google/google-patent-phrase-similarity-dataset"&gt;Patent Phrase Similarity&lt;/a&gt; dataset, which was used as the benchmark dataset in the &lt;a href="https://www.kaggle.com/c/us-patent-phrase-to-phrase-matching"&gt;U.S. Patent Phrase to Phrase Matching&lt;/a&gt; competition, and demonstrate that by using our training data, one can achieve significant improvements compared with existing NLP models.  &lt;/p&gt;   &lt;p&gt;Additional challenging machine learning benchmarks can be generated from the patent corpus, and patent data has made its way into many of today's most-studied models. For example, the &lt;a href="https://arxiv.org/pdf/1910.10683v3.pdf"&gt;C4 text dataset&lt;/a&gt; used to train &lt;a href="https://arxiv.org/pdf/1910.10683v3.pdf"&gt;T5&lt;/a&gt; contains &lt;a href="https://arxiv.org/abs/2104.08758"&gt;many patent documents&lt;/a&gt;. The &lt;a href="https://arxiv.org/abs/2007.14062"&gt;BigBird&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2112.07916"&gt;LongT5&lt;/a&gt; models also use patents via the &lt;a href="https://arxiv.org/pdf/1906.03741.pdf"&gt;BIGPATENT dataset&lt;/a&gt;. The availability, breadth and open usage terms of full text data (see &lt;a href="https://cloud.google.com/blog/topics/public-datasets/google-patents-public-datasets-connecting-public-paid-and-private-patent-data"&gt;Google Patents Public Datasets&lt;/a&gt;) makes patents a unique resource for the research community. Possibilities for future tasks include massively multi-label classification, summarization, information retrieval, image-text similarity, citation graph prediction, and translation. See the &lt;a href="https://arxiv.org/pdf/2208.01171.pdf"&gt;paper&lt;/a&gt; for more details.  &lt;/p&gt;   &lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;i&gt;This work was possible through a collaboration with Kaggle, Satsyil Corp., USPTO, and MaxVal. Thanks to contributors Ian Wetherbee from Google, Will Cukierski and Maggie Demkin from Kaggle. Thanks to Jerry Ma, Scott Beliveau, and Jamie Holcombe from USPTO and Suja Chittamahalingam from MaxVal for their contributions.&lt;/i&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/3533757912595814663/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/announcing-patent-phrase-similarity.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3533757912595814663" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3533757912595814663" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/announcing-patent-phrase-similarity.html" rel="alternate" title="Announcing the Patent Phrase Similarity Dataset" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-5711145003816917897</id><published>2022-08-25T09:55:00.001-07:00</published><updated>2022-10-21T06:31:27.948-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/><category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/><title type="text">High-Definition Segmentation in Google Meet</title><content type="html">&lt;span class="byline-author"&gt;Posted by Tingbo Hou and Juhyun Lee, Software Engineers, Google&lt;/span&gt; &lt;p&gt;In recent years video conferencing has played an increasingly important role in both work and personal communication for many users. Over the past two years, we have enhanced this experience in Google Meet by introducing privacy-preserving machine learning (ML) powered &lt;a href="https://ai.googleblog.com/2020/10/background-features-in-google-meet.html"&gt;background features&lt;/a&gt;, also known as “virtual green screen”, which allows users to blur their backgrounds or replace them with other images. What is unique about this solution is that it runs directly in the browser without the need to install additional software.  &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;p&gt;So far, these ML-powered features have relied on CPU inference made possible by leveraging neural network &lt;a href="https://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html"&gt;sparsity&lt;/a&gt;, a common solution that works across devices, from entry level computers to high-end workstations. This enables our features to reach the widest audience. However, mid-tier and high-end devices often have powerful GPUs that remain untapped for ML inference, and existing functionality allows web browsers to access GPUs via shaders (WebGL).  &lt;/p&gt;&lt;p&gt;With the &lt;a href="https://workspaceupdates.googleblog.com/2022/08/improved-performance-for-google-meet-effects-web.html"&gt;latest update&lt;/a&gt; to Google Meet, we are now harnessing the power of GPUs to significantly improve the fidelity and performance of these background effects. As we detail in “&lt;a href="https://arxiv.org/abs/2208.11666"&gt;Efficient Heterogeneous Video Segmentation at the Edge&lt;/a&gt;”, these advances are powered by two major components: 1) a novel real-time video segmentation model and 2) a new, highly efficient approach for in-browser ML acceleration using &lt;a href="https://www.khronos.org/webgl/"&gt;WebGL&lt;/a&gt;. We leverage this capability to develop fast ML inference via fragment shaders. This combination results in substantial gains in accuracy and latency, leading to crisper foreground boundaries. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYoFEI9Z1DT05sxKHuaDFkHeKOIKkULJ1BzzZWxjfWOf9DhNpu5BtMpFkiaE6amDrRkrOd8rlNj0_gkMisp4JRm1JfiHMF6AgYl_7Eu_8OtfuBqE10z8sx79XetxiwkUrOIxX372pYOaXQGywTzCKGMPb1XI80BWd1WgKOlkOfRVbnPePh4pGTCunWsg/s960/image2.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="540" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYoFEI9Z1DT05sxKHuaDFkHeKOIKkULJ1BzzZWxjfWOf9DhNpu5BtMpFkiaE6amDrRkrOd8rlNj0_gkMisp4JRm1JfiHMF6AgYl_7Eu_8OtfuBqE10z8sx79XetxiwkUrOIxX372pYOaXQGywTzCKGMPb1XI80BWd1WgKOlkOfRVbnPePh4pGTCunWsg/s16000/image2.gif" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;CPU segmentation vs. HD segmentation in Meet.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Moving Towards Higher Quality Video Segmentation Models&lt;/b&gt;&lt;br&gt;To predict finer details, our new segmentation model now operates on high definition (HD) input images, rather than lower-resolution images, effectively doubling the resolution over the previous model. To accommodate this, the model must be of higher capacity to extract features with sufficient detail. Roughly speaking, doubling the input resolution quadruples the computation cost during inference.  &lt;/p&gt;&lt;p&gt;Inference of high-resolution models using the CPU is not feasible for many devices. The CPU may have a few high-performance cores that enable it to execute arbitrary complex code efficiently, but it is limited in its ability for the parallel computation required for HD segmentation. In contrast, GPUs have many, relatively low-performance cores coupled with a wide memory interface, making them uniquely suitable for high-resolution convolutional models. Therefore, for mid-tier and high-end devices, we adopt a significantly faster pure GPU pipeline, which is integrated using WebGL. &lt;/p&gt;&lt;p&gt;This change inspired us to revisit some of the &lt;a href="https://ai.googleblog.com/2020/10/background-features-in-google-meet.html"&gt;prior design decisions&lt;/a&gt; for the model architecture. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Backbone&lt;/i&gt;: We compared several widely-used backbones for on-device networks and found &lt;a href="https://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html"&gt;EfficientNet-Lite&lt;/a&gt; to be a better fit for the GPU because it removes the &lt;a href="https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7"&gt;squeeze-and-excitation&lt;/a&gt; block, a component that is inefficient on WebGL (more below).  &lt;/li&gt;  &lt;li&gt;&lt;i&gt;Decoder&lt;/i&gt;: We switched to a &lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;multi-layer perceptron&lt;/a&gt; (MLP) decoder consisting of 1x1 convolutions instead of using simple &lt;a href="https://en.wikipedia.org/wiki/Bilinear_interpolation"&gt;bilinear upsampling&lt;/a&gt; or the more expensive squeeze-and-excitation blocks. MLP has been successfully adopted in other segmentation architectures, like &lt;a href="https://arxiv.org/pdf/1606.00915v2.pdf"&gt;DeepLab&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1912.08193"&gt;PointRend&lt;/a&gt;, and is efficient to compute on both CPU and GPU. &lt;/li&gt;  &lt;li&gt;&lt;i&gt;Model size&lt;/i&gt;: With our new WebGL inference and the GPU-friendly model architecture, we were able to afford a larger model without sacrificing the real-time frame rate necessary for smooth video segmentation. We explored the width and the depth parameters using a &lt;a href="https://en.wikipedia.org/wiki/Neural_architecture_search"&gt;neural architecture search&lt;/a&gt;. &lt;/li&gt;&lt;/ul&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifJcsUJGVS17V9XcmV2GKC7eXFp5QB7SdifP16D6J_Ga_cIDYncUM1mnfy84c4JFyy0iejTsKI1ih7Kfn2keYY8p57gCQM4htLRwbQ5FRVkSYsRZqjy9C8TXaErH9-CFadH5y20ZzT5N3BgDjft7YkqlXskd2UYX3YzXFsDEUjDgSfMdGgNXnXfGT5Vg/s1065/image3.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="295" data-original-width="1065" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifJcsUJGVS17V9XcmV2GKC7eXFp5QB7SdifP16D6J_Ga_cIDYncUM1mnfy84c4JFyy0iejTsKI1ih7Kfn2keYY8p57gCQM4htLRwbQ5FRVkSYsRZqjy9C8TXaErH9-CFadH5y20ZzT5N3BgDjft7YkqlXskd2UYX3YzXFsDEUjDgSfMdGgNXnXfGT5Vg/s16000/image3.jpg" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;HD segmentation model architecture.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In aggregate, these changes substantially improve the mean &lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;Intersection over Union&lt;/a&gt; (IoU) metric by 3%, resulting in less uncertainty and crisper boundaries around hair and fingers. &lt;br /&gt;&lt;br /&gt;We have also released the accompanying &lt;a href="https://mediapipe.page.link/meet-hd-segmentation-mc"&gt;model card&lt;/a&gt; for this segmentation model, which details our fairness evaluations. Our analysis shows that the model is consistent in its performance across the various regions, skin-tones, and genders, with only small deviations in IoU metrics. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;&lt;b&gt;Model&lt;/b&gt;   &lt;/td&gt;  &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Resolution&lt;/b&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Inference&lt;/b&gt;   &lt;/td&gt;  &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;IoU&lt;/b&gt;   &lt;/td&gt;  &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Latency (ms)&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;i&gt;CPU segmenter&lt;/i&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;256×144    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://ai.googleblog.com/2020/10/background-features-in-google-meet.html"&gt;Wasm SIMD&lt;/a&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;94.0%    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;8.7    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;i&gt;GPU segmenter&lt;/i&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;512×288    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;WebGL    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;96.9%    &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;4.3    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of the previous segmentation model vs. the new HD segmentation model on a Macbook Pro (2018).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Accelerating Web ML with WebGL&lt;/b&gt;&lt;br&gt;One common challenge for web-based inference is that web technologies can incur a performance penalty when compared to apps running natively on-device. For GPUs, this penalty is substantial, only achieving around 25% of native &lt;a href="https://en.wikipedia.org/wiki/OpenGL"&gt;OpenGL&lt;/a&gt; performance. This is because WebGL, the current GPU standard for Web-based inference, was primarily designed for image rendering, not arbitrary ML workloads. In particular, WebGL does not include &lt;a href="https://en.wikipedia.org/wiki/Compute_kernel"&gt;compute shaders&lt;/a&gt;, which allow for general purpose computation and enable ML workloads in mobile and native apps. &lt;/p&gt;&lt;p&gt;To overcome this challenge, we accelerated low-level neural network kernels with &lt;a href="https://en.wikipedia.org/wiki/Shader#Pixel_shaders"&gt;fragment shaders&lt;/a&gt; that typically compute the output properties of a pixel like color and depth, and then applied novel optimizations inspired by the graphics community. As ML workloads on GPUs are often bound by memory bandwidth rather than compute, we focused on rendering techniques that would improve the memory access, such as &lt;a href="https://en.wikipedia.org/wiki/Multiple_Render_Targets"&gt;Multiple Render Targets&lt;/a&gt; (MRT). &lt;/p&gt;&lt;p&gt;MRT is a feature in modern GPUs that allows rendering images to multiple output textures (OpenGL objects that represent images) at once. While MRT was originally designed to support advanced graphics rendering such as &lt;a href="https://en.wikipedia.org/wiki/Deferred_shading"&gt;deferred shading&lt;/a&gt;, we found that we could leverage this feature to drastically reduce the memory bandwidth usage of our fragment shader implementations for critical operations, like convolutions and fully connected layers. We do so by treating intermediate tensors as multiple OpenGL textures. &lt;/p&gt;&lt;p&gt;In the figure below, we show an example of intermediate tensors having four underlying GL textures each. With MRT, the number of GPU threads, and thus effectively the number of memory requests for weights, is reduced by a factor of four and saves memory bandwidth usage. Although this introduces considerable complexities in the code, it helps us reach over 90% of native OpenGL performance,&lt;strong&gt; &lt;/strong&gt;closing the gap with native applications. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT_kMDy1cnR_15fZjXbHnEdTbZPKse_UrnG_9L4iZ2Az8EM8ng_2benILGrAZJ4OpIS9NR22gv09Zgzb_DR6PhLW_AkHEcWMihfJ55PWDmzGsKRNDSmqFQbbQJtFV9vQSp6YdOFMQI7kI-plLL-KVWBo6qeQZSw7UlSvOhzYDxnlsBfgxlZuA72IllLw/s1762/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1214" data-original-width="1762" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT_kMDy1cnR_15fZjXbHnEdTbZPKse_UrnG_9L4iZ2Az8EM8ng_2benILGrAZJ4OpIS9NR22gv09Zgzb_DR6PhLW_AkHEcWMihfJ55PWDmzGsKRNDSmqFQbbQJtFV9vQSp6YdOFMQI7kI-plLL-KVWBo6qeQZSw7UlSvOhzYDxnlsBfgxlZuA72IllLw/s16000/image1.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Left: A classic implementation of Conv2D with 1-to-1 correspondence of tensor and an OpenGL texture.  Red, yellow, green, and blue boxes denote different locations in a single texture each for intermediate tensor A and B. Right: Our implementation of Conv2D with MRT where intermediate tensors A and B are realized with a set of 4 GL textures each, depicted as red, yellow, green, and blue boxes. Note that this reduces the request count for weights by 4x.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br&gt;We have made rapid strides in improving the quality of real-time segmentation models by leveraging the GPU on mid-tier and high-end devices for use with Google Meet. We look forward to the possibilities that will be enabled by upcoming technologies like &lt;a href="https://en.wikipedia.org/wiki/WebGPU"&gt;WebGPU&lt;/a&gt;, which bring compute shaders to the web. Beyond GPU inference, we're also working on improving the segmentation quality for lower powered devices with quantized inference via &lt;a href="https://github.com/google/XNNPACK"&gt;XNNPACK&lt;/a&gt; &lt;a href="https://webassembly.org/"&gt;WebAssembly&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;em&gt;Special thanks to those on the Meet team and others who worked on this project, in particular Sebastian Jansson, Sami Kalliomäki, Rikard Lundmark, Stephan Reiter, Fabian Bergmark, Ben Wagner, Stefan Holmer, Dan Gunnarsson, Stéphane Hulaud, and to all our team members who made this possible: Siargey Pisarchyk, Raman Sarokin, Artsiom Ablavatski, Jamie Lin, Tyler Mullen, Gregory Karpiak, Andrei Kulik, Karthik Raveendran, Trent Tolley, and Matthias Grundmann.&lt;/em&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/5711145003816917897/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5711145003816917897" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5711145003816917897" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/high-definition-segmentation-in-google.html" rel="alternate" title="High-Definition Segmentation in Google Meet" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYoFEI9Z1DT05sxKHuaDFkHeKOIKkULJ1BzzZWxjfWOf9DhNpu5BtMpFkiaE6amDrRkrOd8rlNj0_gkMisp4JRm1JfiHMF6AgYl_7Eu_8OtfuBqE10z8sx79XetxiwkUrOIxX372pYOaXQGywTzCKGMPb1XI80BWd1WgKOlkOfRVbnPePh4pGTCunWsg/s72-c/image2.gif" width="72"/><thr:total>0</thr:total></entry><entry><id>tag:blogger.com,1999:blog-8474926331452026626.post-2305365824069916944</id><published>2022-08-24T11:17:00.001-07:00</published><updated>2022-10-21T06:31:28.837-07:00</updated><category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/><category scheme="http://www.blogger.com/atom/ns#" term="India"/><title type="text">Using ML to Boost Engagement with a Maternal and Child Health Program in India</title><content type="html">&lt;span class="byline-author"&gt;Posted by Aparna Taneja, Software Engineer, and Milind Tambe, Principal Scientist, Google Research, India Research Lab&lt;/span&gt; &lt;p&gt;The widespread availability of mobile phones has enabled non-profits to deliver critical health information to their beneficiaries in a timely manner. While advanced applications on smartphones allow for richer multimedia content and two-way communication between beneficiaries and health coaches, simpler text and voice messaging services can be effective in disseminating information to large communities, particularly those that are underserved with limited access to information and smartphones. &lt;a href="https://armman.org/"&gt;ARMMAN&lt;/a&gt;&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;&lt;span style="font-size: x-small;"&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, one non-profit doing just this, is based in India with the mission of improving maternal and child health outcomes in underserved communities. &lt;/p&gt;&lt;a name='more'&gt;&lt;/a&gt;    &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFapdOe5HopSHcxZLMfKo0Rs9mcxhRQ927F5-ny_y7vve43q60pCzFgIDOGei528_ap7WdV6DwZpz6OIwyy-I3B69NiJShuaJQsHUkwpEI1V5ipLvVPyo6pqT0dr8af_dRk885xzjMNGh9806hiD9y1sODCQTPVXUE2YSFOC40THfPMuC-KDQhteGOuw/s1642/image4.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="876" data-original-width="1642" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFapdOe5HopSHcxZLMfKo0Rs9mcxhRQ927F5-ny_y7vve43q60pCzFgIDOGei528_ap7WdV6DwZpz6OIwyy-I3B69NiJShuaJQsHUkwpEI1V5ipLvVPyo6pqT0dr8af_dRk885xzjMNGh9806hiD9y1sODCQTPVXUE2YSFOC40THfPMuC-KDQhteGOuw/s16000/image4.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Overview of ARMMAN&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;One of the programs run by them is &lt;a href="https://armman.org/mmitra/"&gt;mMitra&lt;/a&gt;, which employs automated voice messaging to deliver timely preventive care information to expecting and new mothers during pregnancy and until one year after birth. These messages are tailored according to the gestational age of the beneficiary. Regular listenership to these messages has been shown to have a high correlation with improved behavioral and health outcomes, such as a 17% increase in infants with tripled birth weight at end of year and a 36% increase in women knowing the importance of taking iron tablets. &lt;/p&gt;&lt;p&gt;However, a key challenge ARMMAN faced was that about 40% of women gradually stopped engaging with the program. While it’s possible to mitigate this with live service calls to women to explain the advantage of listening to the messages, it is infeasible to call all the low listeners in the program because of limited support staff — this highlights the importance of effectively prioritizing who receives such service calls. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://research.google/pubs/pub50848/"&gt;Field Study in Deploying Restless Multi-Armed Bandits: Assisting Non-Profits in Improving Maternal and Child Health&lt;/a&gt;”, published in &lt;a href="https://aaai.org/Conferences/AAAI-22/"&gt;AAAI 2022&lt;/a&gt;, we describe an ML-based solution that uses historical data from the NGO to predict which beneficiaries will benefit most from service calls. We address the challenges that come with a large-scale real world deployment of such a system and show the usefulness of deploying this model in a real study involving over 23,000 participants. The model showed an increase in listenership of 30% compared to the current standard of care group. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Background&lt;/b&gt;&lt;br/&gt;We model this resource optimization problem using &lt;a href="https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/restless-bandits-activity-allocation-in-a-changing-world/DDEB5E22AFFEFF50AA97ADC96B71AE35"&gt;restless multi-armed bandits&lt;/a&gt; (RMABs), which have been well studied for application to such problems in a myriad of domains, including healthcare. An RMAB consists of &lt;em&gt;n&lt;/em&gt; arms where each arm (representing a beneficiary) is associated with a two-state &lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process#:~:text=In%20mathematics%2C%20a%20Markov%20decision,control%20of%20a%20decision%20maker."&gt;Markov decision process&lt;/a&gt; (MDP). Each MDP is modeled as a two-state (good or bad state, where the good state corresponds to high listenership in the previous week), two-action (corresponding to whether the beneficiary was chosen to receive a service call or not) problem. Further, each MDP has an associated reward function (i.e., the reward accumulated at a given state and action) and a transition function indicating the probability of moving from one state to the next under a given action, under the Markov condition that the next state depends only on the previous state and the action taken on that arm in that time step. The term &lt;em&gt;restless&lt;/em&gt; indicates that all arms can change state irrespective of the action. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNXOl5--o7B1DwYCUgN5FnC9d6atNdR-V50S2b14hwwXyTdM62_2uCR4OWne2rP4Mc6R2TGx62EUKgFtA_D60qLTVJ9BTjVTzRnhcjjC24I7hBcT8oKHwdlx1qSrt-terI9FghvTp4KA25KkMhJHKSlVtgFojKy-Lz51KW5FwFGYfXTtJWSyD0TXpWNg/s1628/image3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="570" data-original-width="1628" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNXOl5--o7B1DwYCUgN5FnC9d6atNdR-V50S2b14hwwXyTdM62_2uCR4OWne2rP4Mc6R2TGx62EUKgFtA_D60qLTVJ9BTjVTzRnhcjjC24I7hBcT8oKHwdlx1qSrt-terI9FghvTp4KA25KkMhJHKSlVtgFojKy-Lz51KW5FwFGYfXTtJWSyD0TXpWNg/s16000/image3.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;State of a beneficiary may transition from good (high engagement) to bad (low engagement) with example passive and active transition probabilities shown in the transition matrix.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Model Development&lt;/b&gt;&lt;br/&gt;Finally, the RMAB problem is modeled such that at any time step, given &lt;em&gt;n&lt;/em&gt; total arms, which &lt;em&gt;k&lt;/em&gt; arms should be acted on (i.e., chosen to receive a service call), to maximize reward (engagement with the program).  &lt;/p&gt;&lt;p&gt;The probability of transitioning from one state to another with (active probability) or without (passive probability) receiving a service call are therefore the underlying model parameters that are critical to solving the above optimization. To estimate these parameters, we use the demographic data of the beneficiaries collected at time of enrolment by the NGO, such as age, income, education, number of children, etc., as well as past listenership data, all in-line with the NGO’s data privacy standards (more below).  &lt;/p&gt;&lt;p&gt;However, the limited volume of service calls limits the data corresponding to receiving a service call. To mitigate this, we use &lt;a href="https://en.wikipedia.org/wiki/Cluster_analysis"&gt;clustering techniques&lt;/a&gt; to learn from the collective observations of beneficiaries within a cluster and enable overcoming the challenge of limited samples per individual beneficiary. &lt;/p&gt;&lt;p&gt;In particular, we perform clustering on listenership behaviors, and then compute a mapping from the demographic features to each cluster.  &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijvoldXeXl4h6FOtL8qDSyaRPeNmGju1adEyd8Y-Or9oBKZe6rIWpW1cD_UcfCQ-ul-aZsmwryJKMlE9Jawr5g5VifjZR7VHRe8F6EfnsuFVdXjqDjtzWqusF7oOzSsVOF9Np0zt1jKVqCBKS8_11ybQKcQtfyxalT4wp1Jgv3n8Hv1fpBao1dAS98hg/s1494/image5.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="384" data-original-width="1494" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijvoldXeXl4h6FOtL8qDSyaRPeNmGju1adEyd8Y-Or9oBKZe6rIWpW1cD_UcfCQ-ul-aZsmwryJKMlE9Jawr5g5VifjZR7VHRe8F6EfnsuFVdXjqDjtzWqusF7oOzSsVOF9Np0zt1jKVqCBKS8_11ybQKcQtfyxalT4wp1Jgv3n8Hv1fpBao1dAS98hg/s16000/image5.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Clustering on past listenership data reveals clusters with beneficiaries that behave similarly.  We then infer a mapping from demographic features to clusters.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;This mapping is useful because when a new beneficiary is enrolled, we only have access to their demographic information and have no knowledge of their listenership patterns, since they haven’t had a chance to listen yet. Using the mapping, we can infer transition probabilities for any new beneficiary that enrolls into the system. &lt;/p&gt;&lt;p&gt;We used several qualitative and quantitative metrics to infer the optimal set of of clusters and explored different combinations of training data (demographic features only, features plus passive probabilities, features plus all probabilities, passive probabilities only) to achieve the most meaningful clusters, that are representative of the underlying data distribution and have a low variance in individual cluster sizes.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2eOmAL3E9c1dvhDePWe4XVdzfxue8fAAHo6yS8NAXr9GDscnLKxxSS2xwAzYe8SnADeSK1Vlx9K8yAZzNnkSbvI9RjfXOUQrBQX-FkMtBMW1gbrHsA15cDZurw6OFdwQfZZ61j5Vi2NdRiJHcN1ekiix4ZZiebo23XvAmsKLjf0qrmoFg-lsu3H3uA/s1564/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1360" data-original-width="1564" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW2eOmAL3E9c1dvhDePWe4XVdzfxue8fAAHo6yS8NAXr9GDscnLKxxSS2xwAzYe8SnADeSK1Vlx9K8yAZzNnkSbvI9RjfXOUQrBQX-FkMtBMW1gbrHsA15cDZurw6OFdwQfZZ61j5Vi2NdRiJHcN1ekiix4ZZiebo23XvAmsKLjf0qrmoFg-lsu3H3uA/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of passive transition probabilities obtained from different clustering methods with number of clusters &lt;em&gt;s&lt;/em&gt; = 20 (red dots) and 40 (green dots), using ground truth passive transition probabilities (blue dots). Clustering based on features+passive probabilities (PPF) captures more distinct beneficiary behaviors across the probability space.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Clustering has the added advantage of reducing computational cost for resource-limited NGOs, as the optimization needs to be solved at a cluster level rather than an individual level. Finally, solving RMAB’s is known to be &lt;a href="https://en.wikipedia.org/wiki/PSPACE#PSPACE-completeness"&gt;P-space hard&lt;/a&gt;, so we choose to solve the optimization using the popular &lt;a href="https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/restless-bandits-activity-allocation-in-a-changing-world/DDEB5E22AFFEFF50AA97ADC96B71AE35"&gt;Whittle index approach&lt;/a&gt;, which ultimately provides a ranking of beneficiaries based on their likely benefit of receiving a service call. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br/&gt;We evaluated the model in a real world study consisting of approximately 23,000 beneficiaries who were divided into three groups: the current standard of care (CSOC) group, the "round robin" (RR) group, and the RMAB group. The beneficiaries in the CSOC group follow the original standard of care, where there are no NGO initiated service calls. The RR group represents the scenario where the NGO often conducts service calls using some systematic set order — the idea here is to have an easily executable policy that services enough of a cross-section of beneficiaries and can be scaled up or down per week based on available resources (this is the approach used by the NGO in this particular case, but the approach may vary for different NGOs). The RMAB group receives service calls as predicted by the RMAB model. All the beneficiaries across the three groups continue to receive the automated voice messages independent of the service calls. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfHg1cx-MVYwhILMLHrxooNwLwMA2ajcBYZkIIF7BcohwuDrTvPAgODrDRCaal0vifmQXSzPWwZWeNb9xUMBVurS2fRL3LItZNAvIX2MQitV8q5pPKAJ2rHFdnn4T6JaG2Sl8IHResM2_zBDCNTnUOCsu24WaoEDsCual2CQhRfFCnH3FDrBe8PobA0Q/s1054/image6.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="450" data-original-width="1054" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfHg1cx-MVYwhILMLHrxooNwLwMA2ajcBYZkIIF7BcohwuDrTvPAgODrDRCaal0vifmQXSzPWwZWeNb9xUMBVurS2fRL3LItZNAvIX2MQitV8q5pPKAJ2rHFdnn4T6JaG2Sl8IHResM2_zBDCNTnUOCsu24WaoEDsCual2CQhRfFCnH3FDrBe8PobA0Q/s16000/image6.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Distributions of clusters picked for service calls by RMAB and RR in week 1 (&lt;b&gt;left&lt;/b&gt;) and 2 (&lt;b&gt;right&lt;/b&gt;) are significantly different. RMAB is very strategic in picking only a few clusters with a promising probability of success (blue is high and red is low), RR displays no such strategic selection.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;At the end of seven weeks, RMAB-based service calls resulted in the highest (and statistically significant) reduction in cumulative engagement drops (32%) compared to the CSOC group. &lt;/p&gt;     &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcYEDGefNeTdysyzZLbvl-0OfWBTI0PvOFBhS5YLLHEUSOp6nS5EkmB1SHKL3gDPX6dnwBuDDHedQQzq6-7pV67Ysg1-dHVL2iu2BrJIdQYzqLM4P7lI5ppnqru_uyd80455MlrquHdaayUD4dpKvfoX0nAQ6xK_hyqajHls2smjG92J3SyZoG2nfz1w/s1518/image7.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="724" data-original-width="1518" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcYEDGefNeTdysyzZLbvl-0OfWBTI0PvOFBhS5YLLHEUSOp6nS5EkmB1SHKL3gDPX6dnwBuDDHedQQzq6-7pV67Ysg1-dHVL2iu2BrJIdQYzqLM4P7lI5ppnqru_uyd80455MlrquHdaayUD4dpKvfoX0nAQ6xK_hyqajHls2smjG92J3SyZoG2nfz1w/s16000/image7.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The plot shows cumulative engagement drops prevented compared to the control group.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;  &lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;RMAB vs CSOC&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;   &lt;/td&gt;    &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;RR vs CSOC&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;   &lt;/td&gt;    &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;RMAB vs RR&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;% reduction in cumulative engagement drops&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;32.0%&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;5.2%    &lt;/td&gt;   &lt;td&gt;28.3%    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: left;"&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/P-value"&gt;p-value&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;    &lt;td&gt;&lt;b&gt;0.044&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;0.740    &lt;/td&gt;   &lt;td&gt;0.098    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Ethical Considerations&lt;/b&gt;&lt;br/&gt;An ethics board at the NGO reviewed the study. We took significant measures to ensure participant consent is understood and recorded in a language of the community's choice at each stage of the program. Data stewardship resides in the hands of the NGO, and only the NGO is allowed to share data. The code will soon be available publicly. The pipeline only uses anonymized data and no personally identifiable information (PII) is made available to the models. Sensitive data, such as caste, religion, etc., are not collected by ARMMAN for mMitra. Therefore, in pursuit of ensuring fairness of the model, we worked with public health and field experts to ensure other indicators of socioeconomic status were measured and adequately evaluated as shown below.   &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinTRaA-d_ENirEVSbssIbznJuwt7JJFHxvRx76ACYJR2kkVC-UppoEtJIZxEcJq7yBjS94o5xs8_WPJ-AL6yVKVlaDDTt73qX-vigmrz3ukiLBDSIehQ7FDobZ1t8FGb9a39VbYz5B505dXjfd_2-opFI-1XG_0cnY3YhXtHQlc82FCQHK0tyrKB0gvg/s1248/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="864" data-original-width="1248" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinTRaA-d_ENirEVSbssIbznJuwt7JJFHxvRx76ACYJR2kkVC-UppoEtJIZxEcJq7yBjS94o5xs8_WPJ-AL6yVKVlaDDTt73qX-vigmrz3ukiLBDSIehQ7FDobZ1t8FGb9a39VbYz5B505dXjfd_2-opFI-1XG_0cnY3YhXtHQlc82FCQHK0tyrKB0gvg/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Distribution of highest education received (&lt;b&gt;top&lt;/b&gt;) and monthly family income in Indian Rupees (&lt;b&gt;bottom&lt;/b&gt;) across a cohort that received service calls compared to the whole population.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The proportion of beneficiaries that received a live service call within each income bracket reasonably matches the proportion in the overall population. However, differences are observed in lower income categories, where the RMAB model favors beneficiaries with lower income and beneficiaries with no formal education. Lastly, domain experts at ARMMAN have been deeply involved in the development and testing of this system and have provided continuous input and oversight in data interpretation, data consumption, and model design.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusions&lt;/b&gt;&lt;br/&gt;After thorough testing, the NGO has currently deployed this system for scheduling of service calls on a weekly basis. We are hopeful that this will pave the way for more deployments of ML algorithms for social impact in partnerships with non-profits in service of populations that have so far benefited less from ML. This work was also featured in &lt;a href="https://www.youtube.com/watch?t=2670&amp;amp;v=h-jZQaiCmps&amp;amp;feature=youtu.be"&gt;Google for India 2021&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;This work is part of our &lt;a href="https://ai.google/social-good/"&gt;AI for Social Good&lt;/a&gt; efforts and was led by Google Research, India. Thanks to all our collaborators at ARMMAN, Google Research India, Google.org, and University Relations: &lt;a href="https://www.linkedin.com/in/dr-aparna-hegde/?originalSubdomain=in"&gt;Aparna Hegde&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/neha-madhiwalla-b64a6a141/?originalSubdomain=in"&gt;Neha Madhiwalla&lt;/a&gt;, Suresh Chaudhary, &lt;a href="https://projects.iq.harvard.edu/adityamate/home"&gt;Aditya Mate&lt;/a&gt;, &lt;a href="https://research.google/people/LovishMadaan/"&gt;Lovish Madaan&lt;/a&gt;, &lt;a href="https://research.google/people/ShresthVerma/"&gt;Shresth Verma&lt;/a&gt;, &lt;a href="https://research.google/people/107972/"&gt;Gargi Singh&lt;/a&gt;, &lt;a href="https://research.google/people/106305/"&gt;Divy Thakkar&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;!--Footnotes--&gt;&lt;hr width="80%" /&gt;&lt;p&gt;  &lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;sup&gt;&lt;a name="fn1"&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;&lt;a href="https://armman.org/"&gt;ARMMAN&lt;/a&gt; runs multiple programs to provide preventive care information to women through pregnancy and infancy enabling them to seek care, as well as programs to train and support health workers for timely detection and management of high-risk conditions.&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;</content><link href="http://ai.googleblog.com/feeds/2305365824069916944/comments/default" rel="replies" title="Post Comments" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/using-ml-to-boost-engagement-with.html#comment-form" rel="replies" title="0 Comments" type="text/html"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2305365824069916944" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2305365824069916944" rel="self" type="application/atom+xml"/><link href="http://ai.googleblog.com/2022/08/using-ml-to-boost-engagement-with.html" rel="alternate" title="Using ML to Boost Engagement with a Maternal and Child Health Program in India" type="text/html"/><author><name>Google AI</name><uri>http://www.blogger.com/profile/12098626514775266161</uri><email>noreply@blogger.com</email><gd:image height="16" rel="http://schemas.google.com/g/2005#thumbnail" src="https://img1.blogblog.com/img/b16-rounded.gif" width="16"/></author><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" height="72" url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFapdOe5HopSHcxZLMfKo0Rs9mcxhRQ927F5-ny_y7vve43q60pCzFgIDOGei528_ap7WdV6DwZpz6OIwyy-I3B69NiJShuaJQsHUkwpEI1V5ipLvVPyo6pqT0dr8af_dRk885xzjMNGh9806hiD9y1sODCQTPVXUE2YSFOC40THfPMuC-KDQhteGOuw/s72-c/image4.png" width="72"/><thr:total>0</thr:total></entry></feed>