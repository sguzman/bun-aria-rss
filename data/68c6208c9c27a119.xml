<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[kevin frans blog]]></title><description><![CDATA[kevin frans blog]]></description><link>https://kvfrans.com/</link><image><url>https://kvfrans.com/favicon.png</url><title>kevin frans blog</title><link>https://kvfrans.com/</link></image><generator>Ghost 3.41</generator><lastBuildDate>Sat, 05 Nov 2022 07:42:59 GMT</lastBuildDate><atom:link href="https://kvfrans.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[For AGI, we need better tasks. For better tasks, we need open-endedness. (ALOE 2022 Notes)]]></title><description><![CDATA[<p>Open-endedness is the idea of a system that "endlessly creates increasingly interesting designs". That's a simple concept with big implications. Biological life on earth is diverse and beautiful and the result of open-ended evolution. Human culture is open-ended; we've created a lot of art and technology without any real plan.</p>]]></description><link>https://kvfrans.com/notes-on-aloe/</link><guid isPermaLink="false">626ef9471668172f903c5d99</guid><category><![CDATA[research]]></category><category><![CDATA[thoughts]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Sun, 15 May 2022 19:00:11 GMT</pubDate><content:encoded><![CDATA[<p>Open-endedness is the idea of a system that "endlessly creates increasingly interesting designs". That's a simple concept with big implications. Biological life on earth is diverse and beautiful and the result of open-ended evolution. Human culture is open-ended; we've created a lot of art and technology without any real plan.</p><p>For those of us interested in AI and the emergence of intelligence, open-endedness is especially important. Because even with powerful learning algorithms, intelligence is one of those tricky things that's hard and complicated to specify. But that's the beauty of open-ended systems: they can generate <em>unexpected</em> designs, designs that go beyond the intention of the designer. And so even if intelligence is hard to create, it might be possible to create a system where intelligence naturally emerges.</p><p>This weekend, a bunch of us thinking about how open-endedness can boost AI gathered at the Agent Learning in Open-Endedness (ALOE) workshop at ICLR. I had the pleasure of attending, and since this topic is really fascinating I took some notes, so here they are.</p><p>As context, I (Kevin Frans) am a master's student at MIT under Phillip Isola, and I've spent time at places like OpenAI and Cross Labs. The hypothesis I've been studying is: a strong learning algorithm, coupled with a rich enough set of tasks, will result in intelligence. We have strong learning algorithms, but our tasks are quite bad, so what are consistent principles for making task distributions that are rich, learnable, yet increasingly complex?</p><h2 id="more-tasks-smarter-agents-">More tasks = smarter agents.</h2><p>A common theme in AI is that we take the same basic learning algorithms, and scale them up with harder tasks, and they can discover more complicated behaviors. There's arguments of whether this fact is good or bad for research, but the fact is that it works, and accepting this can lead to cool practical results.</p><p>XLand, the new task set from Deepmind, works by accepting this lesson and pushing it to the limit. XLand aims to create a whole bunch of related tasks by 1) defining a distribution of games, and 2) making them all multi-agent. </p><p>The key lesson here is that distribution of XLand tasks is just so varied and dense. XLand tasks take place in a 3D physics world, and agents need to do things like move blocks around, catch other agents, or play hide-and-seek. These games are defined in a modular way, so some agents get reward based on Distance(Agent, Ball) while others are rewarded for -Distance(AgentA, AgentB). Each task also takes place on a procedurally-generated terrain. Tasks are inherently multi-agent, so the same task can have a different reward landscape based on which agents are involved. </p><p>Now that we've got a rich distribution of games, just train agents with RL and they will be smart. There's some fancy population-based training involved, but it's a side point, the main idea is that agents trained on this rich set of games have no choice but to become generally capable, and they do. Strong XLand agents perform well on a bunch of the tasks, even unseen ones, and seem to generally do the right thing.</p><p>To me, the nice part about XLand is that it confirms we can achieve strong generalization just by engineering the task space to be richer, denser, and larger. Now AGI is a game design problem, and game design is fun. The downside is that this all seems awfully expensive to compute.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-01-at-6.35.49-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2022/05/Screen-Shot-2022-05-01-at-6.35.49-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2022/05/Screen-Shot-2022-05-01-at-6.35.49-PM.png 1000w, https://kvfrans.com/content/images/size/w1600/2022/05/Screen-Shot-2022-05-01-at-6.35.49-PM.png 1600w, https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-01-at-6.35.49-PM.png 2214w" sizes="(min-width: 720px) 720px"></figure><h2 id="procedural-content-generation-ai-">Procedural content generation + AI.</h2><p>My favorite video games can be replayed hundreds of times, and these games usually make use of procedural content generation (PCG). Instead of manually building out levels, we can define an algorithm that builds the levels for us, and use it to generate an infinite amount of content.</p><p>Sam Earle's work is on how PCG and AI intersect. First, can AI methods be used as a PCG algorithm? You could imagine PCG being a data modelling problem – given a set of 1000 Mario levels, train a GAN or autoencoder to model the distribution. But this is open-endedness, we're not satisfied with mimicry. Sam's early work on <a href="https://ojs.aaai.org/index.php/AIIDE/article/view/7416/7341">PCGRL</a> frames content generation as a reinforcement learning task. If we can define a measurement of how good a level is, we can try and learn a generator that best fits this measurement. A <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9619159">follow-up paper</a> takes this further, and lets the user specify a bunch of parameters like level-length or difficulty, which the AI generator does its best to fulfill.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-13-at-4.28.00-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2022/05/Screen-Shot-2022-05-13-at-4.28.00-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2022/05/Screen-Shot-2022-05-13-at-4.28.00-PM.png 1000w, https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-13-at-4.28.00-PM.png 1336w" sizes="(min-width: 720px) 720px"></figure><p>To me, the simplest open-ended loop is a zero-sum competitive game. Two players compete against each other, and adapt to each other's weakness in a continuous learning loop. So if we want to train a really good Mario-playing agent, it's crucial that we think just as hard about a Mario level-generating agent. That's where AI PCG methods come in – we're only going to get so far with human-made levels, so we're going to need to build generator agents that never stop learning.</p><h2 id="the-tao-of-agents-and-environments">The Tao of Agents and Environments</h2><p>"Instead of a real presentation, here are some slides that I threw together in an hour", started Julius Togelius. "Actually I spent two days on this, but it's funnier to pretend I didn't." He then proceeded to show pictures of Agent Smith from The Matrix, and an environment diagram of the H2O cycle.</p><p>The purpose of all this was to point out that our classic framework of "agents interacting with an environment" is limiting. What if our environment has other agents in it? Why does the game-player have to be the agent? Can the environment be the player? If  we start to question these terms, maybe there's new ground we can think about when it comes to "agent-based" systems.</p><p>From an open-endedness perspective, this idea makes sense, because we want to think of our systems as more than just optimization problems. A traditional reinforcement learning problem is to train a robotic agent to solve a control environment, it's clean and simple and solvable. But in an open-ended system, maybe we want the robot to compete against other robots. Maybe the real goal is not to develop strong robots, but to see the emergence of cooperation or communication. Maybe we care about these robots because we want to discover new challenges for new robots. In an open-endedness view, everything is adapting so everything is an agent, and everything affects everything else so everything is also an environment.</p><h2 id="to-see-emergent-complexity-use-antagonistic-pairs-">To see emergent complexity, use antagonistic pairs.</h2><p>A fundamental open-ended loop is:</p><ol><li>Solve a task</li><li>Generate a new task</li><li>Repeat</li></ol><p>Solving a task is well-studied, we have whole fields on how to do this. How to <em>generate new tasks</em>, however, is ripe for exploration. We need to be careful. If the tasks are too similar, then we're not learning anything new, but if the tasks are too distinct, an agent might not be able to solve them. Ideally, we want to define some implicit curriculum of tasks that an agent can gradually work its way through.</p><p>Past works on open-ended task generation include ideas like <a href="https://arxiv.org/abs/1901.01753">POET</a>, where tasks are generated by 1) making 100 variations of a solved task, and 2) trying to solve as many of the variations as we can. This framework does work; but it's limited by the hard-coded method of making task variations. We can do something smarter.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-13-at-4.28.24-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2022/05/Screen-Shot-2022-05-13-at-4.28.24-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2022/05/Screen-Shot-2022-05-13-at-4.28.24-PM.png 1000w, https://kvfrans.com/content/images/2022/05/Screen-Shot-2022-05-13-at-4.28.24-PM.png 1434w" sizes="(min-width: 720px) 720px"></figure><p><a href="https://natashajaques.ai/publication/paired/">PAIRED</a>, as presented by Natasha Jaques, presents a more adaptive way of generating tasks. If we think about the kinds of tasks we want, its tasks that the agent is just barely capable of solving. PAIRED proposes a heuristic using two agents, a protagonist and an antagonist, and a task generator. The generator tries to propose tasks that the protagonist can't solve, but the antagonist can. This way, we're always generating tasks in the sweet spot where they're solvable, but the protagonist hasn't quite figured it out yet. "Hey protagonist", says the generator, "here's a task that your brother the antagonist can solve, so I know you can solve it too, so start trying".</p><p>I think these little loops are quite powerful, and we just need to find better places to apply them. As a researcher, if I want to train a robust AI agent, I should try and parametrize a super-expressive distribution of tasks, then let loose an open-ended process to gradually expand what it can solve.</p><h2 id="panel-discussion">Panel Discussion</h2><p>Along with the presentations, here were some fun thoughts from the panel discussions:</p><ul><li>Ken: What's the difference between AI and ALIFE? Well in AI we're somewhat focusing on economic value, whereas in ALIFE we really just care about what's cool. </li><li>Ken: ALIFE focuses too much on evolution, which we don't really need. But in AI and machine learning, we need to work on weirder and braver ideas.</li><li>Danijar: We need more open-ended environments. Atari is dumb, robotic control is too simple.</li><li>Ken: Environmental design for OE is a deep field. It needs to be understood. What gives our universe properties that lets it sustain OE for so long?</li><li>Julian: People who designed games like Skyrim and Elden Ring have figured out a bunch of heuristics that make good OE experiences.</li><li>Danijar: Minecraft kind of has crafting, but it's not OE crafting, it's hardcoded. We want to have emergent crafting, but also be computationally feasable.</li><li>Joel: The best part of ALIFE is the belief that it's totally possible to make life or life-like evolution, and it should be easy, and it's just right there. </li><li><strong>Moderator: Say you travel 10 years into the future, and there's an open-ended simulation that works. What does it look like?</strong></li><li>(everyone looks up and thinks)</li><li>Joel: We tackle the meta learning objective. We optimize for human preferences, we tell a system to keep making cool stuff that is cool to us as humans.</li><li>Natasha: Inter-group competition. We get something interesting where we have competition, and groups competing against each other, so we have cooperative stuff like language, communities, etc.</li><li>Julian: The system is super diverse. There are many things going on. I don't know what the principles are, but I won’t understand what’s going on. The metrics look great, but it's like looking at a coral reef, I don't know what's going on but there’s pretty colors. Like a coral reef, there will be many extremely different kinds of things.</li><li>Danijar: We get better and better at simulating some version of the real world, at a level of abstraction that is more and more accurate, maybe some MMO game to collect resources, or a mathematics environment where we discover more theorems.</li><li>Ken: There’s so many possibilities its hard to say, at a low level there’s creatures in a world that are reminiscent of nature, there’s theorem proving, agents that are with humans in the web, OE interacting with humans.</li></ul><p>For more, check out the ALOE <a href="https://sites.google.com/view/aloe2022">proceedings</a> and <a href="https://iclr.cc/virtual/2022/workshop/4547">workshop</a>.</p>]]></content:encoded></item><item><title><![CDATA[Kvfrans Thoughts: A Mathematical Definition of Interestingness]]></title><description><![CDATA[<h2 id="a-mathematical-definition-of-interestingness">A Mathematical Definition of Interestingness</h2><p>In open-endedness research, we care about building systems that continously generate interesting designs. So what makes something interesting? Previously, <a href="https://kvfrans.com/thoughts-oct-29/">I argued</a> that interestestness depends on a viewer's perspective. But this answer isn't very satisfying.</p><p>Ideally, we would want some mathematically-definable way to measure how interesting</p>]]></description><link>https://kvfrans.com/kvfrans-thoughts-nov-15/</link><guid isPermaLink="false">6192dc0c4365cb5561d981dc</guid><category><![CDATA[thoughts]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Mon, 06 Dec 2021 22:28:59 GMT</pubDate><content:encoded><![CDATA[<h2 id="a-mathematical-definition-of-interestingness">A Mathematical Definition of Interestingness</h2><p>In open-endedness research, we care about building systems that continously generate interesting designs. So what makes something interesting? Previously, <a href="https://kvfrans.com/thoughts-oct-29/">I argued</a> that interestestness depends on a viewer's perspective. But this answer isn't very satisfying.</p><p>Ideally, we would want some mathematically-definable way to measure how interesting something is. A naive approach would to be to define interestingness as variation or entropy, but we run into the following problem:</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/image-2.png" class="kg-image" alt></figure><p>In this example, you get the highest entropy by mixing milk and coffee, since at the molecular level the particles are the most widely distributed. But somehow the middle cup is the most interesting to us as humans, since the half-mixed milk forms all sorts of cool swirls and twists.</p><p>An analogy I like to draw here is the <a href="https://galaxykate0.tumblr.com/post/139774965871/so-you-want-to-build-a-generator">10,000 oatmeal problem</a>, which states:</p><blockquote>I can easily generate 10,000 bowls of plain oatmeal, with each oat being in a different position and different orientation, and <em>mathematically speaking</em> they will all be completely unique. But the user will likely just see <em>a lot of oatmeal</em>.</blockquote><p>Entropy is not all that great! Sure, we get a lot of unique designs, but at some level they all look like the same design. Something is missing in our interestingness formulation.</p><p>The best definition of interestingness I've come across so far is the idea of <em>sophistication</em>, which is mentioned in <a href="https://scottaaronson.blog/?p=762">this blog post by Scott Aaronson</a>:</p><blockquote>Sophistication(x) is the length of the shortest computer program that describes, not necessarily x itself, but a set S of which x is a “random” or “generic” member.</blockquote><p>Sophistication solves our milk-coffee problem and our oatmeal problem. In both cases, the high-entropy examples can be described by very short programs – just sample random positions for each particle. Something like the swirls in the half-mixed cup has higher sophistication, since you'd need a more complicated program to generate those artifacts.</p><p>How can we apply sophistication in the context of open-endedness and machine learning? Let's say we're trying to generate a lot of interesting image. One formulation is to do something like:</p><ol><li>An image gets less reward the more times it is generated.</li><li>Continously search for high-reward images.</li></ol><p>Obviously, this formulation kind of sucks, since the space of images is really big, and we will basically end up with a lot of images of noise. Ideally, we want to eliminate all images that look like 'static noise' in one swoop, and move on to more interesting kinds of variation.</p><p>Here's a sophistication-like formulation that makes use of smarter learning:</p><ol><li>A generative adversarial network is trained to model all past images that have been generated.</li><li>An image gets less reward if the GAN's discriminator thinks it looks in-distribution.</li><li>Continuously search for high-reward images and re-train the GAN.</li></ol><p>In theory, this GAN-based formulation should explore the image space in a more efficient manner. Once a few static-noise images have been generated, the GAN can model all types of static-noise, so high-reward images need to display other kinds of variation. </p><p>The GAN-based formulation kind of resembles sophistication, where we assume that Soph(x) is just -Disc(x), i.e. an image is sophisticated if it the GAN struggles to model it correctly. As long as we're continously re-training our GAN, this approximation shouldn't be too bad.</p><p>======</p><!--kg-card-begin: html--><!---

meta-thoughts on sophistication
it's just the same as curiosity
what we get is a function of "what states are harder to get to"
* because modeling by neural net is hard
* because finding the action seq that gets here is hard
* because creating an design that is seen as this object is hard
here's another option – can we just optimize directly for how hard something is to do. the interesting part is how to measure 'hard'
difficulty = new, but also hard to learn to do (N number of grads to imitate me)
difficulty = novel (tabular?)
difficulty = unstable
difficulty = shortest program to 
what are we trying to achieve?
emergent behavior that is interesting, to us as humans
technical: ground what it means to interesting
at some point, we do need to tie these things back into what humans call interesting
other definition = interesting if it can be used for downstream task
quality-diversity -> quality-[] difficulty? diversity?

agent parkour = agents in a world, and they try to mimic each other, and they get reward if they can't mimic each other, we can have a reward function to bootstrap off of. Maybe there is a cheaper way to measure 'hard to mimic' than imitation? Heuristics such as: less contact with ground, more instability, center of mass is not supported,
(other idea = mimic)
innovator: trained with RL, reward = inverse of fakabilityfaker: given state sequence, output action sequence that matchesgoal-proposer: come up with a imagegoal-achiever: make imagegoal-proposer: come up with a imagegoal-achiever: autoencode this image
goal-proposer: come up with a clip pointgoal-achiever: make image that matches that point
netork input: CLIPoutput: pixelsnetwork in: CLIPOUtput: VQGAN zoutput 2: pixelsStarting point: use translator as a jump, then optimize a little bit aftergoal-proposer: uniformly sample pointsgoal-achiever: make image that matches that point
one definition that leads to inequality in clip space: some images are harder to generate sequentially
Interestnegness is at some point philosophically a means to acheving quality.

--><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[To extract information from language models, optimize for causal response]]></title><description><![CDATA[<p>One way to view AI progress is that we can measure increasingly abstract concepts.<br>A decade ago, it was really hard to take a photo and measure “how much does this image look like a dog?”. Nowadays, just pick your favorite image classification model and run the image through. Similarly,</p>]]></description><link>https://kvfrans.com/causal-language-model/</link><guid isPermaLink="false">6181770c4365cb5561d97f16</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Fri, 12 Nov 2021 20:56:33 GMT</pubDate><content:encoded><![CDATA[<p>One way to view AI progress is that we can measure increasingly abstract concepts.<br>A decade ago, it was really hard to take a photo and measure “how much does this image look like a dog?”. Nowadays, just pick your favorite image classification model and run the image through. Similarly, it used to be unfeasable to measure “how realistic is this photo?”, but with GANs, we can do it easily.</p><p>With the rise of powerful language models, e.g. GPT-3, we’ve unlocked the ability to work with even higher-level ideas. GPT contains a powerfully rich representation of our world, and it’s been shown to solve logical puzzles, summarize large texts, and utilize factual knowldge about the human world.</p><p>Can we somehow extract this knowledge out of GPT? Let’s say an alien wants to learn about the human world, so it goes up to its local GPT engine and asks, “What is coffee?”</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-08-at-6.18.09-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-08-at-6.18.09-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-08-at-6.18.09-PM.png 1000w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-08-at-6.18.09-PM.png 1440w" sizes="(min-width: 720px) 720px"></figure><p>“Oh, got it”, the alien says, “so coffee is a kind of orange juice.” This definition is shallow! Coffee is definitely something you drink in the morning, but so are a lot of other things. This answer is vague, and vagueness is a common trend when asking GPT to describe various concepts:</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-08-at-6.25.56-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-08-at-6.25.56-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-08-at-6.25.56-PM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-08-at-6.25.56-PM.png 1600w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-08-at-6.25.56-PM.png 1860w" sizes="(min-width: 720px) 720px"></figure><p>How can we improve these results? Let’s take a step back. We know that GPT has good knowledge of what coffee is, but that doesn’t mean it’s good at <em>telling us</em> what it knows. We want a stronger way of interacting with GPT.</p><p>A nice field to borrow from is <a href="https://distill.pub/2017/feature-visualization/">feature visualization</a>, which focuses on understanding what types of inputs activate various areas of neural networks. Crucially, feature visualization techniques tend to examine <em>causal </em>relationships – results are obtained by optimizing through the neural network many times, and discovering an input which maximizes a certain neuron activation.</p><p>So, let’s do the same with language models. Instead of asking GPT what it thinks about certain topics, let’s just search for phrases which directly affect what GPT is holding in memory. We can do this through what I’ll call <em>causal-response optimization,</em> which tries to maximize the probability of GPT replying “Yes” to a specific question.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-12.26.00-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-12-at-12.26.00-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-12-at-12.26.00-AM.png 1000w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-12.26.00-AM.png 1260w" sizes="(min-width: 720px) 720px"></figure><p>Does this metric actually mean anything? It's hard to confirm this quantitatively, but we can at least look at how the causal-response score of "Am I thinking of coffee?" responds to some human-written options.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-02-at-9.01.41-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-02-at-9.01.41-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-02-at-9.01.41-PM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-02-at-9.01.41-PM.png 1600w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-02-at-9.01.41-PM.png 2396w" sizes="(min-width: 720px) 720px"></figure><p>OK, it's not bad! The more details the phrase has, the higher the likeihood that GPT responds with "Yes". That sounds reasonable. Now, we just need to replace the human-written options with automatically generated options.</p><p>There’s actually a whole body of work, known as <a href="https://arxiv.org/abs/2107.13586">“prompting”</a>, that deals with how to locate phrases that adjust language model behavior. But we’re going to take a simple approach: 1) generate 20 varied answers for “What is coffee?”, and 2) select the answer with the greatest causal-response score for “Am I thinking of coffee?”.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-25-at-10.55.56-AM-1.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-25-at-10.55.56-AM-1.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-25-at-10.55.56-AM-1.png 1000w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-25-at-10.55.56-AM-1.png 1586w" sizes="(min-width: 720px) 720px"></figure><p>How well does this setup work? Here’s a comparison example on the word-definition task we looked at earlier.</p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-1.05.52-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-12-at-1.05.52-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-12-at-1.05.52-AM.png 1000w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-1.05.52-AM.png 1372w" sizes="(min-width: 720px) 720px"></figure><p>Nice, we're on the right track. Generating text through causal-response optimization results in substantially different results than pure forward-prediction. In general, the causal response definitions are more detailed and specific, which makes sense in this case, since the objective is to maximize GPT's recognition of the word.</p><p>What else can we do with causal-response optimization? Well, it turns out that GPT has a slight tendency to lie when doing forward-prediction. Causal-optimization can help deal with this issue. Take a look at the following list-completions:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-1.01.46-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-12-at-1.01.46-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-12-at-1.01.46-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-12-at-1.01.46-AM.png 1600w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-12-at-1.01.46-AM.png 1654w" sizes="(min-width: 720px) 720px"><figcaption>Note: Suwa is a city, not a prefecture. Note 2: These are cherry-picked examples, not rigorously tested</figcaption></figure><p>The forward-prediction answers seem right at first glance, but they're a little wrong ... you can't read a painting. If we use causal-response optimization, we get "art in some form", which is a little more satisfying.</p><p>Another fun thing we can do is use causal-response optimization to steer long-form text generation. GPT does a nice job of evaluating various aspects of text through yes-no answers:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-02-at-10.27.38-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-02-at-10.27.38-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-02-at-10.27.38-PM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-02-at-10.27.38-PM.png 1600w, https://kvfrans.com/content/images/size/w2400/2021/11/Screen-Shot-2021-11-02-at-10.27.38-PM.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Since we can now measure concepts like happiness or old-man-ness, we can also optimize for them.</p><p>Let's look at a "story about apples". If we wanted to hear it from the perspective of an old man, one option is to adjust our forward-prediction prompt like so:</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-10-at-10.59.44-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-10-at-10.59.44-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-10-at-10.59.44-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-10-at-10.59.44-AM.png 1600w, https://kvfrans.com/content/images/size/w2400/2021/11/Screen-Shot-2021-11-10-at-10.59.44-AM.png 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>Or, we could apply causal-response optimization to each sentence generated. For each line, ten sentence options are generated, and the one that has the best score for "Is this something an old man would say?" is chosen.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-10-at-11.01.15-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Screen-Shot-2021-11-10-at-11.01.15-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Screen-Shot-2021-11-10-at-11.01.15-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/11/Screen-Shot-2021-11-10-at-11.01.15-AM.png 1600w, https://kvfrans.com/content/images/2021/11/Screen-Shot-2021-11-10-at-11.01.15-AM.png 2130w" sizes="(min-width: 1200px) 1200px"></figure><p>Almost every sentence in the causal-optimization story reinforces the idea of an old man speaking: there are references to growing up in the country, apples straight off the tree, and the absence of supermarkets.</p><p>So, as researchers, why should we care about causal-response optimization? </p><p>I'm not trying to argue that this method constantly provides better quality results. The examples in this post are mostly anecdotal, and are somewhat cherry-picked to showcase key differences. While I'm sure causal-response can be used to improve text-generation if applied correctly, that claim would require more rigor than I can provide at the moment.</p><p>What I do want to present, are some hints that causal-response is a useful tool for extracting knowledge out of language models. Causal optimization results in substantially different content than forward-prediction – we're explicitly probing into the language model, instead of relying on the model to say what it is thinking. We should explore this direction more! Strong pre-trained models are going to be around for a while, so understanding how they view the world is an important task to be looking at.</p><p>The code for these results is available at <a href="https://gist.github.com/kvfrans/1ba1f36d490b7a26c442d36f23e0941a">this gist</a>, although you need an OpenAI GPT-3 key to run it. I think the code is very readable, so it's probably easy to re-implement this stuff with e.g. GPT-Neo.</p><p>Further work can involve:</p><ul><li>How can we quantatively measure how good generated sentences are? <br>Are there good metrics for knowledge-extraction?</li><li>What questions do we have about large language-model representations, and can causal-optimization be used to answer those questions?</li><li>Are there better ways of optimizing text for causal-response than bootstrapping off forward-prediction? I.e. evolutionary search, gradient optimization, etc. (Somewhat addressed in Appendix)</li></ul><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Appendix
    </div><!--kg-card-end: html--><p>Cite as:</p><pre><code class="language-x">@article{frans2021causalresponse,
  title   = "To extract information from language models, optimize for causal response",
  author  = "Frans, Kevin",
  journal = "kvfrans.com",
  year    = "2021",
  url     = "https://kvfrans.com/causal-language-model/"
}</code></pre><p>This work isn't rigorous enough for me to publish it in a formal setting, but I would love to see further research the directions discussed here. Thanks to Phillip Isola and others in the <a href="http://web.mit.edu/phillipi/">Isolab</a> for the help :)</p><!--kg-card-begin: html--><br><div class="float-label" style="clear: both">
	Source Code
</div><!--kg-card-end: html--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://gist.github.com/kvfrans/1ba1f36d490b7a26c442d36f23e0941a"><div class="kg-bookmark-content"><div class="kg-bookmark-title">lang.ipynb</div><div class="kg-bookmark-description">GitHub Gist: instantly share code, notes, and snippets.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicons/favicon.svg"><span class="kg-bookmark-author">Gist</span><span class="kg-bookmark-publisher">262588213843476</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://github.githubassets.com/images/modules/gists/gist-og-image.png"></div></a></figure><!--kg-card-begin: html--><br><div class="float-label" style="clear: both">
	Things To Read
</div><!--kg-card-end: html--><p><a href="https://arxiv.org/pdf/2107.13586.pdf"><strong>"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"</strong></a><strong>, by Liu et al. </strong>A growing field in the last years deals with prompting for pre-trained language models. In prompting, instead of fine-tuning the network parameters on a task, the goal is to learn a <em>prompt phrase</em> that is appended to input text in order to influence the behavior of the language model. This paper is a good review of the area.</p><p><strong>"<strong><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></strong>", by Shin et al. </strong>This paper is one of the works in prompting, I picked it because it cleanly states the prompting problem and presents a framework. In AutoPrompt, the aim is to learn a set of trigger words which influences a language model into predicting the correct output for a set of tasks.</p><p><a href="https://arxiv.org/abs/2102.01645">"<strong>Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search"</strong></a><strong>, by Galatolo et al. </strong>CLIP-GlaSS is a way to move from text-to-image and image-to-text via CLIP encodings. There are similarities to prompting work here, especially in the image-to-text task, where the aim is to locate text which best matches a given image. </p><p>In CLIP-GlaSS they use an evolutionary search to locate text; I tried that initially in the causal-response optimization, but the method struggled with finding gramatically-correct answers.<strong> </strong>I played with various combinations of beam search, where language model logits are weighted by the causal-response objective, but the results had a lot of variation. In the end, generating options via forward-prediction and selecting via causal-response led to be most satisfying results.</p><p><strong><a href="https://arxiv.org/abs/1909.01066">"Language Models as Knowledge Bases?"</a>, by Petroni et al. </strong>This work is an analysis on BERT and how it performs well on various question-answering datasets without additional fine-tuning. To be honest, I chose this paper based on its name, which implies a perspective I agree with – language models are rich knowledge bases, and what we need are tools to extract this knowledge from them.</p></div></div>]]></content:encoded></item><item><title><![CDATA[Kvfrans Thoughts: Oct 29]]></title><description><![CDATA[<p>This post is a bunch of random thoughts about things I've read recently. It's organized into sections, and the sections don't really relate to each other, but here they are.</p><h2 id="neural-networks-are-data-digesters">Neural Networks are Data Digesters</h2><p>A common philosophy around my head is that "representation learning is everything". Need to label</p>]]></description><link>https://kvfrans.com/thoughts-oct-29/</link><guid isPermaLink="false">617c58914365cb5561d97d80</guid><category><![CDATA[thoughts]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Tue, 02 Nov 2021 17:16:40 GMT</pubDate><content:encoded><![CDATA[<p>This post is a bunch of random thoughts about things I've read recently. It's organized into sections, and the sections don't really relate to each other, but here they are.</p><h2 id="neural-networks-are-data-digesters">Neural Networks are Data Digesters</h2><p>A common philosophy around my head is that "representation learning is everything". Need to label some images? Train a linear classifier over a powerful representation space. Need to navigate a robot to pick up some blocks? Train a simple policy over a strong representation of the robot. If you can map your input data into a general enough representation, everything becomes easy.</p><p>Eric Jang's blog post <a href="https://evjang.com/2021/10/23/generalization.html">"Just Ask For Generalization"</a> takes this philosophy a step further. If what we really need is strong representations, and neural networks are really strong sponges for absorbing data, then we might as well cram as much relevant data as we can into our models. Eric argues:</p><blockquote>1. Large amounts of diverse data are more important to generalization than clever model biases.<br>2. If you believe (1), then how much your model generalizes is directly proportional to how fast you can push diverse data into a sufficiently high-capacity model.</blockquote><p>To me, it feels like we're reaching diminishing returns in designing new models – e.g, transformers are highly effective at modeling everything from text to video to decision-making. Progress lies, instead, in how we can feed these models the data they need to properly generalize in whatever dimensions we care about.</p><p>One hypothesis is that we should give our models as much data as we can, regardless of how relevant the data is to the task at hand. Take, for example, language models – the strongest models are pretrained on billions of arbitrary texts from the internet. In reinforcement learning, we're starting to see the same parallels. Traditionally, agents are trained to maximize rewards on the task at hand, but it turns out we get sizable improvements by training agents to <a href="https://arxiv.org/abs/1707.01495">reach past states</a>, <a href="https://arxiv.org/abs/2106.01345">achieve a range of rewards</a>, or <a href="https://arxiv.org/abs/1901.01753">solve many mutations of the task</a>.</p><p>If we believe this hypothesis, we should really be researching how to best extract relevant data for the task at hand.  Should a strong RL agent be trained to achieve many goals instead of one, or to achieve a spectrum of rewards instead of optimal reward? Should its model be able to predict future states? To contrast augmented data from fake data? Should it be trained over thousands of games, instead of just one?</p><h2 id="machine-learning-via-machine-learning">Machine Learning via Machine Learning</h2><p>MIT offered a meta-learning class last year, which I really wanted to take, except I was away on gap year so I couldn't. The highlight of the class is to build "a system that passes 6.036 Introduction to Machine Learning as a virtual student". That's so cool! Thankfully the group behind the class released a paper about it, so <a href="https://arxiv.org/pdf/2107.01238.pdf">here it is</a>.</p><blockquote>We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT’s 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations ... Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.</blockquote><p>Summary-wise, they've got a Transformer model which reads in machine learning questions, and outputs a graph of mathematical operators which solve the questions. It seems to work quite well, and passes the class with an A (!). </p><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/10/Screen-Shot-2021-10-29-at-5.36.24-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/10/Screen-Shot-2021-10-29-at-5.36.24-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/10/Screen-Shot-2021-10-29-at-5.36.24-PM.png 1000w, https://kvfrans.com/content/images/2021/10/Screen-Shot-2021-10-29-at-5.36.24-PM.png 1488w" sizes="(min-width: 720px) 720px"></figure><p>A lot of the benefit seems to come from the data augmentation:</p><blockquote>After collecting questions from 6.036 homework assignments, exercises, and quizzes, each question is augmented by paraphrasing the question text. This results in five variants per question. Each variant, in turn, is then augmented by automatically plugging in 100 different sets of values for variables. Overall, we curate a dataset consisting of 14,000 augmented questions.</blockquote><p>One thing that's a bit dissapointing is that coding questions are skipped. In my opinion well-formulated coding questions are the best part of classes, since they force you to replicate the content and really understand it. Maybe a tandem team with <a href="https://openai.com/blog/openai-codex/">OpenAI Codex</a> can pass the course in full.</p><h2 id="what-makes-something-interesting">What makes something interesting?</h2><p>In open-endedness research, we want to build systems that can keep producing unbounded amounts of interesting things. There are many problems with this definition. What qualifies as unbounded? What makes something interesting? What makes something a thing?</p><p>One answer is that all these questions depend on the perspective that we look at the world in. <a href="https://www.youtube.com/watch?v=2gExH4Tzkzc">Alyssa M Adams' talk at Cross Roads</a> this month gives an example:</p><blockquote>The sea was never blue. Homer described the sea as a deep wine color. The greek color experience was made of movement and shimmer, while modern humans care about hue, saturation and brightness.</blockquote><p>It's a representation learning problem! Interestingness isn't a fact by itself, an object is only interesting to the agent that is looking at it. Normally, this answer isn't that satisfying, since modeling what is interesting <em>to humans</em> is expensive at best and inaccurate at worst.</p><p>With the rise of <a href="https://arxiv.org/abs/2108.07258">foundation models</a>, however, we can make a non-trivial argument that AI models like <a href="https://openai.com/blog/gpt-3-apps/">GPT-3 </a>and <a href="https://openai.com/blog/clip/">CLIP</a> are learning strong human-centric representations of reality. Could we just, like, <em>ask</em> these models what they think is interesting? </p><p>Maybe GPT-3 thinks "walking" and "running" are too similar, but "flying" is qualitatively different. Maybe CLIP thinks a truck is a truck regardless of what roads it's on, but if the truck is in the ocean, now that is interesting.</p><h2 id="interestingness-in-architecture">Interestingness in Architecture</h2><p>"Nearly everything being built is boring, joyless, and/or ugly", writes Nathan Robinson. In <a href="https://www.currentaffairs.org/2021/04/when-is-the-revolution-in-architecture-coming">an April article</a>, Robinson showcases colorful classical temples, mosques, and coastal villages. Then he shows photographs of modern brutalist design – cold and gray.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://kvfrans.com/content/images/2021/11/Venice-1024x682.png" width="1024" height="682" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/Venice-1024x682.png 600w, https://kvfrans.com/content/images/size/w1000/2021/11/Venice-1024x682.png 1000w, https://kvfrans.com/content/images/2021/11/Venice-1024x682.png 1024w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://kvfrans.com/content/images/2021/11/enough-1024x899.jpg" width="1024" height="899" alt srcset="https://kvfrans.com/content/images/size/w600/2021/11/enough-1024x899.jpg 600w, https://kvfrans.com/content/images/size/w1000/2021/11/enough-1024x899.jpg 1000w, https://kvfrans.com/content/images/2021/11/enough-1024x899.jpg 1024w" sizes="(min-width: 720px) 720px"></div></div></div></figure><blockquote>What is beauty? Beauty is that which gives aesthetic pleasure ... a thing can be beautiful to some people and not to others. Pritzker Prize-winning buildings are <em><em>beautiful to architects</em> ..</em>. They are just not beautiful to me.</blockquote><p>Hey, look, it's the problem of interestingness again. What makes a building lively, welcoming, or as Christopher Alexander puts it, contains <a href="https://en.wikipedia.org/wiki/The_Timeless_Way_of_Building">"the quality without a name"</a>? There's probably no objective answer, but there is a human-centered answer. Could an AI understand architectural aesthetics? If it has the right representations, I don't see why not.</p>]]></content:encoded></item><item><title><![CDATA[CLIPDraw: Exploring Text-to-Drawing Synthesis]]></title><description><![CDATA[<!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/clipdraw_cropped.mp4">
                </video>
                <figcaption>
CLIPDraw synthesizes novel drawings from text. <a href="https://colab.research.google.com/github/ kvfrans/clipdraw/blob/main/clipdraw.ipynb">Play with the codebase yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><p>AI-assissted art has always been intriguing to me. To me, visual art is a very human thing -- I can’t imagine a way that a computer rediscovers our own cultural concepts without some kind of experience living in</p>]]></description><link>https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/</link><guid isPermaLink="false">60d36b8e17e26d47c77e91e1</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Mon, 28 Jun 2021 20:07:30 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/clipdraw_cropped.mp4">
                </video>
                <figcaption>
CLIPDraw synthesizes novel drawings from text. <a href="https://colab.research.google.com/github/ kvfrans/clipdraw/blob/main/clipdraw.ipynb">Play with the codebase yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><p>AI-assissted art has always been intriguing to me. To me, visual art is a very human thing -- I can’t imagine a way that a computer rediscovers our own cultural concepts without some kind of experience living in our world. So when AI methods are able to learn from us, and produce artwork that we as humans might make ourselves, something cool is definitely happening.</p><p>This post presents <em>CLIPDraw</em>, a text-to-drawing synthesis method that I’ve been playing with nonstop for the past few weeks. At its core, CLIPDraw is quite simple, yet it is able to produce drawings that display a whole range of interesting behaviors and connections. So for this article, I want to focus on showing off behaviors of CLIPDraw that I personally found intriguing and wanted to learn more about. For more detailed analysis and technical detail, check out the<a href="https://arxiv.org/abs/2106.14843"> CLIPDraw paper</a>, or play around with <a href="https://colab.research.google.com/github/ kvfrans/clipdraw/blob/main/clipdraw.ipynb">the Colab notebook</a> yourself!</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png 1000w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png 1080w"></figure><h2 id="image-synthesis-drawing-synthesis">Image Synthesis -&gt; Drawing Synthesis</h2><p>The field of text-to-image synthesis has a broad history, and recent methods have shown stunningly realistic image generation through GAN-like methods. Realism, however, is a double-edged sword – there's a lot of overhead in generating photorealistic renderings, when often all we want are simple drawings. With CLIPDraw, I took inspiration from the web game <a href="https://skribbl.io/">Skribbl.io</a>, where players only have a few seconds to draw out a word for other players to guess. What if an AI could play Skribbl.io? What would it draw? Are simple shapes enough to represent increasingly complex concepts?</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-18-at-11.05.09-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-18-at-11.05.09-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-18-at-11.05.09-AM.png 1000w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-18-at-11.05.09-AM.png 1214w" sizes="(min-width: 1200px) 1200px"></figure><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-18-at-11.12.52-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-18-at-11.12.52-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-18-at-11.12.52-AM.png 1000w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-18-at-11.12.52-AM.png 1212w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="how-does-clipdraw-work">How does CLIPDraw work?</h2><p>CLIPDraw's knowledge is powered through a pre-trained CLIP model, a recent release which I definitely reccommend <a href="https://openai.com/blog/clip/">reading about.</a> In short, a CLIP model consists of an image encoder and a text encoder, which both map onto the same represenational space. This setup allows us to measure the <em>similarities</em> betweeen images and text. And if we can measure similarities, we can also try to discover images that maximize that similarity, therefore matching a given textual prompt.</p><p>The basic CLIPDraw loop follows this principle of synthesis-through-optimization. First, start with a human-given description prompt and a random set of Bezier curves. Then, gradually adjust those curves through gradient descent so that the drawing best matches the given prompt. There are a few tricks that also help, <a href="https://arxiv.org/abs/2106.14843">as detailed in the paper,</a> but this loop is basically all there is to it.</p><p>Now, let's examine what CLIPDraw comes up with in practice.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.35.48-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-23-at-10.35.48-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-23-at-10.35.48-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/Screen-Shot-2021-06-23-at-10.35.48-AM.png 1600w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.35.48-AM.png 1867w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="what-visual-techniques-does-clipdraw-use">What visual techniques does CLIPDraw use?</h2><p>A reoccuring theme is that CLIPDraw tends to interpret the description prompts in multiple, unexpected ways. A great example for this is "<em>A painting of a starry night sky</em>", which shows a painterly-styled sky with a moon and stars, along with an actual painting canvas and painter in the foreground, which then also features black and blue swirls resembling Van Gogh's <em>"The Starry Night"</em>.</p><p>CLIPDraw also likes to use abstract symbols, the most prominent being when it writes out the literal word inside the image itself. Sometimes, CLIPDraw will use tangentially related concepts, like the Google Maps screenshot when asked for "自転車 (<em>Bicycle</em> in Japanese)”. A fun result is the prompt for "<em>Fast Food", </em>which shows hamburgers and a McDonald's logo, but also has a bunch of joggers racing in the background.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.31.19-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-23-at-10.31.19-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-23-at-10.31.19-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/Screen-Shot-2021-06-23-at-10.31.19-AM.png 1600w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.31.19-AM.png 1855w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="how-does-clipdraw-react-to-different-styles">How does CLIPDraw react to different styles?</h2><p>An experiment I really enjoyed was to synthesize images of cats, but in different artistic styles. With CLIPDraw, this was as easy as changing the descriptor adjectives of the textual prompt. Surprisingly, CLIPDraw is quite robust at handling different styles, contrary to the initial intent to synthesize scribble-like drawings.</p><p>One interesting feature is that CLIPDraw adjusts not only the textures of drawings, ala <a href="https://arxiv.org/abs/1508.06576">Style Transfer</a> methods, but also the structure of the underlying content. In the cat experiments, asking for <em>"a drawing"</em> produces a simplified cartoonish cat, while prompts like <em>"a 3D wireframe"</em> produce a cat in perspective, with depth and shadows.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.41.16-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-23-at-10.41.16-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-23-at-10.41.16-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/Screen-Shot-2021-06-23-at-10.41.16-AM.png 1600w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.41.16-AM.png 1767w" sizes="(min-width: 1200px) 1200px"><figcaption>"The Eiffel Tower"</figcaption></figure><h2 id="does-stroke-count-affect-the-drawings-that-clipdraw-produces">Does stroke count affect the drawings that CLIPDraw produces?</h2><p>A key aspect of CLIPDraw is that drawings are represented as a set of Bezier curves rather than a matrix of pixels. This feature gives us a nice parameter to tweak in the number of curves a drawing is comprised of.</p><p>Emperically, drawings with low stroke counts tend to result in a more cartoonish or abstract representation of the prompt, such as the 16-stroke version of <em>"The Eiffel Tower"</em> being basically made of a few straight lines. As stroke count is increased, CLIPDraw begins to target more structured shapes, shown as our Eiffel Tower begins gaining 3D perspective, lights, and finally a background.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.36.16-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-23-at-10.36.16-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-23-at-10.36.16-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/Screen-Shot-2021-06-23-at-10.36.16-AM.png 1600w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.36.16-AM.png 1739w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="what-happens-if-abstract-words-are-given-as-a-prompt">What happens if abstract words are given as a prompt?</h2><p>A fun thing to push the limits is to give CLIPDraw abstract descriptions, and see what it does with them. As a human artist, even I would have to stop and think about how to convey these concepts through visuals, so it's interesting to see how the AI will approach things.</p><p>In most cases, CLIPDraw likes to use symbols to showcase concepts that are culturally related to the given phrase, like the fireworks and smiles in <em>"Happiness"</em> or the Japanese and English-like letters in <em>"Translation"</em>. </p><p>My favorite here is the drawing for <em>"Self"</em>, which features a body holding up multiple heads. The drawing can almost be seen as a metaphor for e.g. the idea that a person's self may contain multiple outward personalities, or that a self is actually a sum of many cognitive processes. This piece is defintely the most "art-like" example I came across; there's a lot of room for individual interpretation, and it almost feels like CLIP knows something that I don't.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><figure class="kg-card kg-image-card kg-width-wide"><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.40.08-AM.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/Screen-Shot-2021-06-23-at-10.40.08-AM.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/Screen-Shot-2021-06-23-at-10.40.08-AM.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/Screen-Shot-2021-06-23-at-10.40.08-AM.png 1600w, https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-23-at-10.40.08-AM.png 1891w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="can-drawings-be-fine-tuned-via-negative-prompts">Can drawings be fine-tuned via negative prompts?</h2><p>A final experiment was to see if CLIPDraw behavior could be finely adjusted by introducing additional optimization objectives. In the normal process, drawings are optimized to best match a textual prompt. What if we also tried to optimize unsimilarity with a set of <em>negative prompts</em>?</p><p>In a few situations, this works! By penalizing the prompt for "Words and text", the <em>"Hashtag</em>" drawing features less prominent words and instead draws a set of selfie-like faces. Negative prompts can also do things like adjust the color of images, or force drawings to contain only a single subject rather than many.</p><p>Practially, the examples above were pretty hard to achieve, and most of the time negative prompts don't change much about the final drawing at all. I think there's still a lot of room for experimentation on how to improve this technique. One thing I'd love to see is a panacea prompt, like "A messy drawing", that consistently improves drawing quality if used as a negative objective regardless of context.</p><!--kg-card-begin: html--><hr class="gridhr"><br><!--kg-card-end: html--><h2 id="clipdraw-parting-thoughts">CLIPDraw: Parting Thoughts</h2><p>The CLIPDraw algorithm isn't particularly novel; people have doing synthesis-through-optimization for a while through activation-maximization methods, and recently through CLIP-matching objectives. However, I do believe biasing towards drawings rather than photorealism gives images more freedom of expression, and optimizing Bezier curves is a nice way to do this efficiently. I also personally love this artstyle and I think the drawings are quite similar to what an artist would produce ;). </p><p>That being said, I do think the behaviors showcased here should be pretty generalizable to any CLIP-based optimization method. Already a few extensions come to mind – Can we synthesize videos? 3D models? Can an AI play <a href="https://skribbl.io/">Skribbl</a> or <a href="https://www.brokenpicturephone.com/">Broken Picture Phone</a> with itself? I'm sure you as a reader have a bunch of ideas I haven't even considered. So please, feel free to take this method and go wherever you feel is exciting. And then <a href="https://twitter.com/kvfrans">tell me about it!</a></p><p>You can experiment with <a href="https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb">the Colab notebook here</a>, and play with CLIPDraw in the browser. Results are generally pretty quick (within a minute) unless you crank up stroke count and iterations. You can also check out <a href="https://arxiv.org/abs/2106.14843">the full paper</a>, which contains a bit of a deeper analysis and details on the technical implementation.</p><!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                                Thanks for reading!<br>
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" style="width:10em" autoplay muted playsinline loop src="https://kvfrans.com/static/outputleo.mp4">
                </video>
                <figcaption>
                    
"Leonardo Dicaprio cheers meme template"
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Appendix
    </div><!--kg-card-end: html--><p>Thank you to everyone at <a href="http://crosslabs.org/">Cross Labs</a> for assisstance and feedback.</p><!--kg-card-begin: html--><div class="float-label" style="clear: both">
	Things To Read
</div><!--kg-card-end: html--><p><strong><a href="https://kvfrans.com/p/2db72cf5-f055-4ab2-990c-23ac65f4a138/arxiv">"CLIPDraw: Exploring Text-to-Drawing Synthesis via Language-Image Encoders"</a>, by Frans et al. </strong>Of course a nice place to start is the paper for CLIPDraw! The paper mostly contains a similar focus on analyzing interesting behaviors, but also includes some comparisons and a Related Work section of good papers to read up on.</p><p><strong><a href="https://openai.com/blog/clip/">"CLIP: Connecting Text and Images"</a>, by Radford et al. </strong>This is the blog for the original CLIP release, from OpenAI. CLIP is a language-image dual encoder that maps text and images onto the same feature space. Trained on a huge amount of online data, CLIP ends up being super robust and can solve a bunch of image tasks without any additional training. <a href="https://arxiv.org/abs/2103.00020">(Paper)</a></p><p><strong><a href="https://people.csail.mit.edu/tzumao/diffvg/">"Differentiable Vector Graphics Rasterization for Editing and Learning"</a>, by Li et al.</strong> This work is the differentiable vector graphics methods that we use in CLIPDraw to backprop through Bezier curves. It's really impressive and basically works out of the box, so I highly reccommend giving it a read.</p><p><strong><a href=" https://arxiv.org/abs/2105.00162">"Generative Art Using Neural Visual Grammars and Dual Encoders"</a>, by Fernando et al. </strong>This paper is another CLIP-based generative art method, also using stroke-based images, but with an emphasis on AI creativity. Instead of backprop, they use evolutionary methods to evolve a custom LSTM-based grammar which defines the strokes. Also their figures are cool.</p><p><strong><a href="https://twitter.com/images_ai">"Images Generated By AI Machines"</a>, by <a href="https://twitter.com/samburtonking">@samburtonking</a>. </strong>Also check out the <a href="https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing">Colab</a> <a href="https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT?fbclid=IwAR30ZqxIJG0-2wDukRydFA3jU5OpLHrlC_Sg1iRXqmoTkEhaJtHdRi6H7AI#scrollTo=CppIQlPhhwhs">notebooks</a> by <a href="https://twitter.com/advadnoun">@advadnoun</a> and <a href="https://twitter.com/RiversHaveWings">@RiversHaveWings</a>, respectively. This twitter account basically shows off cool generative art made from CLIP + GAN optimization methods. A lot of interesting pieces have been made and it's well worth looking through.</p><p><strong><a href="https://github.com/ajayjain/VectorAscent">"VectorAscent: Generate vector graphics from a textual description"</a> by Ajay Jain. </strong>Here's a project from earlier this year that actually used the same techniques of combining CLIP and diffvg. The key difference is in the image augmentation, which seems to help in ensuring synthesized drawings look more consistent.</p></div></div>]]></content:encoded></item><item><title><![CDATA[StampCA: Growing Emoji with Conditional Neural Cellular Automata]]></title><description><![CDATA[<!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/emoji.mp4"></video>
                <figcaption>
StampCA growing emoji. <a href="https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg#scrollTo=8_qZe_c1uPHf">Play with the codebase yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><p>When a baby is born, it doesn’t just appear out of nowhere -- it starts as a single cell. This seed cell contains all the information needed to replicate and grow into a full adult. In biology, we call this process</p>]]></description><link>https://kvfrans.com/stampca-conditional-neural-cellular-automata/</link><guid isPermaLink="false">6062a14517e26d47c77e8f44</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Sat, 03 Apr 2021 08:54:15 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/emoji.mp4"></video>
                <figcaption>
StampCA growing emoji. <a href="https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg#scrollTo=8_qZe_c1uPHf">Play with the codebase yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><p>When a baby is born, it doesn’t just appear out of nowhere -- it starts as a single cell. This seed cell contains all the information needed to replicate and grow into a full adult. In biology, we call this process <em>morphogenesis</em>: the development of a seed into a structured design. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh5.googleusercontent.com/fhkkGwxPLLkTrMwxA97oC03menLZUIjv9wN8sZpDecGxfEA08O_lFcq9KsU7VKp1nFlF_1DigX1LGfgkwPx3pJ0QJQLDr8VYUUcWKAtz6Zg1WpGTtdUKCOuFzEewfe-k4qgx_dMO" class="kg-image" alt><figcaption>Morphogenesis builds up an embyro. https://www.nature.com/articles/s41467-018-04155-2</figcaption></figure><p>Of course, if there’s a cool biological phenomenon, someone has tried to replicate it in artificial-life land. One family of work examines Neural Cellular Automata (NCA), where a bunch of cells interact with their neighbors to produce cool emergent designs. By adjusting the behavior of the cells, we can influence them to grow whatever we want. NCAs have been trained to produce a whole bunch of things, such as <a href="https://distill.pub/2020/growing-ca/">self-repairing images</a>, <a href="https://distill.pub/selforg/2021/textures/">patternscapes</a>, <a href="https://arxiv.org/abs/2102.02579">soft robots,</a> and <a href="https://arxiv.org/pdf/2103.08737.pdf">Minecraft designs</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh4.googleusercontent.com/f7HLyTjDE5Nrri0h96md7NDbzxTEEupm3wGVdYIEZFO1y4lBeytkRiKuq_uAevQsD8wpXvLEQMdOPCBnuVsJATDDiG1aWUoXvO34eeRxQzrJg3DvaOJshtFavHPAX6LSPZzRZvC_" class="kg-image" alt><figcaption>Growing a Minecraft tree through Neural Cellular Automata. https://arxiv.org/pdf/2103.08737.pdf</figcaption></figure><p>A common thread in these works is that a single NCA represents a single design. Instead, I wanted to explore <em>conditional</em> NCAs, which represent design-producing functions rather than single designs. The analogy here is DNA translation -- our biologies don’t just define our bodies, they define a system which translates DNA into bodies. Change the DNA, and the body changes accordingly.</p><p>This post introduces StampCA, a conditional NCA which acts like a translation network. In StampCA, the starting cell receives an encoding vector, which dictates what design it must grow into. StampCA worlds can mimic the generative capabilities of traditional image networks, such as an MNIST-digit GAN and an Emoji-producing autoencoder. Since design-specific information is stored in cell state, StampCA worlds can 1) support morphogenetic growth for many designs, and 2) grow them all in the same world.</p><p>At the end, I’ll go over some thoughts on how NCAs relate to work in convnets, their potential in evo-devo and artificial life, and why I’m pretty excited about them looking ahead.</p><h2 id="neural-cellular-automata-how-do-they-work">Neural Cellular Automata: How do they work?<br></h2><p>First, let’s go over how an NCA works in general. The ideas here are taken from this <a href="https://distill.pub/2020/growing-ca/">excellent Distill article by Mordvintsev et. al</a>. Go check it out for an nice in-depth explanation.</p><p>The basic idea of an NCA is to simulate a world of cells which locally adjust based on their neighbors. In an NCA, the world is a grid of cells. Let’s say 32 by 32. Each cell has a state, which is a 64-length vector. The first four components correspond to RGBA, while the others are only used internally.</p><p>Every timestep, all cells will adjust their states locally. Crucially, cells can only see the states of themselves and their direct neighbors. This information is given to a neural network, which then updates the cell's state accordingly. The NCA will iterate for 32 timesteps.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://lh4.googleusercontent.com/veb-wU-ubWAbLuz9ItBP0No5UzSM_2UJSwlyscd-326_KRLMzgkitfsZzBDte7JFFTGG68ukZsV9FfmbGP2nKospbP5RjGMCNB1WgO9GReV9tw_JU7aRxm_8baLRS_rhJK6Ne_qQ" class="kg-image" alt><figcaption>One timestep of an NCA. Every cell updates their state based on their neighbors' states, as defined by a convolutional neural network.</figcaption></figure><p>The key idea here is to learn a set of local rules which combine to form a global structure. Cells can only see their neighbors, so they need to rely on their local surroundings to figure out what to do. Again, the analogy is in biological development -- individual cells adjust their structure based on their neighbors, creating a functional animal when viewed as a whole.</p><p>In practice, we can define an NCA as a 3x3 convolution layer, applied recursively many times. This convolutional kernel represents a cell's receptive field – each cell updates its own state based on the states of its neighbors. This is repeated many times, until a final result is produced. Since everything is differentiable, we can plug in our favorite loss function and optimize.</p><p>By being creative with our objectives, we can train NCAs with various behaviors. The classic objective is to start with every cell set to zero except a single starting cell, and to grow a desired design in N timesteps. We can also train NCAs that <em>maintain</em> designs by using old states as a starting point, or NCAs which <em>repair</em> designs by starting with damaged states. There’s a lot of room for play here.</p><!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <img class="b-lazy b-loaded gridvideo" src="https://kvfrans.com/static/open/stampca2/diagram2.png">
                <figcaption>
Different objectives for training NCAs.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<br>
<br><!--kg-card-end: html--><p>The common shortcoming here is that the NCA needs to be retrained if we want to produce a different design. We’ve successfully learned an algorithm which locally generates a global design -- now can we learn an algorithm which locally generates <em>many</em> global designs?</p><h2 id="stampca-conditional-neural-cellular-automata">StampCA: Conditional Neural Cellular Automata</h2><p><br>One way to view biological development is as a system of two factors: DNA and DNA translation. DNA produces a code; a translation system then creates proteins accordingly. In general, the same translation system is present in every living thing. This lets evolution ignore the generic heavy work (how to assemble proteins) and focus on the variation that matters for their species (which proteins to create).</p><p>The goal of a conditional NCA is in the same direction: separate a system which <em>grows</em> designs from a system which <em>encodes</em> designs. The key distinction is that encodings are design-specific, whereas the growth system is shared for all designs.</p><p>It turns out that the NCA formulation gives us a straightforward way to do this, which we’ll refer to as StampCA. In a typical NCA, we initialize the grid with all zeros except for a seed cell. Instead of initializing this seed cell with a constant, let’s set its state to an <em>encoding vector.</em> Thus, the final state of the NCA becomes a function of its neural network parameters along with the given encoding.</p><!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <img style="height: auto" class="b-lazy b-loaded gridvideo" src="https://kvfrans.com/static/open/stampca2/diagram3.png">
                <figcaption>
A StampCA is an NCA where the seed is conditioned on a learned encoding.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<br><!--kg-card-end: html--><p>Armed with this, we can construct an autoencoder-type objective. Given a desired design, an encoder network first compresses it into an encoding vector. This encoding vector is used as the initial state in the NCA, which then iterates until a final design has been created. We then optimize everything so the final design matches the desired design.</p><p>A correctly-trained StampCA learns general morphogenesis rules which can grow an arbitrary distribution of designs. There are two main advantages over single-design NCAs: </p><ol><li>StampCAs can grow new designs without additional training</li><li>Many designs can all grow in the same world</li></ol><p>Let’s look at some examples on Emoji and MNIST datasets.</p><h2 id="stampca-autoencoder-growing-emojis">StampCA Autoencoder: Growing Emojis</h2><p>The <a href="https://distill.pub/2020/growing-ca/">original Distill paper</a> trained NCAs to grow emojis, so let’s follow suit. We’ll take a dataset of ~3000 32x32 emojis from <a href="https://github.com/googlefonts/noto-emoji/tree/main/png/32">this font repo</a>. Our aim here is to train a StampCA which can grow any emoji in the dataset.</p><p>After training for about 4 hours on a Colab GPU, here’s the outputs:</p><!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/stampca_compare.mp4"></video>
                <figcaption>
StampCA Emojis growing. <a href="https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg#scrollTo=8_qZe_c1uPHf">Replicate these results yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><p>Overall, it’s doing the right thing! A single network can grow many kinds of emoji. </p><p>For a StampCA network to correctly work, two main problems need to be solved: cells need to share the encoding information with each other, and cells need to figure out where they are relative to the center.</p><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure">
<video class="b-lazy b-loaded gridvideo-full" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/stampca_zoom.mp4"></video>
    <figcaption>Ripple growth pattern.</figcaption>
</div><!--kg-card-end: html--><p>It’s interesting that the growth behavior for every emoji is roughly the same -- waves ripple out from the center seed, which then form into the right colors. Since cells can only interact with their direct neighbors, information has to travel locally, which can explain this behavior. The ripples may contain information on where each cell is relative to the original seed, and/or information about the overall encoding.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--><p>A cool thing about StampCAs is that multiple designs can grow in the same world. This works because all designs share the same network, they just have different starting seeds. Since NCAs by definition only affect their local neighbors, two seeds won’t interfere with each other, as long as they’re placed far enough apart.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/stampca_circle.mp4"></video>
                <figcaption>
Placing seeds down along a path.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/emoji_rand.mp4"></video>
                <figcaption>
Different designs can grow in the same world.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/stampca_mess.mp4"></video>
                <figcaption>
If seeds are placed too close, weird behavior.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><h2 id="stampca-gan-learning-fake-mnist"><br>StampCA GAN: Learning fake MNIST</h2><p>A nice part about conditional NCAs is that they can approximate any kind of vector-to-image function. In the Emoji experiment we used an autoencoder-style loss to train NCAs that could reconstruct images. This time, let's take a look at a GAN objective: given a random vector, grow a typical MNIST digit.</p><p>OK, in an ideal world, this setup would be pretty straightforward. A GAN setup involves a generator network and a discriminator network. The generator attempts to produce fake designs, while the discriminator attempts to distinguish between real and fake. We train these networks in tandem, and over time the generator learns to produce realistic-looking fakes.</p><p>It turns out that this was a bit tricky to train, so I ended up using a hack. GANs are notoriously hard to stabilize, since the generator and discriminator need to be around the same strength. Early on, NCA behavior is quite unstable, so the NCA-based generator has a hard time getting anywhere.</p><p>Instead, the trick was to first train a traditional generator with a feedforward neural network. This generator learns to take in a random vector, and output a realistic MNIST digit. Then, we train an NCA to mimic the behavior of the generator. This is a more stable objective for the NCA to solve, and it eventually learns to basically match the generator's performance.</p><!--kg-card-begin: html--><figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/mnist_full.mp4"></video>
                <figcaption style="width:80%">
StampCA MNIST digits growing. Unlike with Emojis, we don't need to supply a dataset to encode->decode.<br>MNIST digits are generated from scratch with the GAN objective. <a href="https://colab.research.google.com/drive/1kgYy6jebUl3bPBVlybWtHOlv635HrcL2#scrollTo=iqBXLoqV5bQq">Replicate these results yourself.</a>
                </figcaption>
            </div>
        </div>
    </div>
</figure><!--kg-card-end: html--><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure">
<video class="b-lazy b-loaded gridvideo-full" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/mnist_zoom.mp4"></video>
    <figcaption>MNIST StampCA grows following the digits' curve, in comparison to the ripple of Emoji StampCA.</figcaption>
</div>
<!--kg-card-end: html--><p>The cool thing about these GAN-trained StampCAs is that we don't need a dataset of base images anymore. Since GANs can create designs from scratch, every digit that is grown from this MNIST StampCA is a fake design.</p><p>Another interesting observation is how the MNIST StampCA grows its digits. In the Emoji StampCA, we saw a sort of ripple behavior, where the emojis would grow outwards from a center seed. In the MNIST case, growth more closely follows the path of the digit. This is especially visible when generating "0" digits, since the center is hollow.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/mnist_circle.mp4"></video>
                <figcaption>
Placing seeds down along a path.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/mnist_many.mp4"></video>
                <figcaption>
Many MNIST digits in the same world. Note the "drifter" artifact coming from the 9 digit, reminiscent of cellular automata like Conway's Game of Life.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/stampca2/mnist_crowded.mp4"></video>
                <figcaption>
If seeds are placed too close, weird behavior.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><h2 id="discussion">Discussion</h2><p>First off, you can replicate these experiments through these Colab notebooks, <a href="https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg?usp=sharing">here for the Emojis</a> and <a href="https://colab.research.google.com/drive/1kgYy6jebUl3bPBVlybWtHOlv635HrcL2?usp=sharing">here for the MNIST</a>. </p><p>The point of this post was to introduce conditional NCAs and show that they exist as a tool. I didn't focus much on optimizing for performance, so I didn't spend much time tuning hyperparameters or network structures. If you play around with the models, I'm sure you can improve on the results here.</p><p>An NCA is really just a fancy convolutional network, where the same kernel is applied over and over. If we view things this way, then it's easy to plug in an NCA to whatever kind of image-based tasks you're interested in.</p><p>The interesting part about NCAs are that it becomes easy to define objectives that last over time. In this work, we cared about NCAs that matched the appearance of an image after a certain number of timesteps. We could have also done more complicated objectives: e.g, grow into a pig emoji after 50 timesteps, then transform into a cow emoji after 100.</p><p>The other aspect distinguishing NCAs are their locality constraints. In an NCA, it's guaranteed that a cell can only interact with its direct neighbors. We took advantage of this by placing multiple seeds in the same world, since we can be confident they won't interfere with one another. Locality also means we can increase the size of an NCA world to an arbitrary size, and it won't change the behavior at any location. </p><p>Looking ahead, there are a bunch of weirder experiments we can do with linking NCAs to task-based objectives. We could view an NCA as defining a multi-cellular creature, and train it to grow and capture various food spots. We can also view NCAs as communication systems, since cells have to form channels to tranfer information from one point to another. Finally, there's a bunch of potential in viewing NCAs as evo-devo systems: imagine an NCA defining a plant which dynamically adjusts how it grows based on the surroundings it encounters.</p><p>Code for these experiments is availiable at <a href="https://colab.research.google.com/drive/1FBEuRymdpgQiDPl5aLPrMDPVIZUM5xpg?usp=sharing">these</a> <a href="https://colab.research.google.com/drive/1kgYy6jebUl3bPBVlybWtHOlv635HrcL2#scrollTo=iqBXLoqV5bQq">notebooks</a>. Feel free to <a href="https://twitter.com/kvfrans/status/1379925309311442944">shout at me on Twitter</a> with any thoughts.</p><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Appendix
    </div><!--kg-card-end: html--><p>Cite as:</p><pre><code class="language-x">@article{frans2021stampca,
  title   = "StampCA: Growing Emoji with Conditional Neural Cellular Automata",
  author  = "Frans, Kevin",
  journal = "kvfrans.com",
  year    = "2021",
  url     = "http://kvfrans.com/stampca-conditional-neural-cellular-automata/"
}</code></pre><p>Why a blog post instead of a paper? I like the visualizations that the blog format allows, and I don't think a full scientific comparison is needed here – the main idea is that these kinds of conditional NCA are possible, not that the method proposed is the optimal setup.</p><!--kg-card-begin: html--><div class="float-label" style="clear: both">
	Things To Read
</div><!--kg-card-end: html--><p><strong><a href="https://distill.pub/2020/selforg/mnist/">"Growing Neural Cellular Automata"</a>, by Mordvintsev et al</strong>. This article was the original inspiration for this work in NCAs. They show that you can construct a differentiable cellular automata out of convolutional units, and then train these NCA to grow emojis. With the correct objectives, these emojis are also stable over time and can self-repair any damages. Since it's on Distill, there's a bunch of cool interactive bits to play around with.</p><p><strong><a href="https://distill.pub/selforg/2021/textures/">"Self-Organising Textures"</a>, by Niklasson et al</strong>. This Distill article is a continuation of the one above. They show that you can train NCAs towards producing styles or activating certain Inception layers. The driftiness of the NCAs leads to cool dynamic patterns that move around and are resistant to errors.</p><p><strong><a href="https://arxiv.org/abs/2103.08737">"Growing 3D Artefacts and Functional Machines with Neural Cellular Automata"</a>, by Sudhakaran et al.</strong> This paper takes the traditional NCA setup and shifts it to 3D. They train 3D NCAs to grow Minecraft structures, and are able to retain all the nice self-repair properties in this domain. </p><p><a href="https://arxiv.org/abs/2102.02579">"<strong>Regenerating Soft Robots through Neural Cellular Automata"</strong></a><strong>, by Horibe et al. </strong>This paper looks at NCAs that define soft robots, which are tested in 2D and 3D on locomotion. The cool thing here is they look at regeneration in the context of achieving a behavior – robots are damaged, then allowed to repair to continue walking. Key differences is that everything is optimized through evolution, and a separate growth+generation network is learned.</p><p><strong><a href="https://www.amazon.com/Endless-Forms-Most-Beautiful-Science/dp/0393327795">"Endless Forms Most Beautiful"</a>, by Sean B. Carroll. </strong>This book is all about evolutionary development (evo-devo) from a biological perspective. It shows how the genome defines a system which grows itself, through many interacting systems such as repressor genes and axes of organization. One can argue that the greatest aspect of life on Earth isn't the thousands of species, but that they all share a genetic system that can <em>generate</em> a thousand species. There's a lot of inspiration here in how we should construct artificial systems: to create good designs, we should focus on creating systems that can <em>produce</em> designs.</p><p><strong><a href="https://arxiv.org/abs/2006.12155">"Neural Cellular Automata Manifold"</a>, by Ruiz et al.</strong> This paper tackles a similar problem of using NCAs to grow multiple designs. Instead of encoding the design-specific information in the cell state, they try to produce unique network parameters for every design. The trick is that you can produce these parameters through a secondary neural network to bypass retraining.</p></div></div>]]></content:encoded></item><item><title><![CDATA[Quality Diversity: Evolving Ocean Creatures]]></title><description><![CDATA[<p>If you've ever folded a paper airplane, chances are you folded The Dart. It's fast and simple, and kids all around the world have launched it through the air. Soon, however, they will move on to other designs – airplanes that glide, make loops, or twirl in flight. There are thousands</p>]]></description><link>https://kvfrans.com/quality-diversity-evolving-ocean-creatures/</link><guid isPermaLink="false">60225e9617e26d47c77e8ecf</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Wed, 02 Dec 2020 02:05:00 GMT</pubDate><content:encoded><![CDATA[<p>If you've ever folded a paper airplane, chances are you folded The Dart. It's fast and simple, and kids all around the world have launched it through the air. Soon, however, they will move on to other designs – airplanes that glide, make loops, or twirl in flight. There are thousands of paper airplane designs, each with its own quirks and structure.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://kvfrans.com/content/images/2021/02/dart.jpg" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/02/dart.jpg 600w, https://kvfrans.com/content/images/2021/02/dart.jpg 640w"><figcaption>Paper Airplane Designs: https://www.foldnfly.com/#/1-1-1-1-1-1-1-1-2</figcaption></figure><p>In the artificial life world, we would call paper airplane design a <strong>quality-diversity</strong> algorithm. A paper airplane should be fly fast, but we don’t only care about an optimal design – we’re also interested in airplanes with unique bodies or wing types. Rather than a single design, quality-diversity algorithms aim to develop a population of diverse solutions to a problem.</p><p>In this post, let’s explore how quality-diversity algorithms can give us a peek into a world of swimming, soft creatures.</p><h2 id="oceanworld">Oceanworld</h2><hr><!--kg-card-begin: html--><br>
<div class="float-outside">
<div class="float-figure">
<img style="width:100%" src="https://kvfrans.com/static/open/4/creature.png">
    <figcaption>An Oceanworld organism. Red squares represent nodes, and blue connections are bonds. Green lines represent rigid bonds that are affected by fluid resistance.</figcaption>
</div>
In Oceanworld, a creature is a 16x16 matrix of flexible nodes and bonds. Each node is a simulated physics object, and bonds can rotate around and deform.
<br><br>
A creature moves by a periodic heartbeat. Whenever a heartbeat begins, any muscle bonds will extend their length, deforming the shape of the creature. When the heartbeat ends, the muscle bonds will return to original length. In addition, a bond can also be rigid. Rigid bonds are affected by fluid resistance – when they move, a force will pushback in the opposite direction.
<br><br>
The great part about evolutionary methods is that this is all a black box -- we don't actually need to understand how the physics works, we just need to run the algorithm.
</div><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/1.mp4"></video>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/2.mp4"></video>
                <figcaption>
                    Various Oceanworld creatures formed by sampling random genomes. When rigid bonds move, fluid resistance applies a force.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/3.mp4"></video>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Cool, we have some creatures. But how much of the total possible space do these creatures actually represent?</p><p>One way we can gain intuition about the possible space is through a technique called <strong>novelty search</strong>. Novelty search tries to find a population of creatures that are diverse from each other according to some metric. In our case, let’s use the number of nodes, and the ratio of bonds to nodes.</p><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure-big">
<video class="b-lazy b-loaded gridvideo-full" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/4.mp4"></video>
    <figcaption>Novelty search over Oceanworld genomes. Grey points represent the current population of genomes. Red points represent genomes sampled from a random distribution.</figcaption>
</div><!--kg-card-end: html--><p>Novelty Search works by a simple loop:</p><ol><li>Maintain a population of 1000 genomes.</li><li>Mutate each genome and add it to the population.</li><li>Sum the distance each genome has to its 10 closest neighbors, in terms of some novelty metric.</li><li>Sort the populations by increasing distance, and keep only the top 1000.</li></ol><p>Working correctly, the novelty search algorithm should optimize for a group of genomes that are spread out in terms of number of nodes and bonds-node ratio.</p><p>I always like to sanity check by looking in, so let’s take a peek at some sample points and see how the creatures look.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/5.mp4"></video>
                <figcaption>
                    10% nodes, 100% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/6.mp4"></video>
                <figcaption>
                    50% nodes, 100% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/7.mp4"></video>
                <figcaption>
                    100% nodes, 100% bond ratio
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/8.mp4"></video>
                <figcaption>
                    10% nodes, 10% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/9.mp4"></video>
                <figcaption>
                    50% nodes, 10% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/10.mp4"></video>
                <figcaption>
                    100% nodes, 10% bond ratio
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>There’s a key thing missing in our Oceanworld creatures – they don’t actually do anything. The “Quality” in Quality-Diversity has to measure something. So let’s choose something: a creature that swims faster is better.</p><p>First, let’s try out a greedy evolutionary algorithm. We’ll start with a random creature, and measure how far it travels. Then, we’ll randomly mutate its genome and try again. If the new creature performs better, it replaces the old one.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/11.mp4"></video>
                <figcaption>
Initial genome #1
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/12.mp4"></video>
                <figcaption>
Halfway through training
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/13.mp4"></video>
                <figcaption>
Fully trained
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/14.mp4"></video>
                <figcaption>
Initial genome #2
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/15.mp4"></video>
                <figcaption>
Halfway through training
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/16.mp4"></video>
                <figcaption>
                    Fully trained

                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Like a pair of deformed jellyfish, our creatures have evolved to move forwards by pulsating their muscle bonds. I doubt I would have been able to hand-design any of these creatures – the physics relationships are just too complex. Thankfully, evolution takes care of it for us.</p><p>But are these really the best creatures? The key observation here is that different starting genomes converge to different solutions. This tells us that our possibility space has local minima – genomes where any small change will hurt performance, even though a larger change could be beneficial.</p><p>It turns out that local minima are part of a classic paradox: “the path to success does not look like success”. When Carl Benz proposed the motor engine in a world of horse carriages, it was slow and heavy. But he kept at it, and motor cars emerged the victor.</p><p>In some form, Benz was using a quality-diversity algorithm: motor engines were different, so they deserved a chance. Quality-diversity algorithms help us escape local minima, because they force us to consider a group of genomes rather than a single leader. With more diversity, there is more chance to discover a path forwards.</p><h2 id="map-elites">MAP-Elites</h2><hr><p>MAP-Elites is a quality-diveristy algorithm that combines novelty search with evolution. Instead of evolving a single optimal genome, we want to evolve an optimal genome for every point in some grid of features.</p><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure-big">
<video class="b-lazy b-loaded gridvideo-full" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/18.mp4"></video>
    <figcaption>MAP-Elites over Oceanworld genomes. Yellow spots represent genomes with higher performance.</figcaption>
</div><!--kg-card-end: html--><p>In our case, we can use the two metrics from before – node count and bond ratio. Every spot in the matrix corresponds to some range of the metrics (e.g. 50-60 nodes and 30%-40% bonds), and we store the optimal genome that fits the criteria.</p><p>MAP-Elites works by:</p><ol><li>Maintain a 32 x 32 matrix of genomes.</li><li>Pick a random genome, and mutate it.</li><li>Measure its metrics, and find which spot in the grid the genome belongs to.</li><li>If the new genome performs better than the old genome in the spot, replace it.</li></ol><p>As always, let's get our hands dirty and check out what the optimal genomes end up looking like.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map10.mp4"></video>
                <figcaption>
10% nodes, 100% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map5.mp4"></video>
                <figcaption>
50% nodes, 100% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map3.mp4"></video>
                <figcaption>
100% nodes, 100% bond ratio
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map9.mp4"></video>
                <figcaption>
10% nodes, 50% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map4.mp4"></video>
                <figcaption>
50% nodes, 50% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map6.mp4"></video>
                <figcaption>
100% nodes, 50% bond ratio
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map8.mp4"></video>
                <figcaption>
10% nodes, 10% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map12.mp4"></video>
                <figcaption>
50% nodes, 10% bond ratio
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/map2.mp4"></video>
                <figcaption>
100% nodes, 10% bond ratio
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>In the MAP-Elites genomes, we get a whole bunch of different creatures, and some cool patterns emerge. The fastest creatures turn out to be the smaller ones, likely due to their lower mass. Larger creatures tend to develop a kind of "tail" -- my guess is that this acts as a stabilizer so the creature doesn't drift off direction.</p><p>The last row is especially interesting, since the larger creatures develop a completely different movement strategy. Instead of moving its entire body, the creature unrolls itself to stretch forwards.</p><p>Quality-diversity algorithms are an easy way to explore the possibilty space of an open-ended world. The techniques, such as novelty search and MAP-Elites, are simple but yield strong results when given proper metrics of diversity. In this Oceanworld, the limiting factor is the expressivity of our genome -- there are only so many ways to build swimming creatures out of a single type of bond.</p><p>Further work can involve:</p><ul><li>How can we increase the expressivity of our genome? Can we make the genome more information-dense in relation to the creature it defines? What kinds of genomes allow innovations to transfer between creatures?</li><li>What are good definitions of diversity? Can we define diversity in terms of a creature's behavior, rather than its structure?</li></ul><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Appendix
    </div><!--kg-card-end: html--><p>In the MAP-Elites method, it turns to be helpful to first run novelty search to get a population of diverse genomes. We start the MAP-Elites matrix with this diverse population, rather than a population of random genomes.</p><!--kg-card-begin: html--><div style="float: left;
  position: relative;
  width: 50%">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/17.mp4"></video>
                <p>
MAP-Elites from random starting population.
                </p>
</div>
<div style="float: right;
  position: relative;
  width: 50%">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/4/18.mp4"></video>
                <p>
MAP-Elites from novelty-search population.
                </p>
</div><!--kg-card-end: html--><!--kg-card-begin: html--><div class="float-label" style="clear: both">
	Things To Read
</div><!--kg-card-end: html--><p><em>For more on quality-diversity, a good starting point are the ICML tutorial slides below. The rest are some highlight papers with useful ideas.</em></p><p><strong><a href="https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-09-15-4336-recent_advances.pdf">"Recent Advances in Population-Based Search"</a>, ICML 2019 Tutorial, by Jeff Clune, Joel Lehman, and Ken Stanley</strong>. This is a great presentation that goes over recent works related to quality diversity. It introduces novelty search as a way to get around environments where greedily optimzing traps you in a local minimum, and then goes to to motivate quality-diversity algorithms where we simultaneously train populations of high-performing agents.</p><p><strong><a href="https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf">"Abandoning Objectives: Evolution through the Search for Novelty Alone"</a>, by Joel Lehman and Ken Stanley</strong>. This is the paper on Novelty Search. It argues that basing evolutionary search on fitness is limiting, since only creatures with high fitness will be considered. In environments such as hard mazes, the fitness landscape is non-convex and greedy optimization will fail. Instead, strong diversity metrics are often a better landscape to search through.</p><p><strong><a href="https://arxiv.org/pdf/1504.04909.pdf">"Illuminating search spaces by mapping elites"</a>, by Jean-Baptiste Mouret and Jeff Clune</strong>. This is the MAP-Elites paper. They introduce the idea of "illumination algorithms", which aim to find the highest-performing solution for every point in a given feature space. The core MAP-Elites idea is pretty straightforward -- store the best solution at every point, and keep trying new solutions.</p><p><strong><a href="https://arxiv.org/abs/1901.01753">"POET: Paired Open-Ended Trailblazer"</a>, by Rui Wang et al</strong>. POET is a quality-diversity algorithm with a step towards open-endedness. The idea is similar to MAP-Elites, but instead of niches defined as properties of the genome, each niche is a task. New tasks are introduced randomly, and stay in the pool if agents can solve them. You end up with a growing population of tasks and population of agents that can solve those tasks.</p><p><br><br><em>What are good diversity metrics? Hand-designed metrics such as creature size only work to some degree. In my mind, this is an important question as "novelty" is hard to define concretely.</em></p><p><strong><a href="https://arxiv.org/pdf/1802.06070.pdf">"Diversity is All You Need"</a>, by Benjamin Eysenbach et al</strong>. This paper describes a system where we want to find a set of diverse policies. They define diversity as:</p><ol><li>A discriminator should be able to predict which policy is active based on state history.</li><li>A policy should cover as many states as it can.</li></ol><p><strong><a href="https://arxiv.org/pdf/1704.03012.pdf">"Stochastic Neural Networks for Hierarchical Reinforcement Learning"</a>, by Carlos Florensa, Yan Duan, and Pieter Abeel</strong>. Motivated by information theory, this work aims to find diverse policies by maximizing entropy. This ends up being a similar method as above -- policies should visit different states to be maximally diverse.</p><p><strong><a href="https://arxiv.org/pdf/1905.11874.pdf">"Autonomous skill discovery with Quality-Diversity and Unsupervised Descriptors"</a>, by Antoine Cully</strong>. This work takes hints from dimensionality reducing methods such as PCA (Principle-component analysis). Basically, given a set of trajectories, we can map every trajectory onto a 2D space. This mapping becomes our diversity metric for running a quality-diversity method such as MAP-Elites.</p><p><br><br><em>Aside from how to discover diverse populations, the world and environment this all takes place in is crucial as well. These are some nice references for work in ALIFE and evolved creatures.</em></p><p><strong><a href="https://openai.com/blog/evolution-strategies/">"Evolution Strategies"</a>, OpenAI</strong>. A good overview of how evolutionary systems can potentially outscale traditional reinforcement learning. The basic idea is that ES is inherently parallalizable, so systems with lots of CPU cores can scale well. ES also treats an entire episode with a single reward, so it doesn't suffer from things such as sparsity or time dependency.</p><p><strong><a href="http://www.karlsims.com/papers/siggraph94.pdf">"Evolving Virtual Creatures"</a>, by Karl Sims</strong>. This is a classic work from 1994, where virtual swimming robots are evolved to move forwards. The methods used as similar to what we did in this blog post, but the genome is defined as a hierarchy of parts, and there are basic neural networks for control.</p><p><strong><a href="https://arxiv.org/abs/1711.06605">"Evolving soft locomotion"</a>, by Francesco Corucci et al</strong>. A more recent paper that uses evolutionary methods to design soft swimming and walking robots. The world here uses creatures of deformable voxels, akin to legos, that apply a fluid resistance force when expanding (similar to what we did but in 3D). Their genome is a CPPN, a neural network C(x,y) that outputs the creature's properties at each coordinate.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--></div>]]></content:encoded></item><item><title><![CDATA[Open-Endedness 3: Multicell World]]></title><description><![CDATA[<p>How many shapes can be made from six 2x4 LEGO bricks? The answer is 915,103,765. By combining only a few simple parts, it’s surprisingly easy to achieve high levels of complexity.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://kvfrans.com/content/images/2021/06/lego--1-.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/lego--1-.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/lego--1-.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/lego--1-.png 1600w, https://kvfrans.com/content/images/2021/06/lego--1-.png 1624w" sizes="(min-width: 720px) 720px"><figcaption>A Lego Counting Problem: http://web.math.ku.dk/~eilers/lego.html</figcaption></figure><p>If you look at</p>]]></description><link>https://kvfrans.com/open-endedness-3/</link><guid isPermaLink="false">60225e9617e26d47c77e8ece</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Thu, 12 Nov 2020 01:35:00 GMT</pubDate><content:encoded><![CDATA[<p>How many shapes can be made from six 2x4 LEGO bricks? The answer is 915,103,765. By combining only a few simple parts, it’s surprisingly easy to achieve high levels of complexity.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://kvfrans.com/content/images/2021/06/lego--1-.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/06/lego--1-.png 600w, https://kvfrans.com/content/images/size/w1000/2021/06/lego--1-.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/06/lego--1-.png 1600w, https://kvfrans.com/content/images/2021/06/lego--1-.png 1624w" sizes="(min-width: 720px) 720px"><figcaption>A Lego Counting Problem: http://web.math.ku.dk/~eilers/lego.html</figcaption></figure><p>If you look at the single-celled organisms on Earth today, you’ll mostly be stuck with bacteria. But once you enter the multi-cellular domain, suddenly life shows all sorts of shapes and structures. The <strong>possibilty space</strong> of multi-celled organisms is much richer.</p><p>Recall our principles of open-endedness:</p><ul><li><strong>Possibility Space:</strong> A world consists of a set of organisms. The space of possible organisms must be arbitrarily large.</li><li><strong>Peer Dependency:</strong> When new organisms appear, an environment must develop new niches.</li><li><strong>Search Algorithm:</strong> A procedure must find new organisms to fit new niches.</li></ul><p>In this post, let’s dive into how multi-cellularity can expand our possibility space towards increasingly complex genomes.</p><h2 id="multicell-world">Multicell World</h2><hr><p>Last time, we worked with a simple Cellworld where cells could gain energy and reproduce. Starting with this is a base, let’s restructure the genome to support multi-cellularity, and also mix in the our bitwise chemical system.</p><p>In the Multicell world, an organism is made of a group of cells. Each cell performs a single chemical reaction, creating energy. When a cell has gathered enough energy, it will create a new cell in a neighboring space.</p><p>Similar to the Bitwise Chemicals project, there are eight chemicals. Every space in the grid has its own chemical counts, and a cell will only be able to perform a reaction if the required chemicals are there. Within an organism’s cells, chemical counts and energy are averaged out.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/1.mp4"></video>
                <figcaption>
                    Green cells represent the reaction (01+02). These chemicals are provided periodically, at a higher rate near the bottom.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/2.mp4"></video>
                <figcaption>
                    A brown reaction appears, which reacts the waste produced by the green reaction.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/3.mp4"></video>
                <figcaption>
                    A wide variety of cells coexist, living off the outputs of neighboring reactions.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>So, we now have a world that supports multi-cell organisms, and they can interact with each other and evolve. But there’s an obvious flaw: every cell is doing the same thing!</p><p>While we have different species performing different reactions, within an organism all the cells are the same. Let’s allow for cells to specialize, and perform different roles.</p><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure">
<img style="width:100%" src="https://kvfrans.com/static/open/3/cell2.png">
    <figcaption>An organism comprised of different specialized cells. Cells behave differently depending on their pheromone.</figcaption>
</div>
    In this setup, a genome is a decision tree. When a new cell is born, this tree will tell it what reaction to specialize in. Once a cell specializes, it will only perform that reaction.
<br><br>
Crucially, the genome decision tree takes as input a pheromone – four bits that are unique to each indicidual cell. When a cell makes a child, it can assign it a new pheromone, which can cause the child to specialize in a different reaction.
    <br><br>
Put together, the genome of a organism is like an algorithm that tells the organism how to grow and what its cells should specialize in. Will species evolve to take advantage of multi-cellularity?
</div><!--kg-card-end: html--><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/4.mp4"></video>
                <figcaption>
                    In the beginning, cells consist only of green reactions, living mostly near the bottom.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/5.mp4"></video>
                <figcaption>
                    A species with both green and brown cells is twice as energy-efficient, since the output of green is used to power the brown reaction.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/6.mp4"></video>
                <figcaption>
                    The next generation begins to differentiate, and we see diverse combinations of multi-cell reactions.
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/7.mp4"></video>
                <figcaption>
                    Here, the first species develops Killer cells. Killer cells cost energy, and have to be powered by neighboring reaction cells.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/8.mp4"></video>
                <figcaption>
                    Descendants of the first Killer species spread. They develop different base reactions to power themselves.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/9.mp4"></video>
                <figcaption>
                    The world eventually settles into species containing a few reaction cells along with sporadic Killer cells.

                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Through the multi-cell experiment, we see a bunch of species appear that use multiple specialized cells to perform chains of reactions. These reactions usually synergize, with one providing the materials for the other. As the world advances, we also see cells that use their excess energy to power Killer cells and gain space.</p><p>One interesting note are the patterns of cells. With the way genomes work, there’s no real definition of a “pattern” – it all depends on the dynamics of the pheromones. Perhaps a cell with a pheromone of [0001] will produce a child of pheromone [0010], whose own child will loop back to [0001].</p><p>In real-life plants, we often see the Fibonacci series, or spiral patterns. These are patterns that naturally arise when cells grow by replication, since they are easy ways to gain repetition. In our gridworld, we often see checker-board style patterns, which are similarly easy to achieve.</p><p>Further work can involve:</p><ul><li>How can we increase the complexity of inter-species interactions beyong sharing chemicals and/or killing each other? What kinds of complexity lead to novel/interesing behavior?</li><li>Other than grid-based worlds, are there more interesting ways to represent physical organisms? What is the simplest physics system that will allow for movement?</li></ul><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Appendix
    </div>
    <video class="b-lazy b-loaded" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/3/recfastS.mp4"></video>
<br><br><!--kg-card-end: html--><p>Here is a full run of the Multicell world. Shown on the left are average chemical counts throughout the grid, and on the right is a legend for what reactions each color represents.</p><!--kg-card-begin: html--></div><!--kg-card-end: html--></div>]]></content:encoded></item><item><title><![CDATA[Open-Endedness 2: Bitwise Chemicals]]></title><description><![CDATA[<p>When algae is in the pond, a fish will come to eat it. When fish are in the pond, birds will come to catch them. This is an example of <strong>peer dependency</strong>: a species’ survival depends greatly on the presence of other species.</p><p>Peer dependency is one of the key</p>]]></description><link>https://kvfrans.com/open-endedness-2-bitwise-chemicals/</link><guid isPermaLink="false">60225e9617e26d47c77e8ecd</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Thu, 22 Oct 2020 06:01:00 GMT</pubDate><content:encoded><![CDATA[<p>When algae is in the pond, a fish will come to eat it. When fish are in the pond, birds will come to catch them. This is an example of <strong>peer dependency</strong>: a species’ survival depends greatly on the presence of other species.</p><p>Peer dependency is one of the key principles of open-endedness, which asks, <em>How we can build a world where innovation never ends?</em> Open-ended worlds follow three core principles.</p><ul><li><strong>Possibility Space:</strong> A world consists of a set of organisms. The space of possible organisms must be arbitrarily large.</li><li><strong>Peer Dependency:</strong> When new organisms appear, an environment must develop new niches.</li><li><strong>Search Algorithm:</strong> A procedure must find new organisms to fit new niches.</li></ul><p>Out of these three, peer dependency is the trickiest. In living ecosystems, animals may create byproducts or serve as food, allowing new species to emerge. For example, tall trees created a niche for long-neck giraffes. Due to the complexity of physical interactions, however, it’s hard to model this cleanly.</p><p>Instead, a clean example of peer dependency is found in the realm of chemistry. In chemical systems, some reactions may create byproducts, which will be used by other reactions. By treating each reaction as a species, let’s get a look at how peer dependency affects how new species will emerge.</p><h2 id="bitwise-chemicals">Bitwise Chemicals</h2><hr><p>In chemistry, simple elements form a complex dependency tree. Reactions take in inputs, but they also create outputs. These outputs can then become inputs to other reactions, leading to a large chain.</p><p>Let’s create a simplified chemical world, and examine the dependency tree that forms.</p><p>First, let’s define a chemical as a series of four bits. This gives us a world with a total of 2^4 = 16 chemicals.</p><pre><code class="language-python">Chemical_00: 0000
Chemical_01: 0001
...
Chemical_15: 1111</code></pre><p>Now, let’s define a reaction function that will combine two chemicals.</p><pre><code class="language-python">def React(ChemA, ChemB): # 2:0010, 3:0011
  ChemNew = BitwiseXOR(ChemA, ChemB) # 0010^0011 = 0001
  ChemNewA = Swap(ChemNew, ChemA) # Swap(0001, 2) = 0010
  ChemNewB = Swap(ChemNew, ChemB) # Swap(0001, 3) = 0100
  return ChemNewA, ChemNewB # 2:0010, 4:0100</code></pre><p>This is a simple deterministic function that determines the output of a two-chemical reaction. For example, reacting chemicals 00+01 will always make 05+06.</p><p>You may see where this is going – our world now forms a self-contained reaction system. As chemicals react, they will form new chemicals. These new chemicals can then react with each other, and the process will continue.</p><p>We can view our system as an open-ended world where chemical reactions are the organisms.</p><ul><li><strong>Possibility Space:</strong> The space of all possible chemical reactions.</li><li><strong>Peer Dependency:</strong> As new reactions make byproducts, they create an oppurtunity for new reactions.</li><li><strong>Search Algorithm:</strong> Reactions will randomly occur if their input chemicals are present.</li></ul><p>Let’s take a look at what a typical progression looks like.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/1.mp4"></video>
                <figcaption>
                    We provide chemicals 01 and 04 for free at every timestep.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/2.mp4"></video>
                <figcaption>
                    A reaction appears, combining 01+04 to make 05+06.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/3.mp4"></video>
                <figcaption>
                    Due to the new chemicals, a reaction appears combining 05+06 = 05+09.
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/4.mp4"></video>
                <figcaption>
                    As the chemical count in the world increases, more varieties of reactions appear.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/5.mp4"></video>
                <figcaption>
                    A combination of reactions forms a self-sustaining loop of outputs and inputs.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/6.mp4"></video>
                <figcaption>
                    We reach an stable state, where all outputs are imemdiately consumed as new inputs.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Starting from only two chemicals, a series of reactions popped up, each building on the outputs of the previous. Eventually, the world reached a stable state, where a set of reactions formed a self-sustaining loop.</p><p>One thing to note is the branching factor of our peer depencency tree. When a new reaction occurs, it will create two new outputs. This creates a niche for only a <em>single</em> new reaction. And if one of the chemicals was already present, we might not get any new niche at all.</p><p>Ideally, a new reaction would create opportunities for many new reactions. One change we can make is to allow reactions to compete for chemicals. Let's define a fixed energy output for every reaction.</p><pre><code class="language-python">def ReactEnergy(ChemA, ChemB):
  Energy = Hash(ChemA+ChemB)
  return Energy</code></pre><p>In this setup, reactions can acccess chemicals in order of their energy output. For example, let's say Reaction A and Reaction B both need Chemical 01. Reaction A has an energy output of 0.5, and Reaction B has an energy output of 0.9. Reaction B will get priority over Reaction A, even if Reaction A was around longer.</p><p>If Reaction B stays around long enough, it may use up all of Chemical 01, and Reaction A will die out.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/9.mp4"></video>
                <figcaption>
                    We start with the same first reaction, 01+04 = 05+06. But a reaction with higher energy output also uses chemical 04, so the first reaction dies.

                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/10.mp4"></video>
                <figcaption>
                    When a reaction dies, other reactions dependent on its output will follow suit. Blue squares represent dead reactions.

                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/11.mp4"></video>
                <figcaption>
                    Over time, we widely explore the possibility space of reactions.

                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>In the energy-based system, we can see a much wider area of the possibilty space is explored. Once again, the key is in the branching factor of our peer dependency. Now, when a new chemical is added, it allows for a new potential reaction with every other chemical present in the world.</p><p>One theme so far has been self-sustaining circuits. Our first system stopped progressing when it encountered a encountered a loop in the reaction tree, and the outputs matched the inputs.</p><p>In our second system, it became a lot harder to form a stable circuit, because new reactions could undermine older reactions. Once a reaction died, all other dependent reactions would also die, triggering an extinction wave.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 2 1 0%;">
                <img class="gridimg" src="https://kvfrans.com/static/open/2/7i.png">
                <figcaption>
                    Chemical reactions over time. When key reaction cycles are interrupted, many reactions can run out of inputs and die.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/10.mp4"></video>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Given any system of reactions, there is almost always some kind of reaction that will undermine the system by using up a key chemical. When a system breaks, however, it creates room for new systems to emerge. This cycle allows the world to explore a large portion of the possibile reactions.</p><p>In theory, there should still be a system of reactions that is self-sustaining and unbreakable. For example, a closed loop of high-energy reactions cannot be interrupted. But in practice, it takes a long time to discover such a system.</p><p>Or will it? Let's try a simple change -- increase the mutate rate at which we introduce new reactions.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 2 1 0%;">
                <img class="gridimg" src="https://kvfrans.com/static/open/2/8i.png">
                <figcaption>
                    Chemical reactions over time, with a high mutation rate. When a certain number of reactions is passed, a self-sustaining network of reactions can form.

                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/2/8s.mp4"></video>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Interestingly, when we increase the mutation rate, almost every trial results in a self-sustaining system!</p><p>The crucial component to this system is its redudancy. Instead of simply being a reaction loop, it's a reaction web -- many reactions are all feeding into one another. Even if a few of the reactions were to die out, there are still other sources of chemicals to keep the system going.</p><p>The key cause of these intertwined self-sustaining circuits is the reaction count. It's a simple cycle: the more reactions that are present, the more likely it is that a reaction has a source for its inputs. This makes the reaction more likely to survive, therefore making every other dependent reaction more stable.</p><p>This Bitwise Chemical project was an easy way to create a peer-dependent world. With a simple search algorithm, we follow the chain of dependencies as new reactions appear. The population of species sometimes resembled a wave, where some species would go extinct while new species would rise, or would resemble a stable web, where many species supported each other.</p><p>The major limitation to open-endedness here is the possiblity space of reactions, which is very small (256). In real chemical systems, open-endedness arises because elements can form complex molecules, which can do a whole bunch of functions and interact with one another.</p><p>Further work can involve:</p><ul><li>How can peer dependency increase in complexity while still being interesting? E.g. if we increase chemical count to 100, we would have way more reactions, but they would all qualitatively look the same.</li><li>What types of peer-dependency lead to stable systems? What kinds lead to unstable oscillations?</li></ul><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Things to Read
    </div><!--kg-card-end: html--><p><br><strong><a href="https://www.amazon.com/At-Home-Universe-Self-Organization-Complexity/dp/0195111303">"At Home in the Universe"</a>, by Stuart Kauffman</strong>. This book argues that life on Earth was inevitable, since every sufficiently complex system will self-organize. One argument he makes is that any complex chemical system will eventually develop a self-replicating compound. Our third experiment draws parallels to this, since we easily discover a self-sustaining circuit that grows indefinitely.</p><p><strong><a href="https://medium.com/@SeloSlav/what-is-the-adjacent-possible-17680e4d1198">"What is the Adjacent Possible"</a>, by Martin Erlic</strong>. This term, adjacent possible, provides a nice framework for looking at emergent design. Ideas don't just form out of nowhere -- they are made possible because of current ideas. By following a path of adjacent possibilities, we can see the development of a world. For a design to exist, not only does it need to fulfill a minimum criteria for survival, it also needs to be discoverable by being on a branch of the "possibility tree".</p><!--kg-card-begin: html--></div><!--kg-card-end: html--></div>]]></content:encoded></item><item><title><![CDATA[Open-Endedness 1: Cellworld]]></title><description><![CDATA[<p>When I look into the pond at my local park, there’s an amazing ecosystem of life at play. Algae covers the surface, and small fish come up to nibble on the meal. In the mud, bottom-feeding clams and large eels wait for prey.</p><p>My pond is only one example</p>]]></description><link>https://kvfrans.com/open-endedness/</link><guid isPermaLink="false">60225e9617e26d47c77e8ecc</guid><category><![CDATA[research]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Thu, 15 Oct 2020 03:56:00 GMT</pubDate><content:encoded><![CDATA[<p>When I look into the pond at my local park, there’s an amazing ecosystem of life at play. Algae covers the surface, and small fish come up to nibble on the meal. In the mud, bottom-feeding clams and large eels wait for prey.</p><p>My pond is only one example of the diversity of life on Earth. Just outside the city are dozens of lakes and grasslands, each with their own interconnected ecosystems. How did life emerge into so many coexisting species?</p><p>The answer is hidden in a property called <strong>open-endedness</strong>. In an open-ended world, innovation never ends. Instead, <strong>solutions create opportunities for more solutions</strong>. The algae create a food source for the fish. The fish create an opportunity for larger predators, who provide food for bottom-feeders when they die.</p><p>If you look closely, you can see open-endedness in many domains. In Ancient China, local farmers formed towns and villages, which became the foundation for feudal empires. The computers we use today are built off of semiconductors, which were discovered after research into radio waves.</p><p>The key feature of open-ended systems is that they never stop developing. Life will never be done evolving, and people will never be done creating new technology. As an AI researcher, this is a very useful property – it means we can train AI agents on potentially limitless tasks. It’s hard to imagine achieving general intelligence by classifying cats and dogs, but an AI that can invent increasingly complex technology would surely show us interesting behavior.</p><h2 id="cellworld">Cellworld</h2><hr><p>In this post, let’s explore basic open-ended principles through a cell-based gridworld.</p><!--kg-card-begin: html--><div class="float-outside">
<div class="float-figure">
<img style="width:100%" src="https://kvfrans.com/static/open/1/1.5_cell.png">
    <figcaption>A basic cell with three Photo parts and one Duplicator part.</figcaption>
</div>
    In our primitive Cellworld, a cell consists of a 4x4 grid of parts. Green squares represent Photo parts, which generate energy over time. Yelllow squares are Duplicator parts, which allow a cell to self-replicate.
    <br>
    <br>
When the yellow outline is full, a cell has enough energy to replicate, and will create a clone of itself in a nearby square.
</div><!--kg-card-end: html--><p>Let's take a look at how this simple cell will behave when we set it loose in an 8x8 world.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container gridsmall">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/1_replicate_1.mp4"></video>
                <figcaption>
                    Cells begin replicating.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/1_replicate_2.mp4"></video>
                <figcaption>
                    Cells spread through the whole world.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Easily, the origin cell spreads everywhere. But once the world fills up, the simulation reaches its end.</p><p>This is a common issue when designing open-endedness: the world can reach a stable state, where no more progress can be made.</p><p>In our case, the problem is there’s no space. Let’s try introducing a death timer. A few seconds after being born, cells will die off. In addition, let's introduce a small chance for a cell to mutate its genome when duplicating.</p><p>Together, these changes define a <strong>search algorithm</strong> for new genomes. When a cell dies, there is an opportunity for a new cell to take its place. So if a cell has a mutation that lets it duplicate faster than its neighbors, its genome will become more prevalent.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/2_stall.mp4"></video>
                <figcaption>
                    The original genome only allows a cell to produce a single offspring before dying, so the species is stuck in a low-progress loop.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/3_spread.mp4"></video>
                <figcaption>
                    A mutation occurs giving the cell more Photo parts. Now, it can produce two offspring, and the spread of life begins.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/4_optimal.mp4"></video>
                <figcaption>
                    Photo-dense cells are more likely to produce offspring and claim space. Over time, the population evolves to favor high-Photo genomes.

                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>Success! New genomes appear. Our search algorithm is working.</p><p>However, once Photo-rich genomes have spread, progress stops.</p><p>The problem here is a lack of <strong>peer dependency</strong>. The most crucial aspect of open-endedness is that solutions must create opportunities for more solutions. In other words, when new organisms appear, new niches must develop. Our lake is full of algae; now it’s time for the algae-eating fish to appear.</p><p>To try and see this effect, let’s introduce two more variations into Cellworld.</p><ol><li>Cells can mutate a Killer part, which will periodically kill nearby cells that do not share the same genome.</li><li>Duplicator and Killer parts are now directional. For example, a cell with a Duplicator part in its upper-right quadrant will only be able to replicate up or right.</li></ol><p>How will these changes affect the complexity of our little world? Let's take a peek.</p><!--kg-card-begin: html--><hr class="gridhr">
<figure class="kg-card kg-gallery-card">
    <div class="kg-gallery-container">
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/7_lifeonwall.mp4"></video>
                <figcaption>
                    At first, life is constrained to the right edge of the world. Cells have not yet developed Duplicator parts on their left side, so they cannot spread in that direction.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/8_spread.mp4"></video>
                <figcaption>
                    Species 66 develops a left-side Duplicator, and its offspring begin to spread across the world.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/9_green.mp4"></video>
                <figcaption>
                    For a brief period, life looks similar to before. Cells begin to fill every corner, and cells with more Photo parts begin to display their dominance.
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/10_takeover.mp4"></video>
                <figcaption>
                    The bottleneck to survival is not energy, but space. In comes species AD, who with a crucial Killer part in the top-right corner, begins clearing way for its own children.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/11_war.mp4"></video>
                <figcaption>
                    Soon, two species come out on top. E0 and AD begin a long war, where neither side can gain an upper hand.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/12_03.mp4"></video>
                <figcaption>
                    Breaking the stalemate, a mutation of AD appears and backstabs its ancestor. AD is wiped off the board.
                </figcaption>
            </div>
        </div>
        <div class="kg-gallery-row">
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/13_rebel_a2.mp4"></video>
                <figcaption>
                    Cruicially, 03 cannot attack downwards or left. It cannot defeat E0, the other rival. Luckily, a new species, A2, attacks from the corner and quickly wipes out E0.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/13_world_over.mp4"></video>
                <figcaption>
                    A2 is too small to hold its position. 03 sweeps downwards to take control of the entire world, emerging victorious as the dominant species.
                </figcaption>
            </div>
            <div class="kg-gallery-image" style="flex: 1 1 0%;">
                <video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/14_takeover.mp4"></video>
                <figcaption>
                    All empires must fall. Since 03 cannot attack down or left, it has a single weakness -- the bottom-left corner. Species 22 mutates there, and quickly takes the throne. The cycle will continue.
                </figcaption>
            </div>
        </div>
    </div>
</figure>
<hr class="gridhr"><!--kg-card-end: html--><p>So, that was a bit of overdramatization. But you have to admit, with just these two modifications, much more interesting stories emerge. Instead of a single optimal species, we now have a cycle, where a new species can potentially overthrow an older one.</p><p>The key here is that <strong>there is always an opportunity for new genomes.</strong>. When the world is full of Photo cells, a Killer cell can emerge. When the world is full of Killer cells, a new species can take over by attacking from the right direction.</p><p>In an open-ended world, solutions must create opportunities for more solutions. For worlds of interacting organisms, there are three key principles to sustain this behavior:</p><ul><li><strong>Possibility Space:</strong> Organisms must have a large enough genome to describe arbitrarily complex behaviors.</li><li><strong>Peer Dependency:</strong> When new organisms appear, an environment must develop new niches.</li><li><strong>Search Algorithm:</strong> A procedure must find new organisms to fit new niches.</li></ul><p>Since this Cellworld was fairly simple, both the possibility space and peer dependency of cells were limited, so we eventually stopped seeing novel behavior.</p><p>Further work can involve:</p><ul><li>What rules lead to greater peer dependency? Do we need more complex interactions, or perhaps some form of resource system? Can emergent symbiosis occur?</li><li>Is mutation-based evolution the best search algorithm to fulfill new niches?</li></ul><!--kg-card-begin: html--><hr class="gridhr">
<br>
<div class="float-outside">
<div class="appendix">
    <div class="float-label">
        Things to Read
    </div><!--kg-card-end: html--><p><strong><a href="https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/">"Open-endedness: The last grand challenge you’ve never heard of"</a>, by Kenneth O. Stanley, Joel Lehman and Lisa Soros. </strong>This article is a great motivator for why we should care about open-endedness -- namely, its potential to create innovations that are too complex to be designed for directly. The article also gives an overview of past work in the field their reasonings.</p><p><strong><a href="http://eplex.cs.ucf.edu/papers/soros_alife14.pdf">"Identifying Necessary Conditions for Open-Ended Evolution through the Artificial Life World of Chromaria"</a>, by Lisa Soros and Kenneth O. Stanley</strong>. This paper gives a good foundation for finding the principles that allow open-ended evolution to occur. They propose a minimum criterion for survival, and define peer dependency as creating novel ways for new individuals to achieve this minimum criterion.</p><p><strong><a href="https://openreview.net/pdf?id=160xFQdp7HR">"Self-Organizing Intelligent Matter: A Blueprint for an AI Generating Algorithm"</a> (Anonymous)</strong>. This recent paper works with open-endedness in the neural network domain. They show an interesting formulation where cells are neural networks that can push/pull chemicals in a gradient around them.</p><!--kg-card-begin: html--><div class="float-label">
	Appendix
</div>    
<div class="float-figure">
	<video class="b-lazy b-loaded gridvideo" type="video/mp4" autoplay muted playsinline loop src="https://kvfrans.com/static/open/1/behave.mp4"></video>
</div>
    
    
In a follow-up experiment, here is a Cellworld with a slightly more complicated ruleset. At any timestep, a cell may decide to gain energy, replicate itself, or attack a neighboring cell. A cell's genome is defined as a behavior tree of these actions, along with conditions for them to take place. For exmaple, the origin cell's behavior tree says: duplicate if I have more than 50 energy, otherwise gain energy.

There are two main evolutionary milestones. First is when species E7 learns an attacking behavior. The second is when species 3E learns to not attack its own offspring, a mutation so strong that it takes over the world.<!--kg-card-end: html--><figure class="kg-card kg-image-card"><img src="https://kvfrans.com/content/images/2021/02/behave-1.png" class="kg-image" alt srcset="https://kvfrans.com/content/images/size/w600/2021/02/behave-1.png 600w, https://kvfrans.com/content/images/size/w1000/2021/02/behave-1.png 1000w, https://kvfrans.com/content/images/size/w1600/2021/02/behave-1.png 1600w, https://kvfrans.com/content/images/2021/02/behave-1.png 2222w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: html--><br><!--kg-card-end: html--><!--kg-card-begin: html--></div><!--kg-card-end: html--></div>]]></content:encoded></item><item><title><![CDATA[Omakase 5: Bullet Dance]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>This month was a return to form. Ever since <a href="https://kvfrans.com/rain-project-the-story-of-building-a-touhou-fangame/">Rain Project</a>, I've always had this urge to make a true boss battler, combining platforming and bullet hell. With November's Omakase, I finally took the dive with Bullet Dance.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/s2-2.gif" alt></p>
<p>I'm super proud of this month's Omakase game, and it's probably the</p>]]></description><link>https://kvfrans.com/omakase-5-bullet-dance/</link><guid isPermaLink="false">60225e9617e26d47c77e8ecb</guid><category><![CDATA[games]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Wed, 27 Nov 2019 01:02:52 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>This month was a return to form. Ever since <a href="https://kvfrans.com/rain-project-the-story-of-building-a-touhou-fangame/">Rain Project</a>, I've always had this urge to make a true boss battler, combining platforming and bullet hell. With November's Omakase, I finally took the dive with Bullet Dance.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/s2-2.gif" alt></p>
<p>I'm super proud of this month's Omakase game, and it's probably the most time I've spent on a month game since I started this schedule. I made it a clear priority to focus only on the boss patterns and not worry about anything else, and it felt great to finally put out some real content.</p>
<p>As of now, Bullet Dance is fully playable with four bosses, each with 5-7 stages each. It's a bit of a journey, but in this post I'll walk through how Bullet Dance came to life from start to finish!</p>
<p><a href="https://drive.google.com/drive/folders/13Frhm04RiRGP2c42vzbhH2tUBitOreEu?usp=sharing">NOTE: There are spoilers below. Download the game here for a clean experience!</a></p>
<h6 id="themovement">The Movement</h6>
<p>The first thing I really worked on was the movement. It was super important for the second-to-second control of our main character to feel smooth and satisfying, since that's what the player will be doing the entire game!</p>
<p>As a base, I re-used the custom <a href="https://github.com/cjddmut/Unity-2D-Platformer-Controller">PlatformerController2D</a> script that I had created for <a href="https://kvfrans.com/omakase-1-ropeman/">Ropeman</a>. I made sure to have the player's horizontal acceleration and deceleration be instant, so that the player could easily move around precisely. In addition, the player can control how high they jump by holding down the button, and can jump one additional time in the air. As a rule of thumb, I wanted to make the player feel as light and flexible as possible -- the more freedom I give the player in movement, the harder I can make the boss patterns!</p>
<p><img src="https://kvfrans.com/content/images/2019/11/out.gif" alt></p>
<p>The big design choice here is in the dash. In Rain Project, I gave the player an instant dash in straight line, which granted invincibility. The problem with that approach, I've learned, is it can turn the gameplay into a very reaction-based timing problem where the player just waits until bullets are about to arrive and dashes through them all. In Bullet Dance, I wanted players to plan ahead and learn the patterns, rather than just clearing everything with microsecond-level timing.</p>
<p>To accomplish this, I removed the invincibility during the player's dash. Instead, I allowed the player to pick a direction for their dash, so that they could dash <em>between</em> the bullets rather than <em>through</em> them (which is a very satisfying experience).</p>
<p>In early playtests, people still found it hard to line up the dash angles, so I also added a time slow when choosing the dash direction. This ended up being a super helpful design choice -- by slowing down time, players started treating the dash more as a precise positioning tool rather than a quick escape to the side.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/out-1.gif" alt></p>
<h6 id="thebullets">The Bullets</h6>
<p>While Bullet Dance isn't a Touhou game, it definitely takes inspiration from the series. One thing I really like about the bullet patterns in Touhou is that while they look intimidating, there's actually a lot of leeway that the player is given.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-22-at-10-32-38-PM.png" alt></p>
<blockquote>
<p>Purple = hitbox</p>
</blockquote>
<p>For example, the hitboxes for bullets are always smaller than the bullet's sprite, so the player never feels cheated when a bullet hits that seems like it shouldn't have. Our main heroine herself also has a way smaller hitbox for bullet collisions than for physics, for the same reasons.</p>
<p>On the opposite end of things are the player's bullets. For bosses attacks, we want the bullets to be obvious and slow. For player attacks, we want the opposite -- I don't want the player to have to think too hard about attacking, since I want to let them focus on dodging. For that reason, the player's bullets are auto-aimed at the boss, and are almost impossible to miss due to their speed.</p>
<p>To encourage more risky gameplay, however, I put some details in the player's attacks. The bullets only travel about 1/3 of the screen, so the player needs to stand relatively close to the boss to hit them. In addition, the player fires a spread shot of five bullets every attack, with random angles. If the player is closer to the boss, more of the bullets will hit, and the player will do more damage. This encourages players to go in as close as they can to the bosses without getting hit, creating a risk/reward tradeoff with juicy implications.</p>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/attacking.mp4" type="video/mp4">
</video>
> Attacking
<h6 id="thebosses">The Bosses</h6>
<p>Another Touhou design choice I adopted -- multi-stage bosses! In most games, bosses consist of a few patterns that repeat over and over. Usually this makes sense, as in most contexts bosses are used as a way of testing the player after they have learned skills throughout the level.</p>
<p>In pure bossfighting games, however, the boss itself should be a learning experience! The first stages should be easier, showing off a certain kind of attack pattern that the player will have to learn to clear. The next stages then ramp up the difficulty, introducing variations and new gimmicks, and the final stage of every boss is a final test of everything so far.</p>
<p>I think the idea of &quot;spellcards&quot; from Touhou are a really helpful tool here, and I put them into Bullet Dance without a second thought. In Bullet Dance, bosses are a sequence of non-spells and spellcards, with the non-spells being easier patterns, and the spellcards being trickier patterns with more to think about. Ideally, there is a progression between the stages of each boss -- the non-spells should start easy and gradually build up difficulty, while the spellcards throw the player some curveballs and challenge.</p>
<p>I use this &quot;progression&quot; idea a lot more in the seven-stage Shadow Eye and War Relic bosses, so jump ahead to those sections for some good examples!</p>
<h5 id="thegraphics">The Graphics</h5>
<p>Finally, before we get into the actual boss content, I want to touch on the graphics in Bullet Dance. To put it simply, I wanted to put in as little effort into the graphics as I could while getting away with it.</p>
<p>For the camera, I used the technique I developed with <a href="https://kvfrans.com/omakase-2-suwapond/">SuwaPond</a> of a pixel camera. Instead of rendering at full resolution, the game first renders to a 360x205 canvas, and then upscales it with nearest-neighbor to create a pixelated effect. This lets me use things like flat circles and squares and still end up with cool pixel effects!<br>
<img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-22-at-11-01-14-PM.png" alt></p>
<blockquote>
<p>Left: Default rendering. Right: Pixel rendering.</p>
</blockquote>
<p>In fact, I was able to build almost all the bosses out of only a few basic shapes. This saved a lot of time as usually I would spend 80% of my development time on pixel art, but I could not create satisfying-enough graphics almost entirely in code.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-22-at-11-03-12-PM.png" alt></p>
<h4 id="thebosses">The Bosses</h4>
<p>If you've read up to here, then thanks for sticking with me! In the section, I'm going to go over each of the four bosses and what kind of thoughts went into designing their patterns. If you don't want to spoil the game, I would advise to stop here and try to <a href="https://kvfrans.com/omakase-5-bullet-dance/(https://drive.google.com/drive/folders/13Frhm04RiRGP2c42vzbhH2tUBitOreEu?usp=sharing)">playthrough the game yourself</a>! The content below is mostly just to point out some obscure thoughts and what led to the various design choices I made along the way.</p>
<h5 id="boss1lightinthelantern">Boss 1: Light in the Lantern</h5>
<p>The first boss is based off the many wooden lanterns found inside Japanese shrines. The Light in the Lantern serves as a warning to those who wish to enter the shrine's strong spiritual zone.</p>
<h6 id="nonspell1">Nonspell 1</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/fire1.mp4" type="video/mp4">
</video>
This first nonspell is just a simple introduction to bullets -- the boss fires in four directions while slowly moving.
<h6 id="intimidationsigndancingflames">Intimidation Sign ~Dancing Flames~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/fire2.mp4" type="video/mp4">
</video>
This spellcard represents the Light in the Lantern attempting to scare off visitors through a fire dance. The boss begins moving a lot quicker, and bullets explode to intimidate anyone nearby.
<h6 id="nonspell2">Nonspell 2</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/fire3.mp4" type="video/mp4">
</video>
The second nonspell is just three waves of bullets aimed at the player. The player must learn to stream (move back-and-forth) in order to beat this stage without damage, which probably isn't that great of a design choice for this early of a boss.
<h6 id="lighthousesignshoreofstars">Lighthouse Sign ~Shore of Stars~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/fire4.mp4" type="video/mp4">
</video>
![](/content/images/2019/11/Screen-Shot-2019-11-22-at-11-24-26-PM.png)
This was my attempt at a spellcard where the boss leaves the screen completely, and bullets just spawn wherever they want to. Shore of Stars in particular is based off the lights you see when viewing an island from off sea, hence the title "Lighthouse Sign".
<h6 id="firesignradianceoflonelight">Fire Sign ~Radiance of Lone Light~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/fire5.mp4" type="video/mp4">
</video>
The final spellcard of this first boss is Radiance of Lone Light, which represents the bright shine when the Light in the Lantern finally decides to unleash its full glow. 
<p>In this spellcard is a great example of &quot;effect bullets&quot; -- bullets that don't really affect the player's strategy that much, but make the spellcard look a lot cooler. In this case, the outer ring of bullets could have been a simple static barrier, but I made it move in a spiral for that extra oomph.</p>
<h6 id="movement">Movement</h6>
<p>As a quick aside, the movement for all the bosses in the game are pretty simple when broken down as well. I didn't want the bosses to just stay in one place, but I also didn't want any kind of weird pre-set path. Instead, every few seconds, the boss picks a new &quot;target&quot; location which is in a random circle above the player.</p>
<pre><code>if (changeTargetPositionTimer.time &lt;= 0) {
    changeTargetPositionTimer.time = changeTargetPositionCooldown; 
    targetPosition = new Vector3(GameFlow.Instance.player.position.x, 
         bossStage.position.y + 13, 0);
    targetPosition += Custom.RandomInUnitCircle() * 4;
}
</code></pre>
<p>Then, in every update, the boss will add a force moving itself towards a desired radius from the target point. Of course, the boss will overshoot the desired position, which leads to a dynamic equilibrium where the boss is always somewhat chasing the player, but also constantly moving in a smooth but hard-to-predict manner.</p>
<pre><code>Vector3 delta = transform.position - targetPosition;
if (delta.magnitude &lt; targetPositionRadius) {
    rb.AddForce(-(Vector2) normalized * speed);
}
else {
    rb.AddForce((Vector2) normalized * speed);
}
</code></pre>
<h5 id="boss2architect">Boss 2: Architect</h5>
<p>The second boss takes the form of the architect of the shrine. Armed with a chisel and hammer, the Architect is the designer and builder of the shrine as a whole, and his spellcards revolve around this theme of woodworking.</p>
<h6 id="nonspell1">Nonspell 1</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/arch1.mp4" type="video/mp4">
</video>
As a first warmup, this first nonspell has the Architect face the player, then one second layer shoot towards their position, kicking up bullets in the direction of impact. This is kind of themed around the Architect being a more "physical" being than, say, the light in the lantern.
<h6 id="cedarsignwoodworkarrangement">Cedar Sign ~Woodwork Arrangement~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/arch2.mp4" type="video/mp4">
</video>
The first spellcard of the Architect! This is kind of just a generic bullet rain pattern, but with some large wood cedars that appear periodically on the bottom forcing the player to jump, so they can't just dodge side to side on the ground. The themeing here is that the Architect is moving around huge wooden cedars used in traditional Japanese shrine construction, and the cedars colliding at the top of the screen produce the bullets on hit.
<h6 id="nonspell2">Nonspell 2</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/arch3.mp4" type="video/mp4">
</video>
This is a nonspell, but it could have been a spellcard of its own with some more components. The idea here is a "chisel and hammer": the first spread of bullets represents a chisel placed slowly down, and the circle that launches the bullets is a hammer delivering a strong blow.
<h6 id="gatesignfourcornersoftheworld">Gate Sign ~Four Corners of the World~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/arch4.mp4" type="video/mp4">
</video>
What's a classic piece in Japanese shrine architecture? The gate! This spellcard is a little fantasy of mine, where the Architect builds up these wooden gates and they become portals transporting spirits to and from the world. In the spellcard, the boss himself dashes through the gates he has created and comes in and out in different spaces. The name "Four Corners of the World" is mostly because of the four gates in the corner, but it's also a very slight reference to the world being round and loop-able, which the gates let the stage become.
<h6 id="architectsignthetemplethatstretchestothesky">Architect Sign ~The Temple That Stretches to the Sky~~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/arch5.mp4" type="video/mp4">
</video>
The final spellcard, which I actually came up with first when designing this boss, involves the Architect building up the shrine in real-time. Every few seconds, a new platform is built, and the screen scrolls up so the player has to keep climbing this new temple which "stretches to the sky".
<p>The cedars that make a return from earlier in the fight are again some effect bullets that don't really serve a purpose other than to make the fight seem cooler and discourage players from staying too low on the screen.</p>
<h5 id="boss2shadoweye">Boss 2: Shadow Eye</h5>
<p>This third boss is based off a theme I've been playing with in my head for a while, of the eye monsters that are always looking in from the darkness. I wanted to make a fight with more &quot;terror&quot; and otherworldly aspects than the typical human boss.<br>
<img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-27-at-3-26-28-AM.png" alt></p>
<blockquote>
<p>Inspirations from the phantoms in Noragami, and Pride from Fullmetal Alchemist</p>
</blockquote>
<h6 id="nonspell1">Nonspell 1</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye1.mp4" type="video/mp4">
</video>
With the Eye boss, I began playing with the idea of a repeated nonspell, similar to certain extra bosses in Touhou (e.x, Mokou always throws cards in some [circle-like pattern](https://www.youtube.com/watch?v=bGBgaJgtOAg) for her nonspells). This first nonspell has the Eye shoot out bullets in a curved eyelid pattern, which then turn towards the player after a 2 second delay.
<h6 id="curtainsignwatchingfromtheshadows">Curtain Sign ~Watching from the Shadows~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye2.mp4" type="video/mp4">
</video>
Here, I just wanted to design a spellcard that embodied the idea of many eyes opening in the darkness. Aesthetics wise the eyes definitely stand out against the black background of the spellcard and I am quite happy with it. Gameplay-wise, the eyes simply shoot a sequence of three bullets when opening, but this creates a pretty dynamic dodging pattern that requires some quick jumps to survive.
<h6 id="nonspell2">Nonspell 2</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye3.mp4" type="video/mp4">
</video>
Here's the repeated nonspell in action. The bullets here follow the exact same pattern in Nonspell 1 of turning to the player after two seconds, but the circle patterns make dodging a bit harder as the bullets pose aa bigger formation.
<h6 id="frightsignalurkingfeelinginsidevision">Fright Sign ~A Lurking Feeling in Side Vision~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye4.mp4" type="video/mp4">
</video>
This is probably the most controversial spellcard in the game at the moment, because it's not a bullet hell. I actually came up with the look first, of a giant eye opening up in the background. But I struggled with coming up with an appropriate gameplay element, since black bullets were very hard to see. I ended up going with a sort of quick-dodging sequence but I'm not super satisfied with it.
<h6 id="nonspell3">Nonspell 3</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye5.mp4" type="video/mp4">
</video>
The repeating nonspell becomes harder! Here, the Eye shoots in a spiral stream, which then turn towards the player. I do give a gap between streams every couple of seconds to let the player dodge through, but this is definitely the hardest of the nonspells, and figuring out how to lead the bullets where you want would be tricky if not for the previous nonspell experiences.
<h6 id="anxietysignalltheworldiswatching">Anxiety Sign ~All the World is Watching~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye6.mp4" type="video/mp4">
</video>
I kind of came up with this by pure luck -- I was imagining a scene where the player is surrounded by a circle of eyes, so I just dropped in the eye turrets from Curtain Sign. It turns out that the circling eyes with periodic shots makes for a pretty interesting gameplay loop -- there are a sequence of tight dodges, but since the bullets are aimed at the player, it's entirely up to the player how to "aim" the next set of bullets depending on where they are.
<h6 id="nilsignaloneagainsttheskyofdarkness">Nil Sign ~Alone Against the Sky of Darkness~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/eye7.mp4" type="video/mp4">
</video>
The final spellcard! Here's where I make the greatest use of the repeating nonspells for a grand finale. Aesthetically, I wanted to make the final spellcards as awe-inspiring as I could, so the Eye boss transforms into a huge red eye overlooking the player. I also completely fade away the background and the ground itself to black, creating the "sky of darkness".
<p>Gameplay wise, this entire spellcard is actually just a rehash of the nonspells! There are a lot more bullets, but they all follow the well-established pattern of turning toward the player after 2 seconds. Playtesting this spellcard was great -- while the players were struck by the impact of the boss transformation, they were able to immediately understand the spellcard since they had practice with so many similar patterns through nonspells. As players won't be going in blind, this lets me increase the difficulty to a much higher level fitting of a final spellcard!</p>
<h5 id="boss2embodimentofwarrelics">Boss 2: Embodiment of War Relics</h5>
<p>The fourth and final boss in Bullet Dance is the Embodiment of War Relics, based off the many enshrined spears and swords often found in Japanese memorials. The gameplay feel here is especially inspired from Gate of Babylon and Unlimited Blade Works from Fate, and that tension of having a thousand blades pointed at you.<br>
<img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-27-at-3-54-22-AM.png" alt></p>
<h6 id="nonspell1">Nonspell 1</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war1.mp4" type="video/mp4">
</video>
Here's the repeating nonspell for War Relics. Golden blades fall from the sky, while the boss himself occasionally shoots a sequence of shots directly at the player.
<h6 id="ancientrelicscytheandsickle">Ancient Relic ~Scythe and Sickle~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war2.mp4" type="video/mp4">
</video>
The spellcards for War Relics are all based off of various weapons throughout history. Starting from the ancient age where agriculture was a way of life, this spellcard embodies a scythe and sickle cutting wheat, with the fast slice followed by a circular cut. Gameplay-wise, the swords here are mostly for effect, although they will hit a player who jumps too high. 
<h6 id="nonspell2">Nonspell 2</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war3.mp4" type="video/mp4">
</video>
Second variation on the repeating nonspell has the swords fall from the sky at a spread angle, in classic bullet hell style. The direct boss shots also appear at a faster rate.
<h6 id="medievalrelictwobladesofiron">Medieval Relic ~Two Blades of Iron~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war4.mp4" type="video/mp4">
</video>
We've entered the age of knights! The imagery here should be obvious -- two huge blades form in the air and come crashing down. The real challenge is in continuously moving horizontally to avoid the swords, while not touching any of the trail bullets left behind.
<h6 id="nonspell3">Nonspell 3</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war5.mp4" type="video/mp4">
</video>
A third variation on the repeating nonspell, where the swords fall from above the boss and towards the player. The boss also moves a lot faster and closer to the player.
<h6 id="modernrelicsubmachinegun">Modern Relic ~Sub-machine Gun~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war6.mp4" type="video/mp4">
</video>
If we're facing the Embodiment of War Relics, surely that means modern warfare as well! This spellcard is just a fun idea of the spellcard being a literal gun, which fires directly at the player after a short delay. By itself, this would just be a typical move-horizontally challenge, but the circles emanating make moving tricky and also add to the "charging" effect of the machine gun. 
<h6 id="relicsignallwhomhavefallentohistory">Relic Sign ~All Whom Have Fallen to History~</h6>
<video autoplay muted loop width="120%" controls>
  <source src="https://kvfrans.com/static/bulletdance/war7.mp4" type="video/mp4">
</video>
Final spellcard! Again, I'm using the final spellcard as a climax of what the player has been learning in the nonspells. This time, the swords all appear at the same time, with differing sizes and locations all across the top of the screen. Of course, the boss still fires directly at you, at a highly increased rate. At this point, however, the player should be accustomed to dodging two bullet types at the same time and be able to clear the pattern.
<p>Lore-wise, this is honestly just a huge Fate reference. The gears are just taken from Unlimited Blade Works, and even the naming scheme is a bit of an inside joke -- in Fate, Archer/Gilgamesh seem like they have a ton of weapons, but really they only have one skill: to open up a world with unlimited weapons. That's why in the boss fight, only the final spellcard is called &quot;Relic Sign&quot;, with the rest being named &quot;[xxxx] Relic&quot; as they are not really spellcards but just weapons created from the real, final spellcard.</p>
<h4 id="conclusions">Conclusions</h4>
<p>Bullet Dance was a ton of fun to work with, and it's definitely something I might revisit in the future! If I were to keep going and make Bullet Dance a real game, I think I would revisit some of the first two bosses -- right now, they have a bit too much variation in their spellcards and could benefit for a more streamlined design like the repeating nonspells in the last two bosses.</p>
<p>For now, feel free to <a href="https://drive.google.com/drive/folders/13Frhm04RiRGP2c42vzbhH2tUBitOreEu?usp=sharing">download a build</a> and try out the game! For keyboard and mouse, move with WASD and attack/dodge with the mouse. Bullet Dance also has controller support (which could merit a whole post on its own), and plays very smoothly on a PS4 or Gamecube controller.</p>
<p>As a disclaimer, the music in this build is shamelessly stolen from Demon Slayer (great show) and some of the sound effects are ripped from Touhou -- if I ever release a real version of the game, these will be replaced!</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Omakase 4: Jopper]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>For October's game I collabed with the great <a href="https://twitter.com/jynniit">Jynnie Tang</a>!! Which basically means we came up with an idea late night over juice and decided &quot;hey let's try and make it happen&quot;.</p>
<p>The core concept we wanted to reach was the feeling of excitement when playing together with</p>]]></description><link>https://kvfrans.com/omakase-4-jopper/</link><guid isPermaLink="false">60225e9617e26d47c77e8eca</guid><category><![CDATA[games]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Thu, 21 Nov 2019 23:53:41 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>For October's game I collabed with the great <a href="https://twitter.com/jynniit">Jynnie Tang</a>!! Which basically means we came up with an idea late night over juice and decided &quot;hey let's try and make it happen&quot;.</p>
<p>The core concept we wanted to reach was the feeling of excitement when playing together with friends and someone pulls off an amazing move. As a dorm, we often play games together in our main lounge, and I've come to really enjoy couch co-op as a game style. We wanted to capture some of that feeling by creating a super lightweight arena-style minigame, where anyone could join in by using their phone.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/--s-1.png" alt></p>
<blockquote>
<p>Beautiful concept art</p>
</blockquote>
<p>With that in mind, we started work on what would become known as Jopper. The game was centered around the beat of &quot;&quot; -- rather than continuously control your character, you can only adjust your velocity when the beat hits. At the same time, your character will shoot a projectile behind you, so it's up to the player to decide if they want to move around or try and line up a shot.<br>
<img src="https://kvfrans.com/content/images/2019/11/screencast.gif" alt></p>
<p>After a bit of tinkering in Unity, we managed to build a very basic stage for the game! Every two seconds, the player will launch towards the mouse cursor, bouncing off any walls it happens to encounter.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/s2.gif" alt></p>
<p>Of course, if the game was to be multiplayer, we couldn't just use one player. I had to rewrite a bit of the core engine to work smoothly, but the game was now playable with multiple players on the same field! I also added in a line to show just how far and in what direction the player would launch.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/--s-4.png" alt></p>
<p>At this point we went for burgers and brainstormed on a napkin. Some cool ideas came out, such as slowing down time during the aiming period and how to structure the phone's touch control scheme.<br>
<img src="https://kvfrans.com/content/images/2019/11/Screen-Shot-2019-11-22-at-2-58-52-AM.png" alt></p>
<p>Jessica worked on making a super smooth joystick in native javascript that could open on any phone, and on connecting the server up so it could integrate into Unity.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/s2-1.gif" alt></p>
<p>And with that, we had the first playable build! We could connect in with our phones, and have a duel. It was time to bring in some friends.</p>
<p><img src="https://kvfrans.com/content/images/2019/11/--s-0.png" alt></p>
<blockquote>
<p>Live hotfixing some bugs</p>
</blockquote>
<p>And the final result....?</p>
<video loop width="100%" controls>
  <source src="https://kvfrans.com/static/jopper.mp4" type="video/mp4">
</video>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Omakase 3: Diplomacy]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>&quot;Diplomacy&quot; is a rather infamous game among board game enthusiasts -- It's an experience known to destroy the deepest friendships and create mistrust that can brew for years. That was a bit of exaggeration, but there's a key reason that Diplomacy has earned such a reputation: backstabbing is</p>]]></description><link>https://kvfrans.com/omakase-3-diplomacy/</link><guid isPermaLink="false">60225e9617e26d47c77e8ec9</guid><category><![CDATA[games]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Tue, 24 Sep 2019 14:02:56 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>&quot;Diplomacy&quot; is a rather infamous game among board game enthusiasts -- It's an experience known to destroy the deepest friendships and create mistrust that can brew for years. That was a bit of exaggeration, but there's a key reason that Diplomacy has earned such a reputation: backstabbing is not only expected, but crucial to securing victory.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/maxresdefault.jpg" alt></p>
<p>Diplomacy's ruleset is relatively simple. Each player represents a country along with the troops they control. By moving and capturing areas of the map, players can acquire supply points which grant them additional units every few turns. To win, a player must capture over half of the supply points.</p>
<p>The beauty of Diplomacy comes from the emergent player-to-player negotiation that is essential to gaining an advantage. The game is very clearly zero-sum: to take territory, another player has to lose it. Simply attacking another player head-on will surely lead to losses on both sides. Instead, it's better to form an alliance and join forces towards a common enemy -- but always be aware that your ally could easily betray you and take all the land for themselves!</p>
<p>In this month's Omakase game, I wanted to build a super simplified version of Diplomacy, that would encourage complex player-to-player relationships and negotiations while only taking ~30 minutes rather than 3 hours.</p>
<h6 id="mvpbrainstorming">MVP Brainstorming</h6>
<p>When deciding how to structure the game, I decided that a key tenet to design around was to encourage players to attack with alliances and discourage one-on-one skirmishes. At the same time, committing to an attack should leave one's back open, so it is always a risky idea. It should also always be optimal to gain more territory.</p>
<p>With these constraints in mind, a simple minimum-viable product would look something like:</p>
<ul>
<li>The map is a hex-grid akin to the Civilization games</li>
<li>Players spawn equal distances to randomly-placed supply points.</li>
<li>Each round, players command each of their units to move to a tile.</li>
<li>Turns are taken simultaneously.</li>
<li>If two opposing units move to the same tile, they fight and both are destroyed.</li>
<li>Every 4 rounds, players gain units equal to the number of supply points they control, and can place them adjacent to any friendly unit.</li>
</ul>
<p>Nearing the end of my time at Sizigi, I was able to test out this version on the whiteboard! My only regret is not taking pictures of the board every round, but I'll do my best to dictate the story.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/1--1---1-.jpg" alt></p>
<p>In the beginning, Red, Green, Blue, and Purple all spawn on opposite sides of the map. Triangles represent supply points.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/2--1---1-.jpg" alt></p>
<p>All players move their starting unit out to capture the nearest supply point.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/3--1---1-.jpg" alt></p>
<p>Spawning phase happens (every four movement rounds), and each player gets two unit spawns (1 from supply points, and 1 from their uncaptured home tile). Green places their units defensively, while blue and purple make aggressive plays towards each other and red creeps behind.</p>
<p>At this point, Red asks Purple for an alliance and Purple agrees. Green states that they will be non-aggressive as long as no one attacks them.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/4--1---1-.jpg" alt></p>
<p>Purple attacks Blue, and first blood is shed on the tile marked with yellow. Both Blue and Purple lose a unit. Right before the spawning round, Red sneaks a unit into Purple's home base and captures it.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/5--1---1-.jpg" alt></p>
<p>Green and Red get 4 and 5 spawns respectively, and Purple spawns ontop of a Blue unit, triggering combat once more. At this point, Blue calls out Red for being too powerful.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/6--1---1-.jpg" alt></p>
<p>Blue moves in and finally takes Purple down to a single unit. The stage is very close to a three-kingdoms stalemate. Red and Green are at relatively equal power levels, while Blue is still in the game but is weak. Red asks Blue for an alliance against Green, and Blue agrees if Red will allow them to take Purple's last supply point.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/7--1---1-.jpg" alt></p>
<p>Red backstabs Blue, taking the supply depot for themselves. At the same time, Green accidentally attacks Blue due to a bad prediction of Blue's movement patterns. Green and Blue start a fight among the northern border, and Red still pledges an alliance with Blue.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/8--1---1-.jpg" alt></p>
<p>Blue and Green continue fighting, and Red attacks Green along their border.</p>
<p><img src="https://kvfrans.com/content/images/2019/09/9--1---1-.jpg" alt></p>
<p>In a final play, with Green and Blue unsuspecting, Red attacks the central supply point with three units, capturing it and winning the game.</p>
<h6 id="analysis">Analysis</h6>
<p>So that was a sample round of my Diplomacy variant! Afterwards, I asked the players what they thought of the rules and gameplay.</p>
<p>The biggest feedback was that a lot of time, the game felt like trench warfare -- it was always optimal to hold your lines instead of going for the offensive, since you always lose units in a fight.</p>
<p>A radical solution that was suggested was to allow multiple units on the same tile, so more blitzkrieg-like tactics could be employed.</p>
<p>Overall, I was glad that some level of alliance trickery was involved in the game<br>
(mostly Red strongarming Blue and then betraying them multiple times), and the three-kingdom stalemate did not last as long as expected. It did feel like a lot of Red's victory was due to taking advantage of Purple's early aggression towards Blue, and the map was admittedly not super balanced.</p>
<p>I was originally going to build this into a web-based Diplomacy implementation, but I felt like there was more to be gained by iterating on whiteboards/paper than worrying about an implementation, so that didn't end up being finished.<br>
<img src="https://kvfrans.com/content/images/2019/09/Screen-Shot-2019-08-26-at-12-03-30-AM-copy.jpg" alt></p>
<blockquote>
<p>Grid in pixi.js with randomly-generated mountain tiles</p>
</blockquote>
<p>That's all for this month! I definitely think there's still a big potential in negotiation games, especially in extracting out the interesting player interactions without too much overhead with the turn-by-turn mechanics. Things like information delay and fog-of-war could also create some super cool scenarios.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Omakase 2: Suwapond]]></title><description><![CDATA[<!--kg-card-begin: markdown--><p>With this month's Omakase game, I wanted to take a super simple game idea, and working on polishing the <em>feel</em> of the core mechanic -- not necessarily the mechanic itself, but how satisfying it is to the player.</p>
<p>I was also traveling for a good half of the month, so</p>]]></description><link>https://kvfrans.com/omakase-2-suwapond/</link><guid isPermaLink="false">60225e9617e26d47c77e8ec8</guid><category><![CDATA[games]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Wed, 21 Aug 2019 23:34:32 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><p>With this month's Omakase game, I wanted to take a super simple game idea, and working on polishing the <em>feel</em> of the core mechanic -- not necessarily the mechanic itself, but how satisfying it is to the player.</p>
<p>I was also traveling for a good half of the month, so I put a few constraints on myself to not go overboard:</p>
<ul>
<li>I will write the movement mechanics from scratch.</li>
<li>I will not focus on level design.</li>
<li>I will focus on creating an aesthetic and learning techniques to make things feel good.</li>
</ul>
<p>Eventually, I decided on a super quick arcade-style scenario, where you control a little tadpole in a big pond. You can swim around, and cast a small magic attack.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/1.gif" alt></p>
<p>In this first iteration, I decided to test out an aesthetic I had been considering for a while -- build a scene entirely out of geometric shapes, but render it onto a pixel-art canvas.</p>
<p>The tadpole you see is actually a row of circles, each smaller than the last, connected by a physics rope. When seen together with the pixel-art renderer, they connect and form the shape of a tail!</p>
<p><img src="https://kvfrans.com/content/images/2019/08/2.gif" alt></p>
<p>Next, I wrote a quick bullet shooting behavior. This would end up being the core mechanic that I would polish up and make satisfying. As of now, however, the tadpole just launches a square towards the mouse.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/3.gif" alt></p>
<p>The first starts of adding 'juiciness'! When the bullets hit an enemy, they now disappear and shake the screen a bit. There is also a quick shoot and hit animation where I show a yellow circle.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/4.gif" alt></p>
<p>In the next iteration, I decided to all-in on the pixel feel, and I set the render solution to a tiny 160x90. Enemies can now flash white when hit, and blow up in a little red circle when they're killed.</p>
<p>The game is starting to feel fun to play now! I'm already starting to fall into the trap where I just keep playing the build without actually making any changes. But I manage to break free, and here I introduce a dynamic camera and some particles when shooting.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/6.gif" alt></p>
<p>Keeping with the particle theme, I add a few yellow splashes when the bullet collides with an enemy.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/7.gif" alt></p>
<p>And here we have the last bit of particle effects! Gone is the dinky little red circle -- when an enemy dies, they now explode into a flames and smoke.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/9-1.gif" alt></p>
<p>Finally, to wrap up this little project, I gave a better color scheme to the player, and turned the background into a blue-green gradient to emphasize the water feel. I added in a solid ground, and a water surface where the player can jump out of.</p>
<p><img src="https://kvfrans.com/content/images/2019/08/Screen-Shot-2019-08-21-at-11-39-01-PM.png" alt></p>
<p>And with that, the second Omakase is over! This month's was definitely a short one, due to my limited free time, but it was still a nice excersize in diving into how a few particles and screen shakes can make things so much more satisfying.</p>
<p>As always, grab the game <a href="https://drive.google.com/drive/folders/1NrvYAgVjRoePRswauOTVdAgcDoTabhyZ?usp=sharing">here</a>! Move with WASD and shoot with the mouse. I recommend playing in a small window rather than fullscreen.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[[Link]  How we built the Waifu Vending Machine]]></title><description><![CDATA[<!--kg-card-begin: markdown--><!-- link[https://waifulabs.com/blog/ax] -->
<p>We taught a world-class artificial intelligence how to draw anime. All the drawings on the left were made by a non-human artist! Wild, right? It turns out machines love waifus almost as much as humans do.</p>
<p>We proudly present the next chapter of human history: lit waifu commissions from the</p>]]></description><link>https://kvfrans.com/link-how-we-built-the-waifu-vending-machine/</link><guid isPermaLink="false">60225e9617e26d47c77e8ec7</guid><category><![CDATA[link]]></category><dc:creator><![CDATA[Kevin Frans]]></dc:creator><pubDate>Tue, 23 Jul 2019 18:09:19 GMT</pubDate><content:encoded><![CDATA[<!--kg-card-begin: markdown--><!-- link[https://waifulabs.com/blog/ax] -->
<p>We taught a world-class artificial intelligence how to draw anime. All the drawings on the left were made by a non-human artist! Wild, right? It turns out machines love waifus almost as much as humans do.</p>
<p>We proudly present the next chapter of human history: lit waifu commissions from the world's smartest AI artist.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>