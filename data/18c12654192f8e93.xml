<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Christopher Nguyen on Medium]]></title>
        <description><![CDATA[Stories by Christopher Nguyen on Medium]]></description>
        <link>https://medium.com/@ctn?source=rss-b0cd2903e078------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/0*rVEI2CSMmpDZXFzv.jpg</url>
            <title>Stories by Christopher Nguyen on Medium</title>
            <link>https://medium.com/@ctn?source=rss-b0cd2903e078------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 05 Nov 2022 16:38:08 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@ctn/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Did you enjoy “Algorithms of the Mind”?]]></title>
            <link>https://medium.com/deep-learning-101/did-you-enjoy-algorithms-of-the-mind-118d180916fd?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/118d180916fd</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Sun, 13 Sep 2015 18:54:03 GMT</pubDate>
            <atom:updated>2015-09-13T18:54:03.564Z</atom:updated>
            <content:encoded><![CDATA[<p>If you’re reading this on a leisurely Sunday, perfect!</p><p>This is Christopher Nguyen, CEO of Adatao and one of the editors/contributors to Deep Learning 101.</p><p>It occurs to me that most everyone arrives here via <em>Algorithms of the Mind</em>, and may not have seen <a href="https://medium.com/deep-learning-101/what-you-must-know-about-big-data-and-machine-learning-e6051a76ccd0"><em>What You Must Know About Big Data and Machine Learning</em></a>, the podcast I did with Sonal Chokshi of Andreessen Horowitz back in June.</p><p>Yes? Take a listen to <a href="https://medium.com/deep-learning-101/what-you-must-know-about-big-data-and-machine-learning-e6051a76ccd0"><em>Big Data/Machine Learning</em></a><em>. </em>It’s a good bridge to connect the dots from where we are today to where <em>Algorithms of the Mind</em> will lead us 50 years from now.</p><p>Enjoy &amp; I look forward to hearing from you.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=118d180916fd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/deep-learning-101/did-you-enjoy-algorithms-of-the-mind-118d180916fd">Did you enjoy “Algorithms of the Mind”?</a> was originally published in <a href="https://medium.com/deep-learning-101">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Inceptionism: Google Brain Imagination]]></title>
            <link>https://medium.com/deep-learning-101/inceptionism-google-brain-imagination-3ccbd41ca704?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/3ccbd41ca704</guid>
            <category><![CDATA[imagination]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[visualization]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Sun, 21 Jun 2015 17:35:26 GMT</pubDate>
            <atom:updated>2015-06-21T17:35:26.018Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*if_Jes2Ubx2wL41KTZWzpQ.jpeg" /><figcaption><em>Image Credit: Googler Mike Tyka’s </em><a href="https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB"><em>Inceptionism Library</em></a></figcaption></figure><p>If you liked <a href="https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4"><em>Algorithms of the Mind</em></a>, you’ll love this: <a href="http://qz.com/432678/the-dreams-of-googles-ai-are-equal-parts-amazing-and-disturbing/"><em>The “dreams” of Google’s AI are equal parts amazing and disturbing</em></a>. Also read <a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">the original Google Research blog post</a>.</p><p>Related, here’s a compelling example of “seeing with our brains and not with our eyes”, <a href="http://www.bloomberg.com/news/articles/2015-06-19/now-blind-americans-can-see-with-device-atop-their-tongues">Now Blind Americans Can See with Device Atop Their Tongues</a>.</p><p>And yes, Google is involved.</p><p>Christopher Nguyen — https://medium.com/@ctn</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3ccbd41ca704" width="1" height="1" alt=""><hr><p><a href="https://medium.com/deep-learning-101/inceptionism-google-brain-imagination-3ccbd41ca704">Inceptionism: Google Brain Imagination</a> was originally published in <a href="https://medium.com/deep-learning-101">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What You Must Know About Big Data and Machine Learning]]></title>
            <link>https://medium.com/deep-learning-101/what-you-must-know-about-big-data-and-machine-learning-e6051a76ccd0?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/e6051a76ccd0</guid>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[big-apps]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Wed, 10 Jun 2015 08:45:18 GMT</pubDate>
            <atom:updated>2015-09-13T23:43:02.566Z</atom:updated>
            <content:encoded><![CDATA[<p>Why “Volume, Velocity, and Variety” are wrong</p><p><em>Originally published at </em><a href="http://adatao.com/blog/featured/2015/a16z-podcast-1/"><em>blog.adatao.com</em></a></p><p>A few weeks ago, <a href="http://twitter.com/smc90">Sonal Chokshi</a> of <a href="http://a16z.com">Andreessen Horowitz</a> and I chatted on an a16z podcast. Here’s <a href="https://soundcloud.com/a16z/big-data-machine-learning">her summary</a> of that conversation:</p><blockquote>On this episode of the a16z Podcast, Nguyen puts on his former computer science professor hat to describe “<strong>Big Data</strong>” in relation to “<strong>Machine Learning</strong>”— as well as what comes next with “<strong>Deep Learning</strong>”. Finally, the former Google exec shares how Hadoop and Spark evolved from the efforts of companies dealing with massive amounts of real-time information; what we need to make machine learning a property of every application (Why would we even want to?); and how we can make all this intelligence accessible to everyone.</blockquote><blockquote>“Machine Learning is to Big Data as<br>Human Learning is to Life Experiences”</blockquote><p>We’ve heard from many people that this made so much more sense of Big Data and Machine Learning for them. So, I hope you’ll enjoy listening to this conversation about Big Data, Machine Learning, and the future of Deep Learning.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fw.soundcloud.com%2Fplayer%2F%3Furl%3Dhttp%253A%252F%252Fapi.soundcloud.com%252Ftracks%252F203500102%26show_artwork%3Dtrue&amp;url=https%3A%2F%2Fsoundcloud.com%2Fa16z%2Fbig-data-machine-learning&amp;image=http%3A%2F%2Fa1.sndcdn.com%2Fimages%2Ffb_placeholder.png%3F1433756039&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=soundcloud" width="500" height="166" frameborder="0" scrolling="no"><a href="https://medium.com/media/237621aa956930f6670d3a756c74c1f3/href">https://medium.com/media/237621aa956930f6670d3a756c74c1f3/href</a></iframe><p><a href="https://twitter.com/intent/user?screen_name=pentagoniac">Follow me on Twitter</a> to keep informed of interesting developments on these topics.</p><h4>Transcript</h4><p><strong><em>Sonal Chokshi: </em></strong><em>Hi everyone, welcome to the A16Z Podcast. This is Sonal and I’m here today with Christopher Nguyen from Adatao, which is a big data company and its mission is to democratize data intelligence and help people collaborate across the enterprise.</em></p><p><em>The best way to describe him is he’s an entrepreneurial scientist. He got his PhD from Stanford in Device Physics. He’s a former Google executive and as a professor, he started a computer engineering program at the Hong Kong University of Science and Technology. He’s basically an entrepreneurial scientist who’s merged the worlds of academia and doing a lot of startups.</em></p><p><em>Welcome, Christopher.</em></p><p><strong>Christopher N.: </strong>Thank you, Sonal.</p><p><strong><em>Sonal Chokshi: </em></strong><em>Actually, Christopher, maybe we want to just kick this off is..I actually just want to talk to you starting with big data. That’s a term that people throw around all the time, it’s completely overloaded, it’s buzz word, it means so many things to so many different people. Could you start by just telling me what your definition and take on big data is?</em></p><p><strong>Christopher N.:</strong> There are two ways you can think of the term big data. There is what I think most of the world thinks about when people talk about big data, they think of the V’s, starting out as three V’s, <strong>v</strong>olume, <strong>v</strong>ariety, <strong>v</strong>elocity and so on. Then I think it’s now up to seven or eight different V’s, <strong>v</strong>eracity, <strong>v</strong>ariance, and so on.</p><p>I actually don’t like that definition. I think that definition is functionally correct but it focuses on the problems of big data. These are the challenges that you have to deal with when you deal with big data but the definition skips or misses the part where it says you ask the question, “Why do you want to deal with these problems?” It turns out the reason for big data is machine learning.</p><blockquote>“The reason for Big Data is Machine Learning”</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> The reason for big data is machine learning. That’s actually kind of counter-intuitive because I’ve actually heard it the other way around, that big data exists because of machine learning.</em></p><p><strong>Christopher N.: </strong>I like something that Peter Norvig, the Director of Research at Google said when he referred to big data is, “Big data is not just quantitatively different but it’s qualitatively different.” In other words, there’s something that happens when you have enough data, it crosses a certain threshold.</p><p>For example, if you want to learn whether it hurts to hit your head against a brick wall, about five samples is probably very big data. If you want to learn how to classify images on the Internet, maybe two million samples are not big enough. It’s not so much a matter of how much data you have but how much is enough is to learn from.</p><p>When companies like Google, I would say it’s one of the original big data companies, when they started their life the very first batch of data they dealt with was big data. The term big data does not exists in these companies. They’ve always learned to take advantage of this data to make a lot of decisions.</p><p><strong><em>Sonal Chokshi:</em></strong><em> The way I’ve heard it is that big data … that machine learning is one of many uses for big data.</em></p><p><strong>Christopher N.: </strong>Right.</p><p><strong><em>Sonal Chokshi:</em></strong><em> But you’re basically arguing for something different, can you describe what that is and why?</em></p><p><strong>Christopher N.:</strong> Sure. If you think about the V’s definition of big data they are all problematic. We tend not to want to have problems unless there’s a reason for it, there’s a greater benefit to pay that cost. The benefit of big data is really, because we can unleash algorithms at them and these algorithms can automatically detect patterns and see these patterns.</p><p>I want to jump into that right away, because a lot of us in machine learning say this all the time, “What does it mean to detect patterns and so on?” The people take that for granted but then it’s a little fuzzy. The way I think about big data is, when machines learn from big data is very much like human beings learn from life experiences.</p><blockquote>“Machines learn from Big Data like<br>Humans learn from Life Experiences</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> That’s actually interesting. I just want to hear more about why you make that analogy. You’re basically saying that machine learning is the way humans learn from life experiences, do you mean the way like a kid learns to navigate the world for the first time?</em></p><p><strong>Christopher N.: </strong>Absolutely, that’s exactly right. For example, let’s turn, let’s flip that around and imagine would you like to have a child develop without any experiences and after 20 years what would that child, that person be like? Then why is it that we ascribe wisdom generally to older people than younger people?</p><p>Our brain capacity essentially remains about constant after a certain age, 16, 18, 20, whatever research you read, and yet wisdom continues to grow and accumulate and that’s … as the brain incorporates life experiences it is taking in a lot of big data, just like what machine learning algorithms do with data.</p><p>The opposite of that is rule-based computing or rule-based expert systems. You can come up with 10, 20, 30 rules and so on, but you can never come up with enough rules to handle the exceptions.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Exactly. Is machine learning for the exception handling then, or for everything? How does that work when you’re talking about computing?</em></p><p><strong>Christopher N.:</strong> In a very real sense it is. You can think of it as for exception handling, but I like to think of it in terms of analogy as wisdom. You do have the rules but then you know when the rules don’t apply. The reason you know when the rules don’t apply is because you’ve seen three or four or five corner cases before. Somehow “intuitively” you find that in this situation that rule doesn’t apply, but what we think of as intuition are actually, you can think of as parameters inside a machine-learning model.</p><p><strong><em>Sonal Chokshi: </em></strong><em>That’s interesting but how does … Just to be more concrete about that, that makes a lot of sense logically but concretely for businesses, like when you think about the business intelligence space and where we’ve been and where we are now, what’s different here? What’s happening? What do we get out of it? Basically, I guess I’m asking.</em></p><p><strong>Christopher N.:</strong> Right, that’s a great question. Even the term business intelligence, sometimes we’re captured by what we meant in the past, and so what we said in the past was BI.</p><p><strong><em>Sonal Chokshi: </em></strong><em>BI, being business intelligence.</em></p><p><strong>Christopher N.:</strong> Exactly, business intelligence, can be self-limiting. In other words what business intelligence was, was limited by what was available. What was available was the ability to essentially look backward. You can ask a lot of questions using what we call aggregations.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Aggregations.</em></p><p><strong>Christopher N.:</strong> You have a whole bunch of transactions that come in from all over the world and you can say, “Well, how much revenue did we make yesterday from that particular region of the world?” These are backward looking information, because that’s all we were capable of doing and because there was a particular lack of something and that something was big data. With enough data from all of that experiences what we can do is we’ll build a model out of that and project into the future.</p><p>You can think of business intelligence going forward as the ability to apply machine learning algorithms to big data and not just look at past questions but also future questions. We’re asking to predict the unknowns from the knowns.</p><blockquote>“Business Intelligence will become predicting the Unknowns from the Knowns</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> What’s changed to make that possible? Because in the days of business intelligence, I think of stuff, the products that SAP and similar companies put out. What’s changed to make big data possible? I know the big obvious things are just more computing power, but more concretely like what’s physically making this possible to be able to parse and get all these, get these insights out of this data?</em></p><p><strong>Christopher N.:</strong> Right. If you think about, it a lot of people have pointed out that big data has always existed. It’s always been there, we just didn’t collect it. Then the second insight that I think about is that we don’t necessarily get smarter over time. It’s just that certain technologies get cheaper, they become more available.</p><p>Machine learning algorithms have always been around. The data that exists that you could collect has always been around but it wasn’t until the advent of things like the Hadoop Project, and the launch of companies like Cloudera and MapR back in 2009. It made it affordable for many, many more companies to begin acquiring and storing a lot of this data.</p><p><strong><em>Sonal Chokshi: </em></strong><em>I’m actually glad you brought up Hadoop, Christopher, because one of the things that I see a lot in reading about the big data space is a lot of myths and misconceptions around what Hadoop is, what Spark is. Because now we talk a lot about Apache Spark and we have a lot of, at A16Z full disclosure, we have investments in every level of the BDAS, the big Berkeley Data Analytics Stack coming out of the AMPLab.</em></p><p><em>Can you talk to us a little bit more about what exactly Hadoop does, and what Spark does, and how they all live together, and then, how that actually fits in to big data? For people who don’t actually crunch those numbers behind the scenes?</em></p><p><strong>Christopher N.:</strong> Sure. I think we can look at it from two perspectives. I think that there is a top-down view and there is the bottom-up view. Let me start with the bottom up view because that’s how technology is always developed. We always build things from the bottom up and then we realize there’s a pattern here, and then we look top-down again.</p><p>From the bottom-up view, Hadoop is primarily a storage layer. There is the HDFS, the Hadoop file system. The distinction between that particular file system and other file systems in the past, I think the essential difference is that it is highly parallel.</p><p><strong><em>Sonal Chokshi: </em></strong><em>Parallel, in terms of parallel processing?</em></p><p><strong>Christopher N.:</strong> Parallel with storage, replication, and so on, so that you can have a lot of resiliency, and then it also is capable of running on commodity hardware. For the first time people can afford to buy many terabytes of storage and store it reliably, and still pay only a little amount for that.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Sorry, just to take a step back for a moment, the reason Hadoop and its ELK were able to run on commodity hardware is because the hardware has gotten cheap enough, or because the way that it processes and the way it’s architected it’s optimized for that? They could be the same, in fact in the end, but I do think it’s important to understand what the driver of that is.</em></p><p><strong>Christopher N.:</strong> I think it’s both. It’s a supply and demand thing where sometimes the demand creates supply or sometimes the supply creates the demand. I think you can trace back, again, to companies like Google that started in the late 90&#39;s and early 2000&#39;s, and that started to use a lot of this commodity hardware.</p><p>Then also with Moore’s Law, making everything cheaper, essentially doubling the capacity that you can afford every 18 months. With that and then with companies that have taken down this path proving that there is something valuable about accumulating all these data and making decisions from it.</p><p>It’s all that intuition, as well as the actual economics of hardware prices going down, and the availability of open source projects. I think all of these things, the elements come together to essentially create the big data movement.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Where do Spark fit in to that?</em></p><p><strong>Christopher N.:</strong> Spark, if you go, continuing with this bottom-up view, if you start from the storage level, and you know that storage is not enough. You can’t just store things …</p><p><strong><em>Sonal Chokshi:</em></strong><em> Right, you’re not going to get insights out of just collecting them.</em></p><p><strong>Christopher N.:</strong> Exactly, interestingly lots of database implementations in companies, people actually do put data in and never get anything out. In any computing stack you need more than just storage. You need a compute layer.</p><p><strong><em>Sonal Chokshi:</em></strong><em> The first layer that you’re describing is the big data layer. That’s how you’re describing big data, it’s like storage.</em></p><p><strong>Christopher N.: </strong>That’s exactly right.</p><p><strong><em>Sonal Chokshi:</em></strong><em> I think right now people think about big data as actually getting the insights and analytics out of it, but you’re actually saying big data is just getting the big data, those that many signals and saving them in a certain place.</em></p><p><strong>Christopher N.:</strong> For the purpose of being precise, I’m going to slice this up into levels so that we can refer to them more accurately. At the bottom layer we’ve got this big data and then above that we need big compute, in order to process all of this big data.</p><p><strong><em>Sonal Chokshi:</em></strong><em> The storage layer, the processing layer, and what is big compute?</em></p><p><strong>Christopher N.: </strong>Big compute, the first example of big compute you can think of is MapReduce. MapReduce, I don’t mean in terms of the algorithm but I mean the actual implementation with the Hadoop Project, the Hadoop MapReduce. That’s a parallelized computing system that can take all this data, do some computation with it, and then put it back. Then maybe an aggregation, for example asking the same question, the example that I gave earlier, “How much money do we make off of this widget out of Europe yesterday?” is an aggregation question.</p><p>If you have a thousand transactions you can do it with one machine but if you have, somehow stored a 100 billion of these rows and you want to ask the same question, maybe you have to parallelize it. That’s what MapReduce allows you to do. Unfortunately, MapReduce is actually not designed originally to handle queries.</p><p><strong><em>Sonal Chokshi:</em></strong><em> First they only have two functions, map and reduce. Is that the reason, or is it because …</em></p><p><strong>Christopher N.:</strong> Actually the reason is a little deeper and more pragmatic than that. Interestingly a lot of people may not realize that MapReduce was designed to be slow.</p><p><strong><em>Sonal Chokshi: </em></strong><em>That is interesting. I didn’t know that.</em></p><p><strong>Christopher N.:</strong> Let me unpack that a little bit. MapReduce as implemented at Google by Jeff Dean and Sanjay Ghemawat back in the early 2000&#39;s, and then they published their work in 2004. That MapReduce engine at Google was intended to do one particular job, and that job was to crawl and index the web. When that happens, Google’s approach was to parallelized it over thousands of machines.</p><p>When you have thousands of commodity machines doing a task that may last half a day, the probability of one of those machines going down is approaching 1. In fact it is about 1, any single machine could go down. When a machine goes down a question comes up, “Do we start the job over?” Certainly, you don’t want to have to do that because then it will never finish. It’s designed in such a way that if any single machine goes down another machine can be brought up and pick up where it started.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Hence it’s slow enough to be able to do that.</em></p><p><strong>Christopher N.: </strong>Right, and the way you ensure that reliability is to write down everything every step of the way. If you do job A, and then you write out the results of job A, and then you do job B, write out the results of job B.</p><p><strong><em>Sonal Chokshi:</em></strong><em> What does Spark do differently?</em></p><p><strong>Christopher N.:</strong> Spark takes a different approach and as I said earlier, it’s not that we get smarter, it’s just that the constraints have changed. Spark’s goal is to be able to do a lot of these queries very, very fast. We’ve always known, independent of the economics of hardware and software, we know that the speed of access to RAM is a lot faster than accessing disk. In fact from CPU to RAM, you’re talking about 40 nanoseconds.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Sorry, just to be clear, when you say the speed of accessing RAM is a lot faster than accessing to disk, you’re just talking about how to get to the memory functions.</em></p><p><strong>Christopher N.: </strong>That’s right, but generally the machinery knows … will feel the speed. Getting to information stored in RAM is about six orders of magnitude faster than getting to information stored on disk. Spark’s approach is to use memory. Now Spark, if it was created five, six years before its time would have completely failed because memory was so much more expensive.</p><p><strong><em>Sonal Chokshi: </em></strong><em>Right, the hardware constraints were lifted there, that’s right.</em></p><p>Christopher N.: Exactly, and it was a few years after that of course, something else would have come in before Spark. The timing of Spark has a lot to do with its success. What Spark does for you is give you very fast query processing that the implementation, the MapReduce implementation of Hadoop doesn’t give you.</p><p><strong><em>Sonal Chokshi:</em></strong><em> That helps us understand a little bit more of the difference between Hadoop and Spark. You’re basically talking about the in-memory aspect of it, being able to do things a lot faster.</em></p><p><strong>Christopher N.: </strong>That’s right.</p><p><strong><em>Sonal Chokshi: </em></strong><em>What does that give us concretely for big data and machine learning?</em></p><p><strong>Christopher N.:</strong> That takes me to the top-down view, which is from the top down, e know that we always want things fast but then we also want them cheap.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Sorry, just to be … cantankerous for a second, why do we want things to be fast? Actually, like why do we always want them to be fast? What do we actually get out of that?</em></p><p><strong>Christopher N.:</strong> Fast is competitiveness. If you can get your answer five minutes before I can, you’ll make decisions and then you’ll make that purchase, you make that buy, to supply whatever it is, that will happen before I get there, and you win.</p><p>Sometimes it’s implicitly obvious that we want everything faster because fast is competitive, but it turns out the difference between fast and slow is very, very critical. When you can get something in real time or you can get something in five seconds as opposed to five minutes, you will actually change your workflow.</p><p>You will actually do something, that’s what I learned from the consumer perspective with things like Gmail and so on. We had a phrase we call the “five-second barrier.” If the user can’t get something done within five seconds they won’t ever do it. It’s not like they’ll do it at twice the latency. Fast enables new, different use cases that may otherwise not happen.</p><blockquote>“We had something called ‘The Five-Second Barrier’</blockquote><p><strong><em>Sonal Chokshi: </em></strong><em>That’s actually helpful because I think we tend to take it for granted that fast is better. I know obviously we want that information faster, but you’re basically talking about enabling entirely new workflows and use cases. Going back to what you were saying about the top down approach and where this fits …</em></p><p><strong>Christopher N.:</strong> From the top-down perspective now we have the capability. We have this big compute layer and then we have this big data layer. So we have the capabilities of actually doing things very fast on massive amounts of data. We can apply algorithms. We can bring all these algorithms to bear on big data and get a lot of insight, but that’s still not enough because we haven’t put the human in this place yet. It’s still machines and that’s the problem with the bottom-up approach.</p><p>When you look from the top down there’s humans sitting at the command and control, at the inside layer, and they’ve got to make decisions. So far, our industry has not built the bridge from all of these machinery to that human user. There’s a layer missing.</p><p><strong><em>Sonal Chokshi:</em></strong><em> The learning basically.</em></p><p><strong>Christopher N.:</strong> The learning, as well as the application layer. The interfaces, the user experience, all of that put together can be thought of as the Big Apps layer. I’d like to think of things in terms of big apps, on top of big compute, on top of big data. When you have these three working effectively, harmoniously together then you have a very, very good big data stack.</p><blockquote>“Big Apps, on top of Big Compute, on top of Big Data</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> Taking a step back for a moment, it seems obvious why the natural interfaces are pretty important for people to be able to interact with it. Let’s face it, the reason we are able to do any kind of computing is because of the GUI, like having a graphical user interface that allows us to not have to see the plumbing behind the scene. That seems pretty obvious that we need that.</em></p><p><em>What did that actually get you in the big data world? Sure, you can more easily read your data and get some insights from it, but I just feel like we throw that term around too much that we need a better interface to our data. What does it really get us?</em></p><p><strong>Christopher N.: </strong>I think one way to understand it is to look back into the past. We went from the typewriter to the computer keyboard, and to the mouse, and now to touch screen and so on. You could ask the same question, what does touch screens get us, and why didn’t we do it before?</p><p>The reason touch screens and finger gestures and so on are valuable is because they’re much more natural than using a keyboard, but the reason we didn’t have that before is because the hardware and the software to make that happen were not available, or too expensive to do so.</p><p>The same analogy applies with big data machine learning. We could imagine all those capabilities before but they were too expensive. We didn’t have the storage capabilities for all of the data and we didn’t have the big compute capacity to do all of this. But now that we do and they’re affordable what you will see is that all of this machine learning will be a property of every application.</p><blockquote>“Machine Learning will be a property of every application</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> What does that mean? I’ve heard Peter [Levine] say that as well. He makes the argument as well, that machine learning will be a property of every application as opposed to a stand-alone, isolated function, what does that actually mean?</em></p><p><strong>Christopher N.: </strong>Imagine a world where, let’s say … We work with a lot of people and we expect our colleagues to remember what we say and learn from the interactions and so on and so forth. Can you imagine a world where your colleagues are just simple automatons and they don’t understand what you’re saying, and you told them something and they don’t remember it the next day, and their actions don’t change the result of that?</p><p>I claim that there will be a day very soon when then you will feel that about the machines you work with. In other words you would expect that to be a property of all of these machines.</p><blockquote>“We would expect Learning to be a property of all our machines</blockquote><p><strong><em>Sonal Chokshi: </em></strong><em>You’re right. I think we already do expect it because we carry mobile phones with us all around, and it’s frustrating when you have a certain experience there that you can’t have with an application you’re using at work, on your desktop or anything else.</em></p><p><strong>Christopher N.: </strong>Exactly.</p><p><em>Sonal Chokshi: I definitely think you’re right, that we might already even be there in some way, or that we need to be expecting that, but what does that really give us? Because when I think about big data, I think about it in the abstract. It’s still not clear to me what machine learning being a property of every application, what does that do for us?</em></p><p><strong>Christopher N.:</strong> I’m going to give you an example by a story from one of the … There’s something called TGIF at Google, which actually we do at our company, Adatao, today as well, which is every Friday the execs basically come out and talk about almost every company secret possible to the whole company. People can ask any kind of questions that they want.</p><p>I remember there was one time when, at Google we were dealing with the problem of latency. Google cares a lot about speed. Larry was pushing everyone to make their services a lot faster. There was question people ask and say, “Hey Larry, we went from one second search delay to 500 millisecond, and 300 millisecond, a 100 millisecond. What do you want? I mean, what happens when we get to zero?” Then what Larry said was, “Why stop at zero? Why can’t it be negative latency?”</p><blockquote>“Why can’t it be negative latency?</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> Right.</em></p><p><strong>Christopher N.: </strong>Essentially what he meant was, “Why can’t our machines anticipate what we need? What we want to do.” That’s actually not as ridiculous as it may sound. Certainly, as human beings we do anticipate each other. Maybe if you see that I’m coughing or something, that you just go help me with a cup of water. Right now, I still have to tell the machine, even if we have a robot today to do that, I have to still ask that robot to do that.</p><p><strong><em>Sonal Chokshi: </em></strong><em>You have to specify …</em></p><p><strong>Christopher N.: </strong>What we get with predictive algorithms, what we get with machine learning, what we get with big data, remember big data is just life experiences, what we get with that is that our machines we’ll be able to learn. They will be able to anticipate. They will be able to predict. They will have behaviors that we normally expect of humans, of intelligent beings.</p><p><strong><em>Sonal Chokshi:</em></strong><em> When we take that to every applications though, because why is it not okay to have it be an isolated, stand-alone thing? What do we get out of it when it becomes a part of every application?</em></p><p><strong>Christopher N.:</strong> I think when it becomes part of every application then every component of the application will be receiving data all the time, maybe the screen is receiving my gestures, maybe my calendar is receiving appointments that I’m making, maybe even the location that I am at. Then they will be able to learn from all of this and make intelligent decisions about what calendar events to insert, what gestures to accept, and maybe I don’t even have to say that, it will just do that ahead of time for me.</p><p>In my view, in that world things will happen a lot better for me. It will become a lot easier for me to move around. It will become a lot easier for me to make decisions, and maybe a lot of decisions will also be suggested to me before I even have to think about it too much.</p><p><strong><em>Sonal Chokshi:</em></strong><em> A lot of what you’re talking about is machines inferring and really aiding, learning like humans and helping augment human intelligence. What happens next?</em></p><p><strong>Christopher N.: </strong>I think that’s a great question. I think if you back up and think about human evolution, there’s one variable that’s inexorably increasing. We may get taller, shorter, we may go from one continent to the next and so on, but one thing that’s been a single variable that’s constant or changing in one direction, that’s human intelligence. In fact our species intelligence. There’s absolutely no reason to think that we’re at the end of that. I think we’re just at the very beginning of that increasing intelligence.</p><blockquote>“Intelligence is the inexorably increasing property in Evolution</blockquote><p>A lot of the thing that we’re learning about machine learning itself, I’m really excited about that. If you look at the research in deep learning, what’s happening there, really in just the last 12 months, 24 months, to me, the exciting thing is that we’re learning so much about how our brains might work. It’s not just what the machines can do, but what they teach us about ourselves.</p><p>If you think about it from that perspective and think about how these algorithms are evolving, you actually see this very near future where human intelligence is going to be boosted by all this machine intelligence. That will actually change how we think about Evolution.</p><p><strong><em>Sonal Chokshi:</em></strong><em> It’s interesting because people treat deep learning sometimes as just great for machine learning but you’re basically putting out the same continuum, and saying it’s just more machine learning?</em></p><p><strong>Christopher N.: </strong>I think deep learning just happen to be one moniker of today, but it is a very important one because it’s showing some of us glimpses of the future, more so than at any time in the past. I think that’s the exciting thing.</p><p>What we’re doing, coming back, is the software that we’re building is essentially machine intelligence aiding human intelligence. Today, I would say in very primitive ways, I think it’s very helpful to enterprise but we’re just at the beginning of it. The next set of products we’re going to be building in deep-learning capabilities. We already have machines inside the company that can talk to each other.</p><p>It’s happening a lot faster than people are realizing, and I see it as our job to make sure that we, as the human species continue to leverage that power, as opposed to maybe one day be subjugated by it.</p><blockquote>“We want our species to leverage that power, not subjugated by it</blockquote><p><strong><em>Sonal Chokshi:</em></strong><em> No, totally. Just one last question then, concretely what do we get out of that? It’s interesting academically and clearly it’s interesting beyond academically, because companies are investing in it left and right. In fact, more so in the corporate sphere than even in the university sphere, but what do we get out of that deep learning? What concretely comes out of that?</em></p><p><strong>Christopher N.: </strong>I like to think of it in two ways, and I think they’re both concrete but perhaps one is more concrete than the other to some people’s views. Certainly, companies are helped when they have more intelligence about their data. People talk about, in the past you didn’t even know what was going on at the company level, let alone make decisions based out of it.</p><p>We’re coming to an age where you know what’s going on and the machines are also helping you make decisions. What you get out of it is competitiveness. Companies that invest in this and are good at this, that are data-intelligent, that are data-driven will win. That’s a competitive edge. That’s inevitable, but I think the larger picture also, is that as a species we’re explorers, it’s built in to our genes, and you can count on that as being inevitable.</p><blockquote>“Like space exploration, this is exploration of the mind</blockquote><p>Left alone, we’ll figure out that these are exciting frontiers that we will explore. We will always want to build intelligence. We will always want to build images of ourselves, if you will, maybe that intelligence that emerges would not be the same as human intelligence but we will attempt all of this. It’s just like space exploration, this is exploration of the mind.</p><p><strong><em>Sonal Chokshi:</em></strong><em> Using the computer. That’s great. Thank you, Christopher from Adatao, and that’s another episode of A16Z Podcast. Thanks everyone.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e6051a76ccd0" width="1" height="1" alt=""><hr><p><a href="https://medium.com/deep-learning-101/what-you-must-know-about-big-data-and-machine-learning-e6051a76ccd0">What You Must Know About Big Data and Machine Learning</a> was originally published in <a href="https://medium.com/deep-learning-101">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Algorithms of the Mind]]></title>
            <link>https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/10eb13f61fc4</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[cognition]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Fri, 22 May 2015 09:27:31 GMT</pubDate>
            <atom:updated>2016-02-15T18:37:29.306Z</atom:updated>
            <content:encoded><![CDATA[<p><em>What Machine Learning Teaches Us About Ourselves</em></p><p><em>Originally published at </em><a href="http://arimo.com/blog/featured/2015/algorithms-of-the-mind/"><em>blog.arimo.com</em></a><em>.<br></em><a href="https://twitter.com/intent/user?screen_name=pentagoniac"><em>Follow me on Twitter</em></a><em> to keep informed of interesting developments on these topics.</em></p><p><em>“</em>Science often <em>follows</em> technology, because inventions give us new ways to think about the world and new phenomena in need of explanation.”</p><p>Or so Aram Harrow, an MIT physics professor, counter-intuitively argues in <em>“</em><a href="http://arxiv.org/abs/1501.00011"><em>Why now is the right time to study quantum computing</em></a><em>”.</em></p><p>He suggests that the scientific idea of entropy could not really be conceived until steam engine technology necessitated understanding of thermodynamics. Quantum computing similarly arose from attempts to simulate quantum mechanics on ordinary computers.</p><p>So what does all this have to do with machine learning?</p><p>Much like steam engines, machine learning is a technology intended to solve specific classes of problems. Yet results from the field are indicating intriguing—possibly profound—scientific clues about how our own brains might operate, perceive, and learn. The <em>technology </em>of machine learning is giving us new ways to think about the <em>science</em> of human thought … and imagination.</p><h4>Not Computer Vision, But Computer Imagination</h4><p>Five years ago, deep learning pioneer <a href="http://www.cs.toronto.edu/~hinton/">Geoff Hinton</a> (who currently splits his time between the University of Toronto and Google) published the following <a href="http://www.cs.toronto.edu/~hinton/adi/index.htm">demo</a>.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FaY9crs69epI%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaY9crs69epI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaY9crs69epI%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/682bf3bb68b933897ceeded34e9713ee/href">https://medium.com/media/682bf3bb68b933897ceeded34e9713ee/href</a></iframe><p>Hinton had trained a five-layer neural network to recognize handwritten digits when given their bitmapped images. It was a form of computer vision, one that made handwriting machine-readable.</p><p>But unlike previous works on the same topic, where the main objective is simply to recognize digits, Hinton’s network could also <em>run in reverse. </em>That is, given the <em>concept</em> of a digit, it can regenerate images corresponding to that very <em>concept</em>.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FhiBn1rUawvw%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DhiBn1rUawvw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FhiBn1rUawvw%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/8dee0a9682fbb15b4d7e11f460b049ee/href">https://medium.com/media/8dee0a9682fbb15b4d7e11f460b049ee/href</a></iframe><p>We are seeing, quite literally, a machine imagining an image of the concept of “8”.</p><p>The magic is encoded in the layers between inputs and outputs. These layers act as a kind of <em>associative memory</em>, mapping back-and-forth from image and concept, from concept to image, all in one neural network.</p><blockquote>“Is this how human imagination might work?</blockquote><p>But beyond the simplistic, brain-inspired machine vision technology here, the broader scientific question is whether this is how human imagination — visualization — works. If so, there’s a huge <em>a-ha </em>moment here.</p><p>After all, isn’t this something our brains do quite naturally? When we see the digit 4, we think of the concept “4”. Conversely, when someone says “8”, we can conjure up in our minds’ eye an image of the digit 8.</p><p>Is it all a kind of “running backwards” by the brain from concept to images (or sound, smell, feel, etc.) through the information encoded in the layers? Aren’t we watching this network create new pictures — and perhaps in a more advanced version, even new internal connections — as it does so?</p><h4>On Concepts and Intuitions</h4><p>If visual recognition and imagination are indeed just back-and-forth mapping between images and concepts, what’s happening between those layers? Do deep neural networks have some insight or analogies to offer us here?</p><p>Let’s first go back 234 years, to Immanuel Kant’s <a href="https://books.google.com/books?id=4hYNXsazIUEC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"><em>Critique of Pure Reason</em></a><em>, </em>in which he argues that “Intuition is nothing but the <em>representation </em>of phenomena”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/916/1*ihobuo6jX0CgGXQc4GeeiA.png" /></figure><p>Kant railed against the idea that human knowledge could be explained purely as empirical and rational thought. It is necessary, he argued, to consider intuitions. In his definitions, “intuitions” are representations left in a person’s mind by sensory perceptions, where as “concepts” are descriptions of empirical objects or sensory data. Together, these make up human knowledge.</p><p>Fast forwarding two centuries later, Berkeley CS professor <a href="http://www.eecs.berkeley.edu/Faculty/Homepages/efros.html">Alyosha Efros</a>, who specializes in Visual Understanding, pointed out that “there are many more things in our visual world than we have words to describe them with”. Using word labels to train models, Efros argues, exposes our techniques to a <em>language bottleneck. </em>There are many more un-namable intuitions than we have words for.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pOq7J0t0EGswZ0iYtiJR2w.png" /><figcaption>There is an intriguing mapping between ML Labels and human Concepts, and between ML Encodings and human Intuitions.</figcaption></figure><p>In training deep networks, such as <a href="http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">the seminal “cat-recognition” work</a> led by <a href="http://www.technologyreview.com/lists/innovators-under-35/2014/visionary/quoc-le/">Quoc Le at Google/Stanford</a>, we’re discovering that the activations in successive layers appear to go from lower to higher conceptual levels. An image recognition network encodes bitmaps at the lowest layer, then apparent corners and edges at the next layer, common shapes at the next, and so on. These intermediate layers don’t necessarily have any activations corresponding to explicit high-level concepts, like “cat” or “dog”, yet they do encode a <em>distributed representation</em> of the sensory inputs. Only the final, output layer has such a mapping to human-defined labels, because they are constrained to match those labels.</p><blockquote>“Is this Intuition staring at us in the face?</blockquote><p>Therefore, the above encodings and labels seem to correspond to exactly what Kant referred to as “intuitions” and “concepts”.</p><p>In yet another example of machine learning technology revealing insights about human thought, the network diagram above makes you wonder whether <em>this</em> is how the architecture of Intuition — albeit vastly simplified — is being expressed.</p><h4>The Sapir-Whorf Controversy</h4><p>If — as Efros has pointed out — there are a lot more conceptual patterns than words can describe, then do words constrain our thoughts? This question is at the heart of the Sapir-Whorf or <a href="http://en.wikipedia.org/wiki/Linguistic_relativity">Linguistic Relativity Hypothesis</a>, and the debate about whether language completely determines the boundaries of our cognition, or whether we are unconstrained to conceptualize anything — regardless of the languages we speak.</p><p>In its strongest form, the hypothesis posits that the structure and lexicon of languages constrain how one perceives and conceptualizes the world.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*d93Jr9dsPFP2f2VU0ak8JQ.png" /><figcaption>Can you pick the odd one out? The Himba — who have distinct words for the two shades of green — can pick it out instantly. Credit: <a href="http://boingboing.net/author/mark_frauenfelder_1">Mark Frauenfelder</a>, <a href="http://boingboing.net/2011/08/12/how-language-affects-color-perception.html"><em>How Language Affects Color Perception</em></a>, and <a href="https://medium.com/@arraymac">Randy MacDonald</a> for verifying the RGB’s.</figcaption></figure><p>One of the most striking effects of this is demonstrated in the color test shown here. When asked to pick out the one square with a shade of green that’s distinct from all the others, the Himba people of northern Namibia — who have distinct words for the two shades of green — can find it almost instantly.</p><p>The rest of us, however, have a much harder time doing so.</p><p>The theory is that — once we have words to distinguish one shade from another, our brains will train itself to discriminate between the shades, so the difference would become more and more “obvious” over time. In seeing with our brain, not with our eyes, language drives perception.</p><blockquote>“We see with our brains, not with our eyes.</blockquote><p>With machine learning, we also observe something similar. In supervised learning, we train our models to best match images (or text, audio, etc.) against provided labels or categories. By definition, these models are trained to discriminate much more effectively between categories that have provided labels, than between other possible categories for which we have not provided labels. When viewed from the perspective of supervised machine learning, this outcome is not at all surprising. So perhaps we shouldn’t be too surprised by the results of the color experiment above, either. Language does indeed influence our perception of the world, in the same way that labels in supervised machine learning influence the model’s ability to discriminate among categories.</p><p>And yet, we also know that labels are not strictly required to discriminate between cues. In Google’s “cat-recognizing brain”, the network eventually discovers the concept of “cat”, “dog”, etc. all by itself — even without training the algorithm against explicit labels. After this unsupervised training, whenever the network is fed an image belonging to a certain category like “Cats”, the same corresponding set of “Cat” neurons always gets fired up. Simply by looking at the vast set of training images, this network has discovered the essential patterns of each category, as well as the differences of one category vs. another.</p><p>In the same way, an infant who is repeatedly shown a paper cup would soon recognize the visual pattern of such a thing, even before it ever learns the words “paper cup” to attach that pattern to a name. In this sense, the strong form of the Sapir-Whorf hypothesis cannot be entirely correct — we can, and do, discover concepts even without the words to describe them.</p><p>Supervised and unsupervised machine learning turn out to represent the two sides of the controversy’s coin. And if we recognized them as such, perhaps Sapir-Whorf would not be such a controversy, and more of a reflection of supervised and unsupervised human learning.</p><p>I find these correspondences deeply fascinating — and we’ve only scratched the surface. Philosophers, psychologists, linguists, and neuroscientists have studied these topics for a long time. The connection to machine learning and computer science is more recent, especially with the advances in big data and deep learning. When fed with huge amounts of text, images, or audio data, the latest deep learning architectures are demonstrating near or even better-than-human performance in language translation, image classification, and speech recognition.</p><p>Every new discovery in machine learning demystifies a bit more of what may be going on in our brains. We’re increasingly able to borrow from the vocabulary of machine learning to talk about our minds.</p><h4>Further Reading</h4><ol><li><a href="http://arxiv.org/abs/1501.00011"><em>Why Now Is the Right Time to Study Quantum Computing</em></a>, Aram Harrow — arXiv, Dec. 2014.</li><li><a href="https://books.google.com/books?id=n53FAgAAQBAJ&amp;pg=PA307&amp;lpg=PA307&amp;dq=geoff+hinton+adi&amp;source=bl&amp;ots=hVLVTqicEO&amp;sig=gP383JySLvN-FayEBYRSsUP8UEU&amp;hl=en&amp;sa=X&amp;ei=RjhhVYfqJ4zxoATmvYGAAQ&amp;ved=0CDEQ6AEwAw#v=onepage&amp;q=geoff%20hinton%20adi&amp;f=false"><em>The Anticipating Brain</em></a>, Thomas Trappenberg — Fundamentals of Computational Neuroscience, Oxford University Press, 2010.</li><li><a href="https://books.google.com/books?id=4hYNXsazIUEC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"><em>Critique of Pure Reason</em></a>, Immanuel Kant — The Colonial Press, 1899.</li><li><a href="https://www.smashwords.com/books/download/37727/1/latest/0/0/understanding-kant-concepts-and-intuitions.pdf"><em>Understanding Kant: Concepts and Intuitions</em></a>, Hercules Bantas — The Reluctant Geek, 2011.</li><li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/unsupervised_icml2012.pdf"><em>Building High-level Features Using Large Scale Unsupervised Learning</em></a>, Quoc Le et al. — International Conference on Machine Learning, July 2012.</li><li><a href="https://books.google.com/books?id=W2d1Q4el00QC&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false"><em>Language, Thought, and Reality</em></a>, John B. Carroll — Selected Writings of Benjamin Lee Whorf, 1956.</li><li><a href="http://www1.icsi.berkeley.edu/~kay/tics2.pdf"><em>Language, Thought, and Color: Whorf Was Half Right</em></a><em>, </em>Terry Regier &amp; Paul Kay — Trends in Cognitive Sciences, Oct. 2009.</li><li><a href="http://research.gold.ac.uk/4935/1/davidoff-goldstein-color-terms.pdf"><em>Knowing Color Terms Enhances Recognition: Further Evidence from English and Himba</em></a>, Julie Goldstein, Jules Davidoff, Debi Roberson — Journal of Experimental Child Psychology, Feb. 2009.</li><li><a href="http://boingboing.net/2011/08/12/how-language-affects-color-perception.html"><em>How Language Affects Color Perception</em></a>, Mark Frauenfelder — Boing Boing, Aug. 2011.</li><li><a href="http://strataconf.com/big-data-conference-ca-2015/public/schedule/detail/40181"><em>Visual Understanding Beyond Naming</em></a>, Alyosha Efros — Strata+Hadoop World, Feb. 2015.</li><li><a href="https://www.youtube.com/watch?v=Q-B_ONJIEcE"><em>Linguistics as a Window to Understanding the Brain</em></a>, Steven Pinker — Big Think, Oct. 2012.</li><li><a href="http://arxiv.org/abs/1501.02876"><em>Deep Image: Scaling up Image Recognition</em></a>, Ren Wu et al. — arXiv, May 2015.</li><li><a href="https://medium.com/deep-learning-101/on-deep-learning-a-tweeted-bibliography-68ab095376e7"><em>On Deep Learning, A Tweeted Bibliography</em></a>, Christopher Nguyen — medium.com, 2015.</li><li><a href="http://arxiv.org/pdf/1407.5104.pdf"><em>Pixels to Voxels: Modeling Visual Representation in the Human Brain</em>,</a> Pulkit Agrawal et al. — arXiv, Jul. 2014.</li></ol><p><em>Thanks to </em><a href="https://twitter.com/smc90"><em>Sonal Chokshi</em></a><em> and </em><a href="https://medium.com/@vupham"><em>Vu Pham</em></a><em> for extensive review &amp; edits. Also, </em><a href="https://medium.com/@chrisjagers"><em>chrisjagers</em></a><em>, </em><a href="https://medium.com/@chickamade"><em>chickamade</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=10eb13f61fc4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4">Algorithms of the Mind</a> was originally published in <a href="https://medium.com/deep-learning-101">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Democratizing Data Intelligence for All]]></title>
            <link>https://medium.com/adatao-narratives/democratizing-data-intelligence-for-all-d2411f645f0d?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/d2411f645f0d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[big-apps]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Sat, 02 May 2015 03:04:57 GMT</pubDate>
            <atom:updated>2016-02-22T05:47:20.416Z</atom:updated>
            <content:encoded><![CDATA[<h4>A Major Progress Report</h4><p><em>April 30th, 2015 (</em><a href="http://arimo.com/blog/"><em>Originally published at http://arimo.com/blog/</em></a><em>)</em></p><p>Just as accessibility of the automobile transformed our mobility in the the 20th century, accessibility of data intelligence will transform our minds in the 21st. Over the next 10 years, machine learning is going to be pervasively present in our everyday lives.</p><p>For the past decade, the Big Data conversation has been about HDFS, MapReduce, monads and monoids. Today, we are changing the conversation, from bottom-up to top-down.</p><p>I’m excited to announce the general availability of the <em>Arimo Data Intelligence Platform</em>. The platform comprises of a set of web applications that combine beautiful, consumer-grade user experience with enterprise-grade power and sophistication, for both business users and data scientists. Just as the iPhone democratized the mobile Internet, Arimo Apps are here to democratize data intelligence.</p><h3>An Evolutionary Perspective</h3><p>Twelve years ago, Sanjay Ghemawat et al. published the seminal paper on Google’s distributed filesystem, GFS, that enabled the company to store multiple copies of the entire Internet on inexpensive commodity hardware. A year later, Jeff Dean et al. revealed details of Google’s implementation of MapReduce, the Big-Compute engine that scaled to process all this data. Soon, Hadoop was launched by Doug Cutting, and in 2009, MapR and Cloudera were born to provide commercial support for it. About the same time, Matei Zaharia began work at the Berkeley AMPLab on what became <a href="http://adatao.com/why-how-adatao-bet-on-apache-spark/">Apache Spark</a>, a fast in-memory Big-Compute engine on top of Big-Data storage.</p><p>When we started Arimo with a focus on Big Data and Machine Learning, we knew that Big Compute on Big Data were not enough. We wanted to create the missing Big Apps layer to have all the power and sophistication of big data and machine learning, but also connect all this to human users, with beautiful user experiences. While Steve Jobs’s iPhone was far from being the first mobile phone, it completely changed the conversation from technology to user experience, from bottom-up to top-down, from power to productivity.</p><h3>The Arimo Data Intelligence Platform (ADIP)</h3><p>The <strong>Arimo Data Intelligence Platform</strong> (ADIP) is an execution milestone. It comprises three key components: <strong>(1)</strong> <strong>Arimo</strong> <strong>Apps:</strong> user-facing applications with Google-Docs-like ease of use, yet with the predictive power and sophistication of a state-of-the-art Big Data systems <strong>(2)</strong> <strong>Arimo AppBuilder: </strong>a toolset that allows you to easily build your own custom applications using our patent-pending SmartQuery technology and application templates, and <strong>(3) Arimo PredictiveEngine:</strong> a powerful, distributed server with sophisticated algorithms that scale to petabytes on top of virtually any data source.</p><blockquote>The familiarity of office productivity tools</blockquote><p>If you are in Sales, Marketing or HR, you will experience the familiarity of office productivity tools and be able to ask questions of huge amounts of data without dependence on IT. Arimo Apps blend the simplicity and ease of a consumer application, with the power of Big Compute.</p><blockquote>Use the same languages, directly on all of your data</blockquote><p>If you are a data scientist familiar with R, Python or SQL, you continue to use the same languages, directly on all of your data and not just a sample, then easily share your analyses with a single URL. Arimo helps you eliminate the barriers between advanced analytics and large datasets, between building and deploying models, and facilitates collaboration with your business counterparts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oIHoG7YmJjkOUPvCLKIEmQ.png" /><figcaption>The overview of Adatao Data Intelligence Platform</figcaption></figure><p>Arimo’s Data Intelligence Platform can be deployed on-premise or in the AWS cloud. Arimo has partnered with Databricks, the team that created and continues to drive <a href="https://spark.apache.org">Apache Spark</a>, to provide fully managed Spark clusters, enabling an end-to-end integrated stack for our end users. Arimo has also partnered with Altiscale, the leading Hadoop-as-a-Service provider, to offer an end-to-end Big Data stack to our customers.</p><h3>Our Product Philosophy</h3><p>Our product philosophy is based on three key pillars:</p><ul><li><strong>Natural Interfaces</strong> — Instead of having to deal with clumsy interfaces and stodgy database concepts, business users simply ask questions with Arimo SmartQuery, “How much would revenue change if marketing spend rises by 1%?”, without the need for programming skills.</li><li><strong>Collaboration</strong> — Can you imagine a SWAT team planning a mission by email? Then why would you accept running your hyper-competitive, data-driven business that way? Business users and data scientists collaborate on the same data, at the same time, using tools and languages familiar to them: plain English, R, Python, SQL, Java, or Scala.</li><li><strong>Machine Learning </strong>— Instead of putting Machine Learning (ML) on some intimidating pedestal far removed from business users, Arimo applications are built with inherent predictive capabilities to bring the real value of Big Data to all users. We are at an inflection point in the intelligence of our computing systems; now ML is to software what software was to machines.</li></ul><h3>So why Natural Interfaces, Collaboration, and Machine Learning?</h3><p>Huge end-user value will be realized when the power of Big Data systems is intuitively accessible, through natural human interfaces. And as users of Google Apps testify, when people with diverse skills collaborate in the same space on the same data at the same time, time-to-decision is shortened and team productivity is boosted by 10x-100x.</p><p>The real reason for Big Data is Machine Learning. Big data has existed for a long time, but now data is retained so that our algorithms can automatically learn from them and do useful, intelligent things. ML will become an inherent property in all the tools and devices in our lives. We will demand all of our software and devices to be intelligent enablers for us, through learning from data.</p><p>Today, I welcome you to try the Arimo Data Intelligence Platform and experience the power and ease of use of Natural Interfaces, Collaboration, and Machine Learning in your everyday workflows with your colleagues and friends.</p><p><em>To learn more, please visit </em><a href="http://arimo.com/products/"><em>our product page</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d2411f645f0d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/adatao-narratives/democratizing-data-intelligence-for-all-d2411f645f0d">Democratizing Data Intelligence for All</a> was originally published in <a href="https://medium.com/adatao-narratives">Arimo Narratives™</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2015: The Year of Big Apps]]></title>
            <link>https://medium.com/adatao-narratives/2015-the-year-of-big-apps-7c8974cdb723?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/7c8974cdb723</guid>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Thu, 09 Apr 2015 21:23:33 GMT</pubDate>
            <atom:updated>2015-05-24T10:10:30.860Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Big Intelligence, from Big Apps on Big Compute on Big Data</em></p><p><em>March 17, 2015 (</em><a href="http://adatao.com/blog/featured/2015-the-year-of-big-apps/"><em>Originally published at blog.adatao.com</em></a><em>)</em></p><p>Technology revolutions play out in familiar patterns. And almost always from the bottom up.</p><p>Remember Web 1.0? That was about browsers and Javascript and web server infrastructure. Then in Web 2.0, we shifted the focus to top-down user experience. How about when relational databases were introduced to the market 30 years ago? Much of the focus then was on bottom-up data engines and relational algebra. Then came SQL, 4GL, GUIs, and the cycle finally completed with user-facing business applications.</p><blockquote><em>In Web 2.0, we shifted the focus to top-down user experience.</em></blockquote><p>The Big Data story follows the same arc. Big Data 1.0 has been about storage (e.g., HDFS) and computation (e.g., <a href="https://spark.apache.org/">Apache Spark</a>). We are now at the threshold of Big Data 2.0. It’s time to change the conversation and focus on end-user applications. These are Big-Data-native applications, which business users and data scientists can use to interact directly with their Big Data.</p><p>We’ll call them <em>“Big Apps”</em>.</p><h4>Big Apps on Big Compute on Big Data</h4><p>Big-Data applications (<em>Big Apps</em>) can’t happen before the arrival of Big Compute, sitting on top of Big Data. In that sense, Apache Spark and <a href="http://tachyon-project.org/">Tachyon</a> are key pieces of this larger puzzle. They play the role of the Big Compute engine that fills the gaps between Big Data and <em>Big Apps</em>. As I have written <a href="https://medium.com/adatao-narratives/why-and-how-we-bet-on-apache-spark-b6dd51151a68">elsewhere</a>, in Spark and Tachyon we have the perfect architectural timing. These engines correctly anticipate the cross-over between rising business value and dropping hardware (memory) costs.</p><p>But there’s another significant property of Spark that separates it from all other in-memory Big Compute engines. And this is something most of us do not fully appreciate.</p><p>In the Spark framework, data is stored in RDDs (Resilient Distributed Datasets), which are first-class citizens. RDDs have life cycles that transcend compute cycles.</p><p>This is very different from, say, Hadoop MapReduce, which holds data in memory only temporarily as an internal part of each Map-Reduce stage. When each stage completes, the data, transformed and summarized, is written out to disk. All persistence happens at the disk level.</p><p>What do RDDs buy us, besides memory-speed iterations between compute stages? They buy us something very significant: the ability to have a long-lived applications that can access high-level data structures without having to go back to disk, and without having to recompute them. If you look at other architectures that are Spark’s rivals, most will lack one property or another in this very important dimension.</p><p>Likewise, Tachyon provides us with the facility of persistent in-memory data structures. And once we have such structures, we can not only access them, we can also share them. Thus, Spark and Tachyon make it possible to create <em>collaborative</em> <em>Big Apps</em>.</p><blockquote>Spark RDDs have life cycles that transcend compute cycles, making it possible to create collaborative <em>Big Apps</em></blockquote><h4>Collaboration-Led Productivity: The Most Important Feature/Benefit for Users</h4><p>When I was working on Google Apps, we would often hear people ask, “Why launch Google Spreadsheets? It’s 20 years behind Microsoft Excel and 200 features short!” They didn’t realize that a driving mantra for Google Apps was “It’s the c<em>ollaboration</em>, People!” I have seen metrics, and still experience daily, how Google Apps’ real-time collaboration features boost team task productivity by a factor of 10x or more. It is collaboration among team members with diverse skillsets and points of view that yields these large gains in organizational smarts.</p><blockquote><em>It’s the </em>Collaboration<em>, People!</em></blockquote><p>When something can increase productivity by such a huge amount, arguments about data engines that run 20% faster just pale in comparison.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*lA7AHwJEYvGJkTK463zsYQ.png" /><figcaption>Adatao Big Apps for Business Analysts and Data Scientists</figcaption></figure><p>Therefore, to us at Adatao, the fact that Spark and Tachyon enable deep collaboration over Big Data is very significant. We have built a full suite of user-facing applications that exploits these collaboration capabilities. For example, <em>Adatao</em> <em>Narratives</em> is an interactive document that allows business analysts and data scientists to collaborate on creating data narratives, complete with text, interactive charts &amp; maps, in real time, on the same huge datasets. They can use different access languages, one using plain English, the other using R. They can even collaborate across different client applications, one using a web browser, the other using R-Studio.</p><p>The brain power, insights, and productivity supported by these capabilities are phenomenal.</p><h4>What’s Coming Next?</h4><p>What else can we do to help people and businesses become smarter and more productive? From the very first days where companies such as Google and Facebook started accumulating data at scale, it was about applying algorithms to learn from that data, to build better systems and to drive decisions. So if you think about it, the driving rationale for Big Data is really to marry it with Machine Learning to produce wisdom and insights. Thanks to Big Data and Big Compute, recent major advances in Deep Learning and related areas indicate that we are at the threshold of a significant acceleration in machine intelligence.</p><blockquote><em>Big Data is really about Machine Learning … We are at the threshold of a significant acceleration in machine intelligence.</em></blockquote><p>At Adatao, we are working to ensure that we can all enjoy the benefits of this acceleration. The power of predictive analytics can equip business systems to learn via examples. It will help us discover the unknown by interpolating from the knowns. It will help us forecast future outcomes by extrapolating from the past. Every one of Adatao’s <em>Big Apps </em>has such predictive capabilities built-in as native features, from natural human interfaces to advanced machine-learning algorithms in the engine. Machine Learning is strong in our team’s DNA, and we envision a future in which machine intelligence is increasingly leveraged to aid and boost our human intelligence.</p><p>I am optimistic about this future and excited about delivering Adatao <em>Big Apps</em>, towards a future with Data Intelligence for All.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7c8974cdb723" width="1" height="1" alt=""><hr><p><a href="https://medium.com/adatao-narratives/2015-the-year-of-big-apps-7c8974cdb723">2015: The Year of Big Apps</a> was originally published in <a href="https://medium.com/adatao-narratives">Arimo Narratives™</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[On Deep Learning — A Tweeted Bibliography]]></title>
            <link>https://medium.com/deep-learning-101/on-deep-learning-a-tweeted-bibliography-68ab095376e7?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/68ab095376e7</guid>
            <category><![CDATA[fundamentals]]></category>
            <category><![CDATA[rds]]></category>
            <category><![CDATA[deep-learning]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Sat, 04 Apr 2015 05:32:34 GMT</pubDate>
            <atom:updated>2015-05-24T09:50:09.375Z</atom:updated>
            <content:encoded><![CDATA[<h2>On Deep Learning</h2><p><em>A Tweeted Bibliography</em></p><p>Here’s a collection of my tweets on interesting/exciting developments in Deep Learning or Machine Learning in general.</p><p>It’s in no grand order, but does serve as a convenient reference &amp; provides some context.</p><h3>Christopher Nguyen on Twitter</h3><p>Ever wonder what Stochastic in SGD really means? From 1951, Robbins &amp; Monro http://bit.ly/1DIM0hq #MachineLearning pic.twitter.com/sJqDKsvFUt</p><h3>Adatao [ah-DAY-tao] on Twitter</h3><p>Here: our favorite reference on Discriminative vs. Generative classifiers, by @AndrewYNg http://buff.ly/1wA50bj pic.twitter.com/BbJoCIytYo</p><h3>Christopher Nguyen on Twitter</h3><p>Why do neural networks with more layers perform better than a single layer MLP with same # of params? http://bit.ly/1sGzYOj</p><h3>Christopher Nguyen on Twitter</h3><p>Universal function approximators: widely known, often misunderstood in #DeepLearning. Review http://bit.ly/1GTcWxz . pic.twitter.com/d2tJtn0eRM</p><h3>Christopher Nguyen on Twitter</h3><p>Geoff Hinton&#39;s Very Cool #DeepLearning demo. Run in reverse to see how Imagination might work http://bit.ly/1IWlcgE pic.twitter.com/DQJS8EjJGV</p><h3>Christopher Nguyen on Twitter</h3><p>Good 12/14 arXiv review of object recog w #DeepLearning. Amazingly it&#39;s also already outdated! http://bit.ly/1GRue08 pic.twitter.com/DaES8JFoPT</p><h3>Christopher Nguyen on Twitter</h3><p>A classic. Profoundly interesting in suggesting how we might encode concepts in our brains. http://bit.ly/1H0a0Be pic.twitter.com/SgIbdv7DBv</p><h3>Christopher Nguyen on Twitter</h3><p>Distributed-representation interpretation will prove largely correct #NeuralScience cf YBengio http://bit.ly/19147EK pic.twitter.com/cmZoB6hKOj</p><h3>Christopher Nguyen on Twitter</h3><p>Must-read: classic RHW Nature Letter, casually introducing backprop, &quot;that triggered a boom in neural net research&quot;. pic.twitter.com/hhjGEaOMhG</p><h3>Christopher Nguyen on Twitter</h3><p>MSFT working on custom FPGA sys for #DeepLearning http://bit.ly/1LWs7XP . Should also look into @ylecun&#39;s NeuFlow. pic.twitter.com/qJECGBaS5Z</p><h3>Christopher Nguyen on Twitter</h3><p>MSFT Asia #DeepLearning team just demonstrated better-than-human visual recog. +1 @harryshum! http://bit.ly/1A03iFa pic.twitter.com/Xc28rPW596</p><h3>Christopher Nguyen on Twitter</h3><p>Microsoft team demonstrates shallow nets can rival #DeepLearning nets, suggesting alternative... http://bit.ly/1yEqdIA pic.twitter.com/0IWkwIz4mN</p><h3>Christopher Nguyen on Twitter</h3><p>New @MSFTResearch algorithm helps scale ad predictions &amp; #DeepLearning to billions of vars http://bit.ly/1GGS1KF pic.twitter.com/VmiABaT7iB</p><h3>Andrew Ng on Twitter</h3><p>Deep Speech improves speech recognition; outperforms Bing/Google/Apple APIs in noisy environments! http://onforb.es/1x2BpOu</p><h3>Christopher Nguyen on Twitter</h3><p>DeepLearning UMich group has developed RL technique to best Google&#39;s $400MM DeepMind at Real-Time Atari Games http://bit.ly/1AgRjmx</p><h3>Christopher Nguyen on Twitter</h3><p>GOOG +1 over MSFT! Input norm &gt;Param init, 1/10x training steps #DeepLearning HT @annodomini80 http://bit.ly/1E6dZXq pic.twitter.com/2QARCdZdoQ</p><h3>Christopher Nguyen on Twitter</h3><p>How do you find the best way to visualize a 784-dimensional dataset? http://bit.ly/1DOnE7Y pic.twitter.com/qVBUrUnv9V</p><h3>Christopher Nguyen on Twitter</h3><p>This is profound. Why wait for ASICs given our already very powerful #DeepLearning machine? #OneAlgorithm #Evolution http://bit.ly/1NFqEUK</p><h3>Christopher Nguyen on Twitter</h3><p>Google team&#39;s #DeepLearning sentence translator outperforms statistical machine translation http://bit.ly/15qdvjX pic.twitter.com/e6ZjdodL5s</p><h3>Adatao [ah-DAY-tao] on Twitter</h3><p>DeepLearning is figuring out how to tell stories from pictures #DataNarratives #DataViz http://buff.ly/1Dv9UxA pic.twitter.com/ayzu4yWl8k</p><h3>Christopher Nguyen on Twitter</h3><p>Neural Turing Machine creates its own algorithms (#NotAI, because I know exactly how it works) http://arxiv.org/pdf/1410.5401v2.pdf ... pic.twitter.com/nragAASyvg</p><h3>Christopher Nguyen on Twitter</h3><p>From Phil Colella&#39;s 7 dwarfs to Dave Patterson&#39;s 13: do get that MapReduce is just 1 of many http://bit.ly/1DgZ77Z pic.twitter.com/FTq3lbrh86</p><h3>Christopher Nguyen on Twitter</h3><p>This is the remarkable #DeepLearning Q&amp;A work at @facebook demo&#39;ed by @schrep at recent F8 http://buff.ly/1IQykEP pic.twitter.com/XkPEdMZPzn</p><h3>Christopher Nguyen on Twitter</h3><p>Introducing iRNNs, in a co-pub of Quoc Le and Geoff Hinton (and Navdeep Jaitly) http://bit.ly/1FediQX #DeepLearning pic.twitter.com/0KXgPz3xEK</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=68ab095376e7" width="1" height="1" alt=""><hr><p><a href="https://medium.com/deep-learning-101/on-deep-learning-a-tweeted-bibliography-68ab095376e7">On Deep Learning — A Tweeted Bibliography</a> was originally published in <a href="https://medium.com/deep-learning-101">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Big Data 2.0 Has Arrived]]></title>
            <link>https://medium.com/adatao-narratives/big-data-2-0-has-arrived-1aedb661aa68?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/1aedb661aa68</guid>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[adatao-funding]]></category>
            <category><![CDATA[big-compute]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Tue, 17 Feb 2015 19:36:51 GMT</pubDate>
            <atom:updated>2015-05-24T10:12:34.120Z</atom:updated>
            <content:encoded><![CDATA[<p><em>When We Can Process Data Flexibly At-Scale, What Will We Want To Do? — </em>Provost &amp; Fawcett</p><p><em>Aug 7, 2014 (</em><a href="http://adatao.com/blog/big-data-2-0-has-arrived/"><em>Originally published at blog.adatao.com</em></a><em>)</em></p><p>Two years ago, we set out on a journey. The hype around Big Data was fast outpacing its tangible business value. Yet, we were at the threshold of a fundamental shift in our industry — just as Web 1.0 was about technologies and functionalities of the web, while Web 2.0 was about the focus on the user; we’re seeing the same fundamental shift from Big Data 1.0 to Big Data 2.0.</p><blockquote>We’re seeing a fundamental shift from Big Data 1.0 to Big Data 2.0</blockquote><p>Adatao is the missing puzzle piece that bridges the gap between Big Data 1.0 of the past 5 years, and Big Data 2.0 going forward, with the arrival of “Big Compute” capabilities. We had seen, first-hand, Big Compute in action on Wall Street in the early 2000’s, and then at places like Google in in mid 2000′s. We’d learned that fast, powerful Big Compute engines would enable businesses to use Big Data fluidly and interactively, at the speed of thought. But accessibility was limited due to cost. The Adatao founding team knew that without Big Compute, Big Data could never deliver on its hype.</p><blockquote>Without Big Compute, Big Data could never deliver on its hype</blockquote><p>Two years ago, we <a href="https://medium.com/adatao-narratives/why-and-how-we-bet-on-apache-spark-b6dd51151a68">confidently bet on Apache Spark</a> because we knew where the trends were heading. We set out to build Big Data applications that assumed the mass availability of Big Compute. Today, we are delivering on the promise of Big Data 2.0 with our current products:</p><ul><li><strong>pInsights — the “beauty layer” </strong>that enables business analysts and data scientists to easily and fluidly interact with Big Data in an easy to consume, interactive format. Similar to a Facebook or Google Search engine, predictive SmartQuery was built into a Google Doc type document that allows users to instantly and collaboratively produce embedded analytics within seconds to assist with decision making.</li><li><strong>pAnalytics — the “power layer” </strong>that enables data scientists and data engineers to analyze massive amounts of data in seconds. pAnalytics sifts through the data by representing it as one large, simple table, hiding all the data complexities, enabling data scientists and engineers to work with Big Data analytics in a very simple, powerful way. Data can be pulled in from Cassandra, analyzed in Spark, and the results saved back to S3 — all using one familiar API. This allows data scientists and engineers to focus on data analysis, and multiply their productivity by 10 times.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TytetFQmHmkSHbn5.png" /><figcaption>Ease-of-use + Collaboration + Predictive Capabilities = Data Intelligence</figcaption></figure><p>Since we debuted our products this past December, we’ve seen incredible demand from companies across every sector eager to finally unlock the potential of their investments in Hadoop, and reap the long promised benefits of Big Data.</p><p>As we look to build our team to scale and meet this demand, I’m excited to announce our Series A financing with our new partners at Andreessen Horowitz, who are leading the investment with participation from Lightspeed Venture Partners and Bloomberg Beta.</p><p><a href="http://peter.a16z.com/">Peter Levine</a>, from Andreessen Horowitz will join our board. Peter has extensive ground-up operating leadership experience and teaches a class on building a professional enterprise sales infrastructure at Stanford GSB and MIT Sloan School. <a href="http://blog.pmarca.com/">Marc Andreessen</a> will also support us as a Board Observer — his excitement about the potential of Adatao is summed up in the following:</p><blockquote>As a successful two-time company builder and former Engineering Director of Google Apps, Christopher has the technical company builder background that we love to bet on. Plus, he’s assembled a strong team of engineers and PhDs in parallel systems and machine learning that collectively have a unique and powerful vision.</blockquote><blockquote>But we were really sold when we saw what Adatao has built — we were blown away. Christopher and team see a future convergence of human and machine intelligence that we believe in, and they have the technology roadmap and engineering experience to get there.</blockquote><blockquote>We can’t wait to help as Adatao designs the future of Big Data.</blockquote><blockquote>- <em>Marc Andreessen</em></blockquote><p>We have found each firm to be a true partner who not only shares our vision, but is also incredibly supportive and committed to being there with us every step of the way on our challenging but exciting journey. From <a href="http://lsvp.com/team/lyon-wong/">Lyon Wong</a> and <a href="http://lsvp.com/team/john-vrionis/">John Vrionis</a> of Lightspeed, who recognized our potential in the Big Data ecosystem quite early on, to <a href="http://www.bloombergbeta.com/">James Cham</a> at Bloomberg Beta who understood our value and strength as soon as he experienced the product, we are incredibly excited about what the future will bring.</p><p>In conversation after conversation with enterprises, we’re seeing that business needs are driving the convergence of business intelligence and data science/machine learning, directly on top of big data. This convergence is creating an entirely new set of business value, at a scale that has never been seen before.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l9oS4TwWxjkd2oNb.jpg" /><figcaption>Big Data + Big Compute = Big Data Intelligence</figcaption></figure><p>We can’t wait for the journey we’re embarking on to make Data Intelligence truly available to all.</p><p><em>Originally published at </em><a href="http://blog.adatao.com/big-data-2-0-has-arrived/"><em>blog.adatao.com</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1aedb661aa68" width="1" height="1" alt=""><hr><p><a href="https://medium.com/adatao-narratives/big-data-2-0-has-arrived-1aedb661aa68">Big Data 2.0 Has Arrived</a> was originally published in <a href="https://medium.com/adatao-narratives">Arimo Narratives™</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why and How We Bet on Apache Spark]]></title>
            <link>https://medium.com/adatao-narratives/why-and-how-we-bet-on-apache-spark-b6dd51151a68?source=rss-b0cd2903e078------2</link>
            <guid isPermaLink="false">https://medium.com/p/b6dd51151a68</guid>
            <category><![CDATA[in-memory-computing]]></category>
            <category><![CDATA[apache-spark]]></category>
            <category><![CDATA[adatao-big-apps]]></category>
            <dc:creator><![CDATA[Christopher Nguyen]]></dc:creator>
            <pubDate>Tue, 17 Feb 2015 12:28:06 GMT</pubDate>
            <atom:updated>2015-05-24T10:06:39.582Z</atom:updated>
            <content:encoded><![CDATA[<p><em>The Story of Apache Spark, From Our Perspective</em></p><p><em>June 18, 2014 (</em><a href="http://adatao.com/blog/why-how-adatao-bet-on-apache-spark/"><em>Originally published at blog.adatao.com</em></a><em>)</em></p><p>In early 2012, a group of engineers with background in distributed systems and machine learning came together to form Adatao (ah-DAY-tao). We saw a major unsolved problem in the nascent Hadoop ecosystem: it was largely a storage play. Data was sitting passively on HDFS, with very little value being extracted. To be sure, there was MapReduce, Hive, Pig, etc., but value is a strong function of (a) speed of computation, (b) sophistication of logic, and (c) ease of use. While Hadoop ecosystem was being developed well at the substrate, there were enormous opportunities above it left uncaptured.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/559/1*HBeV2RyPyEpMJBP2Ic04Mw.png" /><figcaption><em>Data was sitting passively on HDFS, with little value being extracted.</em></figcaption></figure><blockquote>In-memory computing was key to the solution.</blockquote><p><strong>On speed: </strong>we had seen data move at-scale and at enormously faster rates in systems like Dremel and PowerDrill at Google. It enabled interactive behavior simply not available to Hadoop users. Without doubt, we knew that interactive speed was necessary, and that in-memory computing was key to the solution. As Cloudera’s Mike Olson has quipped, “We’re lucky to live in an age where there’s a Google. They live about 5 years in the future, and occasionally send messages back to the rest of us.” Google does indeed “live in the future”, in terms of the demands of scale and the value it is extracting from data.</p><p><strong>On sophistication: </strong>for Adatao, the essential difference between “small” and “big” data is whether data is big enough to learn from. For some questions, such as “Does it hurt to hit my head against a brick wall?”, 5 samples suffice. To classify large images, a million samples aren’t enough. We knew this was the second missing key in Big Data: aggregates and descriptives were necessary but insufficient. The Big-Data world needed the sophistication of machine learning. Big Data needed Big Compute. “Predictive” isn’t just another adjective in a long string of X-analytics; it is the quantum change, separating the value of big from small.</p><blockquote>The difference between “small” and “big” data is whether data is big enough to learn from</blockquote><p>Thus Adatao was born as a “Big Data/Machine Learning” company. Our exact product features would be driven by customer conversations, but the core thesis was clear. We wanted to bring “Data Intelligence for All”, specifically with the speed and sophistication discussed above.</p><p>If in-memory compute and machine-learning logic were the key to unlocking the value of Big Data, why hadn’t this been solved already in 2012? Because cost/benefit trade-offs matter, in any technology transition. In the chart below, the crossover points happened at different times for different endeavors; it hit critical mass on Wall Street about 2000–2005, at Google c. 2006–2010, and we project for the enterprise world at-large: about now (2013–2015).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hVqnOwK_EQdrJUDV.png" /><figcaption>Cross-over points for transitions to in-memory computing</figcaption></figure><blockquote>The future increasingly favors RAM</blockquote><p>If this isn’t clearly happening for your organization or industry yet, relax. It will, soon. Because as the latency and bandwidth trend charts below show, the future increasingly favors RAM.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8CnLxftkzgECXmZu.png" /><figcaption>The future increasingly favors a shift to RAM</figcaption></figure><p>As the Adatao team set out to build this big-compute analytic stack on Hadoop, we wanted our solution to reach all the way to the business users, while also exposing convenient APIs for data engineers and scientists. This required a combination of a great collaborative user interface, solid data-mining and machine-learning logic, backed by a powerful big-compute engine. We did a survey of the in-memory landscape, and found a small number of teams also working in the same direction. But virtually all were either too incremental or too aggressive. Some were developing work-arounds such as caching data between MR iterations, or maintaining a low-level memory cache with no persistent, high-level data structures. Others promoted yet-slow &amp; expensive “virtualized memory” architectures, still too early for prime time.</p><blockquote>The AMPLab team made the right architectural decisions for the times</blockquote><p>Then we came across Spark and the Berkeley AMPLab team. Immediately, we knew they had identified the right problem statements, and made the right architectural decisions for the times. Here are some key design choices correctly made for widespread adoption c. 2012:</p><ol><li><strong>Data model: </strong>Spark was the only architecture that supported the concept of a high-level, persistent distributed in-memory dataset. All “in-memory” systems are not equivalent. Spark’s RDDs exist independently of any given compute step, allowing for not only speedy iterative algorithms, with high-level data sets readily available to each iteration without delay. Equally importantly, they made long-running interactive memory-speed applications possible.</li><li><strong>Resiliency by recomputation</strong>: with replication being the other option, Spark made the timely choice to prefer recomputation. Memory had gotten cheaper, but not yet cheap enough for replication to be the order of the day, as it is with HDFS disks.</li><li><strong>General DAG support</strong>: while it was possible to build dedicated SQL query engines to overcome Hadoop MapReduce’s limitations (and others did choose this path), Spark’s general DAG model meant we could build arbitrary algorithms and applications on it.</li></ol><blockquote>We were seriously betting the company on Spark, promoting its goodness in every relevant conversation.</blockquote><p>We were ecstatic. Spark represented years of R&amp;D we didn’t have to spend building an engine before building sophisticated, user-facing applications. When we made the decision to support the AMPLab Spark effort, there were only 1 or 2 others that had made similar commitments. We were seriously betting the company on Spark.</p><p>But thanks to Spark, we were able to move ahead quietly and quickly on Adatao <a href="http://adatao.com/pinsights.html">pInsights</a> and <a href="http://adatao.com/panalytics.html">pAnalytics</a>, iterating on customer feedback while passing our inputs and market data along to the Spark team. We promoted Spark’s goodness in every relevant conversation. By late summer 2013, <a href="http://www.databricks.com/">Databricks</a> was about to be born, further increasing our confidence on the Spark-on-Hadoop ecosystem. There was now going to be an official, commercial entity with an existence predicated on developing the growth of the ecosystem and maintaining its health. And the team at Databricks is doing an excellent job at that stewardship.</p><blockquote>Apache Spark will have a bright future</blockquote><p>Today, Adatao is one of the first applications to be <a href="http://databricks.com/certification"><em>Certified on Spark</em></a><em>. </em>We’re seeing remarkable enterprise adoption speeds for Adatao-on-Spark. The most sophisticated customers tend to be companies that have already deployed Hadoop, who are all too familiar with the failed promises of Big Data. We see immediate excitement in customers the moment they see the Adatao solution: a user-facing analytics application that is interactive, easy-to-use, supports both basic analytics and machine learning, and is actually running in seconds of real time over large Hadoop datasets. Finally, users are truly able to extract data intelligence from data storage. Value creation is no longer just about Big Data. It’s about Big Compute, and Spark has delivered that capability for us.</p><p>Spark has made it as a top-level Apache project, going from incubation to graduation in record time. It is also one of Apache’s most active projects with hundreds of contributors. This is because of its superior architecture and timeliness of engineering choices, as discussed above. With that plus appropriate care and feeding, Apache Spark will have a bright future even as it evolves and adapts to changing technology and business drivers.</p><p><em>Originally published at </em><a href="http://blog.adatao.com/adatao-why-and-how-we-decided-on-apache-spark/"><em>blog.adatao.com</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b6dd51151a68" width="1" height="1" alt=""><hr><p><a href="https://medium.com/adatao-narratives/why-and-how-we-bet-on-apache-spark-b6dd51151a68">Why and How We Bet on Apache Spark</a> was originally published in <a href="https://medium.com/adatao-narratives">Arimo Narratives™</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>