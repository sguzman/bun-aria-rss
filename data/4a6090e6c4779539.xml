<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.DS updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Data Structures and Algorithms</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.06898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.02161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.13606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12859" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01468">
<title>A New Approach to Estimating Effective Resistances and Counting Spanning Trees in Expander Graphs. (arXiv:2211.01468v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01468</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate that for expander graphs, for all $\epsilon &amp;gt; 0,$ there exists
a data structure of size $\widetilde{O}(n\epsilon^{-1})$ which can be used to
return $(1 + \epsilon)$-approximations to effective resistances in
$\widetilde{O}(1)$ time per query. Short of storing all effective resistances,
previous best approaches could achieve $\widetilde{O}(n\epsilon^{-2})$ size and
$\widetilde{O}(\epsilon^{-2})$ time per query by storing Johnson-Lindenstrauss
vectors for each vertex, or $\widetilde{O}(n\epsilon^{-1})$ size and
$\widetilde{O}(n\epsilon^{-1})$ time per query by storing a spectral sketch.
&lt;/p&gt;
&lt;p&gt;Our construction is based on two key ideas: 1) $\epsilon^{-1}$-sparse,
$\epsilon$-additive approximations to $DL^+1_u$ for all $u,$ can be used to
recover $(1 + \epsilon)$-approximations to the effective resistances, 2) In
expander graphs, only $\widetilde{O}(\epsilon^{-1})$ coordinates of a vector
similar to $DL^+1_u$ are larger than $\epsilon.$ We give an efficient
construction for such a data structure in $\widetilde{O}(m + n\epsilon^{-2})$
time via random walks. This results in an algorithm for computing
$(1+\epsilon)$-approximate effective resistances for $s$ vertex pairs in
expanders that runs in $\widetilde{O}(m + n\epsilon^{-2} + s)$ time, improving
over the previously best known running time of $m^{1 + o(1)} + (n +
s)n^{o(1)}\epsilon^{-1.5}$ for $s = \omega(n\epsilon^{-0.5}).$
&lt;/p&gt;
&lt;p&gt;We employ the above algorithm to compute a $(1+\delta)$-approximation to the
number of spanning trees in an expander graph, or equivalently, approximating
the (pseudo)determinant of its Laplacian in $\widetilde{O}(m +
n^{1.5}\delta^{-1})$ time. This improves on the previously best known result of
$m^{1+o(1)} + n^{1.875+o(1)}\delta^{-1.75}$ time, and matches the best known
size of determinant sparsifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lawrence Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1&quot;&gt;Sushant Sachdeva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01612">
<title>Computing a many-to-many matching with demands and capacities between two sets using the Hungarian algorithm. (arXiv:2211.01612v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01612</link>
<description rdf:parseType="Literal">&lt;p&gt;Given two sets A={a_1,a_2,...,a_s} and {b_1,b_2,...,b_t}, a many-to-many
matching with demands and capacities (MMDC) between A and B matches each
element a_i in A to at least \alpha_i and at most \alpha&apos;_i elements in B, and
each element b_j in B to at least \beta_j and at most \beta&apos;_j elements in A
for all 1=&amp;lt;i&amp;lt;=s and 1=&amp;lt;j&amp;lt;=t. In this paper, we present an algorithm for finding
a minimum-cost MMDC between A and B using the well-known Hungarian algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_Alni_F/0/1/0/all/0/1&quot;&gt;Fatemeh Rajabi-Alni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1&quot;&gt;Alireza Bagheri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01661">
<title>Pairing optimization via statistics: Algebraic structure in pairing problems and its application to performance enhancement. (arXiv:2211.01661v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01661</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully pairing all elements of a set while attempting to maximize the total
benefit is a combinatorically difficult problem. Such pairing problems
naturally appear in various situations in science, technology, economics, and
other fields. In our previous study, we proposed an efficient method to infer
the underlying compatibilities among the entities, under the constraint that
only the total compatibility is observable. Furthermore, by transforming the
pairing problem into a traveling salesman problem with a multi-layer
architecture, a pairing optimization algorithm was successfully demonstrated to
derive a high-total-compatibility pairing. However, there is substantial room
for further performance enhancement by further exploiting the underlying
mathematical properties. In this study, we prove the existence of algebraic
structures in the pairing problem. We transform the initially estimated
compatibility information into an equivalent form where the variance of the
individual compatibilities is minimized. We then demonstrate that the total
compatibility obtained when using the heuristic pairing algorithm on the
transformed problem is significantly higher compared to the previous method.
With this improved perspective on the pairing problem using fundamental
mathematical properties, we can contribute to practical applications such as
wireless communications beyond 5G, where efficient pairing is of critical
importance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujita_N/0/1/0/all/0/1&quot;&gt;Naoki Fujita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihana_T/0/1/0/all/0/1&quot;&gt;Takatomo Mihana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horisaki_R/0/1/0/all/0/1&quot;&gt;Ryoichi Horisaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1&quot;&gt;Mikio Hasegawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01699">
<title>A Round and Bipartize Approximation Algorithm for Vertex Cover. (arXiv:2211.01699v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01699</link>
<description rdf:parseType="Literal">&lt;p&gt;The vertex cover problem is a fundamental and widely studied combinatorial
optimization problem. It is known that its standard linear programming
relaxation is integral for bipartite graphs and half-integral for general
graphs. As a consequence, the natural rounding algorithm based on this
relaxation computes an optimal solution for bipartite graphs and a
$2$-approximation for general graphs. This raises the question of whether one
can obtain improved bounds on the approximation ratio, depending on how close
the graph is to being bipartite.
&lt;/p&gt;
&lt;p&gt;In this paper, we consider a round-and-bipartize algorithm that exploits the
knowledge of an induced bipartite subgraph to attain improved approximation
ratios. Equivalently, we suppose that we have access to a subset of vertices
$S$ whose removal bipartizes the graph.
&lt;/p&gt;
&lt;p&gt;If $S$ is an independent set, we prove an approximation ratio of $1 +
1/\rho$, where $2\rho -1$ denotes the odd girth of the contracted graph
$\tilde{\mathcal{G}} := \mathcal{G} /S$ and thus satisfies $\rho \geq 2$. We
show that this is tight for any graph and independent set by providing a family
of weight functions for which this bound is attained. In addition, we give
tight upper bounds for the fractional chromatic number and the integrality gap
of such graphs, both of which also depend on the odd girth.
&lt;/p&gt;
&lt;p&gt;If $S$ is an arbitrary set, we prove a tight approximation ratio of
$\left(1+1/\rho \right) (1 - \alpha) + 2 \alpha$, where $\alpha \in [0,1]$
denotes the total normalized dual sum of the edges lying inside of the set $S$.
As an algorithmic application, we show that for any efficiently $k$-colorable
graph with $k \geq 4$ we can find a bipartizing set satisfying $\alpha \leq 1 -
4/k$. This provides an approximation algorithm recovering the bound of $2 -
2/k$ in the worst case (i.e., when $\rho = 2$), which is best possible for this
setting when using the standard relaxation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashaev_D/0/1/0/all/0/1&quot;&gt;Danish Kashaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_G/0/1/0/all/0/1&quot;&gt;Guido Sch&amp;#xe4;fer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01701">
<title>Efficient Branch-and-Bound Algorithms for Finding Triangle-Constrained 2-Clubs. (arXiv:2211.01701v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01701</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Vertex Triangle 2-Club problem, we are given an undirected graph $G$
and aim to find a maximum-vertex subgraph of $G$ that has diameter at most 2
and in which every vertex is contained in at least $\ell$ triangles in the
subgraph. So far, the only algorithm for solving Vertex Triangle 2-Club relies
on an ILP formulation [Almeida and Br\&apos;as, Comput. Oper. Res. 2019]. In this
work, we develop a combinatorial branch-and-bound algorithm that, coupled with
a set of data reduction rules, outperforms the existing implementation and is
able to find optimal solutions on sparse real-world graphs with more than
100,000 vertices in a few minutes. We also extend our algorithm to the Edge
Triangle 2-Club problem where the triangle constraint is imposed on all edges
of the subgraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1&quot;&gt;Niels Gr&amp;#xfc;ttemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1&quot;&gt;Christian Komusiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kessler_P/0/1/0/all/0/1&quot;&gt;Philipp Heinrich Ke&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1&quot;&gt;Frank Sommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01723">
<title>Model-Checking for First-Order Logic with Disjoint Paths Predicates in Proper Minor-Closed Graph Classes. (arXiv:2211.01723v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2211.01723</link>
<description rdf:parseType="Literal">&lt;p&gt;The disjoint paths logic, FOL+DP, is an extension of First-Order Logic (FOL)
with the extra atomic predicate ${\sf dp}_k(x_1,y_1,\ldots,x_k,y_k),$
expressing the existence of internally vertex-disjoint paths between $x_i$ and
$y_i,$ for $i\in\{1,\ldots, k\}$. This logic can express a wide variety of
problems that escape the expressibility potential of FOL. We prove that for
every proper minor-closed graph class, model-checking for FOL+DP can be done in
quadratic time. We also introduce an extension of FOL+DP, namely the scattered
disjoint paths logic, FOL+SDP, where we further consider the atomic predicate
$s{\sf -sdp}_k(x_1,y_1,\ldots,x_k,y_k),$ demanding that the disjoint paths are
within distance bigger than some fixed value $s$. Using the same technique we
prove that model-checking for FOL+SDP can be done in quadratic time on classes
of graphs with bounded Euler genus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golovach_P/0/1/0/all/0/1&quot;&gt;Petr A. Golovach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_G/0/1/0/all/0/1&quot;&gt;Giannos Stamoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thilikos_D/0/1/0/all/0/1&quot;&gt;Dimitrios M. Thilikos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01725">
<title>Distributed Reconfiguration of Spanning Trees. (arXiv:2211.01725v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01725</link>
<description rdf:parseType="Literal">&lt;p&gt;In a reconfiguration problem, given a problem and two feasible solutions of
the problem, the task is to find a sequence of transformations to reach from
one solution to the other such that every intermediate state is also a feasible
solution to the problem. In this paper, we study the distributed spanning tree
reconfiguration problem and we define a new reconfiguration step, called
$k$-simultaneous add and delete, in which every node is allowed to add at most
$k$ edges and delete at most $k$ edges such that multiple nodes do not add or
delete the same edge.
&lt;/p&gt;
&lt;p&gt;We first observe that, if the two input spanning trees are rooted, then we
can do the reconfiguration using a single $1$-simultaneous add and delete step
in one round in the CONGEST model. Therefore, we focus our attention towards
unrooted spanning trees and show that transforming an unrooted spanning tree
into another using a single $1$-simultaneous add and delete step requires
$\Omega(n)$ rounds in the LOCAL model. We additionally show that transforming
an unrooted spanning tree into another using a single $2$-simultaneous add and
delete step can be done in $O(\log n)$ rounds in the CONGEST model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Siddharth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1&quot;&gt;Shreyas Pai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01726">
<title>SQUID: Faster Analytics via Sampled Quantiles Data-structure. (arXiv:2211.01726v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01726</link>
<description rdf:parseType="Literal">&lt;p&gt;Measurement is a fundamental enabler of network applications such as load
balancing, attack detection and mitigation, and traffic engineering. A key
building block in many critical measurement tasks is \emph{q-MAX}, where we
wish to find the largest $q$ values in a number stream. A standard approach of
maintaining a heap of the largest $q$ values ordered results in logarithmic
runtime, which is too slow for large measurements. Modern approaches attain a
constant runtime by removing small items in bulk and retaining the largest $q$
items at all times. Yet, these approaches are bottlenecked by an expensive
quantile calculation method.
&lt;/p&gt;
&lt;p&gt;We propose SQUID, a method that redesigns q-MAX to allow the use of
\emph{approximate quantiles}, which we can compute efficiently, thereby
accelerating the solution and, subsequently, many measurement tasks. We
demonstrate the benefit of our approach by designing a novel weighted heavy
hitters data structure that is faster and more accurate than the existing
alternatives. Here, we combine our previous techniques with a lazy deletion of
small entries, which expiates the maintenance process and increases the
accuracy. We also demonstrate the applicability of our algorithmic approach in
a general algorithmic scope by implementing the LRFU cache policy with a
constant update time. Furthermore, we also show the practicality of SQUID for
improving real-world networked systems, by implementing a P4 prototype of SQUID
for in-network caching and demonstrating how SQUID enables a wide spectrum of
score-based caching policies directly on a P4 switch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Basat_R/0/1/0/all/0/1&quot;&gt;Ran Ben-Basat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einziger_G/0/1/0/all/0/1&quot;&gt;Gil Einziger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenchen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayh_B/0/1/0/all/0/1&quot;&gt;Bilal Tayh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01776">
<title>Complexity of Simon&apos;s problem in classical sense. (arXiv:2211.01776v1 [cs.CC])</title>
<link>http://arxiv.org/abs/2211.01776</link>
<description rdf:parseType="Literal">&lt;p&gt;Simon&apos;s problem is a standard example of a problem that is exponential in
classical sense, while it admits a polynomial solution in quantum computing. It
is about a function $f$ for which it is given that a unique non-zero vector $s$
exists for which $f(x) = f(x \oplus s)$ for all $x$, where $\oplus$ is the
exclusive or operator. The goal is to find $s$. The exponential lower bound for
the classical sense assumes that $f$ only admits black box access. In this
paper we investigate classical complexity when $f$ is given by a standard
representation like a circuit. We focus on finding the vector space of all
vectors $s$ for which $f(x) = f(x \oplus s)$ for all $x$, for any given $f$.
Two main results are: (1) if $f$ is given by any circuit, then checking whether
this vector space contains a non-zero element is NP-hard, and (2) if $f$ is
given by any ordered BDD, then a basis of this vector space can be computed in
polynomial time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zantema_H/0/1/0/all/0/1&quot;&gt;Hans Zantema&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01912">
<title>Matching Augmentation via Simultaneous Contractions. (arXiv:2211.01912v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01912</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the matching augmentation problem (MAP), where a matching of a
graph needs to be extended into a $2$-edge-connected spanning subgraph by
adding the minimum number of edges to it. We present a polynomial-time
algorithm with an approximation ratio of $13/8 = 1.625$ improving upon an
earlier $5/3$-approximation. The improvement builds on a new
$\alpha$-approximation preserving reduction for any $\alpha\geq 3/2$ from
arbitrary MAP instances to well-structured instances that do not contain
certain forbidden structures like parallel edges, small separators, and
contractible subgraphs. We further introduce, as key ingredients, the technique
of repeated simultaneous contractions and provide improved lower bounds for
instances that cannot be contracted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1&quot;&gt;Mohit Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hommelsheim_F/0/1/0/all/0/1&quot;&gt;Felix Hommelsheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megow_N/0/1/0/all/0/1&quot;&gt;Nicole Megow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01945">
<title>Distributed Maximal Matching and Maximal Independent Set on Hypergraphs. (arXiv:2211.01945v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01945</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the distributed complexity of maximal matching and maximal
independent set (MIS) in hypergraphs in the LOCAL model. A maximal matching of
a hypergraph $H=(V_H,E_H)$ is a maximal disjoint set $M\subseteq E_H$ of
hyperedges and an MIS $S\subseteq V_H$ is a maximal set of nodes such that no
hyperedge is fully contained in $S$. Both problems can be solved by a simple
sequential greedy algorithm, which can be implemented naively in $O(\Delta r +
\log^* n)$ rounds, where $\Delta$ is the maximum degree, $r$ is the rank, and
$n$ is the number of nodes.
&lt;/p&gt;
&lt;p&gt;We show that for maximal matching, this naive algorithm is optimal in the
following sense. Any deterministic algorithm for solving the problem requires
$\Omega(\min\{\Delta r, \log_{\Delta r} n\})$ rounds, and any randomized one
requires $\Omega(\min\{\Delta r, \log_{\Delta r} \log n\})$ rounds. Hence, for
any algorithm with a complexity of the form $O(f(\Delta, r) + g(n))$, we have
$f(\Delta, r) \in \Omega(\Delta r)$ if $g(n)$ is not too large, and in
particular if $g(n) = \log^* n$ (which is the optimal asymptotic dependency on
$n$ due to Linial&apos;s lower bound [FOCS&apos;87]). Our lower bound proof is based on
the round elimination framework, and its structure is inspired by a new round
elimination fixed point that we give for the $\Delta$-vertex coloring problem
in hypergraphs.
&lt;/p&gt;
&lt;p&gt;For the MIS problem on hypergraphs, we show that for $\Delta\ll r$, there are
significant improvements over the naive $O(\Delta r + \log^* n)$-round
algorithm. We give two deterministic algorithms for the problem. We show that a
hypergraph MIS can be computed in $O(\Delta^2\cdot\log r + \Delta\cdot\log
r\cdot \log^* r + \log^* n)$ rounds. We further show that at the cost of a
worse dependency on $\Delta$, the dependency on $r$ can be removed almost
entirely, by giving an algorithm with complexity $\Delta^{O(\Delta)}\cdot\log^*
r + O(\log^* n)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balliu_A/0/1/0/all/0/1&quot;&gt;Alkida Balliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1&quot;&gt;Sebastian Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_F/0/1/0/all/0/1&quot;&gt;Fabian Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivetti_D/0/1/0/all/0/1&quot;&gt;Dennis Olivetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02004">
<title>Truthful Matching with Online Items and Offline Agents. (arXiv:2211.02004v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2211.02004</link>
<description rdf:parseType="Literal">&lt;p&gt;We study truthful mechanisms for welfare maximization in online bipartite
matching. In our (multi-parameter) setting, every buyer is associated with a
(possibly private) desired set of items, and has a private value for being
assigned an item in her desired set. Unlike most online matching settings,
where agents arrive online, in our setting the items arrive online in an
adversarial order while the buyers are present for the entire duration of the
process. This poses a significant challenge to the design of truthful
mechanisms, due to the ability of buyers to strategize over future rounds. We
provide an almost full picture of the competitive ratios in different
scenarios, including myopic vs. non-myopic agents, tardy vs. prompt payments,
and private vs. public desired sets. Among other results, we identify the
frontier for which the celebrated $e/(e-1)$ competitive ratio for the
vertex-weighted online matching of Karp, Vazirani and Vazirani extends to
truthful agents and online items.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1&quot;&gt;Michal Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1&quot;&gt;Stefano Leonardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauras_S/0/1/0/all/0/1&quot;&gt;Simon Mauras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiffenhauser_R/0/1/0/all/0/1&quot;&gt;Rebecca Reiffenh&amp;#xe4;user&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02044">
<title>Competitive Kill-and-Restart Strategies for Non-Clairvoyant Scheduling. (arXiv:2211.02044v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.02044</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the fundamental scheduling problem of minimizing the sum of
weighted completion times on a single machine in the non-clairvoyant setting.
While no non-preemptive algorithm is constant competitive, Motwani, Phillips,
and Torng (SODA &apos;93) proved that the simple preemptive round robin procedure is
$2$-competitive and that no better competitive ratio is possible, initiating a
long line of research focused on preemptive algorithms for generalized variants
of the problem. As an alternative model, Shmoys, Wein, and Williamson (FOCS
&apos;91) introduced kill-and-restart schedules, where running jobs may be killed
and restarted from scratch later, and analyzed then for the makespan objective.
However, to the best of our knowledge, this concept has never been considered
for the total completion time objective in the non-clairvoyant model.
&lt;/p&gt;
&lt;p&gt;We contribute to both models: First we give for any $b &amp;gt; 1$ a tight analysis
for the natural $b$-scaling kill-and-restart strategy for scheduling jobs
without release dates, as well as for a randomized variant of it. This implies
a performance guarantee of $(1+3\sqrt{3})\approx 6.197$ for the deterministic
algorithm and of $\approx 3.032$ for the randomized version. Second, we show
that the preemptive Weighted Shortest Elapsed Time First (WSETF) rule is
$2$-competitive for jobs released in an online fashion over time, matching the
lower bound by Motwani et al. Using this result as well as the competitiveness
of round robin for multiple machines, we prove performance guarantees of
adaptions of the $b$-scaling algorithm to online release dates and unweighted
jobs on identical parallel machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_S/0/1/0/all/0/1&quot;&gt;Sven J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagnol_G/0/1/0/all/0/1&quot;&gt;Guillaume Sagnol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waldschmidt_D/0/1/0/all/0/1&quot;&gt;Daniel Schmidt genannt Waldschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warode_P/0/1/0/all/0/1&quot;&gt;Philipp Warode&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.06898">
<title>The Complexity of Matching Games: A Survey. (arXiv:2202.06898v4 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2202.06898</link>
<description rdf:parseType="Literal">&lt;p&gt;Matching games naturally generalize assignment games, a well-known class of
cooperative games. Interest in matching games has grown recently due to some
breakthrough results and new applications. This state-of-the-art survey
provides an overview of matching games and extensions, such as $b$-matching
games and partitioned matching games; the latter originating from the emerging
area of international kidney exchange. In this survey we focus on computational
complexity aspects of various game-theoretical solution concepts, such as the
core, nucleolus and Shapley value, when the input is restricted to some
(generalized) matching game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedek_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rton Benedek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biro_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe9;ter Bir&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulusma_D/0/1/0/all/0/1&quot;&gt;Dani&amp;#xeb;l Paulusma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xin Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.02161">
<title>Is the Algorithmic Kadison-Singer Problem Hard?. (arXiv:2205.02161v3 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/2205.02161</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the following $\mathsf{KS}_2(c)$ problem: let $c \in\mathbb{R}^+$ be
some constant, and $v_1,\ldots, v_m\in\mathbb{R}^d$ be vectors such that
$\|v_i\|^2\leq \alpha$ for any $i\in[m]$ and $\sum_{i=1}^m \langle v_i,
x\rangle^2 =1$ for any $x\in\mathbb{R}^d$ with $\|x\|=1$. The
$\mathsf{KS}_2(c)$ problem asks to find some $S\subset [m]$, such that it holds
for all $x \in \mathbb{R}^d$ with $\|x\| = 1$ that \[ \left|\sum_{i \in S}
\langle v_i, x\rangle^2 - \frac{1}{2}\right| \leq c\cdot\sqrt{\alpha},\] or
report no if such $S$ doesn&apos;t exist. Based on the work of Marcus et al. and
Weaver, the $\mathsf{KS}_2(c)$ problem can be seen as the algorithmic
Kadison-Singer problem with parameter $c\in\mathbb{R}^+$.
&lt;/p&gt;
&lt;p&gt;Our first result is a randomised algorithm with one-sided error for the
$\mathsf{KS}_2(c)$ problem such that (1) our algorithm finds a valid set $S
\subset [m]$ with probability at least $1-2/d$, if such $S$ exists, or (2)
reports no with probability $1$, if no valid sets exist. The algorithm has
running time \[ O\left(\binom{m}{n}\cdot \mathrm{poly}(m, d)\right)~\mbox{ for
}~n = O\left(\frac{d}{\epsilon^2} \log(d)
\log\left(\frac{1}{c\sqrt{\alpha}}\right)\right), \] where $\epsilon$ is a
parameter which controls the error of the algorithm. This presents the first
algorithm for the Kadison-Singer problem whose running time is quasi-polynomial
in $m$, although having exponential dependency on $d$. Moreover, it shows that
the algorithmic Kadison-Singer problem is easier to solve in low dimensions.
&lt;/p&gt;
&lt;p&gt;Our second result is on the computational complexity of the
$\mathsf{KS}_2(c)$ problem. We show that the $\mathsf{KS}_2(1/(4\sqrt{2}))$
problem is $\mathsf{FNP}$-hard for general values of $d$, and solving the
$\mathsf{KS}_2(1/(4\sqrt{2}))$ problem is as hard as solving the
$\mathsf{NAE\mbox{-}3SAT}$ problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jourdan_B/0/1/0/all/0/1&quot;&gt;Ben Jourdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macgregor_P/0/1/0/all/0/1&quot;&gt;Peter Macgregor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;He Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.13606">
<title>Online Resource Allocation under Horizon Uncertainty. (arXiv:2206.13606v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2206.13606</link>
<description rdf:parseType="Literal">&lt;p&gt;We study stochastic online resource allocation: a decision maker needs to
allocate limited resources to stochastically-generated sequentially-arriving
requests in order to maximize reward. At each time step, requests are drawn
independently from a distribution that is unknown to the decision maker. Online
resource allocation and its special cases have been studied extensively in the
past, but prior results crucially and universally rely on the strong assumption
that the total number of requests (the horizon) is known to the decision maker
in advance. In many applications, such as revenue management and online
advertising, the number of requests can vary widely because of fluctuations in
demand or user traffic intensity. In this work, we develop online algorithms
that are robust to horizon uncertainty. In sharp contrast to the known-horizon
setting, no algorithm can achieve even a constant asymptotic competitive ratio
that is independent of the horizon uncertainty. We introduce a novel
generalization of dual mirror descent which allows the decision maker to
specify a schedule of time-varying target consumption rates, and prove
corresponding performance guarantees. We go on to give a fast algorithm for
computing a schedule of target consumption rates that leads to near-optimal
performance in the unknown-horizon setting. In particular, our competitive
ratio attains the optimal rate of growth (up to logarithmic factors) as the
horizon uncertainty grows large. Finally, we also provide a way to incorporate
machine-learned predictions about the horizon which interpolates between the
known and unknown horizon settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balseiro_S/0/1/0/all/0/1&quot;&gt;Santiago Balseiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1&quot;&gt;Christian Kroer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Rachitesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12859">
<title>A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees. (arXiv:2210.12859v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12859</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an algorithm that allows for find-closest-point and kNN-style
traversals of left-balanced k-d trees, without the need for either recursion or
software-managed stacks; instead using only current and last previously
traversed node to compute which node to traverse next.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_I/0/1/0/all/0/1&quot;&gt;Ingo Wald&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>