<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>eess.IV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Electrical Engineering and Systems Science -- Image and Video Processing (eess.IV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Electrical Engineering and Systems Science -- Image and Video Processing</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.05470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.10611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.02163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.05876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01111" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01371">
<title>Unsupervised Anomaly Detection of Paranasal Anomalies in the Maxillary Sinus. (arXiv:2211.01371v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01371</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) algorithms can be used to automate paranasal anomaly
detection from Magnetic Resonance Imaging (MRI). However, previous works relied
on supervised learning techniques to distinguish between normal and abnormal
samples. This method limits the type of anomalies that can be classified as the
anomalies need to be present in the training data. Further, many data points
from normal and anomaly class are needed for the model to achieve satisfactory
classification performance. However, experienced clinicians can segregate
between normal samples (healthy maxillary sinus) and anomalous samples
(anomalous maxillary sinus) after looking at a few normal samples. We mimic the
clinicians ability by learning the distribution of healthy maxillary sinuses
using a 3D convolutional auto-encoder (cAE) and its variant, a 3D variational
autoencoder (VAE) architecture and evaluate cAE and VAE for this task.
Concretely, we pose the paranasal anomaly detection as an unsupervised anomaly
detection problem. Thereby, we are able to reduce the labelling effort of the
clinicians as we only use healthy samples during training. Additionally, we can
classify any type of anomaly that differs from the training distribution. We
train our 3D cAE and VAE to learn a latent representation of healthy maxillary
sinus volumes using L1 reconstruction loss. During inference, we use the
reconstruction error to classify between normal and anomalous maxillary
sinuses. We extract sub-volumes from larger head and neck MRIs and analyse the
effect of different fields of view on the detection performance. Finally, we
report which anomalies are easiest and hardest to classify using our approach.
Our results demonstrate the feasibility of unsupervised detection of paranasal
anomalies from MRIs with an AUPRC of 85% and 80% for cAE and VAE, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debayan Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Behrendt_F/0/1/0/all/0/1&quot;&gt;Finn Behrendt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Becker_B/0/1/0/all/0/1&quot;&gt;Benjamin Tobias Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beyersdorff_D/0/1/0/all/0/1&quot;&gt;Dirk Beyersdorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1&quot;&gt;Elina Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petersen_M/0/1/0/all/0/1&quot;&gt;Marvin Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_B/0/1/0/all/0/1&quot;&gt;Bastian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eggert_D/0/1/0/all/0/1&quot;&gt;Dennis Eggert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Betz_C/0/1/0/all/0/1&quot;&gt;Christian Betz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoffmann_A/0/1/0/all/0/1&quot;&gt;Anna Sophie Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1&quot;&gt;Alexander Schlaefer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01372">
<title>Investigating the robustness of a learning-based method for quantitative phase retrieval from propagation-based x-ray phase contrast measurements under laboratory conditions. (arXiv:2211.01372v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2211.01372</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative phase retrieval (QPR) in propagation-based x-ray phase contrast
imaging of heterogeneous and structurally complicated objects is challenging
under laboratory conditions due to partial spatial coherence and
polychromaticity. A learning-based method (LBM) provides a non-linear approach
to this problem while not being constrained by restrictive assumptions about
object properties and beam coherence. In this work, a LBM was assessed for its
applicability under practical scenarios by evaluating its robustness and
generalizability under typical experimental variations. Towards this end, an
end-to-end LBM was employed for QPR under laboratory conditions and its
robustness was investigated across various system and object conditions. The
robustness of the method was tested via varying propagation distances and its
generalizability with respect to object structure and experimental data was
also tested. Although the LBM was stable under the studied variations, its
successful deployment was found to be affected by choices pertaining to data
pre-processing, network training considerations and system modeling. To our
knowledge, we demonstrated for the first time, the potential applicability of
an end-to-end learning-based quantitative phase retrieval method, trained on
simulated data, to experimental propagation-based x-ray phase contrast
measurements acquired under laboratory conditions. We considered conditions of
polychromaticity, partial spatial coherence, and high noise levels, typical to
laboratory conditions. This work further explored the robustness of this method
to practical variations in propagation distances and object structure with the
goal of assessing its potential for experimental use. Such an exploration of
any LBM (irrespective of its network architecture) before practical deployment
provides an understanding of its potential behavior under experimental
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Deshpande_R/0/1/0/all/0/1&quot;&gt;Rucha Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Avachat_A/0/1/0/all/0/1&quot;&gt;Ashish Avachat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Brooks_F/0/1/0/all/0/1&quot;&gt;Frank J. Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Anastasio_M/0/1/0/all/0/1&quot;&gt;Mark A. Anastasio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01373">
<title>Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators. (arXiv:2211.01373v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01373</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior knowledge about the imaging physics provides a mechanistic forward
operator that plays an important role in image reconstruction, although myriad
sources of possible errors in the operator could negatively impact the
reconstruction solutions. In this work, we propose to embed the traditional
mechanistic forward operator inside a neural function, and focus on modeling
and correcting its unknown errors in an interpretable manner. This is achieved
by a conditional generative model that transforms a given mechanistic operator
with unknown errors, arising from a latent space of self-organizing clusters of
potential sources of error generation. Once learned, the generative model can
be used in place of a fixed forward operator in any traditional
optimization-based reconstruction process where, together with the inverse
solution, the error in prior mechanistic forward operator can be minimized and
the potential source of error uncovered. We apply the presented method to the
reconstruction of heart electrical potential from body surface potential. In
controlled simulation experiments and in-vivo real data experiments, we
demonstrate that the presented method allowed reduction of errors in the
physics-based forward operator and thereby delivered inverse reconstruction of
heart-surface potential with increased accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toloubidokhti_M/0/1/0/all/0/1&quot;&gt;Maryam Toloubidokhti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Nilesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gyawali_P/0/1/0/all/0/1&quot;&gt;Prashnna K. Gyawali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zenger_B/0/1/0/all/0/1&quot;&gt;Brian Zenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Good_W/0/1/0/all/0/1&quot;&gt;Wilson W. Good&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+MacLeod_R/0/1/0/all/0/1&quot;&gt;Rob S. MacLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01374">
<title>End-to-end deep multi-score model for No-reference stereoscopic image quality assessment. (arXiv:2211.01374v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01374</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based quality metrics have recently given significant
improvement in Image Quality Assessment (IQA). In the field of stereoscopic
vision, information is evenly distributed with slight disparity to the left and
right eyes. However, due to asymmetric distortion, the objective quality
ratings for the left and right images would differ, necessitating the learning
of unique quality indicators for each view. Unlike existing stereoscopic IQA
measures which focus mainly on estimating a global human score, we suggest
incorporating left, right, and stereoscopic objective scores to extract the
corresponding properties of each view, and so forth estimating stereoscopic
image quality without reference. Therefore, we use a deep multi-score
Convolutional Neural Network (CNN). Our model has been trained to perform four
tasks: First, predict the left view&apos;s quality. Second, predict the quality of
the left view. Third and fourth, predict the quality of the stereo view and
global quality, respectively, with the global score serving as the ultimate
quality. Experiments are conducted on Waterloo IVC 3D Phase 1 and Phase 2
databases. The results obtained show the superiority of our method when
comparing with those of the state-of-the-art. The implementation code can be
found at: https://github.com/o-messai/multi-score-SIQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Messai_O/0/1/0/all/0/1&quot;&gt;Oussama Messai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chetouani_A/0/1/0/all/0/1&quot;&gt;Aladine Chetouani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01375">
<title>High-speed processing of X-ray wavefront marking data with the Unified Modulated Pattern Analysis (UMPA) model. (arXiv:2211.01375v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2211.01375</link>
<description rdf:parseType="Literal">&lt;p&gt;Wavefront-marking X-ray imaging techniques use e.g., sandpaper or a grating
to generate intensity fluctuations, and analyze their distortion by the sample
in order to retrieve attenuation, phase-contrast, and dark-field information.
Phase contrast yields an improved visibility of soft-tissue specimens, while
dark-field reveals small-angle scatter from sub-resolution structures. Both
have found many biomedical and engineering applications. The previously
developed Unified Modulated Pattern Analysis (UMPA) model extracts these
modalities from wavefront-marking data. We here present a new UMPA
implementation, capable of rapidly processing large datasets and featuring
capabilities to greatly extend the field of view. We also discuss possible
artifacts and additional new features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Marco_F/0/1/0/all/0/1&quot;&gt;Fabio De Marco&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Savatovic_S/0/1/0/all/0/1&quot;&gt;Sara Savatovi&amp;#x107;&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Smith_R/0/1/0/all/0/1&quot;&gt;Ronan Smith&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Trapani_V/0/1/0/all/0/1&quot;&gt;Vittorio Di Trapani&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Margini_M/0/1/0/all/0/1&quot;&gt;Marco Margini&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lautizi_G/0/1/0/all/0/1&quot;&gt;Ginevra Lautizi&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thibault_P/0/1/0/all/0/1&quot;&gt;Pierre Thibault&lt;/a&gt; (1,2) ((1) Department of Physics, University of Trieste, (2) Elettra-Sincrotrone Trieste, (3) Department of Physics, University of Southampton)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01472">
<title>The Need for Medically Aware Video Compression in Gastroenterology. (arXiv:2211.01472v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01472</link>
<description rdf:parseType="Literal">&lt;p&gt;Compression is essential to storing and transmitting medical videos, but the
effect of compression on downstream medical tasks is often ignored.
Furthermore, systems in practice rely on standard video codecs, which naively
allocate bits between medically relevant frames or parts of frames. In this
work, we present an empirical study of some deficiencies of classical codecs on
gastroenterology videos, and motivate our ongoing work to train a learned
compression model for colonoscopy videos. We show that two of the most common
classical codecs, H264 and HEVC, compress medically relevant frames
statistically significantly worse than medically nonrelevant ones, and that
polyp detector performance degrades rapidly as compression increases. We
explain how a learned compressor could allocate bits to important regions and
allow detection performance to degrade more gracefully. Many of our proposed
techniques generalize to medical video domains beyond gastroenterology
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1&quot;&gt;Nick Johnston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01607">
<title>ImageCAS: A Large-Scale Dataset and Benchmark for Coronary Artery Segmentation based on Computed Tomography Angiography Images. (arXiv:2211.01607v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01607</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiovascular disease (CVD) accounts for about half of non-communicable
diseases. Vessel stenosis in the coronary artery is considered to be the major
risk of CVD. Computed tomography angiography (CTA) is one of the widely used
noninvasive imaging modalities in coronary artery diagnosis due to its superior
image resolution. Clinically, segmentation of coronary arteries is essential
for the diagnosis and quantification of coronary artery disease. Recently, a
variety of works have been proposed to address this problem. However, on one
hand, most works rely on in-house datasets, and only a few works published
their datasets to the public which only contain tens of images. On the other
hand, their source code have not been published, and most follow-up works have
not made comparison with existing works, which makes it difficult to judge the
effectiveness of the methods and hinders the further exploration of this
challenging yet critical problem in the community. In this paper, we propose a
large-scale dataset for coronary artery segmentation on CTA images. In
addition, we have implemented a benchmark in which we have tried our best to
implement several typical existing methods. Furthermore, we propose a strong
baseline method which combines multi-scale patch fusion and two-stage
processing to extract the details of vessels. Comprehensive experiments show
that the proposed method achieves better performance than existing works on the
proposed large-scale dataset. The benchmark and the dataset are published at
https://github.com/XiaoweiXu/ImageCAS-A-Large-Scale-Dataset-and-Benchmark-for-Coronary-Artery-Segmentation-based-on-CT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;An Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chunbiao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Meiping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jian Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Shanshan Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_D/0/1/0/all/0/1&quot;&gt;Dan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ullah_N/0/1/0/all/0/1&quot;&gt;Najeeb Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_K/0/1/0/all/0/1&quot;&gt;Kaleem Nawaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guisen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01618">
<title>Self Supervised Low Dose Computed Tomography Image Denoising Using Invertible Network Exploiting Inter Slice Congruence. (arXiv:2211.01618v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01618</link>
<description rdf:parseType="Literal">&lt;p&gt;The resurgence of deep neural networks has created an alternative pathway for
low-dose computed tomography denoising by learning a nonlinear transformation
function between low-dose CT (LDCT) and normal-dose CT (NDCT) image pairs.
However, those paired LDCT and NDCT images are rarely available in the clinical
environment, making deep neural network deployment infeasible. This study
proposes a novel method for self-supervised low-dose CT denoising to alleviate
the requirement of paired LDCT and NDCT images. Specifically, we have trained
an invertible neural network to minimize the pixel-based mean square distance
between a noisy slice and the average of its two immediate adjacent noisy
slices. We have shown the aforementioned is similar to training a neural
network to minimize the distance between clean NDCT and noisy LDCT image pairs.
Again, during the reverse mapping of the invertible network, the output image
is mapped to the original input image, similar to cycle consistency loss.
Finally, the trained invertible network&apos;s forward mapping is used for denoising
LDCT images. Extensive experiments on two publicly available datasets showed
that our method performs favourably against other existing unsupervised
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bera_S/0/1/0/all/0/1&quot;&gt;Sutanu Bera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biswas_P/0/1/0/all/0/1&quot;&gt;Prabir Kumar Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01670">
<title>Active CT Reconstruction with a Learned Sampling Policy. (arXiv:2211.01670v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01670</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a widely-used imaging technology that assists
clinical decision-making with high-quality human body representations. To
reduce the radiation dose posed by CT, sparse-view and limited-angle CT are
developed with preserved image quality. However, these methods are still stuck
with a fixed or uniform sampling strategy, which inhibits the possibility of
acquiring a better image with an even reduced dose. In this paper, we explore
this possibility via learning an active sampling policy that optimizes the
sampling positions for patient-specific, high-quality reconstruction. To this
end, we design an \textit{intelligent agent} for active recommendation of
sampling positions based on on-the-fly reconstruction with obtained sinograms
in a progressive fashion. With such a design, we achieve better performances on
the NIH-AAPM dataset over popular uniform sampling, especially when the number
of views is small. Finally, such a design also enables RoI-aware reconstruction
with improved reconstruction quality within regions of interest (RoI&apos;s) that
are clinically important. Experiments on the VerSe dataset demonstrate this
ability of our sampling policy, which is difficult to achieve based on uniform
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Ce Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1&quot;&gt;Kun Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haimiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01784">
<title>MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation. (arXiv:2211.01784v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01784</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, some pioneering works have preferred applying more complex modules
to improve segmentation performances. However, it is not friendly for actual
clinical environments due to limited computing resources. To address this
challenge, we propose a light-weight model to achieve competitive performances
for skin lesion segmentation at the lowest cost of parameters and computational
complexity so far. Briefly, we propose four modules: (1) DGA consists of
dilated convolution and gated attention mechanisms to extract global and local
feature information; (2) IEA, which is based on external attention to
characterize the overall datasets and enhance the connection between samples;
(3) CAB is composed of 1D convolution and fully connected layers to perform a
global and local fusion of multi-stage features to generate attention maps at
channel axis; (4) SAB, which operates on multi-stage features by a shared 2D
convolution to generate attention maps at spatial axis. We combine four modules
with our U-shape architecture and obtain a light-weight medical image
segmentation model dubbed as MALUNet. Compared with UNet, our model improves
the mIoU and DSC metrics by 2.39% and 1.49%, respectively, with a 44x and 166x
reduction in the number of parameters and computational complexity. In
addition, we conduct comparison experiments on two skin lesion segmentation
datasets (ISIC2017 and ISIC2018). Experimental results show that our model
achieves state-of-the-art in balancing the number of parameters, computational
complexity and segmentation performances. Code is available at
https://github.com/JCruan519/MALUNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jiacheng Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Mingye Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuzhuo Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01825">
<title>Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation. (arXiv:2211.01825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Mining structural priors in data is a widely recognized technique for
hyperspectral image (HSI) denoising tasks, whose typical ways include
model-based methods and data-based methods. The model-based methods have good
generalization ability, while the runtime cannot meet the fast processing
requirements of the practical situations due to the large size of an HSI data $
\mathbf{X} \in \mathbb{R}^{MN\times B}$. For the data-based methods, they
perform very fast on new test data once they have been trained. However, their
generalization ability is always insufficient. In this paper, we propose a fast
model-based HSI denoising approach. Specifically, we propose a novel
regularizer named Representative Coefficient Total Variation (RCTV) to
simultaneously characterize the low rank and local smooth properties. The RCTV
regularizer is proposed based on the observation that the representative
coefficient matrix $\mathbf{U}\in\mathbb{R}^{MN\times R} (R\ll B)$ obtained by
orthogonally transforming the original HSI $\mathbf{X}$ can inherit the strong
local-smooth prior of $\mathbf{X}$. Since $R/B$ is very small, the HSI
denoising model based on the RCTV regularizer has lower time complexity.
Additionally, we find that the representative coefficient matrix $\mathbf{U}$
is robust to noise, and thus the RCTV regularizer can somewhat promote the
robustness of the HSI denoising model. Extensive experiments on mixed noise
removal demonstrate the superiority of the proposed method both in denoising
performance and denoising speed compared with other state-of-the-art methods.
Remarkably, the denoising speed of our proposed method outperforms all the
model-based techniques and is comparable with the deep learning-based
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiangjun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hailin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiangyong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1&quot;&gt;Xiangyu Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01885">
<title>Using U-Net Network for Efficient Brain Tumor Segmentation in MRI Images. (arXiv:2211.01885v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01885</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic Resonance Imaging (MRI) is the most commonly used non-intrusive
technique for medical image acquisition. Brain tumor segmentation is the
process of algorithmically identifying tumors in brain MRI scans. While many
approaches have been proposed in the literature for brain tumor segmentation,
this paper proposes a lightweight implementation of U-Net. Apart from providing
real-time segmentation of MRI scans, the proposed architecture does not need
large amount of data to train the proposed lightweight U-Net. Moreover, no
additional data augmentation step is required. The lightweight U-Net shows very
promising results on BITE dataset and it achieves a mean
intersection-over-union (IoU) of 89% while outperforming the standard benchmark
algorithms. Additionally, this work demonstrates an effective use of the three
perspective planes, instead of the original three-dimensional volumetric
images, for simplified brain tumor segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_J/0/1/0/all/0/1&quot;&gt;Jason Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Othmani_A/0/1/0/all/0/1&quot;&gt;Alice Othmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_M/0/1/0/all/0/1&quot;&gt;Mayank Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dev_S/0/1/0/all/0/1&quot;&gt;Soumyabrata Dev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01892">
<title>Deep meta-learning for the selection of accurate ultrasound based breast mass classifier. (arXiv:2211.01892v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01892</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard classification methods based on handcrafted morphological and
texture features have achieved good performance in breast mass differentiation
in ultrasound (US). In comparison to deep neural networks, commonly perceived
as &quot;black-box&quot; models, classical techniques are based on features that have
well-understood medical and physical interpretation. However, classifiers based
on morphological features commonly underperform in the presence of the
shadowing artifact and ill-defined mass borders, while texture based
classifiers may fail when the US image is too noisy. Therefore, in practice it
would be beneficial to select the classification method based on the appearance
of the particular US image. In this work, we develop a deep meta-network that
can automatically process input breast mass US images and recommend whether to
apply the shape or texture based classifier for the breast mass
differentiation. Our preliminary results demonstrate that meta-learning
techniques can be used to improve the performance of the standard classifiers
based on handcrafted features. With the proposed meta-learning based approach,
we achieved the area under the receiver operating characteristic curve of 0.95
and accuracy of 0.91.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karwat_P/0/1/0/all/0/1&quot;&gt;Piotr Karwat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ryzhankow_I/0/1/0/all/0/1&quot;&gt;Ivan Ryzhankow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Komorowski_P/0/1/0/all/0/1&quot;&gt;Piotr Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klimonda_Z/0/1/0/all/0/1&quot;&gt;Ziemowit Klimonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fura_L/0/1/0/all/0/1&quot;&gt;Lukasz Fura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pawlowska_A/0/1/0/all/0/1&quot;&gt;Anna Pawlowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zolek_N/0/1/0/all/0/1&quot;&gt;Norbert Zolek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Litniewski_J/0/1/0/all/0/1&quot;&gt;Jerzy Litniewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01933">
<title>Automatic Crater Shape Retrieval using Unsupervised and Semi-Supervised Systems. (arXiv:2211.01933v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01933</link>
<description rdf:parseType="Literal">&lt;p&gt;Impact craters are formed due to continuous impacts on the surface of
planetary bodies. Most recent deep learning-based crater detection methods
treat craters as circular shapes, and less attention is paid to extracting the
exact shapes of craters. Extracting precise shapes of the craters can be
helpful for many advanced analyses, such as crater formation. This paper
proposes a combination of unsupervised non-deep learning and semi-supervised
deep learning approach to accurately extract shapes of the craters and detect
missing craters from the existing catalog. In unsupervised non-deep learning,
we have proposed an adaptive rim extraction algorithm to extract craters&apos;
shapes. In this adaptive rim extraction algorithm, we utilized the elevation
profiles of DEMs and applied morphological operation on DEM-derived slopes to
extract craters&apos; shapes. The extracted shapes of the craters are used in
semi-supervised deep learning to get the locations, size, and refined shapes.
Further, the extracted shapes of the craters are utilized to improve the
estimate of the craters&apos; diameter, depth, and other morphological factors. The
craters&apos; shape, estimated diameter, and depth with other morphological factors
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Atal Tewari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vikrant Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanna_N/0/1/0/all/0/1&quot;&gt;Nitin Khanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01966">
<title>MarginNCE: Robust Sound Localization with a Negative Margin. (arXiv:2211.01966v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01966</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this work is to localize sound sources in visual scenes with a
self-supervised approach. Contrastive learning in the context of sound source
localization leverages the natural correspondence between audio and visual
signals where the audio-visual pairs from the same source are assumed as
positive, while randomly selected pairs are negatives. However, this approach
brings in noisy correspondences; for example, positive audio and visual pair
signals that may be unrelated to each other, or negative pairs that may contain
semantically similar samples to the positive one. Our key contribution in this
work is to show that using a less strict decision boundary in contrastive
learning can alleviate the effect of noisy correspondences in sound source
localization. We propose a simple yet effective approach by slightly modifying
the contrastive loss with a negative margin. Extensive experimental results
show that our approach gives on-par or better performance than the
state-of-the-art methods. Furthermore, we demonstrate that the introduction of
a negative margin to existing methods results in a consistent improvement in
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sooyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1&quot;&gt;Arda Senocak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Joon Son Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01999">
<title>Quantifying Model Uncertainty for Semantic Segmentation using Operators in the RKHS. (arXiv:2211.01999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01999</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for semantic segmentation are prone to poor performance
in real-world applications due to the highly challenging nature of the task.
Model uncertainty quantification (UQ) is one way to address this issue of lack
of model trustworthiness by enabling the practitioner to know how much to trust
a segmentation output. Current UQ methods in this application domain are mainly
restricted to Bayesian based methods which are computationally expensive and
are only able to extract central moments of uncertainty thereby limiting the
quality of their uncertainty estimates. We present a simple framework for
high-resolution predictive uncertainty quantification of semantic segmentation
models that leverages a multi-moment functional definition of uncertainty
associated with the model&apos;s feature space in the reproducing kernel Hilbert
space (RKHS). The multiple uncertainty functionals extracted from this
framework are defined by the local density dynamics of the model&apos;s feature
space and hence automatically align themselves at the tail-regions of the
intrinsic probability density function of the feature space (where uncertainty
is the highest) in such a way that the successively higher order moments
quantify the more uncertain regions. This leads to a significantly more
accurate view of model uncertainty than conventional Bayesian methods.
Moreover, the extraction of such moments is done in a single-shot computation
making it much faster than Bayesian and ensemble approaches (that involve a
high number of forward stochastic passes of the model to quantify its
uncertainty). We demonstrate these advantages through experimental evaluations
of our framework implemented over four different state-of-the-art model
architectures that are trained and evaluated on two benchmark road-scene
segmentation datasets (Camvid and Cityscapes).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.05470">
<title>Standardised convolutional filtering for radiomics. (arXiv:2006.05470v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2006.05470</link>
<description rdf:parseType="Literal">&lt;p&gt;The Image Biomarker Standardisation Initiative (IBSI) aims to improve
reproducibility of radiomics studies by standardising the computational process
of extracting image biomarkers (features) from images. We have previously
established reference values for 169 commonly used features, created a standard
radiomics image processing scheme, and developed reporting guidelines for
radiomic studies. However, several aspects are not standardised.
&lt;/p&gt;
&lt;p&gt;Here we present a preliminary version of a reference manual on the use of
convolutional image filters in radiomics. Filters, such as wavelets or
Laplacian of Gaussian filters, play an important part in emphasising specific
image characteristics such as edges and blobs. Features derived from filter
response maps have been found to be poorly reproducible. This reference manual
forms the basis of ongoing work on standardising convolutional filters in
radiomics, and will be updated as this work progresses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Whybra_P/0/1/0/all/0/1&quot;&gt;Philip Whybra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Griethuysen_J/0/1/0/all/0/1&quot;&gt;Joost van Griethuysen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaer_R/0/1/0/all/0/1&quot;&gt;Roger Schaer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vallieres_M/0/1/0/all/0/1&quot;&gt;Martin Valli&amp;#xe8;res&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zwanenburg_A/0/1/0/all/0/1&quot;&gt;Alex Zwanenburg&lt;/a&gt; (for the Image Biomarker Standardisation Initiative)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.08930">
<title>A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2008.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have been widely applied in different
scenarios thanks to the development of deep neural networks. The original GAN
was proposed based on the non-parametric assumption of the infinite capacity of
networks. However, it is still unknown whether GANs can fit the target
distribution without any prior information. Due to the overconfident
assumption, many issues remain unaddressed in GANs&apos; training, such as
non-convergence, mode collapses, gradient vanishing. Regularization and
normalization are common methods of introducing prior information to stabilize
training and improve discrimination. Although a handful number of
regularization and normalization methods have been proposed for GANs, to the
best of our knowledge, there exists no comprehensive survey that primarily
focuses on objectives and development of these methods, apart from some
in-comprehensive and limited scope studies. In this work, we conduct a
comprehensive survey on the regularization and normalization techniques from
different perspectives of GANs training. First, we systematically describe
different perspectives of GANs training and thus obtain the different
objectives of regularization and normalization. Based on these objectives, we
propose a new taxonomy. Furthermore, we compare the performance of the
mainstream methods on different datasets and investigate the applications of
regularization and normalization techniques that have been frequently employed
in state-of-the-art GANs. Finally, we highlight potential future directions of
research in this domain. Code and studies related to the regularization and
normalization of GANs in this work is summarized on
https://github.com/iceli1007/GANs-Regularization-Review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Rentuo Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1&quot;&gt;Pengfei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.10611">
<title>FourierNets enable the design of highly non-local optical encoders for computational imaging. (arXiv:2104.10611v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.10611</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable simulations of optical systems can be combined with deep
learning-based reconstruction networks to enable high performance computational
imaging via end-to-end (E2E) optimization of both the optical encoder and the
deep decoder. This has enabled imaging applications such as 3D localization
microscopy, depth estimation, and lensless photography via the optimization of
local optical encoders. More challenging computational imaging applications,
such as 3D snapshot microscopy which compresses 3D volumes into single 2D
images, require a highly non-local optical encoder. We show that existing deep
network decoders have a locality bias which prevents the optimization of such
highly non-local optical encoders. We address this with a decoder based on a
shallow neural network architecture using global kernel Fourier convolutional
neural networks (FourierNets). We show that FourierNets surpass existing deep
network based decoders at reconstructing photographs captured by the highly
non-local DiffuserCam optical encoder. Further, we show that FourierNets enable
E2E optimization of highly non-local optical encoders for 3D snapshot
microscopy. By combining FourierNets with a large-scale multi-GPU
differentiable optical simulation, we are able to optimize non-local optical
encoders 170$\times$ to 7372$\times$ larger than prior state of the art, and
demonstrate the potential for ROI-type specific optical encoding with a
programmable microscope.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1&quot;&gt;Diptodip Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sims_R/0/1/0/all/0/1&quot;&gt;Ruth Sims&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Alex B. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Broxton_M/0/1/0/all/0/1&quot;&gt;Michael Broxton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1&quot;&gt;Misha B. Ahrens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1&quot;&gt;Kaspar Podgorski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1&quot;&gt;Srinivas C. Turaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.02163">
<title>TranSMS: Transformers for Super-Resolution Calibration in Magnetic Particle Imaging. (arXiv:2111.02163v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.02163</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic particle imaging (MPI) offers exceptional contrast for magnetic
nanoparticles (MNP) at high spatio-temporal resolution. A common procedure in
MPI starts with a calibration scan to measure the system matrix (SM), which is
then used to set up an inverse problem to reconstruct images of the MNP
distribution during subsequent scans. This calibration enables the
reconstruction to sensitively account for various system imperfections. Yet
time-consuming SM measurements have to be repeated under notable changes in
system properties. Here, we introduce a novel deep learning approach for
accelerated MPI calibration based on Transformers for SM super-resolution
(TranSMS). Low-resolution SM measurements are performed using large MNP samples
for improved signal-to-noise ratio efficiency, and the high-resolution SM is
super-resolved via model-based deep learning. TranSMS leverages a vision
transformer module to capture contextual relationships in low-resolution input
images, a dense convolutional module for localizing high-resolution image
features, and a data-consistency module to ensure measurement fidelity.
Demonstrations on simulated and experimental data indicate that TranSMS
significantly improves SM recovery and MPI reconstruction for up to 64-fold
acceleration in two-dimensional imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gungor_A/0/1/0/all/0/1&quot;&gt;Alper G&amp;#xfc;ng&amp;#xf6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Askin_B/0/1/0/all/0/1&quot;&gt;Baris Askin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soydan_D/0/1/0/all/0/1&quot;&gt;Damla Alptekin Soydan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saritas_E/0/1/0/all/0/1&quot;&gt;Emine Ulku Saritas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Top_C/0/1/0/all/0/1&quot;&gt;Can Bar&amp;#x131;&amp;#x15f; Top&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1&quot;&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.05876">
<title>Adaptive Diffusion Priors for Accelerated MRI Reconstruction. (arXiv:2207.05876v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.05876</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep MRI reconstruction is commonly performed with conditional models that
de-alias undersampled acquisitions to recover images consistent with
fully-sampled data. Since conditional models are trained with knowledge of the
imaging operator, they can show poor generalization across variable operators.
Unconditional models instead learn generative image priors decoupled from the
imaging operator to improve reliability against domain shifts. Recent diffusion
models are particularly promising given their high sample fidelity.
Nevertheless, inference with a static image prior can perform suboptimally.
Here we propose the first adaptive diffusion prior for MRI reconstruction,
AdaDiff, to improve performance and reliability against domain shifts. AdaDiff
leverages an efficient diffusion prior trained via adversarial mapping over
large reverse diffusion steps. A two-phase reconstruction is executed following
training: a rapid-diffusion phase that produces an initial reconstruction with
the trained prior, and an adaptation phase that further refines the result by
updating the prior to minimize reconstruction loss on acquired data.
Demonstrations on multi-contrast brain MRI clearly indicate that AdaDiff
outperforms competing conditional and unconditional methods under domain
shifts, and achieves superior or on par within-domain performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gungor_A/0/1/0/all/0/1&quot;&gt;Alper G&amp;#xfc;ng&amp;#xf6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1&quot;&gt;Salman UH Dar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozturk_S/0/1/0/all/0/1&quot;&gt;&amp;#x15e;aban &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1&quot;&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1&quot;&gt;Gokberk Elmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1&quot;&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1&quot;&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08581">
<title>Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-Ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy, loss and area under the curve) and reduction of execution
times will be studied with respect to the classical case (the centralized
approach). Different clients will be simulated from the training data, selected
in an unbalanced manner. The results of considering three or ten clients are
exposed and compared between them and against the centralized case. Two
different problems related to intermittent clients are discussed, together with
two approaches to be followed for each of them. Specifically, this type of
problems may occur because in a real scenario some clients may leave the
training, and others enter it, and on the other hand because of client
technical or connectivity problems. Finally, improvements and future work in
the field are proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Judith S&amp;#xe1;inz-Pardo D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro L&amp;#xf3;pez Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00166">
<title>Automated segmentation of microvessels in intravascular OCT images using deep learning. (arXiv:2210.00166v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00166</link>
<description rdf:parseType="Literal">&lt;p&gt;To analyze this characteristic of vulnerability, we developed an automated
deep learning method for detecting microvessels in intravascular optical
coherence tomography (IVOCT) images. A total of 8,403 IVOCT image frames from
85 lesions and 37 normal segments were analyzed. Manual annotation was done
using a dedicated software (OCTOPUS) previously developed by our group. Data
augmentation in the polar (r,{\theta}) domain was applied to raw IVOCT images
to ensure that microvessels appear at all possible angles. Pre-processing
methods included guidewire/shadow detection, lumen segmentation, pixel
shifting, and noise reduction. DeepLab v3+ was used to segment microvessel
candidates. A bounding box on each candidate was classified as either
microvessel or non-microvessel using a shallow convolutional neural network.
For better classification, we used data augmentation (i.e., angle rotation) on
bounding boxes with a microvessel during network training. Data augmentation
and pre-processing steps improved microvessel segmentation performance
significantly, yielding a method with Dice of 0.71+/-0.10 and pixel-wise
sensitivity/specificity of 87.7+/-6.6%/99.8+/-0.1%. The network for classifying
microvessels from candidates performed exceptionally well, with sensitivity of
99.5+/-0.3%, specificity of 98.8+/-1.0%, and accuracy of 99.1+/-0.5%. The
classification step eliminated the majority of residual false positives, and
the Dice coefficient increased from 0.71 to 0.73. In addition, our method
produced 698 image frames with microvessels present, compared to 730 from
manual analysis, representing a 4.4% difference. When compared to the manual
method, the automated method improved microvessel continuity, implying improved
segmentation performance. The method will be useful for research purposes as
well as potential future treatment planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juhwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Justin N. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gomez_Perez_L/0/1/0/all/0/1&quot;&gt;Lia Gomez-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gharaibeh_Y/0/1/0/all/0/1&quot;&gt;Yazan Gharaibeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Motairek_I/0/1/0/all/0/1&quot;&gt;Issam Motairek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pereira_G/0/1/0/all/0/1&quot;&gt;Ga-briel T. R. Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimin_V/0/1/0/all/0/1&quot;&gt;Vladislav N. Zimin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dallan_L/0/1/0/all/0/1&quot;&gt;Luis A. P. Dallan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoori_A/0/1/0/all/0/1&quot;&gt;Ammar Hoori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Kindi_S/0/1/0/all/0/1&quot;&gt;Sadeer Al-Kindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guagliumi_G/0/1/0/all/0/1&quot;&gt;Giulio Guagliumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bezerra_H/0/1/0/all/0/1&quot;&gt;Hiram G. Bezerra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;David L. Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00577">
<title>Fine-tuned Generative Adversarial Network-based Model for Medical Images Super-Resolution. (arXiv:2211.00577v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00577</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical image analysis, low-resolution images negatively affect the
performance of medical image interpretation and may cause misdiagnosis. Single
image super-resolution (SISR) methods can improve the resolution and quality of
medical images. Currently, Generative Adversarial Networks (GAN) based
super-resolution models are widely used and have shown very good performance.
Real-Enhanced Super-Resolution Generative Adversarial Network (Real-ESRGAN) is
one of the recent practical GAN-based models which is widely used in the field
of general image super-resolution. Unlike natural datasets, medical datasets do
not have very high spatial resolution. Transfer learning is one of the
effective methods which uses models trained with external datasets (often
natural datasets), and fine-tunes them to enhance the resolution of medical
images. In our proposed approach, the pre-trained generator and discriminator
networks of the Real-ESRGAN model are fine-tuned using medical image datasets.
In this paper, we worked on retinal images and chest X-ray images. We used the
STARE dataset of retinal images and Tuberculosis Chest X-rays (Shenzhen)
dataset. The proposed model produces more accurate and natural textures, and
the output images have better detail and resolution compared to the original
Real-ESRGAN model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aghelan_A/0/1/0/all/0/1&quot;&gt;Alireza Aghelan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rouhani_M/0/1/0/all/0/1&quot;&gt;Modjtaba Rouhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00924">
<title>SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory. (arXiv:2211.00924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00924</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of talking face generation from speech lies in aligning two
different modal information, audio and video, such that the mouth region
corresponds to input audio. Previous methods either exploit audio-visual
representation learning or leverage intermediate structural information such as
landmarks and 3D models. However, they struggle to synthesize fine details of
the lips varying at the phoneme level as they do not sufficiently provide
visual information of the lips at the video synthesis step. To overcome this
limitation, our work proposes Audio-Lip Memory that brings in visual
information of the mouth region corresponding to input audio and enforces
fine-grained audio-visual coherence. It stores lip motion features from
sequential ground truth images in the value memory and aligns them with
corresponding audio features so that they can be retrieved using audio input at
inference time. Therefore, using the retrieved lip motion features as visual
hints, it can easily correlate audio with visual dynamics in the synthesis
step. By analyzing the memory, we demonstrate that unique lip features are
stored in each memory slot at the phoneme level, capturing subtle lip motion
based on memory addressing. In addition, we introduce visual-visual
synchronization loss which can enhance lip-syncing performance when used along
with audio-visual synchronization loss in our model. Extensive experiments are
performed to verify that our method generates high-quality video with mouth
shapes that best align with the input audio, outperforming previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Joanna Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01111">
<title>On the Benefit of Dual-domain Denoising in a Self-supervised Low-dose CT Setting. (arXiv:2211.01111v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01111</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is routinely used for three-dimensional non-invasive
imaging. Numerous data-driven image denoising algorithms were proposed to
restore image quality in low-dose acquisitions. However, considerably less
research investigates methods already intervening in the raw detector data due
to limited access to suitable projection data or correct reconstruction
algorithms. In this work, we present an end-to-end trainable CT reconstruction
pipeline that contains denoising operators in both the projection and the image
domain and that are optimized simultaneously without requiring ground-truth
high-dose CT data. Our experiments demonstrate that including an additional
projection denoising operator improved the overall denoising performance by
82.4-94.1%/12.5-41.7% (PSNR/SSIM) on abdomen CT and 1.5-2.9%/0.4-0.5%
(PSNR/SSIM) on XRM data relative to the low-dose baseline. We make our entire
helical CT reconstruction framework publicly available that contains a raw
projection rebinning step to render helical projection data suitable for
differentiable fan-beam reconstruction operators and end-to-end learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Fabian Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1&quot;&gt;Mareike Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfaff_L/0/1/0/all/0/1&quot;&gt;Laura Pfaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1&quot;&gt;Oliver Aust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pechmann_S/0/1/0/all/0/1&quot;&gt;Sabrina Pechmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weidner_D/0/1/0/all/0/1&quot;&gt;Daniela Weidner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maul_N/0/1/0/all/0/1&quot;&gt;Noah Maul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rohleder_M/0/1/0/all/0/1&quot;&gt;Maximilian Rohleder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utz_J/0/1/0/all/0/1&quot;&gt;Jonas Utz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1&quot;&gt;Felix Denzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>