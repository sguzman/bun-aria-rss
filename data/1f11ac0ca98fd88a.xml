<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/topic/consumer-electronics.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Sat, 05 Nov 2022 16:00:50 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>AI-Generated Fashion Is Next Wave of DIY Design</title><link>https://spectrum.ieee.org/dall-e-fashion-design</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/fashion-platform-cala-s-new-generative-ai-tool-displays-multiple-ai-generated-jackets.jpg?id=32010698&width=1200&height=800&coordinates=71%2C0%2C71%2C0"/><br/><br/><p>Could AI inspire your next ugly holiday sweater?</p><p>As odd as it may sound, recent advancements in machine learning have made it possible. <a href="https://ca.la/" rel="noopener noreferrer" target="_blank">CALA</a>, an “operating system for fashion” that helps designers sketch, prototype, and produce new products, is the first service to implement OpenAI’s <a href="https://apitracker.io/a/openai-dall-e" target="_blank">DALL-E API</a>. Its new generative AI tool is live and free to try.</p><p>“The use case is enabling anyone to get their idea across without a full sketch or 3D renders, by having DALL-E generate ideas via text inputs,” says <a href="https://www.linkedin.com/in/awhyit/" rel="noopener noreferrer" target="_blank">Andrew Wyatt</a>, cofounder and CEO at CALA. “It’s a continuance of us democratizing access in an industry that’s historically been very insular.” </p><h2>DALL-E for e-fashion?</h2><p>Founded in 2016, CALA is a fashion platform built for designers looking for an accessible way to turn ideas into tangible products. The service is available through both its website and a mobile app. Anyone can sign up and try the platform for free—so I did.</p><p>It’s broadly similar to <a href="https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy" target="_self">AI art generators such as DALL-E 2 and Stable Diffusion</a> but customized to fit CALA’s platform. Instead of entering a text prompt in a single, long string of text, designers are guided to first select a base style, such as a sweater, blouse, or tote, from a list of 25 options. Designers then use generative AI to modify the style through two textual prompts. The first describes the design based on adjectives and materials, while the other describes desired trims and features such as cuffs or zippers. </p><p class="pull-quote">“We want to prevent the situation where someone comes in, they type in ‘brown shirt,’ and they’re, like, this sucks.”<br/>—Andrew Wyatt, CALA</p><p>Wyatt believes this alternative user interface will help designers zone in on important features and avoid duds. “What we’re kind of doing here, is we built a UI on top of the prompt engineering. Our goal here is to get people to a meaningful result as quickly as possible.” This, Wyatt hopes, will nudge designers away from dead-end or unappealing results. “We want to prevent the situation where someone comes in, they type in ‘brown shirt,’ and they’re, like, this sucks.”</p><p>I saw the results of this tactic in my own, messy effort to make a Halloween sweater. Fashion design is, admittedly, well out of my comfort zone, but I found the tool approachable. The entire process, including the time spent waiting for results to appear, took less than a minute. CALA presents six results at a time, any of which can then be inserted into the design platform for further iteration.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Fashion platform CALA's generative AI tool displaying multiple suggestions for an ugly halloween sweater. " class="rm-shortcode" data-rm-shortcode-id="9038cb801ac27b8e37fe697e9149f773" data-rm-shortcode-name="rebelmouse-image" id="91655" loading="lazy" src="https://spectrum.ieee.org/media-library/fashion-platform-cala-s-generative-ai-tool-displaying-multiple-suggestions-for-an-ugly-halloween-sweater.jpg?id=32010710&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">CALA's generative AI tool offered ideas for an ugly halloween sweater. I like the one on the bottom left. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">CALA</small></p><p>CALA’s implementation shouldn’t be misunderstood as a one-click design tool. Designers still need to bring their own skills and learn how to use CALA’s platform. However, Wyatt hopes AI will significantly reduce the barrier to entry for new designers and give veteran designers a way to overcome creative roadblocks.</p><p>“We want to let people take an idea and just follow the rabbit trail, through variation after variation after variation,” says Wyatt. “We think it’s going to help people come up with way crazier and different concepts.”</p><h2>Ease of use could drive DALL-E’s surge </h2><p>CALA’s tool is the first live, public implementation of OpenAI’s DALL-E API by a third party. The API is not currently available to the public and doesn’t have a release date. </p><p>This isn’t OpenAI’s first rodeo. <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a>, the company’s deep learning language model, was released as an API in 2020 and was quickly adopted by third parties. GPT-3 is now used by dozens of companies and organizations including <a href="https://copysmith.ai/" target="_blank">Copysmith</a> and <a href="https://www.messagebird.com/en/" rel="noopener noreferrer" target="_blank">MessageBird</a>. Microsoft acquired a license to use the GPT-3 model for <a href="https://blogs.microsoft.com/ai/from-conversation-to-code-microsoft-introduces-its-first-product-features-powered-by-gpt-3/" rel="noopener noreferrer" target="_blank">Microsoft Power Apps</a> and the <a href="https://blogs.microsoft.com/ai/new-azure-openai-service/" rel="noopener noreferrer" target="_blank">Azure OpenAI Service</a>. </p><p>Luke Miller, product manager at OpenAI, says the company learned valuable lessons from GPT-3’s rollout. “Each deployment teaches us more about safety, engineering, and, ultimately, how our technology can create value in the world,” says Miller. “Since releasing the GPT-3 API, we’ve made a number of improvements to our safeguards. For example, we announced an updated <a href="https://openai.com/blog/new-and-improved-content-moderation-tooling/" rel="noopener noreferrer" target="_blank">moderation endpoint</a> in August and we’re continuing to find ways we can improve.” </p><p>CALA’s experience with the DALL-E API hints ease of use will prove the key driver of the API’s adoption once it’s made available to the public. Wyatt says his company’s engineers put the API to use in just a few weeks. </p><p>“We sort of did some high-res concepting that we passed off to [OpenAI] for feedback about eight weeks ago. Then the total build and polish was less than a month,” says Wyatt. “I could see this being a meaningful integration in a multitude of different products.”</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="CALA's generative AI tool offering input options that will generate AI fashion suggestions. " class="rm-shortcode" data-rm-shortcode-id="7636c8179deb0a7744e77b0798c43ef1" data-rm-shortcode-name="rebelmouse-image" id="07bfc" loading="lazy" src="https://spectrum.ieee.org/media-library/cala-s-generative-ai-tool-offering-input-options-that-will-generate-ai-fashion-suggestions.jpg?id=32010727&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">CALA’s use of the DALL-E API is different from the tool found on DALL·E’s own website.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">CALA</small></p><p>In fact, the flood of tools based on DALL-E has already begun. Shutterstock, a service which offers stock photos, images, and videos, <a href="https://www.shutterstock.com/press/20435?irclickid=39YTfO1jIxyNU8EUobwjwUDfUkDV7X2lQ1ECyw0&irgwc=1&utm_medium=Affiliate&utm_campaign=Skimbit%20Ltd.&utm_source=10078&utm_term=theverge.com" rel="noopener noreferrer" target="_blank">plans to implement the DALL-E API</a> “in the coming months.” Shutterstock paired this announcement with a framework to compensate artists on the platform when their work is used to train AI models. <a href="https://blogs.microsoft.com/ai/from-hot-wheels-to-handling-content-how-brands-are-using-microsoft-ai-to-be-more-productive-and-imaginative/" target="_blank">Microsoft is bringing DALL-E to its Azure OpenAI Service</a>, too, though access is currently invite-only.<br/></p><p>“We’ve always felt the future, especially within fashion, is kind of moving towards AI-powered design and automated production,” says Wyatt. “We just thought it was going to be, you know, five years from now. Over the last six months, just seeing the amount of progress...[we] think there’s just going to be tremendous innovation over the next couple of years.” </p>]]></description><pubDate>Sat, 29 Oct 2022 17:42:42 +0000</pubDate><guid>https://spectrum.ieee.org/dall-e-fashion-design</guid><category>Dall-e 2</category><category>Artificial intelligence</category><category>Diy</category><category>Fashion tech</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/fashion-platform-cala-s-new-generative-ai-tool-displays-multiple-ai-generated-jackets.jpg?id=32010698&amp;width=980"></media:content></item><item><title>Digital Resurrection Brings Star Trek Back to the Future</title><link>https://spectrum.ieee.org/digital-recreation-star-trek</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/5-scenes-from-star-trek-episodes.png?id=31639724&width=1200&height=800&coordinates=0%2C0%2C311%2C0"/><br/><br/><p>The bridge of the original <em>U.S.S. Enterprise</em> could soon be a place you can visit—complete with some of the original cast. </p><p>Visual effects that include virtual “performances” by deceased actors, or that drastically de-age those still alive, are becoming commonplace. <em>Rogue One: A Star Wars Story</em> famously used such effects <a href="https://www.nytimes.com/2016/12/27/movies/how-rogue-one-brought-back-grand-moff-tarkin.html" rel="noopener noreferrer" target="_blank">to replicate the late Peter Cushing’s performance as Grand Moff Tarkin</a>. Now, the Roddenberry Archive is using similar effects to give audiences a like-new performance of Leonard Nimoy as Spock—if only inside the virtual world of a video game. </p><p>“The kid inside me had always dreamed of being Spock,” says actor Lawrence Selleck, who performs as Spock in the Roddenberry Archive’s restoration. “And now, suddenly, here I am with the Roddenberry Foundation putting on the best set of ears you can possibly imagine.”</p><hr/><h2>Digitally restoring a 57-year-old pilot</h2><p>The Roddenberry Archive is a collaboration between the Gene Roddenberry Estate and <a href="https://home.otoy.com/" rel="noopener noreferrer" target="_blank">OTOY</a>, a cloud graphics company that builds specialized rendering software. </p><p><a href="https://home.otoy.com/roddenberryarchive/" rel="noopener noreferrer" target="_blank">Announced in 2021</a>, the Roddenberry Archive is showing its first complete project: a re-creation of sets, props, and actors from <em>Star Trek</em>’s 1965 original pilot episode, “The Cage.” The pilot, shot two years prior to the series’ television debut, was unsuccessful, forcing the creation of a second pilot, “Where No Man Has Gone Before.” The experience debuted at Creation Entertainment’s Star Trek convention “The 56-Year Mission: Las Vegas,” which ran from 25 to 28 of August 2022.</p><p>The experience is not a restoration of the episode but rather a re-creation of the episode’s set and props inside an interactive 3D experience. “What [the Roddenberry Archive] had on display were a couple of stations with a flat-screen TV and a game controller, and it allowed you to walk through and see everything,” says Selleck. This version of the experience is built in Unreal Engine and runs locally on a PC. Trek fans at the Las Vegas convention, game controller in hand, could explore “The Cage,” including key sets like the bridge and engineering. </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt='A television on a table displaying "The Cage" digital experience. Another table has a mannequin with Star Trek shirt.' class="rm-shortcode" data-rm-shortcode-id="b9d9038a972e0f85036853f29e10cac4" data-rm-shortcode-name="rebelmouse-image" id="4d29a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-television-on-a-table-displaying-the-cage-digital-experience-another-table-has-a-mannequin-with-star-trek-shirt.png?id=31639617&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The current iteration of “The Cage” is navigated with a game controller.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">OTOY/The Roddenberry Archive</small></p><p>While the iteration of “The Cage” shown at “The 56-Year Mission: Las Vegas” was confined to a television, OTOY’s founder and CEO, Jules Urbach, aspires to bring the experience to more immersive technologies, including holographic displays. “Being able to walk through the sets of the Enterprise is better than using an Xbox controller or using WASD on a Web page,” says Urbach.<br/></p><p>OTOY is working with <a href="https://www.lightfieldlab.com" target="_blank">Light Field Labs,</a> creator of Solid Light holographic displays, to achieve immersion without a headset or glasses. The goal is holographic reproductions of Star Trek sets where fans can tour a life-size Enterprise accurate to the original sets and interact with classic characters. </p><p>“The dream, I think, is to basically build a Holodeck,” says Selleck. </p><h2>Can digital resurrection be used with respect?</h2><p>OTOY’s teasers include the digital re-creation of Leonard Nimoy’s Spock, portrayed by Lawrence Selleck, as well as Laurel Goodwin’s Yeoman L.M. Colt, portrayed by Mahé Thaissa. Both Nimoy and Goodwin are now deceased—Nimoy in 2015 and Goodwin earlier this year. </p><p>The teaser is brief, but convincing, and raises questions like those that followed the re-creation of Peter Cushing’s Grand Moff Tarkin for <em>Rogue One</em>. Disney’s decision to digitally resurrect Cushing was criticized in 2016 by numerous publications including <em><a href="https://www.theguardian.com/commentisfree/2016/dec/21/peter-cushing-rogue-one-resurrection-cgi" rel="noopener noreferrer" target="_blank">The Guardian</a>, </em><a href="https://time.com/4604996/rogue-one-grand-moff-tarkin-princess-leia-cgi/" rel="noopener noreferrer" target="_blank"><em>TIME</em></a>, and <a href="https://variety.com/2016/film/news/rogue-one-peter-cushing-digital-resurrection-cgi-1201943759/" rel="noopener noreferrer" target="_blank"><em>Variety</em></a>. While technically impressive, watching an actor perform two decades after his death didn’t sit well with some viewers.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="afbbe5d301fedbdb1dde16b5dbde3d0a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fzeznWVQu5o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">765874 - Memory Wall (4K)</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=fzeznWVQu5o" target="_blank">www.youtube.com</a>
</small>
</p><p>Selleck is aware of these concerns. “It’s silly to say it’s a holy endeavor, but for <em>Star Trek</em> fans, it really kind of is. You don’t take this lightly.” He can imagine ways the technology could be used to be funny, or trashy, or otherwise run counter to the spirit of the original performance. To Selleck, it’s important to steer clear of that territory. “Not even in between takes, not even goofing around. It’s not appropriate,” he says.</p><p>And there’s another aspect that sets Selleck’s performance, and the Roddenberry Archive’s efforts, apart from application of similar techniques in blockbuster movies: the focus on preservation, not the mass market. “In order to stay in the noncommercial realm, we’re not putting this out there for people to download,” says Urbach. “Ideally, we’d like to do something at the Smithsonian.” </p><p>Digital re-creations of actors are used extensively in Hollywood films to resurrect, de-age, or modify the appearance of actor, but the results are typically part of new commercial films. It’s unclear if past criticisms apply to use of digital effects for preservation of historical performances.</p><p>“From my own perspective? I would love to stand on the bridge next to, nearby, these heroic figures,” says Selleck. </p><p><br/></p><p><strong><em>Update 7 Oct. 2022: </em></strong><em>A previous version of this article, although lacking the present version’s interview with OTOY CEO Jules Urbach, was published online on 13 Sept. </em><br/></p>]]></description><pubDate>Fri, 07 Oct 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/digital-recreation-star-trek</guid><category>Special effects</category><category>Deepfakes</category><category>Digital restoration</category><category>Star trek</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/5-scenes-from-star-trek-episodes.png?id=31639724&amp;width=980"></media:content></item><item><title>This Idea Wasn’t All Wet: The Sensing Water-Saving Showerhead Debuts</title><link>https://spectrum.ieee.org/water-saving-shower-head</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-mounted-and-running-showerhead-that-says-oasense-and-has-a-blue-light-on-it.jpg?id=31831006&width=1200&height=800&coordinates=0%2C182%2C0%2C183"/><br/><br/><p>For Evan Schneider, the family dinner table is a good place for invention. “I’m always, ‘Wouldn’t it be cool if this or that,’” he says, “and people would humor me.”</p><p>In 2012, with California in the midst of a severe drought, Schneider, then a mechanical engineering graduate student at <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford University</a>, once again tossed out a “cool idea.” He imagined a showerhead that would sense when the person showering moved out from under the stream of water. The showerhead would then automatically turn the water off, turning it back on again when the person moved back into range. With such a device, he thought, people could enjoy a long shower without wasting water.</p><p>“But turning the water on and off manually didn’t make sense in our house,” Schneider said. “We had separate knobs for hot and cold, and another one to switch from tub to shower, so you’d have to adjust the water every time you turned it back on. You’d waste more water than you saved. Plus a shower is a blissful time of relaxation. You don’t want to stop the party halfway.”</p><p>Ten years and many starts and stops later, that sensing showerhead is now shipping to customers from <a href="https://www.oasense.com/" rel="noopener noreferrer" target="_blank">Oasense</a>, a company incorporated in 2019.</p><p>“The general idea is really simple,” Schneider says. “A lot of people have said they also thought of this idea. And I’m sure that’s true, but there were a lot of devils in the details.” Oasense’s team has been granted several patents related to their device, the first filed by Schneider in 2016. </p><p>Schneider’s development path started soon after that dinner-table conversation. First, he confirmed that showers were a big part of water usage for a typical household, and that no such device was already on the market. He collected off-the-shelf components, including an infrared sensor scavenged from a high-end automatic faucet, designed a prototype in a CAD system, printed out the plastic parts using a 3D printer, and assembled it. With 4 AA batteries as a power source, the gadget would operate for about a year, thanks to his choice of a latching solenoid valve, one that uses power to switch from open to closed but doesn’t draw any power to hold in one state or another.</p><p>The prototype worked well enough that his parents were willing to throw out their standard showerhead. He assembled dozens of them and distributed them to friends and family—anyone willing to try.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Man seated on box in garage with showerhead casings and parts laid out on the floor in front of him, he holds one showerhead." class="rm-shortcode" data-rm-shortcode-id="76ba3df7a3727af13c157ff01851326e" data-rm-shortcode-name="rebelmouse-image" id="429da" loading="lazy" src="https://spectrum.ieee.org/media-library/man-seated-on-box-in-garage-with-showerhead-casings-and-parts-laid-out-on-the-floor-in-front-of-him-he-holds-one-showerhead.jpg?id=31831165&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Oasense cofounder Ted Li assembles an early version of the company’s sensing showerhead.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Oasense</small></p><p>In 2016, Schneider decided to run a <a href="https://www.kickstarter.com/" target="_blank">Kickstarter</a> campaign to see if the gadget could attract broad interest. The Kickstarter ultimately failed; it drew a decent number of potential buyers, but, says Schneider, “I had set the bar high, because I was busy doing other things, and if I switched to this, I wanted to make sure it would have a good chance of working out. It didn’t meet that bar; it raised about US $34,000 out of its $75,000 goal.”</p><p>So Schneider put his showerhead idea on hold. Instead, he focused on expanding a burgeoning small business that he was also passionate about—3D printing prototypes and various parts for hardware companies.</p><p>But the showerhead wasn’t done with him. In 2017 someone who Schneider had never met edited <a href="https://www.facebook.com/VT/videos/646639645508722/" target="_blank">the video from the Kickstarter pitch</a> and shared it on Facebook. This time, the video got far more attention—millions of views in just weeks.</p><p>Unfortunately, the timing couldn’t have been worse. Schneider was dealing with a flare-up of a chronic illness and his 3D printing business was at a critical growth period. “I had wanted this for years, but it was the worst time for it to happen,” he says.</p><p>“I still believed in the product,” Schneider continued, “but I knew it needed improvements and more attention than I was able to give it. I tried for a couple of weeks to reply to all these people contacting me, thousands of them, but it was too much. I was planning to shelve it.”</p><p>That’s when Chih-Wei Tang, a friend from Stanford’s mechatronics program who had been an early backer of the project on Kickstarter, reached out to Schneider. Tang, who was working as a technical product manager at the <a href="https://corporate.ford.com/operations/locations/silicon-valley.html" rel="noopener noreferrer" target="_blank">Ford Greenfield Labs</a>, convinced Schneider that he could form a team capable of commercializing the product. Tang pulled in his friend Ted Li, who had just left <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> after managing display technology for the iPhone and Apple Watch.</p><p>Tang and Li devoted themselves to the project full-time, Schneider helped part-time as needed. The three started by trying to better adapt an off-the-shelf sensor, but ended up designing a sensor suite with custom hardware and algorithms.</p><p>They incorporated as Oasense in December 2019 as cofounders. In late 2020, the company went out for funding, and brought in about $1 million from angel investors, friends, and family. In addition to the founders, Oasense now has four full-time and three part-time employees.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="3 smiling men in suit jackets" class="rm-shortcode" data-rm-shortcode-id="70a9c15e58ef12fe97c7cd6256a3fb15" data-rm-shortcode-name="rebelmouse-image" id="16097" loading="lazy" src="https://spectrum.ieee.org/media-library/3-smiling-men-in-suit-jackets.jpg?id=31831170&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Oasense cofounders [from left] Ted Li, Evan Schneider, and Chih-Wei Tang.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Oasense</small></p><p>The current version of the device includes several sensors (across a wide range of light wavelengths) and software that allows the sensors to self-calibrate, since every shower environment is different in terms of light, reflectivity, size, and design. Calibration happens during warm-up, when the person showering is unlikely to be standing in the stream. A temperature sensor determines when this warm-up period is over and cuts the flow if the user hasn’t moved under the showerhead. The redesign also replaced the AA batteries with a turbine that generates power from the water flow and sends it to a small rechargeable battery sealed inside the device.</p><p>Says Tang, “It does seem like someone would have built this before, but it turns out to be really complicated. For example, one problem that affects the noise in the sensor signals is fog. In a hot shower, after 3 minutes, our original sensor was blinded by fog. When we designed our new sensors, we had to make sure that didn’t happen.</p><p>“And these sensors are power hungry and need to be on for the duration of the shower, whether water is flowing or not, so generator and sensor efficiency had to be maximized.”</p><p>Oasense officially launched its product, <a href="https://shop.oasense.com/products/reva-showerhead" rel="noopener noreferrer" target="_blank">Reva</a>, in August. The company is working to figure out the best way to sell the gadget; it is now just doing <a href="https://shop.oasense.com/products/reva-showerhead" rel="noopener noreferrer" target="_blank">direct sales at $350</a> per self-installable unit.</p><p>“Two trends are coming together,” Tang says. “Sustainability is what everyone has to be about these days, and technology is invading every corner of our homes. Using technology, we designed sustainability into a product that doesn’t compromise quality or the experience, it just addresses the problem.”</p>]]></description><pubDate>Sun, 02 Oct 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/water-saving-shower-head</guid><category>Gadgets</category><category>Conservation</category><category>Water</category><category>Consumer electronics</category><category>Startups</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-mounted-and-running-showerhead-that-says-oasense-and-has-a-blue-light-on-it.jpg?id=31831006&amp;width=980"></media:content></item><item><title>California’s Proposed Law Could Change the Internet</title><link>https://spectrum.ieee.org/californias-proposed-law-could-change-the-internet</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-little-girls-face-lit-up-by-the-smart-phone-she-is-looking-at-in-a-dark-room.jpg?id=31704747&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><strong>Today, for better </strong>or worse, the Internet is a rather free range for children. Websites ask their users’ ages, sure. But virtually anyone who came of age around the rise of the Internet can probably relate a time or 20 when they gave a false birthdate.</p><p>A California law now in the works might bring that world to a crashing halt.</p><p><a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220AB2273" rel="noopener noreferrer" target="_blank">AB 2273</a>, or the California Age-Appropriate Design Code Act, promises to make the Internet safer for children—in part by tightening age verification. Its opponents instead believe that, in the process, AB 2273 could completely decimate the existing Internet as we know it.</p><hr/><p>AB 2273 isn’t final just yet. To become California law, a bill has to pass both houses of the state legislature—the Assembly and the Senate—and then attain the signature of the governor. AB 2273 passed the Assembly on 29 August, and the Senate the next day, posting it to Governor Gavin Newsom’s desk. As of this writing, Newsom has yet to sign the bill. There’s little indication whether he will.</p><p>Suppose he does sign. Then, beginning on 1 July 2024, any website or app that “conducts business in California” and “provides an online service, product, or feature likely to be accessed by children” would need to follow yet-to-be-crafted code. </p><p>California wouldn’t be the first jurisdiction to tighten age-related design standards for websites. AB 2273 explicitly cites an existing law in the United Kingdom, which expects websites to comply with a bespoke <a href="https://ico.org.uk/for-organisations/guide-to-data-protection/ico-codes-of-practice/age-appropriate-design-a-code-of-practice-for-online-services/" rel="noopener noreferrer" target="_blank">age-appropriate design code</a>. (In fact, both bills <a href="https://www.techdirt.com/2022/08/25/why-is-a-british-baroness-drafting-california-censorship-laws/" rel="noopener noreferrer" target="_blank">share a backer</a>, one Baroness Beeban Kidron, <a href="https://www.telegraph.co.uk/news/2022/08/13/turned-back-film-career-protect-children-online/" rel="noopener noreferrer" target="_blank">a campaigner</a> for children’s rights online.)</p><p>That U.K. law <a href="https://www.ft.com/content/41b33aa7-7a59-4b50-be47-9911adb34d3e" rel="noopener noreferrer" target="_blank">has already made ripples</a>. YouTube disabled its autoplay feature for users under 18. Instagram started preventing adults from messaging under-18s who don’t follow them. TikTok stopped sending under-18s push notifications after a certain point each evening.</p><p>But according to <a href="https://law.scu.edu/faculty/profile/goldman-eric/" rel="noopener noreferrer" target="_blank">Eric Goldman</a>, a law professor at Santa Clara University and one of the bill’s harshest critics, in a U.S. regulatory environment that’s generally even less friendly to businesses, California’s code is likely to be stricter. “Any ‘lessons learned’ in the U.K. do not extend to the U.S. because the law literally cannot be implemented in the same way,” he says.</p><h2>What does California’s AB 2273 require tech companies to do?</h2><p>Though California’s code doesn’t yet exist, AB 2273 lays out a few requirements. For one, websites must report their data-management practices to a California government agency. Also, websites can’t collect or sell data on children (including geolocation) that isn’t absolutely necessary for children to use the website. And websites must tell a child when a parent or guardian is tracking their activity on that site.</p><p>Where AB 2273 becomes more than a little controversial is the requirement that, to determine which users ought to experience what, websites must “estimate the age of child users with a reasonable level of certainty.” </p><p>“Assuming businesses do not want to intentionally degrade their value proposition to adults, then they have no alternative other than to authenticate the age of all of their customers and then segregate adults from children, with different offerings for each,” says Goldman. </p><p>How a website will “estimate the age of child users” isn’t clear, and <a href="https://www.techdirt.com/2022/08/29/age-verification-providers-say-dont-worry-about-california-design-code-youll-just-have-to-scan-your-face-for-every-website-you-visit/" target="_blank">according to Techdirt</a>, it might vary by website. A child entering a “high-risk” website, then, might need to submit an ID document for age verification. That failing, a child might literally have to scan their face. Not only is face recognition a technology whose reliability is questionable, mandating it could make websites inaccessible to people without a functioning camera. </p><p>And although the law champions privacy, it’s not clear that authentication along those lines could even be done in a privacy-conscious manner. Goldman says that websites might rely on insecure third-party services.</p><p>If AB 2273 passes, then its effects could spread well beyond the state’s borders. Websites will be left with two options: geolocating users in California (perhaps blocking them completely, potentially risking revenue), or applying the rules to all their users. Many websites will just find it easier to do the latter.</p><p>Then around the world, users might have to face the same age-authentication gauntlet that Californians would. And, according to Goldman, other jurisdictions might take after California in drafting their own laws.</p><p>Some of AB 2273’s sponsors and defenders see the bill as a necessary measure in a world where children are vulnerable to dangers like <a href="https://www.wired.com/story/facebook-social-media-privacy-dark-patterns/" target="_blank">manipulative websites</a>, <a href="https://news.bloomberglaw.com/privacy-and-data-security/childrens-apps-unknowingly-collecting-data-pose-compliance-risk" rel="noopener noreferrer" target="_blank">invasive apps</a>, and <a href="https://www.theverge.com/2021/11/18/22789886/states-attorneys-general-investigate-instagram-meta-child-safety-engagement" rel="noopener noreferrer" target="_blank">social-media addiction</a>.</p><p>But from many corners, the reaction has been less than positive. AB 2273 has garnered a wide range of opponents,  including  <a href="https://iapp.org/news/a/california-age-appropriate-design-code-final-passage-brings-mixed-reviews/" rel="noopener noreferrer" target="_blank">privacy advocates</a> and <a href="https://www.axios.com/2022/08/29/california-childrens-internet-privacy-rules" rel="noopener noreferrer" target="_blank">big tech</a>. Santa Clara’s Goldman likens the law to a neutron bomb. “It will depopulate the Internet and turn many services into ghost towns,” he says.</p>Of course, this is all still hypothetical. For now, the bill awaits Governor Newsom’s signature. Even if that happens, AB 2273 is hardly immune to lawsuits. NetChoice—an advocacy group that has helped take other laws passed in <a href="https://netchoice.org/unconstitutional-social-media-bill-circumvents-rights-afforded-under-the-constitution/" rel="noopener noreferrer" target="_blank">Florida</a> and <a href="https://netchoice.org/netchoice-ccia-v-paxton/" rel="noopener noreferrer" target="_blank">Texas</a> to court—has already <a href="https://netchoice.org/gov-newsom-must-say-no-to-ca-tech-bills-that-will-fail-families/" rel="noopener noreferrer" target="_blank">come out</a> against the bill.]]></description><pubDate>Tue, 20 Sep 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/californias-proposed-law-could-change-the-internet</guid><category>Age restriction</category><category>California</category><category>Legislation</category><category>Internet</category><category>Privacy</category><dc:creator>Rahul Rao</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-little-girls-face-lit-up-by-the-smart-phone-she-is-looking-at-in-a-dark-room.jpg?id=31704747&amp;width=980"></media:content></item><item><title>Deep Learning Could Bring the Concert Experience Home</title><link>https://spectrum.ieee.org/3d-audio</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image-containing-multiple-aspects-such-as-instruments-and-left-and-right-open-hands.jpg?id=31304511&width=1200&height=800&coordinates=0%2C865%2C0%2C865"/><br/><br/><p>
<strong>Now that recorded</strong> sound has become ubiquitous, we hardly think about it. From our smartphones, smart speakers, TVs, radios, disc players, and car sound systems, it’s an enduring and enjoyable presence in our lives. In 2017, a survey by the polling firm <a href="https://global.nielsen.com/" rel="noopener noreferrer" target="_blank">Nielsen</a> suggested that some 90 percent of the U.S. population listens to music regularly and that, on average, they do so 32 hours per week.
</p><p>
	Behind this free-flowing pleasure are enormous industries applying technology to the long-standing goal of reproducing sound with the greatest possible realism. From Edison’s phonograph and the horn speakers of the 1880s, successive generations of engineers in pursuit of this ideal invented and exploited countless technologies: triode vacuum tubes, dynamic loudspeakers, magnetic phonograph cartridges, solid-state amplifier circuits in scores of different topologies, electrostatic speakers, optical discs, stereo, and surround sound. And over the past five decades, digital technologies, like 
	<a href="https://www.uaudio.com/blog/audio-compression-basics/" rel="noopener noreferrer" target="_blank">audio compression</a> and <a href="https://www.cloudflare.com/learning/video/what-is-streaming/" rel="noopener noreferrer" target="_blank">streaming</a>, have transformed the music industry.
</p><p>
	And yet even now, after 150 years of development, the sound we hear from even a high-end audio system falls far short of what we hear when we are physically present at a live music performance. At such an event, we are in a natural sound field and can readily perceive that the sounds of different instruments come from different locations, even when the sound field is criss-crossed with mixed sound from multiple instruments. There’s a reason why people pay considerable sums to hear live music: It is more enjoyable, exciting, and can generate a bigger emotional impact.
</p><div class="ieee-sidebar-medium">
<p>
		 To hear the author’s 3D Soundstage audio for yourself, grab your headphones and head over to 
		<a href="http://3dsoundstage.com/ieee" rel="noopener noreferrer" target="_blank">3dsoundstage.com/ieee</a>.
	</p>
</div><p>
	Today, researchers, companies, and entrepreneurs, including ourselves, are closing in at last on recorded audio that truly re-creates a natural sound field. The group includes big companies, such as Apple and Sony, as well as smaller firms, such as 
	<a href="https://sg.creative.com/" rel="noopener noreferrer" target="_blank">Creative</a>. Netflix recently disclosed a <a href="https://en-us.sennheiser.com/newsroom/new-audio-experience-now-streaming-on-netflix" rel="noopener noreferrer" target="_blank">partnership with Sennheiser</a> under which the network has begun using a new system, Ambeo 2-Channel Spatial Audio, to heighten the sonic realism of such TV shows as “<a href="https://www.whathifi.com/us/features/stranger-things-s4-sounds-better-thanks-to-sennheisers-ever-morphing-ambeo" rel="noopener noreferrer" target="_blank">Stranger Things</a>” and “The Witcher.”
</p><p>
	There are now at least half a dozen different approaches to producing highly realistic audio. We use the term “soundstage” to distinguish our work from other audio formats, such as the ones referred to as spatial audio or immersive audio. These can represent sound with more spatial effect than ordinary stereo, but they do not typically include the detailed sound-source location cues that are needed to reproduce a truly convincing sound field.
</p><p>
	We believe that soundstage is the future of music recording and reproduction. But before such a sweeping revolution can occur, it will be necessary to overcome an enormous obstacle: that of conveniently and inexpensively converting the countless hours of existing recordings, regardless of whether they’re mono, stereo, or multichannel surround sound (5.1, 7.1, and so on). No one knows exactly how many songs have been recorded, but according to the entertainment-metadata concern Gracenote, 
	<a href="https://www.musicianwave.com/how-many-songs-are-there-in-the-world/" target="_blank">more than 200 million</a> recorded songs are available now on planet Earth. Given that the average duration of a song is about 3 minutes, this is the equivalent of about 1,100 years of music.
</p><h3>Measuring a Head-Related Transfer Function</h3><br/><p>To provide a high degree of spatial realism for a listener, you need to precisely map the details of how that listener’s unique head shape, ears, and nasal cavity affect how he or she hears sound. This is done by determining the listener’s head-related transfer function, which is accomplished by playing sounds from a variety of angles and recording how the user’s head affects the sounds at each position.</p><h3></h3><br/><img alt="Image of a human body dummy on the floor atop a audio device." class="rm-shortcode" data-rm-shortcode-id="9edc7ef9e31b0edad76b23a638712df8" data-rm-shortcode-name="rebelmouse-image" id="f09a7" loading="lazy" src="https://spectrum.ieee.org/media-library/image-of-a-human-body-dummy-on-the-floor-atop-a-audio-device.png?id=31310099&width=980"/><h3></h3><br/><img alt="Image of a human body dummy with labels showing the process of audio." class="rm-shortcode" data-rm-shortcode-id="4ab15dbdccedd7c29a7e21352ee7fb5d" data-rm-shortcode-name="rebelmouse-image" id="0a9eb" loading="lazy" src="https://spectrum.ieee.org/media-library/image-of-a-human-body-dummy-with-labels-showing-the-process-of-audio.jpg?id=31413633&width=980"/><h3></h3><br/><p>That is a <em>lot</em> of music. Any attempt to popularize a new audio format, no matter how promising, is doomed to fail unless it includes technology that makes it possible for us to listen to all this existing audio with the same ease and convenience with which we now enjoy stereo music—in our homes, at the beach, on a train, or in a car.</p><p>We have developed such a technology. Our system, which we call <a href="https://licreative.com/3daudio/" target="_blank">3D Soundstage</a>, permits music playback in soundstage on smartphones, ordinary or smart speakers, headphones, earphones, laptops, TVs, soundbars, and in vehicles. Not only can it convert mono and stereo recordings to soundstage, it also allows a listener with no special training to reconfigure a sound field according to their own preference, using a graphical user interface. For example, a listener can assign the locations of each instrument and vocal sound source and adjust the volume of each—changing the relative volume of, say, vocals in comparison with the instrumental accompaniment. The system does this by leveraging artificial intelligence (AI), virtual reality, and digital signal processing (more on that shortly).</p><p><strong>To re-create convincingly</strong> the sound coming from, say, a string quartet in two small speakers, such as the ones available in a pair of headphones, requires a great deal of technical finesse. To understand how this is done, let’s start with the way we perceive sound.</p><p>When sound travels to your ears, unique characteristics of your head—its physical shape, the shape of your outer and inner ears, even the shape of your nasal cavities—change the audio spectrum of the original sound. Also, there is a very slight difference in the arrival time from a sound source to your two ears. From this spectral change and the time difference, your brain perceives the location of the sound source. The spectral changes and time difference can be modeled mathematically as <a href="https://sites.tufts.edu/eeseniordesignhandbook/files/2017/05/Purple_Cirone_F2.pdf" rel="noopener noreferrer" target="_blank">head-related transfer functions (HRTFs)</a>. For each point in three-dimensional space around your head, there is a pair of HRTFs, one for your left ear and the other for the right.</p><p>So, given a piece of audio, we can process that audio using a pair of HRTFs, one for the right ear, and one for the left. To re-create the original experience, we would need to take into account the location of the sound sources relative to the microphones that recorded them. If we then played that processed audio back, for example through a pair of headphones, the listener would hear the audio with the original cues, and perceive that the sound is coming from the directions from which it was originally recorded.</p><p>If we don’t have the original location information, we can simply assign locations for the individual sound sources and get essentially the same experience. The listener is unlikely to notice minor shifts in performer placement—indeed, they might prefer their own configuration.</p><h3></h3><br/><p>Even now, after 150 years of development, the sound we hear from even a high-end audio system falls far short of what we hear when we are physically present at a live music performance.</p><h3></h3><br/><p>There are many commercial apps that use <a href="https://www.researchgate.net/publication/343130490_Measurement_of_Head-Related_Transfer_Functions_A_Review" target="_blank">HRTF</a>s to create spatial sound for listeners using headphones and earphones. One example is Apple’s <a href="https://www.makeuseof.com/spatial-audio-vs-spatialize-stereo/" target="_blank">Spatialize Stereo</a>. This technology applies HRTFs to playback audio so you can perceive a spatial sound effect—a deeper sound field that is more realistic than ordinary stereo. Apple also offers a head-tracker version that uses sensors on the iPhone and AirPods to track the relative direction between your head, as indicated by the AirPods in your ears, and your iPhone. It then applies the HRTFs associated with the direction of your iPhone to generate spatial sounds, so you perceive that the sound is coming from your iPhone. This isn’t what we would call soundstage audio, because instrument sounds are still mixed together. You can’t perceive that, for example, the violin player is to the left of the viola player.</p><p>Apple does, however, have a product that attempts to provide soundstage audio: <a href="https://www.apple.com/newsroom/2021/05/apple-music-announces-spatial-audio-and-lossless-audio/" target="_blank">Apple Spatial Audio</a>. It is a significant improvement over ordinary stereo, but it still has a couple of difficulties, in our view. One, it incorporates <a href="https://www.dolby.com/technologies/dolby-atmos/" rel="noopener noreferrer" target="_blank">Dolby Atmos</a>, a surround-sound technology developed by Dolby Laboratories. Spatial Audio applies a set of HRTFs to create spatial audio for headphones and earphones. However, the use of Dolby Atmos means that all existing stereophonic music would have to be remastered for this technology. Remastering the millions of songs already recorded in mono and stereo would be basically impossible. Another problem with Spatial Audio is that it can only support headphones or earphones, not speakers, so it has no benefit for people who tend to listen to music in their homes and cars.</p><p><strong>So how does</strong> our system achieve realistic soundstage audio? We start by using <a href="https://www.ibm.com/cloud/learn/machine-learning" rel="noopener noreferrer" target="_blank">machine-learning</a> software to separate the audio into multiple isolated tracks, each representing one instrument or singer or one group of instruments or singers. This separation process is called upmixing. A producer or even a listener with no special training can then recombine the multiple tracks to re-create and personalize a desired sound field.</p><p>Consider a song featuring a quartet consisting of guitar, bass, drums, and vocals. The listener can decide where to “locate” the performers and can adjust the volume of each, according to his or her personal preference. Using a touch screen, the listener can virtually arrange the sound-source locations and the listener’s position in the sound field, to achieve a pleasing configuration. The graphical user interface displays a shape representing the stage, upon which are overlaid icons indicating the sound sources—vocals, drums, bass, guitars, and so on. There is a head icon at the center, indicating the listener’s position. The listener can touch and drag the head icon around to change the sound field according to their own preference.</p><p>Moving the head icon closer to the drums makes the sound of the drums more prominent. If the listener moves the head icon onto an icon representing an instrument or a singer, the listener will hear that performer as a solo. The point is that by allowing the listener to reconfigure the sound field, 3D Soundstage adds new dimensions (if you’ll pardon the pun) to the enjoyment of music.</p><p>The converted soundstage audio can be in two channels, if it is meant to be heard through headphones or an ordinary left- and right-channel system. Or it can be multichannel, if it is destined for playback on a multiple-speaker system. In this latter case, a soundstage audio field can be created by two, four, or more speakers. The number of distinct sound sources in the re-created sound field can even be greater than the number of speakers.</p><h3>An Audio Taxonomy</h3><br/><img alt="Image of a chart showing multiple audio types and examples of audio types." class="rm-shortcode" data-rm-shortcode-id="a4ae0a4a6afa2a76e4bd7d153769ae64" data-rm-shortcode-name="rebelmouse-image" id="c35f5" loading="lazy" src="https://spectrum.ieee.org/media-library/image-of-a-chart-showing-multiple-audio-types-and-examples-of-audio-types.jpg?id=31307874&width=980"/><p class="caption">
	For a listener seeking a high degree of spatial realism, a variety of audio formats and systems are now available for enjoyment through speakers or headphones. On the low end, ordinary mono and stereo recordings provide a minimal spatial-perceptual experience. In the middle range, multichannel recordings, such as 5.1 and 7.1 surround sound, offer somewhat higher levels of spatial realism. At the highest levels are audio systems that start with the individual, separated instrumental tracks of a recording and recombine them, using audio techniques and tools such as head-related transfer functions, to provide a highly realistic spatial experience.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><h3></h3><br/><p>This multichannel approach should not be confused with ordinary <a href="https://thehometheaterdiy.com/surround-sound-channels-explained/" target="_blank">5.1 and 7.1 surround sound</a>. These typically have five or seven separate channels and a speaker for each, plus a subwoofer (the “.1”). The multiple loudspeakers create a sound field that is more immersive than a standard two-speaker stereo setup, but they still fall short of the realism possible with a true soundstage recording. When played through such a multichannel setup, our 3D Soundstage recordings bypass the 5.1, 7.1, or any other special audio formats, including multitrack audio-compression standards.</p><p>A word about these standards. In order to better handle the data for improved surround-sound and immersive-audio applications, new standards have been developed recently. These include the MPEG-H 3D audio standard for immersive spatial audio with Spatial Audio Object Coding (SAOC). These new standards succeed various multichannel audio formats and their corresponding coding algorithms, such as Dolby Digital AC-3 and DTS, which were developed decades ago.</p><p>While developing the new standards, the experts had to take into account many different requirements and desired features. People want to interact with the music, for example by altering the relative volumes of different instrument groups. They want to stream different kinds of multimedia, over different kinds of networks, and through different speaker configurations. SAOC was designed with these features in mind, allowing audio files to be efficiently stored and transported, while preserving the possibility for a listener to adjust the mix based on their personal taste.</p><p>To do so, however, it depends on a variety of standardized coding techniques. To create the files, SAOC uses an encoder. The inputs to the encoder are data files containing sound tracks; each track is a file representing one or more instruments. The encoder essentially compresses the data files, using standardized techniques. During playback, a decoder in your audio system decodes the files, which are then converted back to the multichannel analog sound signals by digital-to-analog converters.</p><p>Our 3D Soundstage technology bypasses this. We use mono or stereo or multichannel audio data files as input. We separate those files or data streams into multiple tracks of isolated sound sources, and then convert those tracks to two-channel or multichannel output, based on the listener’s preferred configurations, to drive headphones or multiple loudspeakers. We use AI technology to avoid multitrack rerecording, encoding, and decoding.</p><p><strong>In fact, one</strong> of the biggest technical challenges we faced in creating the 3D Soundstage system was writing that machine-learning software that separates (or upmixes) a conventional mono, stereo, or multichannel recording into multiple isolated tracks in real time. The software runs on a <a href="https://www.ibm.com/cloud/learn/neural-networks" target="_blank">neural network</a>. We developed this approach for music separation in 2012 and described it in patents that were awarded in <a href="https://patents.google.com/patent/US11240621B2/en?oq=US+11%2c240%2c621+B2" rel="noopener noreferrer" target="_blank">2022</a> and <a href="https://patents.google.com/patent/US9131305B2/en?oq=US+9%2c131%2c305" rel="noopener noreferrer" target="_blank">2015</a> (the U.S. patent numbers are <a href="https://patents.google.com/patent/US11240621B2/en?oq=US+11%2c240%2c621+B2" rel="noopener noreferrer" target="_blank">11,240,621 B2</a> and <a href="https://patents.google.com/patent/US9131305B2/en?oq=US+9%2c131%2c305" rel="noopener noreferrer" target="_blank">9,131,305 B2</a>).</p><h3></h3><br/><p>The listener can decide where to “locate” the performers and can adjust the volume of each, according to his or her personal preference.</p><h3></h3><br/><p>A typical session has two components: training and upmixing. In the training session, a large collection of mixed songs, along with their isolated instrument and vocal tracks, are used as the input and target output, respectively, for the neural network. The training uses machine learning to optimize the neural-network parameters so that the output of the neural network—the collection of individual tracks of isolated instrument and vocal data—matches the target output.</p><p>A neural network is very loosely modeled on the brain. It has an input layer of nodes, which represent biological neurons, and then many intermediate layers, called “hidden layers.” Finally, after the hidden layers there is an output layer, where the final results emerge. In our system, the data fed to the input nodes is the data of a mixed audio track. As this data proceeds through layers of hidden nodes, each node performs computations that produce a sum of weighted values. Then a nonlinear mathematical operation is performed on this sum. This calculation determines whether and how the audio data from that node is passed on to the nodes in the next layer.</p><p>There are dozens of these layers. As the audio data goes from layer to layer, the individual instruments are gradually separated from one another. At the end, in the output layer, each separated audio track is output on a node in the output layer.</p><p>That’s the idea, anyway. While the neural network is being trained, the output may be off the mark. It might not be an isolated instrumental track—it might contain audio elements of two instruments, for example. In that case, the individual weights in the weighting scheme used to determine how the data passes from hidden node to hidden node are tweaked and the training is run again. This iterative training and tweaking goes on until the output matches, more or less perfectly, the target output.</p><p>As with any training data set for machine learning, the greater the number of available training samples, the more effective the training will ultimately be. In our case, we needed tens of thousands of songs and their separated instrumental tracks for training; thus, the total training music data sets were in the thousands of hours.</p><p>After the neural network is trained, given a song with mixed sounds as input, the system outputs the multiple separated tracks by running them through the neural network using the system established during training.</p><h3>Unmixing Audio With a Neural Network</h3><br/><img alt="A diagram depicts a neural network being used to separate a piece of audio into its component tracks." class="rm-shortcode" data-rm-shortcode-id="0d68b1ff73d42976ce0b89cd66682fe4" data-rm-shortcode-name="rebelmouse-image" id="f09a3" loading="lazy" src="https://spectrum.ieee.org/media-library/a-diagram-depicts-a-neural-network-being-used-to-separate-a-piece-of-audio-into-its-component-tracks.jpg?id=31307281&width=980"/><p>
<strong>After separating a</strong> recording into its component tracks, the next step is to remix them into a soundstage recording. This is accomplished by a soundstage signal processor. This soundstage processor performs a complex computational function to generate the output signals that drive the speakers and produce the soundstage audio. The inputs to the generator include the isolated tracks, the physical locations of the speakers, and the desired locations of the listener and sound sources in the re-created sound field. The outputs of the soundstage processor are multitrack signals, one for each channel, to drive the multiple speakers.<br/>
</p><p>
	The sound field can be in a physical space, if it is generated by speakers, or in a virtual space, if it is generated by headphones or earphones. The function performed within the soundstage processor is based on computational acoustics and psychoacoustics, and it takes into account sound-wave propagation and interference in the desired sound field and the HRTFs for the listener and the desired sound field.
</p><p>
	For example, if the listener is going to use earphones, the generator selects a set of HRTFs based on the configuration of desired sound-source locations, then uses the selected HRTFs to filter the isolated sound-source tracks. Finally, the soundstage processor combines all the HRTF outputs to generate the left and right tracks for earphones. If the music is going to be played back on speakers, at least two are needed, but the more speakers, the better the sound field. The number of sound sources in the re-created sound field can be more or less than the number of speakers.
</p><p>
	We released our first soundstage app, for the iPhone, in 2020. It lets listeners configure, listen to, and save soundstage music in real time—the processing causes no discernible time delay. The app, called 
	<a href="https://3dmusica.com/" rel="noopener noreferrer" target="_blank">3D Musica</a>, converts stereo music from a listener’s personal music library, the cloud, or even streaming music to soundstage in real time. (For karaoke, the app can remove vocals, or output any isolated instrument.)
</p><p>
	Earlier this year, we opened a Web portal,
	<a href="http://www.3dsoundstage.com/" rel="noopener noreferrer" target="_blank"> 3dsoundstage.com</a>, that provides all the features of the 3D Musica app in the cloud plus an application programming interface (API) making the features available to streaming music providers and even to users of any popular Web browser. Anyone can now listen to music in soundstage audio on essentially any device.
</p><p class="pull-quote">
	When sound travels to your ears, unique characteristics of your head—its physical shape, the shape of your outer and inner ears, even the shape of your nasal cavities—change the audio spectrum of the original sound.
</p><p>
	We also developed separate versions of the 3D Soundstage software for vehicles and home audio systems and devices to re-create a 3D sound field using two, four, or more speakers. Beyond music playback, we have high hopes for this technology in videoconferencing. Many of us have had the fatiguing experience of attending videoconferences in which we had trouble hearing other participants clearly or being confused about who was speaking. With soundstage, the audio can be configured so that each person is heard coming from a distinct location in a virtual room. Or the “location” can simply be assigned depending on the person’s position in the grid typical of Zoom and other videoconferencing applications. For some, at least, videoconferencing will be less fatiguing and speech will be more intelligible.
</p><p>
<strong>Just as audio</strong> moved from mono to stereo, and from stereo to surround and spatial audio, it is now starting to move to soundstage. In those earlier eras, audiophiles evaluated a sound system by its fidelity, based on such parameters as bandwidth, 
	<a href="https://www.soundguys.com/what-is-distortion-thd-47149/" rel="noopener noreferrer" target="_blank">harmonic distortion</a>, data resolution, response time, lossless or lossy data compression, and other signal-related factors. Now, soundstage can be added as another dimension to sound fidelity—and, we dare say, the most fundamental one. To human ears, the impact of soundstage, with its spatial cues and gripping immediacy, is much more significant than incremental improvements in fidelity. This extraordinary feature offers capabilities previously beyond the experience of even the most deep-pocketed audiophiles.
</p><p>
	Technology has fueled previous revolutions in the audio industry, and it is now launching another one. Artificial intelligence, virtual reality, and digital signal processing are tapping in to psychoacoustics to give audio enthusiasts capabilities they’ve never had. At the same time, these technologies are giving recording companies and artists new tools that will breathe new life into old recordings and open up new avenues for creativity. At last, the century-old goal of convincingly re-creating the sounds of the concert hall has been achieved. 
	<span class="ieee-end-mark"></span>
</p><p>
<em>This article appears in the October 2022 print issue as “How Audio Is Getting Its Groove Back.”</em>
</p>]]></description><pubDate>Sat, 10 Sep 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/3d-audio</guid><category>3d audio</category><category>Consumer electronics</category><category>Immersive audio</category><category>Music playback</category><category>Music recording</category><category>Stereo</category><category>Surround sound</category><category>Virtual reality</category><category>Audio</category><dc:creator>Qi “Peter” Li</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/image-containing-multiple-aspects-such-as-instruments-and-left-and-right-open-hands.jpg?id=31304511&amp;width=980"></media:content></item><item><title>Get the Rohde &amp; Schwarz EMI White Paper</title><link>https://www.rohde-schwarz.com/solutions/test-and-measurement/emc-testing/emi-debugging/white-paper-how-to-measure-and-reduce-common-mode-emi-registration_254960.html?cid=010_de_eml-nl_130_ieee_22-09_int__how-to-measure-and-reduce_eblast_text-ad____MM-1080685</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=26851523&width=980"/><br/><br/><p>Nowadays, electric machines are often driven by power electronic converters. Even though the use of converters brings with it a variety of advantages, common mode (CM) signals are a frequent problem in many installations. Common mode voltages induced by the converter drive common mode currents damage the motor bearings over time and significantly reduce the lifetime of the drive. </p><p><a href="https://www.rohde-schwarz.com/solutions/test-and-measurement/emc-testing/emi-debugging/white-paper-how-to-measure-and-reduce-common-mode-emi-registration_254960.html?cid=010_de_eml-nl_130_ieee_22-09_int__how-to-measure-and-reduce_eblast_text-ad____MM-1080685" rel="noopener noreferrer" target="_blank">Download this free whitepaper now!</a></p><hr/><p>Hence, it’s essential to measure these common mode quantities in order to take suitable countermeasures. Handheld oscilloscopes in combination with Rogowski probes offer a simple and reliable way to accurately determine the required quantities and the effectiveness of different countermeasures.<u></u><u></u></p>]]></description><pubDate>Thu, 08 Sep 2022 12:22:00 +0000</pubDate><guid>https://www.rohde-schwarz.com/solutions/test-and-measurement/emc-testing/emi-debugging/white-paper-how-to-measure-and-reduce-common-mode-emi-registration_254960.html?cid=010_de_eml-nl_130_ieee_22-09_int__how-to-measure-and-reduce_eblast_text-ad____MM-1080685</guid><category>Rohde schwarz</category><category>Electromagnetic interference</category><category>Oscilloscopes</category><category>Power electronic converters</category><category>Power electronics</category><category>Type:whitepaper</category><dc:creator>Rohde &amp; Schwarz</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851523/origin.png"></media:content></item><item><title>Weekend DIY Engineering: Get Your Free Collection of 10 Hands-On Projects Now</title><link>https://engineeringresources.spectrum.ieee.org/free/w_defa3127/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/weekend-diy-engineering-get-your-free-collection-of-10-hands-on-projects-now.gif?id=31520833&width=1200&height=800&coordinates=92%2C0%2C93%2C0"/><br/><br/><p>Articles include:</p> <ul> <li>Ebike "how-tos" that teach you how to convert a conventional bike to a battery powered one. Also, how you can use a bicycle as a fallback power source.</li> <li>A homebrew CPU introduces you to to RISC-V, while a kinetic sculpture demonstrates a development board intended to ease newbies into the mysteries of programming FPGAs.</li> <li>Spectrum editors show you how to upgrade an analog portable TV into a "Geek TV" and how to build a complete 8-bit home computer capable of color graphics with just five chips!</li> <li>The Watchy is a minimalist smartwatch that's pretty much the philosophical opposite of the Apple Watch. Also we profile a phone-sized plug-in for a handheld ham radio that makes it easy to exchange text messages without the need for a cellular network.</li> <li>Our article on building an audio amplifier that punches way above its price tag provoked a big reader response—and inspired a second article, an amp that's not just inexpensive but has digital volume control and is Web-enabled to boot.</li> </ul>]]></description><pubDate>Tue, 06 Sep 2022 15:42:27 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_defa3127/prgm.cgi</guid><category>Type:whitepaper</category><category>Diy</category><category>Engineering</category><dc:creator>IEEE Spectrum</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/weekend-diy-engineering-get-your-free-collection-of-10-hands-on-projects-now.gif?id=31520833&amp;width=980"></media:content></item><item><title>Why Is Meta’s Metaverse Such a Misfire?</title><link>https://spectrum.ieee.org/metaverse-meta-misfire</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-selfie-of-mark-zuckerberg-s-avatar-in-the-metaverse-platform-horizon-worlds.jpg?id=31365998&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Mark Zuckerberg isn’t a great ambassador for the metaverse. </p><p>Meta’s CEO kicked off another round of controversy with a screenshot celebrating the launch of Horizon Worlds, the company’s AR/VR metaverse platform, <a href="https://www.facebook.com/zuck/posts/pfbid036wZ6kRhzuu7rX4kKeiyhKtodHzwyujoXvCGZ8dCXQ4RXUke96PzJcxHAyVecY1mDl?__cft__%5b0%5d=AZVN3_jkwKKvP6qk_3fwvMr-1qdPR3Dg3u8rBZW2JQQgpg8_VImIOlFanh4eX4XDzc8EdCDKPV4OV_ILjEfWVk0y2RNZup5pgic7j8ViFzMV088dbafZoRC9urHEp17KW1A&__tn__=%2CO%2CP-R" rel="noopener noreferrer" target="_blank">in France and Spain</a>. Shot in the style of a selfie, it shows a poorly detailed rendition of Zuckerberg’s avatar staring past the camera. Crude 3D models of national landmarks sit behind him on a generic green landscape.</p><p>“It was a horrific PR move to put out those photos,” says Stu Richards (a.k.a. <a href="https://www.hundo.xyz/stories/meet-the-metaversers-meta-mike" rel="noopener noreferrer" target="_blank">Meta Mike</a>), partner success lead at GigLabs and cdofounder of Versed.</p><hr/><h2>Meta’s metaverse hype leads to real-world backlash </h2><p>Zuckerberg’s virtual selfie quickly went viral across numerous social media accounts. A tweet by user @ordinarytings, <a href="https://twitter.com/ordinarytings/status/1559520901443510274?cxt=HHwWhICz5ej_w6QrAAAA" rel="noopener noreferrer" target="_blank">which claimed Horizon Worlds is “surely dying in the dark,”</a> led the charge with more than 31,000 likes and over 4,500 quote tweets or retweets. </p><p>It’s not unusual for a tech CEO to receive a thrashing on Twitter, but the scale of the response–boosted by <a href="https://mashable.com/article/best-metaverse-tweets" rel="noopener noreferrer" target="_blank">Mashable</a>, <a href="https://www.dailydot.com/debug/mark-zuckerberg-metaverse-selfie-horizon-worlds/" rel="noopener noreferrer" target="_blank">The Daily Dot</a>, and <a href="https://kotaku.com/metaverse-meta-facebook-selfie-vr-zuckerberg-ugly-1849424613" rel="noopener noreferrer" target="_blank">Kotaku</a>–was suffocating. It’s hard to mount any defense of Meta’s ugly, simplistic screenshot. “I think the response is fair,” says Richards. “I’ve not been super impressed by what they’ve put out.” </p><p>Clearly, Zuckerberg’s post did not go as planned. But this raises the question: why?</p><p class="pull-quote">“If they’re going to use game tech to build a VR game platform that’s supposed to be a cross between a Roblox-style UGC platform and a social MMO, maybe they should have people who have experience.”<br/>—Rafael Brown, Symbol Zero</p><p>Rafael Brown, CEO of metaverse event company <a href="https://www.symbolzero.com/" rel="noopener noreferrer" target="_blank">Symbol Zero</a> and former game designer, thinks the company’s metaverse issues are rooted in difficulty keeping up with the level of fidelity common in the game industry.<br/></p><p>“Facebook is out of touch with game-style software-development practices and expectations on art direction and character/avatar development,” says Brown. “Keep in mind their other internal projects like Quill, et cetera, that they’ve jettisoned and lost [staff over].” </p><p>Brown points out that Meta’s avatars have changed numerous times since the company’s <a href="https://about.fb.com/news/2014/03/facebook-to-acquire-oculus/" target="_blank">purchase of VR hardware maker Oculus in 2014</a>. These changes can be witnessed in other, past controversies, <a href="https://www.youtube.com/watch?v=z8q2BQOGRGE" rel="noopener noreferrer" target="_blank">such as Zuckerberg’s ill-advised AR tour of Puerto Rico</a> in the wake of Hurricane Maria. The avatars used then are different from today’s and<a href="https://www.youtube.com/watch?v=qOfpu4RUjuo&t=1s" rel="noopener noreferrer" target="_blank"> radically different from the ghostlike avatars shown by Oculus in 2016</a>. </p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="21cf79453d0cd299091b3e56d7333772" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/qOfpu4RUjuo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This is how the Oculus avatars looked in 2016.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=qOfpu4RUjuo" target="_blank">www.youtube.com</a>
</small>
</p><p>There are other signs of instability. Meta’s VP of Horizon, Vivek Sharma<a href="https://www.reuters.com/technology/meta-head-virtual-reality-platform-horizon-leaving-company-2022-08-26/" rel="noopener noreferrer" target="_blank">, recently said he’s leaving the company for “a new opportunity.”</a> Days later, the company<a href="https://techcrunch.com/2022/08/30/meta-shutting-down-facebook-gaming-app/" rel="noopener noreferrer" target="_blank"> announced it will shut down the Facebook Gaming app</a>, a competitor to Amazon’s live-streaming platform Twitch, which eliminates an important avenue Meta could use to reach users. </p><p>“All I wonder is, if they’re going to use game tech to build a VR game platform that’s supposed to be a cross between a Roblox-style UGC platform and a social MMO, maybe they should have people who have experience,” says Brown. “They really need better art direction, technical art direction, game direction, and tools direction.”</p><h2>Where does Meta go from here?</h2><p>Zuckerberg tried to quell criticism with <a href="https://www.instagram.com/p/Chc8I1VBzk6/?utm_source=ig_embed&ig_rid=f51c6c6e-e5df-46f4-a561-7816f6804f38" rel="noopener noreferrer" target="_blank">a follow-up post about a planned update to avatar graphics</a>. It’s an improvement, to be sure. But the real glimmer of hope was shown at Siggraph, a computer graphics conference held in August of 2022. </p><p>A group of researchers from Reality Labs, Meta’s AR/VR research division, showed a paper titled <a href="https://drive.google.com/file/d/1i4NJKAggS82wqMamCJ1OHRGgViuyoY6R/view" rel="noopener noreferrer" target="_blank">“Authentic Volumetric Avatars from a Phone Scan,”</a> which describes how smartphone photos with depth-of-field data can be paired with machine learning to achieve sharp, photorealistic results with accurate real-time facial animation. The detailed expressions shown by researchers at Reality Labs stands in stark contrast to the current state of Horizon Worlds’ avatars.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="3503cd1e930e04ac7f3b44bbd23a7210" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/t7_TMD7v0Xs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This video, demonstrating authentic volumetric avatars from a phone scan, was shown  at Siggraph 2022.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=t7_TMD7v0Xs" target="_blank">www.youtube.com</a>
</small>
</p><p>Richards notes that Meta’s current mainstream headset, the Meta Quest 2, may be partially responsible for the Horizon Worlds’ limitations. “[Meta is] building out tech that will have the mechanics in place to better focus on things like expression,” says Richards—but the affordable Quest 2 opted not to include sensors that can gauge users’ expression or track their eyes. “They’re trying to create adoption first. Once that happens is when they’ll focus more on integrating features.”</p><p>Meta might be ready to turn that corner with a headset to be announced at <a href="https://www.reddit.com/r/OculusQuest/comments/wlxxhu/when_is_oculusfacebookmeta_connect_2022/" target="_blank">Oculus Connect 2022</a>. While most details remain under wraps, Zuckerberg offered an early overview of its features <a href="https://open.spotify.com/episode/51gxrAActH18RGhKNza598" rel="noopener noreferrer" target="_blank">during a recent interview on <em>The Joe Rogan Experience</em></a>. </p><p>Zuckerberg said the upcoming, yet unnamed headset will offer “the ability to now have eye contact in virtual reality, have your face be tracked so that your avatar is not just this still thing, if you smile, or you frown, or you pout, whatever your expression is, to have that actually in real time translate to your avatar.” His remarks sound a lot like what's already been shown at Siggraph and in other, <a href="https://www.youtube.com/watch?v=w52CziLgnAc" target="_blank">earlier Meta research demos</a>.</p><p>This could silence critiques of Horizon Worlds’ awkward, stilted graphical style—though only if it works as advertised.</p>]]></description><pubDate>Sat, 03 Sep 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/metaverse-meta-misfire</guid><category>Oculus connect</category><category>Virtual reality</category><category>Metaverse</category><category>Meta</category><category>Facebook</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-selfie-of-mark-zuckerberg-s-avatar-in-the-metaverse-platform-horizon-worlds.jpg?id=31365998&amp;width=980"></media:content></item><item><title>The Metaverse Needs Standards, Too</title><link>https://spectrum.ieee.org/metaverse-standards-forum</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-diverse-group-of-people-meeting-in-a-conference-room-in-the-metaverse.jpg?id=31247491&width=1200&height=800&coordinates=300%2C0%2C0%2C0"/><br/><br/><p>When Meta (formerly Facebook) announced in October 2021 that <a href="https://about.fb.com/news/2021/10/facebook-company-is-now-meta/" target="_blank">it would be developing metaverse technologies</a>, it prompted a flurry of speculation and attendant announcements from other companies. Beyond that, it triggered an avalanche of confusion around <a href="https://spectrum.ieee.org/metaverse-second-life" target="_self">what exactly</a> <a href="https://spectrum.ieee.org/meta-offers-nothing-new-to-the-metaverse" target="_self">the metaverse</a> <a href="https://spectrum.ieee.org/silicon-valleys-metaverse-problem" target="_self">is supposed to be</a>.<br/></p><p>Nearly a year later, the concrete details of the metaverse are as opaque as ever. The <a href="https://metaverse-standards.org/" target="_blank">Metaverse Standards Forum</a>, which launched on 21 June 2022, isn’t trying to wrangle those details—not directly. But the forum sees an opportunity to get everyone to sit down at the same (probably virtual) table and hash out the basic technologies needed. With a more solid foundation, the forum believes, the metaverse can better develop and evolve.</p><p>Now, the forum has announced that after two months of hashing priorities, it has a list of initial priority topics that will steer metaverse standards development in its domain working groups. The topics include both straightforward technical problems like augmented and virtual reality standards, as well as concerns around privacy, ethics, and user safety.</p><p class="pull-quote">“I like the theory that there’s only one metaverse, and you go between different experiences within the metaverse. Because we need an analogy to the Web.”<br/>—Neil Trevett, Metaverse Standards Forum</p><p>To be clear: The metaverse does not exist yet, and probably won’t for some years to come. But there’s enough industry interest in beginning the process toward building it—whatever it may ultimately be. So <a href="https://www.linkedin.com/in/neil-trevett-748791" rel="noopener noreferrer" target="_blank">Neil Trevett</a>, the chair of the Metaverse Standards Forum, says now is the time to start standardizing. “I think what we’re seeing, much to everyone’s surprise—including our own—is the level of interest in standards for the metaverse. I think there is a thirst, or hunger, for them.”</p><p>The Metaverse Standards Forum is being organized by the <a href="https://www.khronos.org/" rel="noopener noreferrer" target="_blank">Khronos Group</a>, a software consortium developing royalty-free standards around technologies like virtual reality, augmented reality, and vision processing. The forum began with just 35 founding members, but its roster has in two months already ballooned to 1,500.</p><h3>Standardizing the Standards</h3><p>The MSF isn’t a standards body, Trevett says, so much as it’s a liaison to improve coordination and trust between the big commercial metaverse interests to date—including Google, Meta, Microsoft, and others using the technologies required for the creation, care, and maintenance of virtual worlds—as well as the standards organizations that will define those technologies. </p><p>“What we’re doing is casting a wide net, because some of the companies and organizations in the forum know each other, but there are lots of people that don’t know each other at all,” says Trevett. Trevett is also president of the Khronos Group.</p><p>Although there are numerous competing visions of what the metaverse will be, many such visions have common points of connection. <a href="https://spectrum.ieee.org/the-metaverse-could-help-us-better-understand-reality" target="_self">Virtual reality</a> (VR) and <a href="https://spectrum.ieee.org/metaverse-niantic-augmented-reality" target="_self">augmented reality</a> (AR) tech will probably be involved. As will the drive toward shared experiences—whether that’s something exciting like <a href="https://www.hollywoodreporter.com/business/digital/iheartmedia-iheartland-fortnite-1235204209/" target="_blank">attending a concert</a> or more mundane like renewing your drivers license.</p><p>“I like the theory that there’s only one metaverse and you go between different experiences within the metaverse, because we need an analogy to the Web,” says Trevett.</p><p>“It’s going to be an evolution of the Internet as we know it today,” says <a href="https://www.linkedin.com/in/david-morin" target="_blank">David Morin</a>, the executive director of the <a href="https://www.aswf.io/" target="_blank">Academy Software Foundation</a>, an organization working to standardize the visual-effects technologies pioneered by the movie industry. Morin says one way to envision that evolution is as a collection of situations to experience—a virtual Amazon storefront in which to browse products, rather than a flat Webpage—with the option to move between one situation and another. The idea isn’t entirely novel, he adds: "Many of these thoughts and experiments already exist in some corner of an industry or another.”</p><p>“I think most people have in mind something like the web,” says <a href="https://fr.linkedin.com/in/dominiquehazaelmassieux/fr" rel="noopener noreferrer" target="_blank">Dominque Hazael-Massieux</a>, part of the management team for the <a href="https://www.w3.org/" rel="noopener noreferrer" target="_blank">World Wide Web Consortium</a> standards body. ”You go to one place and another, and it just works, and you can keep some of the data you have from one place to another.”</p><h3>Making the Virtual a Reality</h3><p>So how will the metaverse <a href="https://spectrum.ieee.org/is-the-metaverse-even-feasible" target="_blank">feasibly be realized</a>?<br/></p><p>Some see it as a future iteration of (or replacement for) the Internet. But if so, the technologies that make it happen will need to be  more tightly integrated than they are now. Beyond standardizing just AR and VR come related technologies like 3D modeling, volumetric video, and geospatial data.</p><p>According to <strong></strong>Trevett, the Metaverse Standards Forum’s first meetings—held via Zoom—yielded a Google spreadsheet that spelled out the forum’s priority topics, enabling members to garner support for their own preferences.</p><p> “We’re seeing a number of domains emerging,” says Trevett. “At the highest level, there’s spatial computing—which is all the 3D stuff and 3D assets—avatars and apparel, geospatial, [extended reality], and user interface.”<strong> </strong>Trevett points to additional interest in user identity, privacy, ethics, openness, and accessibility concerns.</p><p><a href="https://www.linkedin.com/in/nadinealameh" target="_blank">Nadine Alameh</a>, the CEO of the <a href="https://www.ogc.org/" target="_blank">Open Geospatial Consortium</a>, which is developing geospatial and location data standards, says part of the forum’s appeal is that, beyond the nuts and bolts of technical standards, there simply hasn’t been a centralized place to talk about the vision and implications of the technology. “Because some of these conversations that are happening are not about standards at all,” she says. “But people don’t have another forum to talk about the vision of the metaverse.”</p><p>In all, members proposed more than 200 potential topics to prioritize for metaverse development. That list <a href="https://metaverse-standards.org/news/blog/setting-course-for-an-open-metaverse-part-1-updates-from-the-first-60-days-of-the-metaverse-standards-forum/" target="_blank">has been narrowed down to eight areas of interest</a>, which will form the first domain working groups for the forum’s members to join. The list includes more technical challenges, such as interoperable 3D assets, user identity, augmented and virtual reality, and user interfaces. Broader challenges are also highlighted, including metaverse ethics, privacy and governance, and education and certification. </p><p>Trevett calls the forum’s approach “Darwinian.” It’s focusing for now on foundational matters like technical standards. Then, how it functions and is used will remain up to member companies like Amazon, Google, Meta, Microsoft, and others. </p><p>Many of the forum members that <em>IEEE Spectrum</em> spoke with felt strongly about the problem of metaverse openness. Open standards—as opposed to proprietary ones—make a shared foundation with increased interoperability more likely. By analogy, think of the Worldwide Web’s <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" target="_blank">HTTP protocol</a>. Were it a closed or proprietary standard, multiple “webs” would likely have cropped up with varying user bases as well as technological standards. And whatever the proprietary “Web” would have ultimately become, it’d also likely have taken much longer to develop and blossom, if it ever would have blossomed at all.</p><p>As a second analogy, the wireless industry began working on 6G cellular standards in 2019. But 6G likely <a href="https://spectrum.ieee.org/6g-geopolitics" target="_self">won’t come to fruition until 2030</a>. Building the metaverse will be a multiyear effort, if not a generational one. “We’re just doing the building blocks,” says Alameh. “It’s really our kids who will actually build whatever that is. We’re just enabling, and I think that’s why you need to inject the ethics and the responsibility, because you’re building something new.”</p>]]></description><pubDate>Wed, 31 Aug 2022 20:07:02 +0000</pubDate><guid>https://spectrum.ieee.org/metaverse-standards-forum</guid><category>Metaverse</category><category>Standards</category><category>Internet</category><category>Virtual reality</category><category>Augmented reality</category><dc:creator>Michael Koziol</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-diverse-group-of-people-meeting-in-a-conference-room-in-the-metaverse.jpg?id=31247491&amp;width=980"></media:content></item><item><title>Get the Rohde &amp; Schwarz EMI White Paper</title><link>https://engineeringresources.spectrum.ieee.org/free/w_rohd49/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/get-the-rohde-schwarz-emi-white-paper.gif?id=31364531&width=1200&height=800&coordinates=71%2C0%2C72%2C0"/><br/><br/><p>Nowadays, electric machines are often driven by power electronic converters. Even though the use of converters brings with it a variety of advantages, common mode (CM) signals are a frequent problem in many installations. Common mode voltages induced by the converter drive common mode currents damage the motor bearings over time and significantly reduce the lifetime of the drive. Hence, it is essential to measure these common mode quantities in order to take suitable countermeasures. Handheld oscilloscopes in combination with Rogowski probes offer a simple and reliable way to accurately determine the required quantities and the effectiveness of different counter measures.</p>]]></description><pubDate>Wed, 31 Aug 2022 15:41:41 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_rohd49/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Rohde &amp; Schwarz</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/get-the-rohde-schwarz-emi-white-paper.gif?id=31364531&amp;width=980"></media:content></item><item><title>Digging Into the New QD-OLED TVs</title><link>https://spectrum.ieee.org/qd-oled</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/television-screen-displaying-closeup-of-crystals.jpg?id=30538375&width=1200&height=800&coordinates=0%2C52%2C0%2C52"/><br/><br/><div class="ieee-editors-note">
<em>Televisions and computer monitors with QD-OLED displays are now on store shelves. The image quality is—as expected—impressive, with amazing black levels, wide viewing angles, a broad color gamut, and high brightness. The products include:</em>
</div><ul>
<li><em><a href="https://www.samsung.com/us/televisions-home-theater/tvs/oled-tvs/65-class-s95b-oled-4k-smart-tv-2022-qn65s95bafxza/?cid=sem-mktg-pfs-tv-us-google-na-03182022-140959-&ds_e=GOOGLE-cr:0-pl:294764987-&ds_c=FF~OLED+Brand_CN~oled_ID~na_PH~on_MK~us_BS~vd_PR~tv_SB~bitv_FS~lo_CA~kew_KS~ba_MT~exact-&ds_ag=AG~Core_ID~na_MK~us_AT~ta_MD~h_PK~cpqlh_PB~google_AI~n-&ds_k=s95b&gclid=Cj0KCQjwuaiXBhCCARIsAKZLt3nI75I1KvrG_hxQa7L3r-NF0WaHttqhxcv4CdKWUAlPl11BCtpVpB0aAjG8EALw_wcB&gclsrc=aw.ds" target="_blank">Samsung’s S95B TV</a> in 55-inch (US $2,200) and 65-inch ($3,000) sizes</em></li>
<li><em><a href="https://electronics.sony.com/tv-video/televisions/all-tvs/p/xr65a95k" rel="noopener noreferrer" target="_blank">Sony’s A95K TV</a> in 55-inch ($3,000) and 65-inch ($4,000) sizes</em></li>
<li><em><a href="https://www.dell.com/en-us/shop/alienware-34-curved-qd-oled-gaming-monitor-aw3423dw/apd/210-bcye/monitors-monitor-accessories" rel="noopener noreferrer" target="_blank">Alienware’s 34-inch gaming monitor</a> ($1,300)</em></li>
</ul><p>
<em>All these products use display panels manufactured by Samsung but have their own unique display assembly, operating system, and electronics.</em>
</p><p class="">
<em>I took apart a 55-inch Samsung S95B to learn just how these new displays are put together (destroying it in the process). I found an extremely thin OLED backplane that generates blue light with an equally thin QD color-converting structure that completes the optical stack. I used a UV light source, a microscope, and a spectrometer to learn a lot about how these displays work.</em>
</p><h3></h3><br/><img alt="rows of green squares alternating with rows of red and blue squares against a black background" class="rm-shortcode" data-rm-shortcode-id="bfb6f5e8984ba61e42885e167915afd6" data-rm-shortcode-name="rebelmouse-image" id="bc6ac" loading="lazy" src="https://spectrum.ieee.org/media-library/rows-of-green-squares-alternating-with-rows-of-red-and-blue-squares-against-a-black-background.jpg?id=30510843&width=980"/><p><em>A few surprises:</em></p><ul><li><em>The pixel layout is unique. Instead of being evenly arrayed, the green quantum dots form their own line, separate from the blue and red [see photo, above]. (The blue pixels draw their light directly from the OLED panel, the red and green pixels are lit by quantum dots.)</em></li><li><em>The bandwidth of the native QD emission is so narrow (resulting in a very wide <a href="https://www.benq.com/en-us/knowledge-center/knowledge/color-gamut-monitor.html" target="_blank">color gamut</a>, that is, the range of colors that can be produced, generally a good thing) that some content doesn’t know how to handle it. So the TV “compresses” the gamut in some cases by adding off-primary colors to bring its primary color points in line with more common gamuts. This is especially dramatic with green, where “pure” green actually contains a significant amount of added red and a small amount of added blue.</em></li><li><em>While taking this thing apart was no easy task, and deconstruction cracked the screen, I was surprised at how easily the QD frontplane and the OLED backplane could be separated. It was easier than splitting an Oreo in half. [See video, below.]</em></li></ul><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="f113cc5369a2e85db692e9d3538b7dda" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/0esg-HFbRkw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p><em>As for the name of this technology, Samsung has used the branding OLED, QD Display, and QD-OLED, while Sony is just using OLED. Alienware uses QD-OLED to describe the new tech (as do most in the display industry).</em></p><p><em>—Peter Palomaki</em></p><h3></h3><br/><p><em>Story from January 2022 follows:</em></p><p>For more than a decade now, OLED (organic light-emitting diode) displays have set the bar for screen quality, albeit at a price. That’s because they produce deep blacks, offer wide viewing angles, and have a broad color range. Meanwhile, QD (quantum dot) technologies have done a lot to improve the color purity and brightness of the more wallet-friendly LCD TVs.</p><p>In 2022, these two rival technologies will merge. The name of the resulting hybrid is still evolving, but QD-OLED seems to make sense, so I’ll use it here, although Samsung has begun to call its version of the technology QD Display.</p><p>To understand why this combination is so appealing, you have to know the basic principles behind each of these approaches to displaying a moving image.</p><p>In an LCD TV, the LED backlight, or at least a big section of it, is on all at once. The picture is created by filtering this light at the many individual pixels. Unfortunately, that filtering process isn’t perfect, and in areas that should appear black some light gets through.<br/></p><p>In OLED displays, the red, green, and blue diodes that comprise each pixel emit light and are turned on only when they are needed. So black pixels appear truly black, while bright pixels can be run at full power, allowing unsurpassed levels of contrast.</p><p>But there’s a drawback. The colored diodes in an OLED TV degrade over time, causing what’s called “burn-in.” And with these changes happening at different rates for the red, green, and blue diodes, the degradation affects the overall ability of a display to reproduce colors accurately as it ages and also causes “ghost” images to appear where static content is frequently displayed.</p><p>Adding QDs into the mix shifts this equation. Quantum dots—nanoparticles of semiconductor material—absorb photons and then use that energy to emit light of a different wavelength. In a QD-OLED display, all the diodes emit blue light. To get red and green, the appropriate diodes are covered with red or green QDs. The result is a paper-thin display with a broad range of colors that remain accurate over time. These screens also have excellent black levels, wide viewing angles, and improved power efficiency over both OLED and LCD displays.</p><p>Samsung is the driving force behind the technology, having sunk billions into retrofitting an LCD fab in Tangjeong, South Korea, for making QD-OLED displays While other companies have published articles and demonstrated similar approaches, only</p><p>Samsung has committed to manufacturing these displays, which makes sense because it holds all of the required technology in house. Having both the OLED fab and QD expertise under one roof gives Samsung a big leg up on other QD-display manufacturers.,</p><p>Samsung first announced QD-OLED plans in 2019, then pushed out the release date a few times. It now seems likely that we will see public demos in early 2022 followed by commercial products later in the year, once the company has geared up for high-volume production. At this point, Samsung can produce a maximum of 30,000 QD-OLED panels a month; these will be used in its own products. In the grand scheme of things, that’s not that much.</p><p>Unfortunately, as with any new display technology, there are challenges associated with development and commercialization.</p><p>For one, patterning the quantum-dot layers and protecting them is complicated. Unlike QD-enabled LCD displays (commonly referred to as QLED) where red and green QDs are dispersed uniformly in a polymer film, QD-OLED requires the QD layers to be patterned and aligned with the OLEDs behind them. And that’s tricky to do. Samsung is expected to employ inkjet printing, an approach that reduces the waste of QD material.</p><p>Another issue is the leakage of blue light through the red and green QD layers. Leakage of only a few percent would have a significant effect on the viewing experience, resulting in washed-out colors. If the red and green QD layers don’t do a good job absorbing all of the blue light impinging on them, an additional blue-blocking layer would be required on top, adding to the cost and complexity.</p><p>Another challenge is that blue OLEDs degrade faster than red or green ones do. With all three colors relying on blue OLEDs in a QD-OLED design, this degradation isn’t expected to cause as severe color shifts as with traditional OLED displays, but it does decrease brightness over the life of the display.</p><p>Today, OLED TVs are typically the most expensive option on retail shelves. And while the process for making QD-OLED simplifies the OLED layer somewhat (because you need only blue diodes), it does not make the display any less expensive. In fact, due to the large number of quantum dots used, the patterning steps, and the special filtering required, QD-OLED displays are likely to be more expensive than traditional OLED ones—and way more expensive than LCD TVs with quantum-dot color purification. Early adopters may pay about US $5,000 for the first QD-OLED displays when they begin selling later this year. Those buyers will no doubt complain about the prices—while enjoying a viewing experience far better than anything they’ve had before.</p>]]></description><pubDate>Thu, 04 Aug 2022 20:30:00 +0000</pubDate><guid>https://spectrum.ieee.org/qd-oled</guid><category>Qd</category><category>Tv</category><category>Quantum dots</category><category>Display</category><category>Oled</category><category>Ces</category><category>Ces 2022</category><category>Samsung</category><category>Sony</category><category>Alienware</category><dc:creator>Peter Palomaki</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/television-screen-displaying-closeup-of-crystals.jpg?id=30538375&amp;width=980"></media:content></item><item><title>AI Earbuds Solve Noisy Zoom Calls</title><link>https://spectrum.ieee.org/-2657726415</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-picture-of-two-hands-one-holding-a-clearbuds-earbud-part-of-one-of-the-first-machine-learning-systems-to-operate-in-real-tim.jpg?id=30274730&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Nothing spoils a Zoom meeting quite like that one team member who insists on dialing in from a noisy cafe. New AI-powered wireless headphones called ClearBuds promise a solution by blocking out background noise and ensuring their microphones pick up only the caller’s voice.</p><p>Speech-enhancement technology is already used in a variety of products including hearing aids, teleconferencing services like Zoom and Google Meet and wireless headphones like Apple’s AirPods Pro. The goal is to strip incoming audio of unwanted noise or distortions and boost the clarity of the speaker’s voice, either using traditional <a data-linked-post="2650278001" href="https://spectrum.ieee.org/signal-processing-finding-weakness-in-strength" target="_blank">signal-processing</a> algorithms or newer <a data-linked-post="2650278023" href="https://spectrum.ieee.org/can-machine-learning-teach-us-anything" target="_blank">machine-learning</a> approaches.<br/></p><p>They work by exploiting spatial cues that help separate out audio sources or acoustic information that can distinguish among different kinds of noises, such as speech or traffic sounds. But doing both at the same time and with a computational budget small enough to run on consumer-grade devices is a significant challenge, and most real-world systems still leave a lot to be desired.<br/></p><p>Using a clever combination of tailor-made in-ear wireless headphones, a bespoke Bluetooth protocol and a lightweight deep-learning model that can run on a smartphone, a team from the University of Washington has built a system dubbed <a href="https://clearbuds.cs.washington.edu/" target="_blank">ClearBuds</a> that almost completely cuts out background noises.<br/></p><p>“For us, ClearBuds were born out of necessity,” says <a href="https://ishanchatterjee.weebly.com/" target="_blank">Ishan Chatterjee</a>, a doctoral student and one of the colead authors of <a href="https://dl.acm.org/doi/10.1145/3498361.3538654" target="_blank">a paper</a> presented at the ACM International Conference on Mobile Systems, Applications, and Services describing the technology. He is not only a classmate, but also a roommate of the other two colead authors, doctoral students <a href="https://www.maruchikim.com/" target="_blank">Maruchi Kim</a> and <a href="https://www.vivekjayaram.com/" target="_blank">Vivek Jayaram</a>.<br/></p><p>“When the pandemic lockdown started, like many others, we found ourselves taking many calls within our house within these close quarters, and there were many noises around the house, kitchen noises, construction noises, conversations,” says Chatterjee. So they decided to pool their experience in hardware, networking, and machine learning to solve the problem.<br/></p><p>One of the biggest challenges for speech enhancement, says Jayaram, is separating out multiple voices. While recent machine-learning approaches have got good at distinguishing different kinds of sounds and using this to block out background noise, they still struggle when two people are talking at the same time.<br/></p><p>The best way to solve this problem is to use multiple microphones spaced slightly apart allowing you to triangulate the source of different noises. This makes it possible to distinguish between two speakers based on where they’re located rather than what they sound like. But for this to be effective the microphones need to be at a reasonable distance.<br/></p><p>Most commercial products have microphones in each earbud, which should be far enough apart to get decent triangulation. But streaming and syncing audio from both is beyond today’s Bluetooth standards, says Kim. That’s why Apple’s AirPods and high-end hearing aids have multiple microphones in each earbud, allowing them to do some limited triangulation before streaming from a single earbud down to the connected smartphone.<br/></p><p>To get around this, the researchers designed a custom wireless protocol that gets one of the earbuds to transmit a time-sync beacon. The second earbud then uses this signal to match it’s internal clock to its partner’s, ensuring that the two audio streams stay in lockstep. The team implemented this protocol on custom-designed earbuds made from commodity electronic components and 3D-printed the enclosures, but syncing up the streams from each earbud solved only part of the problem.<br/></p><p>The researchers wanted to take advantage of the latest deep-learning techniques to process the audio, but they also needed to run their speech-enhancement software on the smartphone paired with the earbuds. These models have significant computational budgets and most commercial products using AI for speech enhancement rely on transmitting the audio to powerful cloud servers. “A mobile phone, even the newer ones, is a fraction of the compute power of the GPU cards, which are typically used for running deep learning,” says Jayaram.<br/></p><p>Their solution was to take a preexisting neural network that can learn to detect time differences in two incoming signals, therefore allowing it to triangulate the source. They then trimmed this down to its bare bones by reducing the number of parameters and layers until it could run on a smartphone. Stripping back the network like this led to a noticeable drop in audio quality, introducing crackles, static, and pops, so the researchers fed the output into another network that learns to filter these kinds of distortions out.<br/></p><p>“The innovation was combining two different types of neural networks, each of which could be very lightweight, and in conjunction they could approach the performance of these really big neural networks that couldn’t run on an iPhone,” says Jayaram.<br/></p><p>When tested against Apple AirPods Pro, the ClearBuds achieved a higher signal-to-distortion ratio across all tests. The team also got 37 volunteers to rate audio clips from noisy real-world environments like loud restaurants or busy traffic intersections. The ones processed through ClearBuds’ neural network were found to have the best noise suppression and overall listening experience. In real-world tests, eight volunteers significantly preferred the ClearBuds over the audio equipment they’d normally use to conduct calls.<br/></p><p>The output does contain some distortions, says <a href="https://www.city.ac.uk/about/people/academics/tillman-weyde" target="_blank">Tillman Weyde</a>, reader in machine learning at City University of London, but they are not especially intrusive and overall the system is very effective at removing background noise and voices. “This is a great result from a student and academic team that has obviously put a tremendous amount of work into this project to make effective progress on a problem that affects hundreds of millions of people using wireless earbuds,” he adds.<br/></p><p><a href="https://ai.honu.io/" target="_blank">Alexandre Défossez</a>, a research scientist at Facebook AI Research Paris, says the work is very impressive but points out that one limitation is the fact that the combined time for transmitting the audio to the smartphone and then processing it is 109 milliseconds. “We always get 50 to 100 milliseconds of latency from the network,” he says. “Adding an extra 100 milliseconds is a big price to pay, and as the communication stack becomes ‘smarter’ and ‘smarter,’ we will end up with fairly noticeable and annoying delays in all our communications.”</p>]]></description><pubDate>Thu, 28 Jul 2022 18:24:05 +0000</pubDate><guid>https://spectrum.ieee.org/-2657726415</guid><category>Signal processing</category><category>Machine learning</category><category>Wireless communication</category><category>Bluetooth</category><category>Earbuds</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-picture-of-two-hands-one-holding-a-clearbuds-earbud-part-of-one-of-the-first-machine-learning-systems-to-operate-in-real-tim.jpg?id=30274730&amp;width=980"></media:content></item><item><title>Artificial Muscles Woven Into Smart Textiles Could Make Clothing Hyperfunctional</title><link>https://spectrum.ieee.org/smart-clothes-artificial-muscles</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/fabric-butterfly-on-a-tree-branch.jpg?id=30193856&width=1200&height=800&coordinates=0%2C52%2C0%2C52"/><br/><br/><p>Recent <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/admt.202000383" rel="noopener noreferrer" target="_blank">advances</a> in soft robotics have opened up possibilities for the construction of smart fibers and textiles that have a variety of mechanical, therapeutic, and wearable possibilities. These fabrics, when programmed to expand or contract through thermal, electric, fluid, or other stimuli, can produce motion, deformation, or force for different functions.<br/></p><p>Engineers at the University of New South Wales (UNSW), Sydney, Australia, have developed a new class of fluid-driven smart textiles that can “shape-shift” into 3D structures. Despite recent advances in the development of active textiles, “they are either limited with slow response times due to the requirement of heating and cooling, or difficult to knit, braid, or weave in the case of fluid-driven textiles,” says <a href="https://www.unsw.edu.au/staff/thanh-nho-do" rel="noopener noreferrer" target="_blank">Thanh Nho Do</a>, senior lecturer at the UNSW’s Graduate School of Biomedical Engineering, who led the <a href="https://www.nature.com/articles/s41598-022-15369-2#author-information" rel="noopener noreferrer" target="_blank">study</a>.</p><p>To overcome these drawbacks, the UNSW team demonstrated a proof of concept of miniature, fast-responding artificial muscles made up of long, fluid-filled silicone tubes that can be manipulated through hydraulic pressure. The silicone tube is surrounded by an outer helical coil as a constraint layer to keep it from expanding like a balloon. Due to the constraint of the outer layer, only axial elongation is possible, giving muscle the ability to expand under increased hydraulic pressure or contract when pressure is decreased. Using this mechanism, says Do, they can program a <a href="https://www.youtube.com/watch?v=VMzJ-0qt9gQ" rel="noopener noreferrer" target="_blank">wide range of motion</a> by changing the hydraulic pressure.</p><p>“A unique feature of our soft muscles compared to others is that we can tune their generated force by varying the stretch ratio of the inner silicone tube at the time they are fabricated, which provides high flexibility for use in specific applications,” Do says.</p><p>The researchers used a simple, low-cost fabrication technique, in which a long, thin silicone tube is directly inserted into a hollow microcoil to produce the artificial muscles, with a diameter ranging from a few hundred micrometers to several millimeters. “With this method, we could mass-produce soft artificial muscles at any scale and size—diameter could be down to 0.5 millimeters, and length at least 5 meters,” Do says.</p><p>The filament structure of the muscles allows them to be stored in spools and cut to meet specific length requirements. The team used two methods to create smart fibers from the artificial muscles. One was using them as active yarns to braid, weave, or knit into active fabrics using traditional textile-making technologies. The other was by integrating them directly into conventional, passive fabrics.</p><p>The combination of hydraulic pressure, fast response times, light weight, small size, and high flexibility makes the UNSW’s smart textiles versatile and programmable. According to Do, the expansion and contraction of their active fabrics is similar to those of human muscle fibers.</p><p>This versatility opens up potential applications in soft robotics, including shape-shifting structures, biomimicking soft robots, locomotion robots, and smart garments. There are possibilities for use as medical/therapeutic wearables, as assistive devices for those needing help with movement, and as soft robots to aid the rescue and recovery of people trapped in confined spaces.</p><p>Although these artificial muscles are still a proof of concept, Do is optimistic about commercialization in the near future. “We have a Patent Cooperation Treaty application around these technologies,” he says. “We are also working on clinical validation of our technology in collaborations with local clinicians, including smart compression garments, wearable assistive devices, and soft haptic interfaces.”</p><p>Meanwhile, the research team continues to work on improvements. “We have currently achieved an outer diameter of 0.5 mm, which we believe is still large compared to the human muscle fibers,” says Do. “[So] one of the main challenges of our technology is how to scale the muscle to a smaller size, let’s say less than 0.1 mm in diameter.”</p><p>Another challenge, he adds, relates to the hydraulic source of power, which requires electric wires to connect and drive the muscles. “Our team is working on the integration of a new soft, miniature pump and wireless communication modules that will enable untethered driving systems to make it a smaller and more compact device.”</p><p>Analytical modeling for bending actuators is yet another area of improvement. Concomitant studies to demonstrate the feasibility of machine-made smart textiles and washable smart textiles in the smart garment industry are also necessary, the researchers say, as are further studies regarding incorporating functional components into smart textiles to provide additional benefits.</p>]]></description><pubDate>Thu, 21 Jul 2022 18:33:12 +0000</pubDate><guid>https://spectrum.ieee.org/smart-clothes-artificial-muscles</guid><category>Artificial muscles</category><category>Soft robotics</category><category>Smart textiles</category><category>Hydraulics</category><dc:creator>Payal Dhar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/fabric-butterfly-on-a-tree-branch.jpg?id=30193856&amp;width=980"></media:content></item><item><title>Does MetaHuman’s Digital Clone Cross the Uncanny Valley?</title><link>https://spectrum.ieee.org/uncanny-valley-metahuman-digital-clone</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-metahuman-character-open-for-edits-in-the-metahuman-software.jpg?id=30126649&width=1200&height=800&coordinates=142%2C0%2C0%2C0"/><br/><br/><p>Creating your virtual clone isn’t as difficult as you’d think.</p><p>Epic Games recently introduced Mesh to <a href="https://www.unrealengine.com/en-US/blog/new-release-brings-mesh-to-metahuman-to-unreal-engine-and-much-more" target="_blank">MetaHuman</a>, a framework for creating photorealistic human characters. It lets creators sculpt an imported mesh to create a convincing character in less than an hour.</p><p>“It’s incredibly simple compared to a lot of other tools,” says Stu Richards (a.k.a. <a href="https://www.hundo.xyz/stories/meet-the-metaversers-meta-mike" target="_blank">Meta Mike</a>), partner success lead at <a href="https://www.giglabs.io/" rel="noopener noreferrer" target="_blank">GigLabs</a> and Cofounder of <a href="https://www.versed.digital/" rel="noopener noreferrer" target="_blank">Versed</a>. “I’d compare it to a character creator in a game.”</p><hr/><h2>Creating your clone</h2><p>Photorealistic characters remain the holy grail of 3D graphics. It’s a difficult task due <a href="https://spectrum.ieee.org/what-is-the-uncanny-valley" target="_self">to the “uncanny valley” phenomenon</a>, which theorizes humanlike characters become less appealing as they approach a realistic representation. Characters in big-budget games or animated films take months (if not years) of effort and often rely on detailed scans of real human models captured with expensive, specialized equipment.</p><p>Mesh to MetaHuman breaks this barrier to entry. Richards used his iPhone to scan a model of his face and create his own MetaHuman avatar. “After, I think, about 40 photos, it put together all the different angles, and creates a 3D model of my face,” he says. “You take that mesh, bring it into Unreal Engine…and overlay a MetaHuman head onto that mesh.” </p><p class="pull-quote">“The barrier of entry to getting into the tool itself is almost zero.” <br/>—Justin Vu, 3D animator</p><p>The mesh begins as an untextured, gray surface, but MetaHuman’s interface provides numerous options that can be changed with a click. Creators can tweak physique, head and facial hair, eyebrows, skin tone, and even the look of pores and opacity of skin to create a convincing virtual clone—or model an idealized avatar. </p><p>MetaHuman also rigs the mesh for animation. “You can select different poses and different animations,” says Richards. “But when you bring that back into Unreal, you have a lot of flexibility. You can create your own animation sets, create full-on controller mapping.” </p><p>Creators can even use Live Link, an Unreal Engine plug-in, to capture facial expressions in real time on a smartphone or tablet and stream it to a MetaHuman character.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="5159714986927a7daa9f3b2555f747e3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lDFLrzZy2R4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Live Link Face Tutorial with New Metahumans in Unreal Engine 4</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=lDFLrzZy2R4" target="_blank">www.youtube.com</a>
</small>
</p><p>While Mesh to MetaHuman is more approachable than previous workflows, Richards cautions that some familiarity with 3D modeling and animation is required for the best results. Creators can expect minor errors in the mesh captured by a smartphone or tablet that must be fixed in 3D-modeling software. And while MetaHuman’s interface is simple, Unreal Engine 5 remains complex.<br/></p><p>“From a user perspective, I think that at this point in time, having something a bit more lightweight, with lower fidelity, and more interoperable, is what’s going to resonate with people,” says Richards. “Especially in the NFT and web3 world.” </p><h2>MetaHuman beyond the metaverse</h2><p>Contrary to its name, MetaHuman is not a tool exclusively for metaverse avatars. In fact, that’s not even its primary use case (for now). Rather, MetaHuman is, according to Epic Games, ideal for creators working on smaller projects with a modest budget, such as independent games and short films. </p><p><a href="https://www.linkedin.com/in/jvu907/" rel="noopener noreferrer" target="_blank">Justin Vu</a>, an animator and 3D generalist specializing in filmmaking with Maya and Unreal Editor, is one such creator. He put MetaHuman to work in a series of promotional shorts from Allstate Insurance Company called <em><a href="https://www.linkedin.com/posts/visual-creatures_unrealengine-creative-thefutureofprotection-activity-6911951738610597888-Yi1C/" target="_blank">The Future of Protection</a></em>. Vu was able to use MetaHuman despite having little prior experience with Unreal Engine.<br/></p><p>“The barrier of entry to getting into the tool itself is almost zero,” says Vu. “So long as you have a computer that runs a modern video game, you can get started.”</p><p>It was especially useful during the peak of COVID. The difficulty of shooting live action in the middle of a lockdown and travel restrictions made animation appealing. MetaHuman helped Vu create convincing animated characters in a fraction of the time that might otherwise be required.</p><p>“What’s great about Metahuman is that it generates high fidelity models, which can be used with facial recording capture,” says Vu. “It’s very convincing to the average person and does a lot to cross the uncanny valley.”</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A MetaHuman character imported into an Unreal Engine project. " class="rm-shortcode" data-rm-shortcode-id="6dc1ecfca5468b4d3bfd0fd4974f880f" data-rm-shortcode-name="rebelmouse-image" id="72e56" loading="lazy" src="https://spectrum.ieee.org/media-library/a-metahuman-character-imported-into-an-unreal-engine-project.jpg?id=30126634&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">MetaHuman characters can be imported to Unreal Engine projects, such as the Matrix Awakens demo. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Epic Games</small></p><p>Animators can conduct a virtual casting by quickly iterating on characters that differ in age, hairstyle, height, facial structure, and physique. The tool’s automatic animation rigging is especially useful, as Vu says manual rigging of a high-fidelity 3D character can take “a few weeks, if not a few months.” Quick rigging helps animators try poses and expressions before deciding which character to use for a project.</p><p>Despite these strengths, Vu shared Richards’s caution that MetaHuman’s approachability has limits. While creating a character is relatively simple, character models and animations require tweaks for best results. He also notes MetaHuman has a limited selection of wardrobe options. This can be changed once a character is imported to Unreal Engine but, again, requires expertise in traditional 3D modeling and animation. </p><p>“While the tools are accessible and easy to start up, it really requires the hand of a good animator, a good actor, or good director, to drag it out of uncanny valley and make it convincing,” says Vu. </p><p>This sets the stage for competition in the space as alternative tools become more capable. <a href="https://blog.unity.com/news/welcome-ziva-dynamics" rel="noopener noreferrer" target="_blank">Unity purchased Ziva Dynamics</a>, a VFX studio known for its work creating lifelike characters, in January 2022. Other alternatives, like <a href="https://www.animaze.us/" rel="noopener noreferrer" target="_blank">Animaze</a> and <a href="https://readyplayer.me/" rel="noopener noreferrer" target="_blank">Ready Player Me</a>, lack realism but don’t require experience with 3D modeling or animation for usable results. These tools are popular with <a href="https://en.wikipedia.org/wiki/VTuber" rel="noopener noreferrer" target="_blank">Vtubers</a> and fans of metaverse social platforms like <a href="https://hello.vrchat.com/" rel="noopener noreferrer" target="_blank">VRChat</a>.</p><p>MetaHuman leads the effort to cross the uncanny valley—but it’s still anyone’s race.</p><p><strong></strong><em><strong>Correction (</strong></em><strong><em>18 July 2022): </em></strong><em>This story was updated to remove reference to a short film Justin Vu worked on, in which he used Unreal Engine but did <u>not</u> use MetaHuman as was originally reported. </em><br/></p>]]></description><pubDate>Sat, 16 Jul 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/uncanny-valley-metahuman-digital-clone</guid><category>Face capture</category><category>Metahumans</category><category>Motion capture</category><category>Metaverse</category><category>Virtual reality</category><category>Unreal engine</category><category>Avatars</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-metahuman-character-open-for-edits-in-the-metahuman-software.jpg?id=30126649&amp;width=980"></media:content></item><item><title>The Day Business Suits Became In-App Purchases</title><link>https://spectrum.ieee.org/metaverse-fashion-roblox-meta</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/meta-avatars-wearing-fashion-from-the-avatars-store.jpg?id=30041643&width=1200&height=800&coordinates=0%2C0%2C300%2C0"/><br/><br/><p>Odds are you’ve added a piece to your wardrobe for a job interview or an important meeting. But what if that meeting happened in the metaverse? Would you spend money to upgrade your avatar? </p><p>Meta is hoping you’ll answer “yes.” CEO Mark Zuckerberg announced the company’s new Avatars Store, slated to go live within weeks, <a href="https://www.facebook.com/zuck/posts/10114529132448901" rel="noopener noreferrer" target="_blank"><u>in an “avatar fashion show” with Instagram’s Eva Chen</u></a>. </p><p class="pull-quote">“The metaverse feels so far off. I have such a hard time imagining anyone going to design school, then doing their first collection in the metaverse, and having that resonate.”<br/>—Amy Odell, Substack fashion journalist</p><p>“Basically, it’s a clothing store for your avatar, [...] but we also want to create this marketplace, so all sorts of creators over time are going to be able to design clothing, and sell it,” <a href="https://www.instagram.com/p/Ce6yFgbptyM/" rel="noopener noreferrer" target="_blank"><u>Zuckerberg said on Instagram</u></a>. </p><h2>Digital suits are a deal, but will designers find value?</h2><p><a href="https://about.fb.com/news/2022/06/introducing-the-meta-avatars-store/" target="_blank">Meta’s launch of the Avatars Store </a>will feature styles from Balenciaga, Prada, and Thom Browne that emulate the look of real-world counterparts. This luxury attire could cost thousands in the real world but will prove more affordable in the metaverse, with prices ranging from US $2.99 to $8.99. </p><p>“I was really surprised, completely shocked by the price point, that it was that little,” <a href="https://amyodell.substack.com/" rel="noopener noreferrer" target="_blank"><u>Amy Odell</u></a>, fashion journalist and author of <a href="https://www.simonandschuster.com/books/Anna/Amy-Odell/9781982122638" target="_blank"><em>Anna: The Biography</em></a>, said in an interview.</p><p>This is the latest in a long and sometimes awkward alliance between fashion and Meta’s social media platforms, which include Facebook and Instagram. </p><p>“Even runway shows used to be closed off,” said Odell. “Fashion labels didn’t want to do that, for a variety of reasons.” </p><p>Social media’s vast reach has eroded the industry’s resistance, however, as Meta has put substantial effort into the relationship. Eva Chen, formerly editor in chief of <em>Lucky</em> magazine, was hired as head of fashion partnerships at Instagram in 2015 and has served in that role since. Zuckerberg also made efforts to build bridges,<a href="https://www.instagram.com/p/CdIm4dbAw6T/" rel="noopener noreferrer" target="_blank"><u> sharing photos of himself and the late Leonardo Del Vecchio</u></a> prior to the launch of the <a href="https://about.fb.com/news/2021/09/introducing-ray-ban-stories-smart-glasses/" rel="noopener noreferrer" target="_blank"><u>Ray-Ban Stories</u></a>.</p><p>Meta is not without competition. Roblox, an online game platform that lets players create their own experiences, has courted Tommy Hilfiger, Ralph Lauren, and Gucci, among others. Hilfiger decided to collaborate with Roblox <a href="https://wwd.com/fashion-news/sportswear/tommy-hilfiger-reveals-tommy-x-roblox-creators-virtual-collection-1235019532/" rel="noopener noreferrer" target="_blank"><u>after learning users were already creating unofficial Tommy Hilfiger fashion</u></a> on the platform.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="164eab410e2823867e769386dc6afb78" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lj3f2kffTes?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Introducing: Tommy X Roblox Creators</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=lj3f2kffTes" target="_blank">www.youtube.com</a>
</small>
</p><p>While not as well known as Meta, Roblox, which claims over 200 million monthly active users (MAUs), is significantly ahead of Meta’s Horizon Worlds, which claimed 300,000 MAUs in February of 2022. Those users are spending, too: The company’s Q2 2022 report states <a href="https://ir.roblox.com/news/news-details/2022/Roblox-Reports-First-Quarter-2022-Financial-Results/default.aspx" target="_blank"><u>an average Roblox user spends about four dollars</u></a> every day they’re on the platform.<br/></p><p>Clearly, there’s a demand for fashion on metaverse platforms. And there may be another source of interest: young fashion designers.</p><p>“Starting a fashion label is expensive. It’s really, really hard, like starting any business,” said Odell. “You have to make a whole collection, and then show it, and it’s expensive. The metaverse solves for that.” </p><p>However, Odell is skeptical about Meta’s execution. Its rollout of features on other platforms, <a href="https://www.vox.com/the-goods/2020/8/18/21372510/instagram-reels-bad" target="_blank"><u>such as Instagram Reels</u></a>, have earned it a reputation for sudden pivots that can harm partners. She’s also unsure most designers will care until Meta’s metaverse is better established.</p><p>“The metaverse feels so far off,” says Odell. “I have such a hard time imagining anyone going to design school, then doing their first collection in the metaverse, and having that resonate.” </p><h2>Meta’s fashion may wear out more quickly than you think.</h2><p>Meta’s announcement of the Avatars Store was followed by <a href="https://www.facebook.com/zuck/posts/10114535743330641" rel="noopener noreferrer" target="_blank"><u>a rebrand of Facebook Pay to Meta Pay</u></a> that positions it as “the wallet for the metaverse.” </p><p>Though the rebranding announcement was separate from the one for the Avatars Store, it’s possible to read between the lines. Meta wants to nail down the fundamentals of how user accounts, avatars, and monetization will work in the metaverse. </p><p>But <a href="https://www.linkedin.com/in/rafaelbrown/" rel="noopener noreferrer" target="_blank"><u>Rafael Brown</u></a>, CEO of metaverse event company <a href="https://www.symbolzero.com/team" target="_blank">SymbolZero</a>, thinks Meta is too eager to move forward with a simple avatar solution that may not age well. “Oculus has shown more complex avatar systems than what they have now,” said Brown. “I think they dumbed it down a little bit.” </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A crowd of Xbox avatars." class="rm-shortcode" data-rm-shortcode-id="d0704e8e9dd6b1880b8835a4aceb6b48" data-rm-shortcode-name="rebelmouse-image" id="ecd72" loading="lazy" src="https://spectrum.ieee.org/media-library/a-crowd-of-xbox-avatars.png?id=30040244&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Game companies like Nintendo and Microsoft have found avatars difficult to maintain.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Microsoft</small></p><p>Brown points out that game companies have tried similar systems in the past. Nintendo, Xbox, and PlayStation all have avatars that can be personalized with a variety of clothing and accessories purchased or earned in-game. All three companies have had to change their avatar systems over time.</p><p>“This is something the metaverse standards are going to go into, because interoperability is a myth,” said Brown. “Anyone who thinks what they’re going to buy now on Quest 2 avatars is going to last, [...] that’s not going to happen. If [Meta is] telling people that everything they’ve bought is going to be maintained, well, nobody does that.”</p><p>Interoperability, in this context, means the ability to clothe your avatar 5, 10, or 20 years from now in the Thom Browne suit you purchase today, and possibly between different platforms. </p><p class="pull-quote">“Anyone who thinks what they’re going to buy now on Quest 2 avatars is going to last, [...] that’s not going to happen.”<br/>—Rafael Brown, SymbolZero</p><p>That promise seems easy to make at first glance, but even a simple avatar can include hundreds of variables ranging from facial expression to skin color or hair style. More complex avatars, such as those that allow real-time tracking of a user’s facial expression, can include tens of thousands of variables.</p><p>Brown also thinks Meta has a more fundamental problem, one that touches on Odell’s concerns: a lack of community. </p><p>“The thing that a lot of Web3 folks don’t get is that it’s not that the clothing has inherent value,” said Brown. “It has value in relation to the community.” </p><p>A lack of community may be why Meta seems focused on the launch of the Avatars Store on Facebook and Instagram, which are far more popular than <a href="https://en.wikipedia.org/wiki/Horizon_Worlds" target="_blank">Horizon Worlds</a>—Meta’s online virtual-reality game created to promote their Oculus VR gear. Meta has yet to prove its vision of the metaverse can snag a critical mass of users. And without users, well—who will appreciate your virtual designer suit? </p>]]></description><pubDate>Fri, 01 Jul 2022 13:21:08 +0000</pubDate><guid>https://spectrum.ieee.org/metaverse-fashion-roblox-meta</guid><category>Roblox</category><category>Roblox creators</category><category>Virtual reality</category><category>Meta</category><category>Fashion tech</category><category>Metaverse</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/meta-avatars-wearing-fashion-from-the-avatars-store.jpg?id=30041643&amp;width=980"></media:content></item><item><title>Vanadium Anodes for Faster-charging, Longer-lived Batteries</title><link>https://spectrum.ieee.org/vanadium-batteries</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-foil-rectangle-labelled-tyfast-with-two-silver-squares-coming-out-of-the-top.jpg?id=30048901&width=1200&height=800&coordinates=0%2C73%2C0%2C74"/><br/><br/><p>To fulfill the vision of EVs that <a href="https://spectrum.ieee.org/lucid-air-500-miles-ev" target="_self">travel a thousand miles</a> or phones that run for days on a single charge, most battery developers are racing to make batteries that can pack twice the energy in the same weight.</p><p>Not startup <a href="https://tyfast.energy/" rel="noopener noreferrer" target="_blank">Tyfast</a>, which is “approaching next-generation battery development in a counter-current direction,” says GJ la O’, CEO and cofounder of the 2021 spinoff from the University of California, San Diego. </p><p>The company wants to make a battery based on a new vanadium-based anode material that can charge in 3 minutes and run for 20,000 charging cycles at the expense of energy density, which la O’ says could be 80 to 90 percent that of present-day batteries. La O' and company cofounder and chief technical officer Haodong Liu plan to develop a commercial product with the help of a new Activate <a href="https://www.activate.org/tyfast" rel="noopener noreferrer" target="_blank">fellowship</a> they were awarded in early June. “Our high-level focus is 20 times faster charging and 20 times longer life than today’s lithium batteries. We’re saying we’ll deliver the same runtime as the previous-generation device, but charge really, really fast such that you forget about the size and energy density.”</p><p>That compromise works for their intended always-on, high-utilization applications such as autonomous warehouse or delivery robots that need to work nearly 24/7. Tyfast’s battery could enable that with very short fast-charging stops. </p><p>“Today’s robot platforms have battery packs that run 8 to 10 hours,” he says. “If you have a faster-charging battery you could have a 4- to 5-hour battery pack. A smaller pack would mean the robot carries more load per trip so it would be more productive.”</p><p>The charge time of lithium-ion batteries is limited by how quickly lithium ions flow in and out of the anode. The graphite used typically for anodes now has a planar structure with ions slipping in between the layers. </p><p>Tyfast instead uses an anode made of lithium vanadium oxide, a material with a 3D crystal structure similar to table salt. Ions move through the crystals in three dimensions, enabling 10 times as fast transport as in graphite. </p><p>The LVO anode, first reported by UCSD nanoengineers, Tyfast cofounder <a href="http://liugroup.ucsd.edu/" rel="noopener noreferrer" target="_blank">Ping Liu</a>, and others in 2020 in the journal <em><a href="https://www.nature.com/articles/s41586-020-2637-6" rel="noopener noreferrer" target="_blank">Nature</a></em>, also expands and contracts less than graphite when it charges and discharges. Graphite changes volume by up to 10 percent whereas LVO expands less than 2 percent. That translates to less mechanical and chemical damage of the anode, allowing longer battery life, la O’ explains. “Our technology is trying to overcome this material limitation that graphite has set.”</p><p>Graphite can, however, hold more ions than LVO by weight, giving a higher battery energy density. Several <a href="https://spectrum.ieee.org/enevates-silicon-anodes-could-give-batteries-that-run-400-km-on-a-5minute-charge" target="_self">companies</a> are developing <a href="https://spectrum.ieee.org/nanograf-lithium-battery-silicon-anode" target="_self">nanoengineered silicon</a> or lithium-metal anodes that would have twice the energy density of even graphite, in order to enable longer EV driving range. The challenge with those materials is also their very high volumetric expansion that can crack the anode.</p><p>Tyfast’s LVO anode might be more robust, but its higher cost could be its downside. It is almost twice as expensive as graphite at over US $20 per kilogram. A battery  with an LVO anode and nickel-manganese-cobalt [NMC] cathode would be 30 to 50 percent more expensive than a graphite-NMC cell, says la O’. But LVO’s longer life could make up for its higher cost. “If you were to use it in an always-on application like autonomous robots, LVO lasts longer, so you’re at a lower cost per charging cycle,” he adds.</p><p>Before robots, the company is initially targeting the wearables market. It has several customers lined up, la O’ says, including a major consumer-electronics brand. In its laboratory, it is building batteries the size of credit cards and sugar packets. These prototypes use LVO anodes and commercial NMC cathodes and electrolytes so far last for over 2,000 charge cycles and charge in under 15 minutes. </p><p>By the end of the year, the team plans to reach 20,000 cycles and under 3-minute charging. “The vision is consumer-electronics devices that get you back to life faster with a Tyfast battery,” he says. “If you take a coffee break, by the time you’re done with your cup, your device is fully charged.”</p>]]></description><pubDate>Thu, 30 Jun 2022 19:35:30 +0000</pubDate><guid>https://spectrum.ieee.org/vanadium-batteries</guid><category>Anodes</category><category>Vanadium</category><category>Fast charging</category><category>Batteries</category><dc:creator>Prachi Patel</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-foil-rectangle-labelled-tyfast-with-two-silver-squares-coming-out-of-the-top.jpg?id=30048901&amp;width=980"></media:content></item><item><title>Mojo Vision Puts Its AR Contact Lens Into Its CEO’s Eyes (Literally)</title><link>https://spectrum.ieee.org/looking-through-mojo-visions-newest-ar-contact-lens</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/closeup-of-eye-with-contact-lens-containing-electronic-components.jpg?id=30041337&width=4032&height=2504&coordinates=0%2C0%2C0%2C184"/><br/><br/><p class="ieee-editors-note">
<em><strong>Update 3 November 2022: </strong>Today, Mojo Vision announced the first major third-party consumer application to be tested on the prototype of its AR contact lens: the Alexa Shopping List. Developed with support from Amazon, the app allows users to ask Alexa, by talking, to add items to a shopping list that they can then view as a scrollable list through the lens while walking through the grocery store. Users can check off items by holding their gaze for a moment. The list updates in real time if another household member adds items to the shopping list.</em>
</p><p>
<em>Is this <em>the</em> killer app for this augmented reality device? No. But the development does show that major companies are taking Mojo Vision’s smart contact lens seriously.</em>
</p><p>
<em>In March, I looked through Mojo Vision’s AR contact lens—but I didn<em>’t</em> put it in my eye. At that point, while nonworking prototypes had been tested for wearability, nobody had worn the fully functional, battery-powered, wirelessly communicating, device. Earlier this year, Mojo announced that its augmented reality lens had gone on-eye—specifically, on the eye of the Mojo Vision CEO, Drew Perkins, on 23 June.</em>
</p><p>
<em>“I’ve worn it. It works...and it was the first ever on-eye demonstration of a feature-complete augmented reality smart contact lens,” reported Perkins in <a href="https://www.mojo.vision/news/today-i-wore-mojo-lens" rel="noopener noreferrer" target="_blank">a blog post</a>. “The final technical hurdle to wearing the lens was ensuring that the power and radio communications systems worked without wires. Cutting the cord [proved] that the lens and all major components are fully functional and reduce many of the technical challenges in building a smart contact lens.”</em>
</p><p>
<em>It’s an exciting milestone for Perkins and the Mojo Vision team, and we’ll continue monitoring their progress.</em>
</p><div class="horizontal-rule">
</div><p class="ieee-editors-note">
<em>Story from 30 March 2022 follows:</em>
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two fingers hold a contact lens with a ring of circuitry on its outer edge" class="rm-shortcode" data-rm-shortcode-id="760452af61c0ec1e627fa49b94d27492" data-rm-shortcode-name="rebelmouse-image" id="13d7e" loading="lazy" src="https://spectrum.ieee.org/media-library/two-fingers-hold-a-contact-lens-with-a-ring-of-circuitry-on-its-outer-edge.jpg?id=29611175&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Early this year, IEEE Spectrum editor Tekla Perry tested Mojo Vision's AR contact lens by holding it very close to one eye and peeking through.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Mojo Vision</small>
</p><h3></h3><br/><p><strong>In March,</strong> <a href="https://www.mojo.vision/" target="_blank">Mojo Vision</a> unveiled its latest AR contact lens. Still a prototype, the device has clinical testing and further development ahead before it can apply for the <a href="https://www.fda.gov/" target="_blank">U.S. Food and Drug Administration </a>(FDA) approval needed to sell to consumers. But Mojo’s engineers are steadily ticking off engineering milestones.</p><p>Last week I got to literally peek through Mojo’s newest lens. Here’s what I saw, and what <a href="https://www.linkedin.com/in/slsinclair" target="_blank">Steve Sinclair</a>, Mojo senior vice president of product and marketing, had to say about the company’s progress so far and the challenges that remain.</p><p>First, the demo. I did not put the lens in my eye—this prototype is still in safety testing, and fitting a contact lens requires an eye exam. Instead, I held a lens very close to one eye and peeked through. I was able to move around freely, but because holding instead of wearing the device means that it cannot track eye movements, Mojo has temporarily incorporated a tiny crosshair into the user interface to help with alignment. The nature of the demo also meant that images I saw were flat; with a lens in both eyes, the images will appear in 3D.</p><h3>What’s New in the Mojo Lens</h3><br/><p>• Medical-grade microbatteries on board along with a custom power management IC</p><p><span></span>• High-res 14,000 pixels per inch monochrome microLED display (up from about 8,000 pixels per inch)</p><p><span></span>• Custom 5-gigahertz radio using a proprietary low-latency communications protocol</p><p><span></span>• Eye tracking using accelerometer, gyroscope, and magnetometer</p><p><span></span>• Coming soon: Custom image sensors</p><h3></h3><br/><p>I tried out several applications. To select them, I looked around the periphery of the real-world view in front of me, which caused icons to appear. By focusing on one (in this case, aligning the crosshairs), I selected it. The app I found to be the most fun to use wasn’t the most complex, but it did show off the sensors on board by tagging compass headings as I turned to face different directions. I also played with a travel tools app that included a high-resolution monochrome image of an incoming Uber driver, a biking app that called up heart rate and other information useful on a training ride, a teleprompter app that naturally scrolled up and down to move through the text, and a monochrome video stream. All these demos took place in the prototype lens, with me fully in control of them. Mojo’s team then moved to a VR simulation to demonstrate eye tracking features that won’t work unless the lens is sitting in your eye.</p><p>This isn’t <a href="https://spectrum.ieee.org/ar-in-a-contact-lens-its-the-real-deal" target="_self">the first time</a> I’ve seen the Mojo lens in person. But since that last demo in 2020, the engineering team has moved from wireless power to batteries on board, increased the resolution of the display from 8,000 pixels per inch to 14,000, thinned commercial motion sensors and developed its own radio and power management, and created several apps. Image sensors—a feature of earlier demos that showed off edge detection in low light and other <a href="https://spectrum.ieee.org/startup-mojo-vision-has-the-earliest-adopters-of-augmented-reality-contact-lenses-in-its-sights" target="_self">vision enhancements</a>—have yet to be built into the current prototype, though Sinclair says that they are in the works, and that the company is continuing to test apps for people with low vision and still expects them to be among the earliest customers for the device.</p><p>Here’s what else Sinclair had to say about the technology as developed so far and the path ahead.</p><p><strong>Let’s talk about the batteries.</strong></p><h3></h3><br/><img alt="Man from shoulders up smiling, wearing glasses and blue collared shirt" class="rm-shortcode" data-rm-shortcode-id="6d8975cac9d43ed4ae5fedc725e79487" data-rm-shortcode-name="rebelmouse-image" id="bb4ba" loading="lazy" src="https://spectrum.ieee.org/media-library/man-from-shoulders-up-smiling-wearing-glasses-and-blue-collared-shirt.jpg?id=29610462&width=980"/><h3></h3><br/><p><strong>Steve Sinclair:</strong> The battery is in the outer ring, embedded in the lens. We are partnering with a medical-grade battery company that makes implantable microbatteries for things like pacemakers, to design something safe to wear. Previously, we were using magnetic inductive coupling, and that seemed fine when you were holding it up to your eye. But the moment you put it on your eye and started moving your head and darting your eye and looking around, we were losing that connection to the wireless field. It wasn’t as reliable as we needed it to be. And so we made a decision about a year and a half ago to switch over to battery power.</p><p><strong>I recall you always wanted to get batteries on board?</strong></p><p><strong>Sinclair: </strong>We thought so. But we sped that path up, because we came to the conclusion that the other path was just not a good way to go.</p><p><strong>Why doesn’t this version have the image sensor?</strong></p><p><strong>Sinclair: </strong>We’ve decided to leave out the imager, the camera, for right now; it’s not critical to the use cases that we’re looking at first.</p><p><strong>But you had been working to develop applications for people with low vision. What happened to that?</strong></p><p><strong>Sinclair: </strong>We’ve been using mainly low-vision capabilities built into smartphones right now to take pictures of things and bring them up to your eye for zooming in and out; we’ll add in the imager and test those capabilities out next. It’s just about there, but it wasn’t necessary to get to this milestone, so we decided to simplify things a little bit.</p><p><strong>Given the amount of lens real estate being taken up by batteries and chips, was oxygenation an issue?</strong></p><p><strong>Sinclair: </strong>Absolutely. That was a core engineering point that needed to be solved before we could get to this lens. So this lens has channels built into it and a special design that allows air to get in and get through. The oxygen diffuses out over the surface of the cornea and the sclera.</p><h3></h3><br/><img alt="A transparent contact lens showing computer chips and circuitry within its periphery" class="rm-shortcode" data-rm-shortcode-id="2310cdfe38972875470b1151decffa04" data-rm-shortcode-name="rebelmouse-image" id="76b59" loading="lazy" src="https://spectrum.ieee.org/media-library/a-transparent-contact-lens-showing-computer-chips-and-circuitry-within-its-periphery.jpg?id=29613762&width=980"/><h3></h3><br/><p><strong>Does the circuitry block vision at all?</strong></p><p><strong>Sinclair: </strong>See the cutout on the side? Imagine I’m wearing it. It’s going to be oriented like this [the cutout away from the nose] because I need the peripheral vision on the outside, not towards the nasal side. Ultimately, there’s even more we can do to push components further toward the edges of the lens to maximize light entering the pupil.</p><p><strong>Now that you explain the arrangement of circuitry to protect peripheral vision, it seems obvious, but some of the most obvious things take figuring out.</strong></p><p><strong>Sinclair: </strong>It wasn’t immediately obvious; our original design had [the electronics] all the way around.</p><p><strong>What happens next?</strong></p><p><strong>Sinclair: </strong>We’re already wearing lenses that don’t have any electronics in them, but they’re shaped the same way, so that we can test for comfort and for how long can we wear them and have the oxygenation still working to our satisfaction.</p><p>Next we start testing [the complete prototype] on-eye, and see just how well it works in different situations. We can’t say exactly when that happens. We hope it’s soon, but we’ve got to make sure it’s safe and everything’s working the way we expect it to work. That testing will start with <a href="https://www.linkedin.com/in/drewdperkins" target="_blank">Drew Perkins</a> wearing it, our CEO, then probably <a href="https://www.linkedin.com/in/mwiemer" target="_blank">Mike Wiemer</a>, our CTO. And then it goes to folks like myself and others on the executive team, along with some of the key engineers that need to start evaluating things quickly.</p><p>We’ll be testing the software, the battery life, and the wireless connectivity and [its] speeds. There’s likely to be a spin of one, two, or three of the chips built into the lens as we discover things that don’t work right or could be optimized to work better together as a system.</p><p><strong>What’s the timeline for commercialization?</strong></p><p><strong>Sinclair: </strong>Everything is predicated on eventually getting FDA certification. We don’t like to presume how long that’s going to take.</p><p><strong>You’re going for the consumer market. Today, it seems, most AR glasses manufacturers have decided to focus on the enterprise market initially. Why the different path?</strong></p><p><strong>Sinclair: </strong>With AR glasses, there are definitely some awesome enterprise use cases. But these are not great for contact lenses. Say you’re an IT manager, and you’ve come up with an awesome contact lens application. Can you tell your workers to wear a contact lens? Not usually. But consumers can make that decision. The reason people wear contact lenses is very consumery: I want to look like myself. I want to use it when I’m working out or doing sports. I don’t like things on my face. I don’t like the fashion sense of glasses; it’s not me. We decided to lean into those reasons that people pick contact lenses.</p><p><strong>Can you talk about pricing?</strong></p><p><strong>Sinclair: </strong>Like anything else, when we first bring it out, it’ll be a little expensive. Our goal when we’re running at volume is that it should come out somewhere close to a high-end smartphone. But factor in the fact that people are already spending US $500, $600, or $700 for eyewear today, subtract that out of the total price, and the adder on top of that is not huge.</p><p>
<em>This article appears in the June 2022 print issue as “My Peek Through Mojo Vision’s AR Contacts.”</em>
</p>]]></description><pubDate>Tue, 28 Jun 2022 21:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/looking-through-mojo-visions-newest-ar-contact-lens</guid><category>Consumer electronics</category><category>Wearables</category><category>Mojo vision</category><category>Displays</category><category>Augmented reality</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/closeup-of-eye-with-contact-lens-containing-electronic-components.jpg?id=30041337&amp;width=980"></media:content></item><item><title>Free On-Demand Webinars on Data Acquisition Boards and Their Applications</title><link>https://go.teledynelecroy.com/on-demand-webinars</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Dive into the world of digitizers and explore how they can benefit your application. Explore the basics of digitizers, pulse detection, peer-to-peer streaming, and more. Whether you are a scientist, engineer, student or if you want to know more about Teledyne SP Devices deep knowledge base there is something for everyone. <a href="https://go.teledynelecroy.com/on-demand-webinars" rel="noopener noreferrer" target="_blank">Register now for these free webinars!</a><u></u></p>]]></description><pubDate>Thu, 23 Jun 2022 22:19:44 +0000</pubDate><guid>https://go.teledynelecroy.com/on-demand-webinars</guid><category>Data acquisition</category><category>Digitizer</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Neuronlike Memristors Could Superspeed 6G Wireless</title><link>https://spectrum.ieee.org/memristor-6g-switches</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conceptual-rendering-of-a-device-of-molybdenum-disulfide-sandwiched-between-gold-electrodes-wiggly-lines-and-light-emit-from-it.jpg?id=29982938&width=1200&height=800&coordinates=0%2C51%2C0%2C52"/><br/><br/><p>The dream for the next generation of wireless communications, <u><a href="https://spectrum.ieee.org/6g-geopolitics" target="_self">6G</a></u>, includes speeds of up to <u><a href="https://spectrum.ieee.org/terahertz-linkdiscovery" target="_self">trillions of bits per second</a></u>. Now a new study reveals that neuronlike devices made of atomically thin films might serve as key switches in upcoming 6G networks.<br/></p><p>The current generation of wireless communications, 5G, began its rollout in 2020. Initial deployment of 6G is not likely to happen until around 2030, and much remains uncertain about the shape that it might take, as no standards for it have been established yet. Still, many envision terabit-per-second data rates for 6G, as well as applications such as autonomous vehicles, <a href="https://spectrum.ieee.org/heres-what-6g-will-be-according-to-the-creator-of-massive-mimo" target="_self">augmented reality</a>, and <a href="https://spectrum.ieee.org/6g-technology" target="_self">immersive telepresence</a>.</p><p>A key building block that will likely require an upgrade for 6G is the analog switch. These commonplace components assist communications between wirelessly connected devices by jumping between networks and frequencies while receiving data.</p><p class="pull-quote">“When we first started the research, we could only dream of such performance, and to be able to actually realize it in practical devices is a welcome surprise.”<br/>—Deji Akinwande, University of Texas at Austin</p><p>Conventional analog switches based on transistors or solid-state diodes are <a href="https://en.wikipedia.org/wiki/Volatile_memory" target="_blank">volatile</a>, consuming energy not only when they switch but also during standby or even when idle. To substantially reduce the overall energy consumption, researchers are exploring nonvolatile memory devices as alternative analog switches. These include <a href="https://spectrum.ieee.org/memristor-random" target="_self">neuronlike memristors</a>, two-dimensional <a href="https://spectrum.ieee.org/atomthin-memristors-discovered" target="_self">atomristors</a>, <a href="https://spectrum.ieee.org/nanoporous-version-of-silicon-oxide-brings-it-back-into-the-race-for-resistive-memory" target="_self">resistive random-access memory (RRAM)</a>, and <a href="https://spectrum.ieee.org/novel-nanostructures-give-boost-to-phase-change-memory" target="_self">phase-change memory (PCM)</a>.</p><p>In the new study, researchers focused on atomically thin, two-dimensional materials that in principle require only tiny amounts of energy, potentially resulting in greater speed and battery life. The researchers had previously developed nonvolatile switches made of <a href="https://spectrum.ieee.org/atomically-thin-circuits-made-from-graphene-and-molybdenite" target="_self">molybdenum disulfide</a> that operated at <a href="https://www.nature.com/articles/s41467-018-04934-x" target="_blank">frequencies of up to 50 gigahertz</a>, and ones made of <a href="https://spectrum.ieee.org/atomthin-switches-5g-6g-radio-signals" target="_self">hexagonal boron nitride</a> that operated at 100 GHz.</p><p>Now these scientists have developed nonvolatile analog switches from molybdenum disulfide that can operate at frequencies of roughly 100 to 500 GHz, a communication band that 6G will likely include.</p><p>The new devices sandwich molybdenum disulfide between two gold electrodes. This metal-insulator-metal structure behaves like a <a href="https://en.wikipedia.org/wiki/Memristor" target="_blank">memristor</a>, or <a href="https://spectrum.ieee.org/tag/memristors" target="_blank">memory resistor</a>, a kind of building block for electronic circuits that scientists predicted roughly 50 years ago but created for the first time only a little more than a decade ago. These components are essentially switches that can remember whether they were toggled on or off after their power is turned off. In theory, memristors can act like artificial neurons capable of both computing and storing data.</p><p>In experiments, the new switch could engage in data communication at rates of up to 100 gigabits per second at a frequency of 320 GHz, with a low bit error rate and a high signal-to-noise ratio. It could also stream real-time, high-definition television with no compression at 1.5 Gb/s at a frequency of 300 GHz. It could switch in just half a nanosecond with an energy of just 50 picojoules, more than one order of magnitude lower than comparable mature devices.</p><p>“When we first started the research, we could only dream of such performance, and to be able to actually realize it in practical devices is a welcome surprise,” says study co–senior author Deji Akinwande, an electrical engineer and materials scientist at the University of Texas at Austin. “The application is quite clear—energy-efficient communication components for future 6G wireless systems.”</p><p>In the future, the researchers aim to integrate these switches with silicon chips and circuits. “Molybdenum disulfide is wafer-scalable and can be integrated with silicon CMOS,” Akinwande says. “Hence, this switch device can be readily integrated with contemporary semiconductor microchip technology that could benefit a diverse array of industries to benefit society.”</p><p>The main criticism of the device at the moment “is the reliability of these switches, especially with respect to endurance,” Akinwande says. “That is indeed a question that is critical to practical applications, and we are working on this matter.”</p><p>The scientists detailed <u><a href="https://www.nature.com/articles/s41928-022-00766-2" rel="noopener noreferrer" target="_blank">their findings</a></u> 30 May in the journal <em>Nature Electronics</em>.</p>]]></description><pubDate>Thu, 16 Jun 2022 21:13:41 +0000</pubDate><guid>https://spectrum.ieee.org/memristor-6g-switches</guid><category>6g</category><category>Wireless communication</category><category>5g</category><category>Switches</category><category>Molybdenum disulfide</category><category>Memristors</category><category>Memristor</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conceptual-rendering-of-a-device-of-molybdenum-disulfide-sandwiched-between-gold-electrodes-wiggly-lines-and-light-emit-from-it.jpg?id=29982938&amp;width=980"></media:content></item><item><title>Apple’s Metaverse Snub Highlights AR/VR Tech Woes</title><link>https://spectrum.ieee.org/apple-virtual-reality-metaverse-snub</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/apple-s-roomplan-was-the-company-s-only-key-ar-innovation-at-wwdc.jpg?id=29985046&width=1200&height=800&coordinates=0%2C102%2C0%2C103"/><br/><br/><p>
<a href="https://developer.apple.com/wwdc22/" target="_blank">Apple’s Worldwide Developer Conference</a> (WWDC) earlier this month was headlined by Apple’s new <a href="https://en.wikipedia.org/wiki/Apple_M2" target="_blank">M2 processor</a> and <a href="https://en.wikipedia.org/wiki/IOS_16" target="_blank">dramatic changes to iOS</a>. But what stood out most was Apple’s silence on the metaverse. Its <a href="https://spectrum.ieee.org/this-is-the-year-for-apples-ar-glassesmaybe" rel="nofollow" target="_self">rumored AR/VR headset, and the RealityOS operating system it may use</a>, was nowhere to be seen.
</p>
<p>
	This could be interpreted as a snub. But Apple has a very particular target in mind: Meta.
</p>
<p>
	“Metaverse, as a term, only really gained momentum when Meta started talking about it,” says <a href="https://moorinsightsstrategy.com/anshel-sag/" rel="noopener noreferrer" target="_blank">Anshel Sag, principal analyst at Moor Insights & Strategy</a>. Facebook’s rebrand immediately generated headlines on every major tech website, spiked Google searches for the term, and <a href="https://spectrum.ieee.org/metaverse-real-estate" target="_self">drove a boom in virtual real estate</a>.
</p>
<p>
	This, of course, is not to Apple’s benefit, and gives Tim Cook plenty of reason to keep the term out of his mouth. But it’s more than a marketing ploy. The vision of the metaverse put forward by Meta is fundamentally opposed to what Apple might embrace should it bring an AR/VR (augmented reality/virtual reality) headset to market.
</p>
<p class="pull-quote">
	“There was a lot of anxiety in the industry about what Apple was going to announce. Now that they didn’t, there’s almost a sense of relief.”<br/>
	—Anshel Sag, Moor Insights & Strategy
</p>
<p>
	“Meta has shipped 15 million [Oculus Quest] units, but they’re doing it in a way that costs them,” says Sag. “They’re probably shipping at a loss on hardware.”<br/>
</p>
<p>
	Zuckerberg’s vision of an “embodied Internet” is focused on social platforms and services. The Quest headset is impressive and, <a href="https://www.polygon.com/23068620/meta-quest-cambria-vr-headset-virtual-pet-facebook" target="_blank">if projects like the Cambria VR headset come to market</a>, will soon see a generational leap. But Meta’s real goal is the establishment of a new social platform—not developing and selling AR/VR headsets.
</p>
<p>
	For Apple, innovative hardware is the entire point. Whether that hardware is used to access Meta’s metaverse, another company’s platform, or isolated experiences crafted by small developers, is almost irrelevant.
</p>
<p>
	Apple’s alternative to Meta’s embodied Internet will likely be a broad offering spanning the iPhone, iPad, Mac, and a future AR/VR headset. The metaverse may simply be an app (or apps) competing with other AR/VR experiences across multiple devices.
</p>
<p>
	“I wouldn’t read too much into Apple not using the word <em>metaverse</em>,” says <a href="https://omdia.tech.informa.com/authors/george-jijiashvili" rel="noopener noreferrer" target="_blank">George Jijiashvili, principal analyst at Omdia</a>. “Rather, focus on what it’s actually doing to enable experiences which could support virtual social interactions in the future.”
</p>
<h2>Competitors breathe a sigh of relief</h2>
<p>
	Aside from the new RoomPlan Swift API, a technology that developers can use to quickly create 3D floor plans of real-world spaces, Apple had no all-new AR/VR announcements. RealityOS, the operating system rumored to power Apple’s upcoming headset, wasn’t even teased.
</p>
<p>
	This seems a vote of no confidence in all things metaverse and, perhaps, the entire AR/VR space. However, Jijiashvili warns against such pessimism. “The reality is that Apple has many strengths in this space, which it continues to gradually improve,” says Jijiashvili.
</p>
<p>
	He points out Apple has acquired over eight AR/VR startups since 2015. <a href="https://developer.apple.com/augmented-reality/" target="_blank">ARKit</a>, Apple’s augmented-reality development platform for iOS devices, continues to see interest from developers, including big names like Snapchat and even Instagram, which is owned by Meta.
</p>
<p>
	However, Apple’s lack of news gives the rest of the industry the chance to prepare for its seemingly inevitable push into the space. Though consumer headsets are dominated by Meta, which produces <a href="https://www.idc.com/promo/arvr" rel="noopener noreferrer" target="_blank">nearly 80 percent of all headsets sold</a>, the industry is rife with midsize companies like HTC, Valve, DPVR, Magic Leap, Pico, Lumus, Vuzix, Pimax, and Varjo—to name just a few. Apple’s arrival in the space could threaten these innovators.
</p>
<p>
	“There was a lot of anxiety in the industry about what Apple was going to announce,” says Sag. “Now that they didn’t, there’s almost a sense of relief.”
</p>
<h2>AR headsets face headwinds</h2>
<p>
	Apple’s decision not to show its headset, which is believed to support both AR and VR, strongly hints the company isn’t satisfied with its progress.
</p>
<p>
	It’s not alone. <a href="https://www.theinformation.com/articles/meta-scales-back-ar-glasses-plan-amid-reality-labs-shakeup" rel="noopener noreferrer" target="_blank">Meta reportedly delayed an upcoming AR headset</a>, known as Project Nazare. The Magic Leap 2 and Microsoft HoloLens 2 seem trapped in the niche world of high-end enterprise solutions despite years of work by both companies.
</p>
<p>
	AR, it seems, is hard to get right.
</p>
<p>
	This leaves no serious alternative to Meta’s Quest 2 on the horizon. Its successor, Project Cambria, is rumored to target a 2022 release and may have little competition if launched this year. “The focus may have shifted more towards VR than people really expected,” says Sag.
</p>
<p>
<a href="https://spectrum.ieee.org/is-the-metaverse-even-feasible" target="_self">The metaverse faces serious challenges</a>, but the dearth of AR alternatives gives Meta time to work out issues. And there’s always the possibility Apple’s headset will, like <a href="https://spectrum.ieee.org/whats-apples-filings-say-about-its-selfdriving-car-program" rel="nofollow" target="_self">the company’s rumored self-driving car</a>, fail to appear. Meta’s vision of a fully embodied metaverse could win by default if the alternatives never show up on store shelves.
</p>]]></description><pubDate>Thu, 16 Jun 2022 14:59:49 +0000</pubDate><guid>https://spectrum.ieee.org/apple-virtual-reality-metaverse-snub</guid><category>Metaverse</category><category>Virtual reality</category><category>Augmented reality</category><category>Meta</category><category>Apple</category><category>Oculus</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/apple-s-roomplan-was-the-company-s-only-key-ar-innovation-at-wwdc.jpg?id=29985046&amp;width=980"></media:content></item><item><title>The First High-Yield, Sub-Penny Plastic Processor</title><link>https://spectrum.ieee.org/plastic-microprocessor</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/at-left-a-circle-with-repeated-texture-at-right-a-textured-square-with-sections-of-different-sizes-in-different-colors-with-t.png?id=29971002&width=1200&height=800&coordinates=435%2C0%2C435%2C0"/><br/><br/><p>
	For decades, hopeful techies have been promising a world where absolutely every object you encounter—bandages, bottles, bananas—will have some kind of smarts thanks to supercheap programmable plastic processors. If you’ve been wondering why that hasn’t happened yet, it’s that nobody has built working processors that can be made in the billions for less than a penny each.
</p><p>
	It hasn’t been for want of trying; in 2021 Arm reproduced its simplest <a href="https://www.nature.com/articles/s41586-021-03625-w" rel="noopener noreferrer" target="_blank">32-bit microcontroller, the M0, in plastic</a>, but even this couldn’t hope to meet the mark. The problem, according to engineers at the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Urbana-Champaign</a> and at British flexible-electronics manufacture <a href="https://www.pragmaticsemi.com/" rel="noopener noreferrer" target="_blank">PragmatIC Semiconductor</a>, is that even the simplest industry-standard microcontrollers are too complex to make on plastic in bulk.
</p><hr/><p>
	In research to be presented at the <a href="https://iscaconf.org/isca2022/" rel="noopener noreferrer" target="_blank">International Symposium on Computer Architecture</a> later this month, the transatlantic team presents a simple yet fully functional plastic processor that could be made at sub-penny prices. The Illinois team designed 4-bit and 8-bit processors specifically to minimize size and maximize the percentage of working integrated circuits produced. Eighty-one percent of the 4-bit version worked, and that’s a good enough yield, says team leader <a href="https://ece.illinois.edu/about/directory/faculty/rakeshk" rel="noopener noreferrer" target="_blank">Rakesh Kumar,</a> to breach the one-penny barrier.
</p><p>
	“Flexible electronics has been niche for decades,” says Kumar. He adds that this yield study shows “that they may be ready for the mainstream.”
</p><p>
	The processors his team built were made using the flexible thin-film semiconductor indium gallium zinc oxide (IGZO), which can be built on plastic and continues to work even when bent around a radius of millimeters. But while a reliable manufacturing process is a prerequisite, it was the design that made the difference.
</p><h3>Why Not Silicon?</h3><br/><p>You might be wondering why silicon processors can’t do the job of supercheap flexible computing. Kumar’s analysis suggest it won’t work. Compared to plastic, silicon is expensive and inflexible, but if you make the chip small enough, the plastic can just bend around it. However, silicon fails at the task for two reasons: One is that although the area of circuitry could be made supersmall, you still need to leave a comparatively large amount of space around the edges so that the chip can be cut out of the wafer. In the case of a microcontroller as simple as the Flexicore, there would be more space around the edge than there is area containing circuitry. What’s more, you’ll need still more room to fit enough I/O pads so data and power can get to the chip. Suddenly, you’ve got a large area of costly blank silicon, pushing up expenses past the critical US $0.01 mark.</p><p>
	Instead of adapting an existing microcontroller architecture to plastic, Kumar’s team started from scratch to create a design called Flexicore. “Yield goes down very quickly as you increase gate count,” says Kumar. Knowing that, they came up with a design meant to minimize the number of gates needed. Using 4-bit and 8-bit logic instead of 16-bit or 32-bit helped. As did separating the memory that stores instructions from the memory that stores data. But they also cut down on the number and complexity of the instructions the processor is capable of executing.
</p><p>
	The team further simplified, by designing the processor so it executes an instruction in a single clock cycle instead of the multistep pipelines of today’s CPUs. Then they designed logic that implements those instructions by reusing parts, further reducing the gate count. “In general, we were able to simplify the design of FlexiCores by tailoring them to the needs of flexible applications, which tend to be computationally simple,” says Nathaniel Bleier, Kumar’s student.
</p><p>
	All of this resulted in a 5.6-square-millimeter 4-bit FlexiCore made up of just 2,104 semiconductor devices (about the same as the number of transistors in an Intel 4004 from 1971) versus some 56,340 devices for PlasticARM. “It’s an order of magnitude less than the tiniest silicon microcontrollers in terms of gate count,” he says. The team also developed an 8-bit version of FlexiCore, but it did not yield as well.
</p><p>
	“This is exactly the kind of design innovation needed to support truly ubiquitous electronics,” says <a href="https://www.pragmaticsemi.com/about/management" rel="noopener noreferrer" target="_blank">Scott White</a>, CEO of PragmatIC Semiconductor.
</p><p>
	With PragmatIC, the Illinois team produced plastic-coated wafers full of 4-bit and 8-bit processors and tested them at a variety of voltages on multiple programs and bent them without mercy. The experiment seems basic, but according to Kumar, it’s groundbreaking. Most research processors built using nonsilicon technologies yield so poorly that results are reported from one or at best a few working chips. “This is the first work, to the best of our knowledge, where anyone reported data from multiple chips for any nonsilicon technology,” he says.
</p><p>
	Not satisfied with this success, Kumar’s team came up with a design tool to explore architectural optimizations for different applications. For example, the tool showed that power consumption could be reduced considerably by allowing the gate count to inch up a bit.
</p><p>
	The chip industry has been targeted toward “the metrics of power and performance and to some degree reliability,” observed Kumar. “We haven’t focused on cost, conformality, and thinness. Focusing on those allows us to build new computer architectures and target new applications.”
</p><p>
	Flexible electronics pioneer <a href="http://rogersgroup.northwestern.edu/" rel="noopener noreferrer" target="_blank">John A. Rogers</a>, at Northwestern University, called the work “very impressive.” He looks forward to experimental studies of the effects of bending on circuit performance.
</p><p>
<em>This article appears in the August 2022 print issue as “Sub-Penny Plastic Processors.”</em>
</p>]]></description><pubDate>Tue, 14 Jun 2022 14:45:25 +0000</pubDate><guid>https://spectrum.ieee.org/plastic-microprocessor</guid><category>Plastic electronics</category><category>Plastic arm</category><category>Computer architecture</category><category>Microcontrollers</category><category>Microprocessor design</category><category>Microprocessors</category><category>Flexible electronics</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/at-left-a-circle-with-repeated-texture-at-right-a-textured-square-with-sections-of-different-sizes-in-different-colors-with-t.png?id=29971002&amp;width=980"></media:content></item><item><title>Acer Goes Big on Glasses-Free, 3D Monitors—Look Out, VR</title><link>https://spectrum.ieee.org/glasses-free-3d-monitor-acer</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-standing-tablet-computer-shows-a-blow-out-of-a-car-that-appears-to-be-coming-out-of-the-display.jpg?id=29828093&width=1200&height=800&coordinates=0%2C62%2C0%2C62"/><br/><br/><p>Acer, the world’s fifth largest PC brand, wants to take the growing AR/VR market by the horns with its SpatialLabs glasses-free stereoscopic 3D displays.<br/></p><p><a href="https://www.acer.com/ac/en/US/content/conceptd-series/conceptd7spatiallabsedition" rel="noopener noreferrer" target="_blank">First teased in 2021 in a variant of Acer’s ConceptD 7 laptop</a>, the technology expands this summer in a pair of portable monitors, the SpatialLabs View and View Pro, and select Acer Predator gaming laptops. The launch is paired with artificial-intelligence-powered software for converting existing 2D content into stereoscopic 3D.</p><p>“We see a convergence of virtual and reality,” Jane Hsu, <a href="https://www.linkedin.com/in/hsujane/?originalSubdomain=tw" rel="noopener noreferrer" target="_blank">head of business Development for SpatialLabs</a>, said in an interview. “It’s a different form for users to start interacting with a virtual world.” Glasses-free stereoscopic 3D isn’t new. </p><h2>Evolutionary, not revolutionary</h2><p>The technology has powered several niche products and prototypes, <a href="https://venturebeat.com/2020/10/15/sony-reveals-spatial-reality-display-a-4k-screen-with-glasses-free-3d/" target="_blank">such as Sony’s Spatial Reality Display</a>, but its most famous debut was Nintendo’s 3DS portable game console.</p><p>The 3DS filtered two images through a display layer called a parallax barrier. This barrier controlled the angle an image reached the user’s eyes to create the 3D effect. Because angle was important, the 3DS used cameras that detected the user’s eyes and adjusted the image to compensate for viewing angle.</p><p class="pull-quote">“The PC in 2022 is encountering a lot of problems.”<br/>—Jerry Kao, Acer</p><p>Acer’s technology is similar. It also displays two images which are filtered through an “optical layer” and has cameras to track and compensate for the user’s viewing angle.</p><p>So, what’s different this time?</p><p>“The fundamental difference is that the computing power is way different, and resolution is way different,” said Hsu. “The Nintendo, that was 800 by 240. In a sense, the technology is the same, but over time it has improved for a crystal-clear, high-resolution experience.”</p><p>Resolution is important to this form of glasses-free 3D. Because it renders two images to create the 3D effect, the resolution of the display is cut in half on the horizontal axis when 3D is on. The 3DS cut resolution to 400 by 240 when 3D was on and blurry visuals were <a href="https://www.theverge.com/2014/10/21/7025667/new-nintendo-3ds-review" target="_blank">a common complaint among critics</a>.</p><p>Acer’s SpatialLabs laptops and displays are a big improvement. Each provides native 4K (3,840 by 2,160 resolution) in 2D. That’s 43 times the pixel count of Nintendo’s 3DS. Turning 3D on shaves resolution to 1,920 by 2,160, which, while lower, is still sharper than that of a 27-inch 4K monitor.</p><p>Hsu says advancements in AI compute are also key. Partners like Nvidia and Intel can now accelerate AI in hardware, a feature that wasn’t common a half decade ago.</p><p>Acer has harnessed this for SpatialLabs GO, a software utility that can convert full-screen content from 2D to stereoscopic 3D. This should make SpatialLabs useful with a wider range of content. It can also help creators generate content for use in stereoscopic 3D by importing and converting existing assets.</p><h2>A new angle on augmented reality</h2><p><a href="https://www.digitaltrends.com/computing/windows-mixed-reality-motion-controllers-build-2017/" target="_blank">Acer was a lead partner in Microsoft’s push for mixed-reality headsets</a>. They were a flop, and their failure taught Acer hard lessons about how people approach AR/VR hardware in the real world.</p><p>“Acer spent a lot bringing VR headsets to market, but...it was not very successful,” <a href="https://www.acer-group.com/ag/en/TW/content/management-team-jerry-kao" rel="noopener noreferrer" target="_blank">Acer Co-COO Jerry Kao said in an interview</a>. “There were limitations. It’s not comfortable, or it’s expensive, and you need space around you. So, we wanted to address this.”</p><p>SpatialLabs is a complementary alternative. Creators can use Spatial Labs to achieve a 3D effect in their home office without pushing aside furniture. The Acer View Pro, meant for commercial use, may have a future in retail displays, a use that headsets can't address.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A display in a store shows a shoe that seems to be coming out of the screen." class="rm-shortcode" data-rm-shortcode-id="9fa4c29e29fb443c69312303b6b6c186" data-rm-shortcode-name="rebelmouse-image" id="7f8cf" loading="lazy" src="https://spectrum.ieee.org/media-library/a-display-in-a-store-shows-a-shoe-that-seems-to-be-coming-out-of-the-screen.jpg?id=29828109&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The View Pro display is built for use in kiosks and retail displays.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Acer</small></p><p>Most of the SpatialLabs product line, including the ConceptD 7 laptop and View displays, lean toward creative professionals using programs like Maya and Blender to create 3D content. Acer says its software suite has “out-of-the-box support for all major file formats.” It recently added support for Datasmith, a plug-in used to import assets to <a href="https://spectrum.ieee.org/unreal-engine-5-metaverse" target="_self">Epic’s Unreal Engine</a>.</p><p>But the technology is also coming to Predator gaming laptops for glasses-free stereoscopic 3D in select titles like <em>Forza Horizon 5</em> and <em>The Witcher 3: Wild Hunt</em>. Gaming seems a natural fit given its history in Nintendo’s handheld, and Hsu thinks it will help attract mainstream attention.</p><p>“When the Turn 10 team [developer of the Forza Horizon series] saw what we had done with <em>Forza Horizon 5</em>, they were like, ‘Wow, this is so great!’ ” said Hsu. “They said, ‘You know what? I think I can build the scene with even more depth.’ And this is just the beginning.”</p><h2>Does glasses-free 3D really stand a chance?</h2><p>SpatialLabs brings gains in resolution and performance, but it’s far from a surefire hit. Acer is the only PC maker currently pursuing the hardware. Going it alone won’t be easy.</p><p>“While the tech seems quite appealing, it will likely remain a niche product that’ll be used in rare instances by designers or developers rather than the average consumer,” <a href="https://www.idc.com/getdoc.jsp?containerId=PRF003967" rel="noopener noreferrer" target="_blank">Jitesh Ubrani, research manager at IDC</a>, said in an email. He thinks Acer could find it difficult to deliver on price and availability, “both of which are tough to do for such a fringe technology.”</p><p>I asked Hsu how Acer will solve these issues. “In a way he’s right, it is difficult. We’re building this ourselves,” said Hsu. “But also, the hardware is more mature.”</p><p>Kao chimed in to say SpatialLabs will stand out in what might be weak year for home computers. “The PC in 2022 is encountering a lot of problems,” Kao said. He sees that as a motivation, not a barrier, for novel technology on the PC.</p><p>“Intel, Google, Microsoft, and a lot of people, they have technology,” said Kao. “But they don’t know how to leverage that technology in the product and deliver the experience to specific people. That is what Acer is good at.”</p>]]></description><pubDate>Fri, 20 May 2022 14:54:57 +0000</pubDate><guid>https://spectrum.ieee.org/glasses-free-3d-monitor-acer</guid><category>Acer</category><category>3d tv</category><category>Augmented reality</category><category>Glasses-free 3d</category><category>Virtual reality</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-standing-tablet-computer-shows-a-blow-out-of-a-car-that-appears-to-be-coming-out-of-the-display.jpg?id=29828093&amp;width=980"></media:content></item><item><title>Simple, Cheap, and Portable: A Filter-Free Desalination System for a Thirsty World</title><link>https://spectrum.ieee.org/portable-desalination-filter-free</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-hard-case-contains-a-white-device-with-beige-layers-with-wires-connecting-to-electronics-on-the-top-of-the-interior-of-t.jpg?id=29796719&width=1200&height=800&coordinates=0%2C137%2C0%2C0"/><br/><br/><p>MIT researchers have developed a prototype of a suitcase-size device that can turn seawater into safe drinking water.<br/></p><p>According to the <a href="https://idadesal.org/" target="_blank">International Desalination Association</a>, more than 300 million people around the world now get their drinking water from the sea. With climate change exacerbating water scarcity globally, seawater <a href="https://e360.yale.edu/features/as-water-scarcity-increases-desalination-plants-are-on-the-rise" target="_blank">desalination</a> is stepping in to fill the void. But whereas commercial desalination plants are designed to meet large-scale demand, there is also a need for portable systems that can be carried into remote regions or set up as stand-ins for municipal water works in the wake of a disaster.</p><p>A group of scientists from MIT has <a href="https://pubs.acs.org/doi/10.1021/acs.est.1c08466" target="_blank">developed</a> just such a portable desalination unit; it’s the size of a medium suitcase and weighs less than 10 kilograms. The unit’s one-button operation requires no technical knowledge. What’s more, it has a completely filter-free design. Unlike existing portable desalination systems based on reverse osmosis, the MIT team’s prototype does not need any high-pressure pumping or maintenance by technicians.</p><p>The MIT researchers described their invention in a paper titled “Portable Seawater Desalination System for Generating Drinkable Water in Remote Locations.” The paper was posted in the 14 April online edition of <em>Environmental Science & Technology</em>, a publication of the American Chemical Society.</p><p>The unit uses produces 0.3 liters of potable drinking water per hour, while consuming a minuscule 9 watt-hours of energy.  Plant-scale reverse-osmosis water-treatment operations may be three to four times as energy efficient, and yield far greater quantities of freshwater at much faster rates, but the researchers say the trade-off in terms of weight and size makes their invention the first and only entrant in a new desalination niche.</p><p>The most notable feature of the unit is its unfiltered design. A filter is a barrier that catches the impurities you don’t want in your water, explains <a href="https://be.mit.edu/directory/jongyoon-han" target="_blank">Jongyoon Han</a>, an electrical and biological engineer, and lead author of the study. “We don’t have that specifically because it always tends to clog, and [then] you need to replace it.” This makes traditional portable systems challenging for laypeople to use. Instead, the researchers use ion-concentration polarization (ICP) and electrodialysis (ED) to separate the salt from the water.</p><p>“Instead of filtering, we are nudging the contaminants [in this case, salt] away from the water,” Han says. This portable unit, he adds, is a good demonstration of the effectiveness of ICP desalination technology. “It is quite different from other technologies, in the sense that I can remove both large particles and solids all together.”</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Hands hold a frame which contains a white rectangle of material with 6 strips on top of it." class="rm-shortcode" data-rm-shortcode-id="07f4bdb37b5ea60e8bdcdfd220bd8915" data-rm-shortcode-name="rebelmouse-image" id="0c150" loading="lazy" src="https://spectrum.ieee.org/media-library/hands-hold-a-frame-which-contains-a-white-rectangle-of-material-with-6-strips-on-top-of-it.jpg?id=29796725&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The setup includes a two-stage ion-concentration polarization (ICP) process, with water flowing through six modules in the first stage and then three in the second stage, followed by a single electrodialysis process.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">M. Scott Brauer</small></p><p>ICP uses an ion-selective membrane that allows the passage of one kind of ion when current is applied—either cations or anions. “What happens is that, [if] these membranes can transfer only cations, what about the anions?” Han asks. “The anions disappear near the membrane because nature really doesn’t like free ions hanging around…. So, [as a result, there is a region] near the membrane that is salt-free.” The salt-free region is the spot from which freshwater is harvested.</p><p>“What is unique about our technology is that we figured out a way to separate…a diverse array of contaminants [from water] in a single process,” says Han. “So we can go [straight] from seawater to drinkable water.”</p><p>It takes 40 liters of seawater to yield a single liter of drinking water. This 2.5 percent recovery rate might seem like a high environmental cost, says Junghyo Yoon, a researcher at Han’s lab. But Yoon reminds us that seawater is an infinite resource, so a low recovery rate is not a significant issue.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A hand adjusts a screw on a white box which sandwiches beige layers of material." class="rm-shortcode" data-rm-shortcode-id="6081612af901d0d941f5ddef250b97e3" data-rm-shortcode-name="rebelmouse-image" id="5549f" loading="lazy" src="https://spectrum.ieee.org/media-library/a-hand-adjusts-a-screw-on-a-white-box-which-sandwiches-beige-layers-of-material.jpg?id=29796726&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The portable device does not require any replacement filters, which greatly reduces the long-term maintenance requirements.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">M. Scott Brauer</small></p><p>The MIT group’s device is an out-of-the box system; you can just power it up, connect it to a saltwater source, and wait for potable water. “The box includes the battery and…[it is] like a typical laptop battery, anywhere between 60 and 100 watts,” Han says. “We think that that can operate for about a day or so.” A solar panel is another option, especially in a disaster zone, where there might not be an accessible electric power source.</p><p>Yoon points out that the results reported in the group’s paper are already a year old. “[Since we recorded the results listed in the paper], we have successfully ramped up the desalination rate to 1 liter [of freshwater] per hour,” he reports. “We are pushing ourselves to scale up to 10 liters per hour for practical applications.” He hopes to secure enough investment by the end of this year to take the next steps toward commercialization. “We expect that we can have the first prototype available for beta testing by the end of 2023. [We predict that] he cost will be [US] $1,500,” says Yoon.</p><p>That price will be far cheaper than portable desalination systems currently on the market—mostly models using reverse-osmosis filtration, which go for around $5,000. “Although they have higher flow rates and generate a larger amount of clean water because [they] are bigger, they are generally not so user friendly,” Han says. “Our system is much smaller, and uses much less power. And the goal here is to generate just enough water, in a manner that is very user friendly to address this particular need of disaster relief.”</p><p>Aside from the flow rate, Han is also not happy with the device’s energy consumption at present. “We don’t think is actually optimal,” he says. “Although [its energy efficiency] is good enough, it can always be made better by optimizing the [process].”</p>]]></description><pubDate>Tue, 17 May 2022 17:08:25 +0000</pubDate><guid>https://spectrum.ieee.org/portable-desalination-filter-free</guid><category>Desalination</category><category>Climate change</category><category>Portable devices</category><dc:creator>Payal Dhar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-hard-case-contains-a-white-device-with-beige-layers-with-wires-connecting-to-electronics-on-the-top-of-the-interior-of-t.jpg?id=29796719&amp;width=980"></media:content></item><item><title>Reduce EMI and EMC Issues with Engineering Simulation Software</title><link>https://engineeringresources.spectrum.ieee.org/free/w_defa2541/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/reduce-emi-and-emc-issues-with-engineering-simulation-software.gif?id=29782039&width=1200&height=800&coordinates=15%2C0%2C15%2C0"/><br/><br/><p>Electronic components and systems exist today in nearly all consumer and industrial products. A major design consideration in all electronics is electromagnetic interference (EMI) and compatibility (EMC). EMI and EMC issues are complex. They can be hard to detect and can be taxing to a design. With the use of engineering simulation software, design engineers can mitigate issues before entering the prototype testing phase. Avoiding the test-retest cycle with simulation can help save time and money all while delivering robust and reliable products.</p>]]></description><pubDate>Tue, 10 May 2022 16:01:06 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_defa2541/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>SimuTech Group</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/reduce-emi-and-emc-issues-with-engineering-simulation-software.gif?id=29782039&amp;width=980"></media:content></item><item><title>Tony Fadell: The Nest Thermostat Disrupted My Life</title><link>https://spectrum.ieee.org/nest-thermostat</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-holds-a-circular-device-in-front-of-a-blue-wall-that-says-nest-on-it.jpg?id=29755091&width=1200&height=800&coordinates=31%2C0%2C0%2C0"/><br/><br/><p>
<strong>The thermostat chased</strong> me for 10 years.
</p><p>
	That is pretty extreme, by the way. If you’ve got an idea for a business or a new product, you usually don’t have to wait a decade to make sure it’s worth doing.
</p><p>
	For most of the 10 years that I idly thought about thermostats, I had no intention of building one. It was the early 2000s, and I was at <a href="http://www.Apple.com" target="_blank">Apple</a> making the first iPhone. I got married, had kids. I was busy.
</p><p>
	But then again, I was also really cold. Bone-chillingly cold.
</p><p>
	Every time my wife and I drove up to our Lake Tahoe ski cabin on Friday nights after work, we’d have to keep our snow jackets on until the next day. The house took all night to heat up.
</p><h3></h3><br/><img alt="Book cover for Build by Tony Fadell" class="rm-shortcode" data-rm-shortcode-id="4d74e5469cb364c7f4243417b35c0392" data-rm-shortcode-name="rebelmouse-image" id="21d34" loading="lazy" src="https://spectrum.ieee.org/media-library/book-cover-for-build-by-tony-fadell.jpg?id=29760293&width=980"/><p>Adapted from the book <em><a href="https://www.amazon.com/Build-Unorthodox-Guide-Making-Things/dp/0063046067/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=1647967135&sr=8-1" rel="noopener noreferrer" target="_blank">BUILD: An Unorthodox Guide to Making Things Worth Making</a></em> by Tony Fadell. Copyright 2022 by Tony Fadell. Reprinted by permission of Harper Business, an imprint of HarperCollins Publishers.</p><h3></h3><br/><p>Walking into that frigid house drove me nuts. It was mind-boggling that there wasn’t a way to warm it up before we got there. I spent dozens of hours and thousands of dollars trying to hack security and computer equipment tied to an analog phone so I could fire up the thermostat remotely. Half my vacations were spent elbow-deep in wiring, electronics littering the floor. But nothing worked. So the first night of every trip was always the same: We’d huddle on the ice block of a bed, under the freezing sheets, watching our breath turn into fog until the house finally warmed up by morning.</p><p>Then on Monday I’d go back to Apple and work on the first iPhone. Eventually I realized I was making a perfect remote control for a thermostat. If I could just connect the HVAC system to my iPhone, I could control it from anywhere. But the technology that I needed to make it happen—reliable low-cost communications, cheap screens and processors—didn’t exist yet.</p><h3></h3><br/><p>How did these ugly, piece-of-crap thermostats cost almost as much as Apple’s most cutting-edge technology?</p><h3></h3><br/><p>A year later we decided to build a new, superefficient house in Tahoe. During the day I’d work on the iPhone, then I’d come home and pore over specs for our house, choosing finishes and materials and solar panels and, eventually, tackling the HVAC system. And once again, the thermostat came to haunt me. All the top-of-the-line thermostats were hideous beige boxes with bizarrely confusing user interfaces. None of them saved energy. None could be controlled remotely. And they cost around US $400. The iPhone, meanwhile, was selling for $499.</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>How did these ugly, piece-of-crap thermostats cost almost as much as Apple’s most cutting-edge technology?<br/></p><p>
	The architects and engineers on the Tahoe project heard me complaining over and over about how insane it was. I told them, “One day, I’m going to fix this—mark my words!” They all rolled their eyes—there goes Tony complaining again!
</p><p>
	At first they were just idle words born of frustration. But then things started to change. The success of the iPhone drove down costs for the sophisticated components I couldn’t get my hands on earlier. Suddenly high-quality connectors and screens and processors were being manufactured by the millions, cheaply, and could be repurposed for other technology.
</p><p>
	My life was changing, too. I quit Apple and began traveling the world with my family. A startup was not the plan. The plan was a break. A long one.
</p><p>
	We traveled all over the globe and worked hard not to think about work. But no matter where we went, we could not escape one thing: the goddamn thermostat. The infuriating, inaccurate, energy-hogging, thoughtlessly stupid, impossible-to-program, always-too-hot-or-too-cold-in-some-part-of-the-house thermostat.
</p><p>
	Someone needed to fix it. And eventually I realized that someone was going to be me.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Hardware including a square with electronics and paper with CAD electronic diagrams." class="rm-shortcode" data-rm-shortcode-id="b2ac20a01cbbc0ff34d127f61c79da8b" data-rm-shortcode-name="rebelmouse-image" id="b8c1b" loading="lazy" src="https://spectrum.ieee.org/media-library/hardware-including-a-square-with-electronics-and-paper-with-cad-electronic-diagrams.jpg?id=29760285&width=980"/>
<small class="image-media media-caption" data-gramm="false" data-lt-tmp-id="lt-320298" placeholder="Add Photo Caption..." spellcheck="false">This 2010 prototype of the Nest thermostat wasn’t pretty. But making the thermometer beautiful would be the easy part. The circuit board diagrams point to the next step—making it round.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tom Crabtree</small></p><p>
	The big companies weren’t going to do it. Honeywell and the other white-box competitors hadn’t truly innovated in 30 years. It was a dead, unloved market with less than $1 billion in total annual sales in the United States.
</p><p>
	The only thing missing was the will to take the plunge. I wasn’t ready to carry another startup on my back. Not then. Not alone.
</p><p>
	Then, magically, Matt Rogers, who’d been one of the first interns on the iPod project, reached out to me. He was a real partner who could share the load. So I let the idea catch me. I came back to Silicon Valley and got to work. I researched the technology, then the opportunity, the business, the competition, the people, the financing, the history.
</p><p>Making it beautiful wasn’t going to be hard. Gorgeous hardware, an intuitive interface—that we could do. We’d honed those skills at Apple. But to make this product successful—and meaningful—we needed to solve two big problems:</p><p> It needed to save energy.</p><p>
	And we needed to sell it.
</p><p>
	In North America and Europe, thermostats control half a home’s energy bill—something like $2,500 a year. Every previous attempt to reduce that number—by thermostat manufacturers, by energy companies, by government bodies—had failed miserably for a host of different reasons. We had to do it for real, while keeping it dead simple for customers.
</p><p>
	Then we needed to sell it. Almost all thermostats at that point were sold and installed by professional HVAC technicians. We were never going to break into that old boys’ club. We had to find a way into people’s minds first, then their homes. And we had to make our thermostat so easy to install that literally anyone could do it themselves.
</p><p>
	It took around 9 to 12 months of making prototypes and interactive models, building bits of software, talking to users and experts, and testing it with friends before Matt and I decided to pitch investors.
</p><h3>“Real People” Test the Nest</h3><p>
	Once we had prototypes of the thermostat, we sent it out to real people to test.
</p><p>It was fatter than we wanted. The screen wasn’t quite what I imagined. Kind of like the first iPod, actually. But it worked. It connected to your phone. It learned what temperatures you liked. It turned itself down when nobody was home. It saved energy. We knew self-installation was potentially a huge stumbling block, so everyone waited with bated breath to see how it went. Did people shock themselves? Start a fire? Abandon the project halfway through because it was too complicated? Soon our testers reported in: Installation went fine. People loved it. But it took about an hour to install. Crap. An hour was way too long. This needed to be an easy DIY project, a quick upgrade.</p><p>
	So we dug into the reports—what was taking so long? What were we missing?
</p><p class="pull-quote">Our testers...spent the first 30 minutes looking for tools.</p><p>Turns out we weren’t missing anything—but our testers were. They spent the first 30 minutes looking for tools—the wire stripper, the flathead screwdriver; no, wait, we need a Phillips. Where did I put that?<br/></p><p>
	Once they gathered everything they needed, the rest of the installation flew by. Twenty, 30 minutes tops.
</p><p>
	I suspect most companies would have sighed with relief. The actual installation took 20 minutes, so that’s what they’d tell customers. Great. Problem solved.
</p><p>
	But this was going to be the first moment people interacted with our device. Their first experience of Nest. They were buying a $249 thermostat—they were expecting a different kind of experience. And we needed to exceed their expectations. Every minute from opening the box to reading the instructions to getting it on their wall to turning on the heat for the first time had to be incredibly smooth. A buttery, warm, joyful experience.
</p><p>
	And we knew Beth. Beth was one of two potential customers we defined. The other customer was into technology, loved his iPhone, was always looking for cool new gadgets. Beth was the decider—she dictated what made it into the house and what got returned. She loved beautiful things, too, but was skeptical of supernew, untested technology. Searching for a screwdriver in the kitchen drawer and then the toolbox in the garage would not make her feel warm and buttery. She would be rolling her eyes. She would be frustrated and annoyed.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A white handheld device with 4 screwdriver heads, one on the bottom, and three at the top." class="rm-shortcode" data-rm-shortcode-id="ee497142bc8ce92c5aebccdaad2f3a39" data-rm-shortcode-name="rebelmouse-image" id="267fc" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-handheld-device-with-4-screwdriver-heads-one-on-the-bottom-and-three-at-the-top.jpg?id=29769357&width=980"/>
<small class="image-media media-caption" data-gramm="false" data-lt-tmp-id="lt-863492" placeholder="Add Photo Caption..." spellcheck="false">Shipping the Nest thermostat with a screwdriver "turned a moment of frustration into a moment of delight"</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Dwight Eschliman</small></p><p>
	So we changed the prototype. Not the thermostat prototype—the installation prototype. We added one new element: a little screwdriver. It had four different head options, and it fit in the palm of your hand. It was sleek and cute. Most importantly, it was unbelievably handy.
</p><p>
	So now, instead of rummaging through toolboxes and cupboards, trying to find the right tool to pry their old thermostat off the wall, customers simply reached into the Nest box and took out exactly what they needed. It turned a moment of frustration into a moment of delight.
</p><h3>Honeywell Laughs</h3><p>
	Sony laughed at the iPod. Nokia laughed at the iPhone. Honeywell laughed at the Nest Learning Thermostat.
</p><p>
	At first.
</p><p>
	In the stages of grief, this is what we call Denial.
</p><p>But soon, as your disruptive product, process, or business model begins to gain steam with customers, your competitors will start to get worried. And when they realize you might steal their market share, they’ll get pissed. Really pissed. When people hit the Anger stage of grief, they lash out, they undercut your pricing, try to embarrass you with advertising, use negative press to undermine you, put in new agreements with sales channels to lock you out of the market.</p><p>
	And they might sue you.
</p><p>
	The good news is that a lawsuit means you’ve officially arrived. We had a party the day Honeywell sued Nest. We were thrilled. That ridiculous lawsuit meant we were a real threat and they knew it. So we brought out the champagne. That’s right, f---ers. We’re coming for your lunch.
</p><h3>Nest Gets Googled</h3><p>
	With every generation, the product became sleeker, slimmer, and less expensive to build. In 2014, Google bought Nest for $3.2 billion. In 2016 Google decided to sell Nest, so I left the company. Months after I left, Google changed its mind. Today, Google Nest is alive and well, and they’re still making new products, creating new experiences, delivering on their version of our vision. I deeply, genuinely, wish them well.
</p>]]></description><pubDate>Sat, 07 May 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/nest-thermostat</guid><category>Consumer electronics</category><category>Nest</category><category>Iot</category><category>Internet of things</category><category>Design case history</category><category>History of technology</category><dc:creator>Tony Fadell</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-holds-a-circular-device-in-front-of-a-blue-wall-that-says-nest-on-it.jpg?id=29755091&amp;width=980"></media:content></item><item><title>Efficient Testing of Multiport EW Receivers</title><link>https://engineeringresources.spectrum.ieee.org/free/w_rohd42/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/efficient-testing-of-multiport-ew-receivers.gif?id=29765722&width=1200&height=800&coordinates=71%2C0%2C72%2C0"/><br/><br/><p>Modern multiport RWRs require modern T&M solutions! Are you interested in generating realistic test signals – in the lab? This webinar can help!</p> <p><strong>Speakers</strong>:</p> <p><strong>Leander Humbert</strong>, <em>Market Segment Manager ADT, Rohde & Schwarz</em></p> <ul> <li>Leander Humbert is a radar technology manager at Rohde & Schwarz. He completed his master's degree in electrical engineering in 2004 at the Helmut Schmidt University in Hamburg. Between 2004 and 2011, he worked as a system engineer for electronic warfare systems at the German Air Force. From 2012 to 2016, he was a technical counselor for missile and air defense related sensor systems at IABG. At Rohde & Schwarz, he deals with all test and measurement aspects and requirements of current and future radar sensors.</li> </ul> <p><strong>Robert Vielhuber</strong>, <em>Senior Product Manager for RF Signal Generators, Rohde & Schwarz</em></p> <ul> <li>Robert Vielhuber covers products for Aerospace & Defense and Automotive applications. He is responsible for the global technical and commercial success of the Rohde & Schwarz radar signal simulation solutions. Robert is a senior specialist in Product Management and Marketing with more than two decades of experience in product management and marketing of electronic high-tech investment goods. He has significant broad experience also due to his earlier experience, where he was responsible for Strategic and Operational Marketing for the Broadcasting products at Rohde & Schwarz. He holds a degree in communication engineering from the University of Applied Sciences in Munich.</li> </ul> <p><strong>Florian Gerbl</strong>, <em>Application Engineer, Rohde & Schwarz</em></p> <ul> <li>Florian Gerbl is an application engineer with Rohde & Schwarz and has a focus on signal generators.</li> </ul>]]></description><pubDate>Thu, 05 May 2022 19:00:01 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_rohd42/prgm.cgi</guid><category>Type:webinar</category><dc:creator>Rohde &amp; Schwarz</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/efficient-testing-of-multiport-ew-receivers.gif?id=29765722&amp;width=980"></media:content></item><item><title>Switching Analysis: Testing for Reliability in Power Converter Design</title><link>https://engineeringresources.spectrum.ieee.org/free/w_rohd41/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/switching-analysis-testing-for-reliability-in-power-converter-design.gif?id=29741542&width=1200&height=800&coordinates=71%2C0%2C72%2C0"/><br/><br/><p>This webinar is intended for engineers who work on power converter designs. We will introduce you to testing for reliability as well as the related challenges and pain points, together with the latest methods and techniques to address them. You will learn tips, traps and tricks for making best use of trigger capabilities of oscilloscopes. Practical examples will illustrate the discussions.</p> <p><strong>Speakers</strong>:</p> <p><strong>Dr. Markus Herdin</strong>, <em>Market Segment Manager</em></p> <ul> <li>Markus Herdin is a seasoned market development professional with a focus on power electronics test & measurement applications. His previous experience at Rohde & Schwarz includes roles in product management, product development and corporate business development.</li> </ul>]]></description><pubDate>Fri, 29 Apr 2022 15:58:02 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_rohd41/prgm.cgi</guid><category>Type:webinar</category><dc:creator>Rohde &amp; Schwarz</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/switching-analysis-testing-for-reliability-in-power-converter-design.gif?id=29741542&amp;width=980"></media:content></item><item><title>Evaluating Mobile Health Tools Is Comparing Apples to Oranges</title><link>https://spectrum.ieee.org/mobile-health</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-hands-holding-a-phone-there-is-a-medical-symbol-in-the-background-with-colored-lines-coming-out-of-it-in-a-disa.jpg?id=29692816&width=1200&height=800&coordinates=0%2C17%2C0%2C18"/><br/><br/><p>Like a doctor on one’s wrist, mobile health (mHealth) tools offer the promise of providing personal physiological data on demand.</p><p>How many steps did you take today? Press the button. What’s your current heart rate or amount of oxygen in your blood? Push the button. Glucose level? Scan the sensor on your arm.</p><p>Unlike a human physician or nurse, though, the digital tools and the mobile apps they often pair with have no way of identifying how each user might respond to them. What might be motivating for one person, such as a personalized encouraging message, may seem intrusive to another. A study published in <em><a href="https://misq.umn.edu/empowering-patients-using-smart-mobile-health-platforms-evidence-of-a-randomized-field-experiment.html" target="_blank">Management Information Systems Quarterly</a></em> in February 2022 examined a digital diabetes management tool through the experiences of 1,070 patients in Asia. The study’s authors found a generic SMS messaging scheme was 18 percent more effective in lowering a patient’s glucose level than a personalized patient-specific message string.</p><p>Moreover, the authors found, “personalization is not as effective as non-personalization if we try to improve diabetes patients’ engagement with the app usage or general life style (i.e., sleeping behavior or movement habits). This is likely because patients might perceive frequent personalized SMS messages as intrusive and annoying.”</p><p>The authors—a group of researchers at <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a>, in Pittsburgh, <a href="http://en.hit.edu.cn/" target="_blank">Harbin Institute of Technology</a>, in China, and <a href="https://www.nyu.edu/" target="_blank">New York University</a>—add that “these findings are surprising and suggest personalized messaging may not always work in the context of mHealth, and the design of the mHealth platform is critical in achieving better patient health outcomes.”</p><p>And, according to those who study the field, there isn’t yet a common approach to assessing the effectiveness of developers’ and researchers’ cultural adaptation of these tools, including aspects of personalization, for different user bases.</p><p>“The implementation science around how to translate digital health tools that perform well in silico into real-world utility, in the form of desired behavior change and better patient outcomes, is still a very nascent field. There is still a lot of work to be done,” says<a href="https://dbmi.hms.harvard.edu/people/jayson-marwaha" target="_blank"> Jayson Marwaha</a>, a postdoctoral research fellow at Harvard University.</p><p>Some researchers have tackled the issue of adapting mHealth tools to different cultures. For instance, in 2020 a team at the <a href="https://www.zhaw.ch/en/university/" target="_blank">Zurich University of Applied Sciences</a> published a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7644382/" rel="noopener noreferrer" target="_blank">comparison survey</a> of Swiss and Chinese consumers and found markedly different reasons why a person might use one depending on the culture of each nation.</p><p>A Swiss consumer might start using an mHealth tool based on a physician’s endorsement and evidence that the device was accurate, they found. A Chinese consumer, however, would be more likely to consider the opinions of members of their social circle and employers, as well as devices that could augment a stretched-thin health system with credible advice.</p><p>The Zurich University team hasn’t pursued the cultural components of Internet or mHealth tool acceptance further. But another group, at the <a href="https://uni-freiburg.de/en/" target="_blank">University of Freiburg</a> and <a href="https://www.uni-ulm.de/en/" target="_blank">Ulm University</a>, in Germany, has in several meta-analyses. Those analyses have found that evaluating the efficacy of these interventions is still very much an apples-to-oranges situation, which may be inhibiting faster and wider adoption of them.</p><p>For example, one of the Ulm researchers, <a href="https://de.linkedin.com/in/sumeyyebalci" target="_blank">Sümeyye Balci</a>, said the tone of SMS messaging is just one aspect of trying to keep participants motivated to keep going with a trial in which they are enrolled.<br/><br/>“The bigger issue in cultural adaptation studies is that we still don’t know to what extent we should adapt an intervention’s content or delivery method, and for which population,” Balci said. “So it’s not entirely clear what works best for which group and what behavior. That’s what we’re trying to understand in our group.”</p><p>Balci and her colleagues have laid out <a href="https://www.nature.com/articles/s41746-021-00498-1" rel="noopener noreferrer" target="_blank">17 discrete components</a> of cultural adaptation that should be considered in deploying a digital tool (Internet-based or mobile) in disparate cultural groups. They outlined these components in the context of mental-health tools, but Balci says they could be used for any tool for any condition; some elements could be given less weight or discarded entirely, depending on the tool’s purpose. For instance, intense personalization may be deemed intrusive by one group for a diabetes tool—such as the Carnegie Mellon/Harbin/New York University group found—but expected and welcomed for a behavioral therapy app.</p><p>Marwaha recently coauthored an editorial in <a href="https://www.nature.com/articles/s41746-021-00516-2" rel="noopener noreferrer" target="_blank"><em>NPJ Digital Medicine</em></a> that called for digital health-tool developers to use those 17 components as a guide when deploying tools across disparate populations. “I think it is a very helpful initial attempt at reducing heterogeneity in how people do these kinds of adaptation efforts,” he said. “Identifying a comprehensive list of all the things you should consider is an incredibly important start.”</p><p>The Freiburg/Ulm group’s latest <a href="https://www.nature.com/articles/s41746-022-00569-x" rel="noopener noreferrer" target="_blank">study</a> is a meta-analysis of 13 studies that looks at mHealth cultural-adaptation efforts across the subjects of healthy eating, physical activity, alcohol consumption, sexual-health behavior, and smoking cessation. The results led the group to conclude that those efforts currently haven’t shown they are worth the effort (only culturally adapted physical activity platforms were superior to control group results). But neither Balci nor Marwaha say that means cultural adaptations aren’t important. Balci says the paper isn’t meant as an argument to halt them entirely, but rather to find common ground in how best to measure their effectiveness: “We should work on specifying to what extent we should do it, or for which population we should do it.”</p><p>Likewise, Marwaha says that concluding that such personalization and adaptation isn’t important is the wrong idea. Instead, he said, “it will just take further study to figure out how to do it right and how to do it in a standardized, consistent fashion. The way researchers are doing it now—at least as seen in the data—doesn’t seem to be improving the clinical impact of these tools, and the clinical impact is what really matters.”</p>]]></description><pubDate>Mon, 18 Apr 2022 14:49:00 +0000</pubDate><guid>https://spectrum.ieee.org/mobile-health</guid><category>Mobile health</category><category>Medical devices</category><category>Digital health</category><dc:creator>Greg Goth</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-hands-holding-a-phone-there-is-a-medical-symbol-in-the-background-with-colored-lines-coming-out-of-it-in-a-disa.jpg?id=29692816&amp;width=980"></media:content></item><item><title>Will the Unreal Engine 5 Realize the Metaverse’s Potential?</title><link>https://spectrum.ieee.org/unreal-engine-5-metaverse</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-female-avatar-is-repeated-endlessly-in-rows-in-the-front-of-one-area-is-the-same-avatar-positioned-slightly-differently-next.jpg?id=29676354&width=1200&height=800&coordinates=0%2C0%2C310%2C0"/><br/><br/><p><em></em>The race to build the metaverse is on—but first, developers need the tools to do it.<br/></p><p><a href="https://www.unrealengine.com/en-US/unreal-engine-5" target="_blank">Unreal Engine 5</a>, released on 5 April, is among the leaders. Its creator, Epic Games, is best known for its megahit game <em>Fortnite</em>, but the company’s engine expertise has deep roots. The engine spun off from the 1998 shooter <em>Unreal</em> and seized an early lead when Epic licensed it for use in over a dozen games from 1999 to 2001.</p><p>“I think Unreal Engine 5 is a great step forward for the metaverse and augmented reality/virtual reality, especially when you consider that a lot of what the company has been building is thinking about the metaverse and AR/VR,” said <a href="https://moorinsightsstrategy.com/anshel-sag/" rel="noopener noreferrer" target="_blank">Anshel Sag</a>, principal analyst at Moor Insights & Strategy.</p><p>After all, immersion and real-time, photorealistic rendering that's as all-encompassing an experience as possible is the name of the game.  </p><strong></strong><p>To that end, Unreal Engine 5 (UE5) packs in dozens of features and refinements, but two take the spotlight: Lumen and Nanite.<br/></p><p>Lumen can handle global illuminations that include diffuse reflections with potentially infinite bounces. If you think this sounds like ray tracing, you’re right. Though handled in software, which reduces accuracy, early demos show results that <a href="https://www.youtube.com/watch?v=ehuKmbIXvtQ" rel="noopener noreferrer" target="_blank">hold up well at a glance</a>. Nanite, meanwhile, is a virtualized geometry system that can alter polygon counts in real time to meet a performance target. It helps developers achieve a level of optimization not previously possible.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="fb42f74daecc6378ae0701c1fb5a7b29" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-50MJf7hyOw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">What is virtualized micropolygon geometry? An explainer on Nanite | Unreal Engine 5</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=-50MJf7hyOw" target="_blank">www.youtube.com</a>
</small>
</p><p>“In my opinion, Nanite could be a vital component to helping VR experiences expand in scope for high-end VR, such as [PC-tethered VR a.k.a. <a href="https://ryanschultz.com/2021/07/31/is-pcvr-dead" target="_blank">PCVR</a>] or [Sony’s] <a href="https://www.tomsguide.com/news/psvr-2-release-date-price-new-controllers-leaks-and-latest-news" target="_blank">PSVR 2</a>, if it can help us to add more detail and scale to our worlds,” <a href="https://www.linkedin.com/in/alistair-hume-21876276/?originalSubdomain=uk" rel="noopener noreferrer" target="_blank">Alistair Hume</a>, cofounder of metaverse developer Axon Park, said in an e-mail.</p><p>These features are supported by <a href="https://www.unrealengine.com/en-US/metahuman-creator" target="_blank">MetaHuman</a>, a tool for generating photorealistic avatars; <a href="https://docs.unrealengine.com/5.0/en-US/world-partition-in-unreal-engine/" target="_blank">World Partition</a>, used for dividing world assets into groups for improved optimization; and <a href="https://quixel.com/bridge" target="_blank">Quixel Bridge</a>, used to import objects from <a href="https://quixel.com/megascans" target="_blank">Megascans</a>, a database of high-fidelity 3D assets.</p><p>Epic has leaned into metaverse hype by releasing <em>The Matrix Awakens: An Unreal Engine 5 Experience </em>alongside the film <em><a href="https://en.wikipedia.org/wiki/The_Matrix_Resurrections" target="_blank">The Matrix: Resurrections</a></em>. The demo delivers a virtual world as realistic as that in the first <em>Matrix</em> movie yet runs in real time on modern home consoles and PCs. UE5 includes the demo’s city as a free sample any developer can download—<a href="https://twitter.com/volodXYZ/status/1511854672574922757" rel="noopener noreferrer" target="_blank">which has already led to a viral hit inspired by <em>Superman</em></a><em>.</em></p><p>You could be forgiven for thinking UE5 is ready to dominate AR/VR and metaverse development. This notion is reinforced by the company’s other metaverse efforts, which <a href="https://www.epicgames.com/site/en-US/news/the-lego-group-and-epic-games-team-up-to-build-a-place-for-kids-to-play-in-the-metaverse" rel="noopener noreferrer" target="_blank">includes a partnership with the LEGO Group focused on a kid-friendly metaverse experience</a>.</p><p>Despite this, Lumen and Nanite do not support VR development at launch, and Epic has no timeline for their release. Developers hoping to use them must take a wait-and-see approach.</p><p>“[<a href="https://en.wikipedia.org/wiki/Extended_reality" target="_blank">Extended reality</a> or XR] developers need as much performance as they can get, and not being able to take advantage of Lumen or Nanite is challenging,” said Sag. “Unreal Engine is already a great engine for XR, but the lack of its flagship new features may turn off some developers.”</p><p>Epic thinks developers will look past the omission. “VR developers can leverage most of UE5’s production-ready tools and features now, such as the new [user experience], the new suite of modeling tools, creator tools such as Control Rig and MetaSounds, and World Partition for large open environments,” an Epic spokesperson said in an e-mail.</p><p>Developers have several alternatives, but one stands out: <a href="https://en.wikipedia.org/wiki/Unity_(game_engine)" target="_blank">Unity</a>. Though less frequently used by big-budget releases, <a href="https://naavik.co/deep-dives/unity-analysing-the-first-game-engine-ipo" rel="noopener noreferrer" target="_blank">Unity is the world’s most popular game engine by market share</a>.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Game avatars stand in a group in a city square with a large device including floating cyan gemstones." class="rm-shortcode" data-rm-shortcode-id="1f3aaf856cb3b3c93e88065d16afa386" data-rm-shortcode-name="rebelmouse-image" id="efbdc" loading="lazy" src="https://spectrum.ieee.org/media-library/game-avatars-stand-in-a-group-in-a-city-square-with-a-large-device-including-floating-cyan-gemstones.jpg?id=29676327&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Zenith: The Last City is a massively multiplayer VR game built with the popular game engine Unity.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ramen VR</small></p><p>Unity powered 2016’s AR hit <em>Pokémon Go </em>and <em>Beat Saber</em>, a 2018 VR game that’s still among the most popular for Oculus headsets. More recently, <a href="https://www.youtube.com/watch?v=7Hl5lS0SI8Q" rel="noopener noreferrer" target="_blank">Unity has touted the success of Ramen VR’s <em>Zenith: The Last City</em></a>, a VR game with an open multiplayer world. Zenith highlights the strength of <a href="https://assetstore.unity.com/" rel="noopener noreferrer" target="_blank">Unity’s Asset Store, which offers over 60,000 assets for download</a>. Ramen VR used store assets to build <em>Zenith</em> with a team of less than 10 developers.</p><p>It’s unlikely either engine will gain a permanent upper hand. The metaverse is one of several frontiers for game engines, which range from product prototyping to CGI and 3D animation for television and film. In short, both UE5 and Unity have room to grow—and that means metaverse development will be just one of many tasks each engine supports.</p><p>“I think both Epic Games and Unity 3D realize that the broader their engines can be applied, the more scale they will be able to drive,” said Sag. “Thinking of them as just game engines is a 5-year-old mentality.”</p>]]></description><pubDate>Wed, 13 Apr 2022 16:01:13 +0000</pubDate><guid>https://spectrum.ieee.org/unreal-engine-5-metaverse</guid><category>Video games</category><category>Metaverse</category><category>Unreal engine</category><category>Computer graphics</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-female-avatar-is-repeated-endlessly-in-rows-in-the-front-of-one-area-is-the-same-avatar-positioned-slightly-differently-next.jpg?id=29676354&amp;width=980"></media:content></item><item><title>An Introduction to EMC Amplifiers</title><link>https://engineeringresources.spectrum.ieee.org/free/w_rohd38/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-introduction-to-emc-amplifiers.gif?id=29671528&width=1200&height=800&coordinates=71%2C0%2C72%2C0"/><br/><br/><p>Broadband amplifiers are necessary for generating the field strengths required for most EMC radiated immunity tests. This whitepaper provides a brief overview of the role of amplifiers in EMC testing as well as a discussion of the parameters and characteristics which have the greatest influence on amplifier performance.</p>]]></description><pubDate>Tue, 12 Apr 2022 15:51:53 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_rohd38/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Rohde &amp; Schwarz</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/an-introduction-to-emc-amplifiers.gif?id=29671528&amp;width=980"></media:content></item></channel></rss>