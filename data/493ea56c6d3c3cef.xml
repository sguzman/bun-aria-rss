<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Statistical Modeling, Causal Inference, and Social Science</title>
	<atom:link href="https://statmodeling.stat.columbia.edu/feed/" rel="self" type="application/rss+xml" />
	<link>https://statmodeling.stat.columbia.edu</link>
	<description></description>
	<lastBuildDate>Sat, 05 Nov 2022 07:09:47 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.3</generator>
	<item>
		<title>What continues to stun me is how something can be clear and unambiguous, and it still takes years or even decades to resolve</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/05/what-continues-to-stun-me-is-how-a-problem-can-be-clear-and-unambiguous-and-it-still-takes-years-or-even-decades-to-resolve/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/05/what-continues-to-stun-me-is-how-a-problem-can-be-clear-and-unambiguous-and-it-still-takes-years-or-even-decades-to-resolve/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sat, 05 Nov 2022 13:26:40 +0000</pubDate>
				<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47806</guid>

					<description><![CDATA[OK, remember Wile E. Coyote when he makes the all-too-common mistake of stepping off a cliff, or standing on a cliff edge that breaks? He stands there in the air, unsupported by anything, until he realizes what&#8217;s happening&#8212;and only then &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/05/what-continues-to-stun-me-is-how-a-problem-can-be-clear-and-unambiguous-and-it-still-takes-years-or-even-decades-to-resolve/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>OK, remember Wile E. Coyote when he makes the all-too-common mistake of stepping off a cliff, or standing on a cliff edge that breaks?  He stands there in the air, unsupported by anything, until he realizes what&#8217;s happening&#8212;and only then does he fall.</p>
<p>We see this a lot with science scandals and political scandals:  the problem is there for ever and ever, people point it out, they holler and scream, but that damn coyote just stands there, oblivious.</p>
<p>One example we talked about a couple years ago was the scam medical-device company <a href="https://statmodeling.stat.columbia.edu/2018/08/09/bad-blood-secrets-lies-silicon-valley-startup/">Theranos</a>, which fell apart around 2015, nearly a decade <em>after</em> they faked a test in 2006, causing one of its chief executives to leave.</p>
<p>And Cornell&#8217;s Food and Brand Lab (&#8220;Pizzagate&#8221;), which fell apart around 2017, <a href="https://statmodeling.stat.columbia.edu/2018/03/03/brian-wansinks-misrepresentation-data-research-methods-known-years/">years after</a> their work had been publicly criticized several times, with the lab never offering any reasonable rebuttal to these points.</p>
<p>Let me be clear here.  It&#8217;s not a surprise to me that people whose bad research has been called out will continue to publish bad research:  there are lots of journals out there that are looking for publishable articles, so if you have the knack for writing an article in that publishable style, you can keep getting published.  And then people will read your article and take it seriously, because the default mode of reading a published article is to take it seriously, so you can get citations etc.  Similarly, it&#8217;s no surprise that companies that are in proximity to rich people or some other source of suckers can get funding even if they have a track record of lies.</p>
<p>So that&#8217;s no surprise.  What surprises me is the high-profile cases.  Theranos didn&#8217;t just pull in some money; it got a lot of publicity.  The Food and Brand Lab people were all over the major media.  In both cases, it took many years for the crash to come.</p>
<p>OK, here are a couple more examples that came up recently.  I read them in Defector, the sports website that never sticks to sports:</p>
<p><a href="https://defector.com/meet-richard-fritz-americas-most-unelectable-elected-official/">Meet Richard Fritz, America’s Most Unelectable Elected Official</a></p>
<p><a href="https://defector.com/cops-are-still-fainting-when-they-touch-fentanyl/">Cops Are Still Fainting When They Touch Fentanyl</a></p>
<p>Both of these are scandals that have been going on for years&#8212;and people have been screaming about them for years&#8212;but they just keep on going.  The perpetrators have that chutzpah which is one of the strongest forces of nature.</p>
<p>And then, the most horrible story.  I just read Nickel Boys&#8212;I&#8217;ll read anything by Colson Whitehead, as long as it&#8217;s not about <a href="https://statmodeling.stat.columbia.edu/2020/01/21/top-5-literary-descriptions-of-poker/">poker</a>&#8212;and then I went to wikipedia to read up on the <a href="https://en.wikipedia.org/wiki/Florida_School_for_Boys">true story</a> that it&#8217;s based on.  OK, here&#8217;s the deal.  The Florida School for Boys in Marianna, Florida, had all these scandals, starting shortly after it was founded in 1990 (&#8220;In 1903, an inspection reported that children at the school were commonly kept in leg irons. . . . A fire in a dormitory at the school in 1914 killed six students and two staff members. . . . A 13-year-old boy sent to the school in 1934 for &#8216;trespassing&#8217; died 38 days after arriving there. . . . there were 81 school-related deaths of students from 1911 to 1973 . . . In 1968, Florida Governor Claude Kirk said, after a visit to the school where he found overcrowding and poor conditions, that &#8216;somebody should have blown the whistle a long time ago.&#8217; . . .&#8221;)  OK, horrible things going on for nearly 70 years, all happening in a racist environment (the &#8220;School&#8221; separately housed whites and blacks), then they finally blew the whistle&#8212;1968 was way too late, but that was a time of reform.</p>
<p>But, no, it didn&#8217;t stop in 1968.  The wikipedia entry continues:  &#8220;In 1982, an inspection revealed that boys at the school were &#8220;hogtied and kept in isolation for weeks at a time&#8221;. The ACLU filed a lawsuit over this and similar mistreatment at a total of three juvenile facilities in Florida. . . . In 1985, the media reported that young ex-students of the school, sentenced to jail terms for crimes committed at Dozier, had subsequently been the victims of torture by guards at the Jackson County jail. The prison guards typically handcuffed the teenagers and hanged them from the bars of their cells, sometimes for over an hour. The guards said their superiors approved the practice and that it was routine. . . . In 1994, the school was placed under the management of the newly created Florida Department of Juvenile Justice . . . On September 16, 1998, a resident of the school lost his right arm in a washing machine. . . . In April 2007, the acting superintendent of the school and one other employee were fired following allegations of abuse of inmates.&#8221;</p>
<p>That&#8217;s 2007&#8212;over 100 years after the first scandal (the inspection in 1903 with the leg irons), nearly 30 years after the governor talked about blowing the whistle, and over 20 years after the reports of torture.  And then, &#8220;In late 2009, the school failed its annual inspection. . . .&#8221;  The &#8220;School&#8221; was finally closed in 2011.</p>
<p>Everything took so long.  It &#8220;failed its annual inspection&#8221; in 2009!  What did they think about all the annual inspections that came before?  At some point, this history should case doubts upon the inspection process itself, no?</p>
<p><strong>How does it happen?</strong></p>
<p>My point here is not to stir up indignation about a past scandal, which is part of the whole &#8220;New Jim Crow&#8221; thing that&#8217;s been doing its part to destroy our country for a long time.  This is just the most striking example I&#8217;ve come across of a general phenomenon of the truth being out there, available to all, but nothing happens.  This is not a new thing&#8212;consider, for example, the Dreyfus affair, where it was clear that the evidence was fabricated, but this didn&#8217;t stop it from being a live issue for years and years after that.  But, that was the 1800s!  We should realize that this sort of thing continues to happen today, in so many different domains, as discussed above.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/05/what-continues-to-stun-me-is-how-a-problem-can-be-clear-and-unambiguous-and-it-still-takes-years-or-even-decades-to-resolve/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>New version of Posteriordb:  a database of posterior distributions for evaluating Bayesian computation algorithms</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/04/new-version-of-posteriordb-a-database-of-posterior-distributions-for-evaluating-bayesian-computation-algorithms/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/04/new-version-of-posteriordb-a-database-of-posterior-distributions-for-evaluating-bayesian-computation-algorithms/#respond</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Fri, 04 Nov 2022 13:37:27 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Stan]]></category>
		<category><![CDATA[Statistical computing]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48155</guid>

					<description><![CDATA[Måns Magnusson writes: Now I and @avehtari are done with the PR review and finally, a new version of posteriordb is out. There is still a need for more complex posteriors > 100 parameters or models that take longer to &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/04/new-version-of-posteriordb-a-database-of-posterior-distributions-for-evaluating-bayesian-computation-algorithms/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Måns Magnusson <a href="https://discourse.mc-stan.org/t/posteriordb-v-0-4-0/29314">writes</a>:</p>
<blockquote><p>Now I and @avehtari are done with the PR review and finally, a new version of posteriordb is out. There is still a need for more complex posteriors > 100 parameters or models that take longer to estimate (minutes or hours rather than seconds). The ideal posteriors are complex posteriors that have already been published where the data is open and can be included. Feel free to reach out if you have implemented such posteriors. I’m happy to include them.</p></blockquote>
<p>The update includes new posterior distributions, bug fixes, and updated documentation.</p>
<p>I think this project is really important.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/04/new-version-of-posteriordb-a-database-of-posterior-distributions-for-evaluating-bayesian-computation-algorithms/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Important questions</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/04/important-questions/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/04/important-questions/#comments</comments>
		
		<dc:creator><![CDATA[Lizzie]]></dc:creator>
		<pubDate>Fri, 04 Nov 2022 13:01:24 +0000</pubDate>
				<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48161</guid>

					<description><![CDATA[This post is by Lizzie. I have been robbed twice in my life: first as a grad student when I was traveling and most of my belongings were in a rental car, and second when some teenagers crow-barred the apartment &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/04/important-questions/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1.jpg"><img loading="lazy" class="alignnone size-full wp-image-48163" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1.jpg" alt="" width="889" height="667" srcset="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1.jpg 889w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1-300x225.jpg 300w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1-768x576.jpg 768w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/11/P1090890sm-1-400x300.jpg 400w" sizes="(max-width: 889px) 100vw, 889px" /></a>This post is by Lizzie.</p>
<p>I have been robbed twice in my life: first as a grad student when I was traveling and most of my belongings were in a rental car, and second when some teenagers crow-barred the apartment I was staying in. The first time I was robbed I learned a some useful information that made getting robbed the second time easier: when you tell people you&#8217;ve been robbed they often say the most useless things. People who purported to be my friends would reply immediately with, &#8216;what type of car was it?&#8217; or &#8216;where exactly did you park?&#8217; While there are definitely predictors that increased the probability of me being robbed, I didn&#8217;t fully see the point of these questions.</p>
<p>Eventually my Mum or some other soul wiser than me explained this to me. They&#8217;re asking these questions to construct a model of the world in which they don&#8217;t get robbed. I sort of get this: it can be traumatic to have most of your worldly possessions stolen. I hope they didn&#8217;t also know that it can be painful to have your friends be so lame and self-centered.</p>
<p>I was trying to explain something similar the other night when a friend and colleague caught me after a group zoom call to ask me something. She had met someone who was a graduate student when I interviewed at their graduate institution for a faculty job. I always enjoyed meeting grad students on interviews; they were so much fun, asked interesting questions and made me hopeful about the world. I interviewed at many places and have run into some of these students since and always really enjoy it, so I thought this was where the conversation was going. No.</p>
<p>The student had recounted that at the end of my job talk a senior white faculty member got up and made an obnoxious and gendered comment that in no way related to my science.</p>
<p>I don&#8217;t remember this. I suspect I don&#8217;t because it was the least of my troubles back then. I remembered the guy though. In our one-on-one interview he asked me if I was planning to freeze my eggs. And in case you as a reader want to chock this up to a one-off, my notes from asking a female faculty member in the department are: &#8220;Everyone has had totally weird interactions with him, when she interviewed he told her all about his relationship with his wife. Someone else interviewing told him to **** off. And nothing slows him down, he&#8217;s gotten worse with age.&#8221;</p>
<p>What I told my colleague on the Zoom call, though, was what hurt was how people replied to it. How many people tried to brush it off in one way or another (even when I couched it in my reality &#8212; I liked this guy in many ways and people are multifaceted and complex, they are rarely all good or evil).</p>
<p>I was trying to recount this an hour later over beers but I didn&#8217;t get to finish the thought. One woman worked to come up with witty quips that I should have said back. And the one guy said, &#8216;oh, I know him. He likes to shock people. You know how you should handle this guy is &#8230;.&#8217; which led onward to &#8216;I wanted to invite him a few years back and my colleague said &#8216;I dunno&#8217; so then we invited him and someone known to be even crazier! And let me tell you about him. He got into a physical altercation with someone &#8230;.&#8217;</p>
<p>So, I just wanted to make a public service announcement to folks who want to reply in similar ways to people telling you about harassment they received &#8212; consider keeping these thoughts to yourself. And then maybe take some time to figure out why you so desperately jump at saying them.</p>
<p>In other news, look what I saw on my hike on Wednesday!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/04/important-questions/feed/</wfw:commentRss>
			<slash:comments>47</slash:comments>
		
		
			</item>
		<item>
		<title>Concreteness vs faithfulness in visual summaries</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/03/concreteness-vs-faithfulness-in-visual-summaries/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/03/concreteness-vs-faithfulness-in-visual-summaries/#comments</comments>
		
		<dc:creator><![CDATA[Jessica Hullman]]></dc:creator>
		<pubDate>Thu, 03 Nov 2022 17:39:45 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Statistical graphics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48154</guid>

					<description><![CDATA[This is Jessica. I recently had a discussion with collaborators that got me thinking about trade-offs we often encounter in summarizing data or predictions. Specifically, how do we weigh the value of deviating from a faithful or accurate representation of &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/03/concreteness-vs-faithfulness-in-visual-summaries/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><span style="font-weight: 400">This is Jessica. I recently had a discussion with collaborators that got me thinking about trade-offs we often encounter in summarizing data or predictions. Specifically, how do we weigh the value of deviating from a faithful or accurate representation of how some data was produced in order to it more interpretable to people? This often comes up as sort of an implicit concern in visualization, when we decide things like whether we should represent probability as frequency to make it more concrete or usable for some inference task. It comes up more explicitly in some other areas like AI/ML interpretability, where people debate the validity of using <a href="https://arxiv.org/abs/1606.03490">post-hoc interpretability</a> methods. </span><span style="font-weight: 400">Thinking about it through a visualization example more has made me realize that at least in visualization research, we still don’t really have principled foundation for resolving these questions.</span></p>
<p><span style="font-weight: 400">My collaborators and I were talking about designing a display for an analysis workflow involving model predictions. We needed to visualize some distributions, so I proposed using a discrete representation of distribution based on how they have been found to</span><span style="font-weight: 400"> lead to more accurate probability judgments and decisions among non-experts in <a href="https://mucollective.northwestern.edu/project/uncertainty-bus">multiple</a> <a href="https://mucollective.northwestern.edu/project/2020-vis-effect-size-judgements">experiments</a>. By “discrete representations” here I mean things like discretizing a probability density function by taking some predetermined number of draws proportional to the inverse cumulative distribution function and showing it in a static plot (<a href="https://vega.github.io/vega/examples/quantile-dot-plot/">quantile dotplot</a>), or animating draws from the distribution we want to show over time (<a href="https://vega.github.io/vega/examples/hypothetical-outcome-plots/">hypothetical outcome plots</a>), or possibly some hybrid of static and animated. However, one of my collaborators questioned whether it really makes sense to use, for example, a </span><a href="https://fivethirtyeight.com/features/why-republicans-are-favored-to-win-the-house-but-not-the-senate/"><span style="font-weight: 400">ball swarm style chart</span></a><span style="font-weight: 400"> if you aren’t using a sampling based approach to quantify uncertainty. </span></p>
<p><span style="font-weight: 400">This made me realize how common it is in visualization research to try to separate the visual encoding aspect from the rest of the workflow. We tend to see the question of how to visualize distribution as mostly independent from how to generate the distribution. So even if we used some analytical method to infer a sampling distribution, the conclusions of visualization research as typically presented would suggest that we should still prefer to visualize it as a set of outcomes sampled from the distribution. We rarely discuss how much the effectiveness of some technique might vary when the underlying uncertainty quantification process is different. </span></p>
<p><span style="font-weight: 400">On some level this seems like an obvious blind spot, to separate the visual representation from the underlying process. But I can think of a few reasons why researchers might default to trying to separate encodings from generating processes and not necessarily question doing this. For one, having worked in visualization for years, at least in the case of uncertainty visualization I&#8217;ve seen various instances where users of charts seem to be more sensitive to changes to visual cues than they are to changes to descriptions of how some uncertainty quantification was arrived at. This implies that aiming for perfect faithfulness in our descriptions is not necessarily where we want to spend our effort. E.g, change an axis scaling and the </span><a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376454"><span style="font-weight: 400">effect size judgments you get in response will be different</span></a><span style="font-weight: 400">, but modifying the way you describe the uncertainty quantification process alone probably won’t result in much of a change to judgments without some addtional change in representation. So the focus naturally goes to trying to “hack” the visual side to get the more accurate or better calibrated responses.</span></p>
<p><span style="font-weight: 400">I could also see this way of thinking becoming ingrained in part because people who care about interfaces have always had to convince others of the value of what they do through evidence that the reprersentation alone matters. Showing the dependence of good decisions on visualization alone is perceived as sort of a fundamental way to argue that visualization should be taken seriously as a distinct area.</span></p>
<p><span style="font-weight: 400">At the same time though, disconnecting visual from process could be criticized for suggesting a certain sloppiness in how we view the function of visualization. Not minding the specific ways that we break the tie between the representation and the process might imply we don&#8217;t have a good understanding of the constraints on what we are trying to achieve. Treating the data generating process as a black box is certainly much easier than trying to align the representations to it, so it’s not necessarily surprising that the research community seems to have settled with the former.</span></p>
<p><span style="font-weight: 400">Under this view, it becomes research-worthy to point out issues that only really arise because we default to thinking that representation and generation are separate. For example, there’s a well known psych study suggesting we </span><a href="https://link.springer.com/content/pdf/10.3758/bf03201236.pdf"><span style="font-weight: 400">don’t want to visualize continuous data with bar charts</span></a><span style="font-weight: 400"> because people will think they are seeing discrete groups (and vice versa). It’s kind of weird that we can have these one-off results be taken very seriously, but then not worry so much about mismatch in other contexts, like acknowledging that making some assumptions to compute a confidence interval and then sampling some hypothetical outcomes from that is different from using sampling directly to infer a distribution. </span></p>
<p><span style="font-weight: 400">I suspect for this particular uncertainty visualization example, the consequences of the visual metaphor not faithfully capturing underlying distribution generation process seem minor relative to the potential benefits of getting people thinking more concretely about the implications of error in the estimate. There’s also a notion of frequency that’s also inherent in the conventional construction of confidence intervals which maybe makes a frequency representation seem less egregiously wrong. Still, there’s the potential for the discrete representation to be read as mechanistic, i.e., as signifying a bootstrap construction process even where it actually doesn’t that my collaborator seemed to be getting at.</span></p>
<p><span style="font-weight: 400">But on the other hand, any data visualization is a concretization of something nebulous, i.e., an abstraction encoded in the visual-spatial realm used to represent our knowledge of some real world thing approximated by a measurement process. So one could also point out that it doesn’t really make sense to act as though there are going to be situations where we are free from representational “distortion.”</span></p>
<p><span style="font-weight: 400">Anyway, I do think there’s a valid criticism to be made through this example of how research hasn’t really attempted to address these trade-offs directly. Despite all of the time we spend emphasizing the importance of the right representation in interactive visualization, I expect most of us would be hard pressed to explain the value of a more concrete representation over a more accurate one for a certain problem without falling back on intuitions. Should we be able to get precise about this, or even quantify it? I like the idea of trying, but in an applied field like infovis would expect the majority to judge it to be not worth the effort (if only because theory over intuition is a tough argument to make when funding exists without it).  </span></p>
<p><span style="font-weight: 400">Like I said above, a similar trade-off seems to come up in areas like AI/ML interpretatibility and explainability, but I&#8217;m not sure if there are attempts yet to theorize it. It could maybe be described as the value of human model alignment, meaning the value of matching the representation of some information to metaphors or priors or levels of resolution that people find easier to mentally compute with, versus generating model alignment, where we constrain the representation to be mechanistically accurate. It would be cool to see examples attempting to quantify this trade-off or otherwise formalize it in a way that could provide design principles.</span></p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/03/concreteness-vs-faithfulness-in-visual-summaries/feed/</wfw:commentRss>
			<slash:comments>6</slash:comments>
		
		
			</item>
		<item>
		<title>&#8220;Banned from presenting research to the organization for the next two years&#8221; . . . how does that happen?</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/03/banned-from-presenting-research-to-the-organization-for-the-next-two-years-how-does-that-happen/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/03/banned-from-presenting-research-to-the-organization-for-the-next-two-years-how-does-that-happen/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 03 Nov 2022 13:07:50 +0000</pubDate>
				<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Public Health]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48152</guid>

					<description><![CDATA[Paul Alper points us to a news article that reports: Oz research rejected from 2003 surgery conference, resulted in 2-year ban In May 2003, Mehmet Oz was the senior author on a study that explored a hot topic at the &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/03/banned-from-presenting-research-to-the-organization-for-the-next-two-years-how-does-that-happen/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Paul Alper points us to <a href="https://www.washingtonpost.com/elections/2022/11/01/oz-senate-doctor-research">a news article</a> that reports:</p>
<blockquote><p>Oz research rejected from 2003 surgery conference, resulted in 2-year ban</p>
<p>In May 2003, Mehmet Oz was the senior author on a study that explored a hot topic at the time . . . But Oz was forced to withdraw his work and was banned from presenting research to the organization for the next two years, according to seven people familiar with the events, whose account of his ban was confirmed by the Oz campaign. Oz is now the Republican nominee for U.S. Senate in Pennsylvania.</p>
<p>He was also prohibited from publishing his work in the society’s medical journal for the same period of time, according to the people familiar with the events, four of whom recalled details of the controversy on the record. </p>
<p>“My understanding is it was the lack of really solid statistical analysis that called everything into question,” said one person who was sympathetic to Oz, saying he simply ran into funding and deadline problems.</p></blockquote>
<p>My reply:  Sure, why would anyone think that just cos someone&#8217;s a good surgeon, that they&#8217;d have any idea how to conduct research?  I know lots of people who are really good at research but it&#8217;s not like they could do surgery.</p>
<p>The interesting thing to me is that last paragraph of the above quote, as it seems to reflect the attitude that statistics is just a bunch of paperwork, or a set of hoops to jump through.  That attitude that researchers have, that their ideas are evidently correct and the statistics is just some rubber stamp.</p>
<p>With Oz, though, there&#8217;s another twist, given his later record of promoting miracle cures.  It&#8217;s hard to imagine he really believes in all the things that he promotes&#8212;or maybe &#8220;belief&#8221; isn&#8217;t really the point; perhaps he just has a very general belief in the power of placebo, in which case anything he recommends would be automatically legit.  Indeed, the more people pay for it, the more effective the placebo power is.  Kinda like academic funding!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/03/banned-from-presenting-research-to-the-organization-for-the-next-two-years-how-does-that-happen/feed/</wfw:commentRss>
			<slash:comments>17</slash:comments>
		
		
			</item>
		<item>
		<title>Cherry-picking during pumpkin-picking season? (the effects of the Jan. 6th hearings)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/02/cherry-picking-during-pumpkin-picking-season-the-effects-of-the-jan-6th-hearings/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/02/cherry-picking-during-pumpkin-picking-season-the-effects-of-the-jan-6th-hearings/#comments</comments>
		
		<dc:creator><![CDATA[Jeff Lax]]></dc:creator>
		<pubDate>Wed, 02 Nov 2022 19:02:09 +0000</pubDate>
				<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Political Science]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48145</guid>

					<description><![CDATA[This post is by Jeff Lax. Is something off in the recent Talking Points Memo by Mindy Finn/Citizen Data, on the effects of the Jan. 6th hearings?  It’s about the change in people’s attitudes in a pair of surveys before &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/02/cherry-picking-during-pumpkin-picking-season-the-effects-of-the-jan-6th-hearings/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>This post is by Jeff Lax.</em></p>
<p><span style="font-weight: 400">Is something off in the recent Talking Points Memo by Mindy Finn/Citizen Data, on the effects of the Jan. 6th hearings?  It’s about the change in people’s attitudes in a pair of surveys before and during the hearings. It is framed around the basic health of our democracy–what could be more serious? I was going to use it to respond to a student question… but then I looked closer.</span></p>
<p><a href="https://talkingpointsmemo.com/cafe/the-jan-6-committee-is-having-a-measurable-impact-on-voter-attitudes"><span style="font-weight: 400">https://talkingpointsmemo.com/cafe/the-jan-6-committee-is-having-a-measurable-impact-on-voter-attitudes</span></a></p>
<p><span style="font-weight: 400">They basically claim only good news for democracy and with great confidence: “While there have been conflicting reports on the impact of the Jan. 6 hearings, our polling has been more conclusive. Since the hearings began, more Americans have come to view Jan. 6 as a violent attempt to overthrow the government and more Americans now see the committee’s findings as legitimate. As we look ahead to the midterms and on to 2024, I believe the committee’s communications offer a playbook to replicate on the journey to protect our democracy and thwart those who threaten it.”  They also claim that, after the Jan. 6 Committee’s hearings, to nearly quote, Americans are more likely to hold Trump accountable, that those supporting him will face headwinds, and that efforts to rebuild democracy are paying off. </span></p>
<p><span style="font-weight: 400">My concerns are NOT about tricky stats or surveying issues. Rather, I think they misread their own results and neglect to note contrary evidence from their own numbers. Why is the piece doing so much cherry-picking in reporting results? Why is there so much cheerleading given the number of findings that are bad or mixed? Why isn’t it at least clear on which numbers are being compared? One worries the findings are just noise and the reporting too selective to be considered properly objective.</span></p>
<p><span style="font-weight: 400">Start with the claims that comparisons between April and July show good news: “Skeptical Americans,” Finn writes, “including those who initially believed the 2020 election was tainted through widespread voter fraud, might be changing their minds” and “the share who believed Joe Biden won the 2020 election increased by 5%” and “Nearly twice the number of Americans who view the 2020 election as ‘stolen’ and Jan. 6th as peaceful now view the events as a violent attempt to overthrow the government.” You can get to some of the underlying numbers through a link in the original post: </span><a href="https://citizendata.docsend.com/view/umfmknfvebgf5uru"><span style="font-weight: 400">https://citizendata.docsend.com/view/umfmknfvebgf5uru</span></a></p>
<p><span style="font-weight: 400">Their analysis of the ‘Did Biden win” question seems right in isolation. Comparing July to April, more people said Biden won, fewer had doubts about this, and fewer said he did not win. But the results from that question are undercut by the numbers in the main question on Jan. 6th.</span></p>
<p><span style="font-weight: 400">This is obscured by the non-standard (or at least overly complicated) set of options respondents are given in the key question. To answer it, you have to implicitly make three decisions with two options each, picking one of a permitted six combinations for labeling Jan. 6th: (1) “Violent attempt to overthrow the government in response to a legitimate election,” (2) “Peaceful protest against a stolen election,” (3) “Violent protest against a legitimate election,” (4) “Violent attempt to overthrow the government in response to a stolen election,” (5) “Violent protest against a stolen election,” or (6) “Peaceful protest against a legitimate election.”  (For better or worse, they don’t allow all eight possibilities, so you can’t say “peaceful attempt to overthrow the government in response to a legitimate election” or same “&#8230; against a stolen election.”)</span></p>
<p><span style="font-weight: 400">So if you think Jan. 6th was “violent,” you narrow it down to options 1, 3, 4, or 5. But then do you think it is “attempt to overthrow” (1 or 4)? Or “protest” (3 or 5)? Then you still have to choose whether the election was “legitimate” (1 or 3) or “stolen” (4 or 5). If you start with “peaceful,” you then have to contend with two options, “stolen” (2) or “legitimate” (4), distributed among the six options in total. Or you can start with “legitimate election” (1, 3, or 6) or “stolen” (2, 4, and 5). Then you have to do “violent attempt” or “violent protest” or “peaceful protest”. So messy. So hard on the survey respondent. Too hard? Perhaps even too hard to get the write-up right?</span></p>
<p><span style="font-weight: 400">Here is what the results look like, tabulated from the “behind-the-scenes” graph they link to (with less information in the article itself):</span></p>
<table dir="ltr" border="1" cellspacing="0" cellpadding="0">
<colgroup>
<col width="204" />
<col width="100" />
<col width="100" />
<col width="100" />
<col width="100" /></colgroup>
<tbody>
<tr>
<td></td>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;category&quot;}">category</td>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;april&quot;}">april</td>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;july&quot;}">july</td>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;change&quot;}">change</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;violent attempt to overthrow the government in response to a legitimate election&quot;}">violent attempt to overthrow the government in response to a legitimate election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:1}">1</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:34.1}">34.1</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:33.9}">33.9</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:-0.20000000000000284}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">-0.2</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;peaceful protest against a stolen election&quot;}">peaceful protest against a stolen election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:2}">2</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:16.1}">16.1</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:14.6}">14.6</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:-1.5000000000000018}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">-1.5</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;violent protest against a legitimate election&quot;}">violent protest against a legitimate election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:3}">3</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:12.4}">12.4</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:13.7}">13.7</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:1.299999999999999}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">1.3</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;violent attempt to overthrow the government in response to a stolen election&quot;}">violent attempt to overthrow the government in response to a stolen election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:4}">4</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:6.5}">6.5</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:11}">11</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:4.5}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">4.5</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;violent protest against a stolen election&quot;}">violent protest against a stolen election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:5}">5</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:9.6}">9.6</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:9.6}">9.6</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:0}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">0</td>
</tr>
<tr>
<td data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;peaceful protest against a legitimate election&quot;}">peaceful protest against a legitimate election</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:6}">6</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:3.7}">3.7</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:6.1}">6.1</td>
<td data-sheets-value="{&quot;1&quot;:3,&quot;3&quot;:2.3999999999999995}" data-sheets-formula="=R[0]C[-1]-R[0]C[-2]">2.4</td>
</tr>
</tbody>
</table>
<p><span style="font-weight: 400">Do we see only good news? If you think (1) (‘violent-overthrow-legitimate’) is correct, then it would be good news if that percentage went up. It didn’t (-.2% April to July). True, fewer (-1.5% April to July) falsely think it a (2) ‘peaceful-protest-stolen’ (that’s good), but more (+2.4%) think it only a (6) ‘peaceful-protest-legitimate’ (that’s bad, or mixed). Those who said (3) ‘violent-protest-legitimate’ went up (+1.3%, that’s bad, or mixed). There was no change in (5) at all (+0.0%). Those who chose 6, just ‘peaceful-protest-legitimate’, went up by (+2.4%, that’s bad, or mixed).</span></p>
<p><span style="font-weight: 400">Where did the main claim come from, that nearly twice the number who said ‘peaceful-stolen’ in April say ‘violent-overthrow-legitimate’ in July? Maybe the write-up was meant to rephrase this text in the linked report: “At the conclusion of the hearings in July, the percentage of </span><span style="font-weight: 400">Americans who viewed January 6th as a &#8220;violent attempt to overthrow the government in response to a stolen election&#8221; nearly doubled from 6.5% to 11%; most of this increase came from those who had previously indicated they were uncertain about the events of January 6th.” That’s category 4’s pair of numbers. But that’s not the same as the main text’s “Nearly twice the number of Americans who view the 2020 election as ‘stolen’ and Jan. 6 as peaceful now view the events as a violent attempt to overthrow the government.”  (Also, any cross-tabs showing where “most of this increase came from” are not shown.)  </span></p>
<p><span style="font-weight: 400">I suppose they could be comparing category 1 in July at 33.9% (violent-overthrow-legitimate) to category 2 in April at 16.1% (peaceful-protest-stolen) but the former is MORE than twice the latter. And I don’t think that comparison is meaningful really.  Heck, one could also say more than twice the number who said that January 6 was ‘violent-overthrow-stolen’ (6.5%, category 4, April) later say it was a ‘peaceful protest against a stolen election’ (14.6%, category 2, July). How dramatically bad does that sound! Moreover, one could have said using only April to April comparisons that over twice as many said good category 1 as bad category 2.  Again, I’m not sure which results are being discussed in the main finding or why it makes sense to arbitrarily compare categories and time periods with so many comparisons that could be done.</span></p>
<p><span style="font-weight: 400">What can we safely say?  Those who admit Jan. 6th was “violent” did go from 62.6 to 68.2 (summing categories, an increase of 5.6%). So that’s good news. Yet a quarter of that overall increase is the increase in the people who said it was only a ‘violent protest’ of a ‘legitimate election’ (there was no change in those who said ‘violent protest of stolen election’). Minimizing it as ‘protest’ (compared to ‘attempt to overthrow’) doesn’t seem so great. And there was an increase of .9% in those who said it was a ‘peaceful protest’ of any type (that’s bad). The percent who thought the’ election legitimate’ did increase by 3.5% (that’s good), but those who said ‘stolen’ went up nearly as much, 3% (that’s bad).  Movement seems to be coming from “don’t know” (down 6.5% from April to July), as much as from converting those with false views.</span></p>
<p><span style="font-weight: 400">Turning to the question on the legitimacy of the committee on the 6th, more say ‘legitimate’ and their recommendations should be ‘seriously considered’ (good!), but more say ‘not legitimate’ and should be ‘ignored’ (bad!). More say legitimate but should be ‘not seriously considered.’ That’s… bad? Good? No idea. And how is ‘not seriously considered’ different from ‘ignored’? I give up.</span></p>
<p>&nbsp;</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/02/cherry-picking-during-pumpkin-picking-season-the-effects-of-the-jan-6th-hearings/feed/</wfw:commentRss>
			<slash:comments>7</slash:comments>
		
		
			</item>
		<item>
		<title>&#8220;Rankings offer no sense of scale&#8221;</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/02/rankings-offer-no-sense-of-scale/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/02/rankings-offer-no-sense-of-scale/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Wed, 02 Nov 2022 13:03:02 +0000</pubDate>
				<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47963</guid>

					<description><![CDATA[Laura &#8220;Namerology&#8221; Wattenberg writes: Rankings offer no sense of scale. There is always a #1 name, even if that name is only a fraction as popular as #1s of past eras. That’s a problem when scale itself—the changing level of &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/02/rankings-offer-no-sense-of-scale/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Laura &#8220;Namerology&#8221; Wattenberg <a href="https://namerology.com/2022/08/24/the-top-of-the-charts-illusion-how-popularity-rankings-lie-about-baby-names/">writes</a>:</p>
<blockquote><p>Rankings offer no sense of scale. There is always a #1 name, even if that name is only a fraction as popular as #1s of past eras. That’s a problem when scale itself—the changing level of consensus—is part of the story.</p>
<p>In the past, popularity charts served as a solid snapshot of everyday name style, especially for boys. America’s top 10 names accounted for about a third of all boys born. But the popularity curve has flattened. Today’s top 10 accounts just one-fourteenth of American boys.</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/08/image.png" alt="" width="350" /></p></blockquote>
<p>Here&#8217;s how we put it in <a href="http://www.stat.columbia.edu/~gelman/regression/">Regression and Other Stories</a>:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/08/Screen-Shot-2022-08-31-at-1.04.25-PM.png" alt="" width="550" /></p>
<p>Wattenberg summarizes:</p>
<blockquote><p>
America’s top-10 list of popular baby names always looks comforting. Or, depending on your perspective, boring. The names change slowly, and their style remains mostly traditional. The overall impression is of gradual evolution and cultural continuity. And it’s a lie.</p>
<p>Change in 21st-century naming has been anything but gradual. It’s been revolutionary, a splintering of style that breaks with the past in dramatic ways. But the top of the popularity charts cannot tell this story. A top-10 list is a perfect instrument to create the illusion of stability in the midst of a cyclone. . . .</p>
<p>By consistently reporting name data in scaleless rankings, news reports obscure the fact that the top 10 is losing relevance as a portrait of how we name children.</p>
<p>Last year, an incredible 31,538 different names registered in U.S. baby name stats. Yet 8% of babies received a name that was too rare to even register in the count—more than received a top-10 name. That’s the real story of American names today. But as long as we continue to focus on the top of the popularity charts, we’ll continue to see only the most traditional sliver of our increasingly freewheeling name culture.</p></blockquote>
<p>Good point, as sociology, statistics, and statistical communication.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/02/rankings-offer-no-sense-of-scale/feed/</wfw:commentRss>
			<slash:comments>8</slash:comments>
		
		
			</item>
		<item>
		<title>Hiring at Flatiron Institute (NYC): research scientists, postdoctoral fellows, interns, and software engineers at Flatiron Institute</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/01/flatiron-institute-hiring-postdocs-research-scientists-interns/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/01/flatiron-institute-hiring-postdocs-research-scientists-interns/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Tue, 01 Nov 2022 19:00:46 +0000</pubDate>
				<category><![CDATA[Jobs]]></category>
		<category><![CDATA[Statistical computing]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48148</guid>

					<description><![CDATA[Edit: Before going into detail, I would like to make it clear up front that I&#8217;m specifically looking for people with experience and a publication record in algorithms, open-source software, and directly relevant theory and methodology. Flatiron Institute as a &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/01/flatiron-institute-hiring-postdocs-research-scientists-interns/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<blockquote><p>
Edit: Before going into detail, I would like to make it clear up front that I&#8217;m specifically looking for people with experience and a publication record in algorithms, open-source software, and directly relevant theory and methodology.  Flatiron Institute as a whole is focused on the physical and biological sciences (not education, social science, finance, etc.).</p>
<p>Edit #2:  I don&#8217;t know anything yet about the new <a href="https://www1.nyc.gov/assets/cchr/downloads/pdf/publications/Salary-Transparency-Factsheet.pdf">Salary Transparency in Job Advertisements</a> law and what that will mean for our job ads.
</p></blockquote>
<p>Hiring season is officially open!  I&#8217;m now officially the group leader for computational statistics and am looking to hire at all levels.  </p>
<p><b>Jobs</b></p>
<ul>
<li><b>Postdoctoral Fellows</b>: 2 year appointments with a 1-year extension baked in. <br />&nbsp;
<li><b>Open-rank Research Scientists</b>: permanent appointments at all levels; we typically do not hire straight out of graduate school for these permanent positions. <br />&nbsp;
<li><b>Software engineers</b>: permanent software engineering positions. <br />&nbsp;
<li><b>Summer interns</b>: we bring in a large number of interns across Flatiron Institute and Simons Foundation, who are mentored by research scientists, postdocs, or software engineers.
</ul>
<p><b>Official job ads</b></p>
<p>Here&#8217;s the link to the official Flatiron Institute jobs board, where you will find the postdoctoral fellow and open-rank job ads:</p>
<ul>
<li><a href="https://www.simonsfoundation.org/careers/?tab=job-openings">Job openings at Flatiron Institute</a>
</ul>
<p><b>Application dates</b></p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/09/flatiron-institute-lunchroom.jpg" alt="Flatiron Institute cafeteria" style="width:350px; float:right; margin:0 0 1em 1em" />Postdoctoral Fellow applications are <b>due 15 December 2022</b> for positions with flexible starting dates in 2023. We accept open-rank applications for permanent research scientist positions at any time, though we will also be hiring in the usual academic cycle (i.e., early 2023 interviews and job offers).  We do not have an ad for software engineers up, but will create one if we find relevant candidates&#8212;we are always looking to hire permanent software engineers.  </p>
<p>The official announcement for interns is not yet up on our jobs site, but I will post a separate blog entry when it&#8217;s up.    </p>
<p><b>Please contact me directly</b></p>
<p>If you&#8217;re interested in one of these positions, please send me an email directly to discuss (<tt><a href="mailto:bcarpenter@flatironinstitute.org">bcarpenter@flatironinstitute.org</a></tt>).</p>
<p><b>Center for Computational Mathematics Mission</b></p>
<p>This is the only place I&#8217;ve ever worked where the mission statement lines up so well with my own interests:</p>
<blockquote><p>
CCM’s mission is to create new mathematical approaches, algorithms and software to advance scientific research in multiple disciplines, often in collaboration with other Flatiron Centers.
</p></blockquote>
<p>The other centers have similar missions, because the institute was established to fill the gap in scientific funding for algorithms and software.  If the mission appeals to you, you will probably like Flatiron Institute.  More specifically, members of our center work on methodology, applications, and theory for computational stats and machine learning, as well as more traditional scientific computing in the form of optimization, differential equation solvers, non-uniform FFTs, phase retrieval, optimization etc.  </p>
<p>We are all principal investigators in the sense that there are no top-down goals from the administration other than to work toward our mission. Our president, <a href="https://scholar.google.com/citations?user=xoOGOFEAAAAJ&#038;hl=en">David Spergel</a>, is an astrophysicist, which gives the whole place a more traditional academic feel than the current MBA-led situation in most universities.</p>
<p><b>Computational Statistics Group</b></p>
<p>Our group so far consists of </p>
<ul>
<li> <a href="https://scholar.google.com.au/citations?user=kPtKWAwAAAAJ&#038;hl=en">Bob Carpenter</a> (Senior Research Scientist), working on computational statistics algorithms, Stan, and applied statistics (crowdsourcing, coherent diffraction imaging, genomics, etc.)
<li> <a href="https://scholar.google.com/citations?user=okKw87MAAAAJ&#038;hl=en">Robert Gower</a> (Research Scientist), who&#8217;s a specialist in optimization and working with us and others on both variational inference and with me on optimization for large scale transcriptomics and biome modeling.
<li> <a href="https://www.simonsfoundation.org/people/brian-ward/">Brian Ward</a> (Software Engineer), who&#8217;s been working on the Stan language, interfaces, and applications like CDI.
<li> <a href="https://scholar.google.com/citations?user=T7P7bwYAAAAJ&#038;hl=en">Yuling Yao</a> (Postdoctoral Fellow), who&#8217;s working on Bayesian methodology, Bayesian inference and beyond, including some really exciting ML-motivated variational inference algorithms.
<li> <a href="https://scholar.google.com/citations?user=nPtLsvIAAAAJ&#038;hl=en">Charles Margossian</a> (Postdoctoral Fellow), who&#8217;s working on massively parallel inference and convergence monitoring, adjoint methods for autodial of implicit functions (like root finders and differential equation solvers), as well as nested Laplace approximation.
</ul>
<p><b>Machine Learning Group</b></p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/09/flatiron-3.jpg" alt="Flatiron Institute roof garden" style="width:300px; float:left; margin:1em 1em 1em 0" />The complementary group to ours is machine learning, led by <a href="https://scholar.google.com/citations?user=Hft_8cgAAAAJ&#038;hl=en">Lawrence Saul</a>, and we&#8217;re hiring in machine learning as well as computational statistics (the line is admittedly blurry and purely administrative). <a href="https://scholar.google.com/citations?user=8OYE6iEAAAAJ&#038;hl=en">Dave Blei</a> is also visiting one day a week after being here for a year on sabbatical.  </p>
<p>In addition to Dave, we have a steady stream of visitors.  In our group, <a href="https://people.cs.umass.edu/~domke/">Justin Domke</a> is visiting for 5 months in early 2023, and <a href="https://www.csuchico.edu/math/people/faculty/roualdes-edward.shtml">Edward Roulades</a> (a Stan developer and applied mathematician/statistician), will be back to work on massively parallel inference next summer.  Robert Gower also has a steady stream of optimization specialists visiting.</p>
<p>We also have a very active internship program and both postdocs and research scientists can recruit interns who we bring to NY and put up for the summer.</p>
<p><b>Focus on algorithms and computation</b></p>
<p>Although most of our permanent staff researchers and software engineers work on open-source software, we realize postdocs need to get a job, so we give them the flexibility to concentrate on research.  We also understand that research is an international collaborative effort and we encourage postdocs and research scientists to maintain and develop academic collaborations outside of Flatiron Institute.</p>
<p><b>A highly collaborative environment</b></p>
<p>My favorite part of this place is that it&#8217;s packed to the rafters with scientists working on deep and interesting problems across the biological and physical sciences (our other centers are for computational astronomy, computational biology, computational neuroscience, and computational quantum physics).  </p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/09/christian-mueller-flatiron.jpg" alt="christian mueller at blackboard" style="width:300px; float:right; margin:1em 0 1em 1em" />I&#8217;m afraid we don&#8217;t do social science or humanities computing and are unlikely to hire anyone who&#8217;s concentrated mainly in these areas.  The centers run popular science talks for each other on a weekly basis, which are really wonderful because they&#8217;re pitched at mathematically- and computationally-oriented scientists (so they assume you know basic physics and differential equations).  We also do in-house training in computational statistics, machine learning, and applied mathematics&#8212;I often say it&#8217;s a great place to work for people who like math class (from both the teacher and student side).  It&#8217;s been a wonderful place to learn new subjects and techniques.</p>
<p><b>Compute and support</b></p>
<p>Another great benefit of working here is our large-scale, state of the art compute clusters and the Scientific Computing Core staff who can help you get your C++, Fortran, or even Python code running efficiently on the cluster.  </p>
<p>Like continental Europe and the old Bell Labs, we have a very pleasant lunch culture where we eat together almost every day (it&#8217;s not required but very pleasant;  one of the pictures above is our cafeteria and another is the roof deck, where we have receptions after events and usually eat during the summer.</p>
<p>We also have truly fantastic admin support at all levels.  I don&#8217;t think I could&#8217;ve imagined admin support this good and it makes a huge difference.  Even our security guards are friendly and helpful.  </p>
<p>We&#8217;re one of the few places that has sufficient meeting rooms for our staff size (as well as great professional AV support).  We&#8217;re also one of those places with chalkboards and white boards in all the halls, and it feels very welcoming to be surrounded by mathematical scribbling (that&#8217;s my office you can see behind Christian Mueller writing on the blackboard below).</p>
<p><b>Simons Foundation</b></p>
<p>Flatiron Institute is part of the Simons Foundation, which is one of the largest non-governmental science funders.  Jim and Marilyn Simons funded the foundation with a large enough endowment to exist in perpetuity.  <img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2021/09/flatiron-4.jpg" alt="Flatiron Institute auditorium" style="width:300px; float:left; margin:1em 1em 0.5em 0" />Simons Foundation also runs all kinds of popular outreach programs around math and science education and engagement, including <i>Quanta Magazine</i> and Sandbox Films (science documentaries), Math for America, an oceanographic data collection arm, observatories, and a very large autism research insitiative dedicated to open-access data collection and curation.  We even fund arXiv.</p>
<p>We are fully funded through the Simons Foundations and cannot apply for grants, though we are free to collaborate with whomever we want.  </p>
<p><b>Offices and in-person work</b></p>
<p>We are not in the Flatiron Building, but have very nice digs in the Flatiron neighborhood of NYC (as you can see above), though hardly anyone has an individual office.  Flatiron Institute and Simons Foundations have offices in two buildings at  Fifth Avenue and 21st St, only a few blocks from Greenwich Village (NYU and subsidized postdoc housing) and Chelsea (Google and the Hudson River).</p>
<p>We&#8217;re 100% back to the office and do not hire people for remote positions.  </p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/01/flatiron-institute-hiring-postdocs-research-scientists-interns/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Another book about poker</title>
		<link>https://statmodeling.stat.columbia.edu/2022/11/01/another-book-about-poker/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/11/01/another-book-about-poker/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Tue, 01 Nov 2022 13:39:09 +0000</pubDate>
				<category><![CDATA[Literature]]></category>
		<category><![CDATA[Sports]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47727</guid>

					<description><![CDATA[I just finished Last Call, a science fiction novel by Tim Powers, that I&#8217;m mentioning here to add to our list of literary descriptions of poker. Last Call is pretty good: it&#8217;s full of action and it reads like a &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/11/01/another-book-about-poker/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I just finished Last Call, a science fiction novel by Tim Powers, that I&#8217;m mentioning here to add to our <a href="https://statmodeling.stat.columbia.edu/2020/01/21/top-5-literary-descriptions-of-poker/">list of literary descriptions of poker</a>.  Last Call is pretty good:  it&#8217;s full of action and it reads like a cross between Stephen King, Roger Zelazny, and George Pelecanos.  I thought the ending was weak, but, hey, <a href="https://statmodeling.stat.columbia.edu/2017/07/28/improved-ending-martian/">nobody&#8217;s</a> perfect.</p>
<p>The poker scenes in Last Call were carried out well.  The only problem I had was in some of the exposition near the beginning, where it seemed that the author was regurgitating a bunch of Frank Wallace&#8217;s classic, &#8220;Poker: A guaranteed income for life by using the advanced concepts of poker,&#8221; even to the extent of repeating the anecdote about the sandwich.  Wallace&#8217;s book remains very readable, and I have no problem using it as background, but it&#8217;s gotta be processed first so it doesn&#8217;t look like raw research.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/11/01/another-book-about-poker/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Here&#8217;s a fun intro lesson on how to read a graph!</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/31/heres-a-fun-intro-lesson-on-how-to-read-a-graph/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/31/heres-a-fun-intro-lesson-on-how-to-read-a-graph/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Mon, 31 Oct 2022 13:11:37 +0000</pubDate>
				<category><![CDATA[Statistical graphics]]></category>
		<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47210</guid>

					<description><![CDATA[Paul Alper sent us this fun feature from the New York Times that teaches students how to read a graph. They start with the above scatterplot and then ask a series of questions: What do you notice? What do you &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/31/heres-a-fun-intro-lesson-on-how-to-read-a-graph/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/02/GenerationSurveyGraphLN-jumbo.jpg" alt="" width="450" /></p>
<p>Paul Alper sent us <a href="https://www.nytimes.com/2022/02/03/learning/whats-going-on-in-this-graph-feb-9-2022.html">this fun feature</a> from the New York Times that teaches students how to read a graph.</p>
<p>They start with the above scatterplot and then ask a series of questions:</p>
<blockquote><p>What do you notice?</p>
<p>What do you wonder?</p>
<p>How does this relate to you and your community?</p>
<p>What’s going on in this graph? Create a catchy headline that captures the graph’s main idea.</p>
<p>The questions are intended to build on one another, so try to answer them in order.</p></blockquote>
<p>Then they follow up with some details:</p>
<blockquote><p>This graph appeared in the Nov. 17, 2021 New York Times Upshot article “<a href="https://www.nytimes.com/2021/11/17/upshot/global-survey-optimism.html">Where Are Young People Most Optimistic? In Poorer Nations</a>.” It displays statistics from an international survey of more than 21,000 people from 21 countries conducted by Gallup for UNICEF. A report entitled “<a href="https://www.unicef.org/globalinsight/media/2266/file">The Changing Childhood Project: A Multigenerational, International Survey of 21st Century Childhood</a>” offers all of the survey’s findings with its 32-question survey and its methodology by country. The survey sample was nationally representative and was conducted by landline and mobile telephone from February to June 2021. The survey’s objective was to find out how childhood is changing in the 21st century, and where divisions are emerging between generations.</p>
<p>Are the 15- to 24 year olds (youth) more optimistic than 40+ year olds (parents)? Does the difference in optimism vary between the least wealthy and most wealthy countries? How might the degree of political stability, economic opportunity, climate change and the Covid pandemic affect the youths’ and parents’ responses? Which countries’ statistics surprise you? What do you think about the statistics for the United States?</p></blockquote>
<p>And:</p>
<blockquote><p>Here are some of the student headlines that capture the stories of these charts: “The Opposing Futures in the Eyes of Different Generations” by Helena of Pewaukee High School and “The Ages of Optimism” by Zoe, both from Wisconsin; “Is Each Generation Making the World Better?” by Maggie of Academy of Saint Elizabeth in Morristown, N.J. and “Generation Battle: Is the World Getting Better or Worse?” by Taim of Gladeville Middle School in Mt. Juliet, Tenn.</p></blockquote>
<p>Cool!  I really like the idea of teaching statistical ideas using recent news.</p>
<p>And it seems they do this every week or two.  Here&#8217;s the next one that came up:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/02/WeddingsGraphLN-superJumbo-v2-1024x522.png" alt="" width="450" /></p>
<p>Since they&#8217;re doing it as a bar graph anyway, I guess they could have the y-axis go all the way down to zero.  Also, hey, let&#8217;s see the time series of divorces too!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/31/heres-a-fun-intro-lesson-on-how-to-read-a-graph/feed/</wfw:commentRss>
			<slash:comments>6</slash:comments>
		
		
			</item>
		<item>
		<title>Tigers need your help.</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/30/tigers-need-your-help/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/30/tigers-need-your-help/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sun, 30 Oct 2022 17:57:47 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Jobs]]></category>
		<category><![CDATA[Sports]]></category>
		<category><![CDATA[Stan]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48137</guid>

					<description><![CDATA[Jim Logue, Director of Baseball R&#038;D at the Detroit Tigers, writes: We are now hiring a Principal Quantitative Analyst. With this position we’re looking for someone with extensive Bayesian experience, with a secondary emphasis on baseball knowledge. The Tigers went &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/30/tigers-need-your-help/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Jim Logue, Director of Baseball R&#038;D at the Detroit Tigers, writes:</p>
<blockquote><p>We are now hiring a <a href="https://www.teamworkonline.com/multiple-properties/ilitch-sports/detroit-tigers/principal-quantitative-analyst-baseball-operations-2024660">Principal Quantitative Analyst</a>. With this position we’re looking for someone with extensive Bayesian experience, with a secondary emphasis on baseball knowledge.</p></blockquote>
<p>The Tigers went 66-96 last year so the good news is that if you join them now you can take some credit for whatever improvement they show next year!</p>
<p>I assume that knowledge of Stan will be a plus.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/30/tigers-need-your-help/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Distrust in science</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/30/distrust-in-science/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/30/distrust-in-science/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sun, 30 Oct 2022 13:14:29 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47840</guid>

					<description><![CDATA[Gary Smith is coming out with a new book, &#8220;Distrust: Big Data, Data Torturing, and the Assault on Science.&#8221; He has a lot of examples of overblown claims in science&#8212;some of these have appeared on this blog, and Smith takes &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/30/distrust-in-science/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Gary Smith is coming out with a new book, &#8220;Distrust:  Big Data, Data Torturing, and the Assault on Science.&#8221;  He has a lot of examples of overblown claims in science&#8212;some of these have appeared on this blog, and Smith takes pretty much the same position that I take on these things, so I won&#8217;t talk about that part further.</p>
<p>Rather, I want to talk about the big picture that Smith paints, which is the idea that science is very important to our lives, and bad science degrades that trust. Ironically, I think this sort of attitude is also behind some of the anti-reform movement by leading academics:  they think science is just wonderful and they&#8217;d prefer if people just kept quiet about scientific errors or chalked things up to &#8220;the self-correcting nature of science.&#8221;  Smith and I are more in the clear-the-rotten-apples-out-of-the-barrel camp.</p>
<p>Smith has a broader perspective than just talking about junk science.  He also talks about misplaced technology enthusiasm, from bitcoin-as-savior to chatbots-as-AI.  One thing he doesn&#8217;t really get into is the political angle, the idea that bad actors are sowing distrust as a tactic to reduce respect for serious science.  Familiar examples are industry-funded junk science on cigarettes and climate change, which then gets picked up by ideologues.  And then there are the HIV/AIDS denialists, which seems more like contrarianism, and the covid denialists, which has a political angle.  I&#8217;m not saying that Smith should&#8217;ve discussed all that&#8212;there&#8217;s a limit to how much can fit in one book&#8212;; I&#8217;m just bringing it up to emphasize that distrust in science is not just an unfortunate byproduct of frauds like the disgraced primatologist and confused people like the ESP guy, it&#8217;s also something that a lot of people are fostering on purpose. </p>
<p>When we write about himmicanes and forking paths and multilevel modeling and all the rest, that&#8217;s just a small part of the story.  And one of the challenges is that we can&#8217;t simply root for &#8220;science&#8221; as an institution.  We root for the good stuff but not the bad stuff.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/30/distrust-in-science/feed/</wfw:commentRss>
			<slash:comments>17</slash:comments>
		
		
			</item>
		<item>
		<title>Update on the fake story about the river laborers paying people to whip them</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/29/update-on-the-fake-story-about-the-river-laborers-paying-people-to-whip-them/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/29/update-on-the-fake-story-about-the-river-laborers-paying-people-to-whip-them/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sat, 29 Oct 2022 13:36:10 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47956</guid>

					<description><![CDATA[As you might remember from a few months ago, there was a story going around that some economists just looooved to tell. The story had all sorts of attributes that you might expect would make economists happy, including a paradox &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/29/update-on-the-fake-story-about-the-river-laborers-paying-people-to-whip-them/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>As you might remember <a href="https://statmodeling.stat.columbia.edu/2022/03/06/who-whips-the-whippers-an-infinite-regress-of-political-economy/">from a few months ago</a>, there was a story going around that some economists just looooved to tell.  The story had all sorts of attributes that you might expect <a href="https://statmodeling.stat.columbia.edu/2018/04/26/quick-rule-thumb-someone-seems-acting-like-jerk-economist-will-defend-behavior-essence-morality-someone-seems-something-nice/">would make economists happy</a>, including a paradox in which apparently bad behavior (whipping people to get them to work harder) was actually good, a subplot in which a do-gooder from the outside just doesn&#8217;t understand the real world, and an ethnic slur!  What more could you possibly ask for?</p>
<p>Well, you could ask that, if the story is being reported as being true, that there was any evidence it actually happened.</p>
<p>Below are a few versions of the story.</p>
<p>From Stephen Cheung in 2018:</p>
<blockquote><p>In 1970, Toronto’s John McManus was my guest in Seattle. I chatted to him about what happened when I was a refugee in wartime Guangxi. The journey from Liuzhou to Guiping was by river, and there were men on the banks whose job was to drag the boat with ropes. There was also an overseer armed with a whip. According to my mother, the whipper was hired to do just that by the boatmen! My tale went the rounds, and it was seized by a number of neo-institutional economists. . . . However, this could be a story invented by my mother – the smartest person I have ever known – to entertain a boy of seven!</p></blockquote>
<p>From Michael Munger in 2018:</p>
<blockquote><p>There’s a famous example in China, where a group of coolies … have to pull a barge up the Yangtze River … There’s a trade-off … how do you make the 30 guys work hard? The insight of the team production problem is we need … division of labor. … If I’m pulling, I can’t spend my time watching you and you can’t spend your time watching me. We’ll create a new job, called the monitor. … We’ll give the monitor a whip. Now this looks like slavery. The great thing about this, and this is from an article by an economist named Steven NS Cheung. He found that this guy with a whip&#8212;and this is the most incredible thing Russ!&#8212;this guy with a whip was hired and paid by the coolies!</p></blockquote>
<p>Interesting how, when a story is described as &#8220;incredible,&#8221; it often is literally not credible.</p>
<p>Clement and McCormick (1989):</p>
<blockquote><p>[the] famous Chinese boatpullers fable [where] the monitor uses his vision, intuition, and experience to determine shirking, counseling the loafers with his whip.</p></blockquote>
<p>Super-clever and counterintuitive to describe whipping as &#8220;counseling&#8221;:  that&#8217;s the kind of outside-the-box thinking that will take you far in academia.</p>
<p>Supreet Kaur, Michael Kremer, and Sendhil Mullainathan (2015):</p>
<blockquote><p>. . . in a story by Steven Cheung (1983, 8): “On a boat trip up China’s Yangtze River in the 19th Century, a titled English woman complained to her host of the cruelty to the oarsmen. One burly coolie stood over the rowers with a whip, making sure there were no laggards. Her host explained that the boat was jointly owned by the oarsmen, and that they hired the man responsible for flogging.”</p></blockquote>
<p>This one&#8217;s notable in that it puts an entire passage in quotes, but it turns out the passage was not in Cheung (1983) at all&#8212;not on page 8 or anywhere else in that article&#8212;it actually came from a collection from 1998 of old Harvard Business School exam questions.</p>
<p>It&#8217;s an awesome transposition in that it adds the &#8220;titled English woman,&#8221; who is a stand-in for all those soft-headed non-economists out there who aren&#8217;t willing to think the unthinkable etc., and also it changes various details, for example the people pulling the boat became oarsman, and of course the word &#8220;coolie&#8221; was introduced.  I guess he had to be &#8220;burly,&#8221; what with all the flogging he had to do!  In a particularly modern touch, the host was there to &#8220;explain&#8221; the principles of Econ 101 to the silly lady.</p>
<p>And what about the woman being &#8220;titled&#8221;&#8212;that particular elaboration seems gratuitous, no?  But it fits in well with economists&#8217; view of themselves as tribunes of the people, consumer sovereignty and all that.  The story wouldn&#8217;t work so well if the person who &#8220;complained&#8221; was herself poor and maybe some had experience with physical pain herself.</p>
<p>On the plus side, Kaur et al. <a href="https://www.journals.uchicago.edu/doi/abs/10.1086/721271">issued a correction</a>:</p>
<blockquote><p>While the incorrect quote also appears in other earlier sources, it does not appear in Cheung’s original article. . . . The inaccurate quote was included simply as a way to illustrate the idea that joint production might necessitate the need for monitoring. . . . However, the quote is in no way central to the core point of the paper, or even for the discussion in section VI of the paper. . . . this incorrect quote can be omitted from the paper without any impact on the substance of the paper.</p></blockquote>
<p>I think they&#8217;re right&#8212;the paper would&#8217;ve been just fine without the quote&#8212;it&#8217;s just funny that 3 authors and some number of reviewers and journal editors all read the paper, and none of them noticed how ridiculous the story was.</p>
<p>And just one more thing.  Let&#8217;s accept that the substance of the paper is unaffected by the whipped-boatmen story.  But, the fact that many economists have presented the story as true, even though it&#8217;s actually the result of a series of wild embellishments from an initially speculative source . . . that&#8217;s interesting, right?  I don&#8217;t have it in me to write an article on the topic that would pass muster at the Journal of Political Economy (or any journal at all!), but I think there&#8217;s something there!</p>
<p>To put it another way, if the whipped-boatmen story is so consistent with the substantive message of the paper that it was included <em>as an example to demonstrate the theory</em>,(as the authors put it in their rejoinder, &#8220;The inaccurate quote was included simply as a way to illustrate the idea that joint production might necessitate the need for monitoring.&#8221;), and then it turns out the story is false, then maybe this should cast some doubt on said theory?  Maybe joint production might not necessitate the need for monitoring as often as they think?  At least not in the sense of &#8220;counseling the loafers with his whip&#8221;?</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/29/update-on-the-fake-story-about-the-river-laborers-paying-people-to-whip-them/feed/</wfw:commentRss>
			<slash:comments>16</slash:comments>
		
		
			</item>
		<item>
		<title>&#8220;Graphs do not lead people to infer causation from correlation&#8221;</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/28/graphs-do-not-lead-people-to-infer-causation-from-correlation/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/28/graphs-do-not-lead-people-to-infer-causation-from-correlation/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Fri, 28 Oct 2022 13:04:31 +0000</pubDate>
				<category><![CDATA[Statistical graphics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47282</guid>

					<description><![CDATA[Madison Fansher, Tyler Adkins, and Priti Shah write: Media articles often communicate the latest scientific findings, and readers must evaluate the evidence and consider its potential implications. Prior work has found that the inclusion of graphs makes messages about scientific &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/28/graphs-do-not-lead-people-to-infer-causation-from-correlation/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Madison Fansher, Tyler Adkins, and Priti Shah <a href="https://psycnet.apa.org/record/2022-36700-001">write</a>:</p>
<blockquote><p>Media articles often communicate the latest scientific findings, and readers must evaluate the evidence and consider its potential implications. Prior work has found that the inclusion of graphs makes messages about scientific data more persuasive (Tal &#038; Wansink, 2016). One explanation for this finding is that such visualizations evoke the notion of “science”; however, results are mixed. In the current investigation we extend this work by examining whether graphs lead people to erroneously infer causation from correlational data. In two experiments we gave participants realistic online news articles in which they were asked to evaluate the research and apply the work’s findings to a real-life hypothetical scenario. Participants were assigned to read the text of the article alone or with an accompanying line or bar graph. We found no evidence that the presence of graphs affected participants’ evaluations of correlational data as causal. Given that these findings were unexpected, we attempted to directly replicate a well-cited article making the claim that graphs are persuasive (Tal &#038; Wansink, 2016), but we were unsuccessful. Overall, our results suggest that the mere presence of graphs does not necessarily increase the likelihood that one infers incorrect causal claims.</p></blockquote>
<p>A paper by Wansink didn&#8217;t replicate???  Color me gobsmacked.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/28/graphs-do-not-lead-people-to-infer-causation-from-correlation/feed/</wfw:commentRss>
			<slash:comments>13</slash:comments>
		
		
			</item>
		<item>
		<title>You can read for free but comments cost money . . . or is it the other way around?</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/27/you-can-read-for-free-but-comments-cost-money-or-is-it-the-other-way-around/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/27/you-can-read-for-free-but-comments-cost-money-or-is-it-the-other-way-around/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 27 Oct 2022 13:26:46 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47955</guid>

					<description><![CDATA[A correspondent who might want to remain anonymous (if not, he can reply in comments) writes: You really do need to have the courage of my convictions and you might make some profit on the way. I am writing about &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/27/you-can-read-for-free-but-comments-cost-money-or-is-it-the-other-way-around/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>A correspondent who might want to remain anonymous (if not, he can reply in comments) writes:</p>
<blockquote><p>You really do need to have the courage of my convictions and you might make some profit on the way.  I am writing about the latest business model whereby one not only has to subscribe, i.e., pay for what was once free, but that only gets one in the door and not into the inner sanctums.  The NYT has features that once were available&#8212;in the distant past of a few months ago&#8212;as part of the user&#8217;s subscription but now require an additional ponying up.</p>
<p>Consequently, you ought to do the same with your blog and offer some sort of tiered entitlement.  An ordinary contributor will be allowed to comment on Wansink or David Brooks but would need to pay a nominal/exorbitant fee to post anything about novelists or the Hoover Institution; in between might be Harvard or the misuse of priors.  Perhaps I have the hierarchy upside down, but you get the idea.</p></blockquote>
<p>If scientific citations are worth $100,000 <a href="https://statmodeling.stat.columbia.edu/2021/09/21/more-on-that-claim-that-scientific-citations-are-worth-100000-each/">each</a>, how much should I be charging for blog comments?  Or how much should I be paying for them?  It&#8217;s never clear which direction the payment should be going.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/27/you-can-read-for-free-but-comments-cost-money-or-is-it-the-other-way-around/feed/</wfw:commentRss>
			<slash:comments>16</slash:comments>
		
		
			</item>
		<item>
		<title>Speculating about assigning probabilities to speculative theories:  Lots of problems here.</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/26/speculating-about-assigning-probabilities-to-speculative-theories-lots-of-problems-here/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/26/speculating-about-assigning-probabilities-to-speculative-theories-lots-of-problems-here/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Wed, 26 Oct 2022 13:43:58 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47781</guid>

					<description><![CDATA[Asher Meir writes: Whenever I see theories about ancient stuff I always feel it is very speculative. &#8220;This artifact is a stone ax from a hominid from c. 800,000 BP&#8221;. &#8220;The Samson story in the book of Judges is based &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/26/speculating-about-assigning-probabilities-to-speculative-theories-lots-of-problems-here/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Asher Meir writes:</p>
<blockquote><p>Whenever I see theories about ancient stuff I always feel it is very speculative. &#8220;This artifact is a stone ax from a hominid from c. 800,000 BP&#8221;. &#8220;The Samson story in the book of Judges is based on folk legends about a Hercules-like half-man, half-god figure, but edited to make it conform to a monotheistic worldview&#8221;. To the extent that these conclusions really represent the best understanding of experts, they sound to me like a maximum likelihood estimator when the likelihood function is very flat. In such a case, the most likely of all possible explanations is quite unlikely to be the right one.</p>
<p>I&#8217;m not really an expert in anthropology or in philology, but when I do look at the professional discussions &#8211; and not only at the conclusions &#8211; I still have the feeling that scholars are trying to weigh one explanation against others and agree which is the most likely of all explanations considered. I don&#8217;t really doubt that they are very good at this, but I&#8217;m not sure they realize how far &#8220;the most likely&#8221; can be from &#8220;likely&#8221;.</p>
<p>When you have a continuous variable, you can give some kind of confidence interval. You can draw it into your graph with interval bars or whatever. I think it&#8217;s worth thinking about how we could generalize this idea to disjoint theories. It&#8217;s either the humerus of a huge dinosaur or the femur of a smaller one. It&#8217;s either a stone ax or a sharpening tool. Either this ancient narrative took a legend about a superhuman figure and cut him down to human size, or perhaps it took stories about an actual historical hero and talked up his feats. Etc.</p>
<p>Instead of arguing over which explanation is the correct one &#8211; which as far as I can tell means arguing over which explanation is most likely to be correct &#8211; experts could argue over what probabilities to attach to each explanation. I often feel that when I read something like &#8220;experts agree that this is a dinosaur bone&#8221; it really means something like: &#8220;There is a 40% chance that this is a dinosaur bone, but also a 20% chance it belonged to a hippopotamus and a 10% chance for each of four other, comparatively improbable possibilities, including that it is not ancient at all.&#8221; Which is much more informative.</p></blockquote>
<p>My reply:  I think it&#8217;s fine to assign such probabilities.  We should just be aware of some difficulties that always seem to arise when we try to do so.</p>
<p>First, there&#8217;s the political or religious angle.  How can we say that there&#8217;s an essentially zero probability that Moses crossed the Red Sea or that that Mormon dude found gold tablets underground or that God said to Abraham, &#8220;Kill me a son&#8221; or whatever?  For politeness we&#8217;re supposed to either give these sorts of things meaningful probabilities or just somehow avoid talking about them.</p>
<p>Second, there are all the things that stop people from freely betting on these probabilities:  thin markets, the vig, betting limits that make it difficult to arbitrage, uncertainty in collection, etc.  All of that arises even in bets with outcomes that can be measured (recall some of the implausibly high probabilities implicitly assigned to some outlandish events during the 2020 presidential election). With the dinosaur bone thing it&#8217;s even more difficult because sometimes you can never be sure about the answer.</p>
<p>The other challenge is how to think about the &#8220;all other possibilities&#8221; option.  Once you include &#8220;all other possibilities&#8221; in the probability tree, you can&#8217;t do any sort of serious Bayesian inference because there&#8217;s no reasonable way to set up a probability model for the data conditional on reality being something not yet considered in the model.</p>
<p>My usual answer to this last point is to emphasize that all our models are conditional on some assumptions, which is fine, but it&#8217;s counter to the scientific goal of evaluating imperfect hypotheses about the world.  That&#8217;s one reason, perhaps that key reason, that I consider Bayesian inference to be a valuable <em>tool</em> of science but not a good <em>framework</em> for scientific learning</p>
<p>I use Bayesian inference to learn within models, to evaluate models, and to make decisions, but when I&#8217;m doing science or engineering I don&#8217;t have a sense of working with an overall Bayesian structure.</p>
<p><strong>P.S.</strong>  Unrelatedly, Meir writes:</p>
<blockquote><p>I sent my &#8220;Gibbs sampling&#8221; thing to a few people, and it turns out almost none of them understood it. Sampling is a very common technique in rap music where you include a clip from a song, usually an old one, and you overlay it with new vocals, instrumentals etc.  (Go to 1:45 <a href="https://www.youtube.com/watch?v=KPV1InMNUf4">here</a>.) The Bee Gees are the Gibb brothers. (Brothers Gibb.) So a rap song that includes a clip from a Bee Gees song (there are many) is &#8220;Gibbs sampling&#8221;. I thought it was very funny but evidently the Venn diagram of the people who are into the various pieces of this riddle intersects only on me.</p></blockquote>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/26/speculating-about-assigning-probabilities-to-speculative-theories-lots-of-problems-here/feed/</wfw:commentRss>
			<slash:comments>54</slash:comments>
		
		
			</item>
		<item>
		<title>Wandering through Sforza castle</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/25/wandering-through-sforza-castle/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/25/wandering-through-sforza-castle/#comments</comments>
		
		<dc:creator><![CDATA[Lizzie]]></dc:creator>
		<pubDate>Tue, 25 Oct 2022 18:05:37 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48133</guid>

					<description><![CDATA[Weekend before last I spent a day in Milan to see an old colleague (I am on leave in Zurich just now so miraculous things like a dayhop to Milan are feasible). He used to be a research technician in &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/25/wandering-through-sforza-castle/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/sm-1.jpg"><img loading="lazy" class="alignnone  wp-image-48134" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/sm-1-1024x768.jpg" alt="" width="474" height="345" /></a></p>
<p>Weekend before last I spent a day in Milan to see an old colleague (I am on leave in Zurich just now so miraculous things like a dayhop to Milan are feasible). He used to be a research technician in my lab where he did the classical ecology lab tasks of things like identifying and tagging trees in forests, making thousands of observations of leaf unfolding in a growth chamber experiment (on paper! Though that part was neither my idea nor his) and nudged the lab forward through working on automating photo capture of leaf unfolding. The resulting images and time lapse videos were gorgeous, but they didn&#8217;t change how I did science.</p>
<p>But since then my Bayesian models have become far more generative&#8212;many thanks to my Stan collaborators&#8211;and I have started to realize some sad hard truths about my data and my science. The first is that you need a lot more data than I often have to fit some of the models I think underlie the processes I am studying. I work on winegrapes because they have way more data than other systems I am interested in (such as forests, which critically store carbon) and when I don&#8217;t have enough data to fit a temperature response curve for winegrapes I can skip trying it on more `natural systems.&#8217; The second is that we also need way better data. A stats-department colleague said to me this year, &#8216;it&#8217;s not like you don&#8217;t have a lot of data, it&#8217;s like the data you have are quarters [and you need higher denominations].&#8217; By later in the day when he next picked up the metaphor, but was now calling my data pennies. (Sigh.)</p>
<p>He&#8217;s not wrong. Ecology has a history of valuing what you can learn staring intently at a backyards pond or an artificial pool near Palermo full of water bugs. We need to understand the &#8216;natural history&#8217; well to understand systems. Very true, but I sometimes wonder how we advance. A massive NSF project to collect lots of large-scale, <a href="https://www.neonscience.org/" target="_blank" rel="noopener">NEON</a>, has not revolutionized much. For my own research, I think we need to cover greater temperature variation &#8212; and work harder to know what happens at the extremes (you have to wait a long time for plants to do something at 5C, but maybe we need to wait for that; at the other end many warming chambers don&#8217;t function about 30 C well, but that&#8217;s well below what&#8217;s too hot for most plants), we need more replicates and we need to collect better data, at finer scales.</p>
<p>Which is part of why I went to Milan. To pick my colleague&#8217;s brain about how my lab can best break out. He&#8217;s now building up new FabLab for his company&#8217;s new North American headquarters in Chicago, and had some useful ideas.</p>
<p>At the end of our meeting he asked if he should go to grad school, which struck me. It struck me for a lot of reasons, one is that some undergrads in my lab, who I think could bridge new technologies to ecology, are getting scooped up by start-ups digitizing agriculture, and putting their undergrad degrees on a possibly never-ending pause.</p>
<p>The same weekend I saw a colleague from my long-lost <a href="https://en.wikipedia.org/wiki/National_Center_for_Ecological_Analysis_and_Synthesis" target="_blank" rel="noopener">NCEAS</a> days. In between nerd-crushing/raving about our colleague Jim Regetz, we discussed the apparent disconnect between the number of PhDs being awarded (erm, not sure about that verb) and the number of job openings where a PhD is critical. Some of his former postdocs were starting a new company, trying to make theoretical ecological models more useful to the point of underpinning a for-profit company.</p>
<p>I imagine academia often feels to be falling behind, but this weekend I felt it a little more acutely. We&#8217;re supposed to have the freedom and metaphorical space to be racing ahead. But it doesn&#8217;t feel that way when I can easily see why students in my lab would &#8216;pause&#8217; undergrad to race around North America to improve how we harvest wheat, when we churn out publications faster and faster at the expense of the time it takes to really advance science (it&#8217;s so much quicker to grab a p-value than to develop a model with parameters you care about, then step back and gape at that the uncertainty around the estimates; p-values are so happy to hide your meaningful parameters and their uncertainty from you), or similarly churn out PhDs without a clear idea of their job prospects (hello Canada&#8217;s `HQP&#8217;). I am not so worried about folks in my lab, we train strongly in computational methods and how to design and answer useful questions&#8212;skills industry and beyond needs, but I worry about the future, and I could certainly train in this area better if there was more pressure, recognition, and support for it in ecology.</p>
<p>On the good news side, I enjoyed my take-out pizza from Milan for two glorious dinners!</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/25/wandering-through-sforza-castle/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
		<item>
		<title>Best letter-to-the-editor ever (Tocqueville and Algeria)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/25/best-letter-to-the-editor-ever-tocqueville-and-algeria/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/25/best-letter-to-the-editor-ever-tocqueville-and-algeria/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Tue, 25 Oct 2022 13:01:53 +0000</pubDate>
				<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47957</guid>

					<description><![CDATA[From the London Review of Books, 7 July 2022: William Davies writes that Alexis de Tocqueville ‘paid little attention to the French colonisation of Algeria’. In fact, Tocqueville was regarded as the National Assembly’s leading expert on Algeria and made &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/25/best-letter-to-the-editor-ever-tocqueville-and-algeria/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>From the <a href="https://www.lrb.co.uk/the-paper/v44/n11/william-davies/destination-unknown">London Review of Books</a>, 7 July 2022:</p>
<blockquote><p>William Davies writes that Alexis de Tocqueville ‘paid little attention to the French colonisation of Algeria’. In fact, Tocqueville was regarded as the National Assembly’s leading expert on Algeria and made two visits to the country in 1841 and 1846, during the army’s counterinsurgency against a rebellion led by the Emir Abdel-Kader. He was also an impassioned advocate for Algeria’s violent colonisation. In his 1841 report on Algeria, he wrote: ‘I believe that the right of war authorises us to ravage the country and that we must do it, either by destroying harvests during the harvest season, or year-round by making those rapid incursions called razzias, whose purpose is to seize men or herds.’ While many of his fellow liberals ‘find it wrong that we burn harvests, that we empty silos, and finally that we seize unarmed men, women and children’, he felt that ‘these, in my view, are unfortunate necessities, but ones to which any people who want to wage war on the Arabs are obliged to submit.’ He advocated total war to defeat the insurrection, followed by the creation of a settler-colony, because ‘until we have a European population in Algeria, we shall never establish ourselves there [in Africa] but shall remain camped on the African coast. Colonisation and war, therefore, must proceed together.’ His brazen imperialism went hand in hand with his liberalism.</p>
<p>Adam Shatz</p></blockquote>
<p>Wow.  Davies really got that one wrong!</p>
<p>History is full of twists of turns, and we shouldn&#8217;t be too harsh on Davies for making that one mistake in this long review&#8212;he was writing about a topic he knew nothing about, and he didn&#8217;t think to google *Tocqueville Algeria*.  Understandable:  you can&#8217;t get around to googling everything.</p>
<p>I do think, though, that it would be gracious of him to respond to that letter with a brief, &#8220;Sorry, my bad,&#8221; and for the LRB to publish this response.  Just publishing the letter without an acknowledgment . . . that&#8217;s the kind of thing you&#8217;d do when there&#8217;s a difference of opinion, not a blatant factual error.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/25/best-letter-to-the-editor-ever-tocqueville-and-algeria/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
		<item>
		<title>Funny NYT corrections</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/24/funny-nyt-corrections/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/24/funny-nyt-corrections/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Mon, 24 Oct 2022 13:58:08 +0000</pubDate>
				<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=46380</guid>

					<description><![CDATA[I happened to come across this one from a few years ago: Correction: June 16, 2014 Nicholas Kristof’s column on Thursday misspelled the middle name of a Vanderbilt professor. She is Cecilia Hyunjung Mo, not Hyunjong. Similarly, there was this &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/24/funny-nyt-corrections/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I happened to come across <a href="https://statmodeling.stat.columbia.edu/2014/06/21/race-top-nyt-columnists-edition/">this one</a> from a few years ago:</p>
<blockquote><p>Correction: June 16, 2014</p>
<blockquote><p>Nicholas Kristof’s column on Thursday misspelled the middle name of a Vanderbilt professor. She is Cecilia Hyunjung Mo, not Hyunjong.</p></blockquote>
<p>Similarly, there was this memorable correction from a David Brooks column a couple years ago:</p>
<blockquote><p>An earlier version of this column misstated the location of a statue in Washington that depicts a rambunctious horse being reined in by a muscular man. The sculpture, Michael Lantz’s ‘Man Controlling Trade’ (1942), is outside the Federal Trade Commission, not the Department of Labor.</p></blockquote>
<p>I think if you’re willing to correct the spelling of one vowel in somebody’s middle name or the location of a statue of a rambunctious horse, you should be willing to correct the erroneous statement, “Researchers find that female-named hurricanes kill about twice as many people as similar male-named hurricanes because some people underestimate them,” or various erroneous <a href="https://statmodeling.stat.columbia.edu/2014/04/03/boldest-experiment-journalism-history-admit-made-mistake/">economic</a> and <a href="https://statmodeling.stat.columbia.edu/2013/02/20/my-beef-with-brooks-the-alternative-to-good-statistics-is-not-no-statistics-its-bad-statistics/">education</a> statistics. Again, outside the <a href="https://statmodeling.stat.columbia.edu/2014/05/18/never-back-culture-poverty-culture-journalism/">code of the street</a>, there should be no shame in issuing a correction. We all make mistakes, dude.</p></blockquote>
<p>In comments, Keith Ellis <a href="https://statmodeling.stat.columbia.edu/2014/06/21/race-top-nyt-columnists-edition/#comment-174523">wrote</a>:</p>
<blockquote><p>One thing that you didn’t mention in your earlier post is that it’s not just that pundits have very strong incentives to maintain an illusion of infallibility, it’s also the case that people generally process information about a source’s reliability asymmetrically and this means that maintaining credibility is extremely fragile.</p>
<p>Specifically, people will more strongly recall, and weight, that someone has been proven wrong than that they’ve been proven right. Furthermore, a single admission of error will disproportionately damage someone’s credibility, and particularly so, because a) this negative information is seen as especially important, b) the admission of error removes all ambiguity about whether an error was made, and c) because such admissions are rare, they are especially memorable.</p>
<p>It’s difficult to overstate how much various intellectual subcultures combat this natural tendency by turning it on its head and seeing a willingness to admit error as a necessary prerequisite to the discovery of truth, socratic-like. But those of us who have strong affiliations with such cultures — say, in academia or science — will correspondingly be somewhat naive about how strong the opposing natural tendency is elsewhere.</p>
<p>And while the natural tendency is strong generally, it’s especially and arguably maximally strong at the nexus of opinion journalism and politics. As you discuss in your prior post, this is a subculture where there is everything to lose by admitting error. . . .</p>
<p>For Brooks or Kristof to admit to a non-trivial error, as you suggest they do, would be for their admissions to immediately become high-profile fodder for their critics. And not just then, but forever more — a link to the admission, a quote of it, will be repeated at any occasion when a club to use against their credibility is wanted. But more to the point, such a club will work. It will work because people strongly recall that someone was proven wrong about something and especially they recall when someone admitted to it and, finally, they weight that information very heavily when evaluating credibility.</p></blockquote>
<p>On the plus side, they have no problem correcting misspellings of people&#8217;s middle names and locations of horse statues.</p></blockquote>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/24/funny-nyt-corrections/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Last post on that $100,000 Uber paper</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/23/last-post-on-that-100000-uber-paper/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/23/last-post-on-that-100000-uber-paper/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sun, 23 Oct 2022 13:33:38 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47950</guid>

					<description><![CDATA[We recently had two posts (here and here) on the controversy involving Alan Krueger, the economist who was paid $100,000 in 2015 by the taxi company Uber to coauthor a paper for them. As I wrote, I&#8217;ve done lots of &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/23/last-post-on-that-100000-uber-paper/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>We recently had two posts (<a href="https://statmodeling.stat.columbia.edu/2022/07/28/what-can-100000-get-you-nowadays-discussion-of-the-market-for-papers-in-economics/">here</a> and <a href="https://statmodeling.stat.columbia.edu/2022/08/19/krueger-uber-100000-update/">here</a>) on the controversy involving Alan Krueger, the economist who was paid $100,000 in 2015 by the taxi company Uber to coauthor a paper for them.  As I wrote, I&#8217;ve done lots of business consulting myself, so I don&#8217;t really see any general problem here, but, at this point it seems that nobody really trusts that particular article anyway.  I guess what Uber got from the payment was a short-term benefit to its reputation:  the article came out at a time when much of the economics profession was in love with Uber, and this paper was just one more part of it.</p>
<p>The discussion was kicked off a couple of months ago by an economist who sent me an email on the topic and who wishes to remain anonymous.  After seeing the two posts, my correspondent writes:</p>
<blockquote><p>I think I largely agree with you that almost everyone is now discounting the paper, pretty much completely. And after I read what the Uber executives were doing (or at least thinking of doing), it&#8217;s hard not to come to the same conclusion. Even if nothing worst was done, the <em>possibility</em> that the some data might not be provided means that there&#8217;s an inherent bias that falls right there with the typical issues you&#8217;ve discussed before of forking paths, except instead of happening due to choices made by the researcher, it&#8217;s happening at the data provider and in a even more blatantly manipulative way. Barring some new revelation from Hall or the other economists engaged by Uber that they somehow managed to stop Uber from acting this way (or that Uber somehow convinces us that they didn&#8217;t), scepticism is more than justified.</p>
<p>Note, that I&#8217;m emphasising here the core issue of the Uber executives and administrative staff much more so than the researchers&#8217; actions, to me that&#8217;s where the biggest problem lies.</p>
<p>There&#8217;s one last issue that I feel remains and I won&#8217;t be surprised if you&#8217;re still thinking of addressing it, which is how to do we move forward from all this? The inherent tension of the costs and benefits of using proprietary data are now even more sharply delimitated now. Should researchers no longer accept payment from companies they engage with if they want to publish in academic journals and working paper series? I know that you&#8217;ve written that you&#8217;ve received payment in such situations in the past, does that make you feel you need to revaluate things? Essentially, I&#8217;m asking what actions can researchers take that would give themselves (and their peers) the assurance that companies are not trying to use them by omitting data and the like?</p>
<p>Don&#8217;t get me wrong, the potential benefits are high from using this sort data and not just from a getting-published-in-a-great-journal perspective. I&#8217;ve come up with several research ideas that would only really be possible with such data and I would leap at such opportunities; to be honest, I have a big research project that, in my esteem is quite important, that&#8217;s currently stalled precisely because of this issue! So, I would certainly not advocate for ceasing these types of collaborations. But it does seem to me that to best benefit society and science, there needs to be a discussion about what&#8217;s the best way forward&#8230;</p>
<p>Anyway, I&#8217;ve clearly written too much as it stands, I have teaching prep and research to do!  I&#8217;ve really enjoyed seeing the debates and I&#8217;m happy that these questions are being addressed openly. We, as in economists, could use some more careful thinking about all of this&#8230;</p></blockquote>
<p>My response:  I think open display of funding is a good idea.  Some journals expect this sort of list and some don&#8217;t.  I include a list of past and present sponsors <a href="https://statmodeling.stat.columbia.edu/sponsors/">here</a>. Also, people have lots of non-financial conflicts of interest relating to friends, political ideologies, fear of pissing off the wrong people, etc.  I bring these up not to say that conflict-of-interest declarations are a bad idea, just to say that things are complicated.</p>
<p>I guess that right now I assume that undeclared conflicts of one sort or another are inevitable in many cases.  In the Uber example, the conflict was declared right away on the front page of the article, so that was pretty clear, and I think the only reason the news media didn&#8217;t make a bigger deal of the conflict at the time was that the article was telling them something they wanted to hear.</p>
<p>The other issue brought up by my correspondent was proprietary data.  I&#8217;m not sure what to say here except that lots of data isn&#8217;t proprietary or confidential at all, but researchers still aren&#8217;t sharing.  An extreme case was that <a href="https://statmodeling.stat.columbia.edu/2014/05/30/mmm-statistical-significance-evilicious/">disgraced primatologist</a> who wouldn&#8217;t even share his videotapes with his own research collaborators.  Another example would be those surveys you sometimes here about where they don&#8217;t tell you how they did the sampling or what were the questions being asked (<a href="https://statmodeling.stat.columbia.edu/2022/08/07/i-know-it-might-sound-strange-but-i-believe-youll-be-coming-back-before-too-long/">here&#8217;s an example</a>).</p>
<p>But, yeah, if as a reader of the article you don&#8217;t have full access to the data or the data-collection mechanism, then that limits the trust you have in the results, whether or not the funding source has been clarified.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/23/last-post-on-that-100000-uber-paper/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Best Meetup swag: OVHcloud</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/22/best-meetup-swag-ovhcloud/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/22/best-meetup-swag-ovhcloud/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Sat, 22 Oct 2022 22:00:51 +0000</pubDate>
				<category><![CDATA[Statistical computing]]></category>
		<category><![CDATA[Zombies]]></category>
		<category><![CDATA[swag]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48120</guid>

					<description><![CDATA[Mitzi gave a talk last night at the Paris PyData Meetup. It was hosted by OVHcloud, a cloud provider based in Paris. Not only was the setting amazing (window ringed conference room with a thunderstorm outside with lightning bolts shooting &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/22/best-meetup-swag-ovhcloud/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Mitzi gave a talk last night at the <a href="https://www.meetup.com/fr-FR/pydata-paris/">Paris PyData Meetup</a>.  It was hosted by OVHcloud, a cloud provider based in Paris.  Not only was the setting amazing (window ringed conference room with a thunderstorm outside with lightning bolts shooting behind Mitzi), they passed out the best swag ever.  Check it out:</p>
<p><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-scaled.jpg"><img loading="lazy" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-768x1024.jpg" alt="" width="584" height="779" class="alignnone size-large wp-image-48121" srcset="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-768x1024.jpg 768w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-225x300.jpg 225w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-1152x1536.jpg 1152w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-1536x2048.jpg 1536w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/OVHcloud-swag-scaled.jpg 1920w" sizes="(max-width: 584px) 100vw, 584px" /></a></p>
<p>At first I thought the thing in the lower left was one of those old school erasers.  But then I realized it was a combination beer key and pizza cutter.  How cool is that for a meetup?  And naturally, this meetup had both beer in bottles and pizza.  But no chance to try it out, as the staff supplied beer keys and had cut all the pizza.</p>
<p>Yes, that&#8217;s a pair of socks on top.  I remember having lunch at a conference with <a href="http://www.eecs.harvard.edu/shieber/">Stu Shieber</a> years ago, when I had plenty of swag from the startup I was at (SpeechWorks).  Stu said we should pass out branded sweatpants, because everyone passed out t-shirts and he had to buy his own sweatpants.</p>
<p>Then there&#8217;s the quintessential French writing device&#8212;the Bic 4-color pen.  Bic&#8217;s a French company, so you see these everywhere.  They were exotic in the states when I was a kid and I used to get one for Xmas every year. There&#8217;s also a pack of biodegradable playing cards, a tote bag, and a really nice notebook.</p>
<p>Thanks, OVHcloud, both for hosting and the super-cool swag.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/22/best-meetup-swag-ovhcloud/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Statistical methods that only work if you don&#8217;t use them (more precisely, they only work well if you avoid using them in the cases where they will fail)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sat, 22 Oct 2022 13:47:29 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47611</guid>

					<description><![CDATA[Here are a couple examples. 1. Bayesian inference You conduct an experiment to estimate a parameter theta. Your experiment produces an unbiased estimate theta_hat with standard error 1.0 (on some scale). Assume the experiment is clean enough that you&#8217;re ok &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Here are a couple examples.</p>
<p><strong>1.  Bayesian inference</strong></p>
<p>You conduct an experiment to estimate a parameter theta.  Your experiment produces an unbiased estimate theta_hat with standard error 1.0 (on some scale).  Assume the experiment is clean enough that you&#8217;re ok with the data model, theta_hat ~ normal(theta, 1.0).  Now suppose that theta_hat happens to equal 1, and further suppose you are doing Bayesian inference with a uniform prior on theta (or, equivalently, a very weak prior such as normal(0, 10)).  Then your posterior distribution is theta ~ normal(1, 1), and the posterior probability is 84% that theta is greater than zero.  (See section 3 <a href="http://www.stat.columbia.edu/~gelman/research/published/physics.pdf">here</a>.)  You then should be willing to bet that theta is greater than zero with 5-1 odds (assuming you&#8217;re not planning to bet against anyone with private information about theta).  OK, maybe not 5-1 because you&#8217;re concerned about the <a href="https://statmodeling.stat.columbia.edu/2015/06/19/in-which-a-complete-stranger-offers-me-a-bet/">jack of spades</a>, etc.  So, 4-1.</p>
<p>Still, it&#8217;s a problem, if you&#8217;re willing to routinely offer 4-1 bets on data that are consistent with noise (which is one way to put it when you observe an estimate that&#8217;s 1 standard error away from zero).  Go around offering those bets to people and you&#8217;ll soon lose all your money.</p>
<p>OK, yeah, the problem is with the flat prior.  But that&#8217;s something that people do!  And the flat-prior-analysis isn&#8217;t terrible; it can summarize the data in useful ways, even if you shouldn&#8217;t &#8220;believe&#8221; or lay bets on all its implications.</p>
<p>In practice, what do we do?  We use the Bayesian inference selectively, carefully.  We report the 95% posterior interval for theta, (-1, 3), but we don&#8217;t talk about the posterior probability that theta is positive, and we don&#8217;t publicize those 5-1 odds.  Similarly, when an estimate is two standard errors away from zero, we consider it as representing some evidence for a positive effect, but we wouldn&#8217;t bet at 39-1 odds.  In a sense, we&#8217;re acting as if we have a high <a href="http://www.stat.columbia.edu/~gelman/research/published/default_prior_zwet.pdf">prior probability</a> that theta is close to zero&#8212;but not quite, as we&#8217;re still giving out that (-1, 3) interval.  We&#8217;re being incoherent, which is fine&#8212;you know what they say about foolish consistency&#8212;but in any case we should be aware of this incoherence.</p>
<p><strong>2.  Null hypothesis significance testing</strong></p>
<p>You have a <a href="http://www.stat.columbia.edu/~gelman/research/published/kanazawa.pdf">hypothesis</a> that beauty is related to the sex ratio of babies, so you find some data and compare the proportion of girl births from attractive and unattractive parents.  You&#8217;ve heard about this thing called <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf">forking paths</a> so you preregister your study.  Fortunately, you still find something statistically significant!  OK, it&#8217;s not quite what you preregistered, and it goes in the wrong direction, and it&#8217;s only significant at the 10% level, but <a href="https://statmodeling.stat.columbia.edu/2018/05/29/exposure-forking-paths-affects-support-publication/">that&#8217;s still enough</a> to get the paper published in a top journal and have it receive two awards.</p>
<p>OK, yeah, the problem is the famous incoherence of classical statistical practice.  What exactly is that p-value?  Is it a measure of evidence (p less than 0.01 is strong evidence, p less than 0.05 is ok evidence, p less than 0.1 is weak evidence, anything else counts as no evidence at all), or is it a hard rule (p less than 0.05 gets converted into Yes, p more than 0.05 becomes No)?  It&#8217;s not clear.  The procedure described in the paragraph immediately above corresponds to treating the p-value as evidence, and this leads to obvious problems so maybe you should just use the hard rule, but that puts you in the uncomfortable position of making strong statements based on noisy data.</p>
<p>In practice, what do we do?  It&#8217;s a mix.  We use that hard 5% rule on the preregistered hypothesis&#8212;ok, not always, as indicated by the above link, but often&#8212;and then we also report p-values as evidence.  Again, incoherence is not in itself a problem, but it can lead to a worst-of-both-worlds situation as we&#8217;ve seen in Psychological Science, PNAS, etc., of a literatures that drift based on some mixture of speculation and noise.</p>
<p><strong>3.  Where are we, then?</strong></p>
<p>I&#8217;m not saying Bayes is wrong or even that null hypothesis significance testing is wrong.  These methods have their place.  What I&#8217;m saying is that they depend on assumptions, and we don&#8217;t always check these assumptions.</p>
<p>To put it another way, Bayesian methods and null hypothesis significance testing methods work&#8212;really, they work for solving engineering problems and increasing our scientific understanding, I&#8217;m not just saying they &#8220;work&#8221; to get papers published&#8212;but the way to get them to work is to use them judiciously, to walk around all the land mines.  The good news is that you can use fake-data simulation to find out where those land mines are.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/feed/</wfw:commentRss>
			<slash:comments>26</slash:comments>
		
		
			</item>
		<item>
		<title>From soup to Bayes:  Make inferences using strong assumptions not because you &#8220;believe&#8221; your model but because you don&#8217;t believe it.</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/21/from-soup-to-bayes-make-inferences-using-strong-assumptions-not-because-you-believe-your-model-but-because-you-dont-believe-it/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/21/from-soup-to-bayes-make-inferences-using-strong-assumptions-not-because-you-believe-your-model-but-because-you-dont-believe-it/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Fri, 21 Oct 2022 13:07:27 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48115</guid>

					<description><![CDATA[Seth Frey writes: There are mostly useless of bits of cognitive psychology that I’ve always loved. For example, a lot of categorization research about life on the edge of what objects are what. How flat can a bowl be before &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/21/from-soup-to-bayes-make-inferences-using-strong-assumptions-not-because-you-believe-your-model-but-because-you-dont-believe-it/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Seth Frey <a href="https://enfascination.com/weblog/post/2476">writes</a>:</p>
<blockquote><p>There are mostly useless of bits of cognitive psychology that I’ve always loved. For example, a lot of categorization research about life on the edge of what objects are what. How flat can a bowl be before it’s a plate? How narrow can a mug be before it’s a cup? How big can a cup be before it’s a bowl? Can it have a handle and not be a cup? When does too much handle make it a spoon? These are questions that can be used to create little microcosms for the study of things like culture, learning, expectations, and all kinds of complexities around the kinds of traits we’re surprisingly sensitive to.</p></blockquote>
<p>This reminds me of the old question of whether cereal with milk is a soup.  The answer seems evidently no, but it&#8217;s not immediately clear why.  If you eat cereal with milk for lunch instead of breakfast, does it become a soup?  If not, I&#8217;d say the problem is not with the bits of cereal&#8212;some soups do have bits of bread or rice or something else floating in them&#8212;but with the milk.  Milk is just too simple to be a soup base.  For that matter, if you drank a bowl of tomato sauce, you&#8217;d be drinking tomato sauce, not tomato soup.  So I think that for something to be &#8220;soup,&#8221; it requires some preparation or alteration of its non-soup components.</p>
<p>Surprisingly, often the dictionary can help with these sorts of challenges of understanding.  Looking up &#8220;soup,&#8221; we get the definition, &#8220;a liquid food made by boiling or simmering meat, fish, or vegetables with various added ingredients.&#8221;  That doesn&#8217;t sound completely right, either&#8212;for example, gazpacho is considered a soup and it doesn&#8217;t involve any boiling&#8212;but there is the &#8220;added ingredients&#8221; thing.  Yes, cereal is added to the bowl of milk (or vice-versa), but the cereal doesn&#8217;t quite feel like an &#8220;ingredient.&#8221;</p>
<p>The dictionary gives this definition of &#8220;soup up&#8221;:  &#8220;to improve the capacity for speed or increase the efficiency of (a motor or engine) by increasing the richness of the fuel mixture or the efficiency of the fuel, or by adjusting the engine.&#8221;  This seems relevant, as it captures the idea of soup being rich in some way, being more than a sum of its parts.  Cereal with milk is just the sum of its parts; it&#8217;s not &#8220;souped up.&#8221;</p>
<p>The point of this discussion is not to obsess over definitions of soup but rather to reflect that this sort of discussion can, paradoxically, be helpful in developing our understanding.</p>
<p>I say &#8220;paradoxically&#8221; here because it would be natural to think that a discussion such as &#8220;Is cereal a soup?&#8221; is pointless because definitions are arbitrary, you can define &#8220;soup&#8221; to be whatever you want it to be, and in any case no definition is completely precise&#8212;it&#8217;s a coastline-of-Britain sort of thing&#8212;and so this would all seem to be as pointless as an argument over whether the nickname for Stephanie should be spelled Steph or Stef.</p>
<p>But, no, I argue that the above example demonstrates that something <em>can</em> be learned from such a discussion, that it is not a sterile debate going in circles.  I&#8217;d argue the debate is more of a helix than a circle, in that it moves forward while turning.  To put it another way, the discussion (I wouldn&#8217;t call it a debate) on whether cereal is a soup is an entry point toward thinking harder about soup.  I wouldn&#8217;t say that thinking hard about soup is the most important goal in life, but, yeah, I do feel that my understanding is slightly deeper now.  I&#8217;ve filled in a couple of nodes in my general network of understanding.</p>
<p>How does this relate to statistics?</p>
<p>All our models are wrong but they can still be useful.  Often the value of a model is in its refutation.  Or, to say it more carefully:  by being specific with our modeling, we can learn from the ways in which it does not accurately predict the world.</p>
<p>This sort of thing happens a lot in science, that the way to understand an idea is to make specific assumptions and push through their implications, looking at, in Frey&#8217;s words, &#8220;life on the edge of what objects are what.&#8221;  And this is one reason I like an assumption-rich approach to statistics.</p>
<p>Set up a full joint distribution and use it to make (Bayesian) inferences, not because you &#8220;believe&#8221; your model but because you <em>don&#8217;t</em> believe it&#8212;because you don&#8217;t believe any model.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/21/from-soup-to-bayes-make-inferences-using-strong-assumptions-not-because-you-believe-your-model-but-because-you-dont-believe-it/feed/</wfw:commentRss>
			<slash:comments>59</slash:comments>
		
		
			</item>
		<item>
		<title>The psychology of thinking discretely</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/20/the-psychology-of-thinking-discretely/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/20/the-psychology-of-thinking-discretely/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 20 Oct 2022 13:40:00 +0000</pubDate>
				<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47721</guid>

					<description><![CDATA[Sander Greenland calls it &#8220;dichotomania,&#8221; I call it discrete thinking, and linguist Mark Liberman calls it &#8220;grouping-think&#8221; (link from Olaf Zimmermann). All joking aside, this seems like an interesting question in cognitive psychology: Why do people slip so easily into &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/20/the-psychology-of-thinking-discretely/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Sander Greenland <a href="https://statmodeling.stat.columbia.edu/2019/09/13/deterministic-thinking-dichotomania/">calls it</a> &#8220;dichotomania,&#8221; I call it discrete thinking, and linguist Mark Liberman <a href="https://languagelog.ldc.upenn.edu/nll/?p=54920">calls it</a> &#8220;grouping-think&#8221; (link from Olaf Zimmermann).</p>
<p>All joking aside, this seems like an interesting question in cognitive psychology:  Why do people slip so easily into binary thinking, even when summarizing data that don&#8217;t show any clustering at all:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/06/image.png" alt="" width="550" /></p>
<p>It&#8217;s a puzzle.  I mean, sure, we can come up with explanations such as the idea that continuous thinking requires a greater cognitive load, but it&#8217;s not just that, right?  Even when people have all the time in the world, they&#8217;ll often inappropriately dichotomize.  I guess it&#8217;s related to essentialism (what isn&#8217;t, right?), but that just pushes the question one step backward.</p>
<p>As Liberman puts it, the key fallacies are:</p>
<blockquote><p>1. Thinking of distributions as points;<br />
2. Inventing convenient but unreal taxonomic categories;<br />
3. Forming stereotypes, especially via confirmation bias.</p></blockquote>
<p><strong>P.S.</strong>  In a follow-up <a href="https://languagelog.ldc.upenn.edu/nll/?p=56746">post</a>, Liberman links to <a href="https://languagelog.ldc.upenn.edu/myl/LibermanAmazon062022.pdf">slides from a talk</a> he gave with several good examples of bad dichotomization in science reporting.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/20/the-psychology-of-thinking-discretely/feed/</wfw:commentRss>
			<slash:comments>40</slash:comments>
		
		
			</item>
		<item>
		<title>Newton&#8217;s Third Law of Reputations</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/19/newtons-third-law-of-reputations/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/19/newtons-third-law-of-reputations/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Wed, 19 Oct 2022 13:39:52 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47603</guid>

					<description><![CDATA[This news article by Tiffany Hsu explains how the big bucks earned by Matt Damon, Larry David, LeBron James, etc., from Crypto.com, etc., did not come for free. These celebs are now paying in terms of their reputation. That&#8217;s all &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/19/newtons-third-law-of-reputations/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>This <a href="https://www.nytimes.com/2022/05/17/business/media/crypto-gwyneth-paltrow-matt-damon-reese-witherspoon.html">news article</a> by Tiffany Hsu explains how the big bucks earned by Matt Damon, Larry David, LeBron James, etc., from Crypto.com, etc., did not come for free.  These celebs are now paying in terms of their reputation.</p>
<p>That&#8217;s all fine. After all, what&#8217;s the point of reputation if you can&#8217;t convert it to something else&#8212;in this case, more money for people who are already unimaginably rich&#8212;actually, my guess is that the reason these celebs were endorsing crypto is not so much for the money but because they felt they were getting in on the ground floor of the next big thing, i.e. they were more conned-upon than conning, although conned-upon in a very mild way as I guess they made some $$$ from it all&#8212;, and a positive social benefit to these stars being considered clueless at best and fraudsters at worst is that maybe for awhile we&#8217;ll be spared the advice of Matt Demon, Larry David, LeBron James, etc., on politics, culture, health, etc.  So good news all around:  these stars made money, and we&#8217;ll be less subject to their influence.</p>
<p>The episode reminded me of what&#8217;s been happening with the medical journal Lancet over the years:  they keep lending their reputation to fraudulent or politically-slanted research, and now it&#8217;s reached the point where when we see that something&#8217;s published in Lancet, we get a little bit suspicious.  Not completely so, as they publish lots of good stuff too, but a little bit.</p>
<p>Or Dr. Oz, who cashed in on the reputation of the medical establishment (and Columbia University!) to get money and fame.  When it turns sour, that reduces the future value of the &#8220;Dr.&#8221; label and the Ivy League connection.  On the other hand, if he gets elected to the Senate, then maybe the presidency, then eventually is elected king of the world, then maybe they&#8217;ll change the rules on who gets to be called Dr.:  maybe it will be an absolut requirement that if you want the &#8220;doctor&#8221; label you have to endorse at least two fraudulent cures.</p>
<p>Or the Wall Street Journal, which published <a href="https://statmodeling.stat.columbia.edu/2022/07/17/what-happens-to-the-wall-street-journals-reputation-when-they-publish-a-regular-column-by-someone-associated-with-fraudulent-research/">a regular column</a> by someone associated with fraudulent research?</p>
<p>Reputation is a <a href="https://statmodeling.stat.columbia.edu/2016/01/05/pace-study-and-the-lancet-journal-reputation-is-a-two-way-street/">two-way street</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/19/newtons-third-law-of-reputations/feed/</wfw:commentRss>
			<slash:comments>13</slash:comments>
		
		
			</item>
		<item>
		<title>David Blackwell stories</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/18/david-blackwell-stories/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/18/david-blackwell-stories/#comments</comments>
		
		<dc:creator><![CDATA[Jessica Hullman]]></dc:creator>
		<pubDate>Tue, 18 Oct 2022 18:15:24 +0000</pubDate>
				<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48108</guid>

					<description><![CDATA[This is Jessica. Recently I got asked, on a podcast, what famous scientific figure alive or dead, real or fictional I would have dinner with, and for the sake of choosing someone who was both brilliant and sounds like an &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/18/david-blackwell-stories/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><span style="font-weight: 400">This is Jessica. Recently I got asked, on a podcast, what famous scientific figure alive or dead, real or fictional I would have dinner with, and for the sake of choosing someone who was both brilliant and sounds like an inspiring person to be around, I said David Blackwell.</span></p>
<p><span style="font-weight: 400">I didn’t really know Blackwell’s work until maybe a year or year and a half ago, when I was introduced to Blackwell Ordering by a colleague. It’s a way of quantifying the instrumental value of some information generating process (aka channel or experiment or information structure) before the state of the world realizes. Imagine we have two different forecasts for predicting who will win an election, based on some set of signals like poll results, and we can choose which one to consult to make some decision, like how much of our time or money we will allocate to help our preferred candidate prior to an election. How do we decide, given that the value we get out of the forecast (assigned by some real valued payoff function) will depend on the decision we make and the state that ultimately realizes? If we know the probability distribution over the possible signals for each possible state of the world, then Blackwell’s theorem tells us that under certain conditions we can choose which forecast to use even without knowing the specific utility function or the distribution over inputs that we are dealing with. The constraint that needs to be satisfied is that one information structure needs to be a “garbling” of the first (or in Blackwell’s terms, the first needs to be sufficient for the second), meaning that we can represent the second as what we get when we apply some post-processing operation to the first (where both are represented in matrix form).</span></p>
<p><span style="font-weight: 400">Once we establish that one forecast is sufficient for the other, we know the second one can’t give us more information about the state: Blackwell showed that the expected utility of deciding using the optimal strategy for the original (non-garbled) forecast is always at least as big as using the optimal strategy with the garbled forecast. This is not just for one particular instantiation of a decision problem, but for any input distribution or utility function</span><i><span style="font-weight: 400">. </span></i><span style="font-weight: 400">So, the ordering gives a concise summary of the statistical conditions under which one experiment is more informative than another. I’m not sure the extent to which work like Blackwell’s, which he did in the early 1950’s, directly influenced later learning theories that are similarly agnostic to the input distribution, like PAC learning, but this way of thinking seems ahead of its time. When I first learned about it, I found Blackwell ordering exciting because I was frustrated with how dependent our knowledge of what makes a better visual representation of uncertainty tends to be on specific decision setups that researchers study. So the idea of theory for doing </span><span style="font-weight: 400">unconditional analysis of information structures reflecting uncertainty around some outcome was like a missing piece I was looking for. Since then I’ve been paying more attention to other work on the value of information, such as in economics. </span></p>
<p><span style="font-weight: 400">More recently I learned more about some of Blackwell’s other major contributions, like approachability. The original formulation asked under what conditions we can expect the row player in a minimax game with vector payoffs to be able to get his payoffs to approach some target set, making it a generalization of minimax for vector payoffs. But there are connections to online learning and forecasting; it turns out Blackwell&#8217;s result implies <a href="http://proceedings.mlr.press/v19/abernethy11b/abernethy11b.pdf">no-regret learning</a> and </span><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1006&amp;context=statistics_papers"><span style="font-weight: 400">calibration</span></a><span style="font-weight: 400">. There’s also Rao-Blackwell theorem, which says that (at least with enough data) it’s possible to make a better estimator of your parameter by taking the conditional expected value of your estimator given some sufficient statistic for that parameter.</span></p>
<p><span style="font-weight: 400">This fall I attended a </span><a href="https://simons.berkeley.edu/workshops/datadriven-2022-bc"><span style="font-weight: 400">workshop on data-driven decision processes</span></a><span style="font-weight: 400">, where there was a day devoted to Blackwell. Some of these were by people who spent time around him, and shared anecdotes about what he was like to work with. He sounds like an inspiring person to be around, someone who undoubtedly experienced a lot more pushback along the way that most of his faculty peers but continued to make major contributions and positively affect the people around him in all sorts of ways. Someone mentioned for instance how he had high standards for communicating ideas in the most direct, comprehensible way, like when teaching, which I respect. There are talks listed under the last day </span><a href="https://simons.berkeley.edu/workshops/schedule/16924"><span style="font-weight: 400">here</span></a><span style="font-weight: 400"> that cover his work and influence for those that are interested.</span></p>
<p><span style="font-weight: 400">However, there is one anecdote that I keep coming across as I’ve been finding out more about Blackwell&#8211; not about Blackwell himself but about his experience as a Black statistician&#8211;that I have to complain about. It involves Blackwell’s first interactions with the UC Berkeley math department, which then contained statistics, where he later spent years as a professor and chair.  Apparently, he was considered as a hire in 1942 but was not extended an offer, because of racism. Many accounts describe how the offer was blocked by the spouse of a faculty member, e.g., “</span><span style="font-weight: 400">Blackwell would later in his career find out the then head of the math department’s wife protested Blackwell’s hiring. It was customary to host faculty members in their home and the wife objected to hosting a black man in her house.”, or “</span><span style="font-weight: 400">Blackwell was blocked by one of the ‘faculty wives.’” Or, put differently: “</span><span style="font-weight: 400">In 1954, after an initial attempt in 1942, which failed due to the racial prejudice of some faculty families, Blackwell was appointed Professor.”</span></p>
<p><span style="font-weight: 400">To be clear, I do not doubt that someone’s wife (or more specifically, the chair’s wife as some records describe) protested to Blackwell getting hired. Racism is not hard to find even now, so pre-civil rights era it does not seem surprising that some professor’s wife would have made blatantly racist comments. And I’m glad that the bias that Blackwell had to deal with during his life is brought to light in recounting his career. </span></p>
<p><span style="font-weight: 400">But… isn’t it a little odd, so long after the fact, to be talking about someone’s wife as the reason Blackwell wasn’t offered a job in what was presumably at the time an all or mostly male department? I respect that speakers and authors who retell this want to acknowledge the racism Blackwell experienced in the mostly white academic institution, and I can understand why some of the original faculty involved might have been frustrated by the influence of someone’s wife. But eighty years later, it seems kind of weird to hear this retold, as if we’re going out of our way to put the blame on some woman who wasn’t even part of the department. The optics distract from the more interesting story of who Blackwell was.</span></p>
<p><span style="font-weight: 400">I doubt the reasons for bringing up this particular anecdote are to intentionally redirect blame, so much as people have heard it and decide to echo it to tell a more entertaining story. From looking around online it seems like it originated from a biography on Neyman written by Constance Reid. </span></p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/18/david-blackwell-stories/feed/</wfw:commentRss>
			<slash:comments>15</slash:comments>
		
		
			</item>
		<item>
		<title>Hey!  It&#8217;s time for another Greatest Seminar Speaker contest!</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/18/hey-its-time-for-another-greatest-seminar-speaker-contest/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/18/hey-its-time-for-another-greatest-seminar-speaker-contest/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Tue, 18 Oct 2022 13:18:40 +0000</pubDate>
				<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48072</guid>

					<description><![CDATA[First competition It all started back in 2015 when we were told that Columbia would be graced by a very special seminar from the philosopher Bruno Latour, author of many bowls of world salad such as this: The result of &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/18/hey-its-time-for-another-greatest-seminar-speaker-contest/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><strong>First competition</strong></p>
<p>It all started <a href="https://statmodeling.stat.columbia.edu/2015/01/15/picking-ideal-seminar-speaker-ultimate-bracket/">back in 2015</a> when we were told that Columbia would be graced by a very special seminar from the philosopher Bruno Latour, author of many bowls of world salad such as this:</p>
<blockquote><p>The result of a twenty five years inquiry, it offers a positive version to the question raised, only negatively, with the publication, in 1991, of &#8220;We have never been modern”: if &#8220;we” have never been modern, then what have &#8220;we” been? From what sort of values should &#8220;we” inherit? In order to answer this question, a research protocol has been developed that is very different from the actor-network theory. The question is no longer only to define &#8220;associations” and to follow networks in order to redefine the notion of &#8220;society” and &#8220;social” (as in &#8220;Reassembling the Social”) but to follow the different types of connectors that provide those networks with their specific tonalities. Those modes of extension, or modes of existence, account for the many differences between law, science, politics, and so on. This systematic effort for building a new philosophical anthropology offers a completely different view of what the &#8220;Moderns” have been and thus a very different basis for opening a comparative anthropology with the other collectives – at the time when they all have to cope with ecological crisis. Thanks to a European research council grant (2011-2014) the printed book will be associated with a very original purpose built digital platform allowing for the inquiry summed up in the book to be pursued and modified by interested readers who will act as co-inquirers and co-authors of the final results. With this major book, readers will finally understand what has led to so many apparently disconnected topics and see how the symmetric anthropology begun forty years ago can come to fruition.</p></blockquote>
<p>And I thought . . . Hey, we could do better than that.  So we created our very first Greatest Seminar Speaker contest, featuring 64 contestants in 8 categories:</p>
<p><a href="http://statmodeling.stat.columbia.edu/wp-content/uploads/2015/04/Bracket-v1.png"><img src="http://statmodeling.stat.columbia.edu/wp-content/uploads/2015/04/Bracket-v1.png" alt="Bracket v1" width="550" /></a></p>
<p>– Philosophers:<br />
Plato (seeded 1 in group)<br />
Alan Turing (seeded 2)<br />
Aristotle (3)<br />
Friedrich Nietzsche (4)<br />
Thomas Hobbes<br />
Jean-Jacques Rousseau<br />
Bertrand Russell<br />
Karl Popper</p>
<p>– Religious Leaders:<br />
Mohandas Gandhi (1)<br />
Martin Luther King (2)<br />
Henry David Thoreau (3)<br />
Mother Teresa (4)<br />
Al Sharpton<br />
Phyllis Schlafly<br />
Yoko Ono<br />
Bono</p>
<p>– Authors:<br />
William Shakespeare (1)<br />
Miguel de Cervantes (2)<br />
James Joyce (3)<br />
Mark Twain (4)<br />
Jane Austen<br />
John Updike<br />
Raymond Carver<br />
Leo Tolstoy</p>
<p>– Artists:<br />
Leonardo da Vinci (1)<br />
Rembrandt van Rijn (2)<br />
Vincent van Gogh (3)<br />
Marcel Duchamp (4)<br />
Thomas Kinkade<br />
Grandma Moses<br />
Barbara Kruger<br />
The guy who did Piss Christ</p>
<p>– Founders of Religions:<br />
Jesus (1)<br />
Mohammad (2)<br />
Buddha (3)<br />
Abraham (4)<br />
L. Ron Hubbard<br />
Mary Baker Eddy<br />
Sigmund Freud<br />
Karl Marx</p>
<p>– Cult Figures:<br />
John Waters (1)<br />
Philip K. Dick (2)<br />
Ed Wood (3)<br />
Judy Garland (4)<br />
Sun Myung Moon<br />
Charles Manson<br />
Joan Crawford<br />
Stanley Kubrick</p>
<p>– Comedians:<br />
Richard Pryor (1)<br />
George Carlin (2)<br />
Chris Rock (3)<br />
Larry David (4)<br />
Alan Bennett<br />
Stewart Lee<br />
Ed McMahon<br />
Henny Youngman</p>
<p>– Modern French Intellectuals:<br />
Albert Camus (1)<br />
Simone de Beauvoir (2)<br />
Bernard-Henry Levy (3)<br />
Claude Levi-Strauss (4)<br />
Raymond Aron<br />
Jacques Derrida<br />
Jean Baudrillard<br />
Bruno Latour</p>
<p>Paul Davidson randomized the names while following seeding constraints and <a href="https://statmodeling.stat.columbia.edu/2015/01/15/bracket/">created the bracket</a>.  We had a great time with this:  one match per day for two months, beginning with <a href="https://statmodeling.stat.columbia.edu/2015/02/03/plato-1-vs-henny-youngman/">Plato vs. Henny Youngman</a> for the first Round 1 matchup and concluding with  <a href="https://statmodeling.stat.columbia.edu/2015/04/06/the-championship-thomas-hobbes-vs-philip-k-dick/">Thomas Hobbes vs. Philip K. Dick</a> in the final.</p>
<p><strong>Second competition</strong></p>
<p>In 2019 we <a href="https://statmodeling.stat.columbia.edu/2019/01/04/back-by-popular-demand-the-greatest-seminar-speaker-contest/">brought it back</a> with <a href="https://statmodeling.stat.columbia.edu/2019/01/06/announcing-the-ultimate-seminar-speaker-contest-2019-edition/">a new set of candidates</a>:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2019/03/Bracket_2019_EoR3-1024x442.png" alt="" width="550" /></a></p>
<p>&#8211; Wits:<br />
Oscar Wilde (seeded 1 in group)<br />
Dorothy Parker (2)<br />
David Sedaris (3)<br />
Voltaire (4)<br />
Veronica Geng<br />
Albert Brooks<br />
Mel Brooks<br />
Monty Python</p>
<p>&#8211; Creative eaters:<br />
M. F. K. Fisher (1)<br />
Julia Child (2)<br />
Anthony Bourdain (3)<br />
Alice Waters (4)<br />
A. J. Liebling<br />
Nora Ephron<br />
The Japanese dude who won the hot dog eating contest<br />
John Belushi</p>
<p>&#8211; Magicians:<br />
Harry Houdini (1)<br />
George H. W. Bush (2)<br />
Penn and Teller (3)<br />
Steve Martin (4)<br />
David Blaine<br />
Eric Antoine<br />
Martin Gardner<br />
Ira Glass</p>
<p>&#8211; Mathematicians:<br />
Carl Friedrich Gauss (1)<br />
Pierre-Simon Laplace (2)<br />
John von Neumann (3)<br />
Alan Turing (4)<br />
Leonhard Euler<br />
Paul Erdos<br />
Stanislaw Ulam<br />
Benoit Mandelbrot</p>
<p>&#8211; TV personalities:<br />
Oprah Winfrey (1)<br />
Johnny Carson (2)<br />
Ed Sullivan (3)<br />
Carol Burnett (4)<br />
Sid Caesar<br />
David Letterman<br />
Ellen DeGeneres<br />
John Oliver</p>
<p>&#8211; People from New Jersey:<br />
Bruce Springsteen (1)<br />
Chris Christie (2)<br />
Frank Sinatra (3)<br />
Philip Roth (4)<br />
William Carlos Williams<br />
Virginia Apgar<br />
Meryl Streep<br />
Joe Pesci</p>
<p>&#8211; GOATs:<br />
Jim Thorpe (1)<br />
Babe Didrikson Zaharias (2)<br />
LeBron James (3)<br />
Bobby Fischer (4)<br />
Serena Williams<br />
Pele<br />
Simone Biles<br />
Lance Armstrong</p>
<p>&#8211; People whose names end in f:<br />
Riad Sattouf (1)<br />
Ian McKellen (2)<br />
Boris Karloff (3)<br />
Darrell Huff (4)<br />
Yakov Smirnoff<br />
DJ Jazzy Jeff<br />
Adam Schiff<br />
Anastasia Romanoff</p>
<p>Alan Turing was included in both competitions; that was an oversight.  The matchups started with <a href="https://statmodeling.stat.columbia.edu/2019/01/07/the-seminar-speaker-contest-begins-jim-thorpe-1-vs-john-oliver/">Jim Thorpe vs. John Oliver</a>, which was won by <a href="https://statmodeling.stat.columbia.edu/2019/01/08/philip-roth-4-vs-dj-jazzy-jeff-jim-thorpe-advances/">Jim</a>:</p>
<blockquote><p>We got a couple arguments in Oliver’s favor&#8212;we’d get to hear him say “Whot?”, and he’s English&#8212;but for Thorpe we heard a lot more, including his uniqueness as greatest athlete of all time, and that we could save money on the helmet if that were required. We also got the following bad reason: “the chance to hear him say, ‘I’ve been asked to advise those of you who are following this talk on social media, whatever that means, to use “octothorpe talktothorpe.”‘” Even that bad reason ain’t so bad, also it’s got 3 levels of quotation nesting, which counts for something right there. What iced it for Thorpe was this comment from Tom: “Seeing as he could do everything better than everyone else, just by giving it a go, he would surely give an incredible seminar.”</p></blockquote>
<p>And it concluded with <a href="https://statmodeling.stat.columbia.edu/2019/03/18/its-the-finals-the-japanese-dude-who-won-the-hot-dog-eating-contest-vs-riad-sattouf/">the finals</a>, pitting the Japanese dude who won the hot dog eating contest against Riad Sattouf.  My favorite match, though, was <a href="https://statmodeling.stat.columbia.edu/2019/01/21/frank-sinatra-3-vs-virginia-apgar-julia-child-advances/">Virginia Apgar vs. Frank Sinatra</a>, a battle of two Jerseyites. I looked up Apgar on wikipedia and learned that she came from a musical family! Meanwhile, Frank Sinatra had friends who put a lot of people in the hospital. So lots of overlap here.</p>
<p><strong>Third competition</strong></p>
<p>Who would be the ultimate seminar speaker? We&#8217;re not asking for the most popular speaker, or the most relevant, or the best speaker, or the deepest, or even the coolest, but rather some combination of the above.</p>
<p>Our new bracket of 64 includes eight current or historical figures from each of the following eight categories:</p>
<p>&#8211; Duplicate names<br />
&#8211; Alleged tax cheats<br />
&#8211; People known by initials<br />
&#8211; Cool people<br />
&#8211; Namesakes<br />
&#8211; Children&#8217;s book authors<br />
&#8211; Creators of laws or rules<br />
&#8211; Traitors</p>
<p><strong>The rules</strong></p>
<p>We’ll post one matchup each day at noon, starting in a few weeks.</p>
<p>Once each pairing is up, all of you are encouraged to comment. We’ll announce the results when posting the next day’s matchup.</p>
<p>We’ll decide each day’s winner not based on a popular vote but based on the strength and amusingness of the arguments given by advocates on both sides. So give it your best!</p>
<p>As with our previous contests, we’ll continue the regular flow of statistical modeling, causal inference, and social science posts. They’ll alternate with these matchup postings.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/18/hey-its-time-for-another-greatest-seminar-speaker-contest/feed/</wfw:commentRss>
			<slash:comments>12</slash:comments>
		
		
			</item>
		<item>
		<title>Cheating in sports vs. cheating in journalism vs. cheating in science</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/17/cheating-in-sports-vs-cheating-in-journalism-vs-cheating-in-science/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/17/cheating-in-sports-vs-cheating-in-journalism-vs-cheating-in-science/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Mon, 17 Oct 2022 13:27:01 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Sports]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48066</guid>

					<description><![CDATA[Sports cheating has been in the news lately. Nothing about the Astros, but the chess-cheating scandal that people keep talking about&#8212;or, at least, people keep sending me emails asking me to blog about it&#8212;and the cheating scandals in poker and &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/17/cheating-in-sports-vs-cheating-in-journalism-vs-cheating-in-science/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Sports cheating has been in the news lately.  Nothing about the Astros, but the chess-cheating scandal that people <a href="https://defector.com/hans-niemann-remains-defiant/">keep talking about</a>&#8212;or, at least, people keep sending me emails asking me to blog about it&#8212;and the cheating scandals in <a href="https://defector.com/historically-strange-poker-hand-leads-to-dramatic-cheating-allegation/">poker</a> and <a href="https://defector.com/pro-fishing-has-its-own-cheating-scandal-involving-weights-in-prize-winning-fish/">fishing</a>.  All of this, though, is nothing compared to the juiced elephant in the room:  the drug-assisted home run totals of 1998-2001, which kept coming up during the past few months as Aaron Judge approached and then eventually reached the record-breaking total of 62 home runs during a season.</p>
<p>On this blog we haven&#8217;t talked much about cheating in sports (there was <a href="https://statmodeling.stat.columbia.edu/2020/12/24/dream-investigation-results-official-report-by-the-minecraft-speedrunning-team/">this post</a>, though, and also something a few years back about one of those runners who wasn&#8217;t really finishing the races), but we&#8217;ve occasionally talked about cheating in journalism (for example <a href="https://statmodeling.stat.columbia.edu/2019/09/15/boston-globe-columnist-suspended-investigation-marathon-bombing-stories-dont-add/">here</a>, <a href="https://statmodeling.stat.columbia.edu/2016/10/17/should/">here</a>, <a href="https://statmodeling.stat.columbia.edu/2013/11/12/plaig-3/">here</a>, and <a href="https://statmodeling.stat.columbia.edu/2015/08/03/the-plagiarist-next-door-strikes-back-different-standards-of-plagiarism-in-different-communities/">here</a>&#8212;hey, those last two are about cheating in journalism about chess!), and we&#8217;ve talked lots and lots about cheating in science.</p>
<p>So this got me thinking:  What are the differences between cheating in sports, journalism, and science?</p>
<p><strong>1.  The biggest difference that I see is that in sports, when you cheat, you&#8217;re actually doing what you claim to do, you&#8217;re just doing it using an unauthorized method.  With cheating in journalism and science, typically you&#8217;re not doing what you claimed.</strong></p>
<p>Let me put it this way:  Barry Bonds may have juiced, but he really did hit 7 zillion home runs.  Lance Armstrong doped, but he really did pedal around France faster than anyone else.  Jose Altuve really did hit the ball out of the park.  Stockfish-aided or not, that dude really did checkmate the other dude&#8217;s king.  Etc.  The only cases I can think of, where the cheaters didn&#8217;t actually do what they claimed to do, are the Minecraft guy, Rosie Ruiz, and those guys who did a &#8220;Mark Twain&#8221; on their fish.  Usually, what sports cheaters do is use unapproved methods to achieve real ends.</p>
<p>But when journalism cheaters cheat, the usual way they do it is by making stuff up.  That is, they put things in the newspaper that didn&#8217;t really happen.  The problem with Stephen Glass or Janet Cooke or Jonah Lehrer is not that they achieved drug-enhanced scoops or even that they broke some laws in order to break some stories.  No, the problem was that they reported things that weren&#8217;t true.  I&#8217;m not saying that journalism cheats are worse than sports cheats, just that it&#8217;s a different thing.  Sometimes cheating writers cheat by copying others&#8217; work without attribution, and that alone doesn&#8217;t necessarily lead to falsehoods getting published, but it often does, which makes sense:  once you start copying without attribution, it becomes harder for readers to <a href="http://www.stat.columbia.edu/~gelman/research/published/GelmanBasbollAmericanScientist.pdf">track down your sources</a> and find your errors, which in turn makes it easier to be sloppy and reduces the incentives for accuracy.</p>
<p>When scientists cheat, sometimes it&#8217;s by just making things up, or presenting claims with no empirical support&#8212;for example, there&#8217;s no evidence that the Irrationality guy ever had that <a href="https://statmodeling.stat.columbia.edu/2022/01/06/shreddergate-and-an-idea-for-a-museum-of-scholarly-misconduct/">custom-made shredder</a>, or that the Pizzagate guy ever really ran a &#8220;masterpiece&#8221; of an experiment with a <a href="https://statmodeling.stat.columbia.edu/2019/08/20/did-that-bottomless-soup-bowl-experiment-ever-happen/">bottomless soup bowl</a> or had people lift an 80-pound rock, or that Mary Rosh ever did that survey.  Other times they just say things that aren&#8217;t true, for example describing a 3-day study as <a href="https://statmodeling.stat.columbia.edu/2018/04/06/important-distinction-truth-evidence/">&#8220;long-term&#8221;</a>.  In that latter case you might say that the scientist in question is just an idiot, not a cheater&#8212;but, ultimately, I do think it&#8217;s a form of cheating to publish a scientific paper with a title that doesn&#8217;t describe its contents.</p>
<p>But I think the main why scientists cheat is by being loose enough with their reasoning that they can make strong claims that aren&#8217;t supported by the data.  Is this &#8220;cheating,&#8221; exactly?  I&#8217;m not sure.  Take something like that ESP guy or the beauty-and-sex-ratio guy who manage to find statistical methods that give them the answers they want.  At some level, the boundary between incompetence and cheating doesn&#8217;t really matter; recall <a href="https://statmodeling.stat.columbia.edu/2009/05/24/handy_statistic/">Clarke&#8217;s Law</a>.</p>
<p>The real point here, though, is that, whatever you want to call it, the problem with bad science is that it comes up with false or unsupported claims.  To put it another way:  it&#8217;s not that Mark Hauser or whoever is taking some drugs that allow him to make a discovery that nobody else could make; the problem is that he&#8217;s claiming something&#8217;s a discovery but it isn&#8217;t.  To put it yet another way:  there is no perpetual motion machine.</p>
<p>The scientific analogy to sports cheating would be something like . . . Scientist B breaks into Scientist A&#8217;s lab, steals his compounds, and uses them to make a big discovery.  Or, Scientist X cuts corners by using some forbidden technique, for example violating some rule regarding safe disposal of chemical waste, and this allows him to work faster and make some discovery.  But I don&#8217;t get a sense that this happens much, or at least I don&#8217;t really hear about it.  There was the Robert Gallo story, but even there the outcome was not a new discovery, it was just a matter of credit.</p>
<p>And the journalistic analogy to sports cheating would be something like that hacked phone scandal in Britain a few years back . . . OK, I guess that does happen sometimes.  But my guess is that the kinds of journalists who&#8217;d hack phones are also the kind of journalists who&#8217;d make stuff up or suppress key parts of a story or otherwise manipulate evidence in a way to mislead.  In which case, again, they can end up publishing something that didn&#8217;t happen, or polluting the <a href="https://statmodeling.stat.columbia.edu/2022/01/07/pnas-gigo-qrp-wtf-approaching-the-platonic-ideal-of-junk-science/">scientific</a> and <a href="https://statmodeling.stat.columbia.edu/2019/08/15/replication-police-methodological-terrorism-stasi-nudge-shoot-the-messenger-wtf/">popular</a> literature.</p>
<p><strong>2.  Another difference is that sports have a more clearly-defined goal than journalism or science.</strong>  An extreme example is bicycle racing:  if the top cyclists are doping and you want to compete on their level, then you have to dope also; there&#8217;s simply no other option.  But in journalism, no matter how successful Mike Barnicle was, other journalists didn&#8217;t have to fabricate to keep up with him.  There are enough true stories to report, that honest journalists can compete.  Yes, restricting yourself to the truth can put you at a disadvantage, but it doesn&#8217;t crowd you out entirely.  Similarly, if you&#8217;re a social scientist who&#8217;s not willing to fabricate surveys or report hyped-up conclusions based on forking paths, yes, your job is a bit harder, but you can still survive in the publication jungle.  There are enough paths to success that cheating is not a necessity, even if it&#8217;s a viable option.</p>
<p><strong>3.  The main similarity I see among misbehavior in sports, journalism, and science is that the boundary between cheating and legitimate behavior is blurry.</strong>  When &#8220;everybody does it,&#8221; is it cheating?  With science there&#8217;s also the unclear distinction between cheating and simple incompetence&#8212;with the twist that incompetence at scientific reasoning could represent a sort of super-competence at scientific self-promotion.  Only a fool would say that the replication rate in psychology is &#8220;<a href="https://statmodeling.stat.columbia.edu/2016/03/09/bruised-and-battered-i-couldnt-tell-what-i-felt-i-was-ungeneralizable-to-myself/">statistically indistinguishable from 100%</a>&#8220;&#8212;but being that sort of fool can be a step toward success in our Ted/Freakonomics/NPR media environment.  You&#8217;d think that professional athletes would be more aware of what drugs they put in their bodies than scientists would be aware of what numbers they put into their t-tests, but sports figures have sometimes claimed that they took banned drugs without their knowledge.  The point is that a lot is happening at once, and there are people who will do what it takes to win.</p>
<p><strong>4.</strong>  Finally, it can be entertaining to talk about cheating in science, but <a href="https://statmodeling.stat.columbia.edu/2022/06/06/should-we-spend-so-much-time-talking-about-cheaters-and-fraudsters/">as I&#8217;ve said before</a>, I think the much bigger problem is scientists who are not trying to cheat but are just using bad methods with noisy data. Indeed, the focus on cheaters can let incompetent but sincere scientists off the hook. Recall our discussion from a few years ago, The flashy crooks get the headlines, but the bigger problem is everyday routine bad science done by non-crooks.  <a href="https://statmodeling.stat.columbia.edu/2020/07/29/the-crooks-get-the-headlines-but-the-real-problem-is-bad-science-done-by-non-crooks/">The flashy crooks get the headlines, but the bigger problem is everyday routine bad science done by non-crooks.</a>  Similarly, with journalism, I&#8217;d say the bigger problem is not the fabricators so much as the everyday corruption of slanted journalism, and <a href="https://statmodeling.stat.columbia.edu/2022/05/22/what-happen-to-that-los-angeles-tunnel-that-axios-was-saying-was-coming-soon-in-2018/">public relations presented in journalistic form</a>.  To me, the biggest concern with journalistic cheating is not so much the cases of fraud as much as when the establishment closes ranks to <a href="https://statmodeling.stat.columbia.edu/2014/11/22/blogs-twitter/">defend</a> the fraudster, just as in academia there&#8217;s no real mechanism to do anything about bad science.</p>
<p>Cheating in sports feels different, maybe in part because a sport is <em>defined</em> by its rules in a way that we would not say about journalism or science.</p>
<p><strong>P.S.</strong>  After posting the above, I got to thinking about cheating in business, politics, and war, which seem to me to have a different flavor than cheating in sports, journalism, or science.  I have personal experience in sports, journalism, and science, but little to no experience in business, politics, and war.  So I&#8217;m just speculating, but here goes:<br />
To me, what&#8217;s characteristic about cheating in business, politics, and war is that some flexible line is pushed to the breaking point.  For example, losing candidates will often try to sow doubt about the legitimacy of an election, but they rarely take it to the next level and get on the phone with election officials and demand they add votes to their total.  Similarly with business cheating such as creative accounting, dumping of waste, etc.:  it&#8217;s standard practice to work at the edge of what&#8217;s acceptable, but cheaters such as the Theranos gang go beyond hype to flat-out lying.  Same thing for war crimes:  there&#8217;s no sharp line, and cheating or violation arises when armies go far beyond what is currently considered standard behavior.  This all seems different than cheating in sports, journalism, or science, all of which are more clearly defined relative to objective truth.</p>
<p>I think there&#8217;s more to be said on all this.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/17/cheating-in-sports-vs-cheating-in-journalism-vs-cheating-in-science/feed/</wfw:commentRss>
			<slash:comments>30</slash:comments>
		
		
			</item>
		<item>
		<title>“MIT Built a Theranos for Plants” update</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-theranos-for-plants-update/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-theranos-for-plants-update/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sun, 16 Oct 2022 13:31:46 +0000</pubDate>
				<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47933</guid>

					<description><![CDATA[Remember this from a couple years ago? This news article by Tom McKay is hilarious: The prestigious multidisciplinary MIT Media Lab built a “personal food computer” that worked so poorly that demos had to be faked Theranos-style . . . &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-theranos-for-plants-update/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><a href="https://statmodeling.stat.columbia.edu/2020/08/21/mit-built-a-theranos-for-plants/">Remember this</a> from a couple years ago?</p>
<blockquote><p><a href="https://gizmodo.com/mit-built-a-theranos-for-plants-1837968240">This news article</a> by Tom McKay is hilarious:</p>
<blockquote><p>The prestigious multidisciplinary MIT Media Lab built a “personal food computer” that worked so poorly that demos had to be faked Theranos-style . . . According to Business Insider, the project—a plastic hydroponic grow box filled with “advanced sensors and LED lights” that would supposedly make it possible to replicate crop conditions from any part of the global—was a sham, with MIT’s Open Agriculture Initiative director Caleb Harper resorting to faking demos . . . According to Business Insider, Harper directed an email requesting comment to an MIT spokesperson, who “didn’t provide a comment.”</p></blockquote>
<p>No comment, huh?  The &#8220;Media Lab&#8221; should be able to do better than that, no?  If that&#8217;s all they can do, they could just as well hire <a href="https://statmodeling.stat.columbia.edu/2017/04/12/pizzagate-update-response-cornell-university-media-relations-office/">the media team from Cornell</a>.  You&#8217;re the goddam media lab, dudes:  you should be able to work with the media, no?</p>
<p>Ahhh, MIT . . . <a href="https://statmodeling.stat.columbia.edu/2014/04/04/going-criticize-blog-im-just-tired-things-like-whats-really-horrible-news-article-takes-seriously-problem/">what&#8217;s happened</a> to you?  Lots has changed since 1986, I guess.</p></blockquote>
<p>Palko shares <a href="https://twitter.com/SeiffertDon/status/1557755379756449796">this update</a> from journalist Don Seiffert, who writes:</p>
<blockquote><p>More than two years ago, MIT quietly shut down one of its most high-profile research projects after allegations that the science being touted didn&#8217;t work. The school still hasn&#8217;t addressed the allegations, nor its oversight of the project, and now a new lawsuit alleges that school administrators were told of the problems more than a year before media reports on the Open Agriculture Initiative came to light. The lawsuit alleges that MIT used the project to burnish its reputation as one of the world&#8217;s top research organizations and to raise money. While the allegations of misrepresentation of research at OpenAg are not new, the episode was largely overshadowed by the Jeffrey Epstein scandal which occurred at the same time. . . .</p></blockquote>
<p>Full story from Seiffert <a href="https://archive.ph/Ji6pJ">here</a>, including these juicy bits:</p>
<blockquote><p>The lawsuit alleges that MIT ended [a former researcher&#8217;s] appointment at MIT “for complaining about and reporting research fraud and fraudulent fundraising activities,” and further claims that the university “knew at all times” that the central innovation of that research, called the food computer, “simply did not operate as it claimed.” . . .</p>
<p>In a press release on April 3, 2019, the school listed OpenAg research funders, including Target Corp., Lee Kum Kee Health Products Group, Welspun, Sentient Technologies, and Cognizant Technology Solutions. . . . The director of the program, Caleb Harper, was invited by MIT President Rafael Reif in May 2016 “to participate in his Campaign for a Better World in New York and London, which sought billions of dollars of unrestricted funds for MIT,” according to the lawsuit . . . MIT has never publicly addressed the allegations of problems with its own internal controls or its oversight of the research, nor whether it has taken steps to ensure similar alleged exaggerations of research aren’t repeated in future projects. . . . </p></blockquote>
<p>Hey!  Are they using <a href="https://statmodeling.stat.columbia.edu/2022/06/23/a-garland-of-retractions-for-the-ohio-state-department-of-chutzpah-cancer-biology-and-genetics/">Ohio State</a> as a model??  On the plus side, their <a href="https://statmodeling.stat.columbia.edu/2022/07/09/columbia-loses-its-no-2-spot-in-the-u-s-news-rankings/">U.S. News ranking data</a> probably aren&#8217;t being fudged, right?</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/16/mit-built-a-theranos-for-plants-update/feed/</wfw:commentRss>
			<slash:comments>17</slash:comments>
		
		
			</item>
		<item>
		<title>How should Bayesian methods for the social sciences differ from Bayesian methods for the physical sciences? (my talk at the Bayesian Methods for the Social Sciences workshop, 21 Oct 2022, Paris)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/15/how-should-bayesian-methods-for-the-social-sciences-differ-from-bayesian-methods-for-the-physical-sciences-my-talk-at-the-bayesian-methods-for-the-social-sciences-workshop-21-oct-2022-paris/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/15/how-should-bayesian-methods-for-the-social-sciences-differ-from-bayesian-methods-for-the-physical-sciences-my-talk-at-the-bayesian-methods-for-the-social-sciences-workshop-21-oct-2022-paris/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sat, 15 Oct 2022 13:08:51 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Economics]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47854</guid>

					<description><![CDATA[The title of the workshop is Bayesian Methods for the Social Sciences, so it makes sense to ask what&#8217;s so special about the social sciences. At first I was thinking of comparing the social sciences to the natural sciences, but &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/15/how-should-bayesian-methods-for-the-social-sciences-differ-from-bayesian-methods-for-the-physical-sciences-my-talk-at-the-bayesian-methods-for-the-social-sciences-workshop-21-oct-2022-paris/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>The title of the workshop is Bayesian Methods for the Social Sciences, so it makes sense to ask what&#8217;s so special about the social sciences.</p>
<p>At first I was thinking of comparing the social sciences to the natural sciences, but I think that social sciences such as political science, economics, and sociology have a lot in common with biological sciences and medicine, along with in-between sciences such as psychology:  all of these are characterized by high variability, measurement challenges, and difficulty in matching scientific theories to mathematical or statistical models.</p>
<p>So I decided that the more interesting comparison is to the <em>physical sciences</em>, where variability tends to be lower, or at least better behaved (for example, shot noise or Brownian motion).  In the social sciences, statistical models&#8212;Bayesian or otherwise&#8212;have a lot more subjectivity, a lot more researcher degrees of freedom.  In theory, Bayesian inference should work for any problem, but it has a different flavor when our models can be way off and there can be big gaps between actual measurements and the goals of measurements.</p>
<p>So I think there will be lots to say on this topic.  I&#8217;m hoping the conf will be in French so that I&#8217;m forced to speak slowly.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/15/how-should-bayesian-methods-for-the-social-sciences-differ-from-bayesian-methods-for-the-physical-sciences-my-talk-at-the-bayesian-methods-for-the-social-sciences-workshop-21-oct-2022-paris/feed/</wfw:commentRss>
			<slash:comments>10</slash:comments>
		
		
			</item>
		<item>
		<title>Hey!  Here&#8217;s a surprisingly fascinating discussion of copyright of transformed images!</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/14/hey-heres-a-surprisingly-fascinating-discussion-of-copyright-of-transformed-images/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/14/hey-heres-a-surprisingly-fascinating-discussion-of-copyright-of-transformed-images/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Fri, 14 Oct 2022 13:13:02 +0000</pubDate>
				<category><![CDATA[Art]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48088</guid>

					<description><![CDATA[I don&#8217;t usually just link to stuff on our blogroll&#8212;after all, you can just go there and check things out whenever you want&#8212;but this post by Chris Gavaler was really interesting so I wanted to share it with you. It &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/14/hey-heres-a-surprisingly-fascinating-discussion-of-copyright-of-transformed-images/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I don&#8217;t usually just link to stuff on our blogroll&#8212;after all, you can just <a href="https://statmodeling.stat.columbia.edu/blogs-i-read/">go there</a> and check things out whenever you want&#8212;but <a href="https://thepatronsaintofsuperheroes.wordpress.com/2022/10/10/scotus-meaningfully-transformative-v-recognizably-derivative/">this post by Chris Gavaler</a> was really interesting so I wanted to share it with you.</p>
<p>It features:</p>
<p>Lynne Goldsmith<br />
Prince<br />
Andy Warhol<br />
the U.S. Supreme Court<br />
Marilyn Monroe<br />
Eugene Korman<br />
somebody named Soglin<br />
Barack Obama<br />
Mannie Garcia<br />
Shepard Fairey<br />
Miles Davis<br />
Jay Maisel<br />
Andy Baio<br />
swipers in comics<br />
Pablo Picasso<br />
Angela Corey<br />
Rick Wilson<br />
George Zimmerman</p>
<p>Interesting and thoughtful, with lots of pictures.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/14/hey-heres-a-surprisingly-fascinating-discussion-of-copyright-of-transformed-images/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
		<item>
		<title>How did Bill James get this one wrong on regression to the mean?  Here are 6 reasons:</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/13/bill-james-vs-kahneman-and-tversky/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/13/bill-james-vs-kahneman-and-tversky/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 13 Oct 2022 13:46:58 +0000</pubDate>
				<category><![CDATA[Causal Inference]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Sports]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47423</guid>

					<description><![CDATA[I&#8217;m a big fan of Bill James, but I think he might be picking up the wrong end of the stick here. The great baseball analyst writes about what he calls the Law of Competitive Balance. His starting point is &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/13/bill-james-vs-kahneman-and-tversky/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I&#8217;m a <a href="https://www.baseballprospectus.com/news/article/13810/baseball-proguestus-a-statistician-rereads-bill-james/">big fan</a> of Bill James, but I think he might be picking up the wrong end of the stick here.</p>
<p>The great baseball analyst <a href="https://web.archive.org/web/20220426120747/https://www.billjamesonline.com/the_law_of_competitive_balance_redux/">writes about</a> what he calls the Law of Competitive Balance.  His starting point is that teams that are behind are motivated to work harder to have a chance of winning, which moves them to switch to high-variance strategies such as long passes in football (more likely to score a touchdown, also more likely to get intercepted), etc.  Here&#8217;s Bill James:</p>
<blockquote><p>Why was there an increased chance of a touchdown being thrown? </p>
<p>Because the team was behind.</p>
<p>Because the team was behind, they had an increased NEED to score.</p>
<p>Because they had an increased need to score points, they scored more points. </p>
<p>That is one of three key drivers of The Law of Competitive Balance:  that success increases when there is an increased need for success.   This applies not merely in sports, but in every area of life.  But in the sports arena, it implies that the sports universe is asymmetrical. . . .</p>
<p>Because this is true, the team which has the larger need is more willing to take chances, thus more likely to score points.  The team which is ahead gets conservative, predictable, limited.  This moves the odds.   The team which, based on their position, would have a 90% chance to win doesn’t actually have a 90% chance to win.  They may have an 80% chance to win; they may have an 88% chance to win, they may have an 89.9% chance to win, but not 90.</p></blockquote>
<p>I think he&#8217;s mixing a correct point here with an incorrect point.</p>
<p>James&#8217;s true statement is that, as he puts it, &#8220;there is an imbalance in the motivation of the two teams, an imbalance in their willingness to take risks.&#8221;  The team that&#8217;s behind is motivated to switch to strategies that increase the variance of the score differential, even at the expense of lowering its expected score differential.  Meanwhile, the team that&#8217;s ahead is motivated to switch to strategies that decrease the variance of the score differential, even at the expense of lowering <em>its</em> expected score differential.  In basketball, it can be as simple as the team that&#8217;s behind pushing up the pace and the team that&#8217;s ahead slowing things down.  The team that&#8217;s trailing is trying to have some chance of catching up&#8212;their goal is to win, not to lose by a smaller margin; conversely, the team that&#8217;s in the lead is trying to minimize the chance of the score differential going to zero, not to run up the score.  As James says, these patterns are averages and won&#8217;t occur from play to play.  Even if you&#8217;re behind by 10 in a basketball game with 3 minutes to play, you&#8217;ll still take the open layup rather than force the 3-pointer, and even if you&#8217;re ahead by 10, you&#8217;ll still take the open shot with 20 seconds left on the shot clock rather than purely trying to eat up time.  But on average the logic of the game leads to different strategies for the leading and trailing teams, and that will have consequences on the scoreboard.</p>
<p>James&#8217;s mistake is to think, when this comes to probability of winning, that this dynamic on balance favors the team that&#8217;s behind.  When strategies are flexible, the team that&#8217;s behind does not necessarily increase its probability of winning relative to what that probability would be if team strategies were constant.  Yes, the team that&#8217;s behind will use strategies to increase the probability of winning, but the team that&#8217;s ahead will alter its strategy too.  Speeding up the pace of play should, on average, increase the probability of winning for the trailing team (for example, increasing the probability from, I dunno, 10% to 15%), but meanwhile the team that&#8217;s ahead is slowing down the pace of play, which should send that probability back down.  On net, will this favor the leading team or the trailing team when it comes to win probability?  It will depend on the game situation.  In some settings (for example, a football game where the team that&#8217;s ahead has the ball on first down with a minute left), it will favor the team that&#8217;s ahead.  In other settings it will go the other way.</p>
<p>James continues:</p>
<blockquote><p>That is one of the three key drivers of the Law of Competitive Balance.  The others, of course, are adjustments and effort.  When you’re losing, it is easier to see what you are doing wrong.  Of course a good coach can recognize flaws in their plan of attack even when they are winning, but when you’re losing, they beat you over the head.</p></blockquote>
<p>I don&#8217;t know about that.  As a coach myself, I could just as well argue the opposite point, as follows.  When you&#8217;re winning, you can see what works while having the freedom to experiment and adapt to fix what doesn&#8217;t work.  But when you&#8217;re losing, it can be hard to know where to start or have a sense of what to do to improve.</p>
<p>Later on in his post, James mentions that, when you&#8217;re winning, part of that will be due to situational factors that won&#8217;t necessarily repeat.  The quick way to say that is that, when you&#8217;re winning, part of your success is likely to be from &#8220;luck&#8221;; a formulation that I&#8217;m OK with as long as we take this term generally enough to refer to factors that don&#8217;t necessarily repeat, such as pitcher/batter matchups, to take an example from James&#8217;s post.</p>
<p>But James doesn&#8217;t integrate this insight into his understanding of the law of competitive balance.  Instead, he writes:</p>
<blockquote><p>If a baseball team is 20 games over .500 one year, they tend to be 10 games over the next.   If a team is 20 games under .500 one year, they tend to be 10 games under the next year.  If a team improves by 20 games in one year (even from 61-101 to 81-81) they tend to fall back by 10 games the next season.   If they DECLINE by 10 games in a year, they tend to improve by 5 games the next season. </p>
<p>I began to notice similar patterns all over the map.   If a batter hits .250 one year and .300 the next, he tends to hit about .275 the third year.  Although I have not demonstrated that similar things happen in other sports, I have no doubt that they do.   I began to wonder if this was actually the same thing happening, but in a different guise.  You get behind, you make adjustments.  You lose 100 games, you make adjustments.  You get busy.  You work harder.  You take more chances.  You win 100 games, you relax.  You stand pat.</p></blockquote>
<p>James&#8217;s description of the data is fine; his mistake is to attribute these changes to teams &#8220;making adjustments&#8221; or &#8220;standing pat.&#8221;  That could be, but it could also be that teams that win 100 games &#8220;push harder&#8221; and that teams that lose 100 games &#8220;give up.&#8221;  The real point is statistical, which is that this sort of &#8220;regression to the mean&#8221; will happen without any such adjustment effects, just from &#8220;luck&#8221; or &#8220;random variation&#8221; or varying situational factors.</p>
<p>Here&#8217;s a famous example from Tversky and Kahneman (1973):</p>
<blockquote><p>The instructors in a flight school adopted a policy of consistent positive reinforcement recommended by psychologists. They verbally reinforced each successful execution of a flight maneuver. After some experience with this training approach, the instructors claimed that contrary to psychological doctrine, high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try.</p></blockquote>
<p>Actually, though:</p>
<blockquote><p>Regression is inevitable in flight maneuvers because performance is not perfectly reliable and progress between successive maneuvers is slow. Hence, pilots who did exceptionally well on one trial are likely to deteriorate on the next, regardless of the instructors&#8217; reaction to the initial success. The experienced flight instructors actually discovered the regression but attributed it to the detrimental effect of positive reinforcement.</p></blockquote>
<p>&#8220;Performance is not perfectly reliable and progress between successive maneuvers is slow&#8221;:  That describes pro sports!</p>
<p>As we write in Regression and Other Stories, the point here is that a <em>quantitative</em> understanding of prediction clarifies a fundamental <em>qualitative</em> confusion about variation and causality.  From purely mathematical considerations, it is expected that the best pilots will decline, relative to the others, while the worst will improve in their rankings, in the same way that we expect daughters of tall mothers to be, on average, tall but not quite as tall as their mothers, and so on.</p>
<p>I was surprised to see Bill James make this mistake.  All the years I&#8217;ve read him writing about the law of competitive balance and the plexiglass principle, I always assumed that he&#8217;d understood this as an inevitable statistical consequence of variation without needing to try to attribute it to poorly-performing teams trying harder etc.</p>
<p><strong>How did he get this one so wrong?  Here are 6 reasons.</strong></p>
<p>This raises a new question, which is how could such a savvy analyst make such a basic mistake?  I have six answers:</p>
<p>1.  Multiplicity.  Statistics is <a href="https://statmodeling.stat.columbia.edu/2016/03/11/statistics-is-like-basketball-or-knitting/">hard</a>, and if you do enough statistics, you&#8217;ll eventually make some mistakes.  I make mistakes too!  It just happens that this &#8220;regression to the mean&#8221; fallacy is a mistake that James made.</p>
<p>2.  It&#8217;s a <em>basic</em> mistake and an <em>important</em> mistake, but it&#8217;s not a <em>trivial</em> mistake.  Regression to the mean is a notoriously difficult topic to teach (you can cruise over to chapter 6 of <a href="http://www.stat.columbia.edu/~gelman/regression/">our book</a> and see how we do; maybe not so great!).</p>
<p>3.  Statistics textbooks, including my own, are full of boring details, so I can see that, whether or not Bill James has read any such books, he wouldn&#8217;t get so much out of them.</p>
<p>4.  In his attribution of regression to the mean, James is making an error of <em>causal reasoning</em> and a <em>modeling</em> error, but it&#8217;s not a <em>prediction</em> error.  The law of competitive balance and the plexiglass principle give valid predictions, and they represent insights that were not widely available in baseball (and many other fields) before James came along.  Conceptual errors aside, James was still moving the ball forward, as it were.  When he goes beyond prediction in his post, for example making strategy recommendations, I&#8217;m doubtful, but I&#8217;m guessing that the main influence on readers of his &#8220;law of competitive balance&#8221; is to the predictive part.</p>
<p>5.  Hero worship.  The man is a living legend.  That&#8217;s great&#8212;he deserves all his fame&#8212;but the drawback is that maybe it&#8217;s a bit too easy for him to fall for his own hype and not question himself or fully hear criticism.  We&#8217;ve seen the same thing happen with baseball and political analyst Nate Silver, who continues to do excellent work but sometimes can&#8217;t seem to digest feedback from outsiders.</p>
<p>6.  Related to point 5 is that James made his breakthroughs by fighting the establishment.  For many decades he&#8217;s been saying outrageous things and standing up for his outrageous claims even when they&#8217;ve been opposed by experts in the field.  So he keeps doing it, which in some ways is great but can also lead him astray, by trusting his intuitions too much and not leaving himself more open for feedback.</p>
<p>I guess we could say that, in sabermetrics, James was on a winning streak for a long time so he relaxes. He stands pat.  He has less motivation to see what&#8217;s going wrong.</p>
<p><strong>P.S.</strong>  Again, I&#8217;m a big fan of Bill James.  It&#8217;s interesting when smart people make mistakes.  When dumb people make mistakes, that&#8217;s boring.  When someone who&#8217;s thought so much about statistics makes such a basic statistical error, that&#8217;s interesting to me.  And, as noted in item 4 above, I can see how James could have this misconception for decades without it having much negative effect on his work.</p>
<p><strong>P.P.S.</strong>  Just to clarify:  Bill James made two statements.  The first was predictive and correct; the second was causal and misinformed.</p>
<p>His first, correct statement is that there is &#8220;regression to the mean&#8221; or &#8220;competitive balance&#8221; or &#8220;plexiglas&#8221; or whatever you want to call it:  players or teams that do well in time 1 tend to decline in time 2, and players or teams that do poorly in time 2 tend to improve in time 2.  This statement, or principle, is correct, and it can be understood as a general mathematical or statistical pattern that arises when correlations are less than 100%.  This pattern is not always true&#8212;it depends on the joint distribution of the before and after measurements (<a href="https://statmodeling.stat.columbia.edu/2022/10/13/bill-james-vs-kahneman-and-tversky/#comment-2104254">see here</a>) but it is typically the case.</p>
<p>His second, misinformed statement is that this is caused by players or teams that are behind at time 1 being more innovative or trying harder and players or teams that are ahead at time 2 being complacent or standing pat.  This statement is misinformed because the descriptive phenomenon of regression-to-the-mean or competitive-balance or plexiglas will happen even in the absence of any behavioral changes.  And, as discussed in the above post, behavioral changes can go in either direction; there&#8217;s no good reason to think that, when both teams perform strategic adjustments, that these adjustments on net will benefit the team that&#8217;s behind.</p>
<p>This is all difficult because it is natural to observe the first, correct predictive statement and from this to mistakenly infer the second, misinformed causal statement.  Indeed this inference is such a common error that it is a major topic in statistics and is typically covered in introductory texts. We devote a whole chapter to it in Regression and Other Stories, and if you&#8217;re interested in understanding this I recommend you read chapter 6; the book is <a href="http://www.stat.columbia.edu/~gelman/regression/">freely available online</a>.</p>
<p>For the reasons discussed above, I&#8217;m not shocked that Bill James made this error:  for his purposes, the predictive observation has been more important than the erroneous causal inference, and he figured this stuff out on his own, without the benefit or hindrance of textbooks, and given his past success as a rebel, I can see how it can be hard for him to accept when outsiders point out a subtle mistake. But, as noted in the P.S. above, when smart people get things wrong, it&#8217;s interesting; hence this post.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/13/bill-james-vs-kahneman-and-tversky/feed/</wfw:commentRss>
			<slash:comments>52</slash:comments>
		
		
			</item>
		<item>
		<title>0 to 100K users in 10 years:  how Stan got to where it is</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/12/0-to-100k-users-in-10-years-how-stan-got-to-where-it-is/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/12/0-to-100k-users-in-10-years-how-stan-got-to-where-it-is/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Wed, 12 Oct 2022 19:00:29 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Stan]]></category>
		<category><![CDATA[Statistical computing]]></category>
		<category><![CDATA[open source]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48077</guid>

					<description><![CDATA[I did a talk at Simons Foundation on how Stan was initially developed and how it got to where it is today: 0 to 100K in 10 years: nurturing an open-source software community [YouTube] It was the first time I&#8217;d &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/12/0-to-100k-users-in-10-years-how-stan-got-to-where-it-is/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I did a talk at Simons Foundation on how Stan was initially developed and how it got to where it is today:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=P9gDFHl-Hss">0 to 100K in 10 years: nurturing an open-source software community</a> [YouTube]
</ul>
<p>It was the first time I&#8217;d presented this and I was having so much fun I ran out of time near the end.  If you&#8217;d like to go more slowly, here&#8217;s</p>
<ul>
<li><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/fi-seminar-carpenter-june-2022.pdf">slides for the talk</a> [.pdf]
</ul>
<p>Also, here&#8217;s a direct link to the the TV clip that was played near the start of the talk.</p>
<ul>
<li><a href="https://twitter.com/betanalpha/status/999106548688400384">Stan and Julia mentioned on <I>Billions</I></a> [Twitter]
</ul>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/12/0-to-100k-users-in-10-years-how-stan-got-to-where-it-is/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Blogs &gt; Twitter again</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/12/blogs-twitter-again/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/12/blogs-twitter-again/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Wed, 12 Oct 2022 13:11:48 +0000</pubDate>
				<category><![CDATA[Causal Inference]]></category>
		<category><![CDATA[Economics]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47185</guid>

					<description><![CDATA[As we&#8217;ve discussed many times, I prefer blogs to twitter because in a blog you can have a focused conversation where you explain your ideas in detail, whereas twitter seems like more of a place for position-taking. An example came &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/12/blogs-twitter-again/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>As we&#8217;ve discussed many times, I <a href="https://statmodeling.stat.columbia.edu/2014/11/22/blogs-twitter/">prefer blogs to twitter</a> because in a blog you can have a focused conversation where you explain your ideas in detail, whereas twitter seems like more of a place for position-taking.</p>
<p>An example came up recently that demonstrates this point.  Jennifer sent me a blurb for her causal inference conference and I <a href="https://statmodeling.stat.columbia.edu/2022/02/16/welcome-to-the-american-causal-inference-conference-2022-data-challenge/">blogged it</a>.  This was an announcement and not much more; it could&#8217;ve been on twitter without any real loss of information.  A commenter then <a href="https://statmodeling.stat.columbia.edu/2022/02/16/welcome-to-the-american-causal-inference-conference-2022-data-challenge/#comment-2045114">shot back</a>:</p>
<blockquote><p>Do you see how your policies might possibly negatively impact an outlier such as myself, when you arbitrarily reward contestants for uncovering effects you baked in? How do you know winners just haven’t figured out how you think about manipulating data to find effects? How far removed from my personal, actual, non-ergodic life are your statistical stories, and what policies that impede me unintentionally are you contributing to?</p></blockquote>
<p>OK, this has more words than your typically twitter post, but if I saw it on twitter I&#8217;d be cool with it: it&#8217;s an expression of strong disagreement.</p>
<p>It&#8217;s the next step where things get interesting.  When I saw the above comment, my quick reaction is, &#8220;What a crank!&#8221;  And of course I have no duty to respond at all; responding to blog comments is something I can do for fun when I have the time for it:  it can be helpful to explore the limits of what can be communicated.  In a twitter setting I think the appropriate response would be some snappy response.</p>
<p>But this is a blog, not twitter, so I replied as follows:</p>
<blockquote><p>That’s a funny way of putting things! I’d say that if you don’t buy the premise of this competition, then you don’t have to play. Kinda like if you aren’t interested in winter sports you don’t need to watch the olympics right now. I guess you might reply that our tax money is (indirectly) funding this competition, but then again our tax money funds the olympics too.</p>
<p>Getting to the topic at hand: No, I don’t know that the research resulting from this sort of competition will ultimately improve education policy. Or, even if does, it presumably won’t improve everyone’s education, and it could be that students who are similar to you in some ways will be among those who end up with worse outcomes. All I can say is that this sort of question&#8212;variation in treatment effects, looking at effects on individuals, not just on averages&#8212;is a central topic of modern causal inference and has been so for awhile. So, to the extent that you’re interested in evaluating policies in this way, I think this sort of competition is going in the right direction.</p>
<p>Regarding specifics: I think that after the competition is over, the team that constructed it will publicly release the details of what they did. So at that point in the not-so-distant future, you can take a look, and, if you see problems with it, you can publish your criticisms. That could be useful.</p></blockquote>
<p>I&#8217;m not saying this response of mine was perfect.  I&#8217;m just saying that the blog format was well suited to a thoughtful response, a deepening of the intellectual exchange and a rhetorical de-escalation, which is kind of the opposite of position-taking on twitter.</p>
<p><strong>P.S.</strong>  Also relevant is <a href="https://robjhyndman.com/hyndsight/forecasting-competitions/">this post by Rob Hyndman</a>, A brief history of time series forecasting competitions.  I don&#8217;t know anything about the history of causal inference competitions, or the extent to which these were inspired by forecasting competitions.  The same general question arise, of what&#8217;s being averaged over. </p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/12/blogs-twitter-again/feed/</wfw:commentRss>
			<slash:comments>17</slash:comments>
		
		
			</item>
		<item>
		<title>&#8220;Depressingly unbothered&#8221;</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/11/depressingly-unbothered/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/11/depressingly-unbothered/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Tue, 11 Oct 2022 13:24:31 +0000</pubDate>
				<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47589</guid>

					<description><![CDATA[I just finished listening to the Trojan Horse Affair podcast, and . . . ok, you might have heard about it already, it&#8217;s the story of a ridiculous hoax leading to a horrible miscarriage of justice . . . I &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/11/depressingly-unbothered/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I just finished listening to the <a href="https://www.nytimes.com/interactive/2022/podcasts/trojan-horse-affair.html">Trojan Horse Affair</a> podcast, and . . . ok, you might have heard about it already, it&#8217;s the story of a ridiculous hoax leading to a horrible miscarriage of justice . . . I agree with Brian Reed, the co-host of the show, when he says this near the end of the final episode:</p>
<blockquote><p>Rarely is there one big revelation that undoes years of misinformation and untruth. Most decent journalism is an exercise in incremental understanding. The Trojan Horse letter though, even with my [Reed&#8217;s] tempered expectations, I was surprised by how willing people have been to let it stand unchallenged. People are depressingly unbothered that this harmful myth about Muslims persists.</p></blockquote>
<p>This reminds me of something we&#8217;ve discussed before, that an important aspect of being a scientist is <a href="https://statmodeling.stat.columbia.edu/2016/06/06/29188/">the capacity for being upset</a>.  We learn so much through the recognition and resolution of anomalies.</p>
<p>Like Reed, I&#8217;m upset by people not getting upset by clear anomalies.  This annoyed me when New York Times columnist David Brooks <a href="https://statmodeling.stat.columbia.edu/2015/06/16/the-david-brooks-files-how-many-uncorrected-mistakes-does-it-take-to-be-discredited/">promoted</a> anti-Jewish propaganda and nobody seemed to care.  (I wasn&#8217;t saying Brooks should be fired or fined or anything like that, just that the newspaper should run a goddam correction notice.)  And it annoys me when the British national and local government promotes anti-Muslim propaganda and nobody in charge seems to care.  I guess I can also say that I&#8217;m also bothered when Fox News pushes lies, but that&#8217;s a little different because they&#8217;re just in the propaganda business.  We get worked up in a different way about Brooks and the U.K. government because it doesn&#8217;t seem like their original goal is to lie; rather, they act as a sort of flypaper, attracting stories that fit their preconceptions and then sticking with them even after they&#8217;ve been refuted.</p>
<p>Anyway, to return to the title of this post:  in addition to being bothered by lies, I&#8217;m bothered by how unbothered people are about them.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/11/depressingly-unbothered/feed/</wfw:commentRss>
			<slash:comments>13</slash:comments>
		
		
			</item>
		<item>
		<title>Quantitative science is (indirectly) relevant to the real world, also some comments on a book called The Case Against Education</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/10/quantitative-science-is-indirectly-relevant-to-the-real-world-also-some-comments-on-the-case-against-education/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/10/quantitative-science-is-indirectly-relevant-to-the-real-world-also-some-comments-on-the-case-against-education/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Mon, 10 Oct 2022 13:49:49 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Sociology]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47181</guid>

					<description><![CDATA[Joe Campbell points to this post by economist Bryan Caplan, who writes: The most painful part of writing The Case Against Education was calculating the return to education. I spent fifteen months working on the spreadsheets. I came up with &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/10/quantitative-science-is-indirectly-relevant-to-the-real-world-also-some-comments-on-the-case-against-education/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Joe Campbell points to <a href="https://www.econlib.org/no-one-cared-about-my-spreadsheets/">this post</a> by economist Bryan Caplan, who writes:</p>
<blockquote><p>The most painful part of writing The Case Against Education was calculating the return to education.  I spent fifteen months working on the spreadsheets.  I came up with the baseline case, did scores of “variations on a theme,”  noticed a small mistake or blind alley, then started over. . . .  About half a dozen friends gave up whole days of their lives to sit next to me while I gave them a guided tour of the reasoning behind my number-crunching. . . . When the book finally came out, I published final versions of all the spreadsheets . . . </p>
<p>Now guess what?  Since the 2018 publication of The Case Against Education, precisely zero people have emailed me about those spreadsheets. . . . Don’t get me wrong; The Case Against Education drew plenty of criticism.  Almost none of it, however, was quantitative. . . .</p>
<p>It’s hard to avoid a disheartening conclusion: Quantitative social science is barely relevant in the real world – and almost every social scientist covertly agrees.  The complex math that researchers use is disposable.   You deploy it to get a publication, then move on with your career.  When it comes time to give policy advice, the math is AWOL.  If you’re lucky, researchers default to common sense.  Otherwise, they go with their ideology and status-quo bias, using the latest prestigious papers as fig leaves.</p></blockquote>
<p>Regarding the specifics, I suspect that commenter Andrew (no relation) has a point when he <a href="https://www.econlib.org/no-one-cared-about-my-spreadsheets/#comment-291132">responded</a>:</p>
<blockquote><p>You didn’t waste your time. If you had made your arguments without the spreadsheets&#8212;just guesstimating &#038; eyeballing, you would’ve gotten quantitative criticism. A man who successfully deters burglars didn’t waste his money on a security system just because it never got used.</p></blockquote>
<p>But then there&#8217;s the general question about quantitative social science.  I actually wrote a post on this topic last year, <a href="https://statmodeling.stat.columbia.edu/2021/03/12/the-social-sciences-are-useless-so-why-do-we-study-them-heres-a-good-reason/">The social sciences are useless. So why do we study them? Here’s a good reason</a>.  Here was my summary:</p>
<blockquote><p>The utilitarian motivation for the natural sciences is that can make us healthier, happier, and more comfortable. The utilitarian motivation for the social sciences is they can protect us from bad social-science reasoning. It’s a lesser thing, but that’s what we’ve got, and it’s not nothing.</p></blockquote>
<p>That post stirred some people up, as it sounded like I was making some techbro-type argument that society didn&#8217;t matter.  But I wasn&#8217;t saying that <em>society</em> was useless, I was saying that <em>social science</em> was useless, at least relative to the natural sciences.  Some social science research is really cool, but it&#8217;s nothing compared to natural-science breakthroughs such as transistors, plastics, vaccines, etc.</p>
<p>Anyway, my point is that quantitative social science has value in that it can displace empty default social science arguments.  Caplan is disappointed that people didn&#8217;t engage with his spreadsheets, but I think that&#8217;s partly because he was presenting his ideas in book form.  My colleagues and I had a similar experience with our Red State Blue State book a few years ago:  our general point got out there, but people didn&#8217;t seem to really engage with the details.  We had lots of quantitative analyses in there, but it was a book, so people weren&#8217;t expecting to engage in that way.  Frustrating, but it would be a mistake to generalize from that experience to all of social science.  If you want people to engage with your spreadsheets, I think you&#8217;re better off publishing an article rather than a book.</p>
<p>As a separate matter, Caplan&#8217;s &#8220;move on with your career&#8221; statement is all too true, but that&#8217;s a separate issue.  Biology, physics, electrical engineering, etc., are all undeniably useful, but researchers in these fields also move on with their careers, etc.  That just is telling us that research doesn&#8217;t have 100% efficiency, which is a product of the decentralized system that we have.  It&#8217;s not like WW2 where the government was assigning people to projects.</p>
<p><strong>Comments on The Case Against Education</strong></p>
<p>This discussion reminded me that six years ago Caplan sent me a draft of his book, and I sent him comments.  I might as well share them here:</p>
<blockquote><p>1.  Your intro is fine, it’s good to tell the reader where you’re coming from.  But . . . the way it’s framed, it looks a bit like the “professors are pampered” attack on higher education.  I don’t think this is the tack you want to be taking, for two reasons:  First, most teaching jobs are not like yours:  most teaching jobs are at the elementary or secondary level, and even at the college level, much of the teaching is done by adjuncts.  So, while your presentation of _your_ experience is valid, it’s misleading if it is taken as a description of the education system in general.  Second—and I know you’re aware of this too—if education were useful, there’d be no good reason to complain that some of its practitioners have good working conditions.  Again, this does not affect your main argument but I think you want to avoid sounding like the proudly-overpaid guy discussed here:  http://andrewgelman.com/2011/06/01/the_cushy_life/</p>
<p>This comes up again in your next chapter where you say you have very few skills and that “The stereotype of the head-in-the-clouds Ivory Tower professor is funny because it’s true.”  Ummm, I don’t know about that.  The stereotype of the head-in-the-clouds Ivory Tower professor is not so true, in the statistical sense.  The better stereotype might be that adjunct working 5 jobs.</p>
<p>2.  You write, “Junior high and high schools add higher mathematics, classic literature, and foreign languages – vital for a handful of budding scientists, authors, and translators, irrelevant for everyone else.”  This seems pretty extreme.  One point of teaching math—even the “higher mathematics” that is taught in high school—is to give people the opportunity to find out that they are “budding scientists” or even budding accountants.  As to “authors,” millions of people are authors:  you’ve heard of blogs, right?  It can useful to understand how sentences, paragraphs, and chapters are put together, even if you’re not planning to be Joyce Carol Oates.  As to foreign languages:  millions of people speak multiple languages, it’s a way of understand the world that I think is very valuable.  If _you_ want to say that you’re happy only speaking one language, or that many other people are happy speaking just one language, that’s fine—but I think it’s a real plus to give kids the opportunity to learn to speak and read in other languages.  Now, at this point you might argue that most education in math, literature, and foreign language is crappy—that’s a case you can make, but I think you’re way overdoing it my minimizing the value of these subjects.</p>
<p>3.  Regarding signaling:  Suppose I take a biology course at a good college and get an A, but I don’t go into biology.  Still, the A contributes to my GPA and to my graduation from the good college, which is a job signal.  You might count this as part of the one-third signaling.  But that would be a mistake!  You’re engaging in retrospective reasoning.  Even if I never use that biology in my life, I didn’t know that when took the course.  Taking that bio course was an investment.  I invest the time and effort to learn some biology in order to decide whether to do more of it.  And even if I don’t become a biologist I might end up working in some area that uses biology.  I won’t know ahead of time.  This is not a new idea, it’s the general principle of a “well-rounded education,” which is popular in the U.S. (maybe not so much in Europe, where their post-secondary education is more focused on a student’s major.)  Also relevant on this “signaling” point is this comment:  http://andrewgelman.com/2011/02/17/credentialism_a/#comment-58035</p>
<p>4.  Also, signaling is complicated and even non-monotonic!  Consider this example (which I wrote up here:  http://andrewgelman.com/2011/02/17/credentialism_a/):<br />
&#8220;My senior year I applied to some grad schools (in physics and in statistics) and to some jobs. I got into all the grad schools and got zero job interviews. Not just zero jobs. Zero interviews. And these were not at McKinsey, Goldman Sachs, etc. (none of which I’d heard of). They were places like TRW, etc. The kind of places that were interviewing MIT physics grads (which is how I thought of applying for these jobs in the first place). And after all, what could a company like that do with a kid with perfect physics grades from MIT? Probably not enough of a conformist, eh?”<br />
This is not to say your signaling story is wrong, just that I think it’s much more complicated than you’re portraying.</p>
<p>5.  This is a minor point, but you write, “If the point of education is certifying the quality of labor, society would be better off if we all got less.”  This is not so clear.  From psychometric principles, more information will allow better discrimination.  It’s naive to think of all students as being ranked on a single dimension so that employers just need to pick from the “top third.”  There are many dimensions of abilities and it could take a lot of courses at different levels to make the necessary distinctions.  Again, this isn’t central to your argument but you just have to be careful here because you’re saying something that’s not quite correct, statistically speaking.</p>
<p>6.  You write, &#8220;Consider the typical high school curriculum. English is the international language of business, but American high school students spend years studying Spanish, or even French. During English, many students spend more time deciphering the archaic language of Shakespeare than practicing business writing. Few jobs require knowledge of higher mathematics, but over 80% of high school grads suffer through geometry.”  I think all these topics could be taught better, but my real issue here is that this argument contradicts what you said back on page 6, that you were <em>not</em> going to just “complain we aren’t spending our money in the right way.”</p>
<p>To put it another way:  </p>
<p>7.  You write, “The Ivory Tower ignores the real world.”  I think you need to define your terms.  Is “Ivory Tower” all of education?  All college education?  All education at certain departments at certain colleges?  Lots of teachers of economics are engaged with the real world, no?  Actually it’s not so clear to me what you mean by the real world.  I guess it does not include the world of teaching and learning.  So what parts of the economy do count as real?  I’m not saying you can’t make a point here, but I think you need to define your terms in some way to keep your statements from being meaningless!</p>
<p>And a couple things you didn’t talk about in your book, but I think you should:</p>
<p>&#8211; Side effects of Big Education:  Big Ed provides jobs for a bunch of politically left-wing profs and grad students, it also gives them influence.  For example, Paul Krugman without the existence of a big economics educational establishment would, presumably, not be as influential as the actual Paul Krugman.  One could say the same thing about, say, Greg Mankiw, but the point is that academia as a whole, and prestigious academia in particular, contains lots more liberal Krugmans than conservative Mankiws.  Setting aside one’s personal political preferences, one might consider this side effect of Big Ed to be bad (in that it biases the political system) or good (in that it provides a counterweight to the unavoidable conservative biases of Big Business) or neutral.  Another side effect of Big Ed is powerful teachers unions.  Which, once again, could be considered a plus, a minus, or neutral, depending on your political perspective.  Yet another side effect of Big Ed is that it funds various things associated with schools, such as high school sports (they’re a big deal in Texas, or so I’ve heard!), college sports, and research in areas ranging from Shakespeare to statistics.  Again, one can think of these extracurricular activities as a net benefit, a net cost, or a washout.</p>
<p>In any case, I think much of the debate over the <em>value</em> of education and the <em>structuring</em> of education is driven by attitudes toward its side effects.  This is not something you discuss in your book but I think it’s worth mentioning.  Where <em>you </em>stand on the side effects can well affect your attitude toward the efficacy of the education establishment.  There’s a political dimension here.  You’re a forthright guy and I think your book will be strengthened if you openly acknowledge the political dimension rather than leaving it implicit.</p></blockquote>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/10/quantitative-science-is-indirectly-relevant-to-the-real-world-also-some-comments-on-the-case-against-education/feed/</wfw:commentRss>
			<slash:comments>92</slash:comments>
		
		
			</item>
		<item>
		<title>Pathfinder, Causality, and SARS-CoV-2 talks in Paris this week</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/10/pathfinder-and-sars-cov-2-talks-in-paris-this-week/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/10/pathfinder-and-sars-cov-2-talks-in-paris-this-week/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Mon, 10 Oct 2022 10:13:05 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Public Health]]></category>
		<category><![CDATA[Statistical computing]]></category>
		<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48079</guid>

					<description><![CDATA[Tuesday, 11 October 2022 Andrew and I both have talks tomorrow, so I&#8217;m bummed I have to miss his. I&#8217;ll be talking about Pathfinder at the Polytechnique: Hi! PARIS Seminar &#8211; Bob Carpenter, 11 October 2022 As you may recall &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/10/pathfinder-and-sars-cov-2-talks-in-paris-this-week/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><b>Tuesday, 11 October 2022</b></p>
<p>Andrew and I both have talks tomorrow, so I&#8217;m bummed I have to miss his.  I&#8217;ll be talking about Pathfinder at the Polytechnique:</p>
<blockquote class="wp-embedded-content" data-secret="xHAWxUNJyB"><p><a href="https://www.hi-paris.fr/2022/09/15/bob-carpenter-11-october-2022/">Hi! PARIS Seminar &#8211; Bob Carpenter, 11 October 2022</a></p></blockquote>
<p><iframe class="wp-embedded-content" sandbox="allow-scripts" security="restricted" title="&#8220;Hi! PARIS Seminar &#8211; Bob Carpenter, 11 October 2022&#8221; &#8212; Hi! PARIS" src="https://www.hi-paris.fr/2022/09/15/bob-carpenter-11-october-2022/embed/#?secret=xHAWxUNJyB" data-secret="xHAWxUNJyB" width="584" height="329" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></p>
<p>As you may recall from his earlier post, Andrew&#8217;s giving <a href="https://sites.google.com/view/all-about-that-bayes/#h.7o1vrsqwz75h">a talk on causal inference</a> the same day at Marie Curie in the center of Paris.</p>
<p><b>Thursday, 13 October 2022</b></p>
<p>On Thursday, I&#8217;ll be giving a talk on SARS-CoV-2 diagnostic testing to France Mentre&#8217;s group at Paris Diderot / INSERM.  Here&#8217;s the title and abstract:</p>
<blockquote><p>
Test Site Variation and Sensitivity Time Trends in Covid Diagnostic Tests</p>
<p>I&#8217;ll present two relatively simple models of Covid test sensitivity and specificity.  In the first, sensitivity and specificity are allowed to vary by test site.  We use calibration tests at sites for known positive or known negative cases to develop a hierarchical model to predict accuracy at a new site.  I&#8217;ll present results for the data from Santa Clara early in the pandemic.  This first model can act as a component in any diagnostic study, much like a PK model for a drug can be coupled with a range of PD models.  In the second model, sensitivity is allowed to grow from infection to a peak at symptom onset and decay after that.  I&#8217;ll provide several parameterizations and show results for different cohorts of patience.  I&#8217;ll discuss coupling the second model with time trends of infection fatality and hopspitalization in order to infer population infection levels.  Theoretically, these models are easy to combine, but practically speaking, we do not have a good data set to fit a joint model without very strong assumptions.</p>
<p>The first model is joint work with Andrew Gelman (Columbia University).  The second model is joint work with Tom Ward (UK Health Security Agency).
</p></blockquote>
<p>The location is</p>
<blockquote><p>
2.30 pm, Oct 13, University Paris Diderot, 16 rue Henri Huchard, room 342
</p></blockquote>
<p>and apparently you have to leave ID with security in order to access floor 3 (it&#8217;s in the medical school).</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/10/pathfinder-and-sars-cov-2-talks-in-paris-this-week/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Who rotated my cookie?  The all-too-common mistake when presenting Bayesian inference to strain at the gnat of the prior while swallowing the camel of the likelihood</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/09/who-rotated-my-cookie-the-all-too-common-mistake-when-presenting-bayesian-inference-to-strain-at-the-gnat-of-the-prior-while-swallowing-the-camel-of-the-likelihood/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/09/who-rotated-my-cookie-the-all-too-common-mistake-when-presenting-bayesian-inference-to-strain-at-the-gnat-of-the-prior-while-swallowing-the-camel-of-the-likelihood/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sun, 09 Oct 2022 13:22:46 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Teaching]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47858</guid>

					<description><![CDATA[Sander Greenland pointed me to this amusing little book that introduces Bayesian inference using a simple example of a kid taking a bite out of a cookie: Literal-minded statistician that I am, I noticed a problem here: that 1/3 probability &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/09/who-rotated-my-cookie-the-all-too-common-mistake-when-presenting-bayesian-inference-to-strain-at-the-gnat-of-the-prior-while-swallowing-the-camel-of-the-likelihood/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Sander Greenland pointed me to <a href="https://www.youtube.com/watch?v=CcnLnKU26dg">this amusing little book</a> that introduces Bayesian inference using a simple example of a kid taking a bite out of a cookie:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-24-at-9.44.15-PM.png" alt="" width="200" /><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-24-at-9.44.25-PM.png" alt="" width="200" /></p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-24-at-9.42.48-PM.png" alt="" width="200" /><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-24-at-9.42.53-PM.png" alt="" width="200" /></p>
<p>Literal-minded statistician that I am, I noticed a problem here:  that 1/3 probability in the likelihood seems too high.  Given the picture of the cookie with the candies, I&#8217;d say the probability of getting a bite with no candies is more like 1/10.</p>
<p>Then there&#8217;s all this with the prior distribution:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-24-at-9.46.05-PM.png" alt="" width="200" /></p>
<p>This demonstrates a general point that we see in Bayesian analysis: lots of obsessing over the prior, but the likelihood is set up without much thought at all!  Obv the numbers in the cookie example are arbitrary:  for the purpose of teaching, it doesn&#8217;t matter if it&#8217;s 1/3 or 1/10 or whatever.  The larger problem comes both in teaching and in practice, when people use likelihoods that are way off, and they never even think to check.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/09/who-rotated-my-cookie-the-all-too-common-mistake-when-presenting-bayesian-inference-to-strain-at-the-gnat-of-the-prior-while-swallowing-the-camel-of-the-likelihood/feed/</wfw:commentRss>
			<slash:comments>13</slash:comments>
		
		
			</item>
		<item>
		<title>It&#8217;s . . . doxtastic!</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/08/its-doxtastic/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/08/its-doxtastic/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Sat, 08 Oct 2022 13:42:54 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47865</guid>

					<description><![CDATA[Paul Alper writes: In recent months I have been mumbling that no one else seems to recognize that Bayesian Revision needs revision because on an empirical basis, humans unlike machines, ignore evidence that contradicts a prior belief&#8212;the House Committee is &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/08/its-doxtastic/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Paul Alper writes:</p>
<blockquote><p>In recent months I have been mumbling that no one else seems to recognize that Bayesian Revision needs revision because on an empirical basis, humans unlike machines, ignore evidence that contradicts a prior belief&#8212;the House Committee is on tonight and I will be glued to what transpires* even though my priors indicate nothing will move the needle.  Then I discovered that I am very late to this party regarding doxastic logic.</p>
<p>From Wikipedia:</p>
<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-25-at-2.44.07-AM.png" alt="" width="400" /></p>
<p>In all your blogging items, has doxastic ever been mentioned by anyone?</p>
<p>*<em>Transpired</em> is an interesting word to me because, as incredible as it may seem, the synonym used today for it, i.e., happened, was fiercely opposed in the 1920s.  The preferred synonym was limited to perspired.</p></blockquote>
<p>My response:  I&#8217;ve never heard the term &#8220;doxastic&#8221; before, but this discussion reminds me of the distinction between evidence and truth, which I&#8217;ve discussed in many places (for example <a href="http://www.stat.columbia.edu/~gelman/research/published/ethics22.pdf">here</a> and <a href="https://statmodeling.stat.columbia.edu/2018/04/06/important-distinction-truth-evidence/">here</a>) and which I think causes no end of troubles among scientists who conflate these two ideas.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/08/its-doxtastic/feed/</wfw:commentRss>
			<slash:comments>7</slash:comments>
		
		
			</item>
		<item>
		<title>Mira magic&#8212;a probabilistic card trick</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/07/mira-magic-a-probabilistic-card-trick/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/07/mira-magic-a-probabilistic-card-trick/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Fri, 07 Oct 2022 19:00:41 +0000</pubDate>
				<category><![CDATA[Statistical computing]]></category>
		<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48070</guid>

					<description><![CDATA[One of the fun and educational exhibits at Antonietta Mira&#8217;s children&#8217;s science museum exhibition, Numb3d by Numbers, is a probabilistic card trick. Antonietta didn&#8217;t invent the trick and can&#8217;t recall who did (nor did a quick internet search reveal the &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/07/mira-magic-a-probabilistic-card-trick/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>One of the fun and educational exhibits at Antonietta Mira&#8217;s children&#8217;s science museum exhibition, <a href="https://statmodeling.stat.columbia.edu/2022/10/06/numb3d-by-numbers-a-must-see-kids-science-exhibition-about-measurement-counting-and-stats/">Numb3d by Numbers</a>, is a probabilistic card trick.  Antonietta didn&#8217;t invent the trick and can&#8217;t recall who did (nor did a quick internet search reveal the originator).  The trick is characteristic of probabilistic card tricks, which are tricks that work with some high probability.</p>
<p><b>The card trick</b></p>
<p>Kids divide up a standard deck of 52 playing cards and lay them out end to end in any order.  The cards are big enough for kids to stand on, and laying 52 of them out creates a winding path through the exhibition hall. The exhibition is just full of these nice touches&#8212;everything is highly interactive and scaled for not just one child, but groups of children.  </p>
<p>Next, each kid chooses a starting point from among the first five cards and stands on the card they have chosen.  At this point, the science educator leading the tour tells them where they will all end up.  Each child proceeds along the path of cards the path (down the deck), each time advancing by the value of the card on which they were standing (cards 1–9 have their natural values, whereas ace, 10, jack, queen, king all count as 1). </p>
<p>How does the educator know where they&#8217;ll all end up and what&#8217;s the probability that the trick works?</p>
<p><b>Coupling in Markov chains</b></p>
<p>The first observation to make about this is that the trick works by coupling.  That is, each kids&#8217; path picks out a Markov chain, and they will couple with high probability.  Coupling here means that they get to the same card at some point before the end of the path.  Given that the path is deterministic, as soon as a pair of paths couple, they are guaranteed to wind up in the same location.</p>
<p><b>Stanislaw Ulam to the rescue</b></p>
<p>Ulam developed the Monte Carlo method when he was laid up in hospital in 1946 and playing solitaire to pass the time.  He couldn&#8217;t work out the exact probability of winning analytically, so he played a bunch of games and estimated the chance of winning by the empirical win frequency.  Fun aside:  according to the <a href="https://bookdown.org/manuele_leonelli/SimBook/a-bit-of-history.html">A bit of history</a> section of Manuelle Leonelli&#8217;s online book <i>Simulation and Modelling to Understand Change</I>, the name &#8220;Monte Carlo&#8221; was chosen by Metropolis as a code name for von Neumann and Ulam&#8217;s work, which was classified (being during the Cold War).</p>
<p>We&#8217;re going to follow in Ulam&#8217;s footsteps and figure out the answer by simulation.  Mitzi and I wrote the following Python code to do the simulation on the train after visiting Antonietta at Università della Svizzera Italiana (USI) in Lugano.</p>
<p><tt style="font-family: ui-monospace">mira-magic.py</tt>:</p>
<pre style="font-family: ui-monospace">
import random

suit = [1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1]
deck = 4 * suit

def walk(deck, start):
    """returns final position for deck from start position"""
    pos = start
    while pos + deck[pos] < len(deck):
        pos += deck[pos]
    return pos

def couples(deck):
    """return True if two random starts in 0:4 of a shuffled deck couple"""
    random.shuffle(deck)
    starts = random.sample(range(5), 2)
    end1 = walk(deck, starts[0])
    end2 = walk(deck, starts[1])
    return end1 == end2

N = 10**6
successes = 0
for _ in range(N):
    successes += couples(deck)

rate = successes / N    
var = rate * (1 - rate) * N
sd = var**0.5
se = sd / N
conf_lb = rate - 1.96 * se
conf_ub = rate + 1.96 * se

print(f'Based on {N} simulations:')
print(f'  probability of coupling: {rate:5.3f}')
print(f'  se: {se:7.5f}')
print(f'  95% confidence interval = ({conf_lb:6.4f}, {conf_ub:6.4f})')
</pre>
<p>We define one suit of a deck of cards and then repeat it 4 times for a full deck.  We then defined a function <tt style="font-family: ui-monospace">walk()</tt> that evaluates the final position given a starting position and the deck.  We define a second function <tt style="font-family: ui-monospace">couples()</tt> that returns <tt style="font-family: monospace">True</tt> if we shuffle the deck, choose two different starting points among the first five cards, and wind up at the same place.  The main loop just does a bunch of simulations and counts the number of successes.  This lets us define the success rate.  The variance of a binomial distribution is known and computed as indicated.  We then calculate the standard deviation and standard error, which lets us compute the bounds of a confidence interval.  We then print the results with carefully chosen precision to match the standard errors.</p>
<p><b>The results</b></p>
<p>Here's what happens when we run the program.</p>
<pre style="font-family: ui-monospace">
card-coupling$ python3 mira-magic.py 
Based on 1000000 simulations:
  probability of coupling: 0.983
  se: 0.00013
  95% confidence interval = (0.9828, 0.9833)
</pre>
<p>So that's our answer---98.3%.  The confidence interval just shows we've run enough simulations.  The whole simulation takes close to 15s on my new M2-based MacBook Air, because Python is really really slow for loops [edit: one of the comments has a faster Julia version.]</p>
<p>I wonder what the science educators do when either (a) kids count wrong, or (b) the trick just fails probabilistically.</p>
<p><b>Variants</b></p>
<p>We can also consider variants by slightly tweaking the code.</p>
<ul>
<li>If the kids are forced to choose the first two cards, the probability goes up to 98.8%.
<li>If we count 10 as 10, the probability goes down to 94.3%.
<li>If we count 10 as 10 and all the face cards as 10, the probability goes way down to 69.5%.
<li>If we add two jokers and have them count as 1, the probability goes up to 99.7%.
<li>If we were to use two decks instead of one, the probability goes up to 99.98%.
<li>If we use two decks and add two jokers, the probability is above 99.99%.
</ul>
<p>All those cards with value 1 clearly have a strong effect on the probability of coupling, as does the size of the deck.</p>
<p>For more about coupling in Markov chains and applications to shuffling cards and random walks on a hypercube, here's a nice <a href="https://people.engr.tamu.edu/andreas-klappenecker/csce658-s18/coupling.pdf">slide deck from Andreas Klappenecker</a>.</p>
<p>[edit:  I forgot to say how the educator knew the answer---they just count from card 1 as the kids lay the cards out.  This also seems like it might be error prone, at least for those of us who are mathematically inclined but sloppy at arithmetic.]</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/07/mira-magic-a-probabilistic-card-trick/feed/</wfw:commentRss>
			<slash:comments>32</slash:comments>
		
		
			</item>
		<item>
		<title>Les Distributions a Priori pour l&#8217;Inférence Causale (my talk in Paris Tues 11 Oct 14h)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/07/les-distributions-a-priori-pour-linference-causale-my-talk-in-paris-tues-11-oct-14h/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/07/les-distributions-a-priori-pour-linference-causale-my-talk-in-paris-tues-11-oct-14h/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Fri, 07 Oct 2022 13:58:46 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Causal Inference]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48063</guid>

					<description><![CDATA[Here it is: Les Distributions a Priori pour l&#8217;Inférence Causale En l&#8217;inférence bayésienne on doit spécifier un modèle pour les données (donc un likelihood) et un modèle pour les paramètres (un loi a priori). Envisagez deux questions: 1. Pourquoi est-ce &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/07/les-distributions-a-priori-pour-linference-causale-my-talk-in-paris-tues-11-oct-14h/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><a href="https://sites.google.com/view/all-about-that-bayes/#h.7o1vrsqwz75h">Here it is</a>:</p>
<blockquote><p>Les Distributions a Priori pour l&#8217;Inférence Causale</p>
<p>En l&#8217;inférence bayésienne on doit spécifier un modèle pour les données (donc un likelihood) et un modèle pour les paramètres (un loi a priori).  Envisagez deux questions:<br />
1.  Pourquoi est-ce plus difficile de préciser le likelihood que le loi a priori?<br />
2.  Pour préciser le loi a priori, comment peut-on sauter entre la literature théorique (l&#8217;invariance, le tendance au loi normal, etc) et la literature appliquée (l&#8217;elicitation des experts, la robustesse, etc.)?<br />
Je discuter ces questions dans la domaine de l&#8217;inférence causale:  les lois a priori pour les effets causals, les coefficients de la regression, et les autres paramètres dans les modèles causals.</p></blockquote>
<p>If you follow the link you&#8217;ll see that somewhere along the line they translated my title and abstract into English.  It seems that the talk is supposed to be in English too, which I guess will make it a bit more coherent.  The causal inference connection should be interesting.  It&#8217;s not a talk about causal inference; it&#8217;s more that thinking about causal inference can give us some insight into setting up models.  As is often the case, we can do more when we engage with subject-matter storylines.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/07/les-distributions-a-priori-pour-linference-causale-my-talk-in-paris-tues-11-oct-14h/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Chess cheating: how to detect it (other than catching someone with a shoe phone)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/06/chess-cheating-how-to-detect-it-other-than-catching-someone-with-a-shoe-phone/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/06/chess-cheating-how-to-detect-it-other-than-catching-someone-with-a-shoe-phone/#comments</comments>
		
		<dc:creator><![CDATA[Phil]]></dc:creator>
		<pubDate>Thu, 06 Oct 2022 22:34:25 +0000</pubDate>
				<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Sports]]></category>
		<category><![CDATA[cheating]]></category>
		<category><![CDATA[chess]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48045</guid>

					<description><![CDATA[This post is by Phil Price, not Andrew. Some of you have surely heard about the cheating scandal that has recently rocked the chess world (or perhaps it&#8217;s more correct to say the &#8216;cheating-accusation scandal.&#8217;) The whole kerfuffle started when &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/06/chess-cheating-how-to-detect-it-other-than-catching-someone-with-a-shoe-phone/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[
<p>This post is by Phil Price, not Andrew.<br><br>Some of you have surely heard about the cheating scandal that has recently rocked the chess world (or perhaps it&#8217;s more correct to say the &#8216;cheating-accusation scandal.&#8217;)  The whole kerfuffle started when World Champion Magnus Carlsen withdrew from a tournament after losing a game to a guy named Hans Niemann. Carlsen didn&#8217;t say at the time why he resigned, in fact he said &#8220;I really prefer not to speak. If I speak, I am in big trouble.&#8221;  Most people correctly guessed that Carlsen suspected that Niemann had cheated to win. Carlsen later confirmed that suspicion. Perhaps he didn&#8217;t say so at the start because he was afraid of being sued for slander. <br><br>Carlsen faced Niemann again in a tournament just a week or two after the initial one, and Carlsen resigned on move 2.<br><br>In both of those cases, Carlsen and Niemann were playing &#8220;over the board&#8221; or &#8220;OTB&#8221;, i.e. sitting across a chess board from each other and moving the pieces by hand.  That&#8217;s in contrast to &#8220;online&#8221; chess, in which players compete by moving pieces on a virtual board.  Cheating in online chess is very easy: you just run a &#8220;chess engine&#8221; (a chess-playing program) and enter the moves from your game into the engine as you play, and let it tell you what move to make next.  Cheating in OTB chess is not so simple: at high-level tournaments players go through a metal detector before playing and are not allowed to carry a phone or other device. (A chess engine running on a phone can easily beat the best human players.  A chess commentator once responded to the claim &#8220;my phone can beat the world chess champion&#8221; by saying &#8220;that&#8217;s nothing, my microwave can beat the world chess champion.&#8221;).  But if the incentives are high enough, some people will take difficult steps in order to win. In at least one tournament it seems that a player was using a chess computer (or perhaps a communication device)<a href="https://en.chessbase.com/post/the-shoe-aistant--ivanov-forfeits-at-blagoevgrad-051013"> concealed in his shoe</a>. <br><br>I don&#8217;t know if there are specific allegations related to how Niemann might have cheated in OTB games. A shoe device again, which Niemann uses to both enter the moves as they occur and to get the results through vibration? A confederate who enters the moves and signals Niemann somehow (a suppository that vibrates?). I&#8217;m not really sure what the options are. It would be very hard to &#8220;prove&#8221; cheating simply by looking at the moves that are made in a single game: at the highest levels both players can be expected to play <em>almost</em> perfectly, usually making one of the top two or three moves on every move (as evaluated by the computer), so simply playing very very well is not enough to prove anything.</p>



<span id="more-48045"></span>



<p>But even if it&#8217;s impossible to prove cheating by looking at the moves in a single game, there&#8217;s additional information available from the arc of a player&#8217;s career. And although cheating in online games could, in principle, be at least as hard to detect as cheating in OTB chess &#8212; actually it should be harder, because you can&#8217;t do a physical search of the players! &#8212; in practice the ways people cheat in online games seem to be rather easily detectable. A company called chess.com that is used by millions of players (including me!) has posted <a href="https://www.chess.com/blog/CHESScom/hans-niemann-report">a long article</a> about their cheat-detection methods, specifically as regards Hans Niemann &#8212; who has, they believe, cheated in many games, including when playing for monetary prizes &#8212;  and it lists several ways they look for cheating. Their bullet list is a bit confused so I&#8217;ve cleaned it up here:</p>



<ul><li>Comparing the moves made to engine-recommended moves (after removing some opening and endgame moves that are memorized and thus uninformative about player skill).</li><li>Comparing player past performance to their historical strength</li><li>Comparing a player&#8217;s performance to comparable peers</li><li>Looking at behavioral factors (e.g. &#8216;browser behavior&#8217;)</li><li>Reviewing time usage for finding easy vs difficult moves. </li></ul>



<p>The &#8216;browser behavior&#8217; item needs explanation. If you play on chess.com, they can tell when your computer&#8217;s &#8216;focus&#8217; is on the window where you are playing. If you click to another window, they can tell. At least two things would be suspicious: (1) clicking away from chess.com after every move or almost every move, since this would suggest you might be entering moves into the engine and looking at results, and (2) playing stronger moves after returning to the chess.com window from somewhere else, as compared to the moves you play when you stay on the chess.com window for multiple consecutive moves. <br><br>Item (2) requires an assessment of move quality, which is usually summarized in the pleasing unit of &#8220;centipawns.&#8221; By tradition, a pawn is considered to be &#8220;worth&#8221; one point on average, and chess programs are programmed to evaluate positions by assigning points according to that scale. If you a move causes you to lose a pawn (without other compensation such as a stronger position) then, as far as the program is concerned, you have lost a point.  To evaluate a human&#8217;s move you can compare their position after their move to the position if they had made the move the computer thinks is best. If they made the computer move, they have a loss of &#8216;0 centipawns&#8217;. If the computer thinks their position is 0.12 pawns worse than it would have been if they had made the computer move, then they have lost 12 centipawns. Measures such as &#8216;average centipawn loss&#8217; can be used to evaluate player strength, both for online play and OTB play (and even for players who played long before the advent of computers). Chess.com creates a &#8220;Strength Score&#8221; based on this sort of information (I don&#8217;t think they have published their algorithm); this is highly correlated with the player &#8220;ELO ratings&#8221;, which are based on win-loss-draw records against other players but chess.com evidently prefers it to using the ratings alone.  </p>



<p>In online play, if a player averages a 4 centipawn loss per move when they stay on the chess.com window, but only a 1 centipawn move when they return to it after clicking away to another window, that suggests cheating. (But it doesn&#8217;t <em>prove</em> cheating.  Perhaps you only click away to check Facebook if the position is very simple and you know you can pick the best move). It seems to me that if you were going to cheat in an online tournament you would simply have a second computer to run your chess engine. Maybe that&#8217;s what people will do, now that they know chess.com can and does detect when they click away.<br><br>By using these approaches, chess.com has concluded that Niemann &#8220;very likely&#8221; cheated in a great many individual games and in many online tournaments. You can read the chess.com article for a list. (By the way, Niemann has admitted to cheating in some online games, although he claims never to have done so when money was at stake.)</p>



<p>Other than &#8216;browser behavior&#8217;, the other methods chess.com can be used for detecting cheating in OTB chess as well&#8230;and those also make Niemann&#8217;s performance look very, very suspicious. For example, considering only OTB chess, Niemann&#8217;s improvement from age 11 through 19 has been faster than any other player in history. Second on the list is Bobby Fischer, fourth is Magnus Carlsen. This alone doesn&#8217;t <em>prove</em> anything. We know Fischer wasn&#8217;t improving because of an engine, and yet he had this huge improvement, and who&#8217;s to say nobody could ever do it faster than Fischer? But <em>either</em> Niemann improved faster than any other chess player of all time, <em>or</em> he cheated.  <br><br>Finally, there&#8217;s some other, less quantifiable evidence that Niemann has cheated in OTB chess. For example, many grandmasters supposedly find that his post-game discussions of his games show lack of a deep understanding of the positions that he just played. And some grandmasters think Niemann&#8217;s pattern of time usage is suspicious: that he takes too little time in subtle positions but still finds the best moves, while sometimes taking too much time in clearer positions where he should be able to move more quickly. (Perhaps I should have mentioned earlier: all games, online and OTB, are played using a chess clock so that players are limited in how much time they have. The amount of time allowed is different from tournament to tournament). <br><br>I guess that&#8217;s enough for now. I think the evidence that Niemann cheated very frequently in online chess is extremely convincing, and I think all of the smart money is betting that he has also cheated OTB. Certainly that&#8217;s where my money is, and my money isn&#8217;t even smart!<br><br>If you have any interest in this general topic, I highly recommend reading the chess.com article (link is in the fifth paragraph, above). <br><br>This post is by Phil. </p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/06/chess-cheating-how-to-detect-it-other-than-catching-someone-with-a-shoe-phone/feed/</wfw:commentRss>
			<slash:comments>67</slash:comments>
		
		
			</item>
		<item>
		<title>Numb3d by Numbers&#8212;a must see kid&#8217;s science exhibition about measurement, counting, and stats</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/06/numb3d-by-numbers-a-must-see-kids-science-exhibition-about-measurement-counting-and-stats/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/06/numb3d-by-numbers-a-must-see-kids-science-exhibition-about-measurement-counting-and-stats/#comments</comments>
		
		<dc:creator><![CDATA[Bob Carpenter]]></dc:creator>
		<pubDate>Thu, 06 Oct 2022 19:00:37 +0000</pubDate>
				<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48060</guid>

					<description><![CDATA[Mitzi and I just visited Antonietta Mira in Lugano. We went to talk about applying her delayed rejection algorithm to Hamiltonian Monte Carlo and partial momentum refresh Metropolis-adjusted Langevin. Numb3d by Numbers We were absolutely blown away when Antonietta took &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/06/numb3d-by-numbers-a-must-see-kids-science-exhibition-about-measurement-counting-and-stats/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Mitzi and I just visited <a href="https://search.usi.ch/en/people/f8960de6d60dd08a79b6c1eb20b7442b/mira-antonietta">Antonietta Mira</a> in Lugano.  We went to talk about applying her delayed rejection algorithm to Hamiltonian Monte Carlo and partial momentum refresh Metropolis-adjusted Langevin.</p>
<p><b>Numb3d by Numbers</b> </p>
<p>We were absolutely blown away when Antonietta took us on a tour of her science museum exhibition on probability, statistics, and simulation (Diamo I Numeri! in Italian, Numb3d by Numbers in English, which are not quite direct translations).  If I had seen this when I was a kid, you wouldn&#8217;t have been able to tear me away from the exhibits until I&#8217;d figured each one out.  The kids that were there when we were touring were all enthralled. </p>
<p>Here&#8217;s a description of the exhibit (in English): <a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/numb3d-by-numbers.pdf">Numb3d by Numbers</a>.  And here&#8217;s a short video in Italian, which shows how beautiful the exhibits are: <a href="https://youtu.be/p4wfzwREvkI">Diamo I Numeri!</a>  </p>
<p><b>The exhibits</b></p>
<p>Among the exhibits Mitzi and I recall (and we were only there for an hour or so!):</p>
<ul>
<li> the coolest, largest-scale Galton board I&#8217;ve ever seen
<li> a horse race with two six-sided dice and horses numbered 2&#8211;12
<li> the towers of Hanoi
<li> number writing systems that had kids translate their age
<li> ancient games of counting
<li> a magic trick with a giant deck of cards (see below)
<li> population pyramids where people put their number down
<li> crowdsourced guesses of jars of regular and irregular items
<li> visualizations with strings of what you&#8217;re good and bad about to see trends
<li> tangrams and other shape-based puzzles
<li> magic squares with all the math
<li> combinatorics and growth of possibilities
<li> real-time counters, scales, and how they need to be contextualized
<li> emergent patterns like dragonfly wings and sunflower illustrations of Fibonacci sequences
<li> randomness and patterns
<li> parity and how shepherds count goats
<li> positive predictive accuracy of diagnostic tests using frequencies rather than probabilities
<li> abacuses and calculating methods
</ul>
<p>The exhibits are aimed at kids, but they also work for adults and some of them even for young kids. It&#8217;s set up in a beautiful hillside science museum (all of Lugano is amazingly beautiful, being situated on a crystal clear alpine lake).  The exhibition is designed to be led by a science educator, though it&#8217;s also possible to do a self-guided tour.  </p>
<p><b>On the road</b></p>
<p>The great news is that Antonietta and crew just got a grant to translate the exhibit into English and German (it&#8217;s now in Italian) and take it on the road.  She&#8217;s looking for collaborators interested in bringing it to their home towns.  If you have any interest in stats education for kids, this is a golden opportunity.</p>
<p><b>A probabilistic card trick</b></p>
<p>I was fascinated by the probabilistic card trick (Antonietta&#8217;s a mathemagician and did a bunch of other card tricks for us).  Kids divide up a deck of cards and lay them out end to end in any order (these were huge cards so the deck made a winding path through the exhibits).  Then kids chose different starting points in the first five cards and stood on the card they chose.  Then each proceeds to move along the path (down the deck), each time advancing by the value of the card on which they were standing (cards 1&#8211;9 had their natural values, ace, 10, jack, queen, king all count as 1).  The science educator conducting the tour tells the kids where they&#8217;ll land before they start.  </p>
<p>So the question is how does this work?  If you&#8217;re used to thinking about simulation, you may understand the theory of why this is likely to happen.  Mitzi and I had to put on our <a href="https://bookdown.org/manuele_leonelli/SimBook/a-bit-of-history.html">Stan(islaw Ulam) hats</a> to work out how probable the trick is to work by simulation.  I&#8217;ll leave it as an exercise for the reader (and provide a solution in a follow-up post).</p>
<p><b>A horse race</b></p>
<p>I really liked the horse race because it involved dice and was very graphic and tactile.  Kids all get a pair of really big dice and the horses are numbered 2&#8211;12.  Each time a horse&#8217;s number comes up, it advances one step.  The first horse to take 20 (?) steps wins.  Of course, the 7 horse is the strong favorite, especially with a lot of steps.  We didn&#8217;t get around to doing the simulation for that one, but it&#8217;d be a nice exercise to introduce discrete Monte Carlo methods.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/06/numb3d-by-numbers-a-must-see-kids-science-exhibition-about-measurement-counting-and-stats/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>(One of) the hardest things about differential privacy could also be seen as opportunity </title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/06/one-of-the-hardest-things-about-differential-privacy-could-also-be-seen-as-opportunity/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/06/one-of-the-hardest-things-about-differential-privacy-could-also-be-seen-as-opportunity/#comments</comments>
		
		<dc:creator><![CDATA[Jessica Hullman]]></dc:creator>
		<pubDate>Thu, 06 Oct 2022 17:41:03 +0000</pubDate>
				<category><![CDATA[Miscellaneous Science]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48065</guid>

					<description><![CDATA[This is Jessica. During a discussion recently a collaborator expressed a fact about differential privacy that I think captures what is fundamentally difficult about getting data analysts to accept it. The fact is: If you can describe a small set &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/06/one-of-the-hardest-things-about-differential-privacy-could-also-be-seen-as-opportunity/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><span style="font-weight: 400">This is Jessica. During a discussion recently a collaborator expressed a fact about differential privacy that I think captures what is fundamentally difficult about getting data analysts to accept it. The fact is:</span></p>
<blockquote><p><span style="font-weight: 400">If you can describe a small set of queries that capture what you fundamentally care about, designing the differentially private mechanism for that set will be much more efficient than any mechanism that tries to give data that allows you to do a wider set of queries.</span></p></blockquote>
<p><span style="font-weight: 400">Given that every query leaks information, and you can avoid some leakage by designing the privacy mechanism to account for the structure of the queries (e.g. where outputs can be expected to correlate), it will always be to your advantage to have information on the analyst’s priorities. This creates a design problem: how do we get the analyst to give you their preferences over what’s important so you can give them the most efficient privacy mechanism? </span></p>
<p><span style="font-weight: 400">This aspect of differential privacy can seem at odds with all the wisdom around the value of exploring the data to identify how to best analyze it. This older </span><a href="https://www.cs.cmu.edu/afs/cs/usr/wing/www/class/15-895/LarryWasserman.pdf"><span style="font-weight: 400">slide deck from Wasserstein</span></a><span style="font-weight: 400"> describes a cultural divide that crops up, where many statisticians would much prefer a sanitized database where they can issue whatever queries they want, but that’s not what the computer scientists have in mind.</span></p>
<p><span style="font-weight: 400">I’ve generally leaned a bit more toward the side of statisticians than privacy researchers, in the sense of agreeing that its not trivial to expect people to be able to tell you about their highest priority queries before they see the data. But, taking the above fact as inherent to privacy protection, maybe we could instead see this as a feature of differential privacy, in that it could inspire new approaches to getting analysts to more efficiently arrive at and specify their interests or fundamental questions when doing interactive data analysis. In visualization research for instance, we’ve mostly shied away from thinking like mechanism designers who want to elicit analysts’ prior domain knowledge in order to recommend useful views to them (or maybe we equate “seeing data” with elicitation). The design mantras focus on simple defaults for how to show the observed data, like Provide an overview first, but offer details on demand, or Give them the ability to focus on certain data, but don’t let them lose the context. I don’t think these ideas are incongruent with getting analysts to provide information about what they are most interested in, but there’s a tendency to shoot for domain general, knowledge-agnostic tools where possible. Ideas like trying to elicit analysts’ priors or expectations can seem controversial (see, e.g., our </span><a href="https://hdsr.mitpress.mit.edu/pub/w075glo6/release/1"><span style="font-weight: 400">discussion article on model checking as a theory for visualization</span></a><span style="font-weight: 400">).  </span></p>
<p><span style="font-weight: 400">Perhaps more importantly, there haven’t really been popular use cases where you couldn’t just see all the data, outside of the researcher degrees of freedom type examples discussed in science reform more generally. There have been some suggestions that popular visual analysis tools like Tableau might be p-hacking machines in a sense, by letting people make all sorts of comparisons, but it’s hard to find really compelling evidence that this is a major risk. So, maybe the awkward constraints differential privacy imposes are a blessing in disguise, and we’ll get more sustained attempts to elicit domain knowledge as an alternative to the “just let them click around until they find it” approach.</span></p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/06/one-of-the-hardest-things-about-differential-privacy-could-also-be-seen-as-opportunity/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>A baseball analytics job using Stan!</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/06/a-baseball-analytics-job-using-stan/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/06/a-baseball-analytics-job-using-stan/#respond</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 06 Oct 2022 16:44:32 +0000</pubDate>
				<category><![CDATA[Jobs]]></category>
		<category><![CDATA[Sports]]></category>
		<category><![CDATA[Stan]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48062</guid>

					<description><![CDATA[Tony Williams writes: I have nothing to do with this job, but it might be interesting to your readers since they specifically mention Stan as a desired skill. From the link: Data Scientist, Baseball Research &#038; Development The Cleveland Guardians &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/06/a-baseball-analytics-job-using-stan/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Tony Williams writes:</p>
<blockquote><p>I have nothing to do with <a href="https://boards.greenhouse.io/clevelandguardiansbops/jobs/6303371002">this job</a>, but it might be interesting to your readers since they specifically mention Stan as a desired skill.</p></blockquote>
<p>From the link:</p>
<blockquote><p>Data Scientist, Baseball Research &#038; Development</p>
<p>The Cleveland Guardians Baseball Research &#038; Development (R&#038;D) group is seeking data scientists at a variety of experience levels . . . You will analyze video, player tracking, and biomechanics data as well as traditional baseball data sources like box scores to help us acquire and develop baseball players into a championship-caliber team. . . .</p>
<p>Qualifications </p>
<p>&#8211; Demonstrated experience or advanced degree in a quantitative field such as Statistics, Computer Science, Economics, Machine Learning, or Operations Research. </p>
<p>&#8211; Programming skills in a language such as R or Python to work efficiently at scale with large data sets. </p>
<p>&#8211; Desire to continue learning about data science applications in baseball. </p></blockquote>
<p>And then in the Preferred Experience section, along with &#8220;Demonstrated research experience in a sports context (baseball is a plus)&#8221; and &#8220;Experience with computer vision&#8221; and a few other things, they have:</p>
<blockquote><p><strong>&#8211; Experience with Bayesian statistics and languages such as Stan.</strong></p></blockquote>
<p>How cool is that??</p>
<p>And, hey!  I just looked it up . . . the Guardians have a winning record this year and they&#8217;re headed for the playoffs!  Nothing like the Cleveland MLB teams I remember from my childhood . . .</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/06/a-baseball-analytics-job-using-stan/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>(Now that faculty aren&#8217;t coming into the office anymore) Will universities ever recover?</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/06/now-that-faculty-arent-coming-into-the-office-anymore-will-universities-ever-recover/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/06/now-that-faculty-arent-coming-into-the-office-anymore-will-universities-ever-recover/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Thu, 06 Oct 2022 13:19:24 +0000</pubDate>
				<category><![CDATA[Sociology]]></category>
		<category><![CDATA[Teaching]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47886</guid>

					<description><![CDATA[A few years ago I taught a course at Sciences Po in Paris. The classes were fine, the students were fine, but there was almost no academic community. I had an office in some weird building where they stuck visitors. &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/06/now-that-faculty-arent-coming-into-the-office-anymore-will-universities-ever-recover/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>A few years ago I taught a course at Sciences Po in Paris.  The classes were fine, the students were fine, but there was almost no academic community.  I had an office in some weird building where they stuck visitors.  The place was mostly pretty empty.  Sometimes I&#8217;d go over to the department of economics, which was hosting me (but had no office space)&#8212;it was in a fancy building on, I think it was the 4th floor but maybe it was the 2nd floor and they were just very long flights of stairs&#8212;and most of the faculty were never there.  So I didn&#8217;t bother to come by very often:  what&#8217;s the point if there&#8217;s no office for you and no colleagues to talk with.  I don&#8217;t know what it was like for the students who wanted intellectual experiences outside of class:  maybe there were places where the grad students hung out?  I don&#8217;t know.</p>
<p>Anyway, the American universities that I&#8217;ve attended and taught at have been nothing like that.  Buzzing with faculty and grad students, lots of opportunities for spontaneous conversations.</p>
<p>Then came covid.  Classes were moved online, then we weren&#8217;t allowed to come into the office or teach in person.  At some point they started allowing in-person teaching but they were still discouraging us from showing up to the office or having in-person meetings outside of class.  Eventually all become allowed, but then there became the new norm of zoom meetings, faculty who didn&#8217;t want to come into work if they didn&#8217;t have to, students who wanted to avoid the commute to school, etc.  And then, as with Sciences Po those many years ago, I was less motivated to show up to work myself, which resulted in fewer spontaneous interactions with students and colleagues.  Online can be convenient&#8212;hey, look at this blog!&#8212;but I still think something is missing.</p>
<p>So here&#8217;s the question:  Will universities ever recover?</p>
<p>Sadly, I suspect the answer is no.  It&#8217;s just too easy not so show up, also this is just the continuation of a decades-long trend of fewer weeks in the semester, fewer days of class in the week, much less need for the physical library, etc.  Also, the people at Sciences Po back in 2009 seemed just fine with closed doors and empty corridors.  So that arid academic environment seems like a stable equilibrium.  It makes me sad.  Obv it&#8217;s the least of our problems in the world today, but still.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/06/now-that-faculty-arent-coming-into-the-office-anymore-will-universities-ever-recover/feed/</wfw:commentRss>
			<slash:comments>31</slash:comments>
		
		
			</item>
		<item>
		<title>What Nested R-hat teaches us about the classical R-hat</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/05/what-nested-r-hat-teaches-us-about-the-classical-r-hat/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/05/what-nested-r-hat-teaches-us-about-the-classical-r-hat/#comments</comments>
		
		<dc:creator><![CDATA[Charles Margossian]]></dc:creator>
		<pubDate>Wed, 05 Oct 2022 19:00:00 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Statistical computing]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=48054</guid>

					<description><![CDATA[(this post is by Charles) My colleagues Matt Hoffman, Pavel Sountsov, Lionel Riou-Durand, Aki Vehtari, Andrew Gelman, and I released a preprint titled “Nested R-hat: assessing the convergence of Markov chain Monte Carlo when running many short chains”. This is &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/05/what-nested-r-hat-teaches-us-about-the-classical-r-hat/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p class="p1"><span class="s1">(this post is by Charles)</span></p>
<p class="p1" style="text-align: left;"><span class="s1">My colleagues Matt Hoffman, Pavel Sountsov, Lionel Riou-Durand, Aki Vehtari, Andrew Gelman, and I released a <a href="https://arxiv.org/abs/2110.13017">preprint</a> titled “Nested R-hat: assessing the convergence of Markov chain Monte Carlo when running many short chains”. This is a revision of an earlier preprint. Here’s the abstract:</span></p>
<blockquote>
<p class="p3"><span class="s1">The growing availability of hardware accelerators such as GPUs has generated interest in Markov chains Monte Carlo (MCMC) workflows which run a large number of chains in parallel. Each chain still needs to forget its initial state but the subsequent sampling phase can be almost arbitrarily short. To determine if the resulting short chains are reliable, we need to assess how close the Markov chains are to convergence to their stationary distribution. The R-hat statistic is a battle-tested convergence diagnostic but unfortunately can require long chains to work well. We present a nested design to overcome this challenge, and introduce tuning parameters to control the reliability, bias, and variance of convergence diagnostics.</span></p>
</blockquote>
<p class="p3"><span class="s1">The paper is motivated by the possibility of running many Markov chains in parallel on modern hardware, such as GPU. Increasing the number of chains allows you to reduce the variance of your Monte Carlo estimator, which is what the sampling phase is for, but not the bias, which is what the warmup phase is for (that&#8217;s the short story).<span class="Apple-converted-space">  </span>So you can trade length of the sampling phase for number of chains but you still need to achieve approximate convergence.</span></p>
<p class="p3"><span class="s1">There’s more to be said about the many-short-chains regime but what I want to focus on is what we’ve learned about <a href="https://arxiv.org/abs/1903.08008">the more classic R-hat</a>. The first step is to rewrite the condition, R-hat &lt; 1.01, as a tolerance on the variance of the <i>per chain </i>Monte Carlo estimator. Intuitively, we’re running a stochastic algorithm to estimate an expectation value, which is a non-random quantity. Hence, different chains should, despite their different initialization and seed, still come to an “agreement”. This agreement is measured by the variance of the estimator produced by each chain.</span></p>
<p class="p3"><span class="s1">Now here’s the paradox. The expected squared error of a <i>per chain </i>Monte Carlo estimator decomposes into a squared bias and a variance. When diagnosing convergence, we’re really interested in making sure the bias has decayed sufficiently (a common phrase is “has become negligible”, but I find it useful to think of MCMC as a biased algorithm). But, with R-hat, we’re really monitoring the variance, not the bias! So how can this be a useful diagnostic?</span></p>
<p class="p3"><span class="s1">This paradox occurred to us when we rewrote R-hat to monitor the variance of Monte Carlo estimators constructed using groups of chains or <i>superchains</i>, rather than a single chain. The resulting nested R-hat decays to 1 provided we have enough chains, even if the individual chains are short (think a single iteration). But here’s the issue: regardless of wether the chains are close to convergence or not, R-hat can be made arbitrarily close to 1 by increasing the size of each superchain and thence decreasing the variance of their Monte Carlo estimator. Which goes back to my earlier point: you cannot monitor bias simply by looking at variance.</span></p>
<p class="p3"><span class="s1">Or can you?</span></p>
<p class="p3"><span class="s1">Here’s the twist: we now force all the chains within a superchain to start at the same point. I had this idea initially to deal with multimodal distributions. The chains within a group are no longer independent, though eventually they (hopefully) will forget about each other. In the mean time we have artificially increased the variance. Doing a standard variance decomposition:</span></p>
<p class="p3"><span class="s1">total variance = variance of conditional expectation + expected conditional variance</span></p>
<p class="p3"><span class="s1">Here we’re conditioning on the initial point. If the expected value of each chain no longer depends on the initialization, then the first term — variance of the conditional expectation — goes to 0. This is a measurement of “how well the chains forget their starting point”, and we call it the <i>violation of stationarity</i>. It is indifferent to the number of chains. The second term, on the other hand, persists even if your chains are stationary but it decays to 0 as you increase the number of chains. More generally, this <i>persistent variance</i> can be linked to the Effective Sample Size.</span></p>
<p><a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/nested-rhat_monitor_large-1.png"><img loading="lazy" class="wp-image-48059  aligncenter" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/10/nested-rhat_monitor_large-1.png" alt="" width="520" height="184" /></a></p>
<p class="p3"><span class="s1">We argue that nested R-hat is a (scaled) measure of the <i>violation of stationarity</i>, biased by the <i>persistent variance</i>. How does this link to the squared bias? Well, both bias and violation decay as we warm up our chains, so one can be used as a “proxy clock” of the other. I don’t have a fully general theory for this but if you consider a Gaussian target and are willing to solve an SDE, you can show that the violation and the squared bias decay at the same rate. This is also gives us insight about how over-dispersed initializations should be (or not be) for nested R-hat to be reliable.</span></p>
<p class="p3"><span class="s1">Now nested R-hat is a generalization of R-hat, meaning our analysis carries over! We moreover have a theory of what R-hat measures which does not assume stationarity. Part of the conceptual leap is to do an asymptotic analysis which considers an infinite number of finite (non-stationary) chains, rather than a single infinitely long (and hence stationary) chain.</span></p>
<p class="p3"><span class="s1">Moving forward, I hope this idea of a proxy clock will help us identify cases where R-hat and its nested version are reliable, and how we might revise our MCMC processes to get more reliable diagnostics. Two examples discussed in the preprint: choice of initial variance and how to split a fixed total number of chains into superchains.</span></p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/05/what-nested-r-hat-teaches-us-about-the-classical-r-hat/feed/</wfw:commentRss>
			<slash:comments>7</slash:comments>
		
		
			</item>
		<item>
		<title>Not frequentist enough.</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/05/not-frequentist-enough/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/05/not-frequentist-enough/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Wed, 05 Oct 2022 13:58:00 +0000</pubDate>
				<category><![CDATA[Bayesian Statistics]]></category>
		<category><![CDATA[Decision Theory]]></category>
		<category><![CDATA[Miscellaneous Statistics]]></category>
		<category><![CDATA[Political Science]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47932</guid>

					<description><![CDATA[I think that many mistakes in applied statistics could be avoided if people were to think in a more frequentist way. Look at it this way: In the usual way of thinking, you apply a statistical procedure to the data, &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/05/not-frequentist-enough/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>I think that many mistakes in applied statistics could be avoided if people were to think in a more frequentist way.</p>
<p>Look at it this way:</p>
<p>In <strong>the usual way of thinking</strong>, you apply a statistical procedure to the data, and if the result reaches some statistical-significance threshold, and you get similar results from a robustness study, changing some things around, then you&#8217;ve made a discovery.</p>
<p>In <strong>the frequentist way of thinking</strong>, you consider your entire procedure  (all the steps above) as a single unit, and you consider what would happen if you apply this procedure to a long series of similar problems.</p>
<p>The first thing to recognize is that <em>the frequentist way of thinking requires extra effort</em>:  you need to define this potential series of similar problems and then either do some mathematical analysis or, more likely, set up a simulation on the computer.</p>
<p>The second thing to recognize is that, just because a statistical method is defined in a &#8220;classical&#8221; or hypothesis-testing framework, that doesn&#8217;t make it a &#8220;frequentist&#8221; method.  For a method to be frequentist, it needs to defined relative to some frequency calibration.  A p-value or a confidence interval, by itself, is not frequentist; for it to be frequentist there needs to be some model of what would be done in a family of replications of the procedure.  This is the point that Loken and I make in section 1.2 of our <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf">forking paths paper</a>.</p>
<p>In the usual way of teaching statistics, the extra effort required by the frequentist approach is not clear, for two reasons.  First, textbooks present the general theory in the context of simple examples such as linear models with no selection, where there are simple analytic solutions.  Second, textbook examples of statistical theory typically start with an assumed probability model for the data, in which case most of the hard work has already been done.  The model is just <em>there</em>, postulated; it doesn&#8217;t look like a set of &#8220;assumptions&#8221; at all.  It&#8217;s <a href="https://statmodeling.stat.columbia.edu/2009/05/24/handy_statistic/">the camel that is the likelihood</a> (although, strictly speaking, the likelihood is not the data model; additional assumptions are required to go from an (unnormalized) likelihood function to get a generative model for the data).</p>
<p><strong>An example</strong></p>
<p>To demonstrate this point, I&#8217;ll use an example from a recent article, <a href="http://www.stat.columbia.edu/~gelman/research/published/causal_paths_3.pdf">Criticism as asynchronous collaboration: An example from social science research</a>, where I discussed a published data analysis that claimed to show that &#8220;politicians winning a close election live 5–10 years longer than candidates who lose,&#8221; with this claim being based on a point estimate from a few hundred elections:  the estimate was statistically significantly different from zero and similar estimates were produced in a robustness study in which various aspects of the model were tweaked.  The published analysis was done using what I describe above as &#8220;the usual way of thinking.&#8221;</p>
<p>Now let&#8217;s consider the frequentist approach.  We have to make some assumptions.  Suppose you start with the assumption that losing an election has the effect of increasing your lifespan by X years, where X has some value between -1 and 1.  (From an epidemiological point of view, an effect of 1 year is large, really on the very high end of what could be expected as an average treatment effect of something as indirect as winning or losing an election.)  From there you can work out what might happen from a few hundred elections, and you&#8217;ll see that any estimate will be super noisy, to the extent that if you fit a model and select on statistical significance, you&#8217;ll get an estimated effect that&#8217;s much higher than the real effect (a large type M error, as we say).  You&#8217;ll also see that, if you want to get a large effect (large effects are exciting, right!) then you&#8217;ll want the standard error of your estimate to be larger, and you can get this by the simple expedient of predicting future length of life without including current age as a predictor.  For more discussion of all these issues, see section 4 of the <a href="http://www.stat.columbia.edu/~gelman/research/published/causal_paths_3.pdf">linked article</a>.  My point here is that whatever analysis we do, there is a benefit to thinking about it from a frequentist perspective&#8212;what would things look like if the procedure were replied repeatedly to many datasets?&#8212;rather than to fixate on the results of the analysis as applied to the data at hand.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/05/not-frequentist-enough/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Rich guys and their dumb graphs:  The visual equivalents of &#8220;Dow 36,000&#8221;</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/04/dow-36000-but-in-graph-form/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/04/dow-36000-but-in-graph-form/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Tue, 04 Oct 2022 13:34:14 +0000</pubDate>
				<category><![CDATA[Economics]]></category>
		<category><![CDATA[Statistical graphics]]></category>
		<category><![CDATA[Zombies]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47868</guid>

					<description><![CDATA[Palko links to this post by Russ Mitchell linking to this post by Hassan Khan casting deserved shade on this post, &#8220;The Third Transportation Revolution,&#8221; from 2016 by Lyft Co-Founder John Zimmer, which includes the above graph. What is it &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/04/dow-36000-but-in-graph-form/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/07/Screen-Shot-2022-07-25-at-2.40.09-PM.png" alt="" width="500" /></p>
<p>Palko <a href="http://observationalepidemiology.blogspot.com/2022/07/thursday-tweets.html">links</a> to <a href="https://twitter.com/russ1mitchell/status/1549553137429598209">this post</a> by Russ Mitchell linking to <a href="https://twitter.com/hassankhan/status/1536794842071588864">this post</a> by Hassan Khan casting deserved shade on this post, &#8220;The Third Transportation Revolution,&#8221; from 2016 by Lyft Co-Founder John Zimmer, which includes the above graph.</p>
<p>What is it about rich guys and <a href="https://statmodeling.stat.columbia.edu/2014/09/09/suspicious-graph-purporting-show-percentage-slaves-serfs-world/">their graphs</a>?</p>
<p><img src="http://statmodeling.stat.columbia.edu/wp-content/uploads/2014/07/slaves-serfs-1024x576.png" alt="slaves-serfs" width="500" /></p>
<p><img src="http://statmodeling.stat.columbia.edu/wp-content/uploads/2016/11/Screen-Shot-2016-11-30-at-12.22.53-PM-1024x809.png" alt="screen-shot-2016-11-30-at-12-22-53-pm" width="500" /></p>
<p>Or is it just a problem with <a href="https://statmodeling.stat.columbia.edu/2014/01/21/commissar-traffic-presents-latest-five-year-plan/">transportation forecasts</a>?</p>
<p><img src="http://statmodeling.stat.columbia.edu/wp-content/uploads/2014/01/VMT-C-P-chart-big1-541x550.png" alt="VMT-C-P-chart-big1-541x550" width="500" /></p>
<p>I&#8217;m tempted to say that taking a silly statement and putting it in graph form makes it more persuasive.  But maybe not.  Maybe the graph thing is just an artifact of the power point era.</p>
<p><strong>Rich guys . . .</strong></p>
<p>I think the other problem is that people give these rich guys a lot of slack because, y&#8217;know, they&#8217;re rich, so they must know what they&#8217;re doing, right?  That&#8217;s not a ridiculous bit of reasoning.  But there are a few complications:</p>
<p>1.  Overconfidence.  You&#8217;re successful so you start to believe your own hype.  It feels good to make big pronouncements, kinda like when Patrick Ewing kept &#8220;guaranteeing&#8221; the Knicks would win.</p>
<p>2.  Luck.  Successful people typically have had some lucky breaks.   It can be natural to attribute that to skill.</p>
<p>3.  Domain specificity.  Skill in one endeavor does not necessarily translate to skill in another.  You might be really skillful at persuading people to invest money in your company, or you might have had some really good ideas for starting a business, but that won&#8217;t necessarily translate into expertise in transportation forecasting.  Indeed, your previous success in other areas might reduce your motivation to check with actual experts before mouthing off.</p>
<p>4.  No safe haven.  As indicated by the last graph above, some of the official transportation experts don&#8217;t know jack.  So it&#8217;s not clear that it would even make sense to consult an official transportation expert before making your forecast.  There&#8217;s no safe play, no good anchor for your forecast, so anything goes.</p>
<p>5.  Selection.  More extreme forecasts get attention.  It&#8217;s the man-bites-dog thing.  We don&#8217;t hear so much about all the non-ridiculous things that people say.</p>
<p>6.  Motivations other than truth.  Without commenting on this particular case, in general people can have financial incentives to take certain positions.  Someone with a lot of money invested in a particular industry will want people to think that this industry has a bright future.  That&#8217;s true of me too:  I want to spread the good news about Stan.</p>
<p>So, yeah, rich people speak with a lot of authority, but we should be careful not to take their <a href="https://statmodeling.stat.columbia.edu/2012/03/27/pushback-against-internet-self-help-gurus/">internet-style confident</a> assertions too seriously.</p>
<p><strong>P.S.</strong>  I have no reason to be believe that rich people make stupider graphs than poor people do.  Richies just have more resources so we all get to see their follies.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/04/dow-36000-but-in-graph-form/feed/</wfw:commentRss>
			<slash:comments>12</slash:comments>
		
		
			</item>
		<item>
		<title>Some things are more difficult than you might think (literary interviewing edition)</title>
		<link>https://statmodeling.stat.columbia.edu/2022/10/03/some-things-are-more-difficult-than-you-might-think-literary-interviewing-edition/</link>
					<comments>https://statmodeling.stat.columbia.edu/2022/10/03/some-things-are-more-difficult-than-you-might-think-literary-interviewing-edition/#comments</comments>
		
		<dc:creator><![CDATA[Andrew]]></dc:creator>
		<pubDate>Mon, 03 Oct 2022 13:02:18 +0000</pubDate>
				<category><![CDATA[Literature]]></category>
		<guid isPermaLink="false">https://statmodeling.stat.columbia.edu/?p=47853</guid>

					<description><![CDATA[Awhile ago I picked up this book, &#8220;The Book That Changed My Life,&#8221; which is a collection of interviews with 15 pretty famous authors (Don DeLillo, Diane Johnson, E. L. Doctorow, Grace Paley, and some other names who many middle-aged &#8230; <a href="https://statmodeling.stat.columbia.edu/2022/10/03/some-things-are-more-difficult-than-you-might-think-literary-interviewing-edition/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
										<content:encoded><![CDATA[<p>Awhile ago I picked up this book, &#8220;The Book That Changed My Life,&#8221; which is a collection of interviews with 15 pretty famous authors (Don DeLillo, Diane Johnson, E. L. Doctorow, Grace Paley, and some other names who many middle-aged and elderly readers of this blog might recognize) on the books that influenced them when they were young, along with more specific questions about influences on their well-known books.  The interviewer, Diane Osen, asks good questions, but the whole thing doesn&#8217;t work for me.  I found the book to be boring and difficult to read.</p>
<p>OK, that&#8217;s just my reaction.  You might love the book.  I&#8217;m not trying to offer any sort of universal take here.  My point really is just that this idea&#8212;find a bunch of authors of interesting literature, ask them some open-ended questions, and edit the results&#8212;seems like a foolproof formula for a book.  And asking about books that changed their lives, that also seems like a great idea, a good way to structure the whole volume.  I love the Paris Review interviews, and this seems kinda similar.</p>
<p>But, no, I guess it&#8217;s harder than it looks.  I&#8217;d like to think that if I could have some time to interview Grace Paley (when she was alive), Don DeLillo, etc., I&#8217;d have material for a fun book, but, no, I guess not.  I guess it takes more than a willing supply of quality interview subjects to put together a readable book of this form.</p>
<p>I like <a href="https://statmodeling.stat.columbia.edu/2015/09/09/irwin-shaw-might-mistrust-intellectuals-id-mistrust-nonintellectuals-even/">interviews with authors</a>, in the same way that I like <a href="https://statmodeling.stat.columbia.edu/2018/05/02/anthony-wests-literary-essays/">book reviews</a> and other <a href="https://statmodeling.stat.columbia.edu/2014/01/12/things-like-almost-nobody-else-interested/">meta-literature</a>, so I&#8217;m really in the sweet spot of the intended audience for this one.  So if this didn&#8217;t interest me, given all it seemed to have going for it, this is evidence that a lot more skill is involved in this endeavor than I&#8217;d imagined.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://statmodeling.stat.columbia.edu/2022/10/03/some-things-are-more-difficult-than-you-might-think-literary-interviewing-edition/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
	</channel>
</rss>
