<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>stat updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Statistics (stat) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>www-admin@arxiv.org</dc:publisher>
<dc:subject>Statistics</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1701.06686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.01711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.07545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.11831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.08844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.11957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.13599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.14860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.04102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.07435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.10726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.12515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.13164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.00292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.10085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.04065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.08031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.09340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.03630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01227" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01429">
<title>Fast Bayesian estimation of brain activation with cortical surface fMRI data using EM. (arXiv:2211.01429v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01429</link>
<description rdf:parseType="Literal">&lt;p&gt;Task functional magnetic resonance imaging (fMRI) is a type of neuroimaging
data used to identify areas of the brain that activate during specific tasks or
stimuli. These data are conventionally modeled using a massive univariate
approach across all data locations, which ignores spatial dependence at the
cost of model power. We previously developed and validated a spatial Bayesian
model leveraging dependencies along the cortical surface of the brain in order
to improve accuracy and power. This model utilizes stochastic partial
differential equation spatial priors with sparse precision matrices to allow
for appropriate modeling of spatially-dependent activations seen in the
neuroimaging literature, resulting in substantial increases in model power. Our
original implementation relies on the computational efficiencies of the
integrated nested Laplace approximation (INLA) to overcome the computational
challenges of analyzing high-dimensional fMRI data while avoiding issues
associated with variational Bayes implementations. However, this requires
significant memory resources, extra software, and software licenses to run. In
this article, we develop an exact Bayesian analysis method for the general
linear model, employing an efficient expectation-maximization algorithm to find
maximum a posteriori estimates of task-based regressors on cortical surface
fMRI data. Through an extensive simulation study of cortical surface-based fMRI
data, we compare our proposed method to the existing INLA implementation, as
well as a conventional massive univariate approach employing ad-hoc spatial
smoothing. We also apply the method to task fMRI data from the Human Connectome
Project and show that our proposed implementation produces similar results to
the validated INLA implementation. Both the INLA and EM-based implementations
are available through our open-source BayesfMRI R package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spencer_D/0/1/0/all/0/1&quot;&gt;Daniel A. Spencer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bolin_D/0/1/0/all/0/1&quot;&gt;David Bolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mejia_A/0/1/0/all/0/1&quot;&gt;Amanda F. Mejia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01457">
<title>Small area estimation using multiple imputation in three-parameter logistic models. (arXiv:2211.01457v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2211.01457</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel methodology relating item response theory methods with
small area estimation strategies in the presence of missing data. Specifically,
we propose an unbiased estimator for the average ability parameter of
three-parameter logistic models. Thus, we carry out an extensive simulation
study in order to compare our estimator with the well-known Horvitz-Thompson
estimator. According to our experiments with synthetic data, our proposal has
substantial lower standard errors than its competitor. In addition, we perform
an actual application by considering the Mathematics results of the 2015
Program for International Student Assessment (PISA), and also, compare our
results with previous analyses. Our findings strongly suggest that our
methodology is a high competitive alternative for generating compelling
official statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tellez_Pinerez_C/0/1/0/all/0/1&quot;&gt;Cristian Tellez-Pi&amp;#xf1;erez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trujillo_L/0/1/0/all/0/1&quot;&gt;Leonardo Trujillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gutierrez_Rojas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Guti&amp;#xe9;rrez-Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sosa_J/0/1/0/all/0/1&quot;&gt;Juan Sosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01492">
<title>Fast, effective, and coherent time series modeling using the sparsity-ranked lasso. (arXiv:2211.01492v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01492</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparsity-ranked lasso (SRL) has been developed for model selection and
estimation in the presence of interactions and polynomials. The main tenet of
the SRL is that an algorithm should be more skeptical of higher-order
polynomials and interactions *a priori* compared to main effects, and hence the
inclusion of these more complex terms should require a higher level of
evidence. In time series, the same idea of ranked prior skepticism can be
applied to the possibly seasonal autoregressive (AR) structure of the series
during the model fitting process, becoming especially useful in settings with
uncertain or multiple modes of seasonality. The SRL can naturally incorporate
exogenous variables, with streamlined options for inference and/or feature
selection. The fitting process is quick even for large series with a
high-dimensional feature set. In this work, we discuss both the formulation of
this procedure and the software we have developed for its implementation via
the **srlTS** R package. We explore the performance of our SRL-based approach
in a novel application involving the autoregressive modeling of hourly
emergency room arrivals at the University of Iowa Hospitals and Clinics. We
find that the SRL is considerably faster than its competitors, while producing
more accurate predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peterson_R/0/1/0/all/0/1&quot;&gt;Ryan Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cavanaugh_J/0/1/0/all/0/1&quot;&gt;Joseph Cavanaugh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01498">
<title>On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach. (arXiv:2211.01498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01498</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable and explainable machine learning has seen a recent surge of
interest. We focus on safety as a key motivation behind the surge and make the
relationship between interpretability and safety more quantitative. Toward
assessing safety, we introduce the concept of maximum deviation via an
optimization problem to find the largest deviation of a supervised learning
model from a reference model regarded as safe. We then show how
interpretability facilitates this safety assessment. For models including
decision trees, generalized linear and additive models, the maximum deviation
can be computed exactly and efficiently. For tree ensembles, which are not
regarded as interpretable, discrete optimization techniques can still provide
informative bounds. For a broader class of piecewise Lipschitz functions, we
leverage the multi-armed bandit literature to show that interpretability
produces tighter (regret) bounds on the maximum deviation. We present case
studies, including one on mortgage approval, to illustrate our methods and the
insights about models that may be obtained from deviation maximization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dennis Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1&quot;&gt;Rahul Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1&quot;&gt;Amit Dhurandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1&quot;&gt;Elizabeth M. Daly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01503">
<title>Jensen&apos;s and Cantelli&apos;s Inequalities with Imprecise Previsions. (arXiv:2211.01503v1 [math.PR])</title>
<link>http://arxiv.org/abs/2211.01503</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate how basic probability inequalities can be extended to an
imprecise framework, where (precise) probabilities and expectations are
replaced by imprecise probabilities and lower/upper previsions. We focus on
inequalities giving information on a single bounded random variable $X$,
considering either convex/concave functions of $X$ (Jensen&apos;s inequalities) or
one-sided bounds such as $(X\geq c)$ or $(X\leq c)$ (Markov&apos;s and Cantelli&apos;s
inequalities). As for the consistency of the relevant imprecise uncertainty
measures, our analysis considers coherence as well as weaker requirements,
notably $2$-coherence, which proves to be often sufficient. Jensen-like
inequalities are introduced, as well as a generalisation of a recent
improvement to Jensen&apos;s inequality. Some of their applications are proposed:
extensions of Lyapunov&apos;s inequality and inferential problems. After discussing
upper and lower Markov&apos;s inequalities, Cantelli-like inequalities are proven
with different degrees of consistency for the related lower/upper previsions.
In the case of coherent imprecise previsions, the corresponding Cantelli&apos;s
inequalities make use of Walley&apos;s lower and upper variances, generally ensuring
better bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pelessoni_R/0/1/0/all/0/1&quot;&gt;Renato Pelessoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vicig_P/0/1/0/all/0/1&quot;&gt;Paolo Vicig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01512">
<title>Convergence in KL Divergence of the Inexact Langevin Algorithm with Application to Score-based Generative Models. (arXiv:2211.01512v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01512</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the Inexact Langevin Algorithm (ILA) for sampling using estimated
score function when the target distribution satisfies log-Sobolev inequality
(LSI), motivated by Score-based Generative Modeling (SGM). We prove a long-term
convergence in Kullback-Leibler (KL) divergence under a sufficient assumption
that the error of the score estimator has a bounded Moment Generating Function
(MGF). Our assumption is weaker than $L^\infty$ (which is too strong to hold in
practice) and stronger than $L^2$ error assumption, which we show not
sufficient to guarantee convergence in general. Under the $L^\infty$ error
assumption, we additionally prove convergence in R\&apos;enyi divergence, which is
stronger than KL divergence. We then study how to get a provably accurate score
estimator which satisfies bounded MGF assumption for LSI target distributions,
by using an estimator based on kernel density estimation. Together with the
convergence results, we yield the first end-to-end convergence guarantee for
ILA in the population level. Last, we generalize our convergence analysis to
SGM and derive a complexity guarantee in KL divergence for data satisfying LSI
under MGF-accurate score estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wibisono_A/0/1/0/all/0/1&quot;&gt;Andre Wibisono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaylee Yingxi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01518">
<title>Bayesian Counterfactual Mean Embeddings and Off-Policy Evaluation. (arXiv:2211.01518v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01518</link>
<description rdf:parseType="Literal">&lt;p&gt;The counterfactual distribution models the effect of the treatment in the
untreated group. While most of the work focuses on the expected values of the
treatment effect, one may be interested in the whole counterfactual
distribution or other quantities associated to it. Building on the framework of
Bayesian conditional mean embeddings, we propose a Bayesian approach for
modeling the counterfactual distribution, which leads to quantifying the
epistemic uncertainty about the distribution. The framework naturally extends
to the setting where one observes multiple treatment effects (e.g. an
intermediate effect after an interim period, and an ultimate treatment effect
which is of main interest) and allows for additionally modelling uncertainty
about the relationship of these effects. For such goal, we present three novel
Bayesian methods to estimate the expectation of the ultimate treatment effect,
when only noisy samples of the dependence between intermediate and ultimate
effects are provided. These methods differ on the source of uncertainty
considered and allow for combining two sources of data. Moreover, we generalize
these ideas to the off-policy evaluation framework, which can be seen as an
extension of the counterfactual estimation problem. We empirically explore the
calibration of the algorithms in two different experimental settings which
require data fusion, and illustrate the value of considering the uncertainty
stemming from the two sources of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martinez_Taboada_D/0/1/0/all/0/1&quot;&gt;Diego Martinez-Taboada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1&quot;&gt;Dino Sejdinovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01521">
<title>Inferring independent sets of Gaussian variables after thresholding correlations. (arXiv:2211.01521v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01521</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider testing whether a set of Gaussian variables, selected from the
data, is independent of the remaining variables. We assume that this set is
selected via a very simple approach that is commonly used across scientific
disciplines: we select a set of variables for which the correlation with all
variables outside the set falls below some threshold. Unlike other settings in
selective inference, failure to account for the selection step leads, in this
setting, to excessively conservative (as opposed to anti-conservative) results.
Our proposed test properly accounts for the fact that the set of variables is
selected from the data, and thus is not overly conservative. To develop our
test, we condition on the event that the selection resulted in the set of
variables in question. To achieve computational tractability, we develop a new
characterization of the conditioning event in terms of the canonical
correlation between the groups of random variables. In simulation studies and
in the analysis of gene co-expression networks, we show that our approach has
much higher power than a ``naive&apos;&apos; approach that ignores the effect of
selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Arkajyoti Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Witten_D/0/1/0/all/0/1&quot;&gt;Daniela Witten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01528">
<title>Fair and Optimal Classification via Transports to Wasserstein-Barycenter. (arXiv:2211.01528v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01528</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in automated decision-making systems has gained increasing attention
as their applications expand to real-world high-stakes domains. To facilitate
the design of fair ML systems, it is essential to understand the potential
trade-offs between fairness and predictive power, and the construction of the
optimal predictor under a given fairness constraint. In this paper, for general
classification problems under the group fairness criterion of demographic
parity (DP), we precisely characterize the trade-off between DP and
classification accuracy, referred to as the minimum cost of fairness. Our
insight comes from the key observation that finding the optimal fair classifier
is equivalent to solving a Wasserstein-barycenter problem under $\ell_1$-norm
restricted to the vertices of the probability simplex. Inspired by our
characterization, we provide a construction of an optimal fair classifier
achieving this minimum cost via the composition of the Bayes regressor and
optimal transports from its output distributions to the barycenter. Our
construction naturally leads to an algorithm for post-processing any
pre-trained predictor to satisfy DP fairness, complemented with finite sample
guarantees. Experiments on real-world datasets verify and demonstrate the
effectiveness of our approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1&quot;&gt;Ruicheng Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Han Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01547">
<title>A Systematic Paradigm for Detecting, Surfacing, and Characterizing Heterogeneous Treatment Effects (HTE). (arXiv:2211.01547v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01547</link>
<description rdf:parseType="Literal">&lt;p&gt;To effectively optimize and personalize treatments, it is necessary to
investigate the heterogeneity of treatment effects. With the wide range of
users being treated over many online controlled experiments, the typical
approach of manually investigating each dimension of heterogeneity becomes
overly cumbersome and prone to subjective human biases. We need an efficient
way to search through thousands of experiments with hundreds of target
covariates and hundreds of breakdown dimensions. In this paper, we propose a
systematic paradigm for detecting, surfacing and characterizing heterogeneous
treatment effects. First, we detect if treatment effect variation is present in
an experiment, prior to specifying any breakdowns. Second, we surface the most
relevant dimensions for heterogeneity. Finally, we characterize the
heterogeneity beyond just the conditional average treatment effects (CATE) by
studying the conditional distributions of the estimated individual treatment
effects. We show the effectiveness of our methods using simulated data and
empirical studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;John Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weinan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01561">
<title>Benefits of Monotonicity in Safe Exploration with Gaussian Processes. (arXiv:2211.01561v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01561</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sequentially maximising an unknown function over a
set of actions while ensuring that every sampled point has a function value
below a given safety threshold. We model the function using kernel-based and
Gaussian process methods, while differing from previous works in our assumption
that the function is monotonically increasing with respect to a safety
variable. This assumption is motivated by various practical applications such
as adaptive clinical trial design and robotics. Taking inspiration from the
GP-UCB and SafeOpt algorithms, we propose an algorithm, monotone safe UCB
(M-SafeUCB) for this task. We show that M-SafeUCB enjoys theoretical guarantees
in terms of safety, a suitably-defined regret notion, and approximately finding
the entire safe boundary. In addition, we illustrate that the monotonicity
assumption yields significant benefits in terms of both the guarantees obtained
and the algorithmic simplicity. We support our theoretical findings by
performing empirical evaluations on a variety of functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Losalka_A/0/1/0/all/0/1&quot;&gt;Arpan Losalka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1&quot;&gt;Jonathan Scarlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01575">
<title>Are Synthetic Control Weights Balancing Score?. (arXiv:2211.01575v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this short note, I outline conditions under which conditioning on
Synthetic Control (SC) weights emulates a randomized control trial where the
treatment status is independent of potential outcomes. Specifically, I
demonstrate that if there exist SC weights such that (i) the treatment effects
are exactly identified and (ii) these weights are uniformly and cumulatively
bounded, then SC weights are balancing scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parikh_H/0/1/0/all/0/1&quot;&gt;Harsh Parikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01588">
<title>A Convergence Theory for Federated Average: Beyond Smoothness. (arXiv:2211.01588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01588</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning enables a large amount of edge computing devices to learn
a model without data sharing jointly. As a leading algorithm in this setting,
Federated Average FedAvg, which runs Stochastic Gradient Descent (SGD) in
parallel on local devices and averages the sequences only once in a while, have
been widely used due to their simplicity and low communication cost. However,
despite recent research efforts, it lacks theoretical analysis under
assumptions beyond smoothness. In this paper, we analyze the convergence of
FedAvg. Different from the existing work, we relax the assumption of strong
smoothness. More specifically, we assume the semi-smoothness and semi-Lipschitz
properties for the loss function, which have an additional first-order term in
assumption definitions. In addition, we also assume bound on the gradient,
which is weaker than the commonly used bounded gradient assumption in the
convergence analysis scheme. As a solution, this paper provides a theoretical
convergence study on Federated Learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Runzhou Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangyi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01591">
<title>A Bayesian Semiparametric Method For Estimating Causal Quantile Effects. (arXiv:2211.01591v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01591</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard causal inference characterizes treatment effect through averages,
but the counterfactual distributions could be different in not only the central
tendency but also spread and shape. To provide a comprehensive evaluation of
treatment effects, we focus on estimating quantile treatment effects (QTEs).
Existing methods that invert a nonsmooth estimator of the cumulative
distribution functions forbid inference on probability density functions
(PDFs), but PDFs can reveal more nuanced characteristics of the counterfactual
distributions. We adopt a semiparametric conditional distribution regression
model that allows inference on any functionals of counterfactual distributions,
including PDFs and multiple QTEs. To account for the observational nature of
the data and ensure an efficient model, we adjust for a double balancing score
that augments the propensity score with individual covariates. We provide a
Bayesian estimation framework that appropriately propagates modeling
uncertainty. We show via simulations that the use of double balancing score for
confounding adjustment improves performance over adjusting for any single score
alone, and the proposed semiparametric model estimates QTEs more accurately
than other semiparametric methods. We apply the proposed method to the North
Carolina birth weight dataset to analyze the effect of maternal smoking on
infant&apos;s birth weight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Steven G. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1&quot;&gt;Brian J. Reich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01610">
<title>Proximal Subgradient Norm Minimization of ISTA and FISTA. (arXiv:2211.01610v1 [math.OC])</title>
<link>http://arxiv.org/abs/2211.01610</link>
<description rdf:parseType="Literal">&lt;p&gt;For first-order smooth optimization, the research on the acceleration
phenomenon has a long-time history. Until recently, the mechanism leading to
acceleration was not successfully uncovered by the gradient correction term and
its equivalent implicit-velocity form. Furthermore, based on the
high-resolution differential equation framework with the corresponding emerging
techniques, phase-space representation and Lyapunov function, the squared
gradient norm of Nesterov&apos;s accelerated gradient descent (\texttt{NAG}) method
at an inverse cubic rate is discovered. However, this result cannot be directly
generalized to composite optimization widely used in practice, e.g., the linear
inverse problem with sparse representation. In this paper, we meticulously
observe a pivotal inequality used in composite optimization about the step size
$s$ and the Lipschitz constant $L$ and find that it can be improved tighter. We
apply the tighter inequality discovered in the well-constructed Lyapunov
function and then obtain the proximal subgradient norm minimization by the
phase-space representation, regardless of gradient-correction or
implicit-velocity. Furthermore, we demonstrate that the squared proximal
subgradient norm for the class of iterative shrinkage-thresholding algorithms
(ISTA) converges at an inverse square rate, and the squared proximal
subgradient norm for the class of faster iterative shrinkage-thresholding
algorithms (FISTA) is accelerated to convergence at an inverse cubic rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bowen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ya-xiang Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01627">
<title>Inverting Regional Sensitivity Analysis to reveal sensitive model behaviors. (arXiv:2211.01627v1 [math.ST])</title>
<link>http://arxiv.org/abs/2211.01627</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the question of sensitivity analysis for model outputs of any
dimension using Regional Sensitivity Analysis (RSA). Classical RSA computes
sensitivity indices related to the impact of model inputs variations on the
occurrence of a target region of the model output space. In this work, we
invert this perspective by proposing to find, for a given target model input,
the region whose occurrence is best explained by the variations of this input.
When it exists, this region can be seen as a model behavior which is
particularly sensitive to the variations of the model input under study. We
name this method iRSA (for inverse RSA). iRSA is formalized as an optimization
problem using region-based sensitivity indices and solved using dedicated
numerical algorithms. Using analytical and numerical examples, including an
environmental model producing time series, we show that iRSA can provide a new
graphical and interpretable characterization of sensitivity for model outputs
of various dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Roux_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Roux&lt;/a&gt; (MISTEA), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loisel_P/0/1/0/all/0/1&quot;&gt;Patrice Loisel&lt;/a&gt; (MISTEA), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Buis_S/0/1/0/all/0/1&quot;&gt;Samuel Buis&lt;/a&gt; (EMMAH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01645">
<title>Towards federated multivariate statistical process control (FedMSPC). (arXiv:2211.01645v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01645</link>
<description rdf:parseType="Literal">&lt;p&gt;The ongoing transition from a linear (produce-use-dispose) to a circular
economy poses significant challenges to current state-of-the-art information
and communication technologies. In particular, the derivation of integrated,
high-level views on material, process, and product streams from (real-time)
data produced along value chains is challenging for several reasons. Most
importantly, sufficiently rich data is often available yet not shared across
company borders because of privacy concerns which make it impossible to build
integrated process models that capture the interrelations between input
materials, process parameters, and key performance indicators along value
chains. In the current contribution, we propose a privacy-preserving, federated
multivariate statistical process control (FedMSPC) framework based on Federated
Principal Component Analysis (PCA) and Secure Multiparty Computation to foster
the incentive for closer collaboration of stakeholders along value chains. We
tested our approach on two industrial benchmark data sets - SECOM and ST-AWFD.
Our empirical results demonstrate the superior fault detection capability of
the proposed approach compared to standard, single-party (multiway) PCA.
Furthermore, we showcase the possibility of our framework to provide
privacy-preserving fault diagnosis to each data holder in the value chain to
underpin the benefits of secure data sharing and federated process modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duy_D/0/1/0/all/0/1&quot;&gt;Du Nguyen Duy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gabauer_D/0/1/0/all/0/1&quot;&gt;David Gabauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikzad_Langerodi_R/0/1/0/all/0/1&quot;&gt;Ramin Nikzad-Langerodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01686">
<title>Principal Balances of Compositional Data for Regression and Classification using Partial Least Squares. (arXiv:2211.01686v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional compositional data are commonplace in the modern omics
sciences amongst others. Analysis of compositional data requires a proper
choice of orthonormal coordinate representation as their relative nature is not
compatible with the direct use of standard statistical methods. Principal
balances, a specific class of log-ratio coordinates, are well suited to this
context since they are constructed in such a way that the first few coordinates
capture most of the variability in the original data. Focusing on regression
and classification problems in high dimensions, we propose a novel Partial
Least Squares (PLS) based procedure to construct principal balances that
maximize explained variability of the response variable and notably facilitates
interpretability when compared to the ordinary PLS formulation. The proposed
PLS principal balance approach can be understood as a generalized version of
common logcontrast models, since multiple orthonormal (instead of one)
logcontrasts are estimated simultaneously. We demonstrate the performance of
the method using both simulated and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nesrstova_V/0/1/0/all/0/1&quot;&gt;V. Nesrstov&amp;#xe1;&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilms_I/0/1/0/all/0/1&quot;&gt;I. Wilms&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Palarea_Albaladejo_J/0/1/0/all/0/1&quot;&gt;J. Palarea-Albaladejo&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Filzmoser_P/0/1/0/all/0/1&quot;&gt;P. Filzmoser&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martin_Fernandez_J/0/1/0/all/0/1&quot;&gt;J. A. Mart&amp;#xed;n-Fern&amp;#xe1;ndez&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Friedecky_D/0/1/0/all/0/1&quot;&gt;D. Friedeck&amp;#xfd;&lt;/a&gt; (5), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hron_K/0/1/0/all/0/1&quot;&gt;K. Hron&lt;/a&gt; (1) ((1) Department of Mathematical Analysis and Applications of Mathematics, Palack&amp;#xfd; University Olomouc, Faculty of Science, the Czech Republic (2) Department of Quantitative Economics, Maastricht University, Maastricht, the Netherlands (3) Department of Computer Science, Applied Mathematics and Statistics, University of Girona, Girona, Spain (4) Department of Statistics and Probability Theory, Vienna University of Technology, Vienna, Austria (5) Laboratory for Inherited Metabolic Disorders, Department of Clinical Biochemistry, University Hospital Olomouc and Faculty of Medicine and Dentistry, Palack&amp;#xfd; University Olomouc, Olomouc, the Czech Republic)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01688">
<title>Nearly tight universal bounds for the binomial tail probabilities. (arXiv:2211.01688v1 [math.PR])</title>
<link>http://arxiv.org/abs/2211.01688</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive simple but nearly tight upper and lower bounds for the binomial
lower tail probability (with straightforward generalization to the upper tail
probability) that apply to the whole parameter regime. These bounds are easy to
compute and are tight within a constant factor of $89/44$. Moreover, they are
asymptotically tight in the regimes of large deviation and moderate deviation.
By virtue of a surprising connection with Ramanujan&apos;s equation, we also provide
strong evidences suggesting that the lower bound is tight within a factor of
$1.26434$. It may even be regarded as the natural lower bound, given its
simplicity and appealing properties. Our bounds significantly outperform the
familiar Chernoff bound and reverse Chernoff bounds known in the literature and
may find applications in various research areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huangjun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hayashi_M/0/1/0/all/0/1&quot;&gt;Masahito Hayashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01689">
<title>Isotropic Gaussian Processes on Finite Spaces of Graphs. (arXiv:2211.01689v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01689</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a principled way to define Gaussian process priors on various sets
of unweighted graphs: directed or undirected, with or without loops. We endow
each of these sets with a geometric structure, inducing the notions of
closeness and symmetries, by turning them into a vertex set of an appropriate
metagraph. Building on this, we describe the class of priors that respect this
structure and are analogous to the Euclidean isotropic processes, like squared
exponential or Mat\&apos;ern. We propose an efficient computational technique for
the ostensibly intractable problem of evaluating these priors&apos; kernels, making
such Gaussian processes usable within the usual toolboxes and downstream
applications. We go further to consider sets of equivalence classes of
unweighted graphs and define the appropriate versions of priors thereon. We
prove a hardness result, showing that in this case, exact kernel computation
cannot be performed efficiently. However, we propose a simple Monte Carlo
approximation for handling moderately sized cases. Inspired by applications in
chemistry, we illustrate the proposed techniques on a real molecular property
prediction task in the small data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1&quot;&gt;Viacheslav Borovitskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimi_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Somnath_V/0/1/0/all/0/1&quot;&gt;Vignesh Ram Somnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01703">
<title>Zero-Sum Games with Noisy Observations. (arXiv:2211.01703v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2211.01703</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, $2 \times 2$ zero-sum games (ZSGs) are studied under the
following assumptions: (1) One of the players (the leader) publicly and
irrevocably commits to choose its actions by sampling a given probability
measure (strategy);(2) The leader announces its action, which is observed by
its opponent (the follower) through a binary channel; and (3) the follower
chooses its strategy based on the knowledge of the leader&apos;s strategy and the
noisy observation of the leader&apos;s action. Under these conditions, the
equilibrium is shown to always exist and be often different from the Nash and
Stackelberg equilibria. Even subject to noise, observing the actions of the
leader is either beneficial or immaterial to the follower for all possible
commitments. When the commitment is observed subject to a distortion, the
equilibrium does not necessarily exist. Nonetheless, the leader might still
obtain some benefit in some specific cases subject to equilibrium refinements.
For instance, $\epsilon$-equilibria might exist in which the leader commits to
suboptimal strategies that allow unequivocally predicting the best response of
its opponent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlaza_S/0/1/0/all/0/1&quot;&gt;Samir M. Perlaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jean_Marie_A/0/1/0/all/0/1&quot;&gt;Alain Jean-Marie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01717">
<title>Learning Hypergraphs From Signals With Dual Smoothness Prior. (arXiv:2211.01717v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01717</link>
<description rdf:parseType="Literal">&lt;p&gt;The construction of a meaningful hypergraph topology is the key to processing
signals with high-order relationships that involve more than two entities.
Learning the hypergraph structure from the observed signals to capture the
intrinsic relationships among the entities becomes crucial when a hypergraph
topology is not readily available in the datasets. There are two challenges
that lie at the heart of this problem: 1) how to handle the huge search space
of potential hyperedges, and 2) how to define meaningful criteria to measure
the relationship between the signals observed on nodes and the hypergraph
structure. In this paper, to address the first challenge, we adopt the
assumption that the ideal hypergraph structure can be derived from a learnable
graph structure that captures the pairwise relations within signals. Further,
we propose a hypergraph learning framework with a novel dual smoothness prior
that reveals a mapping between the observed node signals and the hypergraph
structure, whereby each hyperedge corresponds to a subgraph with both node
signal smoothness and edge signal smoothness in the learnable graph structure.
Finally, we conduct extensive experiments to evaluate the proposed framework on
both synthetic and real world datasets. Experiments show that our proposed
framework can efficiently infer meaningful hypergraph topologies from observed
signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Bohan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaowen Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01720">
<title>Response Times Parametric Estimation of Real-Time Systems. (arXiv:2211.01720v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2211.01720</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time systems are a set of programs, a scheduling policy and a system
architecture, constrained by timing requirements. Most of daily embedded
devices are real-time systems, e.g. airplanes, cars, trains, spatial probes,
etc. The time required by a program for its end-to-end execution is called its
response time. Usually, upper-bounds of response times are computed in order to
provide safe deadline miss probabilities. In this paper, we propose a suited
re-parametrization of the inverse Gaussian mixture distribution adapted to
response times of real-time systems and the estimation of deadline miss
probabilities. The parameters and their associated deadline miss probabilities
are estimated with an adapted Expectation-Maximization algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zagalo_K/0/1/0/all/0/1&quot;&gt;Kevin Zagalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Verbytska_O/0/1/0/all/0/1&quot;&gt;Olena Verbytska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cucu_Grosjean_L/0/1/0/all/0/1&quot;&gt;Liliana Cucu-Grosjean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bar_Hen_A/0/1/0/all/0/1&quot;&gt;Avner Bar-Hen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01727">
<title>Bayesian methods of vector autoregressions with tensor decompositions. (arXiv:2211.01727v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01727</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector autoregressions (VARs) are popular in analyzing economic time series.
However, VARs can be over-parameterized if the numbers of variables and lags
are moderately large. Tensor VAR, a recent solution to overparameterization,
treats the coefficient matrix as a third-order tensor and estimates the
corresponding tensor decomposition to achieve parsimony. In this paper, the
inference of Tensor VARs is inspired by the literature on factor models.
Firstly, we determine the rank by imposing the Multiplicative Gamma Prior to
margins, i.e. elements in the decomposition, and accelerate the computation
with an adaptive inferential scheme. Secondly, to obtain interpretable margins,
we propose an interweaving algorithm to improve the mixing of margins and
introduce a post-processing procedure to solve column permutations and
sign-switching issues. In the application of the US macroeconomic data, our
models outperform standard VARs in point and density forecasting and yield
interpretable results consistent with the US economic history.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yiyong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Griffin_J/0/1/0/all/0/1&quot;&gt;Jim E. Griffin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01743">
<title>Beyond the Best: Estimating Distribution Functionals in Infinite-Armed Bandits. (arXiv:2211.01743v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01743</link>
<description rdf:parseType="Literal">&lt;p&gt;In the infinite-armed bandit problem, each arm&apos;s average reward is sampled
from an unknown distribution, and each arm can be sampled further to obtain
noisy estimates of the average reward of that arm. Prior work focuses on
identifying the best arm, i.e., estimating the maximum of the average reward
distribution. We consider a general class of distribution functionals beyond
the maximum, and propose unified meta algorithms for both the offline and
online settings, achieving optimal sample complexities. We show that online
estimation, where the learner can sequentially choose whether to sample a new
or existing arm, offers no advantage over the offline setting for estimating
the mean functional, but significantly reduces the sample complexity for other
functionals such as the median, maximum, and trimmed mean. The matching lower
bounds utilize several different Wasserstein distances. For the special case of
median estimation, we identify a curious thresholding phenomenon on the
indistinguishability between Gaussian convolutions with respect to the noise
level, which may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baharav_T/0/1/0/all/0/1&quot;&gt;Tavor Baharav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yanjun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1&quot;&gt;David Tse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01746">
<title>Log-density gradient covariance and automatic metric tensors for Riemann manifold Monte Carlo methods. (arXiv:2211.01746v1 [stat.CO])</title>
<link>http://arxiv.org/abs/2211.01746</link>
<description rdf:parseType="Literal">&lt;p&gt;A metric tensor for Riemann manifold Monte Carlo particularly suited for
non-linear Bayesian hierarchical models is proposed. The metric tensor is built
from here proposed symmetric positive semidefinite log-density gradient
covariance (LGC) matrices. The LGCs measure the joint information content and
dependence structure of both a random variable and the parameters of said
variable. The proposed methodology is highly automatic and allows for
exploitation of any sparsity associated with the model in question. When
implemented in conjunction with a Riemann manifold variant of the recently
proposed numerical generalized randomized Hamiltonian Monte Carlo processes,
the proposed methodology is highly competitive, in particular for the more
challenging target distributions associated with Bayesian hierarchical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kleppe_T/0/1/0/all/0/1&quot;&gt;Tore Selland Kleppe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01774">
<title>Jump-Diffusion Langevin Dynamics for Multimodal Posterior Sampling. (arXiv:2211.01774v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01774</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian methods of sampling from a posterior distribution are becoming
increasingly popular due to their ability to precisely display the uncertainty
of a model fit. Classical methods based on iterative random sampling and
posterior evaluation such as Metropolis-Hastings are known to have desirable
long run mixing properties, however are slow to converge. Gradient based
methods, such as Langevin Dynamics (and its stochastic gradient counterpart)
exhibit favorable dimension-dependence and fast mixing times for log-concave,
and &quot;close&quot; to log-concave distributions, however also have long escape times
from local minimizers. Many contemporary applications such as Bayesian Neural
Networks are both high-dimensional and highly multimodal. In this paper we
investigate the performance of a hybrid Metropolis and Langevin sampling method
akin to Jump Diffusion on a range of synthetic and real data, indicating that
careful calibration of mixing sampling jumps with gradient based chains
significantly outperforms both pure gradient-based or sampling based schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guidolin_J/0/1/0/all/0/1&quot;&gt;Jacopo Guidolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kungurtsev_V/0/1/0/all/0/1&quot;&gt;Vyacheslav Kungurtsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Ku&amp;#x17e;elka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01798">
<title>Phase Transitions in Learning and Earning under Price Protection Guarantee. (arXiv:2211.01798v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01798</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the prevalence of ``price protection guarantee&quot;, which allows a
customer who purchased a product in the past to receive a refund from the
seller during the so-called price protection period (typically defined as a
certain time window after the purchase date) in case the seller decides to
lower the price, we study the impact of such policy on the design of online
learning algorithm for data-driven dynamic pricing with initially unknown
customer demand. We consider a setting where a firm sells a product over a
horizon of $T$ time steps. For this setting, we characterize how the value of
$M$, the length of price protection period, can affect the optimal regret of
the learning process. We show that the optimal regret is
$\tilde{\Theta}(\sqrt{T}+\min\{M,\,T^{2/3}\})$ by first establishing a
fundamental impossible regime with novel regret lower bound instances. Then, we
propose LEAP, a phased exploration type algorithm for \underline{L}earning and
\underline{EA}rning under \underline{P}rice Protection to match this lower
bound up to logarithmic factors or even doubly logarithmic factors (when there
are only two prices available to the seller). Our results reveal the surprising
phase transitions of the optimal regret with respect to $M$. Specifically, when
$M$ is not too large, the optimal regret has no major difference when compared
to that of the classic setting with no price protection guarantee. We also show
that there exists an upper limit on how much the optimal regret can deteriorate
when $M$ grows large. Finally, we conduct extensive numerical experiments to
show the benefit of LEAP over other heuristic methods for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jasin_S/0/1/0/all/0/1&quot;&gt;Stefanus Jasin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01799">
<title>Statistical Inference for Scale Mixture Models via Mellin Transform Approach. (arXiv:2211.01799v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01799</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with statistical inference for the scale mixture models. We
study an estimation approach based on the Mellin - Stieltjes transform that can
be applied to both discrete and absolute continuous mixing distributions. The
accuracy of the corresponding estimate is analysed in terms of its expected
pointwise error. As an important technical result, we prove the analogue of the
Berry - Esseen inequality for the Mellin transforms. The proposed statistical
approach is illustrated by numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belomestny_D/0/1/0/all/0/1&quot;&gt;Denis Belomestny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morozova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Morozova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panov_V/0/1/0/all/0/1&quot;&gt;Vladimir Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01832">
<title>Extra-Newton: A First Approach to Noise-Adaptive Accelerated Second-Order Methods. (arXiv:2211.01832v1 [math.OC])</title>
<link>http://arxiv.org/abs/2211.01832</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a universal and adaptive second-order method for
minimizing second-order smooth, convex functions. Our algorithm achieves
$O(\sigma / \sqrt{T})$ convergence when the oracle feedback is stochastic with
variance $\sigma^2$, and improves its convergence to $O( 1 / T^3)$ with
deterministic oracles, where $T$ is the number of iterations. Our method also
interpolates these rates without knowing the nature of the oracle apriori,
which is enabled by a parameter-free adaptive step-size that is oblivious to
the knowledge of smoothness modulus, variance bounds and the diameter of the
constrained set. To our knowledge, this is the first universal algorithm with
such global guarantees within the second-order optimization literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Antonakopoulos_K/0/1/0/all/0/1&quot;&gt;Kimon Antonakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kavis_A/0/1/0/all/0/1&quot;&gt;Ali Kavis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01842">
<title>Towards Discovering Neural Architectures from Scratch. (arXiv:2211.01842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of neural architectures from scratch is the long-standing goal
of Neural Architecture Search (NAS). Searching over a wide spectrum of neural
architectures can facilitate the discovery of previously unconsidered but
well-performing architectures. In this work, we take a large step towards
discovering neural architectures from scratch by expressing architectures
algebraically. This algebraic view leads to a more general method for designing
search spaces, which allows us to compactly represent search spaces that are
100s of orders of magnitude larger than common spaces from the literature.
Further, we propose a Bayesian Optimization strategy to efficiently search over
such huge spaces, and demonstrate empirically that both our search space design
and our search strategy can be superior to existing baselines. We open source
our algebraic NAS approach and provide APIs for PyTorch and TensorFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1&quot;&gt;Simon Schrodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1&quot;&gt;Danny Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1&quot;&gt;Rhea Sukthanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01873">
<title>Port-metriplectic neural networks: thermodynamics-informed machine learning of complex physical systems. (arXiv:2211.01873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01873</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop inductive biases for the machine learning of complex physical
systems based on the port-Hamiltonian formalism. To satisfy by construction the
principles of thermodynamics in the learned physics (conservation of energy,
non-negative entropy production), we modify accordingly the port-Hamiltonian
formalism so as to achieve a port-metriplectic one. We show that the
constructed networks are able to learn the physics of complex systems by parts,
thus alleviating the burden associated to the experimental characterization and
posterior learning process of this kind of systems. Predictions can be done,
however, at the scale of the complete system. Examples are shown on the
performance of the proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Q/0/1/0/all/0/1&quot;&gt;Quercus Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1&quot;&gt;Alberto Bad&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1&quot;&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1&quot;&gt;El&amp;#xed;as Cueto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01877">
<title>Convex Clustering through MM: An Efficient Algorithm to Perform Hierarchical Clustering. (arXiv:2211.01877v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01877</link>
<description rdf:parseType="Literal">&lt;p&gt;Convex clustering is a modern method with both hierarchical and $k$-means
clustering characteristics. Although convex clustering can capture the complex
clustering structure hidden in data, the existing convex clustering algorithms
are not scalable to large data sets with sample sizes greater than ten
thousand. Moreover, it is known that convex clustering sometimes fails to
produce hierarchical clustering structures. This undesirable phenomenon is
called cluster split and makes it difficult to interpret clustering results. In
this paper, we propose convex clustering through majorization-minimization
(CCMM) -- an iterative algorithm that uses cluster fusions and sparsity to
enforce a complete cluster hierarchy with reduced memory usage. In the CCMM
algorithm, the diagonal majorization technique makes a highly efficient update
for each iteration. With a current desktop computer, the CCMM algorithm can
solve a single clustering problem featuring over one million objects in
seven-dimensional space within 70 seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Touw_D/0/1/0/all/0/1&quot;&gt;Daniel J. W. Touw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Groenen_P/0/1/0/all/0/1&quot;&gt;Patrick J. F. Groenen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Terada_Y/0/1/0/all/0/1&quot;&gt;Yoshikazu Terada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01903">
<title>A Consistent Estimator for Confounding Strength. (arXiv:2211.01903v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2211.01903</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression on observational data can fail to capture a causal relationship in
the presence of unobserved confounding. Confounding strength measures this
mismatch, but estimating it requires itself additional assumptions. A common
assumption is the independence of causal mechanisms, which relies on
concentration phenomena in high dimensions. While high dimensions enable the
estimation of confounding strength, they also necessitate adapted estimators.
In this paper, we derive the asymptotic behavior of the confounding strength
estimator by Janzing and Sch\&quot;olkopf (2018) and show that it is generally not
consistent. We then use tools from random matrix theory to derive an adapted,
consistent estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rendsburg_L/0/1/0/all/0/1&quot;&gt;Luca Rendsburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vankadara_L/0/1/0/all/0/1&quot;&gt;Leena Chennuru Vankadara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghoshdastidar_D/0/1/0/all/0/1&quot;&gt;Debarghya Ghoshdastidar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luxburg_U/0/1/0/all/0/1&quot;&gt;Ulrike von Luxburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01938">
<title>betaclust: a family of mixture models for beta valued DNA methylation data. (arXiv:2211.01938v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.01938</link>
<description rdf:parseType="Literal">&lt;p&gt;The DNA methylation process has been extensively studied for its role in
cancer. Promoter cytosine-guanine dinucleotide (CpG) island hypermethylation
has been shown to silence tumour suppressor genes. Identifying the
differentially methylated CpG (DMC) sites between benign and tumour samples can
help understand the disease. The EPIC microarray quantifies the methylation
level at a CpG site as a beta value which lies within [0,1). There is a lack of
suitable methods for modelling the beta values in their innate form. The DMCs
are identified via multiple t-tests but this can be computationally expensive.
Also, arbitrary thresholds are often selected and used to identify the
methylation state of a CpG site. We propose a family of novel beta mixture
models (BMMs) which use a model-based clustering approach to cluster the CpG
sites in their innate beta form to (i) objectively identify methylation state
thresholds and (ii) identify the DMCs between different samples. The family of
BMMs employs different parameter constraints that are applicable to different
study settings. Parameter estimation proceeds via an EM algorithm, with a novel
approximation during the M-step providing tractability and computational
feasibility. Performance of the BMMs is assessed through a thorough simulation
study, and the BMMs are used to analyse a prostate cancer dataset and an
esophageal squamous cell carcinoma dataset. The BMM approach objectively
identifies methylation state thresholds and identifies more DMCs between the
benign and tumour samples in both cancer datasets than conventional methods, in
a computationally efficient manner. The empirical cumulative distribution
function of the DMCs related to genes implicated in carcinogenesis indicates
hypermethylation of CpG sites in the tumour samples in both cancer settings. An
R package betaclust is provided to facilitate the use of the developed BMMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Majumdar_K/0/1/0/all/0/1&quot;&gt;Koyel Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Romina Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Antoinette Sabrina Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Watson_R/0/1/0/all/0/1&quot;&gt;Ronald William Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murphy_T/0/1/0/all/0/1&quot;&gt;Thomas Brendan Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gormley_I/0/1/0/all/0/1&quot;&gt;Isobel Claire Gormley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01939">
<title>Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation. (arXiv:2211.01939v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01939</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of model selection in causal inference, specifically for
the case of conditional average treatment effect (CATE) estimation under binary
treatments. Unlike model selection in machine learning, we cannot use the
technique of cross-validation here as we do not observe the counterfactual
potential outcome for any data point. Hence, we need to design model selection
techniques that do not explicitly rely on counterfactual data. As an
alternative to cross-validation, there have been a variety of proxy metrics
proposed in the literature, that depend on auxiliary nuisance models also
estimated from the data (propensity score model, outcome regression model).
However, the effectiveness of these metrics has only been studied on synthetic
datasets as we can observe the counterfactual data for them. We conduct an
extensive empirical analysis to judge the performance of these metrics, where
we utilize the latest advances in generative modeling to incorporate multiple
realistic datasets. We evaluate 9 metrics on 144 datasets for selecting between
415 estimators per dataset, including datasets that closely mimic real-world
datasets. Further, we use the latest techniques from AutoML to ensure
consistent hyperparameter selection for nuisance models for a fair comparison
across metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1&quot;&gt;Divyat Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1&quot;&gt;Ioannis Mitliagkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neal_B/0/1/0/all/0/1&quot;&gt;Brady Neal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01955">
<title>Quantifying the `end of history&apos; through a Bayesian Markov-chain approach. (arXiv:2211.01955v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2211.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;Political regimes have been changing throughout human history. After the
apparent triumph of liberal democracies at the end of the twentieth century,
Francis Fukuyama and others have been arguing that humankind is approaching an
`end of history&apos; (EoH) in the form of a universality of liberal democracies.
This view has been challenged by recent developments that seem to indicate the
rise of defective democracies across the globe. There has been no attempt to
quantify the expected EoH with a statistical approach. In this study, we model
the transition between political regimes as a Markov process and -- using a
Bayesian inference approach -- we estimate the transition probabilities between
political regimes from time-series data describing the evolution of political
regimes from 1800--2018. We then compute the steady state for this Markov
process which represents a mathematical abstraction of the EoH and predicts
that approximately 46 % of countries will be full democracies. Furthermore, we
find that, under our model, the fraction of autocracies in the world is
expected to increase for the next half-century before it declines. Using
random-walk theory, we then estimate survival curves of different types of
regimes and estimate characteristic lifetimes of democracies and autocracies of
244 years and 69 years, respectively. Quantifying the expected EoH allows us to
challenge common beliefs about the nature of political equilibria.
Specifically, we find no statistical evidence that the EoH constitutes a fixed,
complete omnipresence of democratic regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Klimm_F/0/1/0/all/0/1&quot;&gt;Florian Klimm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01962">
<title>A Posterior Sampling Framework for Interactive Decision Making. (arXiv:2211.01962v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01962</link>
<description rdf:parseType="Literal">&lt;p&gt;We study sample efficient reinforcement learning (RL) under the general
framework of interactive decision making, which includes Markov decision
process (MDP), partially observable Markov decision process (POMDP), and
predictive state representation (PSR) as special cases. Toward finding the
minimum assumption that empowers sample efficient learning, we propose a novel
complexity measure, generalized eluder coefficient (GEC), which characterizes
the fundamental tradeoff between exploration and exploitation in online
interactive decision making. In specific, GEC captures the hardness of
exploration by comparing the error of predicting the performance of the updated
policy with the in-sample training error evaluated on the historical data. We
show that RL problems with low GEC form a remarkably rich class, which subsumes
low Bellman eluder dimension problems, bilinear class, low witness rank
problems, PO-bilinear class, and generalized regular PSR, where generalized
regular PSR, a new tractable PSR class identified by us, includes nearly all
known tractable POMDPs. Furthermore, in terms of algorithm design, we propose a
generic posterior sampling algorithm, which can be implemented in both
model-free and model-based fashion, under both fully observable and partially
observable settings. The proposed algorithm modifies the standard posterior
sampling algorithm in two aspects: (i) we use an optimistic prior distribution
that biases towards hypotheses with higher values and (ii) a loglikelihood
function is set to be the empirical loss evaluated on the historical data,
where the choice of loss function supports both model-free and model-based
learning. We prove that the proposed algorithm is sample efficient by
establishing a sublinear regret upper bound in terms of GEC. In summary, we
provide a new and unified understanding of both fully observable and partially
observable RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Han Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sirui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01972">
<title>The role of prior information and computational power in Machine Learning. (arXiv:2211.01972v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01972</link>
<description rdf:parseType="Literal">&lt;p&gt;Science consists on conceiving hypotheses, confronting them with empirical
evidence, and keeping only hypotheses which have not yet been falsified. Under
deductive reasoning they are conceived in view of a theory and confronted with
empirical evidence in an attempt to falsify it, and under inductive reasoning
they are conceived based on observation, confronted with empirical evidence and
a theory is established based on the not falsified hypotheses. When the
hypotheses testing can be performed with quantitative data, the confrontation
can be achieved with Machine Learning methods, whose quality is highly
dependent on the hypotheses&apos; complexity, hence on the proper insertion of prior
information into the set of hypotheses seeking to decrease its complexity
without loosing good hypotheses. However, Machine Learning tools have been
applied under the pragmatic view of instrumentalism, which is concerned only
with the performance of the methods and not with the understanding of their
behavior, leading to methods which are not fully understood. In this context,
we discuss how prior information and computational power can be employed to
solve a learning problem, but while prior information and a careful design of
the hypotheses space has as advantage the interpretability of the results,
employing high computational power has the advantage of a higher performance.
We discuss why learning methods which combine both should work better from an
understanding and performance perspective, arguing in favor of basic
theoretical research on Machine Learning, in special about how properties of
classifiers may be identified in parameters of modern learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcondes_D/0/1/0/all/0/1&quot;&gt;Diego Marcondes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonis_A/0/1/0/all/0/1&quot;&gt;Adilson Simonis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrera_J/0/1/0/all/0/1&quot;&gt;Junior Barrera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02003">
<title>Single SMPC Invocation DPHelmet: Differentially Private Distributed Learning on a Large Scale. (arXiv:2211.02003v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributing machine learning predictors enables the collection of
large-scale datasets while leaving sensitive raw data at trustworthy sites. We
show that locally training support vector machines (SVMs) and computing their
averages leads to a learning technique that is scalable to a large number of
users, satisfies differential privacy, and is applicable to non-trivial tasks,
such as CIFAR-10. For a large number of participants, communication cost is one
of the main challenges. We achieve a low communication cost by requiring only a
single invocation of an efficient secure multiparty summation protocol. By
relying on state-of-the-art feature extractors (SimCLR), we are able to utilize
differentially private convex learners for non-trivial tasks such as CIFAR-10.
Our experimental results illustrate that for $1{,}000$ users with $50$ data
points each, our scheme outperforms state-of-the-art scalable distributed
learning methods (differentially private federated learning, short DP-FL) while
requiring around $500$ times fewer communication costs: For CIFAR-10, we
achieve a classification accuracy of $79.7\,\%$ for an $\varepsilon = 0.59$
while DP-FL achieves $57.6\,\%$. More generally, we prove learnability
properties for the average of such locally trained models: convergence and
uniform stability. By only requiring strongly convex, smooth, and
Lipschitz-continuous objective functions, locally trained via stochastic
gradient descent (SGD), we achieve a strong utility-privacy tradeoff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschte_M/0/1/0/all/0/1&quot;&gt;Moritz Kirschte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meiser_S/0/1/0/all/0/1&quot;&gt;Sebastian Meiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1&quot;&gt;Esfandiar Mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02020">
<title>Sensitivity of Bayesian Casual Forests to Modeling Choices: A Re-analysis of the 2022 ACIC Data Challenge. (arXiv:2211.02020v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2211.02020</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how Hahn et al.&apos;s Bayesian Causal Forests model (BCF) can be
used to estimate conditional average treatment effects for the longitudinal
dataset in the 2022 American Causal Inference Conference Data Challenge.
Unfortunately, existing implementations of BCF do not scale to the size of the
challenge data. Therefore, we developed flexBCF -- a more scalable and flexible
implementation of BCF -- and used it in our challenge submission. We
investigate the sensitivity of our results to two ad hoc modeling choices we
made during our initial submission: (i) the choice of propensity score
estimation method and (ii) the use of sparsity-inducing regression tree priors.
While we found that our overall point predictions were not especially sensitive
to these modeling choices, we did observe that running BCF with flexibly
estimated propensity scores often yielded better-calibrated uncertainty
intervals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kokandakar_A/0/1/0/all/0/1&quot;&gt;Ajinkya H. Kokandakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hyunseung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deshpande_S/0/1/0/all/0/1&quot;&gt;Sameer K. Deshpande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02023">
<title>Research on the impact of asteroid mining on global equity. (arXiv:2211.02023v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2211.02023</link>
<description rdf:parseType="Literal">&lt;p&gt;In the future situation, aiming to seek more resources, human beings decided
to march towards the mysterious and bright starry sky, which opened the era of
great interstellar exploration. According to the Outer Space Treaty, any
exploration of celestial bodies should be aimed at promoting global equality
and for the benefit of all nations. Firstly, we defined global equity and set a
Unified Equity Index (UEI) model to measure it. We merge the factors with
greater correlation, and finally, get 6 elements, and then use the entropy
method (TEM) to find the dispersion of these elements in different countries.
Then use principal component analysis (PCA) to reduce the dimensionality of the
dispersion, and then use the scandalized index to obtain the global equity.
Secondly, we simulated a future with asteroid mining and evaluated its impact
on Unified Equity Index (UEI). Then, we divided the mineable asteroids into
three classes with different mining difficulties and values, identified 28
mining entities including private companies, national and international
organizations. We considered changes in the asteroid classes, mining
capabilities and mining scales to determine the changes in the value of
minerals mined between 2025 and 2085. We convert mining output value into
mineral transaction value through allocation matrix. Based on grey relational
analysis (GRA). Finally, we presented three possible versions of the future of
asteroid mining by changing the conditions. We propose two sets of
corresponding policies for changes in future trends in global fairness with
asteroid mining. We test the separate and combined effects of these policies
and find that they are positive, strongly supporting the effectiveness of our
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;He Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junfeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02032">
<title>To spike or not to spike: the whims of the Wonham filter in the strong noise regime. (arXiv:2211.02032v1 [math.PR])</title>
<link>http://arxiv.org/abs/2211.02032</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the celebrated Shiryaev-Wonham filter in its historical setup of
Wonham (1964) where the hidden Markov jump process has two states. We are
interested in the weak noise regime for the observation equation.
Interestingly, this becomes a strong noise regime for the filtering equations.
&lt;/p&gt;
&lt;p&gt;Earlier results of the authors show the appearance of spikes in the filtered
process, akin to a metastability phenomenon. This paper is aimed at
understanding the smoothed optimal filter, which is relevant for any system
with feedback. In particular, we demonstrate that there is a sharp phase
transition between a spiking regime and a regime with perfect smoothing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cedric_B/0/1/0/all/0/1&quot;&gt;Bernardin C&amp;#xe9;dric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reda_C/0/1/0/all/0/1&quot;&gt;Chhaibi Reda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Joseph_N/0/1/0/all/0/1&quot;&gt;Najnudel Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Clement_P/0/1/0/all/0/1&quot;&gt;Pellegrini Cl&amp;#xe9;ment&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02039">
<title>The Projected Covariance Measure for assumption-lean variable significance testing. (arXiv:2211.02039v1 [math.ST])</title>
<link>http://arxiv.org/abs/2211.02039</link>
<description rdf:parseType="Literal">&lt;p&gt;Testing the significance of a variable or group of variables $X$ for
predicting a response $Y$, given additional covariates $Z$, is a ubiquitous
task in statistics. A simple but common approach is to specify a linear model,
and then test whether the regression coefficient for $X$ is non-zero. However,
when the model is misspecified, the test may have poor power, for example when
$X$ is involved in complex interactions, or lead to many false rejections. In
this work we study the problem of testing the model-free null of conditional
mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does
not depend on $X$. We propose a simple and general framework that can leverage
flexible nonparametric or machine learning methods, such as additive models or
random forests, to yield both robust error control and high power. The
procedure involves using these methods to perform regressions, first to
estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data,
and then to estimate the expected conditional covariance between this
projection and $Y$ on the remaining half of the data. While the approach is
general, we show that a version of our procedure using spline regression
achieves what we show is the minimax optimal rate in this nonparametric testing
problem. Numerical experiments demonstrate the effectiveness of our approach
both in terms of maintaining Type I error control, and power, compared to
several existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lundborg_A/0/1/0/all/0/1&quot;&gt;Anton Rask Lundborg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Ilmun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajen D. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Samworth_R/0/1/0/all/0/1&quot;&gt;Richard J. Samworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02045">
<title>Fast and robust Bayesian Inference using Gaussian Processes with GPry. (arXiv:2211.02045v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/2211.02045</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the GPry algorithm for fast Bayesian inference of general
(non-Gaussian) posteriors with a moderate number of parameters. GPry does not
need any pre-training, special hardware such as GPUs, and is intended as a
drop-in replacement for traditional Monte Carlo methods for Bayesian inference.
Our algorithm is based on generating a Gaussian Process surrogate model of the
log-posterior, aided by a Support Vector Machine classifier that excludes
extreme or non-finite values. An active learning scheme allows us to reduce the
number of required posterior evaluations by two orders of magnitude compared to
traditional Monte Carlo inference. Our algorithm allows for parallel
evaluations of the posterior at optimal locations, further reducing wall-clock
times. We significantly improve performance using properties of the posterior
in our active learning scheme and for the definition of the GP prior. In
particular we account for the expected dynamical range of the posterior in
different dimensionalities. We test our model against a number of synthetic and
cosmological examples. GPry outperforms traditional Monte Carlo methods when
the evaluation time of the likelihood (or the calculation of theoretical
observables) is of the order of seconds; for evaluation times of over a minute
it can perform inference in days that would take months using traditional
methods. GPry is distributed as an open source Python package (pip install
gpry) and can also be found at https://github.com/jonaselgammal/GPry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gammal_J/0/1/0/all/0/1&quot;&gt;Jonas El Gammal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Schoneberg_N/0/1/0/all/0/1&quot;&gt;Nils Sch&amp;#xf6;neberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Torrado_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s Torrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fidler_C/0/1/0/all/0/1&quot;&gt;Christian Fidler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02046">
<title>Seamless Phase 2-3 Design: A Useful Strategy to Reduce the Sample Size for Dose Optimization. (arXiv:2211.02046v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2211.02046</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional more-is-better dose selection paradigm, developed based on
cytotoxic chemotherapeutics, is often problematic When applied to the
development of novel molecularly targeted agents (e.g., kinase inhibitors,
monoclonal antibodies, and antibody-drug conjugates). The US Food and Drug
Administration (FDA) initiated Project Optimus to reform the dose optimization
and dose selection paradigm in oncology drug development and call for more
attention to benefit-risk consideration.
&lt;/p&gt;
&lt;p&gt;We systematically investigated the operating characteristics of the seamless
phase 2-3 design as a strategy for dose optimization, where in stage 1
(corresponding to phase 2) patients are randomized to multiple doses, with or
without a control; and in stage 2 (corresponding to phase 3) the efficacy of
the selected optimal dose is evaluated with a randomized concurrent control or
historical control. Depending on whether the concurrent control is included and
the type of endpoints used in stages 1 and 2, we describe four types of
seamless phase 2-3 dose-optimization designs, which are suitable for different
clinical settings. The statistical and design considerations that pertain to
dose optimization are discussed. Simulation shows that dose optimization phase
2-3 designs are able to control the familywise type I error rates and yield
appropriate statistical power with substantially smaller sample size than the
conventional approach. The sample size savings range from 16.6% to 27.3%,
depending on the design and scenario, with a mean savings of 22.1%. Due to the
interim dose selection, the phase 2-3 dose-optimization design is logistically
and operationally more challenging, and should be carefully planned and
implemented to ensure trial integrity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liyun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ying Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.06686">
<title>Nested Markov Properties for Acyclic Directed Mixed Graphs. (arXiv:1701.06686v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional independence models associated with directed acyclic graphs
(DAGs) may be characterized in at least three different ways: via a
factorization, the global Markov property (given by the d-separation
criterion), and the local Markov property. Marginals of DAG models also imply
equality constraints that are not conditional independences; the well-known
`Verma constraint&apos; is an example. Constraints of this type are used for testing
edges, and in a computationally efficient marginalization scheme via variable
elimination.
&lt;/p&gt;
&lt;p&gt;We show that equality constraints like the `Verma constraint&apos; can be viewed
as conditional independences in kernel objects obtained from joint
distributions via a fixing operation that generalizes conditioning and
marginalization. We use these constraints to define, via ordered local and
global Markov properties, and a factorization, a graphical model associated
with acyclic directed mixed graphs (ADMGs). We prove that marginal
distributions of DAG models lie in this model, and that a set of these
constraints given by Tian provides an alternative definition of the model.
Finally, we show that the fixing operation used to define the model leads to a
particularly simple characterization of identifiable causal effects in hidden
variable causal DAG models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richardson_T/0/1/0/all/0/1&quot;&gt;Thomas S. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_R/0/1/0/all/0/1&quot;&gt;Robin J. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robins_J/0/1/0/all/0/1&quot;&gt;James M. Robins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.01711">
<title>Dynamic Causal Effects Evaluation in A/B Testing with a Reinforcement Learning Framework. (arXiv:2002.01711v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2002.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;A/B testing, or online experiment is a standard business strategy to compare
a new product with an old one in pharmaceutical, technological, and traditional
industries. Major challenges arise in online experiments of two-sided
marketplace platforms (e.g., Uber) where there is only one unit that receives a
sequence of treatments over time. In those experiments, the treatment at a
given time impacts current outcome as well as future outcomes. The aim of this
paper is to introduce a reinforcement learning framework for carrying A/B
testing in these experiments, while characterizing the long-term treatment
effects. Our proposed testing procedure allows for sequential monitoring and
online updating. It is generally applicable to a variety of treatment designs
in different industries. In addition, we systematically investigate the
theoretical properties (e.g., size and power) of our testing procedure.
Finally, we apply our framework to both simulated data and a real-world data
example obtained from a technological company to illustrate its advantage over
the current practice. A Python implementation of our test is available at
https://github.com/callmespring/CausalRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chengchun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shikai Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jieping Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Rui Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.07545">
<title>Interpretable Personalization via Policy Learning with Linear Decision Boundaries. (arXiv:2003.07545v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2003.07545</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of the digital economy and an explosion of available
information about consumers, effective personalization of goods and services
has become a core business focus for companies to improve revenues and maintain
a competitive edge. This paper studies the personalization problem through the
lens of policy learning, where the goal is to learn a decision-making rule (a
policy) that maps from consumer and product characteristics (features) to
recommendations (actions) in order to optimize outcomes (rewards). We focus on
using available historical data for offline learning with unknown data
collection procedures, where a key challenge is the non-random assignment of
recommendations. Moreover, in many business and medical applications,
interpretability of a policy is essential. We study the class of policies with
linear decision boundaries to ensure interpretability, and propose learning
algorithms using tools from causal inference to address unbalanced treatments.
We study several optimization schemes to solve the associated non-convex,
non-smooth optimization problem, and find that a Bayesian optimization
algorithm is effective. We test our algorithm with extensive simulation studies
and apply it to an anonymized online marketplace customer purchase dataset,
where the learned policy outputs a personalized discount recommendation based
on customer and product features in order to maximize gross merchandise value
(GMV) for sellers. Our learned policy improves upon the platform&apos;s baseline by
88.2\% in net sales revenue, while also providing informative insights on which
features are important for the decision-making process. Our findings suggest
that our proposed policy learning framework using tools from causal inference
and Bayesian optimization provides a promising practical approach to
interpretable personalization across a wide range of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1&quot;&gt;Zhaonan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_I/0/1/0/all/0/1&quot;&gt;Isabella Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.11831">
<title>DBS: Dynamic Batch Size For Distributed Deep Neural Network Training. (arXiv:2007.11831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2007.11831</link>
<description rdf:parseType="Literal">&lt;p&gt;Synchronous strategies with data parallelism, such as the Synchronous
StochasticGradient Descent (S-SGD) and the model averaging methods, are widely
utilizedin distributed training of Deep Neural Networks (DNNs), largely owing
to itseasy implementation yet promising performance. Particularly, each worker
ofthe cluster hosts a copy of the DNN and an evenly divided share of the
datasetwith the fixed mini-batch size, to keep the training of DNNs
convergence. In thestrategies, the workers with different computational
capability, need to wait foreach other because of the synchronization and
delays in network transmission,which will inevitably result in the
high-performance workers wasting computation.Consequently, the utilization of
the cluster is relatively low. To alleviate thisissue, we propose the Dynamic
Batch Size (DBS) strategy for the distributedtraining of DNNs. Specifically,
the performance of each worker is evaluatedfirst based on the fact in the
previous epoch, and then the batch size and datasetpartition are dynamically
adjusted in consideration of the current performanceof the worker, thereby
improving the utilization of the cluster. To verify theeffectiveness of the
proposed strategy, extensive experiments have been conducted,and the
experimental results indicate that the proposed strategy can fully utilizethe
performance of the cluster, reduce the training time, and have good
robustnesswith disturbance by irrelevant tasks. Furthermore, rigorous
theoretical analysis hasalso been provided to prove the convergence of the
proposed strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Mingjia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.08844">
<title>Complete the Missing Half: Augmenting Aggregation Filtering with Diversification for Graph Convolutional Networks. (arXiv:2008.08844v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2008.08844</link>
<description rdf:parseType="Literal">&lt;p&gt;The core operation of current Graph Neural Networks (GNNs) is the aggregation
enabled by the graph Laplacian or message passing, which filters the
neighborhood node information. Though effective for various tasks, in this
paper, we show that they are potentially a problematic factor underlying all
GNN methods for learning on certain datasets, as they force the node
representations similar, making the nodes gradually lose their identity and
become indistinguishable. Hence, we augment the aggregation operations with
their dual, i.e. diversification operators that make the node more distinct and
preserve the identity. Such augmentation replaces the aggregation with a
two-channel filtering process that, in theory, is beneficial for enriching the
node representations. In practice, the proposed two-channel filters can be
easily patched on existing GNN methods with diverse training strategies,
including spectral and spatial (message passing) methods. In the experiments,
we observe desired characteristics of the models and significant performance
boost upon the baselines on 9 node classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1&quot;&gt;Sitao Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mingde Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1&quot;&gt;Chenqing Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiao-Wen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.11957">
<title>Analytical and statistical properties of local depth functions motivated by clustering applications. (arXiv:2008.11957v5 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2008.11957</link>
<description rdf:parseType="Literal">&lt;p&gt;Local general depth ($LGD$) functions are used for describing the local
geometric features and mode(s) in multivariate distributions. In this paper, we
undertake a rigorous systematic study of $LGD$ and establish several analytical
and statistical properties. First, we show that, when the underlying
probability distribution is absolutely continuous with density $f(\cdot)$, the
scaled version of $LGD$ (referred to as $\tau$-approximation) converges,
uniformly and in $L^d(\mathbb{R}^p)$ to $f(\cdot)$ when $\tau$ converges to
zero. Second, we establish that, as the sample size diverges to infinity the
centered and scaled sample $LGD$ converge in distribution to a centered
Gaussian process uniformly in the space of bounded functions on
$\mathcal{H}_G$, a class of functions yielding $LGD$. Third, using the sample
version of the $\tau$-approximation ($S \tau A$) and the gradient system
analysis, we develop a new clustering algorithm. The validity of this algorithm
requires several results concerning the uniform finite difference approximation
of the gradient system associated with $S \tau A$. For this reason, we
establish \emph{Bernstein}-type inequality for deviations between the centered
and scaled sample $LGD$, which is also of independent interest. Finally,
invoking the above results, we establish consistency of the clustering
algorithm. Applications of the proposed methods to mode estimation and upper
level set estimation are also provided. Finite sample performance of the
methodology are evaluated using numerical experiments and data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Francisci_G/0/1/0/all/0/1&quot;&gt;Giacomo Francisci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Agostinelli_C/0/1/0/all/0/1&quot;&gt;Claudio Agostinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nieto_Reyes_A/0/1/0/all/0/1&quot;&gt;Alicia Nieto-Reyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vidyashankar_A/0/1/0/all/0/1&quot;&gt;Anand N. Vidyashankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.13599">
<title>Design-Based Inference for Spatial Experiments with Interference. (arXiv:2010.13599v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2010.13599</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider design-based causal inference in settings where randomized
treatments have effects that bleed out into space in complex ways that overlap
and in violation of the standard &quot;no interference&quot; assumption for many causal
inference methods. We define a spatial &quot;average marginalized response,&quot; which
characterizes how, in expectation, units of observation that are a specified
distance from an intervention point are affected by treatments at that point,
averaging over effects emanating from other intervention points. We establish
conditions for non-parametric identification, asymptotic distributions of
estimators, and recovery of structural effects. We propose methods for both
sample-theoretic and permutation-based inference. We provide illustrations
using randomized field experiments on forest conservation and health.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Samii_C/0/1/0/all/0/1&quot;&gt;Cyrus Samii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haoge Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aronow_P/0/1/0/all/0/1&quot;&gt;P.M. Aronow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.14860">
<title>The Evidence Lower Bound of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2010.14860</link>
<description rdf:parseType="Literal">&lt;p&gt;The central objective function of a variational autoencoder (VAE) is its
variational lower bound (the ELBO). Here we show that for standard (i.e.,
Gaussian) VAEs the ELBO converges to a value given by the sum of three
entropies: the (negative) entropy of the prior distribution, the expected
(negative) entropy of the observable distribution, and the average entropy of
the variational distributions (the latter is already part of the ELBO). Our
derived analytical results are exact and apply for small as well as for
intricate deep networks for encoder and decoder. Furthermore, they apply for
finitely and infinitely many data points and at any stationary point (including
local maxima and saddle points). The result implies that the ELBO can for
standard VAEs often be computed in closed-form at stationary points while the
original ELBO requires numerical approximations of integrals. As a main
contribution, we provide the proof that the ELBO for VAEs is at stationary
points equal to entropy sums. Numerical experiments then show that the obtained
analytical results are sufficiently precise also in those vicinities of
stationary points that are reached in practice. Furthermore, we discuss how the
novel entropy form of the ELBO can be used to analyze and understand learning
behavior. More generally, we believe that our contributions can be useful for
future theoretical and practical studies on VAE learning as they provide novel
information on those points in parameters space that optimization of VAEs
converges to.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damm_S/0/1/0/all/0/1&quot;&gt;Simon Damm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Forster_D/0/1/0/all/0/1&quot;&gt;Dennis Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Velychko_D/0/1/0/all/0/1&quot;&gt;Dmytro Velychko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg L&amp;#xfc;cke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.04102">
<title>Reliable Off-policy Evaluation for Reinforcement Learning. (arXiv:2011.04102v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.04102</link>
<description rdf:parseType="Literal">&lt;p&gt;In a sequential decision-making problem, off-policy evaluation estimates the
expected cumulative reward of a target policy using logged trajectory data
generated from a different behavior policy, without execution of the target
policy. Reinforcement learning in high-stake environments, such as healthcare
and education, is often limited to off-policy settings due to safety or ethical
concerns, or inability of exploration. Hence it is imperative to quantify the
uncertainty of the off-policy estimate before deployment of the target policy.
In this paper, we propose a novel framework that provides robust and optimistic
cumulative reward estimates using one or multiple logged trajectories data.
Leveraging methodologies from distributionally robust optimization, we show
that with proper selection of the size of the distributional uncertainty set,
these estimates serve as confidence bounds with non-asymptotic and asymptotic
guarantees under stochastic or adversarial environments. Our results are also
generalized to batch reinforcement learning and are supported by empirical
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.07435">
<title>Functorial Manifold Learning. (arXiv:2011.07435v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.07435</link>
<description rdf:parseType="Literal">&lt;p&gt;We adapt previous research on category theory and topological unsupervised
learning to develop a functorial perspective on manifold learning, also known
as nonlinear dimensionality reduction. We first characterize manifold learning
algorithms as functors that map pseudometric spaces to optimization objectives
and that factor through hierarchical clustering functors. We then use this
characterization to prove refinement bounds on manifold learning loss functions
and construct a hierarchy of manifold learning algorithms based on their
equivariants. We express several popular manifold learning algorithms as
functors at different levels of this hierarchy, including Metric
Multidimensional Scaling, IsoMap, and UMAP. Next, we use interleaving distance
to study the stability of a broad class of manifold learning algorithms. We
present bounds on how closely the embeddings these algorithms produce from
noisy data approximate the embeddings they would learn from noiseless data.
Finally, we use our framework to derive a set of novel manifold learning
algorithms, which we experimentally demonstrate are competitive with the state
of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1&quot;&gt;Dan Shiebler&lt;/a&gt; (University of Oxford)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.08013">
<title>What makes you unique?. (arXiv:2105.08013v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2105.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a uniqueness Shapley measure to compare the extent to
which different variables are able to identify a subject. Revealing the value
of a variable on subject $t$ shrinks the set of possible subjects that $t$
could be. The extent of the shrinkage depends on which other variables have
also been revealed. We use Shapley value to combine all of the reductions in
log cardinality due to revealing a variable after some subset of the other
variables has been revealed. This uniqueness Shapley measure can be aggregated
over subjects where it becomes a weighted sum of conditional entropies.
Aggregation over subsets of subjects can address questions like how identifying
is age for people of a given zip code. Such aggregates have a corresponding
expression in terms of cross entropies. We use uniqueness Shapley to
investigate the differential effects of revealing variables from the North
Carolina voter registration rolls and in identifying anomalous solar flares. An
enormous speedup (approaching 2000 fold in one example) is obtained by using
the all dimension trees of Moore and Lee (1998) to store the cardinalities we
need.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seiler_B/0/1/0/all/0/1&quot;&gt;Benjamin B. Seiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mase_M/0/1/0/all/0/1&quot;&gt;Masayoshi Mase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Owen_A/0/1/0/all/0/1&quot;&gt;Art B. Owen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.10726">
<title>A class of smooth, possibly data-adaptive nonparametric copula estimators containing the empirical beta copula. (arXiv:2106.10726v5 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2106.10726</link>
<description rdf:parseType="Literal">&lt;p&gt;A broad class of smooth, possibly data-adaptive nonparametric copula
estimators that contains empirical Bernstein copulas introduced by Sancetta and
Satchell (and thus the empirical beta copula proposed by Segers, Sibuya and
Tsukahara) is studied. Within this class, a subclass of estimators that depend
on a scalar parameter determining the amount of marginal smoothing and a
functional parameter controlling the shape of the smoothing region is
specifically considered. Empirical investigations of the influence of these
parameters suggest to focus on two particular data-adaptive smooth copula
estimators that were found to be uniformly better than the empirical beta
copula in all of the considered Monte Carlo experiments. Finally, with future
applications to change-point detection in mind, conditions under which related
sequential empirical copula processes converge weakly are provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kojadinovic_I/0/1/0/all/0/1&quot;&gt;Ivan Kojadinovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yi_B/0/1/0/all/0/1&quot;&gt;Bingqing Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.12515">
<title>Convergence Rates for Learning Linear Operators from Noisy Data. (arXiv:2108.12515v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2108.12515</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the learning of linear operators between
infinite-dimensional Hilbert spaces. The training data comprises pairs of
random input vectors in a Hilbert space and their noisy images under an unknown
self-adjoint linear operator. Assuming that the operator is diagonalizable in a
known basis, this work solves the equivalent inverse problem of estimating the
operator&apos;s eigenvalues given the data. Adopting a Bayesian approach, the
theoretical analysis establishes posterior contraction rates in the infinite
data limit with Gaussian priors that are not directly linked to the forward map
of the inverse problem. The main results also include learning-theoretic
generalization error guarantees for a wide range of distribution shifts. These
convergence rates quantify the effects of data smoothness and true eigenvalue
decay or growth, for compact or unbounded operators, respectively, on sample
complexity. Numerical evidence supports the theory in diagonal and non-diagonal
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hoop_M/0/1/0/all/0/1&quot;&gt;Maarten V. de Hoop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1&quot;&gt;Nikola B. Kovachki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nelsen_N/0/1/0/all/0/1&quot;&gt;Nicholas H. Nelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1&quot;&gt;Andrew M. Stuart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.13164">
<title>Neural network stochastic differential equation models with applications to financial data forecasting. (arXiv:2111.13164v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.13164</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we employ a collection of stochastic differential equations
with drift and diffusion coefficients approximated by neural networks to
predict the trend of chaotic time series which has big jump properties. Our
contributions are, first, we propose a model called L\&apos;evy induced stochastic
differential equation network, which explores compounded stochastic
differential equations with $\alpha$-stable L\&apos;evy motion to model complex time
series data and solve the problem through neural network approximation. Second,
we theoretically prove that the numerical solution through our algorithm
converges in probability to the solution of corresponding stochastic
differential equation, without curse of dimensionality. Finally, we illustrate
our method by applying it to real financial time series data and find the
accuracy increases through the use of non-Gaussian L\&apos;evy processes. We also
present detailed comparisons in terms of data patterns, various models,
different shapes of L\&apos;evy motion and the prediction lengths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Luxuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Ting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yubin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinqiao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.00292">
<title>Fair Data Representation for Machine Learning at the Pareto Frontier. (arXiv:2201.00292v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2201.00292</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning powered decision making is playing an increasingly
important role in our daily lives, it is imperative to strive for fairness of
the underlying data processing and algorithms. We propose a pre-processing
algorithm for fair data representation via which L2- objective supervised
learning algorithms result in an estimation of the Pareto frontier between
prediction error and statistical disparity. In particular, the present work
applies the optimal positive definite affine transport maps to approach the
post-processing Wasserstein barycenter characterization of the optimal fair
L2-objective supervised learning via a pre-processing data deformation. We call
the resulting data Wasserstein pseudo-barycenter. Furthermore, we show that the
Wasserstein geodesics from the learning outcome marginals to the barycenter
characterizes the Pareto frontier between L2-loss and total Wasserstein
distance among learning outcome marginals. Thereby, an application of McCann
interpolation generalizes the pseudo-barycenter to a family of data
representations via which L2-objective supervised learning algorithms result in
the Pareto frontier. Numerical simulations underscore the advantages of the
proposed data representation: (1) the pre-processing step is compositive with
arbitrary L2-objective supervised learning methods and unseen data; (2) the
fair representation protects data privacy by preventing the training machine
from direct or indirect access to the sensitive information of the data; (3)
the optimal affine map results in efficient computation of fair supervised
learning on high-dimensional data; (4) experimental results shed light on the
fairness of L2-objective unsupervised learning via the proposed fair data
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shizhou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1&quot;&gt;Thomas Strohmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.10085">
<title>Bettors&apos; reaction to match dynamics -- Evidence from in-game betting. (arXiv:2202.10085v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2202.10085</link>
<description rdf:parseType="Literal">&lt;p&gt;It is still largely unclear to what extent bettors update their prior
assumptions about the strength and form of competing teams considering the
dynamics during the match. This is of interest not only from the psychological
perspective, but also as the pricing of live odds ideally should be driven both
by the (objective) outcome probabilities and also the bettors&apos; behaviour. Using
state-space models (SSMs) to account for the dynamically evolving latent
sentiment of the betting market, we analyse a unique high-frequency data set on
stakes placed during the match. We find that stakes in the live-betting market
are driven both by perceived pre-game strength and by in-game strength, the
latter as measured by the Valuing Actions by Estimating Probabilities (VAEP)
approach. Both effects vary over the course of the match.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michels_R/0/1/0/all/0/1&quot;&gt;Rouven Michels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Otting_M/0/1/0/all/0/1&quot;&gt;Marius &amp;#xd6;tting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langrock_R/0/1/0/all/0/1&quot;&gt;Roland Langrock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.04065">
<title>Honest calibration assessment for binary outcome predictions. (arXiv:2203.04065v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2203.04065</link>
<description rdf:parseType="Literal">&lt;p&gt;Probability predictions from binary regressions or machine learning methods
ought to be calibrated: If an event is predicted to occur with probability $x$,
it should materialize with approximately that frequency, which means that the
so-called calibration curve $p(\cdot)$ should equal the identity, $p(x) = x$
for all $x$ in the unit interval. We propose honest calibration assessment
based on novel confidence bands for the calibration curve, which are valid only
subject to the natural assumption of isotonicity. Besides testing the classical
goodness-of-fit null hypothesis of perfect calibration, our bands facilitate
inverted goodness-of-fit tests whose rejection allows for the sought-after
conclusion of a sufficiently well specified model. We show that our bands have
a finite sample coverage guarantee, are narrower than existing approaches, and
adapt to the local smoothness of the calibration curve $p$ and the local
variance of the binary observations. In an application to model predictions of
an infant having a low birth weight, the bounds give informative insights on
model calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dimitriadis_T/0/1/0/all/0/1&quot;&gt;Timo Dimitriadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duembgen_L/0/1/0/all/0/1&quot;&gt;Lutz Duembgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Henzi_A/0/1/0/all/0/1&quot;&gt;Alexander Henzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Puke_M/0/1/0/all/0/1&quot;&gt;Marius Puke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ziegel_J/0/1/0/all/0/1&quot;&gt;Johanna Ziegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.08031">
<title>Limit theorems of Chatterjee&apos;s rank correlation. (arXiv:2204.08031v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2204.08031</link>
<description rdf:parseType="Literal">&lt;p&gt;Establishing the limiting distribution of Chatterjee&apos;s rank correlation for a
general, possibly non-independent, pair of random variables has been eagerly
awaited to many. This paper shows that (a) Chatterjee&apos;s rank correlation is
asymptotically normal as long as one variable is not a measurable function of
the other, (b) the corresponding asymptotic variance is uniformly bounded by
36, and (c) a consistent variance estimator exists. Similar results also hold
for Azadkia-Chatterjee&apos;s graph-based correlation coefficient, a multivariate
analogue of Chatterjee&apos;s original proposal. The proof is given by appealing to
H\&apos;ajek representation and Chatterjee&apos;s nearest-neighbor CLT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhexiao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Han_F/0/1/0/all/0/1&quot;&gt;Fang Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04399">
<title>Nonparametric estimation of the incubation time distribution. (arXiv:2205.04399v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04399</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonparametric maximum likelihood estimators (MLEs) in inverse problems often
have non-normal limit distributions, like Chernoff&apos;s distribution. However, if
one considers smooth functionals of the model, with corresponding functionals
of the MLE, one gets normal limit distributions and faster rates of
convergence. We demonstrate this for a model for the incubation time of a
disease. The usual approach in the latter models is to use parametric
distributions, like Weibull and gamma distributions, which leads to
inconsistent estimators. Smoothed bootstrap methods are discussed for
constructing confidence intervals. The classical bootstrap, based on the
nonparametric MLE itself, has been proved to be inconsistent in this situation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Groeneboom_P/0/1/0/all/0/1&quot;&gt;Piet Groeneboom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04539">
<title>Efficient algorithms for building representative matched pairs with enhanced generalizability. (arXiv:2205.04539v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04539</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent efforts center on assessing the ability of real-world evidence
(RWE) generated from non-randomized, observational data to produce results
compatible with those from randomized controlled trials (RCTs). One noticeable
endeavor is the RCT DUPLICATE initiative (Franklin et al., 2020, 2021). To
better reconcile findings from an observational study and an RCT, or two
observational studies based on different databases, it is desirable to
eliminate differences between study populations. We outline an efficient,
network-flow-based statistical matching algorithm that designs well-matched
pairs from observational data that resemble the covariate distributions of a
target population, for instance, the target-RCT-eligible population in the RCT
DUPLICATE initiative studies or a generic population of scientific interest. We
demonstrate the usefulness of the method by revisiting the inconsistency
regarding a cardioprotective effect of the hormone replacement therapy (HRT) in
the Women&apos;s Health Initiative (WHI) clinical trial and corresponding
observational study. We found that the discrepancy between the trial and
observational study persisted in a design that adjusted for study populations&apos;
cardiovascular risk profile, but seemed to disappear in a study design that
further adjusted for the HRT initiation age and previous
estrogen-plus-progestin use. The proposed method is integrated into the R
package match2C.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13255">
<title>Active Labeling: Streaming Stochastic Gradients. (arXiv:2205.13255v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13255</link>
<description rdf:parseType="Literal">&lt;p&gt;The workhorse of machine learning is stochastic gradient descent. To access
stochastic gradients, it is common to consider iteratively input/output pairs
of a training dataset. Interestingly, it appears that one does not need full
supervision to access stochastic gradients, which is the main motivation of
this paper. After formalizing the &quot;active labeling&quot; problem, which focuses on
active learning with partial supervision, we provide a streaming technique that
provably minimizes the ratio of generalization error over the number of
samples. We illustrate our technique in depth for robust regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1&quot;&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1&quot;&gt;Vianney Perchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15856">
<title>coVariance Neural Networks. (arXiv:2205.15856v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15856</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNN) are an effective framework that exploit
inter-relationships within graph-structured data for learning. Principal
component analysis (PCA) involves the projection of data on the eigenspace of
the covariance matrix and draws similarities with the graph convolutional
filters in GNNs. Motivated by this observation, we study a GNN architecture,
called coVariance neural network (VNN), that operates on sample covariance
matrices as graphs. We theoretically establish the stability of VNNs to
perturbations in the covariance matrix, thus, implying an advantage over
standard PCA-based data analysis approaches that are prone to instability due
to principal components associated with close eigenvalues. Our experiments on
real-world datasets validate our theoretical results and show that VNN
performance is indeed more stable than PCA-based statistical approaches.
Moreover, our experiments on multi-resolution datasets also demonstrate that
VNNs are amenable to transferability of performance over covariance matrices of
different dimensions; a feature that is infeasible for PCA-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sihag_S/0/1/0/all/0/1&quot;&gt;Saurabh Sihag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mateos_G/0/1/0/all/0/1&quot;&gt;Gonzalo Mateos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMillan_C/0/1/0/all/0/1&quot;&gt;Corey McMillan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04805">
<title>Monitoring pollution pathways in river water by predictive path modelling using untargeted GC-MS measurements. (arXiv:2207.04805v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04805</link>
<description rdf:parseType="Literal">&lt;p&gt;A comprehensive approach to protect river water quality is needed within the
European Water Framework Directive. Non-target screening of a complete chemical
fingerprint of the aquatic ecosystem is essential, to identify chemicals of
emerging concern and to reveal their suspicious dynamic patterns in river
water. This requires a new combination of two measurement paradigms: the path
of potential pollution should be traced through the river network, while there
may be many compounds that make up this chemical composition - both known and
unknown. Dedicated data processing of ongoing GC-MS measurements at 9 sites
along the Rhine using PARAFAC2 for non-target screening, combined with
spatiotemporal modelling of these sites within the river network using path
modelling (Process PLS), provided a new integrated approach to track chemicals
through the Rhine catchment, and tentatively identify known and as-yet unknown
potential pollutants based on non-target screening and spatiotemporal
behaviour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cairoli_M/0/1/0/all/0/1&quot;&gt;Maria Cairoli&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doel_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; van den Doel&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Postma_B/0/1/0/all/0/1&quot;&gt;Berber Postma&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Offermans_T/0/1/0/all/0/1&quot;&gt;Tim Offermans&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zemmelink_H/0/1/0/all/0/1&quot;&gt;Henk Zemmelink&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stroomberg_G/0/1/0/all/0/1&quot;&gt;Gerard Stroomberg&lt;/a&gt; (1 and 3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buydens_L/0/1/0/all/0/1&quot;&gt;Lutgarde Buydens&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kollenburg_G/0/1/0/all/0/1&quot;&gt;Geert van Kollenburg&lt;/a&gt; (1 and 4), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jansen_J/0/1/0/all/0/1&quot;&gt;Jeroen Jansen&lt;/a&gt; (1) ((1) Radboud University, Department of Analytical Chemistry &amp;amp; Chemometrics, Nijmegen, The Netherlands (2) Rijkswaterstaat, Lelystad, The Netherlands (3) RIWA Rijn, Nieuwegein, The Netherlands (4) Eindhoven University of Technology, Interconnected Resource-aware Intelligent Systems, Eindhoven, The Netherlands)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.09340">
<title>A coherence parameter characterizing generative compressed sensing with Fourier measurements. (arXiv:2207.09340v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2207.09340</link>
<description rdf:parseType="Literal">&lt;p&gt;In Bora et al. (2017), a mathematical framework was developed for compressed
sensing guarantees in the setting where the measurement matrix is Gaussian and
the signal structure is the range of a generative neural network (GNN). The
problem of compressed sensing with GNNs has since been extensively analyzed
when the measurement matrix and/or network weights follow a subgaussian
distribution. We move beyond the subgaussian assumption, to measurement
matrices that are derived by sampling uniformly at random rows of a unitary
matrix (including subsampled Fourier measurements as a special case).
Specifically, we prove the first known restricted isometry guarantee for
generative compressed sensing with subsampled isometries, and provide recovery
bounds with nearly order-optimal sample complexity, addressing an open problem
of Scarlett et al. (2022, p. 10). Recovery efficacy is characterized by the
coherence, a new parameter, which measures the interplay between the range of
the network and the measurement matrix. Our approach relies on subspace
counting arguments and ideas central to high-dimensional probability.
Furthermore, we propose a regularization strategy for training GNNs to have
favourable coherence with the measurement operator. We provide compelling
numerical simulations that support this regularized training strategy: our
strategy yields low coherence networks that require fewer measurements for
signal recovery. This, together with our theoretical results, supports
coherence as a natural quantity for characterizing generative compressed
sensing with subsampled isometries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berk_A/0/1/0/all/0/1&quot;&gt;Aaron Berk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brugiapaglia_S/0/1/0/all/0/1&quot;&gt;Simone Brugiapaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1&quot;&gt;Babhru Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plan_Y/0/1/0/all/0/1&quot;&gt;Yaniv Plan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1&quot;&gt;Matthew Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yilmaz_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;zg&amp;#xfc;r Yilmaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.03630">
<title>Generalized Estimators, Slope, Efficiency, and Fisher Information Bounds. (arXiv:2208.03630v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2208.03630</link>
<description rdf:parseType="Literal">&lt;p&gt;Point estimators may not exist, need not be unique, and their distributions
are not parameter invariant. Generalized estimators provide distributions that
are parameter invariant, unique, and exist when point estimates do not.
Comparing point estimators using variance is less useful when estimators are
biased. A squared slope $\Lambda$ is defined that can be used to compare both
point and generalized estimators and is unaffected by bias. Fisher information
$I$ and variance are fundamentally different quantities: the latter is defined
at a distribution that need not belong to a family, while the former cannot be
defined without a family of distributions, $M$. Fisher information and
$\Lambda$ are similar quantities as both are defined on the tangent bundle
$T\!M$ and $I$ provides an upper bound, $\Lambda\le I$, that holds for all
sample sizes -- asymptotics are not required. Comparing estimators using
$\Lambda$ rather than variance supports Fisher&apos;s claim that $I$ provides a
bound even in small samples. $\Lambda$-efficiency is defined that extends the
efficiency of unbiased estimators based on variance. While defined by the
slope, $\Lambda$-efficiency is simply $\rho^{2}$, the square of the correlation
between estimator and score function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vos_P/0/1/0/all/0/1&quot;&gt;Paul W. Vos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06727">
<title>Reliable emulation of complex functionals by active learning with error control. (arXiv:2208.06727v2 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06727</link>
<description rdf:parseType="Literal">&lt;p&gt;A statistical emulator can be used as a surrogate of complex physics-based
calculations to drastically reduce the computational cost. Its successful
implementation hinges on an accurate representation of the nonlinear response
surface with a high-dimensional input space. Conventional ``space-filling&apos;&apos;
designs, including random sampling and Latin hypercube sampling, become
inefficient as the dimensionality of the input variables increases, and the
predictive accuracy of the emulator can degrade substantially for a test input
distant from the training input set. To address this fundamental challenge, we
develop a reliable emulator for predicting complex functionals by active
learning with error control (ALEC). The algorithm is applicable to
infinite-dimensional mapping with high-fidelity predictions and a controlled
predictive error. The computational efficiency has been demonstrated by
emulating the classical density functional theory (cDFT) calculations, a
statistical-mechanical method widely used in modeling the equilibrium
properties of complex molecular systems. We show that ALEC is much more
accurate than conventional emulators based on the Gaussian processes with
``space-filling&apos;&apos; designs and alternative active learning methods. Besides, it
is computationally more efficient than direct cDFT calculations. ALEC can be a
reliable building block for emulating expensive functionals owing to its
minimal computational cost, controllable predictive error, and fully automatic
features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xinyi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Mengyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianzhong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07330">
<title>Semiparametric Best Arm Identification with Contextual Information. (arXiv:2209.07330v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07330</link>
<description rdf:parseType="Literal">&lt;p&gt;We study best-arm identification with a fixed budget and contextual
(covariate) information in stochastic multi-armed bandit problems. In each
round, after observing contextual information, we choose a treatment arm using
past observations and current context. Our goal is to identify the best
treatment arm, a treatment arm with the maximal expected reward marginalized
over the contextual distribution, with a minimal probability of
misidentification. First, we derive semiparametric lower bounds of the
misidentification probability for this problem, where we regard the gaps
between the expected rewards of the best and suboptimal treatment arms as
parameters of interest, and all other parameters, such as the expected rewards
conditioned on contexts, as the nuisance parameters. We then develop the
``Contextual RS-AIPW strategy,&apos;&apos; which consists of the random sampling (RS)
rule tracking a target allocation ratio and the recommendation rule using the
augmented inverse probability weighting (AIPW) estimator. Our proposed
Contextual RS-AIPW strategy is optimal because the upper bound for the
probability of misidentification by the strategy matches the semiparametric
lower bound, when the budget goes to infinity and the gaps converge to zero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishihara_T/0/1/0/all/0/1&quot;&gt;Takuya Ishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitagawa_T/0/1/0/all/0/1&quot;&gt;Toru Kitagawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07370">
<title>A Geometric Perspective on Variational Autoencoders. (arXiv:2209.07370v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new interpretation of the Variational Autoencoder
framework by taking a fully geometric point of view. We argue that vanilla VAE
models unveil naturally a Riemannian structure in their latent space and that
taking into consideration those geometrical aspects can lead to better
interpolations and an improved generation procedure. This new proposed sampling
method consists in sampling from the uniform distribution deriving
intrinsically from the learned Riemannian latent space and we show that using
this scheme can make a vanilla VAE competitive and even better than more
advanced versions on several benchmark datasets. Since generative models are
known to be sensitive to the number of training samples we also stress the
method&apos;s robustness in the low data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chadebec_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Chadebec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Allassonniere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phanie Allassonni&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08860">
<title>A Survey of Deep Causal Models. (arXiv:2209.08860v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08860</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of causality plays a significant role in human cognition. In the
past few decades, causal inference has been well developed in many fields, such
as computer science, medicine, economics, and other industrial applications.
With the advancement of deep learning, it has been increasingly applied in
causal inference against counterfactual data. Typically, deep causal models map
the characteristics of covariates to a representation space and then design
various objective functions to estimate counterfactual data unbiasedly.
Different from the existing surveys on causal models in machine learning, this
paper mainly focuses on the overview of the deep causal models, and its core
contributions are as follows: 1) we summarize the popularly adopted relevant
metrics under multiple treatments and continuous-dose treatment; 2) we cast
insight on a comprehensive overview of deep causal models from both timeline of
development and method classification perspectives; 3) we also endeavor to
present a detailed categorization and analysis on relevant datasets, source
codes and experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12054">
<title>From Local to Global: Spectral-Inspired Graph Neural Networks. (arXiv:2209.12054v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12054</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are powerful deep learning methods for
Non-Euclidean data. Popular GNNs are message-passing algorithms (MPNNs) that
aggregate and combine signals in a local graph neighborhood. However, shallow
MPNNs tend to miss long-range signals and perform poorly on some heterophilous
graphs, while deep MPNNs can suffer from issues like over-smoothing or
over-squashing. To mitigate such issues, existing works typically borrow
normalization techniques from training neural networks on Euclidean data or
modify the graph structures. Yet these approaches are not well-understood
theoretically and could increase the overall computational complexity. In this
work, we draw inspirations from spectral graph embedding and propose
$\texttt{PowerEmbed}$ -- a simple layer-wise normalization technique to boost
MPNNs. We show $\texttt{PowerEmbed}$ can provably express the top-$k$ leading
eigenvectors of the graph operator, which prevents over-smoothing and is
agnostic to the graph topology; meanwhile, it produces a list of
representations ranging from local features to global signals, which avoids
over-squashing. We apply $\texttt{PowerEmbed}$ in a wide range of simulated and
real graphs and demonstrate its competitive performance, particularly for
heterophilous graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Ningyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villar_S/0/1/0/all/0/1&quot;&gt;Soledad Villar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Da Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengyue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Braverman_V/0/1/0/all/0/1&quot;&gt;Vladimir Braverman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00340">
<title>Speed Up the Cold-Start Learning in Two-Sided Bandits with Many Arms. (arXiv:2210.00340v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00340</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-armed bandit (MAB) algorithms are efficient approaches to reduce the
opportunity cost of online experimentation and are used by companies to find
the best product from periodically refreshed product catalogs. However, these
algorithms face the so-called cold-start at the onset of the experiment due to
a lack of knowledge of customer preferences for new products, requiring an
initial data collection phase known as the burn-in period. During this period,
MAB algorithms operate like randomized experiments, incurring large burn-in
costs which scale with the large number of products. We attempt to reduce the
burn-in by identifying that many products can be cast into two-sided products,
and then naturally model the rewards of the products with a matrix, whose rows
and columns represent the two sides respectively. Next, we design two-phase
bandit algorithms that first use subsampling and low-rank matrix estimation to
obtain a substantially smaller targeted set of products and then apply a UCB
procedure on the target products to find the best one. We theoretically show
that the proposed algorithms lower costs and expedite the experiment in cases
when there is limited experimentation time along with a large product set. Our
analysis also reveals three regimes of long, short, and ultra-short horizon
experiments, depending on dimensions of the matrix. Empirical evidence from
both synthetic data and a real-world dataset on music streaming services
validates this superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayati_M/0/1/0/all/0/1&quot;&gt;Mohsen Bayati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Junyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wanning Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01757">
<title>Purely prognostic variables may modify marginal treatment effects for non-collapsible effect measures. (arXiv:2210.01757v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01757</link>
<description rdf:parseType="Literal">&lt;p&gt;In evidence synthesis, effect measure modifiers are typically described as
variables that induce treatment effect heterogeneity at the individual level,
through treatment-covariate interactions in an outcome model parametrized at
such level. As such, effect modification is defined with respect to a
conditional measure. However, marginal effect estimates are required for
population-level decisions in health technology assessment. For non-collapsible
effect measures, purely prognostic variables that do not predict response to
treatment at the individual level may modify marginal treatment effects at the
population level. This has important implications for recommended practices for
evidence synthesis. Firstly, unadjusted indirect comparisons of marginal
effects may be biased in the absence of individual-level treatment effect
heterogeneity. Secondly, covariate adjustment may be necessary to account for
cross-study imbalances in purely prognostic variables. Popular summary measures
in meta-analysis such as odds ratios and hazard ratios are non-collapsible.
Collapsible measures would facilitate the transportability of marginal effects
between studies by: (1) removing dependence on model-based covariate adjustment
when there is treatment effect homogeneity at the individual level; and (2)
facilitating the selection of baseline characteristics for covariate adjustment
when there is heterogeneity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Remiro_Azocar_A/0/1/0/all/0/1&quot;&gt;Antonio Remiro-Az&amp;#xf3;car&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02631">
<title>Data-driven Approaches to Surrogate Machine Learning Model Development. (arXiv:2210.02631v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02631</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate the adaption of three established methods to the field of
surrogate machine learning model development. These methods are data
augmentation, custom loss functions and transfer learning. Each of these
methods have seen widespread use in the field of machine learning, however,
here we apply them specifically to surrogate machine learning model
development. The machine learning model that forms the basis behind this work
was intended to surrogate a traditional engineering model used in the UK
nuclear industry. Previous performance of this model has been hampered by poor
performance due to limited training data. Here, we demonstrate that through a
combination of additional techniques, model performance can be significantly
improved. We show that each of the aforementioned techniques have utility in
their own right and in combination with one another. However, we see them best
applied as part of a transfer learning operation. Five pre-trained surrogate
models produced prior to this research were further trained with an augmented
dataset and with our custom loss function. Through the combination of all three
techniques, we see an improvement of at least $38\%$ in performance across the
five models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_H/0/1/0/all/0/1&quot;&gt;H. Rhys Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tingting Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1&quot;&gt;Andrei C. Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulehman_Y/0/1/0/all/0/1&quot;&gt;Yusuf Sulehman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13133">
<title>Markov-modulated marked Poisson processes for modelling disease dynamics based on medical claims data. (arXiv:2210.13133v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13133</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore Markov-modulated marked Poisson processes (MMMPPs) as a natural
framework for modelling patients&apos; disease dynamics over time based on medical
claims data. In claims data, observations do not only occur at random points in
time but are also informative, i.e. driven by unobserved disease levels, as
poor health conditions usually lead to more frequent interactions with the
healthcare system. Therefore, we model the observation process as a
Markov-modulated Poisson process, where the rate of healthcare interactions is
governed by a continuous-time Markov chain. Its states serve as proxies for the
patients&apos; latent disease levels and further determine the distribution of
additional data collected at each observation time, the so-called marks.
Overall, MMMPPs jointly model observations and their informative time points by
comprising two state-dependent processes: the observation process
(corresponding to the event times) and the mark process (corresponding to
event-specific information), which both depend on the underlying states. The
approach is illustrated using claims data from patients diagnosed with chronic
obstructive pulmonary disease (COPD) by modelling their drug use and the
interval lengths between consecutive physician consultations. The results
indicate that MMMPPs are able to detect distinct patterns of healthcare
utilisation related to disease processes and reveal inter-individual
differences in the state-switching dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mews_S/0/1/0/all/0/1&quot;&gt;Sina Mews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Surmann_B/0/1/0/all/0/1&quot;&gt;Bastian Surmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hasemann_L/0/1/0/all/0/1&quot;&gt;Lena Hasemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elkenkamp_S/0/1/0/all/0/1&quot;&gt;Svenja Elkenkamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15264">
<title>Generating the right evidence at the right time: Principles of a new class of flexible augmented clinical trial designs. (arXiv:2210.15264v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15264</link>
<description rdf:parseType="Literal">&lt;p&gt;The past few years have seen an increasing number of initiatives aimed at
integrating information generated outside of confirmatory randomised clinical
trials (RCTs) into drug development. However, data generated non-concurrently
and through observational studies can provide results that are difficult to
compare with randomised trial data. Moreover, the scientific questions these
data can serve to answer often remain vague. Our starting point is to use
clearly defined objectives for evidence generation, which are formulated
towards early discussion with health technology assessment (HTA) bodies and are
additional to regulatory requirements for authorisation of a new treatment. We
propose FACTIVE (Flexible Augmented Clinical Trial for Improved eVidencE
generation), a new class of study designs enabling flexible augmentation of
confirmatory randomised controlled trials with concurrent and
close-to-real-world elements. These enabling designs facilitate estimation of
certain treatment effects in the confirmatory part and other, complementary
treatment effects in a concurrent real-world part. Each stakeholder should use
the evidence that is relevant within their own decision-making framework. High
quality data are generated under one single protocol and the use of
randomisation ensures rigorous statistical inference and interpretation within
and between the different parts of the experiment. Evidence for the
decision-making of HTA bodies could be available earlier than is currently the
case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunger_Baldauf_C/0/1/0/all/0/1&quot;&gt;Cornelia Dunger-Baldauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hemmings_R/0/1/0/all/0/1&quot;&gt;Rob Hemmings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bretz_F/0/1/0/all/0/1&quot;&gt;Frank Bretz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jones_B/0/1/0/all/0/1&quot;&gt;Byron Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiel_A/0/1/0/all/0/1&quot;&gt;Anja Schiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Chris Holmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17177">
<title>Variational Inference Aided Estimation of Time Varying Channels. (arXiv:2210.17177v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17177</link>
<description rdf:parseType="Literal">&lt;p&gt;One way to improve the estimation of time varying channels is to incorporate
knowledge of previous observations. In this context, Dynamical VAEs (DVAEs)
build a promising deep learning (DL) framework which is well suited to learn
the distribution of time series data. We introduce a new DVAE architecture,
called k-MemoryMarkovVAE (k-MMVAE), whose sparsity can be controlled by an
additional memory parameter. Following the approach in [1] we derive a k-MMVAE
aided channel estimator which takes temporal correlations of successive
observations into account. The results are evaluated on simulated channels by
QuaDRiGa and show that the k-MMVAE aided channel estimator clearly outperforms
other machine learning (ML) aided estimators which are either memoryless or
naively extended to time varying channels without major adaptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bock_B/0/1/0/all/0/1&quot;&gt;Benedikt B&amp;#xf6;ck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baur_M/0/1/0/all/0/1&quot;&gt;Michael Baur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rizzello_V/0/1/0/all/0/1&quot;&gt;Valentina Rizzello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utschick_W/0/1/0/all/0/1&quot;&gt;Wolfgang Utschick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01170">
<title>Estimating intracluster correlation for ordinal data. (arXiv:2211.01170v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01170</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: In this paper we consider the estimation of intracluster correlation
for ordinal data. We focus on pure-tone audiometry hearing threshold data,
where thresholds are measured in 5 decibel increments. We estimate the
intracluster correlation for tests from iPhone-based hearing assessment
application as a measure of test/retest reliability. Methods: We present a
method to estimate the intracluster correlation using mixed effects cumulative
logistic and probit models, which assume the outcome data are ordinal. This
contrasts with using a mixed effects linear model which assumes that the
outcome data are continuous. Results: In simulation studies we show that using
a mixed effects linear model to estimate the intracluster correlation for
ordinal data results in a negative finite sample bias, while using mixed
effects cumulative logistic or probit models reduces this bias. The estimated
intracluster correlation for the iPhone-based hearing assessment application is
higher when using the mixed effects cumulative logistic and probit models
compared to using a mixed effects linear model. Conclusion: When data are
ordinal, using mixed effects cumulative logistic or probit models reduces the
bias of intracluster correlation estimates relative to using a mixed effects
linear model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langworthy_B/0/1/0/all/0/1&quot;&gt;Benjamin W. Langworthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Curhan_G/0/1/0/all/0/1&quot;&gt;Gary C. Curhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Curhan_S/0/1/0/all/0/1&quot;&gt;Sharon G. Curhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Molin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01227">
<title>Conformalized survival analysis with adaptive cutoffs. (arXiv:2211.01227v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a method that constructs valid and efficient lower
predictive bounds (LPBs) for survival times with censored data. Traditional
methods for survival analysis often assume a parametric model for the
distribution of survival time as a function of the measured covariates, or
assume that this conditional distribution is captured well with a
non-parametric method such as random forests; however, these methods may lead
to undercoverage if their assumptions are not satisfied. In this paper, we
build on recent work by Cand\`es et al. (2021), which offers a more
assumption-lean approach to the problem. Their approach first subsets the data
to discard any data points with early censoring times and then uses a
reweighting technique (namely, weighted conformal inference (Tibshirani et al.,
2019)) to correct for the distribution shift introduced by this subsetting
procedure. For our new method, instead of constraining to a fixed threshold for
the censoring time when subsetting the data, we allow for a covariate-dependent
and data-adaptive subsetting step, which is better able to capture the
heterogeneity of the censoring mechanism. As a result, our method can lead to
LPBs that are less conservative and give more accurate information. We show
that in the Type I right-censoring setting, if either of the censoring
mechanism or the conditional quantile of survival time is well estimated, our
proposed procedure achieves approximately exact marginal coverage, where in the
latter case we additionally have approximate conditional coverage. We evaluate
the validity and efficiency of our proposed algorithm in numerical experiments,
illustrating its advantage when compared with other competing methods. Finally,
our method is applied to a real dataset to generate LPBs for users&apos; active
times on a mobile app.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gui_Y/0/1/0/all/0/1&quot;&gt;Yu Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hore_R/0/1/0/all/0/1&quot;&gt;Rohan Hore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhimei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>