<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.05470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.10443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.14027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.15670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.10611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.03149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.08868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.00242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.02399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.02846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.09303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.04944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.00447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.05876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.03917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1906.11898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.02849" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01369">
<title>Gravitational Dimensionality Reduction Using Newtonian Gravity and Einstein&apos;s General Relativity. (arXiv:2211.01369v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01369</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the effectiveness of using machine learning in physics, it has been
widely received increased attention in the literature. However, the notion of
applying physics in machine learning has not been given much awareness to. This
work is a hybrid of physics and machine learning where concepts of physics are
used in machine learning. We propose the supervised Gravitational
Dimensionality Reduction (GDR) algorithm where the data points of every class
are moved to each other for reduction of intra-class variances and better
separation of classes. For every data point, the other points are considered to
be gravitational particles, such as stars, where the point is attracted to the
points of its class by gravity. The data points are first projected onto a
spacetime manifold using principal component analysis. We propose two variants
of GDR -- one with the Newtonian gravity and one with the Einstein&apos;s general
relativity. The former uses Newtonian gravity in a straight line between points
but the latter moves data points along the geodesics of spacetime manifold. For
GDR with relativity gravitation, we use both Schwarzschild and Minkowski metric
tensors to cover both general relativity and special relativity. Our
simulations show the effectiveness of GDR in discrimination of classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1&quot;&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Smriti Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01370">
<title>Class Interference of Deep Neural Networks. (arXiv:2211.01370v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01370</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing and telling similar objects apart is even hard for human beings.
In this paper, we show that there is a phenomenon of class interference with
all deep neural networks. Class interference represents the learning difficulty
in data, and it constitutes the largest percentage of generalization errors by
deep networks. To understand class interference, we propose cross-class tests,
class ego directions and interference models. We show how to use these
definitions to study minima flatness and class interference of a trained model.
We also show how to detect class interference during training through label
dancing pattern and class dancing notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_D/0/1/0/all/0/1&quot;&gt;Dongcui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hengshuai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bei Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01374">
<title>End-to-end deep multi-score model for No-reference stereoscopic image quality assessment. (arXiv:2211.01374v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01374</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based quality metrics have recently given significant
improvement in Image Quality Assessment (IQA). In the field of stereoscopic
vision, information is evenly distributed with slight disparity to the left and
right eyes. However, due to asymmetric distortion, the objective quality
ratings for the left and right images would differ, necessitating the learning
of unique quality indicators for each view. Unlike existing stereoscopic IQA
measures which focus mainly on estimating a global human score, we suggest
incorporating left, right, and stereoscopic objective scores to extract the
corresponding properties of each view, and so forth estimating stereoscopic
image quality without reference. Therefore, we use a deep multi-score
Convolutional Neural Network (CNN). Our model has been trained to perform four
tasks: First, predict the left view&apos;s quality. Second, predict the quality of
the left view. Third and fourth, predict the quality of the stereo view and
global quality, respectively, with the global score serving as the ultimate
quality. Experiments are conducted on Waterloo IVC 3D Phase 1 and Phase 2
databases. The results obtained show the superiority of our method when
comparing with those of the state-of-the-art. The implementation code can be
found at: https://github.com/o-messai/multi-score-SIQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Messai_O/0/1/0/all/0/1&quot;&gt;Oussama Messai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chetouani_A/0/1/0/all/0/1&quot;&gt;Aladine Chetouani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01412">
<title>CAMANet: Class Activation Map Guided Attention Network for Radiology Report Generation. (arXiv:2211.01412v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01412</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology report generation (RRG) has gained increasing research attention
because of its huge potential to mitigate medical resource shortages and aid
the process of disease decision making by radiologists. Recent advancements in
Radiology Report Generation (RRG) are largely driven by improving models&apos;
capabilities in encoding single-modal feature representations, while few
studies explore explicitly the cross-modal alignment between image regions and
words. Radiologists typically focus first on abnormal image regions before they
compose the corresponding text descriptions, thus cross-modal alignment is of
great importance to learn an abnormality-aware RRG model. Motivated by this, we
propose a Class Activation Map guided Attention Network (CAMANet) which
explicitly promotes cross-modal alignment by employing the aggregated class
activation maps to supervise the cross-modal attention learning, and
simultaneously enriches the discriminative information. Experimental results
demonstrate that CAMANet outperforms previous SOTA methods on two commonly used
RRG benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalerao_A/0/1/0/all/0/1&quot;&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1&quot;&gt;Terry Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1&quot;&gt;Simon See&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yulan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01427">
<title>TextCraft: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Text. (arXiv:2211.01427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01427</link>
<description rdf:parseType="Literal">&lt;p&gt;Language is one of the primary means by which we describe the 3D world around
us. While rapid progress has been made in text-to-2D-image synthesis, similar
progress in text-to-3D-shape synthesis has been hindered by the lack of paired
(text, shape) data. Moreover, extant methods for text-to-shape generation have
limited shape diversity and fidelity. We introduce TextCraft, a method to
address these limitations by producing high-fidelity and diverse 3D shapes
without the need for (text, shape) pairs for training. TextCraft achieves this
by using CLIP and using a multi-resolution approach by first generating in a
low-dimensional latent space and then upscaling to a higher resolution,
improving the fidelity of the generated shape. To improve shape diversity, we
use a discrete latent space which is modelled using a bidirectional transformer
conditioned on the interchangeable image-text embedding space induced by CLIP.
Moreover, we present a novel variant of classifier-free guidance, which further
improves the accuracy-diversity trade-off. Finally, we perform extensive
experiments that demonstrate that TextCraft outperforms state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1&quot;&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Rao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1&quot;&gt;Vivian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1&quot;&gt;Hooman Shayani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Hosein Khasahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1&quot;&gt;Srinath Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1&quot;&gt;Daniel Ritchie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01472">
<title>The Need for Medically Aware Video Compression in Gastroenterology. (arXiv:2211.01472v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01472</link>
<description rdf:parseType="Literal">&lt;p&gt;Compression is essential to storing and transmitting medical videos, but the
effect of compression on downstream medical tasks is often ignored.
Furthermore, systems in practice rely on standard video codecs, which naively
allocate bits between medically relevant frames or parts of frames. In this
work, we present an empirical study of some deficiencies of classical codecs on
gastroenterology videos, and motivate our ongoing work to train a learned
compression model for colonoscopy videos. We show that two of the most common
classical codecs, H264 and HEVC, compress medically relevant frames
statistically significantly worse than medically nonrelevant ones, and that
polyp detector performance degrades rapidly as compression increases. We
explain how a learned compressor could allocate bits to important regions and
allow detection performance to degrade more gracefully. Many of our proposed
techniques generalize to medical video domains beyond gastroenterology
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johnston_N/0/1/0/all/0/1&quot;&gt;Nick Johnston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01484">
<title>The Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01484</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional lottery ticket hypothesis (LTH) claims that there exists a
sparse subnetwork within a dense neural network and a proper random
initialization method, called the winning ticket, such that it can be trained
from scratch to almost as good as the dense counterpart. Meanwhile, the
research of LTH in vision transformers (ViTs) is scarcely evaluated. In this
paper, we first show that the conventional winning ticket is hard to find at
weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs
to input images consisting of image patches inspired by the input dependence of
ViTs. That is, there exists a subset of input image patches such that a ViT can
be trained from scratch by using only this subset of patches and achieve
similar accuracy to the ViTs trained by using all image patches. We call this
subset of input patches the winning tickets, which represent a significant
amount of information in the input. Furthermore, we present a simple yet
effective method to find the winning tickets in input patches for various types
of ViT, including DeiT, LV-ViT, and Swin Transformers. More specifically, we
use a ticket selector to generate the winning tickets based on the
informativeness of patches. Meanwhile, we build another randomly selected
subset of patches for comparison, and the experiments show that there is clear
difference between the performance of models trained with winning tickets and
randomly selected subsets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1&quot;&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1&quot;&gt;Minghai Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Peiyan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Geng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01505">
<title>Implicit Neural Representation as a Differentiable Surrogate for Photon Propagation in a Monolithic Neutrino Detector. (arXiv:2211.01505v1 [physics.ins-det])</title>
<link>http://arxiv.org/abs/2211.01505</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical photons are used as signal in a wide variety of particle detectors.
Modern neutrino experiments employ hundreds to tens of thousands of photon
detectors to observe signal from millions to billions of scintillation photons
produced from energy deposition of charged particles. These neutrino detectors
are typically large, containing kilotons of target volume, with different
optical properties. Modeling individual photon propagation in form of look-up
table requires huge computational resources. As the size of a table increases
with detector volume for a fixed resolution, this method scales poorly for
future larger detectors. Alternative approaches such as fitting a polynomial to
the model could address the memory issue, but results in poorer performance.
Both look-up table and fitting approaches are prone to discrepancies between
the detector simulation and the data collected. We propose a new approach using
SIREN, an implicit neural representation with periodic activation functions, to
model the look-up table as a 3D scene and reproduces the acceptance map with
high accuracy. The number of parameters in our SIREN model is orders of
magnitude smaller than the number of voxels in the look-up table. As it models
an underlying functional shape, SIREN is scalable to a larger detector.
Furthermore, SIREN can successfully learn the spatial gradients of the photon
library, providing additional information for downstream applications. Finally,
as SIREN is a neural network representation, it is differentiable with respect
to its parameters, and therefore tunable via gradient descent. We demonstrate
the potential of optimizing SIREN directly on real data, which mitigates the
concern of data vs. simulation discrepancies. We further present an application
for data reconstruction where SIREN is used to form a likelihood function for
photon statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lei_M/0/1/0/all/0/1&quot;&gt;Minjie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tsang_K/0/1/0/all/0/1&quot;&gt;Ka Vang Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gasiorowski_S/0/1/0/all/0/1&quot;&gt;Sean Gasiorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nashed_Y/0/1/0/all/0/1&quot;&gt;Youssef Nashed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Petrillo_G/0/1/0/all/0/1&quot;&gt;Gianluca Petrillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Piazza_O/0/1/0/all/0/1&quot;&gt;Olivia Piazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ratner_D/0/1/0/all/0/1&quot;&gt;Daniel Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Terao_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Terao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01513">
<title>Optimizing Fiducial Marker Placement for Improved Visual Localization. (arXiv:2211.01513v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01513</link>
<description rdf:parseType="Literal">&lt;p&gt;Adding fiducial markers to a scene is a well-known strategy for making visual
localization algorithms more robust. Traditionally, these marker locations are
selected by humans who are familiar with visual localization techniques. This
paper explores the problem of automatic marker placement within a scene.
Specifically, given a predetermined set of markers and a scene model, we
compute optimized marker positions within the scene that can improve accuracy
in visual localization. Our main contribution is a novel framework for modeling
camera localizability that incorporates both natural scene features and
artificial fiducial markers added to the scene. We present optimized marker
placement (OMP), a greedy algorithm that is based on the camera localizability
framework. We have also designed a simulation framework for testing marker
placement algorithms on 3D models and images generated from synthetic scenes.
We have evaluated OMP within this testbed and demonstrate an improvement in the
localization rate by up to 20 percent on three different scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiangqiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeGol_J/0/1/0/all/0/1&quot;&gt;Joseph DeGol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragoso_V/0/1/0/all/0/1&quot;&gt;Victor Fragoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1&quot;&gt;Sudipta N. Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1&quot;&gt;John J. Leonard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01527">
<title>Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments. (arXiv:2211.01527v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01527</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach for autonomous sensor control for information
gathering under partially observable, dynamic and sparsely sampled
environments. We consider the problem of controlling a sensor that makes
partial observations in some space of interest such that it maximizes
information about entities present in that space. We describe our approach for
the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to
search for and track unknown, dynamic signals in the environment. To this end,
we develop and demonstrate enhancements of the Deep Anticipatory Network (DAN)
Reinforcement Learning (RL) framework that uses prediction and information-gain
rewards to learn information-maximization policies in reward-sparse
environments. We also extend this problem to situations in which taking samples
from the actual RF spectrum/field is limited and expensive, and propose a
model-based version of the original RL algorithm that fine-tunes the controller
using a model of the environment that is iteratively improved from limited
samples taken from the RF field. Our approach was thoroughly validated by
testing against baseline expert-designed controllers in simulated RF
environments of different complexity, using different rewards schemes and
evaluation metrics. The results show that our system outperforms the standard
DAN architecture and is more flexible and robust than several hand-coded
agents. We also show that our approach is adaptable to non-stationary
environments where the agent has to learn to adapt to changes from the emitting
sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burns_J/0/1/0/all/0/1&quot;&gt;J. Brian Burns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_A/0/1/0/all/0/1&quot;&gt;Aravind Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sequeira_P/0/1/0/all/0/1&quot;&gt;Pedro Sequeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadhu_V/0/1/0/all/0/1&quot;&gt;Vidyasagar Sadhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01556">
<title>Ground Plane Matters: Picking Up Ground Plane Prior in Monocular 3D Object Detection. (arXiv:2211.01556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01556</link>
<description rdf:parseType="Literal">&lt;p&gt;The ground plane prior is a very informative geometry clue in monocular 3D
object detection (M3OD). However, it has been neglected by most mainstream
methods. In this paper, we identify two key factors that limit the
applicability of ground plane prior: the projection point localization issue
and the ground plane tilt issue. To pick up the ground plane prior for M3OD, we
propose a Ground Plane Enhanced Network (GPENet) which resolves both issues at
one go. For the projection point localization issue, instead of using the
bottom vertices or bottom center of the 3D bounding box (BBox), we leverage the
object&apos;s ground contact points, which are explicit pixels in the image and easy
for the neural network to detect. For the ground plane tilt problem, our GPENet
estimates the horizon line in the image and derives a novel mathematical
expression to accurately estimate the ground plane equation. An unsupervised
vertical edge mining algorithm is also proposed to address the occlusion of the
horizon line. Furthermore, we design a novel 3D bounding box deduction method
based on a dynamic back projection algorithm, which could take advantage of the
accurate contact points and the ground plane equation. Additionally, using only
M3OD labels, contact point and horizon line pseudo labels can be easily
generated with NO extra data collection and label annotation cost. Extensive
experiments on the popular KITTI benchmark show that our GPENet can outperform
other methods and achieve state-of-the-art performance, well demonstrating the
effectiveness and the superiority of the proposed approach. Moreover, our
GPENet works better than other methods in cross-dataset evaluation on the
nuScenes dataset. Our code and models will be published.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuchen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_K/0/1/0/all/0/1&quot;&gt;Kai Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1&quot;&gt;Guiguang Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01566">
<title>NaRPA: Navigation and Rendering Pipeline for Astronautics. (arXiv:2211.01566v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2211.01566</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Navigation and Rendering Pipeline for Astronautics
(NaRPA) - a novel ray-tracing-based computer graphics engine to model and
simulate light transport for space-borne imaging. NaRPA incorporates lighting
models with attention to atmospheric and shading effects for the synthesis of
space-to-space and ground-to-space virtual observations. In addition to image
rendering, the engine also possesses point cloud, depth, and contour map
generation capabilities to simulate passive and active vision-based sensors and
to facilitate the designing, testing, or verification of visual navigation
algorithms. Physically based rendering capabilities of NaRPA and the efficacy
of the proposed rendering algorithm are demonstrated using applications in
representative space-based environments. A key demonstration includes NaRPA as
a tool for generating stereo imagery and application in 3D coordinate
estimation using triangulation. Another prominent application of NaRPA includes
a novel differentiable rendering approach for image-based attitude estimation
is proposed to highlight the efficacy of the NaRPA engine for simulating
vision-based navigation and guidance operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eapen_R/0/1/0/all/0/1&quot;&gt;Roshan Thomas Eapen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaskara_R/0/1/0/all/0/1&quot;&gt;Ramchander Rao Bhaskara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majji_M/0/1/0/all/0/1&quot;&gt;Manoranjan Majji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01567">
<title>Galaxy Image Deconvolution for Weak Gravitational Lensing with Physics-informed Deep Learning. (arXiv:2211.01567v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2211.01567</link>
<description rdf:parseType="Literal">&lt;p&gt;Removing optical and atmospheric blur from galaxy images significantly
improves galaxy shape measurements for weak gravitational lensing and galaxy
evolution studies. This ill-posed linear inverse problem is usually solved with
deconvolution algorithms enhanced by regularisation priors or deep learning. We
introduce a so-called &quot;physics-based deep learning&quot; approach to the Point
Spread Function (PSF) deconvolution problem in galaxy surveys. We apply
algorithm unrolling and the Plug-and-Play technique to the Alternating
Direction Method of Multipliers (ADMM) with a Poisson noise model and use a
neural network to learn appropriate priors from simulated galaxy images. We
characterise the time-performance trade-off of several methods for galaxies of
differing brightness levels, showing an improvement of 26% (SNR=20)/48%
(SNR=100) compared to standard methods and 14% (SNR=20) compared to modern
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Alexander_E/0/1/0/all/0/1&quot;&gt;Emma Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01579">
<title>Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01579</link>
<description rdf:parseType="Literal">&lt;p&gt;Several companies often safeguard their trained deep models (i.e. details of
architecture, learnt weights, training details etc.) from third-party users by
exposing them only as black boxes through APIs. Moreover, they may not even
provide access to the training data due to proprietary reasons or sensitivity
concerns. We make the first attempt to provide adversarial robustness to the
black box models in a data-free set up. We construct synthetic data via
generative model and train surrogate network using model stealing techniques.
To minimize adversarial contamination on perturbed samples, we propose `wavelet
noise remover&apos; (WNR) that performs discrete wavelet decomposition on input
images and carefully select only a few important coefficients determined by our
`wavelet coefficient selection module&apos; (WCSM). To recover the high-frequency
content of the image after noise removal via WNR, we further train a
`regenerator&apos; network with an objective to retrieve the coefficients such that
the reconstructed image yields similar to original predictions on the surrogate
model. At test time, WNR combined with trained regenerator network is prepended
to the black box network, resulting in a high boost in adversarial accuracy.
Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01%
on state-of-the-art Auto Attack compared to baseline, even when the attacker
uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box
architecture (Alexnet) with same model stealing strategy as defender. The code
is available at https://github.com/vcl-iisc/data-free-black-box-defense
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1&quot;&gt;Inder Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Randive_S/0/1/0/all/0/1&quot;&gt;Shubham Randive&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1&quot;&gt;Ruchit Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Anirban Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01589">
<title>PolyBuilding: Polygon Transformer for End-to-End Building Extraction. (arXiv:2211.01589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01589</link>
<description rdf:parseType="Literal">&lt;p&gt;We present PolyBuilding, a fully end-to-end polygon Transformer for building
extraction. PolyBuilding direct predicts vector representation of buildings
from remote sensing images. It builds upon an encoder-decoder transformer
architecture and simultaneously outputs building bounding boxes and polygons.
Given a set of polygon queries, the model learns the relations among them and
encodes context information from the image to predict the final set of building
polygons with fixed vertex numbers. Corner classification is performed to
distinguish the building corners from the sampled points, which can be used to
remove redundant vertices along the building walls during inference. A 1-d
non-maximum suppression (NMS) is further applied to reduce vertex redundancy
near the building corners. With the refinement operations, polygons with
regular shapes and low complexity can be effectively obtained. Comprehensive
experiments are conducted on the CrowdAI dataset. Quantitative and qualitative
results show that our approach outperforms prior polygonal building extraction
methods by a large margin. It also achieves a new state-of-the-art in terms of
pixel-level coverage, instance-level precision and recall, and geometry-level
properties (including contour regularity and polygon complexity).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhou Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01598">
<title>Robust Few-shot Learning Without Using any Adversarial Samples. (arXiv:2211.01598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01598</link>
<description rdf:parseType="Literal">&lt;p&gt;The high cost of acquiring and annotating samples has made the `few-shot&apos;
learning problem of prime importance. Existing works mainly focus on improving
performance on clean data and overlook robustness concerns on the data
perturbed with adversarial noise. Recently, a few efforts have been made to
combine the few-shot problem with the robustness objective using sophisticated
Meta-Learning techniques. These methods rely on the generation of adversarial
samples in every episode of training, which further adds a computational
burden. To avoid such time-consuming and complicated procedures, we propose a
simple but effective alternative that does not require any adversarial samples.
Inspired by the cognitive decision-making process in humans, we enforce
high-level feature matching between the base class data and their corresponding
low-frequency samples in the pretraining stage via self distillation. The model
is then fine-tuned on the samples of novel classes where we additionally
improve the discriminability of low-frequency query set features via cosine
similarity. On a 1-shot setting of the CIFAR-FS dataset, our method yields a
massive improvement of $60.55\%$ &amp;amp; $62.05\%$ in adversarial accuracy on the PGD
and state-of-the-art Auto Attack, respectively, with a minor drop in clean
accuracy compared to the baseline. Moreover, our method only takes $1.69\times$
of the standard training time while being $\approx$ $5\times$ faster than
state-of-the-art adversarial meta-learning methods. The code is available at
https://github.com/vcl-iisc/robust-few-shot-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1&quot;&gt;Ruchit Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1&quot;&gt;Inder Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Anirban Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01600">
<title>nerf2nerf: Pairwise Registration of Neural Radiance Fields. (arXiv:2211.01600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01600</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a &apos;&apos;surface field&apos;&apos; -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goli_L/0/1/0/all/0/1&quot;&gt;Lily Goli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1&quot;&gt;Daniel Rebain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1&quot;&gt;Sara Sabour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Animesh Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01629">
<title>Image-based Early Detection System for Wildfires. (arXiv:2211.01629v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01629</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildfires are a disastrous phenomenon which cause damage to land, loss of
property, air pollution, and even loss of human life. Due to the warmer and
drier conditions created by climate change, more severe and uncontrollable
wildfires are expected to occur in the coming years. This could lead to a
global wildfire crisis and have dire consequences on our planet. Hence, it has
become imperative to use technology to help prevent the spread of wildfires.
One way to prevent the spread of wildfires before they become too large is to
perform early detection i.e, detecting the smoke before the actual fire starts.
In this paper, we present our Wildfire Detection and Alert System which use
machine learning to detect wildfire smoke with a high degree of accuracy and
can send immediate alerts to users. Our technology is currently being used in
the USA to monitor data coming in from hundreds of cameras daily. We show that
our system has a high true detection rate and a low false detection rate. Our
performance evaluation study also shows that on an average our system detects
wildfire smoke faster than an actual person.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranadive_O/0/1/0/all/0/1&quot;&gt;Omkar Ranadive&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Serin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_Y/0/1/0/all/0/1&quot;&gt;Youngseo Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Heechan Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minkook Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1&quot;&gt;Young K. Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01631">
<title>$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing. (arXiv:2211.01631v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01631</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xinzhe Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1&quot;&gt;Xiahai Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01634">
<title>P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving. (arXiv:2211.01634v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01634</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is crucial in enabling safe motion planning for autonomous
vehicles in interactive scenarios. It allows the planner to identify potential
conflicts with other traffic agents and generate safe plans. Existing motion
predictors often focus on reducing prediction errors, yet it remains an open
question on how well they help identify the conflicts for the planner. In this
paper, we evaluate state-of-the-art predictors through novel conflict-related
metrics, such as the success rate of identifying conflicts. Surprisingly, the
predictors suffer from a low success rate and thus lead to a large percentage
of collisions when we test the prediction-planning system in an interactive
simulator. To fill the gap, we propose a simple but effective alternative that
combines a physics-based trajectory generator and a learning-based relation
predictor to identify conflicts and infer conflict relations. We demonstrate
that our predictor, P4P, achieves superior performance over existing
learning-based predictors in realistic interactive driving scenarios from Waymo
Open Motion Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1&quot;&gt;Brian C. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01639">
<title>Temporal Consistency Learning of inter-frames for Video Super-Resolution. (arXiv:2211.01639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01639</link>
<description rdf:parseType="Literal">&lt;p&gt;Video super-resolution (VSR) is a task that aims to reconstruct
high-resolution (HR) frames from the low-resolution (LR) reference frame and
multiple neighboring frames. The vital operation is to utilize the relative
misaligned frames for the current frame reconstruction and preserve the
consistency of the results. Existing methods generally explore information
propagation and frame alignment to improve the performance of VSR. However, few
studies focus on the temporal consistency of inter-frames. In this paper, we
propose a Temporal Consistency learning Network (TCNet) for VSR in an
end-to-end manner, to enhance the consistency of the reconstructed videos. A
spatio-temporal stability module is designed to learn the self-alignment from
inter-frames. Especially, the correlative matching is employed to exploit the
spatial dependency from each frame to maintain structural stability. Moreover,
a self-attention mechanism is utilized to learn the temporal correspondence to
implement an adaptive warping operation for temporal consistency among
multi-frames. Besides, a hybrid recurrent architecture is designed to leverage
short-term and long-term information. We further present a progressive fusion
module to perform a multistage fusion of spatio-temporal features. And the
final reconstructed frames are refined by these fused features. Objective and
subjective results of various experiments demonstrate that TCNet has superior
performance on different benchmark datasets, compared to several
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meiqin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shuo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chunyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01644">
<title>StereoPose: Category-Level 6D Transparent Object Pose Estimation from Stereo Images via Back-View NOCS. (arXiv:2211.01644v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01644</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing methods for category-level pose estimation rely on object point
clouds. However, when considering transparent objects, depth cameras are
usually not able to capture meaningful data, resulting in point clouds with
severe artifacts. Without a high-quality point cloud, existing methods are not
applicable to challenging transparent objects. To tackle this problem, we
present StereoPose, a novel stereo image framework for category-level object
pose estimation, ideally suited for transparent objects. For a robust
estimation from pure stereo images, we develop a pipeline that decouples
category-level pose estimation into object size estimation, initial pose
estimation, and pose refinement. StereoPose then estimates object pose based on
representation in the normalized object coordinate space~(NOCS). To address the
issue of image content aliasing, we further define a back-view NOCS map for the
transparent object. The back-view NOCS aims to reduce the network learning
ambiguity caused by content aliasing, and leverage informative cues on the back
of the transparent object for more accurate pose estimation. To further improve
the performance of the stereo framework, StereoPose is equipped with a parallax
attention module for stereo feature fusion and an epipolar loss for improving
the stereo-view consistency of network predictions. Extensive experiments on
the public TOD dataset demonstrate the superiority of the proposed StereoPose
framework for category-level 6D transparent object pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Stephen James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_C/0/1/0/all/0/1&quot;&gt;Congying Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01664">
<title>PointSee: Image Enhances Point Cloud. (arXiv:2211.01664v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01664</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a trend to fuse multi-modal information for 3D object detection
(3OD). However, the challenging problems of low lightweightness, poor
flexibility of plug-and-play, and inaccurate alignment of features are still
not well-solved, when designing multi-modal fusion newtorks. We propose
PointSee, a lightweight, flexible and effective multi-modal fusion solution to
facilitate various 3OD networks by semantic feature enhancement of LiDAR point
clouds assembled with scene images. Beyond the existing wisdom of 3OD, PointSee
consists of a hidden module (HM) and a seen module (SM): HM decorates LiDAR
point clouds using 2D image information in an offline fusion manner, leading to
minimal or even no adaptations of existing 3OD networks; SM further enriches
the LiDAR point clouds by acquiring point-wise representative semantic
features, leading to enhanced performance of existing 3OD networks. Besides the
new architecture of PointSee, we propose a simple yet efficient training
strategy, to ease the potential inaccurate regressions of 2D object detection
networks. Extensive experiments on the popular outdoor/indoor benchmarks show
numerical improvements of our PointSee over twenty-two state-of-the-arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lipeng Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xuefeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Lina Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Haoran Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fu Lee Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mingqiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01670">
<title>Active CT Reconstruction with a Learned Sampling Policy. (arXiv:2211.01670v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01670</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a widely-used imaging technology that assists
clinical decision-making with high-quality human body representations. To
reduce the radiation dose posed by CT, sparse-view and limited-angle CT are
developed with preserved image quality. However, these methods are still stuck
with a fixed or uniform sampling strategy, which inhibits the possibility of
acquiring a better image with an even reduced dose. In this paper, we explore
this possibility via learning an active sampling policy that optimizes the
sampling positions for patient-specific, high-quality reconstruction. To this
end, we design an \textit{intelligent agent} for active recommendation of
sampling positions based on on-the-fly reconstruction with obtained sinograms
in a progressive fashion. With such a design, we achieve better performances on
the NIH-AAPM dataset over popular uniform sampling, especially when the number
of views is small. Finally, such a design also enables RoI-aware reconstruction
with improved reconstruction quality within regions of interest (RoI&apos;s) that
are clinically important. Experiments on the VerSe dataset demonstrate this
ability of our sampling policy, which is difficult to achieve based on uniform
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Ce Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1&quot;&gt;Kun Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haimiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01671">
<title>Physically Adversarial Attacks and Defenses in Computer Vision: A Survey. (arXiv:2211.01671v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01671</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Deep Neural Networks (DNNs) have been widely applied in various
real-world scenarios, they are vulnerable to adversarial examples. The current
adversarial attacks in computer vision can be divided into digital attacks and
physical attacks according to their different attack forms. Compared with
digital attacks, which generate perturbations in the digital pixels, physical
attacks are more practical in the real world. Owing to the serious security
problem caused by physically adversarial examples, many works have been
proposed to evaluate the physically adversarial robustness of DNNs in the past
years. In this paper, we summarize a survey versus the current physically
adversarial attacks and physically adversarial defenses in computer vision. To
establish a taxonomy, we organize the current physical attacks from attack
tasks, attack forms, and attack methods, respectively. Thus, readers can have a
systematic knowledge about this topic from different aspects. For the physical
defenses, we establish the taxonomy from pre-processing, in-processing, and
post-processing for the DNN models to achieve a full coverage of the
adversarial defenses. Based on the above survey, we finally discuss the
challenges of this research field and further outlook the future direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_B/0/1/0/all/0/1&quot;&gt;Bangzheng Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiefan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01770">
<title>Exploring Explainability Methods for Graph Neural Networks. (arXiv:2211.01770v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01770</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing use of deep learning methods, particularly graph neural
networks, which encode intricate interconnectedness information, for a variety
of real tasks, there is a necessity for explainability in such settings. In
this paper, we demonstrate the applicability of popular explainability
approaches on Graph Attention Networks (GAT) for a graph-based super-pixel
image classification task. We assess the qualitative and quantitative
performance of these techniques on three different datasets and describe our
findings. The results shed a fresh light on the notion of explainability in
GNNs, particularly GATs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1&quot;&gt;Harsh Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahni_S/0/1/0/all/0/1&quot;&gt;Shivam Sahni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01777">
<title>Evaluating a Synthetic Image Dataset Generated with Stable Diffusion. (arXiv:2211.01777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01777</link>
<description rdf:parseType="Literal">&lt;p&gt;We generate synthetic images with the &quot;Stable Diffusion&quot; image generation
model using the Wordnet taxonomy and the definitions of concepts it contains.
This synthetic image database can be used as training data for data
augmentation in machine learning applications, and it is used to investigate
the capabilities of the Stable Diffusion model.
&lt;/p&gt;
&lt;p&gt;Analyses show that Stable Diffusion can produce correct images for a large
number of concepts, but also a large variety of different representations. The
results show differences depending on the test concepts considered and problems
with very specific concepts. These evaluations were performed using a vision
transformer model for image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stockl_A/0/1/0/all/0/1&quot;&gt;Andreas St&amp;#xf6;ckl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01778">
<title>Progressive Transformation Learning For Leveraging Virtual Images in Training. (arXiv:2211.01778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01778</link>
<description rdf:parseType="Literal">&lt;p&gt;To effectively interrogate UAV-based images for detecting objects of
interest, such as humans, it is essential to acquire large-scale UAV-based
datasets that include human instances with various poses captured from widely
varying viewing angles. As a viable alternative to laborious and costly data
curation, we introduce Progressive Transformation Learning (PTL), which
gradually augments a training dataset by adding transformed virtual images with
enhanced realism. Generally, a virtual2real transformation generator in the
conditional GAN framework suffers from quality degradation when a large domain
gap exists between real and virtual images. To deal with the domain gap, PTL
takes a novel approach that progressively iterates the following three steps:
1) select a subset from a pool of virtual images according to the domain gap,
2) transform the selected virtual images to enhance realism, and 3) add the
transformed virtual images to the training set while removing them from the
pool. In PTL, accurately quantifying the domain gap is critical. To do that, we
theoretically demonstrate that the feature representation space of a given
object detector can be modeled as a multivariate Gaussian distribution from
which the Mahalanobis distance between a virtual object and the Gaussian
distribution of each object category in the representation space can be readily
computed. Experiments show that PTL results in a substantial performance
increase over the baseline, especially in the small data and the cross-domain
regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yi-Ting Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungtae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Heesung Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1&quot;&gt;Shuvra Shikhar Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01779">
<title>Exploring explicit coarse-grainend structure in artificial neural networks. (arXiv:2211.01779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01779</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to employ the hierarchical coarse-grained structure in the
artificial neural networks explicitly to improve the interpretability without
degrading performance. The idea has been applied in two situations. One is a
neural network called TaylorNet, which aims to approximate the general mapping
from input data to output result in terms of Taylor series directly, without
resorting to any magic nonlinear activations. The other is a new setup for data
distillation, which can perform multi-level abstraction of the input dataset
and generate new data that possesses the relevant features of the original
dataset and can be used as references for classification. In both cases, the
coarse-grained structure plays an important role in simplifying the network and
improving both the interpretability and efficiency. The validity has been
domonstrated on MNIST and CIFAR-10 datasets. Further improvement and some open
questions related are also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi-Ci Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Z. Y. Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao-Tao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01781">
<title>Video Event Extraction via Tracking Visual States of Arguments. (arXiv:2211.01781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01781</link>
<description rdf:parseType="Literal">&lt;p&gt;Video event extraction aims to detect salient events from a video and
identify the arguments for each event as well as their semantic roles. Existing
methods focus on capturing the overall visual scene of each frame, ignoring
fine-grained argument-level information. Inspired by the definition of events
as changes of states, we propose a novel framework to detect video events by
tracking the changes in the visual states of all involved arguments, which are
expected to provide the most informative evidence for the extraction of video
events. In order to capture the visual state changes of arguments, we decompose
them into changes in pixels within objects, displacements of objects, and
interactions among multiple arguments. We further propose Object State
Embedding, Object Motion-aware Embedding and Argument Interaction Embedding to
encode and track these changes respectively. Experiments on various video event
extraction tasks demonstrate significant improvements compared to
state-of-the-art models. In particular, on verb classification, we achieve
3.49% absolute gains (19.53% relative gains) in F1@5 on Video Situation
Recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Manling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Fu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01783">
<title>Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks. (arXiv:2211.01783v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01783</link>
<description rdf:parseType="Literal">&lt;p&gt;There is limited understanding of the information captured by deep
spatiotemporal models in their intermediate representations. For example, while
evidence suggests that action recognition algorithms are heavily influenced by
visual appearance in single frames, no quantitative methodology exists for
evaluating such static bias in the latent representation compared to bias
toward dynamics. We tackle this challenge by proposing an approach for
quantifying the static and dynamic biases of any spatiotemporal model, and
apply our approach to three tasks, action recognition, automatic video object
segmentation (AVOS) and video instance segmentation (VIS). Our key findings
are: (i) Most examined models are biased toward static information. (ii) Some
datasets that are assumed to be biased toward dynamics are actually biased
toward static information. (iii) Individual channels in an architecture can be
biased toward static, dynamic or a combination of the two. (iv) Most models
converge to their culminating biases in the first half of training. We then
explore how these biases affect performance on dynamically biased datasets. For
action recognition, we propose StaticDropout, a semantically guided dropout
that debiases a model from static information toward dynamics. For AVOS, we
design a better combination of fusion and cross connection layers compared with
previous architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1&quot;&gt;Matthew Kowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Amirul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1&quot;&gt;Neil D. B. Bruce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1&quot;&gt;Richard P. Wildes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01784">
<title>MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation. (arXiv:2211.01784v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01784</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, some pioneering works have preferred applying more complex modules
to improve segmentation performances. However, it is not friendly for actual
clinical environments due to limited computing resources. To address this
challenge, we propose a light-weight model to achieve competitive performances
for skin lesion segmentation at the lowest cost of parameters and computational
complexity so far. Briefly, we propose four modules: (1) DGA consists of
dilated convolution and gated attention mechanisms to extract global and local
feature information; (2) IEA, which is based on external attention to
characterize the overall datasets and enhance the connection between samples;
(3) CAB is composed of 1D convolution and fully connected layers to perform a
global and local fusion of multi-stage features to generate attention maps at
channel axis; (4) SAB, which operates on multi-stage features by a shared 2D
convolution to generate attention maps at spatial axis. We combine four modules
with our U-shape architecture and obtain a light-weight medical image
segmentation model dubbed as MALUNet. Compared with UNet, our model improves
the mIoU and DSC metrics by 2.39% and 1.49%, respectively, with a 44x and 166x
reduction in the number of parameters and computational complexity. In
addition, we conduct comparison experiments on two skin lesion segmentation
datasets (ISIC2017 and ISIC2018). Experimental results show that our model
achieves state-of-the-art in balancing the number of parameters, computational
complexity and segmentation performances. Code is available at
https://github.com/JCruan519/MALUNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jiacheng Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Mingye Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuzhuo Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01785">
<title>Rethinking Hierarchicies in Pre-trained Plain Vision Transformer. (arXiv:2211.01785v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01785</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised pre-training vision transformer (ViT) via masked image
modeling (MIM) has been proven very effective. However, customized algorithms
should be carefully designed for the hierarchical ViTs, e.g., GreenMIM, instead
of using the vanilla and simple MAE for the plain ViT. More importantly, since
these hierarchical ViTs cannot reuse the off-the-shelf pre-trained weights of
the plain ViTs, the requirement of pre-training them leads to a massive amount
of computational cost, thereby incurring both algorithmic and computational
complexity. In this paper, we address this problem by proposing a novel idea of
disentangling the hierarchical architecture design from the self-supervised
pre-training. We transform the plain ViT into a hierarchical one with minimal
changes. Technically, we change the stride of linear embedding layer from 16 to
4 and add convolution (or simple average) pooling layers between the
transformer blocks, thereby reducing the feature size from 1/4 to 1/32
sequentially. Despite its simplicity, it outperforms the plain ViT baseline in
classification, detection, and segmentation tasks on ImageNet, MS COCO,
Cityscapes, and ADE20K benchmarks, respectively. We hope this preliminary study
could draw more attention from the community on developing effective
(hierarchical) ViTs while avoiding the pre-training cost by leveraging the
off-the-shelf checkpoints. The code and models will be released at
https://github.com/ViTAE-Transformer/HPViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yufei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01796">
<title>Beyond Instance Discrimination: Relation-aware Contrastive Self-supervised Learning. (arXiv:2211.01796v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01796</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive self-supervised learning (CSL) based on instance discrimination
typically attracts positive samples while repelling negatives to learn
representations with pre-defined binary self-supervision. However, vanilla CSL
is inadequate in modeling sophisticated instance relations, limiting the
learned model to retain fine semantic structure. On the one hand, samples with
the same semantic category are inevitably pushed away as negatives. On the
other hand, differences among samples cannot be captured. In this paper, we
present relation-aware contrastive self-supervised learning (ReCo) to integrate
instance relations, i.e., global distribution relation and local interpolation
relation, into the CSL framework in a plug-and-play fashion. Specifically, we
align similarity distributions calculated between the positive anchor views and
the negatives at the global level to exploit diverse similarity relations among
instances. Local-level interpolation consistency between the pixel space and
the feature space is applied to quantitatively model the feature differences of
samples with distinct apparent similarities. Through explicitly instance
relation modeling, our ReCo avoids irrationally pushing away semantically
identical samples and carves a well-structured feature space. Extensive
experiments conducted on commonly used benchmarks justify that our ReCo
consistently gains remarkable performance improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qixiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01806">
<title>BATT: Backdoor Attack with Transformation-based Triggers. (arXiv:2211.01806v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.01806</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor
adversaries intend to maliciously control the predictions of attacked DNNs by
injecting hidden backdoors that can be activated by adversary-specified trigger
patterns during the training process. One recent research revealed that most of
the existing attacks failed in the real physical world since the trigger
contained in the digitized test samples may be different from that of the one
used for training. Accordingly, users can adopt spatial transformations as the
image pre-processing to deactivate hidden backdoors. In this paper, we explore
the previous findings from another side. We exploit classical spatial
transformations (i.e. rotation and translation) with the specific parameter as
trigger patterns to design a simple yet effective poisoning-based backdoor
attack. For example, only images rotated to a particular angle can activate the
embedded backdoor of attacked DNNs. Extensive experiments are conducted,
verifying the effectiveness of our attack under both digital and physical
settings and its resistance to existing backdoor defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01814">
<title>Self Similarity Matrix based CNN Filter Pruning. (arXiv:2211.01814v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01814</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, most of the deep learning solutions are targeted to be
deployed in mobile devices. This makes the need for development of lightweight
models all the more imminent. Another solution is to optimize and prune regular
deep learning models. In this paper, we tackle the problem of CNN model pruning
with the help of Self-Similarity Matrix (SSM) computed from the 2D CNN filters.
We propose two novel algorithms to rank and prune redundant filters which
contribute similar activation maps to the output. One of the key features of
our method is that there is no need of finetuning after training the model.
Both the training and pruning process is completed simultaneously. We benchmark
our method on two of the most popular CNN models - ResNet and VGG and record
their performance on the CIFAR-10 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakshith_S/0/1/0/all/0/1&quot;&gt;S Rakshith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vachhani_J/0/1/0/all/0/1&quot;&gt;Jayesh Rajkumar Vachhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gothe_S/0/1/0/all/0/1&quot;&gt;Sourabh Vasant Gothe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_R/0/1/0/all/0/1&quot;&gt;Rishabh Khurana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01825">
<title>Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation. (arXiv:2211.01825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Mining structural priors in data is a widely recognized technique for
hyperspectral image (HSI) denoising tasks, whose typical ways include
model-based methods and data-based methods. The model-based methods have good
generalization ability, while the runtime cannot meet the fast processing
requirements of the practical situations due to the large size of an HSI data $
\mathbf{X} \in \mathbb{R}^{MN\times B}$. For the data-based methods, they
perform very fast on new test data once they have been trained. However, their
generalization ability is always insufficient. In this paper, we propose a fast
model-based HSI denoising approach. Specifically, we propose a novel
regularizer named Representative Coefficient Total Variation (RCTV) to
simultaneously characterize the low rank and local smooth properties. The RCTV
regularizer is proposed based on the observation that the representative
coefficient matrix $\mathbf{U}\in\mathbb{R}^{MN\times R} (R\ll B)$ obtained by
orthogonally transforming the original HSI $\mathbf{X}$ can inherit the strong
local-smooth prior of $\mathbf{X}$. Since $R/B$ is very small, the HSI
denoising model based on the RCTV regularizer has lower time complexity.
Additionally, we find that the representative coefficient matrix $\mathbf{U}$
is robust to noise, and thus the RCTV regularizer can somewhat promote the
robustness of the HSI denoising model. Extensive experiments on mixed noise
removal demonstrate the superiority of the proposed method both in denoising
performance and denoising speed compared with other state-of-the-art methods.
Remarkably, the denoising speed of our proposed method outperforms all the
model-based techniques and is comparable with the deep learning-based
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiangjun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hailin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiangyong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1&quot;&gt;Xiangyu Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01842">
<title>Towards Discovering Neural Architectures from Scratch. (arXiv:2211.01842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of neural architectures from scratch is the long-standing goal
of Neural Architecture Search (NAS). Searching over a wide spectrum of neural
architectures can facilitate the discovery of previously unconsidered but
well-performing architectures. In this work, we take a large step towards
discovering neural architectures from scratch by expressing architectures
algebraically. This algebraic view leads to a more general method for designing
search spaces, which allows us to compactly represent search spaces that are
100s of orders of magnitude larger than common spaces from the literature.
Further, we propose a Bayesian Optimization strategy to efficiently search over
such huge spaces, and demonstrate empirically that both our search space design
and our search strategy can be superior to existing baselines. We open source
our algebraic NAS approach and provide APIs for PyTorch and TensorFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1&quot;&gt;Simon Schrodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1&quot;&gt;Danny Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1&quot;&gt;Rhea Sukthanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01847">
<title>Seeing the Unseen: Errors and Bias in Visual Datasets. (arXiv:2211.01847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01847</link>
<description rdf:parseType="Literal">&lt;p&gt;From face recognition in smartphones to automatic routing on self-driving
cars, machine vision algorithms lie in the core of these features. These
systems solve image based tasks by identifying and understanding objects,
subsequently making decisions from these information. However, errors in
datasets are usually induced or even magnified in algorithms, at times
resulting in issues such as recognising black people as gorillas and
misrepresenting ethnicities in search results. This paper tracks the errors in
datasets and their impacts, revealing that a flawed dataset could be a result
of limited categories, incomprehensive sourcing and poor classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongrui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01859">
<title>Computed tomography coronary angiogram images, annotations and associated data of normal and diseased arteries. (arXiv:2211.01859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01859</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography Coronary Angiography (CTCA) is a non-invasive method to
evaluate coronary artery anatomy and disease. CTCA is ideal for geometry
reconstruction to create virtual models of coronary arteries. To our knowledge
there is no public dataset that includes centrelines and segmentation of the
full coronary tree.
&lt;/p&gt;
&lt;p&gt;We provide anonymized CTCA images, voxel-wise annotations and associated data
in the form of centrelines, calcification scores and meshes of the coronary
lumen in 20 normal and 20 diseased cases. Images were obtained along with
patient information with informed, written consent as part of Coronary Atlas
(https://www.coronaryatlas.org/). Cases were classified as normal (zero calcium
score with no signs of stenosis) or diseased (confirmed coronary artery
disease). Manual voxel-wise segmentations by three experts were combined using
majority voting to generate the final annotations.
&lt;/p&gt;
&lt;p&gt;Provided data can be used for a variety of research purposes, such as 3D
printing patient-specific models, development and validation of segmentation
algorithms, education and training of medical personnel and in-silico analyses
such as testing of medical devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharleghi_R/0/1/0/all/0/1&quot;&gt;Ramtin Gharleghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adikari_D/0/1/0/all/0/1&quot;&gt;Dona Adikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellenberger_K/0/1/0/all/0/1&quot;&gt;Katy Ellenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webster_M/0/1/0/all/0/1&quot;&gt;Mark Webster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_C/0/1/0/all/0/1&quot;&gt;Chris Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sowmya_A/0/1/0/all/0/1&quot;&gt;Arcot Sowmya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_S/0/1/0/all/0/1&quot;&gt;Sze-Yuan Ooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beier_S/0/1/0/all/0/1&quot;&gt;Susann Beier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01866">
<title>ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations. (arXiv:2211.01866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01866</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning vision systems are widely deployed across applications where
reliability is critical. However, even today&apos;s best models can fail to
recognize an object when its pose, lighting, or background varies. While
existing benchmarks surface examples challenging for models, they do not
explain why such mistakes arise. To address this need, we introduce ImageNet-X,
a set of sixteen human annotations of factors such as pose, background, or
lighting the entire ImageNet-1k validation set as well as a random subset of
12k training images. Equipped with ImageNet-X, we investigate 2,200 current
recognition models and study the types of mistakes as a function of model&apos;s (1)
architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g.
supervised vs. self-supervised, and (3) training procedures, e.g., data
augmentation. Regardless of these choices, we find models have consistent
failure modes across ImageNet-X categories. We also find that while data
augmentation can improve robustness to certain factors, they induce spill-over
effects to other factors. For example, strong random cropping hurts robustness
on smaller objects. Together, these insights suggest to advance the robustness
of modern vision models, future research should focus on collecting additional
data and understanding data augmentation schemes. Along with these insights, we
release a toolkit based on ImageNet-X to spur further study into the mistakes
image recognition systems make.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Idrissi_B/0/1/0/all/0/1&quot;&gt;Badr Youbi Idrissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1&quot;&gt;Diane Bouchacourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1&quot;&gt;Ivan Evtimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1&quot;&gt;Caner Hazirbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1&quot;&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1&quot;&gt;Pascal Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1&quot;&gt;Michal Drozdzal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01885">
<title>Using U-Net Network for Efficient Brain Tumor Segmentation in MRI Images. (arXiv:2211.01885v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01885</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic Resonance Imaging (MRI) is the most commonly used non-intrusive
technique for medical image acquisition. Brain tumor segmentation is the
process of algorithmically identifying tumors in brain MRI scans. While many
approaches have been proposed in the literature for brain tumor segmentation,
this paper proposes a lightweight implementation of U-Net. Apart from providing
real-time segmentation of MRI scans, the proposed architecture does not need
large amount of data to train the proposed lightweight U-Net. Moreover, no
additional data augmentation step is required. The lightweight U-Net shows very
promising results on BITE dataset and it achieves a mean
intersection-over-union (IoU) of 89% while outperforming the standard benchmark
algorithms. Additionally, this work demonstrates an effective use of the three
perspective planes, instead of the original three-dimensional volumetric
images, for simplified brain tumor segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_J/0/1/0/all/0/1&quot;&gt;Jason Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Othmani_A/0/1/0/all/0/1&quot;&gt;Alice Othmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_M/0/1/0/all/0/1&quot;&gt;Mayank Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dev_S/0/1/0/all/0/1&quot;&gt;Soumyabrata Dev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01886">
<title>Analysing the effectiveness of a generative model for semi-supervised medical image segmentation. (arXiv:2211.01886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01886</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation is important in medical imaging, providing valuable,
quantitative information for clinical decision-making in diagnosis, therapy,
and intervention. The state-of-the-art in automated segmentation remains
supervised learning, employing discriminative models such as U-Net. However,
training these models requires access to large amounts of manually labelled
data which is often difficult to obtain in real medical applications. In such
settings, semi-supervised learning (SSL) attempts to leverage the abundance of
unlabelled data to obtain more robust and reliable models. Recently, generative
models have been proposed for semantic segmentation, as they make an attractive
choice for SSL. Their ability to capture the joint distribution over input
images and output label maps provides a natural way to incorporate information
from unlabelled images. This paper analyses whether deep generative models such
as the SemanticGAN are truly viable alternatives to tackle challenging medical
image segmentation problems. To that end, we thoroughly evaluate the
segmentation performance, robustness, and potential subgroup disparities of
discriminative and generative segmentation methods when applied to large-scale,
publicly available chest X-ray datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosnati_M/0/1/0/all/0/1&quot;&gt;Margherita Rosnati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1&quot;&gt;Miguel Monteiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel Coelho de Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01892">
<title>Deep meta-learning for the selection of accurate ultrasound based breast mass classifier. (arXiv:2211.01892v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01892</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard classification methods based on handcrafted morphological and
texture features have achieved good performance in breast mass differentiation
in ultrasound (US). In comparison to deep neural networks, commonly perceived
as &quot;black-box&quot; models, classical techniques are based on features that have
well-understood medical and physical interpretation. However, classifiers based
on morphological features commonly underperform in the presence of the
shadowing artifact and ill-defined mass borders, while texture based
classifiers may fail when the US image is too noisy. Therefore, in practice it
would be beneficial to select the classification method based on the appearance
of the particular US image. In this work, we develop a deep meta-network that
can automatically process input breast mass US images and recommend whether to
apply the shape or texture based classifier for the breast mass
differentiation. Our preliminary results demonstrate that meta-learning
techniques can be used to improve the performance of the standard classifiers
based on handcrafted features. With the proposed meta-learning based approach,
we achieved the area under the receiver operating characteristic curve of 0.95
and accuracy of 0.91.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karwat_P/0/1/0/all/0/1&quot;&gt;Piotr Karwat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ryzhankow_I/0/1/0/all/0/1&quot;&gt;Ivan Ryzhankow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Komorowski_P/0/1/0/all/0/1&quot;&gt;Piotr Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klimonda_Z/0/1/0/all/0/1&quot;&gt;Ziemowit Klimonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fura_L/0/1/0/all/0/1&quot;&gt;Lukasz Fura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pawlowska_A/0/1/0/all/0/1&quot;&gt;Anna Pawlowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zolek_N/0/1/0/all/0/1&quot;&gt;Norbert Zolek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Litniewski_J/0/1/0/all/0/1&quot;&gt;Jerzy Litniewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01917">
<title>Expanding Accurate Person Recognition to New Altitudes and Ranges: The BRIAR Dataset. (arXiv:2211.01917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01917</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition technology has advanced significantly in recent years due
largely to the availability of large and increasingly complex training datasets
for use in deep learning models. These datasets, however, typically comprise
images scraped from news sites or social media platforms and, therefore, have
limited utility in more advanced security, forensics, and military
applications. These applications require lower resolution, longer ranges, and
elevated viewpoints. To meet these critical needs, we collected and curated the
first and second subsets of a large multi-modal biometric dataset designed for
use in the research and development (R&amp;amp;D) of biometric recognition technologies
under extremely challenging conditions. Thus far, the dataset includes more
than 350,000 still images and over 1,300 hours of video footage of
approximately 1,000 subjects. To collect this data, we used Nikon DSLR cameras,
a variety of commercial surveillance cameras, specialized long-rage R&amp;amp;D
cameras, and Group 1 and Group 2 UAV platforms. The goal is to support the
development of algorithms capable of accurately recognizing people at ranges up
to 1,000 m and from high angles of elevation. These advances will include
improvements to the state of the art in face recognition and will support new
research in the area of whole-body recognition using methods based on gait and
anthropometry. This paper describes methods used to collect and curate the
dataset, and the dataset&apos;s characteristics at the current stage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornett_D/0/1/0/all/0/1&quot;&gt;David Cornett III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brogan_J/0/1/0/all/0/1&quot;&gt;Joel Brogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_N/0/1/0/all/0/1&quot;&gt;Nell Barber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aykac_D/0/1/0/all/0/1&quot;&gt;Deniz Aykac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baird_S/0/1/0/all/0/1&quot;&gt;Seth Baird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burchfield_N/0/1/0/all/0/1&quot;&gt;Nick Burchfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dukes_C/0/1/0/all/0/1&quot;&gt;Carl Dukes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_A/0/1/0/all/0/1&quot;&gt;Andrew Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrell_R/0/1/0/all/0/1&quot;&gt;Regina Ferrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goddard_J/0/1/0/all/0/1&quot;&gt;Jim Goddard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_G/0/1/0/all/0/1&quot;&gt;Gavin Jager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1&quot;&gt;Matt Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_B/0/1/0/all/0/1&quot;&gt;Bart Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_C/0/1/0/all/0/1&quot;&gt;Christi Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shelley_I/0/1/0/all/0/1&quot;&gt;Ian Shelley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_N/0/1/0/all/0/1&quot;&gt;Nisha Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stockwell_B/0/1/0/all/0/1&quot;&gt;Brandon Stockwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1&quot;&gt;Leanne Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yohe_M/0/1/0/all/0/1&quot;&gt;Matt Yohe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Robert Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolvin_S/0/1/0/all/0/1&quot;&gt;Scott Dolvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Villalobos_H/0/1/0/all/0/1&quot;&gt;Hector J. Santos-Villalobos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolme_D/0/1/0/all/0/1&quot;&gt;David S. Bolme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01930">
<title>Photorealistic Facial Wrinkles Removal. (arXiv:2211.01930v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01930</link>
<description rdf:parseType="Literal">&lt;p&gt;Editing and retouching facial attributes is a complex task that usually
requires human artists to obtain photo-realistic results. Its applications are
numerous and can be found in several contexts such as cosmetics or digital
media retouching, to name a few. Recently, advancements in conditional
generative modeling have shown astonishing results at modifying facial
attributes in a realistic manner. However, current methods are still prone to
artifacts, and focus on modifying global attributes like age and gender, or
local mid-sized attributes like glasses or moustaches. In this work, we revisit
a two-stage approach for retouching facial wrinkles and obtain results with
unprecedented realism. First, a state of the art wrinkle segmentation network
is used to detect the wrinkles within the facial region. Then, an inpainting
module is used to remove the detected wrinkles, filling them in with a texture
that is statistically consistent with the surrounding skin. To achieve this, we
introduce a novel loss term that reuses the wrinkle segmentation network to
penalize those regions that still contain wrinkles after the inpainting. We
evaluate our method qualitatively and quantitatively, showing state of the art
results for the task of wrinkle removal. Moreover, we introduce the first
high-resolution dataset, named FFHQ-Wrinkles, to evaluate wrinkle detection
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_M/0/1/0/all/0/1&quot;&gt;Marcelo Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triginer_G/0/1/0/all/0/1&quot;&gt;Gil Triginer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1&quot;&gt;Coloma Ballester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raad_L/0/1/0/all/0/1&quot;&gt;Lara Raad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1&quot;&gt;Eduard Ramon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01933">
<title>Automatic Crater Shape Retrieval using Unsupervised and Semi-Supervised Systems. (arXiv:2211.01933v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01933</link>
<description rdf:parseType="Literal">&lt;p&gt;Impact craters are formed due to continuous impacts on the surface of
planetary bodies. Most recent deep learning-based crater detection methods
treat craters as circular shapes, and less attention is paid to extracting the
exact shapes of craters. Extracting precise shapes of the craters can be
helpful for many advanced analyses, such as crater formation. This paper
proposes a combination of unsupervised non-deep learning and semi-supervised
deep learning approach to accurately extract shapes of the craters and detect
missing craters from the existing catalog. In unsupervised non-deep learning,
we have proposed an adaptive rim extraction algorithm to extract craters&apos;
shapes. In this adaptive rim extraction algorithm, we utilized the elevation
profiles of DEMs and applied morphological operation on DEM-derived slopes to
extract craters&apos; shapes. The extracted shapes of the craters are used in
semi-supervised deep learning to get the locations, size, and refined shapes.
Further, the extracted shapes of the craters are utilized to improve the
estimate of the craters&apos; diameter, depth, and other morphological factors. The
craters&apos; shape, estimated diameter, and depth with other morphological factors
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Atal Tewari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vikrant Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanna_N/0/1/0/all/0/1&quot;&gt;Nitin Khanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01946">
<title>Revisiting and Optimising a CNN Colour Constancy Method for Multi-Illuminant Estimation. (arXiv:2211.01946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01946</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of colour constancy is to discount the effect of the scene
illumination from the image colours and restore the colours of the objects as
captured under a &apos;white&apos; illuminant. For the majority of colour constancy
methods, the first step is to estimate the scene illuminant colour. Generally,
it is assumed that the illumination is uniform in the scene. However, real
world scenes have multiple illuminants, like sunlight and spot lights all
together in one scene. We present in this paper a simple yet very effective
framework using a deep CNN-based method to estimate and use multiple
illuminants for colour constancy. Our approach works well in both the multi and
single illuminant cases. The output of the CNN method is a region-wise estimate
map of the scene which is smoothed and divided out from the image to perform
colour constancy. The method that we propose outperforms other recent and state
of the art methods and has promising visual results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemrit_G/0/1/0/all/0/1&quot;&gt;Ghalia Hemrit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meehan_J/0/1/0/all/0/1&quot;&gt;Joseph Meehan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01950">
<title>Unlocking the potential of two-point cells for energy-efficient training of deep nets. (arXiv:2211.01950v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2211.01950</link>
<description rdf:parseType="Literal">&lt;p&gt;Context-sensitive two-point layer 5 pyramidal cells (L5PC) were discovered as
long ago as 1999. However, the potential of this discovery to provide useful
neural computation has yet to be demonstrated. Here we show for the first time
how a transformative L5PC-driven deep neural network (DNN), termed the
multisensory cooperative computing (MCC) architecture, can effectively process
large amounts of heterogeneous real-world audio-visual (AV) data, using far
less energy compared to best available `point&apos; neuron-driven DNNs. A novel
highly-distributed parallel implementation on a Xilinx UltraScale+ MPSoC device
estimates energy savings up to $245759 \times 50000$ $\mu$J (i.e., $62\%$ less
than the baseline model in a semi-supervised learning setup) where a single
synapse consumes $8e^{-5}\mu$J. In a supervised learning setup, the
energy-saving can potentially reach up to 1250x less (per feedforward
transmission) than the baseline model. This remarkable performance in pilot
experiments demonstrates the embodied neuromorphic intelligence of our proposed
L5PC based MCC architecture that contextually selects the most salient and
relevant information for onward transmission, from overwhelmingly large
multimodal information utilised at the early stages of on-chip training. Our
proposed approach opens new cross-disciplinary avenues for future on-chip DNN
training implementations and posits a radical shift in current neuromorphic
computing paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeel_A/0/1/0/all/0/1&quot;&gt;Ahsan Adeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adetomi_A/0/1/0/all/0/1&quot;&gt;Adewale Adetomi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_K/0/1/0/all/0/1&quot;&gt;Khubaib Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Amir Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arslan_T/0/1/0/all/0/1&quot;&gt;Tughrul Arslan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_W/0/1/0/all/0/1&quot;&gt;W.A. Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01957">
<title>Sub-network Multi-objective Evolutionary Algorithm for Filter Pruning. (arXiv:2211.01957v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2211.01957</link>
<description rdf:parseType="Literal">&lt;p&gt;Filter pruning is a common method to achieve model compression and
acceleration in deep neural networks (DNNs).Some research regarded filter
pruning as a combinatorial optimization problem and thus used evolutionary
algorithms (EA) to prune filters of DNNs. However, it is difficult to find a
satisfactory compromise solution in a reasonable time due to the complexity of
solution space searching. To solve this problem, we first formulate a
multi-objective optimization problem based on a sub-network of the full model
and propose a Sub-network Multiobjective Evolutionary Algorithm (SMOEA) for
filter pruning. By progressively pruning the convolutional layers in groups,
SMOEA can obtain a lightweight pruned result with better
performance.Experiments on VGG-14 model for CIFAR-10 verify the effectiveness
of the proposed SMOEA. Specifically, the accuracy of the pruned model with
16.56% parameters decreases by 0.28% only, which is better than the widely used
popular filter pruning criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weize Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaowu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01966">
<title>MarginNCE: Robust Sound Localization with a Negative Margin. (arXiv:2211.01966v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01966</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this work is to localize sound sources in visual scenes with a
self-supervised approach. Contrastive learning in the context of sound source
localization leverages the natural correspondence between audio and visual
signals where the audio-visual pairs from the same source are assumed as
positive, while randomly selected pairs are negatives. However, this approach
brings in noisy correspondences; for example, positive audio and visual pair
signals that may be unrelated to each other, or negative pairs that may contain
semantically similar samples to the positive one. Our key contribution in this
work is to show that using a less strict decision boundary in contrastive
learning can alleviate the effect of noisy correspondences in sound source
localization. We propose a simple yet effective approach by slightly modifying
the contrastive loss with a negative margin. Extensive experimental results
show that our approach gives on-par or better performance than the
state-of-the-art methods. Furthermore, we demonstrate that the introduction of
a negative margin to existing methods results in a consistent improvement in
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sooyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1&quot;&gt;Arda Senocak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Joon Son Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01969">
<title>Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing. (arXiv:2211.01969v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a framework for jointly grounding objects that follow
certain semantic relationship constraints given in a scene graph. A typical
natural scene contains several objects, often exhibiting visual relationships
of varied complexities between them. These inter-object relationships provide
strong contextual cues toward improving grounding performance compared to a
traditional object query-only-based localization task. A scene graph is an
efficient and structured way to represent all the objects and their semantic
relationships in the image. In an attempt towards bridging these two modalities
representing scenes and utilizing contextual information for improving object
localization, we rigorously study the problem of grounding scene graphs on
natural images. To this end, we propose a novel graph neural network-based
approach referred to as Visio-Lingual Message PAssing Graph Neural Network
(VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object
proposals as nodes and an edge between a pair of nodes representing a plausible
relation between them. Then a three-step inter-graph and intra-graph message
passing is performed to learn the context-dependent representation of the
proposals and query objects. These object representations are used to score the
proposals to generate object localization. The proposed method significantly
outperforms the baselines on four public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1&quot;&gt;Aditay Tripathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Anand Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Anirban Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01999">
<title>Quantifying Model Uncertainty for Semantic Segmentation using Operators in the RKHS. (arXiv:2211.01999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01999</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for semantic segmentation are prone to poor performance
in real-world applications due to the highly challenging nature of the task.
Model uncertainty quantification (UQ) is one way to address this issue of lack
of model trustworthiness by enabling the practitioner to know how much to trust
a segmentation output. Current UQ methods in this application domain are mainly
restricted to Bayesian based methods which are computationally expensive and
are only able to extract central moments of uncertainty thereby limiting the
quality of their uncertainty estimates. We present a simple framework for
high-resolution predictive uncertainty quantification of semantic segmentation
models that leverages a multi-moment functional definition of uncertainty
associated with the model&apos;s feature space in the reproducing kernel Hilbert
space (RKHS). The multiple uncertainty functionals extracted from this
framework are defined by the local density dynamics of the model&apos;s feature
space and hence automatically align themselves at the tail-regions of the
intrinsic probability density function of the feature space (where uncertainty
is the highest) in such a way that the successively higher order moments
quantify the more uncertain regions. This leads to a significantly more
accurate view of model uncertainty than conventional Bayesian methods.
Moreover, the extraction of such moments is done in a single-shot computation
making it much faster than Bayesian and ensemble approaches (that involve a
high number of forward stochastic passes of the model to quantify its
uncertainty). We demonstrate these advantages through experimental evaluations
of our framework implemented over four different state-of-the-art model
architectures that are trained and evaluated on two benchmark road-scene
segmentation datasets (Camvid and Cityscapes).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02006">
<title>SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency. (arXiv:2211.02006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.02006</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the dominant DETR-based approaches apply central-concept spatial
prior to accelerate Transformer detector convergency. These methods gradually
refine the reference points to the center of target objects and imbue object
queries with the updated central reference information for spatially
conditional attention. However, centralizing reference points may severely
deteriorate queries&apos; saliency and confuse detectors due to the indiscriminative
spatial prior. To bridge the gap between the reference points of salient
queries and Transformer detectors, we propose SAlient Point-based DETR
(SAP-DETR) by treating object detection as a transformation from salient points
to instance objects. In SAP-DETR, we explicitly initialize a query-specific
reference point for each object query, gradually aggregate them into an
instance object, and then predict the distance from each side of the bounding
box to these points. By rapidly attending to query-specific reference region
and other conditional extreme regions from the image features, SAP-DETR can
effectively bridge the gap between the salient point and the query-based
Transformer detector with a significant convergency speed. Our extensive
experiments have demonstrated that SAP-DETR achieves 1.4 times convergency
speed with competitive performance. Under the standard training scheme,
SAP-DETR stably promotes the SOTA approaches by 1.0 AP. Based on ResNet-DC-101,
SAP-DETR achieves 46.9 AP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jiang Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02043">
<title>Could Giant Pretrained Image Models Extract Universal Representations?. (arXiv:2211.02043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.02043</link>
<description rdf:parseType="Literal">&lt;p&gt;Frozen pretrained models have become a viable alternative to the
pretraining-then-finetuning paradigm for transfer learning. However, with
frozen models there are relatively few parameters available for adapting to
downstream tasks, which is problematic in computer vision where tasks vary
significantly in input/output format and the type of information that is of
value. In this paper, we present a study of frozen pretrained models when
applied to diverse and representative computer vision tasks, including object
detection, semantic segmentation and video action recognition. From this
empirical analysis, our work answers the questions of what pretraining task
fits best with this frozen setting, how to make the frozen setting more
flexible to various downstream tasks, and the effect of larger model sizes. We
additionally examine the upper bound of performance using a giant frozen
pretrained model with 3 billion parameters (SwinV2-G) and find that it reaches
competitive performance on a varied set of major benchmarks with only one
shared frozen base network: 60.0 box mAP and 52.2 mask mAP on COCO object
detection test-dev, 57.6 val mIoU on ADE20K semantic segmentation, and 81.7
top-1 accuracy on Kinetics-400 action recognition. With this work, we hope to
bring greater attention to this promising path of freezing pretrained image
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yutong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Stephen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yue Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02048">
<title>Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.02048</link>
<description rdf:parseType="Literal">&lt;p&gt;During image editing, existing deep generative models tend to re-synthesize
the entire output from scratch, including the unedited regions. This leads to a
significant waste of computation, especially for minor editing operations. In
this work, we present Spatially Sparse Inference (SSI), a general-purpose
technique that selectively performs computation for edited regions and
accelerates various generative models, including both conditional GANs and
diffusion models. Our key observation is that users tend to make gradual
changes to the input image. This motivates us to cache and reuse the feature
maps of the original image. Given an edited image, we sparsely apply the
convolutional filters to the edited regions while reusing the cached features
for the unedited regions. Based on our algorithm, we further propose Sparse
Incremental Generative Engine (SIGE) to convert the computation reduction to
latency reduction on off-the-shelf hardware. With 1.2%-area edited regions, our
method reduces the computation of DDIM by 7.5$\times$ and GauGAN by 18$\times$
while preserving the visual fidelity. With SIGE, we accelerate the speed of
DDIM by 3.0x on RTX 3090 and 6.6$\times$ on Apple M1 Pro CPU, and GauGAN by
4.2$\times$ on RTX 3090 and 14$\times$ on Apple M1 Pro CPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Muyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Ji Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.05470">
<title>Standardised convolutional filtering for radiomics. (arXiv:2006.05470v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2006.05470</link>
<description rdf:parseType="Literal">&lt;p&gt;The Image Biomarker Standardisation Initiative (IBSI) aims to improve
reproducibility of radiomics studies by standardising the computational process
of extracting image biomarkers (features) from images. We have previously
established reference values for 169 commonly used features, created a standard
radiomics image processing scheme, and developed reporting guidelines for
radiomic studies. However, several aspects are not standardised.
&lt;/p&gt;
&lt;p&gt;Here we present a preliminary version of a reference manual on the use of
convolutional image filters in radiomics. Filters, such as wavelets or
Laplacian of Gaussian filters, play an important part in emphasising specific
image characteristics such as edges and blobs. Features derived from filter
response maps have been found to be poorly reproducible. This reference manual
forms the basis of ongoing work on standardising convolutional filters in
radiomics, and will be updated as this work progresses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Whybra_P/0/1/0/all/0/1&quot;&gt;Philip Whybra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Griethuysen_J/0/1/0/all/0/1&quot;&gt;Joost van Griethuysen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaer_R/0/1/0/all/0/1&quot;&gt;Roger Schaer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vallieres_M/0/1/0/all/0/1&quot;&gt;Martin Valli&amp;#xe8;res&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zwanenburg_A/0/1/0/all/0/1&quot;&gt;Alex Zwanenburg&lt;/a&gt; (for the Image Biomarker Standardisation Initiative)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.08930">
<title>A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2008.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have been widely applied in different
scenarios thanks to the development of deep neural networks. The original GAN
was proposed based on the non-parametric assumption of the infinite capacity of
networks. However, it is still unknown whether GANs can fit the target
distribution without any prior information. Due to the overconfident
assumption, many issues remain unaddressed in GANs&apos; training, such as
non-convergence, mode collapses, gradient vanishing. Regularization and
normalization are common methods of introducing prior information to stabilize
training and improve discrimination. Although a handful number of
regularization and normalization methods have been proposed for GANs, to the
best of our knowledge, there exists no comprehensive survey that primarily
focuses on objectives and development of these methods, apart from some
in-comprehensive and limited scope studies. In this work, we conduct a
comprehensive survey on the regularization and normalization techniques from
different perspectives of GANs training. First, we systematically describe
different perspectives of GANs training and thus obtain the different
objectives of regularization and normalization. Based on these objectives, we
propose a new taxonomy. Furthermore, we compare the performance of the
mainstream methods on different datasets and investigate the applications of
regularization and normalization techniques that have been frequently employed
in state-of-the-art GANs. Finally, we highlight potential future directions of
research in this domain. Code and studies related to the regularization and
normalization of GANs in this work is summarized on
https://github.com/iceli1007/GANs-Regularization-Review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Rentuo Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1&quot;&gt;Pengfei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.10443">
<title>Towards glass-box CNNs. (arXiv:2101.10443v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2101.10443</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution neural networks (CNNs) are brain-inspired architectures popular
for their ability to train and relearn visually complex tasks. It is
incremental and scalable; however, CNN is mostly treated as black-box and
involves multiple trial &amp;amp; error runs. We observe that CNN constructs powerful
internal representations that help achieve state-of-the-art performance. Here
we propose three layer glass-box (analytical) CNN for two-class image
classifcation problems. First is a representation layer that encompasses both
the class information (group invariant) and symmetric transformations (group
equivariant) of input images. It is then passed through dimension reduction
layer (PCA). Finally the compact yet complete representation is provided to a
classifer. Analytical machine learning classifers and multilayer perceptrons
are used to assess sensitivity. Proposed glass-box CNN is compared with
equivariance of AlexNet (CNN) internal representation for better understanding
and dissemination of results. In future, we would like to construct glass-box
CNN for multiclass visually complex tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manaswini_P/0/1/0/all/0/1&quot;&gt;Piduguralla Manaswini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_J/0/1/0/all/0/1&quot;&gt;Jignesh S. Bhatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.14027">
<title>USB: Universal-Scale Object Detection Benchmark. (arXiv:2103.14027v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.14027</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarks, such as COCO, play a crucial role in object detection. However,
existing benchmarks are insufficient in scale variation, and their protocols
are inadequate for fair comparison. In this paper, we introduce the
Universal-Scale object detection Benchmark (USB). USB has variations in object
scales and image domains by incorporating COCO with the recently proposed Waymo
Open Dataset and Manga109-s dataset. To enable fair comparison and inclusive
research, we propose training and evaluation protocols. They have multiple
divisions for training epochs and evaluation image resolutions, like weight
classes in sports, and compatibility across training protocols, like the
backward compatibility of the Universal Serial Bus. Specifically, we request
participants to report results with not only higher protocols (longer training)
but also lower protocols (shorter training). Using the proposed benchmark and
protocols, we conducted extensive experiments using 15 methods and found
weaknesses of existing COCO-biased methods. The code is available at
https://github.com/shinya7y/UniverseNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1&quot;&gt;Yosuke Shinya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.15670">
<title>On the Adversarial Robustness of Vision Transformers. (arXiv:2103.15670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.15670</link>
<description rdf:parseType="Literal">&lt;p&gt;Following the success in advancing natural language processing and
understanding, transformers are expected to bring revolutionary changes to
computer vision. This work provides a comprehensive study on the robustness of
vision transformers (ViTs) against adversarial perturbations. Tested on various
white-box and transfer attack settings, we find that ViTs possess better
adversarial robustness when compared with MLP-Mixer and convolutional neural
networks (CNNs) including ConvNeXt, and this observation also holds for
certified robustness. Through frequency analysis and feature visualization, we
summarize the following main observations contributing to the improved
robustness of ViTs: 1) Features learned by ViTs contain less high-frequency
patterns that have spurious correlation, which helps explain why ViTs are less
sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is
a high correlation between how much the model learns high-frequency features
and its robustness against different frequency-based perturbations. 2)
Introducing convolutional or tokens-to-token blocks for learning high-frequency
features in ViTs can improve classification accuracy but at the cost of
adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs
including activation function, layer norm, larger kernel size to imitate the
global attention, and patchify the images as inputs, etc., could help bridge
the performance gap between ViTs and CNNs not only in terms of performance, but
also certified and empirical adversarial robustness. Moreover, we show
adversarial training is also applicable to ViT for training robust models, and
sharpness-aware minimization can also help improve robustness, while
pre-training with clean images on larger datasets does not significantly
improve adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.10611">
<title>FourierNets enable the design of highly non-local optical encoders for computational imaging. (arXiv:2104.10611v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.10611</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable simulations of optical systems can be combined with deep
learning-based reconstruction networks to enable high performance computational
imaging via end-to-end (E2E) optimization of both the optical encoder and the
deep decoder. This has enabled imaging applications such as 3D localization
microscopy, depth estimation, and lensless photography via the optimization of
local optical encoders. More challenging computational imaging applications,
such as 3D snapshot microscopy which compresses 3D volumes into single 2D
images, require a highly non-local optical encoder. We show that existing deep
network decoders have a locality bias which prevents the optimization of such
highly non-local optical encoders. We address this with a decoder based on a
shallow neural network architecture using global kernel Fourier convolutional
neural networks (FourierNets). We show that FourierNets surpass existing deep
network based decoders at reconstructing photographs captured by the highly
non-local DiffuserCam optical encoder. Further, we show that FourierNets enable
E2E optimization of highly non-local optical encoders for 3D snapshot
microscopy. By combining FourierNets with a large-scale multi-GPU
differentiable optical simulation, we are able to optimize non-local optical
encoders 170$\times$ to 7372$\times$ larger than prior state of the art, and
demonstrate the potential for ROI-type specific optical encoding with a
programmable microscope.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deb_D/0/1/0/all/0/1&quot;&gt;Diptodip Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sims_R/0/1/0/all/0/1&quot;&gt;Ruth Sims&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Alex B. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Broxton_M/0/1/0/all/0/1&quot;&gt;Michael Broxton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahrens_M/0/1/0/all/0/1&quot;&gt;Misha B. Ahrens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Podgorski_K/0/1/0/all/0/1&quot;&gt;Kaspar Podgorski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turaga_S/0/1/0/all/0/1&quot;&gt;Srinivas C. Turaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.03149">
<title>Large-scale Unsupervised Semantic Segmentation. (arXiv:2106.03149v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;Empowered by large datasets, e.g., ImageNet, unsupervised learning on
large-scale data has enabled significant advances for classification tasks.
However, whether the large-scale unsupervised semantic segmentation can be
achieved remains unknown. There are two major challenges: i) we need a
large-scale benchmark for assessing algorithms; ii) we need to develop methods
to simultaneously learn category and shape representation in an unsupervised
manner. In this work, we propose a new problem of large-scale unsupervised
semantic segmentation (LUSS) with a newly created benchmark dataset to help the
research progress. Building on the ImageNet dataset, we propose the ImageNet-S
dataset with 1.2 million training images and 50k high-quality semantic
segmentation annotations for evaluation. Our benchmark has a high data
diversity and a clear task objective. We also present a simple yet effective
method that works surprisingly well for LUSS. In addition, we benchmark related
un/weakly/fully supervised methods accordingly, identifying the challenges and
possible directions of LUSS. The benchmark and source code is publicly
available at https://github.com/LUSSeg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shanghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong-Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junwei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05765">
<title>DHA: End-to-End Joint Optimization of Data Augmentation Policy, Hyper-parameter and Architecture. (arXiv:2109.05765v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05765</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated machine learning (AutoML) usually involves several crucial
components, such as Data Augmentation (DA) policy, Hyper-Parameter Optimization
(HPO), and Neural Architecture Search (NAS). Although many strategies have been
developed for automating these components in separation, joint optimization of
these components remains challenging due to the largely increased search
dimension and the variant input types of each component. In parallel to this,
the common practice of searching for the optimal architecture first and then
retraining it before deployment in NAS often suffers from low performance
correlation between the searching and retraining stages. An end-to-end solution
that integrates the AutoML components and returns a ready-to-use model at the
end of the search is desirable. In view of these, we propose DHA, which
achieves joint optimization of Data augmentation policy, Hyper-parameter and
Architecture. Specifically, end-to-end NAS is achieved in a differentiable
manner by optimizing a compressed lower-dimensional feature space, while DA
policy and HPO are regarded as dynamic schedulers, which adapt themselves to
the update of network parameters and network architecture at the same time.
Experiments show that DHA achieves state-of-the-art (SOTA) results on various
datasets and search spaces. To the best of our knowledge, we are the first to
efficiently and jointly optimize DA policy, NAS, and HPO in an end-to-end
manner without retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shoukang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.08868">
<title>Clean-label Backdoor Attack against Deep Hashing based Retrieval. (arXiv:2109.08868v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.08868</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep hashing has become a popular method in large-scale image retrieval due
to its computational and storage efficiency. However, recent works raise the
security concerns of deep hashing. Although existing works focus on the
vulnerability of deep hashing in terms of adversarial perturbations, we
identify a more pressing threat, backdoor attack, when the attacker has access
to the training data. A backdoored deep hashing model behaves normally on
original query images, while returning the images with the target label when
the trigger presents, which makes the attack hard to be detected. In this
paper, we uncover this security concern by utilizing clean-label data
poisoning. To the best of our knowledge, this is the first attempt at the
backdoor attack against deep hashing models. To craft the poisoned images, we
first generate the targeted adversarial patch as the backdoor trigger.
Furthermore, we propose the confusing perturbations to disturb the hashing code
learning, such that the hashing model can learn more about the trigger. The
confusing perturbations are imperceptible and generated by dispersing the
images with the target label in the Hamming space. We have conducted extensive
experiments to verify the efficacy of our backdoor attack under various
settings. For instance, it can achieve 63% targeted mean average precision on
ImageNet under 48 bits code length with only 40 poisoned images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kuofeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jiawang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongxian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13751">
<title>StereoSpike: Depth Learning with a Spiking Neural Network. (arXiv:2109.13751v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13751</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation is an important computer vision task, useful in particular
for navigation in autonomous vehicles, or for object manipulation in robotics.
Here we solved it using an end-to-end neuromorphic approach, combining two
event-based cameras and a Spiking Neural Network (SNN) with a slightly modified
U-Net-like encoder-decoder architecture, that we named StereoSpike. More
specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It
provides a depth ground-truth, which was used to train StereoSpike in a
supervised manner, using surrogate gradient descent. We propose a novel readout
paradigm to obtain a dense analog prediction -- the depth of each pixel -- from
the spikes of the decoder. We demonstrate that this architecture generalizes
very well, even better than its non-spiking counterparts, leading to
state-of-the-art test accuracy. To the best of our knowledge, it is the first
time that such a large-scale regression problem is solved by a fully spiking
network. Finally, we show that low firing rates (&amp;lt;10%) can be obtained via
regularization, with a minimal cost in accuracy. This means that StereoSpike
could be efficiently implemented on neuromorphic chips, opening the door for
low power and real time embedded systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rancon_U/0/1/0/all/0/1&quot;&gt;Ulysse Ran&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuadrado_Anibarro_J/0/1/0/all/0/1&quot;&gt;Javier Cuadrado-Anibarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cottereau_B/0/1/0/all/0/1&quot;&gt;Benoit R. Cottereau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.00242">
<title>3rd Place Scheme on Instance Segmentation Track of ICCV 2021 VIPriors Challenges. (arXiv:2110.00242v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.00242</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a data-efficient instance segmentation method we
used in the 2021 VIPriors Instance Segmentation Challenge. Our solution is a
modified version of Swin Transformer, based on the mmdetection which is a
powerful toolbox. To solve the problem of lack of data, we utilize data
augmentation including random flip and multiscale training to train our model.
During inference, multiscale fusion is used to boost the performance. We only
use a single GPU during the whole training and testing stages. In the end, our
team achieved the result of 0.366 for AP@0.50:0.95 on the test set, which is
competitive with other top-ranking methods while only one GPU is used. Besides,
our method achieved the AP@0.50:0.95 (medium) of 0.592, which ranks second
among all contestants. In the end, our team ranked third among all the
contestants, as announced by the organizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wanhua Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.02399">
<title>VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.02399</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention
recently for its transferable visual representation learning. However, due to
the semantic gap within datasets, CLIP&apos;s pre-trained image-text alignment
becomes sub-optimal on downstream tasks, which severely harms its transferring
performance. To better adapt the cross-modality embedding space, we propose to
enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide
textual features of different categories to adaptively explore informative
regions on the image and aggregate visual features by attention mechanisms. In
this way, the texts become visual-guided, namely, more semantically correlated
with downstream images, which greatly benefits the category-wise matching
process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known
classification datasets to demonstrate its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Longtian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyao Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangnan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09802">
<title>Automated Domain Discovery from Multiple Sources to Improve Zero-Shot Generalization. (arXiv:2112.09802v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) methods aim to develop models that generalize to
settings where the test distribution is different from the training data. In
this paper, we focus on the challenging problem of multi-source zero shot DG
(MDG), where labeled training data from multiple source domains is available
but with no access to data from the target domain. A wide range of solutions
have been proposed for this problem, including the state-of-the-art
multi-domain ensembling approaches. Despite these advances, the na\&quot;ive ERM
solution of pooling all source data together and training a single classifier
is surprisingly effective on standard benchmarks. In this paper, we hypothesize
that, it is important to elucidate the link between pre-specified domain labels
and MDG performance, in order to explain this behavior. More specifically, we
consider two popular classes of MDG algorithms -- distributional robust
optimization (DRO) and multi-domain ensembles, in order to demonstrate how
inferring custom domain groups can lead to consistent improvements over the
original domain labels that come with the dataset. To this end, we propose (i)
Group-DRO++, which incorporates an explicit clustering step to identify custom
domains in an existing DRO technique; and (ii) DReaME, which produces effective
multi-domain ensembles through implicit domain re-labeling with a novel
meta-optimization algorithm. Using empirical studies on multiple standard
benchmarks, we show that our variants consistently outperform ERM by
significant margins (1.5% - 9%), and produce state-of-the-art MDG performance.
Our code can be found at https://github.com/kowshikthopalli/DREAME
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thopalli_K/0/1/0/all/0/1&quot;&gt;Kowshik Thopalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katoch_S/0/1/0/all/0/1&quot;&gt;Sameeksha Katoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1&quot;&gt;Pavan Turaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09130">
<title>Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review. (arXiv:2201.09130v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09130</link>
<description rdf:parseType="Literal">&lt;p&gt;Death by suicide is the seventh leading death cause worldwide. The recent
advancement in Artificial Intelligence (AI), specifically AI applications in
image and voice processing, has created a promising opportunity to
revolutionize suicide risk assessment. Subsequently, we have witnessed
fast-growing literature of research that applies AI to extract audiovisual
non-verbal cues for mental illness assessment. However, the majority of the
recent works focus on depression, despite the evident difference between
depression symptoms and suicidal behavior and non-verbal cues. This paper
reviews recent works that study suicide ideation and suicide behavior detection
through audiovisual feature analysis, mainly suicidal voice/speech acoustic
features analysis and suicidal visual cues. Automatic suicide assessment is a
promising research direction that is still in the early stages. Accordingly,
there is a lack of large datasets that can be used to train machine learning
and deep learning models proven to be effective in other, similar tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhelim_S/0/1/0/all/0/1&quot;&gt;Sahraoui Dhelim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1&quot;&gt;Huansheng Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugent_C/0/1/0/all/0/1&quot;&gt;Chris Nugent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.02846">
<title>Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images. (arXiv:2203.02846v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.02846</link>
<description rdf:parseType="Literal">&lt;p&gt;Top-down instance segmentation framework has shown its superiority in object
detection compared to the bottom-up framework. While it is efficient in
addressing over-segmentation, top-down instance segmentation suffers from
over-crop problem. However, a complete segmentation mask is crucial for
biological image analysis as it delivers important morphological properties
such as shapes and volumes. In this paper, we propose a region proposal
rectification (RPR) module to address this challenging incomplete segmentation
problem. In particular, we offer a progressive ROIAlign module to introduce
neighbor information into a series of ROIs gradually. The ROI features are fed
into an attentive feed-forward network (FFN) for proposal box regression. With
additional neighbor information, the proposed RPR module shows significant
improvement in correction of region proposal locations and thereby exhibits
favorable instance segmentation performances on three biological image datasets
compared to state-of-the-art baseline methods. Experimental results demonstrate
that the proposed RPR module is effective in both anchor-based and anchor-free
top-down instance segmentation approaches, suggesting the proposed method can
be applied to general top-down instance segmentation of biological images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhangli_Q/0/1/0/all/0/1&quot;&gt;Qilong Zhangli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jingru Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Di Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1&quot;&gt;Qi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Ligong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunhe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Song Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.09303">
<title>MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks. (arXiv:2203.09303v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.09303</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous systems not only need to understand their current environment, but
should also be able to predict future actions conditioned on past states, for
instance based on captured camera frames. However, existing models mainly focus
on forecasting future video frames for short time-horizons, hence being of
limited use for long-term action planning. We propose Multi-Scale Hierarchical
Prediction (MSPred), a novel video prediction model able to simultaneously
forecast future possible outcomes of different levels of granularity at
different spatio-temporal scales. By combining spatial and temporal
downsampling, MSPred efficiently predicts abstract representations such as
human poses or locations over long time horizons, while still maintaining a
competitive performance for video frame prediction. In our experiments, we
demonstrate that MSPred accurately predicts future video frames as well as
high-level representations (e.g. keypoints or semantics) on bin-picking and
action recognition datasets, while consistently outperforming popular
approaches for future frame prediction. Furthermore, we ablate different
modules and design choices in MSPred, experimentally validating that combining
features of different spatial and temporal granularity leads to a superior
performance. Code and models to reproduce our experiments can be found in
https://github.com/AIS-Bonn/MSPred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1&quot;&gt;Angel Villar-Corrales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karapetyan_A/0/1/0/all/0/1&quot;&gt;Ani Karapetyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boltres_A/0/1/0/all/0/1&quot;&gt;Andreas Boltres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1&quot;&gt;Sven Behnke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13479">
<title>Enhancing Transferability of Adversarial Examples with Spatial Momentum. (arXiv:2203.13479v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13479</link>
<description rdf:parseType="Literal">&lt;p&gt;Many adversarial attack methods achieve satisfactory attack success rates
under the white-box setting, but they usually show poor transferability when
attacking other DNN models. Momentum-based attack is one effective method to
improve transferability. It integrates the momentum term into the iterative
process, which can stabilize the update directions by adding the gradients&apos;
temporal correlation for each pixel. We argue that only this temporal momentum
is not enough, the gradients from the spatial domain within an image, i.e.
gradients from the context pixels centered on the target pixel are also
important to the stabilization. For that, we propose a novel method named
Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the
mechanism of momentum accumulation from temporal domain to spatial domain by
considering the context information from different regions within the image.
SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize
the gradients&apos; update direction from both the temporal and spatial domains.
Extensive experiments show that our method indeed further enhances adversarial
transferability. It achieves the best transferability success rate for multiple
mainstream undefended and defended models, which outperforms the
state-of-the-art attack methods by a large margin of 10\% on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoqiu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Huanqian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.04944">
<title>Semantic Segmentation for Point Cloud Scenes via Dilated Graph Feature Aggregation and Pyramid Decoders. (arXiv:2204.04944v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.04944</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of point clouds generates comprehensive understanding
of scenes through densely predicting the category for each point. Due to the
unicity of receptive field, semantic segmentation of point clouds remains
challenging for the expression of multi-receptive field features, which brings
about the misclassification of instances with similar spatial structures. In
this paper, we propose a graph convolutional network DGFA-Net rooted in dilated
graph feature aggregation (DGFA), guided by multi-basis aggregation loss
(MALoss) calculated through Pyramid Decoders. To configure multi-receptive
field features, DGFA which takes the proposed dilated graph convolution
(DGConv) as its basic building block, is designed to aggregate multi-scale
feature representation by capturing dilated graphs with various receptive
regions. By simultaneously considering penalizing the receptive field
information with point sets of different resolutions as calculation bases, we
introduce Pyramid Decoders driven by MALoss for the diversity of receptive
field bases. Combining these two aspects, DGFA-Net significantly improves the
segmentation performance of instances with similar spatial structures.
Experiments on S3DIS, ShapeNetPart and Toronto-3D show that DGFA-Net
outperforms the baseline approach, achieving a new state-of-the-art
segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1&quot;&gt;Wenhui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zonghao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaonan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kun Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.00447">
<title>CD$^2$: Fine-grained 3D Mesh Reconstruction with Twice Chamfer Distance. (arXiv:2206.00447v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.00447</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular 3D reconstruction is to reconstruct the shape of object and its
other information from a single RGB image. In 3D reconstruction, polygon mesh,
with detailed surface information and low computational cost, is the most
prevalent expression form obtained from deep learning models. However, the
state-of-the-art schemes fail to directly generate well-structured meshes, and
most of meshes have two severe problems Vertices Clustering (VC) and Illegal
Twist (IT). By diving into the mesh deformation process, we pinpoint that the
inappropriate usage of Chamfer Distance (CD) loss is the root causes of VC and
IT problems in the training of deep learning model. In this paper, we initially
demonstrate these two problems induced by CD loss with visual examples and
quantitative analyses. Then, we propose a fine-grained reconstruction method
CD$^2$ by employing Chamfer distance twice to perform a plausible and adaptive
deformation. Extensive experiments on two 3D datasets and comparisons with five
latest schemes demonstrate that our CD$^2$ directly generates well-structured
meshes and outperforms others by alleviating VC and IT problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1&quot;&gt;Rongfei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1&quot;&gt;Mai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruiyun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11443">
<title>Image-based Stability Quantification. (arXiv:2206.11443v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11443</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative evaluation of human stability using foot pressure/force
measurement hardware and motion capture (mocap) technology is expensive, time
consuming, and restricted to the laboratory. We propose a novel image-based
method to estimate three key components for stability computation: Center of
Mass (CoM), Base of Support (BoS), and Center of Pressure (CoP). Furthermore,
we quantitatively validate our image-based methods for computing two classic
stability measures, CoMtoCoP and CoMtoBoS distances, against values generated
directly from laboratory-based sensor output (ground truth) using a publicly
available, multi-modality (mocap, foot pressure, two-view videos), ten-subject
human motion dataset. Using Leave One Subject Out (LOSO) cross-validation,
experimental results show: 1) our image-based CoM estimation method (CoMNet)
consistently outperforms state-of-the-art inertial sensor-based CoM estimation
techniques; 2) stability computed by our image-based method combined with
insole foot pressure sensor data produces consistent, strong, and statistically
significant correlation with ground truth stability measures (CoMtoCoP r = 0.79
p &amp;lt; 0.001, CoMtoBoS r = 0.75 p &amp;lt; 0.001); 3) our fully image-based estimation of
stability produces consistent, positive, and statistically significant
correlation on the two stability metrics (CoMtoCoP r = 0.31 p &amp;lt; 0.001, CoMtoBoS
r = 0.22 p &amp;lt; 0.043). Our study provides promising quantitative evidence for the
feasibility of image-based stability evaluation in natural environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1&quot;&gt;Jesse Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Challis_J/0/1/0/all/0/1&quot;&gt;John Challis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_R/0/1/0/all/0/1&quot;&gt;Robert T. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanxi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00225">
<title>Keeping Less is More: Point Sparsification for Visual SLAM. (arXiv:2207.00225v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2207.00225</link>
<description rdf:parseType="Literal">&lt;p&gt;When adapting Simultaneous Mapping and Localization (SLAM) to real-world
applications, such as autonomous vehicles, drones, and augmented reality
devices, its memory footprint and computing cost are the two main factors
limiting the performance and the range of applications. In sparse feature based
SLAM algorithms, one efficient way for this problem is to limit the map point
size by selecting the points potentially useful for local and global bundle
adjustment (BA). This study proposes an efficient graph optimization for
sparsifying map points in SLAM systems. Specifically, we formulate a maximum
pose-visibility and maximum spatial diversity problem as a minimum-cost
maximum-flow graph optimization problem. The proposed method works as an
additional step in existing SLAM systems, so it can be used in both
conventional or learning based SLAM systems. By extensive experimental
evaluations we demonstrate the proposed method achieves even more accurate
camera poses with approximately 1/3 of the map points and 1/2 of the
computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yeonsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Soohyun Bae&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.05876">
<title>Adaptive Diffusion Priors for Accelerated MRI Reconstruction. (arXiv:2207.05876v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.05876</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep MRI reconstruction is commonly performed with conditional models that
de-alias undersampled acquisitions to recover images consistent with
fully-sampled data. Since conditional models are trained with knowledge of the
imaging operator, they can show poor generalization across variable operators.
Unconditional models instead learn generative image priors decoupled from the
imaging operator to improve reliability against domain shifts. Recent diffusion
models are particularly promising given their high sample fidelity.
Nevertheless, inference with a static image prior can perform suboptimally.
Here we propose the first adaptive diffusion prior for MRI reconstruction,
AdaDiff, to improve performance and reliability against domain shifts. AdaDiff
leverages an efficient diffusion prior trained via adversarial mapping over
large reverse diffusion steps. A two-phase reconstruction is executed following
training: a rapid-diffusion phase that produces an initial reconstruction with
the trained prior, and an adaptation phase that further refines the result by
updating the prior to minimize reconstruction loss on acquired data.
Demonstrations on multi-contrast brain MRI clearly indicate that AdaDiff
outperforms competing conditional and unconditional methods under domain
shifts, and achieves superior or on par within-domain performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gungor_A/0/1/0/all/0/1&quot;&gt;Alper G&amp;#xfc;ng&amp;#xf6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1&quot;&gt;Salman UH Dar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozturk_S/0/1/0/all/0/1&quot;&gt;&amp;#x15e;aban &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1&quot;&gt;Yilmaz Korkmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1&quot;&gt;Gokberk Elmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1&quot;&gt;Muzaffer &amp;#xd6;zbey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1&quot;&gt;Tolga &amp;#xc7;ukur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08581">
<title>Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-Ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy, loss and area under the curve) and reduction of execution
times will be studied with respect to the classical case (the centralized
approach). Different clients will be simulated from the training data, selected
in an unbalanced manner. The results of considering three or ten clients are
exposed and compared between them and against the centralized case. Two
different problems related to intermittent clients are discussed, together with
two approaches to be followed for each of them. Specifically, this type of
problems may occur because in a real scenario some clients may leave the
training, and others enter it, and on the other hand because of client
technical or connectivity problems. Finally, improvements and future work in
the field are proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Judith S&amp;#xe1;inz-Pardo D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro L&amp;#xf3;pez Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10936">
<title>Long-tailed Instance Segmentation using Gumbel Optimized Loss. (arXiv:2207.10936v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10936</link>
<description rdf:parseType="Literal">&lt;p&gt;Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandridis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Panagiotis Alexandridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.03917">
<title>Exploring Target Representations for Masked Autoencoders. (arXiv:2209.03917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.03917</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked autoencoders have become popular training paradigms for
self-supervised visual representation learning. These models randomly mask a
portion of the input and reconstruct the masked portion according to the target
representations. In this paper, we first show that a careful choice of the
target representation is unnecessary for learning good representations, since
different targets tend to derive similarly behaved models. Driven by this
observation, we propose a multi-stage masked distillation pipeline and use a
randomly initialized model as the teacher, enabling us to effectively train
high-capacity models without any efforts to carefully design target
representations. Interestingly, we further explore using teachers of larger
capacity, obtaining distilled students with remarkable transferring ability. On
different tasks of classification, transfer learning, object detection, and
semantic segmentation, the proposed method to perform masked knowledge
distillation with bootstrapped teachers (dBOT) outperforms previous
self-supervised methods by nontrivial margins. We hope our findings, as well as
the proposed method, could motivate people to rethink the roles of target
representations in pre-training masked autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xingbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinghao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1&quot;&gt;Tao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xianming Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11350">
<title>Oracle Analysis of Representations for Deep Open Set Detection. (arXiv:2209.11350v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11350</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of detecting a novel class at run time is known as Open Set
Detection &amp;amp; is important for various real-world applications like medical
application, autonomous driving, etc. Open Set Detection within context of deep
learning involves solving two problems: (i) Must map the input images into a
latent representation that contains enough information to detect the outliers,
and (ii) Must learn an anomaly scoring function that can extract this
information from the latent representation to identify the anomalies. Research
in deep anomaly detection methods has progressed slowly. One reason may be that
most papers simultaneously introduce new representation learning techniques and
new anomaly scoring approaches. The goal of this work is to improve this
methodology by providing ways of separately measuring the effectiveness of the
representation learning and anomaly scoring. This work makes two methodological
contributions. The first is to introduce the notion of Oracle anomaly detection
for quantifying the information available in a learned latent representation.
The second is to introduce Oracle representation learning, which produces a
representation that is guaranteed to be sufficient for accurate anomaly
detection. These two techniques help researchers to separate the quality of the
learned representation from the performance of the anomaly scoring mechanism so
that they can debug and improve their systems. The methods also provide an
upper limit on how much open category detection can be improved through better
anomaly scoring mechanisms. The combination of the two oracles gives an upper
limit on the performance that any open category detection method could achieve.
This work introduces these two oracle techniques and demonstrates their utility
by applying them to several leading open category detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrepalli_R/0/1/0/all/0/1&quot;&gt;Risheek Garrepalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14610">
<title>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14610</link>
<description rdf:parseType="Literal">&lt;p&gt;Mathematical reasoning, a core ability of human intelligence, presents unique
challenges for machines in abstract thinking and logical reasoning. Recent
large pre-trained language models such as GPT-3 have achieved remarkable
progress on mathematical reasoning tasks written in text form, such as math
word problems (MWP). However, it is unknown if the models can handle more
complex problems that involve math reasoning over heterogeneous information,
such as tabular data. To fill the gap, we present Tabular Math Word Problems
(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that
require mathematical reasoning on both textual and tabular data. Each question
in TabMWP is aligned with a tabular context, which is presented as an image,
semi-structured text, and a structured table. There are two types of questions:
free-text and multi-choice, and each problem is annotated with gold solutions
to reveal the multi-step reasoning process. We evaluate different pre-trained
models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier
studies suggest, since few-shot GPT-3 relies on the selection of in-context
examples, its performance is unstable and can degrade to near chance. The
unstable issue is more severe when handling complex problems like TabMWP. To
mitigate this, we further propose a novel approach, PromptPG, which utilizes
policy gradient to learn to select in-context examples from a small amount of
training data and then constructs the corresponding prompt for the test
example. Experimental results show that our method outperforms the best
baseline by 5.31% on the accuracy metric and reduces the prediction variance
significantly compared to random selection, which verifies its effectiveness in
the selection of in-context examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Liang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1&quot;&gt;Tanmay Rajpurohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1&quot;&gt;Ashwin Kalyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00166">
<title>Automated segmentation of microvessels in intravascular OCT images using deep learning. (arXiv:2210.00166v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00166</link>
<description rdf:parseType="Literal">&lt;p&gt;To analyze this characteristic of vulnerability, we developed an automated
deep learning method for detecting microvessels in intravascular optical
coherence tomography (IVOCT) images. A total of 8,403 IVOCT image frames from
85 lesions and 37 normal segments were analyzed. Manual annotation was done
using a dedicated software (OCTOPUS) previously developed by our group. Data
augmentation in the polar (r,{\theta}) domain was applied to raw IVOCT images
to ensure that microvessels appear at all possible angles. Pre-processing
methods included guidewire/shadow detection, lumen segmentation, pixel
shifting, and noise reduction. DeepLab v3+ was used to segment microvessel
candidates. A bounding box on each candidate was classified as either
microvessel or non-microvessel using a shallow convolutional neural network.
For better classification, we used data augmentation (i.e., angle rotation) on
bounding boxes with a microvessel during network training. Data augmentation
and pre-processing steps improved microvessel segmentation performance
significantly, yielding a method with Dice of 0.71+/-0.10 and pixel-wise
sensitivity/specificity of 87.7+/-6.6%/99.8+/-0.1%. The network for classifying
microvessels from candidates performed exceptionally well, with sensitivity of
99.5+/-0.3%, specificity of 98.8+/-1.0%, and accuracy of 99.1+/-0.5%. The
classification step eliminated the majority of residual false positives, and
the Dice coefficient increased from 0.71 to 0.73. In addition, our method
produced 698 image frames with microvessels present, compared to 730 from
manual analysis, representing a 4.4% difference. When compared to the manual
method, the automated method improved microvessel continuity, implying improved
segmentation performance. The method will be useful for research purposes as
well as potential future treatment planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juhwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Justin N. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gomez_Perez_L/0/1/0/all/0/1&quot;&gt;Lia Gomez-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gharaibeh_Y/0/1/0/all/0/1&quot;&gt;Yazan Gharaibeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Motairek_I/0/1/0/all/0/1&quot;&gt;Issam Motairek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pereira_G/0/1/0/all/0/1&quot;&gt;Ga-briel T. R. Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimin_V/0/1/0/all/0/1&quot;&gt;Vladislav N. Zimin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dallan_L/0/1/0/all/0/1&quot;&gt;Luis A. P. Dallan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoori_A/0/1/0/all/0/1&quot;&gt;Ammar Hoori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Kindi_S/0/1/0/all/0/1&quot;&gt;Sadeer Al-Kindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guagliumi_G/0/1/0/all/0/1&quot;&gt;Giulio Guagliumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bezerra_H/0/1/0/all/0/1&quot;&gt;Hiram G. Bezerra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;David L. Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01123">
<title>Towards Learned Simulators for Cell Migration. (arXiv:2210.01123v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01123</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulators driven by deep learning are gaining popularity as a tool for
efficiently emulating accurate but expensive numerical simulators. Successful
applications of such neural simulators can be found in the domains of physics,
chemistry, and structural biology, amongst others. Likewise, a neural simulator
for cellular dynamics can augment lab experiments and traditional computational
methods to enhance our understanding of a cell&apos;s interaction with its physical
environment. In this work, we propose an autoregressive probabilistic model
that can reproduce spatiotemporal dynamics of single cell migration,
traditionally simulated with the Cellular Potts model. We observe that standard
single-step training methods do not only lead to inconsistent rollout
stability, but also fail to accurately capture the stochastic aspects of the
dynamics, and we propose training strategies to mitigate these issues. Our
evaluation on two proof-of-concept experimental scenarios shows that neural
methods have the potential to faithfully simulate stochastic cellular dynamics
at least an order of magnitude faster than a state-of-the-art implementation of
the Cellular Potts model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Minartz_K/0/1/0/all/0/1&quot;&gt;Koen Minartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Poels_Y/0/1/0/all/0/1&quot;&gt;Yoeri Poels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Menkovski_V/0/1/0/all/0/1&quot;&gt;Vlado Menkovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05361">
<title>Uncertainty-Aware Unsupervised Image Deblurring with Deep Residual Prior. (arXiv:2210.05361v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05361</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-blind deblurring methods achieve decent performance under the accurate
blur kernel assumption. Since the kernel uncertainty (i.e. kernel error) is
inevitable in practice, semi-blind deblurring is suggested to handle it by
introducing the prior of the kernel (or induced) error. However, how to design
a suitable prior for the kernel (or induced) error remains challenging.
Hand-crafted prior, incorporating domain knowledge, generally performs well but
may lead to poor performance when kernel (or induced) error is complex.
Data-driven prior, which excessively depends on the diversity and abundance of
training data, is vulnerable to out-of-distribution blurs and images. To
address this challenge, we suggest a dataset-free deep residual prior for the
kernel induced error (termed as residual) expressed by a customized untrained
deep neural network, which allows us to flexibly adapt to different blurs and
images in real scenarios. By organically integrating the respective strengths
of deep priors and hand-crafted priors, we propose an unsupervised semi-blind
deblurring model which recovers the latent image from the blurry image and
inaccurate blur kernel. To tackle the formulated model, an efficient
alternating minimization algorithm is developed. Extensive experiments
demonstrate the favorable performance of the proposed method as compared to
data-driven and model-driven methods in terms of image quality and the
robustness to the kernel error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaole Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xile Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yuchun Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07729">
<title>Model-Based Imitation Learning for Urban Driving. (arXiv:2210.07729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;An accurate model of the environment and the dynamic agents acting in it
offers great potential for improving motion planning. We present MILE: a
Model-based Imitation LEarning approach to jointly learn a model of the world
and a policy for autonomous driving. Our method leverages 3D geometry as an
inductive bias and learns a highly compact latent space directly from
high-resolution videos of expert demonstrations. Our model is trained on an
offline corpus of urban driving data, without any online interaction with the
environment. MILE improves upon prior state-of-the-art by 31% in driving score
on the CARLA simulator when deployed in a completely new town and new weather
conditions. Our model can predict diverse and plausible states and actions,
that can be interpretably decoded to bird&apos;s-eye view semantic segmentation.
Further, we demonstrate that it can execute complex driving manoeuvres from
plans entirely predicted in imagination. Our approach is the first camera-only
method that models static scene, dynamic scene, and ego-behaviour in an urban
driving environment. The code and model weights are available at
https://github.com/wayveai/mile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Anthony Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Gianluca Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_N/0/1/0/all/0/1&quot;&gt;Nicolas Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1&quot;&gt;Zak Murez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurau_C/0/1/0/all/0/1&quot;&gt;Corina Gurau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1&quot;&gt;Hudson Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1&quot;&gt;Alex Kendall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1&quot;&gt;Jamie Shotton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09887">
<title>MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos. (arXiv:2210.09887v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09887</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural network inference on video input is computationally
expensive and has high memory bandwidth requirements. Recently, researchers
managed to reduce the cost of processing upcoming frames by only processing
pixels that changed significantly. Using sparse convolutions, the sparsity of
frame differences can be translated to speedups on current inference devices.
However, previous work was relying on static cameras. Moving cameras add new
challenges in how to fuse newly unveiled image regions with already processed
regions efficiently to minimize the update rate - without increasing memory
overhead and without knowing the camera extrinsics of future frames. In this
work, we propose MotionDeltaCNN, a CNN framework that supports moving cameras
and variable resolution input. We propose a spherical buffer which enables
seamless fusion of newly unveiled regions and previously processed regions -
without increasing the memory footprint. Our evaluations show that we
outperform previous work by up to 90% by explicitly adding support for moving
camera input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1&quot;&gt;Mathias Parger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chengcheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neff_T/0/1/0/all/0/1&quot;&gt;Thomas Neff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1&quot;&gt;Christopher D. Twigg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1&quot;&gt;Cem Keskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Robert Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1&quot;&gt;Markus Steinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11277">
<title>TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition. (arXiv:2210.11277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11277</link>
<description rdf:parseType="Literal">&lt;p&gt;Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiabao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yabin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kui Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13768">
<title>GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks. (arXiv:2210.13768v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13768</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) have been studied over decades to incorporate
their biological plausibility and leverage their promising energy efficiency.
Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly
adopted to formulate the spiking neuron and evolves into numerous variants with
different biological features. However, most LIF-based neurons support only
single biological feature in different neuronal behaviors, limiting their
expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF,
a unified spiking neuron, to fuse different bio-features in different neuronal
behaviors, enlarging the representation space of spiking neurons. In GLIF,
gating factors, which are exploited to determine the proportion of the fused
bio-features, are learnable during training. Combining all learnable
membrane-related parameters, our method can make spiking neurons different and
constantly changing, thus increasing the heterogeneity and adaptivity of
spiking neurons. Extensive experiments on a variety of datasets demonstrate
that our method obtains superior performance compared with other SNNs by simply
changing their neuronal formulations to GLIF. In particular, we train a spiking
ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on
CIFAR-100, which has advanced the state-of-the-art. Codes are available at
\url{https://github.com/Ikarosy/Gated-LIF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xingting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fanrong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1&quot;&gt;Zitao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15137">
<title>ScoreMix: A Scalable Augmentation Strategy for Training GANs with Limited Data. (arXiv:2210.15137v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15137</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) typically suffer from overfitting when
limited training data is available. To facilitate GAN training, current methods
propose to use data-specific augmentation techniques. Despite the
effectiveness, it is difficult for these methods to scale to practical
applications. In this work, we present ScoreMix, a novel and scalable data
augmentation approach for various image synthesis tasks. We first produce
augmented samples using the convex combinations of the real samples. Then, we
optimize the augmented samples by minimizing the norms of the data scores,
i.e., the gradients of the log-density functions. This procedure enforces the
augmented samples close to the data manifold. To estimate the scores, we train
a deep estimation network with multi-scale score matching. For different image
synthesis tasks, we train the score estimation network using different data. We
do not require the tuning of the hyperparameters or modifications to the
network architecture. The ScoreMix method effectively increases the diversity
of data and reduces the overfitting problem. Moreover, it can be easily
incorporated into existing GAN models with minor modifications. Experimental
results on numerous tasks demonstrate that GAN models equipped with the
ScoreMix method achieve significant improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Mandi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junchi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16031">
<title>UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance. (arXiv:2210.16031v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16031</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative models have recently greatly improved the power of
text-conditioned image generation. Existing image generation models mainly
include text conditional diffusion model and cross-modal guided diffusion
model, which are good at small scene image generation and complex scene image
generation respectively. In this work, we propose a simple yet effective
approach, namely UPainting, to unify simple and complex scene image generation,
as shown in Figure 1. Based on architecture improvements and diverse guidance
schedules, UPainting effectively integrates cross-modal guidance from a
pretrained image-text matching model into a text conditional diffusion model
that utilizes a pretrained Transformer language model as the text encoder. Our
key findings is that combining the power of large-scale Transformer language
model in understanding language and image-text matching model in capturing
cross-modal semantics and style, is effective to improve sample fidelity and
image-text alignment of image generation. In this way, UPainting has a more
general image generation capability, which can generate images of both simple
and complex scenes more effectively. To comprehensively compare text-to-image
models, we further create a more general benchmark, UniBench, with well-written
Chinese and English prompts in both simple and complex scenes. We compare
UPainting with recent models and find that UPainting greatly outperforms other
models in terms of caption similarity and image fidelity in both simple and
complex scenes. UPainting project page \url{https://upainting.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xinyan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiachen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guohao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhifan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1&quot;&gt;Qiaoqiao She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yajuan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hua Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16810">
<title>SL3D: Self-supervised-Self-labeled 3D Recognition. (arXiv:2210.16810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16810</link>
<description rdf:parseType="Literal">&lt;p&gt;There are a lot of promising results in 3D recognition, including
classification, object detection, and semantic segmentation. However, many of
these results rely on manually collecting densely annotated real-world 3D data,
which is highly time-consuming and expensive to obtain, limiting the
scalability of 3D recognition tasks. Thus in this paper, we study unsupervised
3D recognition and propose a Self-supervised-Self-Labeled 3D Recognition (SL3D)
framework. SL3D simultaneously solves two coupled objectives, i.e., clustering
and learning feature representation to generate pseudo labeled data for
unsupervised 3D recognition. SL3D is a generic framework and can be applied to
solve different 3D recognition tasks, including classification, object
detection, and semantic segmentation. Extensive experiments demonstrate its
effectiveness. Code is available at https://github.com/fcendra/sl3d.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cendra_F/0/1/0/all/0/1&quot;&gt;Fernando Julio Cendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiajun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16993">
<title>STN: a new tensor network method to identify stimulus category from brain activity pattern. (arXiv:2210.16993v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16993</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural decoding is still a challenge and hot topic in neurocomputing science.
Recently, many studies have shown that brain network pattern containing rich
spatial and temporal structure information, which represented the activation
information of brain under external stimuli. The traditional method is to
extract brain network features directly from the common machine learning
method, then put these features into the classifier, and realize to decode
external stimuli. However, this method cannot effectively extract the
multi-dimensional structural information, which is hidden in the brain network.
The tensor researchers show that the tensor decomposition model can fully mine
unique spatio-temporal structure characteristics in multi-dimensional structure
data. This research proposed a stimulus constrain tensor brain model(STN),
which involved the tensor decomposition idea and stimulus category constraint
information. The model was verified on the real neuroimaging data sets (MEG and
fMRI). The experimental results show that the STN model achieved more 11.06%
and 18.46% compared with others methods on two modal data sets. These results
imply the superiority of extracting discriminative characteristics about STN
model, especially for decoding object stimuli with semantic information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chunyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiacai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00577">
<title>Fine-tuned Generative Adversarial Network-based Model for Medical Images Super-Resolution. (arXiv:2211.00577v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00577</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical image analysis, low-resolution images negatively affect the
performance of medical image interpretation and may cause misdiagnosis. Single
image super-resolution (SISR) methods can improve the resolution and quality of
medical images. Currently, Generative Adversarial Networks (GAN) based
super-resolution models are widely used and have shown very good performance.
Real-Enhanced Super-Resolution Generative Adversarial Network (Real-ESRGAN) is
one of the recent practical GAN-based models which is widely used in the field
of general image super-resolution. Unlike natural datasets, medical datasets do
not have very high spatial resolution. Transfer learning is one of the
effective methods which uses models trained with external datasets (often
natural datasets), and fine-tunes them to enhance the resolution of medical
images. In our proposed approach, the pre-trained generator and discriminator
networks of the Real-ESRGAN model are fine-tuned using medical image datasets.
In this paper, we worked on retinal images and chest X-ray images. We used the
STARE dataset of retinal images and Tuberculosis Chest X-rays (Shenzhen)
dataset. The proposed model produces more accurate and natural textures, and
the output images have better detail and resolution compared to the original
Real-ESRGAN model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aghelan_A/0/1/0/all/0/1&quot;&gt;Alireza Aghelan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rouhani_M/0/1/0/all/0/1&quot;&gt;Modjtaba Rouhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00924">
<title>SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory. (arXiv:2211.00924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00924</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of talking face generation from speech lies in aligning two
different modal information, audio and video, such that the mouth region
corresponds to input audio. Previous methods either exploit audio-visual
representation learning or leverage intermediate structural information such as
landmarks and 3D models. However, they struggle to synthesize fine details of
the lips varying at the phoneme level as they do not sufficiently provide
visual information of the lips at the video synthesis step. To overcome this
limitation, our work proposes Audio-Lip Memory that brings in visual
information of the mouth region corresponding to input audio and enforces
fine-grained audio-visual coherence. It stores lip motion features from
sequential ground truth images in the value memory and aligns them with
corresponding audio features so that they can be retrieved using audio input at
inference time. Therefore, using the retrieved lip motion features as visual
hints, it can easily correlate audio with visual dynamics in the synthesis
step. By analyzing the memory, we demonstrate that unique lip features are
stored in each memory slot at the phoneme level, capturing subtle lip motion
based on memory addressing. In addition, we introduce visual-visual
synchronization loss which can enhance lip-syncing performance when used along
with audio-visual synchronization loss in our model. Extensive experiments are
performed to verify that our method generates high-quality video with mouth
shapes that best align with the input audio, outperforming previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Joanna Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01111">
<title>On the Benefit of Dual-domain Denoising in a Self-supervised Low-dose CT Setting. (arXiv:2211.01111v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01111</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is routinely used for three-dimensional non-invasive
imaging. Numerous data-driven image denoising algorithms were proposed to
restore image quality in low-dose acquisitions. However, considerably less
research investigates methods already intervening in the raw detector data due
to limited access to suitable projection data or correct reconstruction
algorithms. In this work, we present an end-to-end trainable CT reconstruction
pipeline that contains denoising operators in both the projection and the image
domain and that are optimized simultaneously without requiring ground-truth
high-dose CT data. Our experiments demonstrate that including an additional
projection denoising operator improved the overall denoising performance by
82.4-94.1%/12.5-41.7% (PSNR/SSIM) on abdomen CT and 1.5-2.9%/0.4-0.5%
(PSNR/SSIM) on XRM data relative to the low-dose baseline. We make our entire
helical CT reconstruction framework publicly available that contains a raw
projection rebinning step to render helical projection data suitable for
differentiable fan-beam reconstruction operators and end-to-end learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Fabian Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1&quot;&gt;Mareike Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfaff_L/0/1/0/all/0/1&quot;&gt;Laura Pfaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1&quot;&gt;Oliver Aust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pechmann_S/0/1/0/all/0/1&quot;&gt;Sabrina Pechmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weidner_D/0/1/0/all/0/1&quot;&gt;Daniela Weidner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maul_N/0/1/0/all/0/1&quot;&gt;Noah Maul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rohleder_M/0/1/0/all/0/1&quot;&gt;Maximilian Rohleder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utz_J/0/1/0/all/0/1&quot;&gt;Jonas Utz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1&quot;&gt;Felix Denzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01226">
<title>DEArt: Dataset of European Art. (arXiv:2211.01226v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;Large datasets that were made publicly available to the research community
over the last 20 years have been a key enabling factor for the advances in deep
learning algorithms for NLP or computer vision. These datasets are generally
pairs of aligned image / manually annotated metadata, where images are
photographs of everyday life. Scholarly and historical content, on the other
hand, treat subjects that are not necessarily popular to a general audience,
they may not always contain a large number of data points, and new data may be
difficult or impossible to collect. Some exceptions do exist, for instance,
scientific or health data, but this is not the case for cultural heritage (CH).
The poor performance of the best models in computer vision - when tested over
artworks - coupled with the lack of extensively annotated datasets for CH, and
the fact that artwork images depict objects and actions not captured by
photographs, indicate that a CH-specific dataset would be highly valuable for
this community. We propose DEArt, at this point primarily an object detection
and pose classification dataset meant to be a reference for paintings between
the XIIth and the XVIIIth centuries. It contains more than 15000 images, about
80% non-iconic, aligned with manual annotations for the bounding boxes
identifying all instances of 69 classes as well as 12 possible poses for boxes
identifying human-like objects. Of these, more than 50 classes are CH-specific
and thus do not appear in other datasets; these reflect imaginary beings,
symbolic entities and other categories related to art. Additionally, existing
datasets do not include pose annotations. Our results show that object
detectors for the cultural heritage domain can achieve a level of precision
comparable to state-of-art models for generic images via transfer learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reshetnikov_A/0/1/0/all/0/1&quot;&gt;Artem Reshetnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinescu_M/0/1/0/all/0/1&quot;&gt;Maria-Cristina Marinescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1&quot;&gt;Joaquim More Lopez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01311">
<title>Distill and Collect for Semi-Supervised Temporal Action Segmentation. (arXiv:2211.01311v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01311</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent temporal action segmentation approaches need frame annotations during
training to be effective. These annotations are very expensive and
time-consuming to obtain. This limits their performances when only limited
annotated data is available. In contrast, we can easily collect a large corpus
of in-domain unannotated videos by scavenging through the internet. Thus, this
paper proposes an approach for the temporal action segmentation task that can
simultaneously leverage knowledge from annotated and unannotated video
sequences. Our approach uses multi-stream distillation that repeatedly refines
and finally combines their frame predictions. Our model also predicts the
action order, which is later used as a temporal constraint while estimating
frames labels to counter the lack of supervision for unannotated videos. In the
end, our evaluation of the proposed approach on two different datasets
demonstrates its capability to achieve comparable performance to the full
supervision despite limited annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sovan Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhodes_A/0/1/0/all/0/1&quot;&gt;Anthony Rhodes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manuvinakurike_R/0/1/0/all/0/1&quot;&gt;Ramesh Manuvinakurike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffa_G/0/1/0/all/0/1&quot;&gt;Giuseppe Raffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckwith_R/0/1/0/all/0/1&quot;&gt;Richard Beckwith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01335">
<title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01335</link>
<description rdf:parseType="Literal">&lt;p&gt;The tremendous success of CLIP (Radford et al., 2021) has promoted the
research and application of contrastive learning for vision-language
pretraining. In this work, we construct a large-scale dataset of image-text
pairs in Chinese, where most data are retrieved from publicly available
datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5
Chinese CLIP models of multiple sizes, spanning from 77 to 958 million
parameters. Furthermore, we propose a two-stage pretraining method, where the
model is first trained with the image encoder frozen and then trained with all
parameters being optimized, to achieve enhanced model performance. Our
comprehensive experiments demonstrate that Chinese CLIP can achieve the
state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups
of zero-shot learning and finetuning, and it is able to achieve competitive
performance in zero-shot image classification based on the evaluation on the
ELEVATER benchmark (Li et al., 2022). We have released our codes, models, and
demos in https://github.com/OFA-Sys/Chinese-CLIP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;An Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Junshu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1&quot;&gt;Rui Men&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1906.11898">
<title>InsectUp: Crowdsourcing Insect Observations to Assess Demographic Shifts and Improve Classification. (arXiv:1906.11898v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1906.11898</link>
<description rdf:parseType="Literal">&lt;p&gt;Insects play such a crucial role in ecosystems that a shift in demography of
just a few species can have devastating consequences at environmental, social
and economic levels. Despite this, evaluation of insect demography is strongly
limited by the difficulty of collecting census data at sufficient scale. We
propose a method to gather and leverage observations from bystanders, hikers,
and entomology enthusiasts in order to provide researchers with data that could
significantly help anticipate and identify environmental threats. Finally, we
show that there is indeed interest on both sides for such collaboration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giro_Larraz_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Giro-Larraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guille_Escuret_C/0/1/0/all/0/1&quot;&gt;Charles Guille-Escuret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1&quot;&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kegl_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs K&amp;#xe9;gl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.02849">
<title>Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition. (arXiv:2201.02849v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2201.02849</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing the dependencies between joints is critical in skeleton-based
action recognition task. Transformer shows great potential to model the
correlation of important joints. However, the existing Transformer-based
methods cannot capture the correlation of different joints between frames,
which the correlation is very useful since different body parts (such as the
arms and legs in &quot;long jump&quot;) between adjacent frames move together. Focus on
this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is
proposed. The skeleton sequence is divided into several parts, and several
consecutive frames contained in each part are encoded. And then a
spatio-temporal tuples self-attention module is proposed to capture the
relationship of different joints in consecutive frames. In addition, a feature
aggregation module is introduced between non-adjacent frames to enhance the
ability to distinguish similar actions. Compared with the state-of-the-art
methods, our method achieves better performance on two large-scale datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Helei Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Biao Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1&quot;&gt;Bo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>