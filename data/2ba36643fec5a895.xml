<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Arthur Juliani on Medium]]></title>
        <description><![CDATA[Stories by Arthur Juliani on Medium]]></description>
        <link>https://medium.com/@awjuliani?source=rss-18dfe63fa7f0------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*kLlWemBAJPjdMTEh5hDtUg.jpeg</url>
            <title>Stories by Arthur Juliani on Medium</title>
            <link>https://medium.com/@awjuliani?source=rss-18dfe63fa7f0------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 05 Nov 2022 16:42:15 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@awjuliani/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Interaction-Grounded Learning: Learning from feedback, not rewards]]></title>
            <link>https://awjuliani.medium.com/interaction-grounded-learning-learning-from-feedback-not-rewards-934a0035cc56?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/934a0035cc56</guid>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Wed, 26 Jan 2022 18:34:59 GMT</pubDate>
            <atom:updated>2022-01-26T21:42:08.616Z</atom:updated>
            <content:encoded><![CDATA[<p>In a typical reinforcement learning problem, an agent situated in an environment makes observations, takes actions, and receives rewards. The goal of the agent is to learn to receive the largest sum of (sometimes discounted) rewards possible. To do so, the reward at each time-step is used to adjust the likelihood of the actions the agent takes in a given state, such that in the future the agent will receive more reward on average than it did in the past. This setting has been extensively studied, and very efficient algorithms exist in both the tabular and deep learning settings to solve it.</p><p>What if there were no rewards available, and instead the agent simply receives a feedback signal from the environment? In that case, most traditional RL approaches no longer apply. It is this problem setting which was recently described as “Interaction-Grounded Learning” (IGL) by Tengyang Xie and his collaborators at Microsoft Research in their <a href="https://arxiv.org/abs/2106.04887">ICLR 2021 paper of the same name</a>. In that work, they not only laid out the IGL setting, but also proposed a couple of preliminary algorithms which can solve IGL problems. In this post, I will walk through IGL in a little more depth, and provide code for solving a simple digit identification problem using feedback instead of rewards. I open-sourced my PyTorch code, which can be found at this link:</p><p><a href="https://github.com/awjuliani/interaction-grounded-learning">https://github.com/awjuliani/interaction-grounded-learning</a></p><h4>The IGL Setting</h4><p>In the paper, the authors motivate IGL with examples from human-computer interface research. If we want machines which can interact with humans in a natural way, we need them to be able to learn from human feedback in a natural way as well. Asking the human to provide a discrete reward signal to train the agent after every action it takes is an unreasonably cumbersome burden. It is also the case that demonstration data may not be available, or may not make sense in a number of contexts. Instead, if the computer could learn to interpret the humans hand gestures, facial features, or even brain signal to infer the latent reward signal, learning could happen in a much smoother way.</p><p>Making things more concrete, the authors propose a much simpler toy problem to validate their early approach. This problem is a simple MNIST digit identification task. At each trial, the agent is shown an image of an MNIST digit, and must guess the identity of the digit (between 0 and 9). If the agent guesses correctly, it is provided with a feedback signal corresponding to an image of the digit one. If it guesses incorrectly, it is provided an image of a zero digit. The problem is to learn to infer the meaning of this feedback, and to use it to improve the performance of the agent.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BHMo9CrBmwQKZuVIP1JnPg.png" /><figcaption>Taken from <a href="https://arxiv.org/abs/2106.04887">Xie et al., 2021</a></figcaption></figure><p>Such a task is indeed solvable, provided that you make some simplifying assumptions. The key assumption made in the IGL paper is that the desired policy is significantly different from a random policy. We can see this in the case of the MNIST task, where a random policy will provide the agent with a feedback signal consisting of many more images of zeros than images of ones. In contrast, the optimal policy will result in feedback consisting only of images of ones.</p><p><em>The learning problem is then to jointly learn a policy and a reward decoder for which the expected value of the learned policy with respect to the decoded rewards is greater than a random policy with respect to the decoded rewards.</em></p><p>In the paper, the authors provide both an offline and online algorithm for solving this problem, and provide a set of theoretical analyses regarding there solution. I highly recommend taking a look at their paper for all the details.</p><p>My own implementation of an IGL agent differs slightly from theirs, but solves the problem with comparable efficiency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vvEWuylEzcQ80IKIYYsRbw.png" /><figcaption>Code available <a href="https://github.com/awjuliani/interaction-grounded-learning/blob/main/learn.ipynb">here</a>.</figcaption></figure><p>The main idea is to collect mini-batches of trials from both the policy we are training (exploit) and the random policy (explore). The exploit policy is then updated with its decoded rewards using policy-gradient to improve the likelihood of taking rewarding actions. The decoded rewards of the random policy are also minimized using gradient descent to decrease the average rewards received by the random policy. The process is repeated until convergence. The result of this procedure is that the learned policy adopts an increasingly different policy than the random policy. Since getting a feedback signal of images of only ones makes the learned policy maximally different than the random policy (with respect to feedback), that is what it learns to do. You can see the results of the learning process below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tYewc98VcRgtf8XqRK9e5Q.png" /><figcaption>Learned and true reward achieved over time. X-axis corresponds to epochs. Y-axis corresponds to achieved reward.</figcaption></figure><p>The code <a href="https://github.com/awjuliani/interaction-grounded-learning">available here</a> can be used to reproduce these results in about a minute.</p><h4>Applying IGL to “Real Problems”</h4><p>As I mentioned above, IGL has the potential to be applicable to many real-world domains where a nice reward signal is not available, but a messy feedback signal still might be. It is likely that a number of extensions might be required to the current approach before that becomes feasible though. Indeed, this novel formulation is riple for additional follow-up work.</p><p>In the algorithms described in the paper and here, an assumption is made that the desired policy is significantly different from a random policy. This is not always the case. We can also imagine counter-examples, where the optimal policy is not only similar to the random policy, but where there are much worse policies which are quite different. Imagine a version of the MNIST task where selecting the true digit identity was undesirable (i.e. would traditionally provide a -1 reward). The current formulation also assumes a contextual bandit setting, which despite its broad applicability, may not work for settings which are better suited to an MDP formulation. Feedback from humans is also quite messy, and the difference between “good” and “bad” feedback may be quite noisy, or even change over time.</p><p>Regardless, if we want to be able to arrive at a world where agents and humans interact in more natural and fluent ways, agents which learn from ambiguous feedback is going to be an essential step towards getting there, and IGL provides a useful formalism towards that goal.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=934a0035cc56" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on “The Enigma of Reason”]]></title>
            <link>https://awjuliani.medium.com/thoughts-on-the-enigma-of-reason-11983c90cb75?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/11983c90cb75</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[reason]]></category>
            <category><![CDATA[cognitive-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[psychology]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Tue, 20 Jul 2021 17:39:19 GMT</pubDate>
            <atom:updated>2021-07-20T17:39:19.749Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*erCij98Vwdae_b4g" /><figcaption><em>Cicero denouncing Catiline at a meeting of the Roman Senate. A famous instance of the successful use of reason.</em></figcaption></figure><p>I recently read <a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674237827"><em>The Enigma of Reason</em> by Dan Sperber and Hugo Mercier</a>. As the title suggests, the book attempts to make sense of what seems to be a paradox of human’s ability to reason. The book seeks to explain how a mechanism which has been assumed to have evolved in humans in order to improve their problem solving ability seems to be so poor at that job. While reading the book, it struck me that some of the ideas presented within were relevant for not just cognitive science, but also for the field of artificial intelligence (AI). As such, I decided to share some of my thoughts on the book.</p><p>Sperber and Mercier see reason as an evolved module within the brain which specializes in producing reasons for our behavior. These reasons serve to justify our behavior to ourselves and others. It also enables us to evaluate the reasons proposed by others for their behavior. As such, it serves a primarily social role, one of coordination and cooperation, rather than an intellectual one of abstract problem solving. The authors oppose their theory to the popular <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">two-system hypothesis</a> about decision making, which proposes that there is a fast and biased unconscious inference mechanism paired with a slow, accurate and conscious reasoning mechanism. They instead see reason as just one of many intuitive inferential mechanisms within the brain. These mechanisms may be performing <a href="https://en.wikipedia.org/wiki/Bayesian_inference">bayesian inference</a>, or some other probabilistic learning process, however the specifics are not seen as very important. For Sperber and Mercier, what is important is that there is nothing unique about reason per-se, it simply operates on meta-level representations rather than low level primitives as most other inference mechanisms such as visual perception do.</p><p>The authors propose that reason leads us astray in the pursuit of objective knowledge not because it is flawed, but because that is simply not what it evolved to do in the first place. They demonstrate this by walking the reader through a long series of psychology experiments conducted over the past century which show again and again that human reason is “biased” in all sorts of informative ways. Rather than reflecting some sort of transcendent capacity for pure abstract logic, human reason is often messy, and relies more on context and perceived relevance rather than stone-cold rules of logical deduction. Chief among these is the so-called myside bias (a generalization of <a href="https://en.wikipedia.org/wiki/Confirmation_bias">confirmation bias</a>), where humans produce reasons which reinforce their previously held beliefs. The myside bias ironically often leads people to become more wrong after thinking deeply about a problem than if they had to make a decision on the spur of the moment.</p><p>What does this have to do with AI? For one thing, it provides an additional perspective on how misguided the idea of artificial general intelligence (AGI) is. Proponents of AGI will often make recourse to the supposedly general nature of human reason when discussing its possibility in artificial agents. If the hypothesis put forward by Sperber and Mercier is correct though, human reason isn’t even for problem solving, let alone for some hypothetical context-agnostic problem solving. Rather, it is for justification and explanation of behavior and decisions reached implicitly within an embodied social context. In this view, the so-called flaws in human reasoning which a superhuman intelligence would supposedly correct for are not even flaws, but rather the features of the mechanism working as they evolved to. If we accept this view, then to extend human reason into a general problem-solving system would be like attempting to extend a skyscraper in New York City to be a means of getting to the moon.</p><p>All hope is not lost however, as the authors do discuss the conditions within which reason does in fact aid in the obtaining of objective truth or knowledge, and those are primarily social. For Sperber and Mercier, it is when reason is placed within its proper context, and humans are forced to justify and explain themselves to others, that reason does more than simply serve a social function. This is because critically, humans are better at evaluating reasons than we are at producing them. As a result, the bias towards producing reasons which justify our implicit decisions and the ability of others to find flaws in our reasons end up working together. These two abilities make possible open argumentation and debate to produce reasons which are better tuned to reality, and can thus ultimately drive better problem solving in the future. The authors support this conclusion with a wealth of cognitive psychology research which shows the critical role of argumentation in reaching informed decisions above and beyond what individuals in a group would have arrived at alone.</p><p>Within the context of AI, this perhaps suggests that rather than building a single system which is a “general reasoner,” a better approach to achieving super-human level problem solving would be to develop a kind of society of artificial agents, each with a unique perspective, and each with human-like reasoning abilities. Specifically, each with the propensity to produce many reasons to justify its behavior, and also to accurately discriminate between good and bad reasons in other agents. These agents could then argue and debate their ideas to arrive at better informed ones. We already see trends toward this in the current literature on <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble methods</a>, where multiple agents each come to a different decision, and some higher-level process takes all of those decisions into account when producing a final output. A further step however is to allow each agent to exchange information with all others through the bottleneck of reasons, which must be justified and explained. Coordination through learned multi-agent communication is potentially another promising avenue toward this kind of system.</p><p>Reading<em> The Enigma of Reason</em> also encouraged me to think deeper about what reasons really are, and how they might be instantiated within artificial intelligence systems today. One approach to operationalizing reason is to treat it as a kind of causal model. “The ground is wet because it rained last night” is a reason, but it is also a description of a causal link between rain and a wet ground. Importantly these kinds of relationships can be learned from experience. “I brought an umbrella because it might rain. It might rain because there are dark clouds in the sky” is a chain of reasons which ultimately exist to justify one’s behavior. There is currently exciting work being done in the field of causal discovery in deep learning models, and I believe that this work can help make possible agents which can produce coherent reasons. This work is still in the early stages, but it is exciting to think about the implications. It likely will not lead to some general problem solver, but it will potentially lead to agents which can better understand their worlds, and more importantly, explain themselves to us and other agents with which they interact. A not only more achievable goal, but one which is necessary for such systems to truly become a beneficial and everyday part of our lives.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=11983c90cb75" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Japanese Role Playing Games as a Meta Reinforcement Learning Benchmark]]></title>
            <link>https://awjuliani.medium.com/japanese-role-playing-games-as-a-meta-reinforcement-learning-benchmark-2907f527f0f3?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/2907f527f0f3</guid>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[games]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Mon, 12 Apr 2021 18:10:05 GMT</pubDate>
            <atom:updated>2021-04-13T16:28:26.880Z</atom:updated>
            <content:encoded><![CDATA[<h4>A Proposal</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JzN9LC5HDppebcHO" /></figure><p>The field of Deep Reinforcement Learning (DeepRL) has made significant gains in the past half-decade. To go from sometimes solving simple Atari games to achieving superhuman performance in complex modern games like Starcraft 2 and DOTA in such a short time is quite an accomplishment. Indeed, given such successes one might think that RL has solved most interesting problems in games. There are however domains where there is still much progress to be made. One such area is in tasks which change over time, and require online adaptation on the part of the agent. The field of Meta-RL has sought to address such environments. In the Meta-RL formulation, an agent learns to solve not just one task, but rather to solve a distribution of tasks. Trained agents are then evaluated based on how well they can perform in unseen tasks drawn from a similar distribution. Often performing well in the never-before-seen tasks involves learning how to properly probe the environment to understand its potentially novel dynamics. This process has various names, such as hypothesis testing, online inference, or active inference, and is an active area of research.</p><p>Recently, a group at DeepMind has proposed a novel 3D environment for evaluating Meta-RL agents called <a href="https://arxiv.org/abs/2102.02926">Alchemy</a>. In this environment, the agent must mix various items together in a pot to create a desired outcome. The nature of the items and the rules governing their interaction changes between episodes, thus requiring meta-learning in the form of online hypothesis testing to solve optimally. A surprising outcome of their work was the discovery that current state-of-the-art DeepRL algorithms fail to perform this task optimally. In doing so, the learned agents display a lack of hypothesis testing ability, preferring to act based on average outcomes over all episodes, rather than paying attention to the unique rules of a given episode. What the work suggested was that although current DeepRL agents were able to learn to solve the control problem without issue, the underlying problem of properly probing each environment to understand how it differs from the others, and exploiting that difference was beyond the capability of the trained agents. This weakness leaves open space for future research in the area.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/906/0*oyNAE2a-ooJe9vfw" /><figcaption>A. A rendering of the 3D environment in Alchemy. B. The hierarchical structure of the meta-learning process in Alchemy.</figcaption></figure><p>While the Alchemy environment is quite extensive and well designed in its approach to testing for these key cognitive abilities, it involves a large amount of low-level motor control on the part of the agent, making it less accessible to researchers with smaller computational budgets (<em>4/13/2021 Update: I have learned there is also a symbolic version which does away with the compute-intensive 3D requirements</em>). Like many purpose-built environments, it also lacks a certain amount of ecological validity, since the rules of the task were developed a priori for DeepRL research. I was recently thinking about these limitations, and it struck me that there exists a ready-made class of environments which display many of the desirable properties for testing Meta-RL agents, and could be simulated at very little computational cost. These are <a href="https://en.wikipedia.org/wiki/History_of_Eastern_role-playing_video_games">Japanese Role Playing Games (JRPGs),</a> a genre of video game which became popular in the mid-90s with long running series such as Final Fantasy, Dragon Quest, Persona, and Pokemon.</p><p>JRPGs typically involve long complicated fantasy stories, where a group of individuals come together to save the world by battling a series of increasingly challenging enemies over the course of dozens of hours of gameplay. The main form of challenge in these games lies in their battle system, where the player controls one or more protagonists and must battle one or more enemies at a time. These battles traditionally take the form of so-called turn-based encounters, where the protagonists and enemies take turns taking actions. I believe it is these turn-based battles which are particularly ripe for benchmarking Meta-RL agents. Before I get into that though, a few words on what makes a useful Meta-RL benchmark to begin with.</p><p><em>(Note: While this article focuses on examples from Japanese turn-based RPGs, many Western turn-based RPGs share the same relevant mechanics, and you can feel free to swap in your preferred game into the examples discussed below. I personally grew up playing and enjoying many JRPGs, so those are the examples presented here.)</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WdO1UYpYzRCVhVsH" /><figcaption>The multi-armed bandit meta-learning benchmark. An agent is presented with a series of bandits, each with a different set of returns provided by each arm.</figcaption></figure><p>In order to make a worthwhile Meta-RL benchmark, there must be a distribution of similar environments, each with different underlying dynamics which require different policies to solve. Sometimes these differences are quite small, but other times they can result in significantly different strategies on the part of the agent. The simplest version of this setup used in research is often a distribution of multi-armed bandits, where each bandit has a different probability distribution over outcomes for each of its arms. In this case the agent must experiment in each environment by determining which arm of the bandit provides the best return on average, and then exploiting that knowledge. To do so, the agent must learn a policy at two levels of abstraction. At the highest level it must learn a policy for performing the experiment in each sampled environment, and then at the lower level it must learn a policy for exploiting that learned knowledge. It is this multi-level learning process which earns Meta-RL the “Meta” name.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l-Fm3lTIIgD-ZMij" /><figcaption>Two examples of battles in JRPGs. Left: Dragon Quest 8. Right: Final Fantasy VI.</figcaption></figure><p>In a typical JRPG, the criteria described above is readily met by there being a variety of different possible enemies for the player to encounter in a given game. In fact, many JRPGs can have up to hundreds of unique enemies, many of which are only encountered once. When encountering a never-before-seen enemy, the player must perform hypothesis testing to determine the optimal strategy for defeating it. In Meta-RL context, this involves probing the environment to discover the hidden dynamics. This can take the form of learning about various possible elemental weaknesses and strengths, determining whether the enemy has a high defense, or other contextual vulnerabilities. The player does this by acting within the environment. Physically attacking an enemy provides information about whether it has a high or low physical defense. Casting an ice spell on the enemy will allow the player to determine whether the enemy has a weakness or resistance to ice magic. Once such a weakness has been discovered, the optimal strategy can then be deployed to defeat the enemy.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FTl05JRiAq9Q&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DTl05JRiAq9Q&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FTl05JRiAq9Q%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/6e4a5b886864ea3033c25fb1659c67cb/href">https://medium.com/media/6e4a5b886864ea3033c25fb1659c67cb/href</a></iframe><p>Of course, the enemy does not sit still, while the player attempts to learn and exploit its weaknesses, the enemy is attempting to defeat the player by taking actions of its own. The adversarial nature of these encounters induces a natural pressure for the player to learn and exploit the optimal strategy as quickly and efficiently as possible. Either poor hypothesis testing or sub-optimal strategy exploitation can lead to the player being defeated by the enemy. Once an optimal strategy is quickly discovered though, and the enemy is defeated, the process repeats with a potentially novel enemy and a new set of hypotheses to test regarding its latent properties.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hqthItkq9xR1Wcfc" /><figcaption>The hierarchical structure of a meta-learning benchmark based on JRPGs.</figcaption></figure><p>The above description maps quite nicely onto the bandit problem, where there are two hierarchical levels of policies to be learned. What sets JRPGs apart however is that there is a third hierarchical level at which an agent needs to meta-learn. It is not only the case that within a game different enemies will require different policies to defeat, but between games the rules governing those optimal strategies will differ. Take for example two games within the same series: Final Fantasy X and Final Fantasy XIV. In Final Fantasy X, elemental weaknesses work in a paired fashion. Ice magic damages fire-based enemies, and fire magic damages ice-based enemies. In Final Fantasy XIV however, elemental weaknesses work on a rock-paper-scissors basis, where fire beats ice, ice beats wind, and wind beats fire. Other games in the genre have even more varying elemental interactions, with the Pokemon games for example containing <a href="https://www.polygon.com/pokemon-sword-shield-guide/2019/11/16/20968169/type-strength-weakness-super-effective-weakness-chart">dozens of elemental affinities with complex relationships between them</a>. As humans, we can quickly learn the rules of each game and apply them within that game. In doing so, we deploy a flexible policy at three levels of abstraction: the game, the battle within the game, and the turn within the battle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pOxvLw73OezQVOw7" /><figcaption>Two sets of elemental weakness relationships in different JRPGs within the same game series.</figcaption></figure><p>In addition to online hypothesis testing, performing well in the set of all possible JRPGs involves a powerful and flexible memory system. Whatever an agent might learn about a single enemy might be useful if the agent encounters that enemy again within the game, or within a similar game. Furthermore, the properties of enemies within a game often follow set patterns. Enemies with ice resistance, for example, may often have visual cues in their design which might imply their status, such as a crystal-blue appearance. An ice wolf or an ice bird enemy thus might both share similar appearances. Enemies with high defensive stats might be visually represented as wearing heavy armor. Each game typically deploys an internally consistent visual language to convey this information to players. Importantly though, this visual languages changes between games. As such, the hypothesis space when encountering a new enemy within a game is not uniform over all possible strategies. Learning to remember these regularities over time allows the agent to more quickly arrive at the optimal strategy within any battle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TEVu4OQJz437Fsv6HPWYqg.png" /><figcaption>Examples of opponents from the Pokemon games which are of the fire elemental type. They all bear similar visual properties, which an agent could learn to use to infer the underlying properties of the opponent.</figcaption></figure><p>I hope that all of the properties described above make it clear that JRPGs are a potentially fertile ground for DeepRL research, and Meta-RL research in particular. This is leaving aside the other kinds of interesting learning problems these games often pose, such as puzzle solving, inventory management, equipment upgrading, and multi-agent coordination when the player’s team consists of more than one protagonist. While Starcraft 2 and DOTA are significant challenges, there is much more that games have to offer the field as we strive to develop agents with more and more general intelligence.</p><p>I also recognize that the qualities described here with respect to JRPGs are not totally unique to the genre. Many games, for example, involve hypothesis testing whenever new mechanisms or enemies are introduced. What I believe sets the JRPG genre apart however is the extent to which these aspects are formalized, and largely removed from considerations of visuals, physics, or control. What is complex and cognitively interesting about these games is not that there is necessarily a large state or action space, as many game genres typically considered challenging for RL possesses, but rather that each encounter with a new enemy poses a slightly different kind of challenge from all previous encounters, and thus requires hypothesis testing and the online learning of a new policy. Importantly, the agent must actively experiment with each new enemy or set of enemies to learn the optimal strategy. While I believe that it is this aspect which makes the genre unique, I would be very interested in hearing other opinions, as there are perhaps other genres which are equally ripe for testing Meta-RL agents.</p><p>For those convinced of the value of JRPGs to Meta-RL research, and looking for practical directions with respect to where to go next, I think that there are two possible paths to follow. One would be to assemble a suite of JRPGs and create a benchmark out of them. This poses a number of technical difficulties, and similar attempts to follow this path such as the OpenAI <a href="https://github.com/openai/retro">Gym Retro</a> have failed to attract the attention which the original Atari suite garnered. The other approach is to take the mechanicals principles of JRPGs described here and to develop a novel benchmark suite from scratch.</p><p>Thankfully, what is compelling about these games is not complicated physics, visuals, or controls, but rather the need for performing hypothesis testing, and the learning and general application of rules over multiple scales of abstraction. In fact, the underlying rules governing these games are simple to write, and can be implemented easily in any programming language. Special attention would need to be paid however to ensure that such a benchmark stays true to the complexities of real games within the genre, and does not become overly accommodated to the current capabilities of DeepRL agents. As an additional means of validating such a benchmark, classic JRPGs from the Super Nintendo or other console emulators could be used as a held-out set of test environments for an agent.</p><p>If anyone is interested in such a project and has the time to commit to it, feel free to reach out. I would be happy to collaborate on such an endeavor as an open-source project. <em>Or, if you know of a similar project that already exists, please let me know!</em> For the reasons discussed above, I believe it has the potential to provide a useful addition to the current field of Meta-RL benchmarks by filling a currently under-explore niche. In the meantime, if you are a researcher looking for a challenging benchmark that will push the current state of the art, DeepMind’s <a href="https://github.com/deepmind/dm_alchemy">Alchemy is open-source</a> and poses a number of interesting yet-to-be-solved problems.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2907f527f0f3" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on “Things Hidden Since the Foundation of the World”]]></title>
            <link>https://awjuliani.medium.com/thoughts-on-things-hidden-since-the-foundation-of-the-world-51855f479b5f?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/51855f479b5f</guid>
            <category><![CDATA[anthropology]]></category>
            <category><![CDATA[philosophy]]></category>
            <category><![CDATA[psychology]]></category>
            <category><![CDATA[literature-review]]></category>
            <category><![CDATA[religion]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Tue, 06 Apr 2021 00:12:34 GMT</pubDate>
            <atom:updated>2021-04-12T16:28:17.285Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v9VLGB8X2PKCtoy-OJILIw.jpeg" /></figure><p>When I shared my thoughts on the book <em>Sapiens</em> by Yuval Noah Harari, I expressed the hope that the book would present a sweeping and all-encompassing theory of human culture. Despite being compelling in its own right, it largely failed to live up to such an expectation. On the other hand, <a href="https://en.wikipedia.org/wiki/Things_Hidden_Since_the_Foundation_of_the_World"><em>Things Hidden Since the Foundation of the World</em></a> written in 1978 by Rene Girard (hereafter referred to as <em>Things Hidden</em>), more than delivers on the same promise. In this work, Girard, a philosopher and historian, presents a remarkably simple hypothesis which he claims can account for large swaths of findings in anthropology, theology, psychology, and literature. His theory proposes that imitation (referred to in the book as mimesis, a word with its own long history in philosophy) is the driving force behind the dynamics of all of human culture. The audacity of such a proposal alone had me hooked.</p><p>An expansion on that basic thesis goes something like this. Girard acknowledges that our capacity for imitation is not wholly unique, and rather is part of a continuity of abilities shared with other animals. Nonetheless, he suggests that we humans imitate to a degree at which something special begins to happen. Unlike other animals, our capacity for imitation is strong enough to enable rapid learning of skills and knowledge, which can be handed down across generations, thus making possible the development of lasting cultures. According to Girard, this capacity also leads inevitably to increased conflict and violence, which must be managed collectively. It is the dual implications of imitation through which Girard unlocks the genesis and development of our cultural institutions. For Girard, the first and primary cultural institution is that of religion, which he suggests exists to provide prohibitions on certain behaviors which would aggravate the human tendency toward violence, as well as to provide a mechanism by which the violent escalation caused by mimesis can be negated: the expulsion of a sacrificial victim in the form of the scapegoat.</p><p><em>Things Hidden</em> as a book is the result of a series of conversations that Girard had with two psychoanalysts, Jean-Michel Oughourlian and Guy Lefort, over the course of a number of days. This produced a work which is surprisingly easy to read and even entertaining at points, due to its conversational nature. It also starkly contrasts with the works by many of Girard’s French contemporaries at the time, who can be poetic at best and impenetrable at worst. The two other voices serve as interviewers, acolytes, and occasionally critics, both asking questions and providing relevant commentary throughout the work. At certain points it even struck me that the two interviewers had fit themselves into the classic good-cop/bad-cop dynamic, where one would speak in praising terms of Girard’s theories, and the other would bring up relevant criticism in sequence. The effect produced by this choice of structure allowed me to appreciate many of the nuances of the argument that I would have easily missed if the work was simply presented from a univocal perspective. Girard’s disdain for the obfuscative writing of his contemporaries is also quite apparent, and his general desire for clarity and comprehension comes through both in his discussion of his theories and within the internal logic of the theories themselves.</p><p>As I mentioned above, the core of Girard’s thesis is that a whole host of complex human behavior and cultural institutions can be made sense of simply by thinking through the implications of what he calls the mimetic process. As someone with a background in psychology, what came to my mind at the mention of this mimetic process was the now-famous <a href="https://en.wikipedia.org/wiki/Mirror_neuron">mirror neurons</a>. This class of neurons were first discovered in monkeys, when experiments demonstrated that certain motor neurons in the monkey’s cortex would activate both when the animal performed a task and when the animal observed a human perform the task. These cells were given the name mirror neurons, as they seemed to be mirrorining what they observed. Importantly, it was shown that populations of neurons could activate both when they were directly involved in a task as well as when a task was being imagined or even observed. Since the initial discovery of motor neurons with this mirroring property, many other brain regions have been found which contain similar mirror neurons. One group which has gotten particular attention have been those involved in affective processing, which have been hypothesized to provide the basis for empathy.</p><p>We can interpret these neurons as providing the basis for Girard’s notion of mimetic desire. When we see someone perform an action, regardless of how inherently desirable that action might have been to us beforehand, a part of us now wants to carry out that action ourselves. It is easy to see how this kind of desire can enable the spread of useful cultural skills and knowledge. Violence and conflict first come into the picture when two individual’s desire to carry out the same action becomes incompatible. Imagine a scenario where a person sees his neighbor picking and eating an apple from the tree. The observer of this scenario might now want to pick and eat an apple from that tree themselves. But, let’s imagine that there is now only a single apple left. If two people both want to eat the last apple, a conflict arises. If one of these neighbors commits an act of violence in order to ensure that they have exclusive access to the apple, then a third party might witness the violence, and then seek to avenge the attacked individual. In this way the mimetic conflict becomes a kind of contagion which spreads throughout a community, and the original source of the conflict becomes long since forgotten. Indeed, history is filled with countless examples of such bloody rivalries between groups extending for generations and generations, with the original offense lost to time. In this way, what serves as the mechanism by which learning can take place also serves as the mechanism by which humans enter into such violent struggle. According to Girard these two phenomena are coextensive with one another.</p><p>All of what I have described above is largely described in the first of three sections of the <em>Things Hidden</em>. The second part focuses on a textual analysis of the Christian Bible, while the third presents a discussion of psychoanalytic theory in the context of mimesis. All three are quite striking, even if I found myself somewhat incredulous in the face of the increasingly sweeping theories presented. The central thesis of the second part of the book is that the Christian Gospels present a radically anti-sacrifice and anti-violence message, in more or less contrast to all religious systems beforehand which were explicitly based on the violent sacrifice. It seems that this special place Girard affords Christianity largely ignores the particulars of eastern religions such as Buddhism, which could just as well be presented as embodying a similar radical departure from the primitive role of the religion which Girard describes in the first section of the book. It is perhaps also the case that the specifically Catholic experience of living in France in the 20th century greatly colors his thinking here. I am also sure that contemporary scholarship would likely point out that there were many ancient cultures not based around ritual sacrifice, and thus don’t so easily fit the mould he describes here.</p><p>The third section serves as an extensive critique of Freudian concepts of Narcissism and the Oedipal complex. Here Girard applies his mimetic theory to deconstructing these concepts by showing that they can all be more simply and completely accounted for using his mimetic theory. Narcissism, for example, is presented not as an extreme love of oneself, but rather as a dynamic whereby one plays at loving oneself in order to provide a mimetic model for others to love one in return. He also includes a long discussion on why Marcel Proust has a more acute understanding of human psychology than Freud. My personal bias is to favor Proust as well, though for readers less interested in literature, this section could be a bit of a slog. This is especially true because by this point in the book, Girard’s approach is more than clear, and the application of mimesis to nearly any problem of human behavior begins to look a bit like a hammer in perpetual search for the nail. Still, it is impressive the extent to which Girard is able to fashion so many aspects of human psychology into just such nails.</p><p><em>Things Hidden Since the Foundation of the World</em> was written over forty years ago by a man clearly steeped in philosophical and literary, rather than scientific, traditions. As such, it is difficult to take what is presented in the book as a scientific theory, especially in light of all we now know about human psychology, anthropology, and neuroscience that he didn’t. That being said, I found myself imagining how to bring Girard’s theories to bear on more modern phenomena than anything he could have known in the 1970s: the internet and social media. In his book, Girard speaks a lot about mimetic models, which provide the example by which mimetic desire can become attached. Social media does not simply present a handful of mimetic models to an individual, as one might have found at some earlier point in human culture, but instead provides a countless number of such models, all of whom are also reacting and desiring in ways which are the product of the desires and actions of others.</p><p>Girard’s theory of mimetic desire actually gives us a way to think about this process, not because it is complex, but rather because it is simple. The reason that a theory like that of mimetic desire can ostensibly account for the complex phenomena that it does is because it relies on the emergent properties of dynamical systems. While he does not use these terms, Girard presents a theory which relies on exactly the same kind of mechanism that govern <a href="https://en.wikipedia.org/wiki/Conway&#39;s_Game_of_Life">Conway’s Game of Life</a>, or any emergent system. A few simple rules describing atomic interactions over time produce complex emergent behavior and structure which could not have been predicted from the outset. Mimesis is in effect one such rule: humans will desire to do to do what they see others doing. Social media exists as this process playing itself out on a rapid time scale and with a large and highly connected number of individuals. One could even imagine a researcher running simulations of mimetic desire in such complex networks of individuals, and watching the results play out similarly to how they do in reality.</p><p>Throughout my time reading the book, I also found myself reflecting on the ways in which Girard’s mimetic desire principle operates within my own life and relationship with social media. Like many, I spend a certain amount of time each day on social media, reading about the actions and accomplishments of my peers. In light of Girard’s theory, I have found myself wondering the extent to which my own desires may not so much be the result of inner drives, as much as the result of the mimetic process at work. Indeed, every individual I follow online or post I see serves as a potential mimetic model to follow. Whether or not I do end up following (or desiring to follow) that model is the result of both conscious and unconscious processes and interactions. I found myself contemplating a thought experiment: if I had been exposed to different mimetic models only years earlier, how different might my interests be at this very moment? It is of course an impossible question to answer, but one that feels worthwhile reflecting on. Especially because I do believe that there is such a thing as internal drives which are not simply the result of mimesis, it is all the more important to attempt to sift through which are which.</p><p>As far as I know, the theories which Girard developed and presented in this book have largely fallen out of favor, or else been taken up under very different, more scientific framings by those who followed. In one sense, his theory and work might be seen as a kind of dead-end, or literary curiosity. A work of philosophy which aspires to being a new science of behavior, but doesn’t quite reach that mark. Still, I have found myself coming back to mimetic desire, and thinking about the social world around me in what feels like new and exciting ways the past few weeks. As far as I am concerned, that counts as a success for Rene Girard and his work.</p><p><em>(04/12/2021) Postscript: After publishing this essay, I learned about the role that venture capitalist </em><a href="https://en.wikipedia.org/wiki/Peter_Thiel"><em>Peter Thiel</em></a><em> has played in promoting Girard’s ideas in Silicon Valley and beyond. Thiel was a student of Girard’s at Stanford in the 1980s, and his understanding of Girard’s theory of mimetic desire apparently helped to influence his decision to become an early investor in Facebook. Since then, he has regularly recommended reading Girard’s work, and “Things Hidden” in particular. For another perspective on Girard, which goes deeper into his influence on Thiel, see </em><a href="https://justinehsmith.substack.com/p/who-is-ren-girard"><em>this somewhat more critical article</em></a><em>. I am personally not a fan of Peter Thiel’s philosophy or politics, and am reminded of the ways in which theories meant to be descriptive of social behavior, such as “mimetic desire,” can unfortunately all too often be manipulated to be used as active tools of social engineering and exploitation.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=51855f479b5f" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on “Symbolic Behavior in Artificial Intelligence”]]></title>
            <link>https://awjuliani.medium.com/thoughts-on-symbolic-behavior-in-artificial-intelligence-15321c86e3bb?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/15321c86e3bb</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[philosophy]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[language]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Fri, 12 Feb 2021 18:36:01 GMT</pubDate>
            <atom:updated>2021-02-12T18:36:01.991Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zcs8H1kbdeGsQUTIyQ8tZg.jpeg" /></figure><p>I recently read a new pre-print titled <a href="https://arxiv.org/abs/2102.03406"><em>Symbolic Behaviour in Artificial Intelligence</em></a>, by Adam Santoro, Andrew Lampinen, and collaborators at DeepMind, and was so taken by it that I thought I would write up some brief thoughts. Unlike much of the research published by DeepMind, which is largely empirical in nature, this paper presents a philosophical perspective on the classic symbolic vs connectionist debate. Rather than making the now-typical proposal to combine symbolic reasoning and neural network approaches into a hybrid system where deep learning is used for “low level” processing, and a classical AI system is used at the “high level,” the authors of the paper take a step back and discuss how symbol use emerges in humans in the first place. As they note, it is really only in humans that we have any real example of symbol use to begin with. In doing so, I think that they arrive at a few key points which provide a useful way of thinking about symbols in artificial intelligence and beyond.</p><p>Key to the author’s thinking is the concept of <em>symbolic behavior</em>, which could be seen to stand in contrast to the typical notion used in artificial intelligence of symbolic representation or symbolic reasoning. Putting their argument simply, we humans are led to believe that we possess symbolic representations because we act in the world as if we did. Instead of focusing on the inferred symbolic representations, the authors suggest we focus our attention on understanding the properties of the behavior itself. When thinking about AI systems, what we are likewise interested in as researchers is the development of artificial agents which act like they possess symbolic representations. This can be seen clearly in the classic formulation of the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing test</a>, where it is enough that the agent can behave as if it is intelligent.</p><p>Taking behavior as our starting point, we can then examine just what characterizes our symbolic behavior in humans. The authors suggest that symbolic behavior emerges naturally as the result of developing and maintaining shared conventions which are actively participated in by one or more agents within a world. This sharing-of and participating-in conventions can be between different agents, as is the case in social and cultural interaction, or can take place between a single agent at various points, thanks to one’s ability to store and retrieve memories over time. In this view, any symbolic representations which an agent may possess are emergent properties of the system, rather than necessarily being the result of some hard-coded symbol processing mechanisms. As a result, such hard-coded symbolic reasoning isn’t the only, best, or even necessarily correct way of arriving at agents which display symbolic behavior.</p><p>One might still argue that it seems straightforward that the best way to get symbolic behavior is to develop agents with strong symbolic reasoning priors. The authors make what I believe is a strong refutation of this argument, especially when it calls for a return to the symbolic approaches used in the so-called “Good Old Fashioned AI” (GOFAI) of the late 20th century. They are not alone in this thinking, as there have been great philosophical critiques of the GOFAI approaches, such as those of <a href="https://en.wikipedia.org/wiki/Hubert_Dreyfus%27s_views_on_artificial_intelligence">Hubert Dreyfus</a> in his seminal work <em>What Computers Can’t Do</em>, a classic for anyone interested in understanding why the approaches taken at the time were likely doomed from the start. On the other hand, there has been a wealth of empirical evidence in support of the connectionist (and not the symbolic) approach in the past decade with the great successes of deep learning at everything from image-detection to game-playing to car-driving. Some groups, like OpenAI have taken the approach of continuing down the purely connectionist route, utilizing more and more compute and network capacity. As <a href="https://openai.com/blog/openai-api/">GPT-3 </a>and <a href="https://openai.com/blog/dall-e/">other recent large models</a> testify, it does indeed seem that explicit recourse to symbolic reasoning is not necessary for increasingly human-like abilities.</p><p>Despite this, we still find ourselves as a community at a bit of a crossroads, with many not convinced that the approach of simply scaling connectionist systems will lead to truly human-like intelligence. As such, the spectre of GOFAI continues to loom over the field of AI research, with many <a href="https://www.youtube.com/watch?v=vNOTDn3D_RI">critics</a> in recent years suggesting that what needs to be done is to somehow marry the classical and deep learning approaches. The problem with this line of thinking is that while we humans display symbolic <em>behavior</em> quite naturally, we do not display symbolic reasoning very naturally at all! As the authors point out, human symbolic reasoning displays a graded quality which suggests that we are far from ideal symbol manipulators. For example, it takes many years of mathematics courses before algebraic reasoning can be reliably deployed by a typical human. In contrast, it is in fact computers, just the things which we seek to make more human, which are already essentially perfect at such tasks. For humans, it isn’t the ability to arbitrarily manipulate symbols, but rather the ability to ground them in meaning, and maintain a shared system of conventions over time and space which makes us intelligent symbol users. This approach is motivating <a href="https://arxiv.org/abs/2012.05672">work at DeepMind</a> and elsewhere to focus on agents which must communicate and collaborate in order to solve complex tasks, with symbolic representations being a result, not a starting point.</p><p>Whether the approach explicitly outlined in this paper is the right one, I can’t say. I do believe however that this grounding of empirical research in philosophical discourse can serve only to benefit the field of artificial intelligence. At worst, it can help to prevent some of the mistakes the field has made in the past. At best, it can open new directs for empirical research which prove to be much more promising than the typical performance improvement chasing which so often dominates the field.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=15321c86e3bb" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on Sapiens by Yuval Noah Harari]]></title>
            <link>https://awjuliani.medium.com/thoughts-on-sapiens-by-yuval-noah-harari-4d448b2ffe24?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/4d448b2ffe24</guid>
            <category><![CDATA[philosophy]]></category>
            <category><![CDATA[evolution]]></category>
            <category><![CDATA[psychology]]></category>
            <category><![CDATA[biology]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Sat, 05 Dec 2020 20:36:26 GMT</pubDate>
            <atom:updated>2020-12-05T20:38:40.198Z</atom:updated>
            <content:encoded><![CDATA[<h3>Thoughts on “Sapiens”</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/880/1*nYJbIEVwws5DeINBLcNkQg.jpeg" /></figure><p><em>Sapiens by Yuval Noah Harari was released in English in 2014, but it was not until last month that I finally read it. As such, the thoughts presented here are likely not new. Though, I hope they are at least of interest.</em></p><p>Before reading <em>Sapiens</em>, my only experience with Harari was watching about fifteen minutes of an interview he gave at Google for his more recent book <em>Homo Deus</em>, which was published in English in 2016. I don’t perfectly remember the conversation but it was far enough into gene editing and cognitive enhancement that I felt I had heard enough. It seemed more or less the same set of sci-fi prophetic ideas Ray Kurzweil has been advocating for the past couple decades. Hearing these ideas led to a vague apprehension about Harari as an intellectual, which consequently delayed my picking up <em>Sapiens</em> until recently.</p><p>Still, when I finally felt compelled to read it this year, after the recommendation of a few friends, I did my best to go into the book with an open mind. Ancient history, human psychology, and evolutionary theory are all topics which I have always enjoyed reading and thinking about. I also enjoy reading bold takes on providing big picture theories about human nature. In many cases, the more outlandish or ambitious the theory is, the more fun it is to read. I went into <em>Sapiens</em> expecting just that, a bold new theory about human nature based on a sweeping integration of prehistoric and ancient-historical sources. Unfortunately, I found myself disappointed. Not only are there no major new theories presented by Harari in the book, but the concepts he does present are philosophically problematic upon anything deeper than a surface-level reading.</p><p>True, there is still much to recommend in the book. <em>Sapiens</em> starts strongly enough, as Harari sets the stage with a discussion of primate evolution and the early landscape of the genus Homo as it evolved many millions of years ago. It is here where Harari’s perspective as a historian provides the insight to tell a compelling story about the beginning of our species, Homo Sapiens. I found myself entertained and engaged by his clear and colorful language. I also admired his willingness to cut against certain idealist pictures of ancient peoples. The description of how ancient hunter-gatherers were responsible for various mass extinctions helps to put humans’ current role in ecological destruction into better context. His explanation of the disaster that was the agricultural revolution, and why it was locally optimal for the individuals who slowly instituted it was likewise fascinating. Not much further after that, I started to have my doubts about the story being presented though. This happened as Harari began to speculate about the cognitive experiences of these early humans, and how these may have given them the edge necessary to start down the path toward becoming the dominant species that we are today.</p><p>While it is clear that he is a gifted historian and storyteller, it struck me that Harari is much less of a philosopher. Philosophers deal with, among other things, the nature of knowledge and truth, two concepts central to Harari’s story of humankind. Harari attempts to provide a macro-scale perspective on Homo Sapiens as a species, a perspective which tries to be as objective as possible. As a historian, taking an objective perspective is appealing, since so many previous stories of our species tend to become bogged down in whatever ideology prevails during the time of its composition. Harari himself calls out a number of various ideologies, such as those held by ancient Romans, or those held by early American colonizers. Speaking objectively is a difficult thing though, and Harari falls into the same pitfalls as so many of his predecessors, both ancient and modern. As <em>Sapiens</em> progresses, the particulars of the modern scientific ideology strongly influence the story Harari tells, to the point that I couldn’t help but begin to question the soundness of the story itself.</p><p>Harari proposes that the main development which made Homo Sapiens so successful as a species is the ability to create and share what he refers to as cognitive fictions, or imagined realities. These fictions can be thought of as abstract concepts which a human might be able to mentally represent, but which does not have a real physical correlate in the world. According to Harari, the development of this capacity for such fictions enabled the cultural and technological advancements which led to our species domination of the planet. Harari provides example after example of these fictions, such as human systems of laws, objects of spiritual practice, and monetary systems. Abstracted beyond the individual, these can be thought of as belief systems, which are maintained by a community over time. In explaining these cognitive fictions, Harari seems to paint a picture in black and white though. Either a mental representation corresponds to true reality, or it is a cognitive fiction, there is no discussion of the all important in-between. My personal view is that most mental representations do indeed exist in-between, with each taking some level of truthfulness, or correspondence to a grounding in reality.</p><p>For example, Harari makes the comparison between belief in the legal entity of a company and belief in a particular deity in a religious context. He suggests that both of these are useful cognitive fictions, which enable certain kinds of human organization. In discussing these two together, Harari suggests that both are equally unreal. It struck me that to so casually group these two things together might not be so justified. As I see it, the belief in a corporation can be thought of as a kind of invention, a cultural game which everyone is in on, but everyone knows is a game. In contrast, belief in a deity is a kind of discovery, which some take on face value, and others verify for themselves. While the reality of various spiritual practices is up for ample debate, the phenomenological evidence which a person who undergoes a religious experience receives is not of the same order as that of the belief in a corporation. Yet, Harari treats these as both simply things humans have made up to serve other purposes.</p><p>Later in the text, Harari compares the beliefs the Founding Fathers of the United States espoused in the declaration of independence to those of the Babylonian King Hammurabi. The implication of this comparison is that both sets of beliefs -those of universal human equality, or those of universal human inequality — are cognitive fictions, and are likewise equally unreal. Harari then employs the “objective” findings of biology and evolutionary psychology as the golden measure by which the fictitiousness of both belief systems could be demonstrated. In doing so, Harari ignores the fact that modern biology is based on a system of intersubjective agreement with exactly the same dynamics as the beliefs of the Babylonians or the early colonizers of the United States. By Harari’s own definition, the theory of evolution by natural selection is as much of a cognitive fiction as any other example in his book, let this is nowhere acknowledged by him. The three sets of beliefs are not of a different type, with modern knowledge somehow being categorically true, and the prior belief systems being categorically untrue. Each can be seen as only having a different quantity of truthfulness, with modern biology representing the accumulation of greater intersubjective agreement. However, just like other belief systems, there is no guarantee that contemporary biological theories are completely true. As such, it is just as mutable as any other fiction. Such a nuance in the discussion is not provided by Harari. Instead, he uses biology to point out the arbitrariness of past beliefs, then moves on to the next topic.</p><p>Throughout the course of the book, Harari again and again relies on the “objectivity” of modern sciences to provide authority to his arguments. This culminates in the final chapter as Harari takes up the Kurzweilian mantle and makes vague gestures towards a possible future for the human species, name-dropping cyborgs and the singularity along the way. As some who have studied belief in the singularity <a href="https://aeon.co/essays/why-is-the-language-of-transhumanists-and-religion-so-similar">point out</a>, it is as much a religious belief as a scientific one. And it is here that we find Harari himself engaging most clearly in the cognitive fictions of our current age. At their best, practicing scientists make a point of avoiding such scientific dogmatism, and instead appeal to skepticism of their findings, treating all knowledge as conditional. It is much more often exercised by those looking to shore up their own arguments with an appeal to the objective, wherever they think they can find it.</p><p>Of course it is impossible to write an objective history of humanity, and I can’t fault Harari for trying. The popularity of the book speaks to the extent to which its accessibility and clear writing have sparked the imaginations of many readers. It is a compelling story, but like all compelling stories, it shouldn’t be mistaken for truth.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4d448b2ffe24" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The present in terms of the future: Successor representations in Reinforcement learning]]></title>
            <link>https://awjuliani.medium.com/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/316b78c5fa3</guid>
            <category><![CDATA[neuroscience]]></category>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[psychology]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Thu, 31 Oct 2019 19:21:32 GMT</pubDate>
            <atom:updated>2019-11-04T21:42:32.483Z</atom:updated>
            <content:encoded><![CDATA[<p>A few years ago wrote a <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">series of articles</a> on the basics of Reinforcement Learning (RL). In those, I walked through a number of the fundamental algorithms and ideas of RL, providing code along the way. This is a kind of continuation of that series, with the difference being that this article (and possibly future ones) will dive into some of the more eclectic and experimental elements of the RL world. I want to begin with a topic close to my heart and recent research interests, something called the <strong>successor representation (SR)</strong>. In this post I will walk through the inspiration and basic mathematics of SRs, demonstrate a simple working example, and discuss the ways in which SRs have been applied in deep learning, psychology, and neuroscience in recent years.</p><p><em>(Note: If you’d like to jump ahead and follow along with the tutorial code, you can find it </em><a href="https://github.com/awjuliani/successor_examples"><em>here</em></a><em>).</em></p><h4>Background — Value estimates</h4><p>Almost all reinforcement learning algorithms are concerned, in one way or another, with the task of estimating value. At the simplest level this means learning how good it is to be in the current state — i.e. V(s), or how good is it to take any of the actions available in the current state — i.e. Q(s, a). It is with this latter piece of information that informed decisions can be made so as to optimize future reward. These two functions, V and Q, serve as the backbone of the majority of contemporary algorithms, ranging from <a href="https://daiwk.github.io/assets/dqn.pdf">DQN</a> and <a href="https://arxiv.org/abs/1801.01290">SAC</a> to <a href="https://arxiv.org/pdf/1707.06347">PPO</a> and many more.</p><p>The process of learning value estimates from the rewards in an environment is referred to as credit assignment. Given an often sparse reward signal, how do we appropriately assign value to the states and actions which eventually lead to these rewards? Put even more simply, what states and actions were useful for acquiring the reward, and which were not? If we can learn this, we can learn an optimal policy.</p><p>The vast majority of algorithms involve learning these value estimates directly from the reward. There are other ways to arrive at a value estimate though. In fact, value estimates can be seen as the combination of two pieces of information which are even more fundamental: (1) — <em>the</em> <em>reward function of an environment</em>, and (2) — <em>the dynamics of the environment and agent itself</em>. This second piece makes up the <strong>successor representation</strong>, and it provides a mathematically straightforward way for an agent to <em>represent the present state in terms of the future</em>.</p><h4>Basics of the Successor Representation</h4><p>The value estimate for a given state corresponds to the expected temporally-discounted return which an agent is expected to receive over time starting from that state.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2Q20BwbbIgL6Ps23PuyBwg.png" /></figure><p>The insight described by <a href="http://gatsby.ucl.ac.uk/~dayan/papers/sr93.pdf">Peter Dayan, who first formalized the successor representation</a> is that this function can be decomposed into the expected discounted future states which will be encountered and the expected reward in those states. With this insight, we can re-write the value function as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SXeqJG1EUPmNNAp_Gn6Lsw.png" /></figure><p>If the states of the environment are discrete and known in advance, then we can represent V(s) using the product of a reward array and a successor matrix. In this case, R(s) corresponds to an array in which each element represents a state, and the value of that element is the reward received in that particular state. M(s, s’) corresponds to a matrix with the rows and columns each corresponding to all the states in the environment, and the values of the matrix corresponding to the expected future occupancy of one state given another. A given row of M(s, *) reflects how often all other states are likely to be visited from a given state. When R(s) and M(s,s’) are combined using the dot product, the result is a traditional value estimate for any given state of the environment.</p><p>We can learn R(s) and M(s,s’) using all of the same traditional methods for learning value estimates in reinforcement learning. In order to learn R(s), all that is needed is to minimize the distance between the true reward and R(s):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AIuGzDSog8Olhfq3zFePpQ.png" /></figure><p><em>(Where alpha is the learning rate and r is the reward)</em></p><p>In order to learn M(s,s’), we can use the canonical temporal difference learning algorithm:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hAbx6rsl0TwQmEneoWT_fA.png" /></figure><p><em>(Where alpha is the learning rate, gamma is the discount factor, and I is a one-hot vector with 1 in the index of the state)</em></p><p>If we want to learn a state/action Q-function instead of a state value function, we can simply learn a successor tensor, of the shape <em>actions x states x states</em>.</p><h4>An Interactive Example</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hRqMqBf5m9vi1jORK4AT0w.png" /><figcaption>Simple four-room gridworld environment. Red corresponds to the agent position. Blue corresponds to the walls. Green corresponds to the goal position.</figcaption></figure><p>What is appealing about learning these two pieces of information separately is that changes to one of the two parts no longer impacts the other, as would be the case in traditional value-learning algorithms. Take for example a simple gridworld environment with four rooms. In this environment a specific state in the rightmost room contains a treasure chest and provides an agent with a +1 reward, and all other states provide no reward.</p><p><em>If you’d like to try this example out for yourself, you can find an interactive ipython notebook, along with all the required code in a GitHub repository </em><a href="https://github.com/awjuliani/successor_examples"><em>here</em></a><em>.</em></p><p>We can train an agent using any number of algorithms to move toward the rewarding chest. Let’s say that after the initial learning phase, our environment has changed, and the rewarding chest has been moved to the bottom right room. If we have used a traditional value learning algorithm, the agent now has to completely re-learn the value estimates, since the reward landscape has changes, and all the value estimates are wrong. If, however it learned the reward function and the successor representation separately, it only needs to re-learn the reward function, and can re-use the successor representation it learned originally.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Kaz9IbVXBwIf72QeAd5ghA.png" /><figcaption>Successor representations of nine states from four-room environment. Each state is represented in terms of future expected states.</figcaption></figure><p>There is one small caveat, which is that depending on the policy used to learn the the successor representation, it may be biased toward the original reward location, and might need a little fine-tuning. Why is that? Because the successor representation is itself actually composed of two separate entities itself! Don’t worry though, the recursive decomposition ends here. The successor representation is the product of the inherent transition dynamics of the environment itself (i.e. what happens when you take each action in each state — T(s, a) = s’) and the policy the agent is following (i.e. what action the agent chooses to take in each state — P(s) = a). Because of this, unless the policy is completely uniform random, the successor representation will be biased towards certain parts of the state space based on the biases in the policy function. In the case of the four-room environment, the successor states will likely point toward the original reward location, even when the reward is placed in the new location. Despite this, using the successor representation still results in much faster learning than using just a value function by itself.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DMyKIlSl7l7HixhY6wSDew.png" /><figcaption>Agent state occupancy during test-time after being exposed to both reward locations. Agent learns two separate paths to each of the goals.</figcaption></figure><p>What the example above demonstrates is that successor representations enable a simple version of task transfer. Because we have learned separate functions for the state representation and the reward function, we can now transfer what we’ve learned about how the agent moves around the space, and apply it to any arbitrary reward function possible within this environment. I encourage those <a href="https://github.com/awjuliani/successor_examples">following along with the example code</a> to do just this, and see what kind of results come about.</p><h4>The Successor Representation and Hierarchical RL</h4><p>Once we have learned a successor representation, it can be analyzed to discover things about the nature of the environment. Going back again to the four-room environment example, once we have learned a successor representation, we can perform dimensionality reduction on the learned representation of each state. This allows us to visualize the representations in a human interpretable way, even though they are inherently high-dimensional. If we plot them on a 2D plane (see figure below), we can make a couple observations. The first is that all of the states in each of the four rooms are represented as being very similar to all other states in that room. If we think about what the successor representation captures, this is to be expected, as each of the states in a given room is most likely to lead to another state in the room following any given policy.</p><p>More interestingly, the four states corresponding to the doors between the rooms are represented far apart from all other states, and are between the groupings of within-room states. This is because from these states the agent can end up in very different parts of the state space. We can refer to these states as <em>bottlenecks</em>, since they allow for transitions between different groups of states. Learning which states are bottlenecks can be useful, especially if we want to then use them as the states in a higher-level controller as part of a <a href="https://www.sciencedirect.com/science/article/pii/S0004370299000521">hierarchical learning algorithm</a>. It is exactly these kinds of states which make ideal candidates for option termination points (or sub-goals) in a hierarchical reinforcement learning framework. Many people also believe this kind of hierarchical decomposition of state space is what <a href="https://t.co/QjMEJVjCpC?amp=1">naturally takes place in human and other animal brains</a> (More on the implications of the successor representation for psychology below).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5ZO_xilOJaykKN0z4cCbcQ.png" /><figcaption>All state’s successor representations from four-room environment plotted on 2D grid. Bottleneck states marked in orange. All other states are marked in purple.</figcaption></figure><h4>The Successor Representation and Deep Learning</h4><p>So far, I have only described learning successor representations in the tabular domain. There is in fact <a href="http://papers.nips.cc/paper/6994-successor-features-for-transfer-in-reinforcement-learning">plenty of</a> <a href="https://arxiv.org/abs/1606.02396">work</a> that has been done to extend the successor framework to the world of deep learning and neural networks as well. The key insight to make this happen is that instead of R(s) and M(s,s’) being a vector and matrix, they can become the high-dimensional outputs of neural networks. Likewise, rather than being a simple one-hot index, the state can be represented as any vector of real numbers. Because of this property, successors in the deep learning framework are referred to as <em>successor features, </em>a phrase proposed by <a href="http://papers.nips.cc/paper/6994-successor-features-for-transfer-in-reinforcement-learning">Andre Barreto and colleagues</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EKmyUDCb76KDkY5juZNo5g.png" /><figcaption>Example of a neural network architecture utilizing successor features. Specifically, this is the <a href="https://arxiv.org/abs/1812.07626">Universal Successor Feature Approximator</a> agent.</figcaption></figure><p>In the past few years there have been a number of interesting lines of research taking advantage of successor features. <a href="https://arxiv.org/abs/1812.07626">One great example</a> of this is utilizing them in a powerful multi-task learning setting. Borsa and colleagues have shown that agents using successor features can flexibly transfer between multiple different tasks in a high-dimensional 3D navigation task. Agents using successor features have <a href="https://arxiv.org/abs/1906.05030">also been shown</a> to be able to learn to play multiple Atari games with only limited exposure to the true reward function. Successor features have <a href="https://arxiv.org/abs/1807.11622">also been used</a> by researchers at the University of Alberta and Google Brain to generate an intrinsic exploration reward signal for agents playing Atari games. Since the successor features measure how likely certain future states are to appear, it turns out that they can also be used to determine how novel new states are as well.</p><h4>The Successor Representation in Psychology and Neuroscience</h4><p>It isn’t just in the domain of machine learning that the successor representation has gained traction as a paradigm of choice, it has also caught the eye of a number of psychologists and neuroscientists as well.</p><p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005768">Research by Evan Russek and colleagues has demonstrated</a> that agents using a modified form of the successor representation are able to mimic a number of the basic capacities of rodents when navigating mazes. These capabilities fall under what psychologists refer to as a cognitive map. This ‘map’ provides the animals the ability to represent and navigate space in a non-reactive way. For rodents, this <a href="https://psycnet.apa.org/fulltext/1949-00103-001.pdf">has been demonstrated</a> in their ability to perform latent learning (the ability to use irrelevant information from a previous task to solve a new task) and the ability to quickly adapt to shortcuts and detours. Agents trained using a successor representation and a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.329.6065&amp;rep=rep1&amp;type=pdf">form of experience replay called Dyna</a> were able to demonstrate both of these abilities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RCCBTQrbu-BN3OVcB3ybNQ.png" /><figcaption>Firing activity of a place cell in a rodent as it navigates around a circular space. B and C show effects of moving an indicator on the wall of the space. The place cell firing is anchored by the position of the indicator. Reproduced from <a href="https://www.jneurosci.org/content/7/7/1951.short">Muller &amp; Kubie 1987</a>.</figcaption></figure><p>Research scientist <a href="https://www.nature.com/articles/nn.4650">Kimberly Stachenfeld and colleagues</a> have proposed that the activity of place cells can be modeled using a successor representation. For those unfamiliar with place cells, they are cells in the mammalian hippocampus which fire whenever the animal is in a specific location in space, regardless of the direction the animal is facing. These cells are thought of as one of the building blocks of the cognitive map. It turns out that these cells display certain biases in their response to the location of the animal which the successor representation captures nicely.</p><p>The connection goes further though. There is another canonical group of brain cells (this time in the entorhinal cortex), referred to as <a href="https://www.nature.com/articles/nature03721">grid cells</a>. These cells fire consistently as the animal moves around an environment, with each cell responding with a uniform grid-like firing affinity for different parts of the environment. It also happens to be the case that taking the <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a> of the successor representation of the place cells results in a representation that looks a lot like those grid cells.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/888/1*i-VeQDgfxXPskzyhzB3-Fg.png" /><figcaption>Grid cell activity of rodent as it navigates environments of different shapes. Reproduced from <a href="https://www.nature.com/articles/nn.4650">Stachenfeld et al., 2017</a>.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/896/1*6jyGbxYRfKKdznNQNW_5nw.png" /><figcaption>Eigendecomposition of successor representation of agent that navigated environments of different shapes. Reproduced from <a href="https://www.nature.com/articles/nn.4650">Stachenfeld et al., 2017</a>.</figcaption></figure><p><a href="https://www.nature.com/articles/s41562-017-0180-8">Another line of behavioral research</a> by research scientist Ida Momennejad and colleagues has shown that humans make decisions which are more consistent with learning a successor representation, rather than learning either a direct value function, or a more complex model of the environment. The authors of the study demonstrated this by showing that like the agent navigating the four-room environment, humans are better able to adapt when the reward location changes than they are when the structure of the environment itself changes. The successor approach also has advantages over pure planning, where an agent or person would have to consciously run through a chain of possible future outcomes to pick the best one. With a successor representation, that information is already available in a summarized form for you.</p><h4>Epilogue</h4><p>All of this research is still in the early phases, but it is encouraging to see that a useful and insightful computation principle from over twenty years ago is being so fruitfully applied in both the domains of deep learning and neuroscience. As you may have guessed by this point, it is also something that has been at the center of my own research this past year, and I hope to be able to share the fruit of some of that work at some point in the not-too-distant future.</p><p>Even if the successor formulation turns out to not exactly be how humans and animals represent their ‘current state,’ the concept of representing the present in terms of the future is an appealing one. We are, after all, always forward-looking creatures, and predictive representations of one form or another are what can make that orientation possible.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=316b78c5fa3" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Maximum Entropy Policies in Reinforcement Learning & Everyday Life]]></title>
            <link>https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/f5a1cc18d32d</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[psychology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[philosophy]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Fri, 02 Nov 2018 17:26:47 GMT</pubDate>
            <atom:updated>2018-11-02T17:26:47.947Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iSaDYGVjl6yns7mjy8C0PQ.jpeg" /></figure><p>As those who follow this blog are probably aware, I spend <a href="https://medium.com/@awjuliani">a lot of time</a> thinking about Reinforcement Learning (RL). These thoughts naturally extend into my everyday life, and the ways in which the formalisms provided by RL can be applied to the world beyond artificial agents. For those unfamiliar, Reinforcement Learning is an area of Machine Learning dedicated to optimizing behaviors in the context of external rewards. If you’d like an introduction, I wrote a <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">series of introductory articles</a> on the topic a couple years ago. What I want to talk about in this article is not RL as a whole, but specifically the role of randomness in action selection.</p><p>At first glance, the notion of randomness may seem counterintuitive for an algorithm with the intention of arriving at optimal behavior. Surely optimal behavior isn’t random. It turns out however that random actions are essential to the learning process. We use random actions in RL because we want our agents to be able to explore their worlds, and in lieu of some a-priori knowledge about the world, random actions are as good a policy to take as any other to start exploring the environment. That being said, these random actions are taken under specific conditions, such as when selecting actions from a probability distribution, like you will find in policy gradient methods, or using an epsilon-greedy schedule for action selection, like you will find in value-based methods.</p><h4>Entropy in Reinforcement Learning</h4><p>In many RL learning algorithms, such as the policy-gradient and actor-critic families, the actions are defined as a probability distribution, condition on the state of the environment: <em>p(a | s)</em>. When an agent takes a discrete action, picking one of many possible actions, a categorical distribution is used. In the case of a continuous control agent, a gaussian distribution with a mean and standard deviation may be used. With these kinds of policies, the randomness of the actions an agent takes can be quantified by the <em>entropy</em> of that probability distribution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*7Ja9Up5ljzxSgi_Fk5X42A.png" /><figcaption>Categorical (left), and Gaussian (Right) distributions. Orange shows low-entropy distributions, while blue shows high-entropy distributions.</figcaption></figure><p>Entropy is a term with a long history. It was originally used in physics to denote the lack of order within a system. From there it was integrated into the core of information theory, as a measure of information present within a communication. In the case of RL, the information theoretic definition gets repurposed. Because RL is all about learned behaviors, entropy here relates directly to the unpredictability of the actions which an agent takes in a given policy. The greater the entropy, the more random the actions an agent takes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KULoIEbu5k1sp5M5SoqSCA.png" /><figcaption>Equation for entropy of a discrete probability distribution (p).</figcaption></figure><p>In RL, the goal is typically formaized as optimizing the long-term sum of discounted rewards. This means learning to take specific sequences of actions which can accomplish this goal at the exclusions of other possible action sequences. Such a learning process will naturally lead to the entropy of the action selection policy decreasing. This is only reasonable, since if we expect purposeful and coordinated behavior, then that behavior will naturally be less random than the original policy.</p><h4>Encouraging Entropy</h4><p>Those familiar with the RL literature however will know that this is not the entire story. In addition to encouraging a policy to converge toward a set of probabilities over actions which lead to a high long-term reward, it is also typical to add what is sometimes called an “entropy bonus” to the loss function. This bonus encourages the agent to take actions <em>more unpredictably</em>, rather than less so.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3vsYK6DI6Q106dLXr17F8A.png" /><figcaption>Update equation for A3C. Entropy bonus is <strong>H(</strong>π<strong>)</strong> term.</figcaption></figure><p>Entropy bonuses are used because without them an agent can too quickly converge on a policy that is locally optimal, but not necessarily globally optimal. Anyone who has worked on RL problems empirically can attest to how often an agent may get stuck learning a policy that only runs into walls, or only turns in a single direction, or any number of clearly suboptimal, but low-entropy behaviors. In the case where the globally optimal behavior is difficult to learn due to sparse rewards or other factors, an agent can be forgiven for settling on something simpler, but less optimal. The entropy bonus is used to attempt to counteract this tendency by adding an entropy increasing term to the loss function, and it works well in most cases. Indeed, many of the current state of the art on-policy Deep RL methods such as <a href="https://arxiv.org/abs/1602.01783">A3C</a>, <a href="https://arxiv.org/abs/1707.06347">PPO</a>, and others take this approach.</p><h4>Maximizing for Long-term Entropy</h4><p>While entropy bonuses are widely used, they actually connect to a much more fundamental concept in the theory of learning behavior. The entropy bonus described so far is what is referred to as a one-step bonus. This is because it applies to only the current state of the agent in an environment, and doesn’t take account of future states the agent may find itself in. It can be thought of as a “greedy” optimization of entropy. We can draw a parallel to how RL agents learn from rewards. Rather than optimizing for the reward at every timestep, agents are trained to optimize for the long-term sum of future rewards. We can apply this same principle to the entropy of the agent’s policy, and optimize for the long-term sum of entropy.</p><p>There is actually theoretical work from a <a href="https://arxiv.org/abs/1702.08165">number</a> <a href="https://arxiv.org/abs/1805.00909">of</a> <a href="https://arxiv.org/abs/1704.06440">researchers</a>, to suggest that not only providing an entropy bonus at each time step, but optimizing for this long-term objective is an even better approach. What this means is that it is optimal for an agent to learn not only to get as much future reward as possible, but also to <em>put itself in positions where its future entropy will be the largest</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IhqM9ERl4tAksk6HrG5RkA.png" /><figcaption>Equation for Maximum Entropy Reinforcement Learning. Optimal policy π corresponds to maximum over both discounted rewards and entropy.</figcaption></figure><p>One way to think about it is that an optimal agent does everything necessary to get as much reward as possible, but is as non-committal as possible about the specific set of actions it is taking, such that it can change its behavior in the future. An ever simpler way to think about it is that optimizing for long-term entropy means optimizing for long-term <em>adaptability</em>. This way, if a better way of acting presents itself, either within the current episode of learning, or within the training process as a whole, the agent can most easily transition to another policy. The formalism around this approach is referred to as <strong>maximum entropy reinforcement learning</strong>. As you can probably imagine, it is called this because we want to jointly optimize for <em>long-term reward as well as long-term entropy</em>. This approach is useful under a number of circumstances, all of which related to changes in the agent’s knowledge of the environment, or changes in the environment itself over time. There has been some work to empirically validate this approach as well, as you can see in the figure below. Even in these few Atari tasks, which are stationary, the use of a long-term entropy reward leads to similar or better performance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q_6kh5UAZ23QH2rNp54BiA.png" /><figcaption>Results from experiments comparing one-step entropy bonus (red) to long-term optimization of entropy (blue). In the six tasks compared, the long-term entropy optimization leads to as good or better performance than the naive one-step entropy optimization. Taken from <a href="https://arxiv.org/abs/1704.06440">https://arxiv.org/abs/1704.06440</a>.</figcaption></figure><h4>Maximum Entropy Policies in Everyday Life</h4><p>I’d like to argue that this maximum entropy reinforcement learning principal actually applies much more broadly than just to RL, and touches many aspects of our lives as well. In maximum entropy RL, the basic principle is that optimal behavior corresponds to a proper balance between commitment and adaptability. I believe that this applies just as well to life decisions as it does to the behavior of artificial agents.</p><p>Consider the hypothetical example of moving to a new city in a colder climate than where you grew up. You might have developed a habit of frequently wearing t-shirts and shorts where you came from. In the new city this may lead to a less comfortable experience. Your willingness to adjust your wardrobe to fit the new circumstances is directly related to how “high-entropy” your clothing policy was. In the original city you optimized your clothing for comfort. If you had a high-entropy policy, you will quickly be able to adapt to the new city. If you had a low-entropy clothing policy, then you may stubbornly hold on to your pre-existing clothing patterns, and suffer as a result. The key here is not only having a high-entropy policy in the moment, but ensuring that when something like a move to a new city happens, the entropy will also be high. This would correspond to not spending all your money on T-shirts, for example.</p><p>The above example may seem somewhat silly, but I think it is reflective of a vast array of phenomena we encounter in modern society. Let’s consider another example at the societal level: that of scientific development. Take for example any revolution in science, such as the Copernican, Darwinian, or your personal favorite. Scientists attempting to optimize the rewards (fame, truth, societal/technological impact, etc) of scientific discovery, were faced with an opportunity to either continue along their pre-existing lines of research, or to adapt to the new paradigm. Those with “high-entropy” research programs are more likely to adapt to a scientific program based on a sun-centered universe, or natural-selection based development of an organism’s traits. In contrast, those with low-entropy policies are more likely to continue with their pre-existing programs to their detriment. The long-term maximum-entropy aspect comes in early in these scientists careers. They are faced with the opportunities to ensure that their research programs don’t become too closed or focused on specific theoretical beliefs. Making these decisions early in a career then enables swift changes later in one’s life.</p><p>The examples provided above are just a few of countless possible ones related to our personal and collective decision making in the world. Similar examples can easily be drawn from interpersonal life, politics, and any number of other life decisions. In all cases the key is to plan not only for a good outcome, but the ability to change when the world does. This is an insight that many successful individuals have already ingrained into their lives, and I imagine there will likely be many artificial individuals imbued with the same insight to come.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f5a1cc18d32d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[On “solving” Montezuma’s Revenge]]></title>
            <link>https://awjuliani.medium.com/on-solving-montezumas-revenge-2146d83f0bc3?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/2146d83f0bc3</guid>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[robotics]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Fri, 13 Jul 2018 09:16:20 GMT</pubDate>
            <atom:updated>2018-07-14T08:39:26.087Z</atom:updated>
            <content:encoded><![CDATA[<h4>Looking beyond the hype of recent Deep RL successes</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/768/1*Hk_GzhlHa2y1Dea4O4u7ag.jpeg" /></figure><p>In recent weeks DeepMind and OpenAI have each shared that they developed agents which can learn to complete the first level of the Atari 2600 game Montezuma’s Revenge. These claims are important because Montezuma’s Revenge is important. Unlike the vast majority of the games in the <a href="https://arxiv.org/abs/1709.06009">Arcade Learning Environment</a> (ALE), which are now easily solved at superhuman level by learned agents, Montezuma’s Revenge has been hitherto unsolved by Deep Reinforcement Learning methods and was thought by some to be unsolvable for years to come.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UB4n5q9HWfUsLOA6R-u1rA.png" /><figcaption><em>Figure 1. The first room of Montezuma’s Revenge.</em></figcaption></figure><p>What distinguishes Montezuma’s Revenge from other games in the ALE is its relatively sparse rewards. For those unfamiliar, that means that the agent only receives reward signals after completing specific series of actions over extended periods of time. In the case of the first room of Montezuma’s Revenge (see Figure 1 above), this means descending a ladder, jumping across an open space using a rope, descending another ladder, jumping over a moving enemy, and then finally climbing another ladder. All of this is just to get the very first key in the very first room! In the first level there are 23 more such rooms for the agent to navigate through in order to complete the level (see Figure 2 below). To complicate this further, the failure conditions in the game are fairly strict, with the agent’s death happening due to any number of possible events, the most punishing of which is simply falling from too high a place. I encourage those who are unfamiliar with the game to try playing it and see how long it takes you to even get past the first room, let alone the first level. You can find an online version of the game here: <a href="https://www.retrogames.cz/play_124-Atari2600.php?language=EN">https://www.retrogames.cz/play_124-Atari2600.php?language=EN</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*H5CUZ0lQqRk6xbzNRa2M9A.png" /><figcaption><em>Figure 2. Solution to first level of Montezuma’s Revenge.</em></figcaption></figure><p>Due to this notorious difficulty, the game has been seen as a kind of grand-challenge for Deep RL methods. In fact, the game has inspired the development of some of the more interesting approaches to augmenting or reworking the traditional Deep RL algorithm, with approaches utilizing novel methods for <a href="https://arxiv.org/abs/1703.01161">hierarchical control</a>, <a href="https://arxiv.org/abs/1703.01310">exploration</a>, and <a href="https://arxiv.org/abs/1806.05635">experience replay</a>. So, it was big news (at least in certain circles) when DeepMind and OpenAI each claimed to have developed algorithms capable of playing the game so well. To give you a sense of how much better, consider that the previous state of the art in the game was 2600 points, with these new methods achieving scores in the tens of thousands of points. All three proposed methods are impressive efforts from both an engineering and theoretical perspective, all with things to learn from. Unfortunately, the claims of solving Montezuma’s Revenge with Deep Reinforcement Learning are not quite what they seem. In all three cases (two papers by DeepMind, and one blog post by OpenAI), the use of expert human demonstrations was an integral part of the algorithm, changing fundamentally the nature of the learning problem.</p><p>In this post, I want to discuss what these methods do in order to solve the first level of Montezuma’s Revenge, and why in the context of the game, and long-term goals for Deep RL, this approach isn’t as interesting or meaningful as it might seem. Finally, I will briefly discuss what I would see as truly impressive results on the notorious game, one which would point the way forward for the field.</p><h4>DeepMind’s Results</h4><p><strong>Learning from YouTube</strong></p><p>Sporting the eye-catching title “ <a href="https://arxiv.org/abs/1805.11592">Playing hard exploration games by watching YouTube</a>,” DeepMind offers the most interesting of the three approaches to solving Montezuma’s Revenge. As the title suggests, the research group devised a method by which videos of expert players completing the first level of the game can be used to aid the learning process. The problem of learning from videos is in itself an interesting challenge, completely outside of the additional challenges posed by the game in question. As the authors point out, videos found on YouTube contain a various arrangement of artifacts which can prevent the easy mapping between what is happening in the video, and what an agent playing in the ALE might observe. In order to get around this “gap,” they create a method which is able to embed observations of the game state (visual and auditory) into a common embedding space.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*tqE3an0mxW_ceV2EQwbdoQ.png" /><figcaption><em>Figure 3. Comparison of different demonstrations videos to emulator image.</em></figcaption></figure><p>This embedding space is then utilized to provide a kind of bread-crumb reward to the learning agent as it progresses. Instead of only receiving the sparse rewards provided by the original game, the agent has access to intermediate rewards which correspond to reaching checkpoints along a path which are provided by expert players. In this way the agent has access to a much stronger learning signal and is able to ultimately complete the first level of the game with a score of 41,000.</p><p><strong>Q-Learning from Demonstrations</strong></p><p>Around the same time that the YouTube paper was released, DeepMind shared results on another set of experiments with a somewhat less conspicuous title: “ <a href="https://arxiv.org/abs/1805.11593">Observe and Look Further: Achieving Consistent Performance on Atari</a>.” In the paper they propose a set of interesting algorithmic improvements to Deep Q-learning which are able to increase the stability and capability of the algorithm. The first of these is a method for increasing the discount factor in the Q-update, such that longer-term temporal dependencies can be learned, without the typical downsides of higher discount factors. The second is a means of enabling Deep Q-learning to account for reward signals at varying orders of magnitude, and thus enable their algorithm to solve tasks in which an optimal policy involves learning about these varying rewards.</p><p>Alongside these two improvements however, they also propose the use of human demonstrations as a means of augmenting the exploration process, by automatically providing information to the network about trajectories through state space that an expert player would follow. With all of these three improvements combined, the authors arrive at an agent which is able to learn to complete the first level of Montezuma’s Revenge with a score of 38,000. Noticeably however, the first two improvements without the expert demonstrations are not enough to lead to compelling performance on the game, scoring only in the 2,000 point range.</p><h4>OpenAI’s Results</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/798/1*_6C8tKDR0dZcudBNHD93VA.png" /><figcaption><em>Figure 4. Restarts used during training over time.</em></figcaption></figure><p>A few weeks after the DeepMind results, OpenAI shared a <a href="https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/">blog post</a> in which they describe a method by which they also are able to train an agent to complete the first level of Montezuma’s Revenge. This one also relies on human demonstrations but utilized them in a slightly different way than the DeepMind approach. Rather than using demonstrations as part of the reward or learning signal, demonstrations are used as a means of intelligently restarting the agent. Given an expert human trajectory through the game, the agent is started near the end of the game, and then slowly moved backwards through the trajectory on every restart as learning progresses. This has the effect of exposing the agent to only parts of the game which a human player has navigated through, and only widening the scope as the agent itself becomes more competent. With this method there is no change to the actual learning algorithm, as a default Proximal Policy Optimization (PPO) is used. Simply starting the agent in the “right” place is enough to ensure it stumbles onto the correct solution, earning an impressive score of 74,500.</p><h4>Limitations of Imitation</h4><p>The one thing all of the approaches described above have in common is that they utilize a set of expert human demonstrations. The first approach utilized the demonstrations in order to learn a reward signal, the second utilized them in order to learn more accurate Q-values, and the third utilized them to more intelligently restart the agent. In all three cases the demonstrations were critical to the learning process. In general the use of demonstrations is a compelling way to provide agents with meaningful knowledge about a task. Indeed, it is how many humans learn countless tasks. The key though to the humanness of our ability to learn from demonstrations is our ability to abstract and generalize a single demonstration to new situations. In the case of Montezuma’s Revenge, rather than developing a general-purpose solution to game playing (as the two DeepMind papers titles suggest), what has really been developed is an intelligent method for exploiting the game’s key weakness as an experimental platform: its determinism.</p><p>Every time a human or agent plays Montezuma’s Revenge, they are presented with the exact same set of rooms, each containing the exact same set of obstacles and puzzles. As such, the simple memorization of the movements through each room is enough to lead to a high-score, and the ability to complete the level. While this wouldn’t necessarily be a meaningful flaw if the agents were forced to learn from scratch, it becomes one when expert demonstrations enter into the situation. All three solutions exploit the deterministic nature of the game to allow the agent to more easily learn the solution path through the game. What is ultimately learned is not how to play difficult platformers, but how to execute a pre-determined set of actions in order to complete a specific game.</p><p>The OpenAI blog post briefly mentions the issue of determinism but does so at the level of the Atari emulator itself, rather than the specific game. Their solution is to use a randomized frame-skip to prevent the agent from memorizing the trajectory. While this prevents the agent from literally memorizing a sequence of actions, it does not prevent the memorization of the general trajectory through state space.</p><p>In all cases Montezuma’s Revenge no longer serves its original purpose of being a hard problem of sparse reward problem solving, and rather becomes an easier problem of learning a trajectory through a fixed state space. This is a shame, because in its original formulation the game still has the potential to provide one of the more compelling challenges to Deep Reinforcement Learning researchers.</p><h4>Solving Montezuma’s Revenge, the hard way</h4><p>I have personally kept an eye on Montezuma’s Revenge results for a few years now because I have seen them as a litmus test for the ability of Deep Reinforcement Learning agents to begin to show signs of more general reasoning and learning. Many results have shown that given enough computational ability Deep Reinforcement Learning, or <a href="https://arxiv.org/abs/1803.07055">even random search</a> are able to solve naïve optimization problems. The human-level intelligence so many researchers are interested in however does not involve simple optimization. It involves learning and reasoning about concepts over multiple levels of abstraction. It involves then generalizing that learned conceptual knowledge from one problem space to many in an adaptable way.</p><p>When you present any person the first room of Montezuma’s Revenge, and ask them what they need to do, they will quickly begin to describe to you a series of actions and observations which suggest a complex understanding of the likely dynamics of the game. The most obvious manifestation of this will be the recognition of the key as a desirable object, the skull as something to be avoided, and the ladders as having the affordance of movement. Keys then suggest the ability to open locked doors, and suddenly complex multi-step plans begin to emerge as to how to complete the level. This reasoning and planning doesn’t just work on one fixed level of the game, but works on any possible similar level or game we are presented with. It is these kinds of skills which are essential to human intelligence, and are of interest to those trying to push Deep Reinforcement Learning beyond the realm of a set of a simple optimization algorithms. However, utilizing human demonstrations in a deterministic environment completely bypasses the need for these exact skills.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/814/1*YbMweKmLI98uf9Z80oyzIw.png" /><figcaption><em>Figure 5. An example of what a game like Montezuma’s Revenge might look like to us without the priors we typically rely on to interpret visual scenes.</em></figcaption></figure><p>Of course, these skills are also the ones that are most difficult to put into algorithmic form, especially when they are still not fully understood in their human manifestations. Especially in the case of conceptual learning, outside general knowledge often needs to be brought to bear on a new problem. As a <a href="https://rach0012.github.io/humanRL_website/">group at Berkeley have pointed out</a>, without our human priors (either biological or learned throughout life), many of the video games which we take for granted as being simple turn out to be much more complex. To demonstrate this, they even have <a href="https://high-level-4.herokuapp.com/experiment">interactive browser game</a> which you can play that simulates what a randomly initialized pixel-based agent might “experience.”</p><p>The problem then becomes how can agents naturalistically learn the priors required to make sense of a game like Montezuma’s Revenge. Furthermore, how can these learned priors be used to learn to play not just one fixed level of the game, but any level of any similar game. There is interesting work being done in the areas of <a href="https://arxiv.org/abs/1707.08475">representation learning</a>, and <a href="https://arxiv.org/abs/1707.03389">conceptual grounding</a> which I think will be essential to tackling these kinds of problems. There is also work being done to develop more stochastic environments which better test the generalization ability of agents, most compelling among these approaches being the <a href="http://www.gvgai.net/">GVGAI competition</a>. This research direction is still in its early stages, but shows a lot of promise.</p><p>I eagerly look forward to the day we can say without a doubt that an agent can learn to play Montezuma’s Revenge from scratch. When that day comes there will be a lot to be excited about.</p><p><em>Feel free to reply in the comments with your thoughts and opinions. What are presented here are just my personal thoughts on the topic, and would love to hear from others, especially if you work in Deep RL and have experience with Montezuma’s Revenge.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2146d83f0bc3" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning]]></title>
            <link>https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565?source=rss-18dfe63fa7f0------2</link>
            <guid isPermaLink="false">https://medium.com/p/79cf1e83d565</guid>
            <category><![CDATA[neural-networks]]></category>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[robotics]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Arthur Juliani]]></dc:creator>
            <pubDate>Wed, 31 Jan 2018 17:24:11 GMT</pubDate>
            <atom:updated>2018-02-15T08:09:01.797Z</atom:updated>
            <content:encoded><![CDATA[<h4>What goes into a stable, accurate reinforcement signal?</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uDwUOs1zGADrkDxJH0VVXQ.png" /></figure><p><em>(This post assumes some familiarity with machine learning, and reinforcement learning in particular. If you are new to RL, I’d recommend checking out a </em><a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"><em>series of blog posts</em></a><em> I wrote in 2016 on the topic as a primer)</em></p><h4>Introduction</h4><p>Since the <a href="https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/">launch</a> of the ML-Agents platform a few months ago, I have been surprised and delighted to find that thanks to it and other tools like OpenAI Gym, a new, wider audience of individuals are building Reinforcement Learning (RL) environments, and using them to train state-of-the-art models. The ability to work with these algorithms, previously something reserved for ML PhDs, is opening up to a wider world. As a result, I have had the unique opportunity to not just write about applying RL to existing problems, but also to help developers and researchers debug their models in a more active way. In doing so, I often get questions which come down to a matter of understanding the unique hyperparameters and learning process around the RL paradigm. In this article, I want to attempt to highlight one of these conceptual pieces: <strong>bias and variance in RL</strong>, and attempt to demystify it to some extent. My hope is that in doing so a greater number of people will be able to debug their agent’s learning process with greater confidence.</p><h4>Supervised Machine Learning</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/602/1*x8CBE7eAbaifwM15KNHuUA.png" /><figcaption>If you’ve studied ML, these charts may be<a href="https://www.coursera.org/learn/machine-learning"> familiar to you</a>.</figcaption></figure><p>Many machine learning practitioners are familiar with the traditional bias-variance trade-off. For those who aren’t, it goes as follows: on the one hand, a “biased” model generalizes well, but doesn’t fit the data perfectly (“under-fitting”). On the other hand, a high-variance model fits the training data well, too well in-fact, to the detriment of generalization (“overfitting”). In this situation, the problem becomes one of limiting the capacity of a model with some regularization method. In many cases, dropout or L2 regularization with a large enough data set is enough to do the trick. That is the story for typical supervised learning. RL is a little different, as it has its own separate bias-variance trade-off which operates in addition to, and at a higher level than the typical ML one.</p><h4>Reinforcement Learning</h4><p>In RL, bias and variance no longer just refer to how well the model fits the training data, as in supervised learning, but also to <em>how well the reinforcement signal reflects the true reward structure of the environment</em>. To understand that statement, we have to backup a little. In reinforcement learning, instead of a set of labeled training examples to derive a signal from, an agent receives a reward at every decision-point in an environment. The goal of an agent is to learn a <em>policy</em> (method for taking actions) which will lead to obtaining the greatest reward over time. We must do this using only the individual rewards that agent receives, without the help of an outside oracle to designate what count as “good” or “bad” actions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/1*lzpznPvKnSUvLwNfG9pOew.png" /><figcaption>Rewarding state denoted by yellow star. Value estimates denoted by green spheres. <strong>Above</strong>: Without credit assignment, only rewarding state is seen as being valuable. <strong>Below</strong>: By using discounted sums over future rewards, the trajectory toward star has meaningful value estimates.</figcaption></figure><p>A naive approach to an RL learning algorithm would be to encourage actions which were associated with positive rewards, and discourage actions associated with negative rewards. Instead of updating our agent’s policy based on immediate rewards though, we often want to account for actions (and the states of the environment when those actions were taken) which lead up to rewards. For example, imagine walking down a corridor to a rewarding object. It isn’t just the final step we want to perform again, but all the steps up to that rewarding one. There are a number of approaches for doing this, all of which involving doing a form of <em>credit assignment</em>. This means giving some credit to the series of actions which led to a positive reward, not just the most recent action. This credit assignment is often referred to as learning a value estimate: <em>V(s)</em> for state, and <em>Q(s, a)</em> for state-action pair.</p><p>We control how rewarding past actions and states are considered to be by using a discount factor (γ, ranging from 0 to 1). Large values of γ lead to assigning credit to states and actions far into the past, while a small value leads to only assigning credit to more recent states and actions. In the case of RL, variance now refers to a noisy, but on average accurate value estimate, whereas bias refers to a stable, but inaccurate value estimate. To make this more concrete, imagine a game of darts. A high-bias player is one who always hits close to the target, but is always consistently off in some direction. A high-variance player, on the other hand, is one who sometimes hits the target, and is sometimes off, but on average near the target.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/811/1*aZNcm3rx8EqKIQRDuzKZlA.png" /><figcaption><strong>Red</strong>: True value of a given state/action. <strong>Blue</strong>: Low-variance, high-bias estimate. <strong>Green</strong>: low-bias, high-variance estimates.</figcaption></figure><p>There is a multitude of ways of assigning credit, given an agent’s trajectory through an environment, each with different amounts of variance or bias. <em>Monte-Carlo sampling</em> of action trajectories as well as <em>Temporal-Difference learning</em> are two classic algorithms used for value estimation, and both are prototypical examples of methods which are variance and bias heavy, respectively.</p><h4>High-Variance Monte-Carlo Estimate</h4><p>In <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte-Carlo (MC) sampling</a>, we rely on full trajectories of an agent acting within an episode of the environment to compute the reinforcement signal. Given a trajectory, we produce a value estimate <em>R(s, a) </em>for each step in the path by calculating a discounted sum of future rewards for each step in the trajectory. The problem is that the policies we are learning (and often the environments we are learning in) are stochastic, which means there is a certain level of noise to account for. This stochasticity leads to variance in the rewards received in any given trajectory. Imagine again the example with the reward at the end of the corridor. Given that an agent’s policy might be stochastic, it could be the case that in some trajectories the agent is able to walk to the rewarding state at the end, and in other trajectories it fails to do so. These two kinds of trajectories would provide very different value estimates, with the former suggesting the end of the corridor is valuable, and the latter suggesting it isn’t. This variance is typically mitigated by using a large number of action trajectories, with the hope that the variance introduced in any one trajectory will be reduced in aggregate, and provide an estimate of the “true” reward structure of the environment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3wb1I1Hgl6jtb5gogeySeg.png" /><figcaption>Monte-Carlo Estimate of Reward Signal. <strong>t</strong> refers to time-step in the trajectory. <strong>r</strong> refers to reward received at each time-step.</figcaption></figure><h4>High-Bias Temporal Difference Estimate</h4><p>On the other end of the spectrum is one-step <a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">Temporal Difference (TD) learning</a>. In this approach, the reward signal for each step in a trajectory is composed of the immediate reward plus a learned estimate of the value at the next step. By relying on a value estimate rather than a Monte-Carlo rollout there is much less stochasticity in the reward signal, since our value estimate is relatively stable over time. The problem is that the signal is now biased, due to the fact that our estimate is never completely accurate. In our corridor example, we might have some estimate of the value of the end of the corridor, but it may suggest that the corridor is less valuable than it actually is, since our estimate may not be able to distinguish between it and other similar unrewarding corridors. Furthermore, in the case of Deep Reinforcement Learning, the value estimate is often modeled using a deep neural network, making things worse. In <a href="https://deepmind.com/research/dqn/">Deep Q-Networks</a> for example, the Q-estimates (value estimates over actions) are computed using an old copy of the network (a “target” network), which will provide “older” Q-estimates, with a very specific kind of bias, relating to the belief of an outdated model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nma9I0Ot4TyzoL7rmqFatQ.png" /><figcaption>Temporal-Difference Estimate of Reward Signal. <strong>r</strong> refers to reward at time t. <strong>V(s)</strong> refers to parameterized value estimate.</figcaption></figure><h4>Approaches to Balancing Bias and Variance</h4><p>Now that we understand bias and variance and their causes, how do we address them? There are a number of approaches which attempt to mitigate the negative effect of too much bias or too much variance in the reward signal. I am going to highlight a few of the most commonly used approaches in modern systems such as <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization</a> (PPO), <a href="https://arxiv.org/abs/1602.01783">Asynchronous Advantage Actor-Critic</a> (A3C), <a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a> (TRPO), and others.</p><ol><li><strong>Advantage Learning</strong></li></ol><p>One of the most common approaches to reducing the variance of an estimate is to employ a baseline which is subtracted from the reward signal to produce a more stable value. Many of the baselines chosen fall into the category of <a href="https://arxiv.org/abs/1602.01783">Advantage-based Actor-Critic methods</a>, which utilize both an actor which defines the policy, and a critic (often a parameterized value estimate) which provides a more reduced variance reward signal to update the actor. The thinking goes that variance can simply be subtracted out from a Monte-Carlo sample (R/Q) using a more stable learned value function V(s) in the critic. This value function is typically a neural network, and can be learned using either Monte-Carlo sampling, or Temporal difference (TD) learning. The resulting Advantage A(s, a) is then the difference between the two estimates. This advantage estimate has the other nice property of corresponding to <em>how much better the agent actually performed than was expected on average,</em> thus allowing for intuitively interpretable values.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NUi91JHh4YhcGi7vRBp9HQ.png" /><figcaption>Advantage Estimate Equation. <strong>Pi</strong> refers to the current policy. <strong>Q(s, a)</strong> here refers to Monte-Carlo sampled reward signal analogous to R(s, a), rather than a learned estimate. <strong>V(s)</strong> refers to parameterized value estimate.</figcaption></figure><p><strong>2. Generalized Advantage Estimate</strong></p><p>We can also arrive at advantage functions in other ways than employing a simple baseline. For example, the value function can be applied to directly smooth the reinforcement signal obtained from a series of trajectories. The <a href="https://arxiv.org/abs/1506.02438">Generalized Advantage Estimate</a> (GAE), introduced by John Schulman in 2016 does just this. The GAE formulation allows for an interpolation between pure TD learning and pure Monte-Carlo sampling using a lambda parameter. By setting lambda to 0, the algorithm reduces to TD learning, while setting it to 1 produces Monte-Carlo sampling. Values in-between (particularly those in the 0.9 to 0.999 range) produce better empirical performance by trading off the bias of <em>V(s)</em> with the variance of the trajectory.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/925/1*RwKSm0KeX1Vkm3-ew17Zeg.png" /><figcaption>Generalized Advantage Estimate under two edge cases which reduce to: TD Learning (<strong>Above</strong>), and MC Sampling (<strong>Below</strong>).</figcaption></figure><p><strong>3. Value-Function Bootstrapping</strong></p><p>Outside of calculating an advantage function, the bias-variance trade-off presents itself when deciding what to do at the end of a trajectory when learning. Instead of waiting for an entire episode to complete before collecting a trajectory of experience, modern RL algorithms often break experience batches down into smaller sub-trajectories, and use a value-estimate to bootstrap the Monte-Carlo signal when that trajectory doesn’t end with the termination of the episode. By using a bootstrap signal, that estimate can contain information about <em>the rewards the agent might have gotten, if it continued going to the end of the episode</em>. It is essentially a guess about how the episode will turn out from that point onward. Take again our example of the corridor. If we are using a time horizon for our trajectories that ends halfway through the corridor, and if our value estimate reflects the fact that there is a rewarding state at the end, we will be able to assign value to the early part of the corridor, even though the agent didn’t experience the reward. As one might expect, the longer the trajectory length we use, the less frequently value estimates are used for bootstrapping, and thus the greater the variance (and lower the bias). In contrast, using short trajectories means relying more on the value estimate, creating a more biased reinforcement signal. By deciding how long the trajectory needs to be before cutting it off and bootstrapping it, we can propagate the reward signal in a more efficient way, but only if we get the balance right.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zSWwqPzzptvquHrTppZqYg.png" /><figcaption>Arrow corresponds to agent trajectory. <strong>Above</strong>: The value estimate at time step 3 is used to bootstrap trajectory value estimate. <strong>Below</strong>: no bootstrapping is used, and no value is assumed for these states.</figcaption></figure><h4>How to approach the trade-off</h4><p>Say you have some environment you’d like to have an agent learn to perform a task within (for example, an environment made using <a href="https://github.com/Unity-Technologies/ml-agents">Unity ML-Agents</a>). How do you decide how to control the <em>GAE lambda</em> and/or <em>trajectory time horizon</em>? The outcome of setting these hyperparameters in various ways often depends on the task, and come down to a couple of factors:</p><ul><li><strong>How well can your value estimator (using function approximation) capture the “true” reward structure of the environment?</strong> If it is possible for the value estimate to be reflective of the actual future expected reward, then the bias of the reinforcement signal is low, and using it to update the policy will lead to stable reward-increasing improvement. One simple proxy for the quality of the value estimate can be the loss when updating the model responsible for V(s)/Q(s, a). If the loss decreases to (near) zero, then the value estimate is reflective of the rewards the agent is receiving from the environment. More often that not however, this won’t be the case. There are a number of reasons a value estimate may be biased. One simple reason comes back to problems of supervised learning, where an under-capacity model being used to learn <em>V(s)</em> will likely produce a biased estimate. Increasing the capacity of the model might help here, as well as providing a richer state representation. The other problem is that the “true” reward structure is always a moving target. As an agent improves its policy, then the expected rewards should increase as well. This causes the unintuitive experience of the loss function increasing, even though the agent’s policy is more successful.</li><li><strong>How stochastic are the rewards experienced in the environment?</strong> This deals directly with the variance piece of the equation. Environments with more uncertain outcomes (due to multi-agent dynamics, stochasticity in policies and environment, and other factors) will result in less predictable rewards, and hence greater variance in the reward signal. As mentioned above, the traditional fix for this is to simply use larger batches of data during each step of stochastic gradient descent. This has a downside however. It is known that at least for supervised learning problems, large batches of gradient descent are much less efficient than smaller batches. It is also difficult to fit large batches into the memory of a system, especially when images of any considerable size are being used as observations. This approach is also the most data-intensive, as large batches require more interaction with the environment.</li></ul><p>Ultimately, correctly balancing the trade-off comes down to a few things: gaining an intuition for the kind of problem under consideration, and knowing what hyperparameters for any given algorithm correspond to what changes in the learning process. In the case of an algorithm like PPO, this corresponds to the <em>discount factor, GAE lambda, and bootstrapping time horizon</em>. Below are a few guidelines which may be helpful:</p><ul><li><strong>Discount Factor:</strong> Ensure that this captures how far ahead agents should be predicting rewards. For environments where agents need to think thousands of steps into the future, something like 0.999 might be needed. In cases where agents only need to think a few steps into the future, then something closer to 0.9 might be all that is needed.</li><li><strong>GAE Lambda</strong>: When using the Generalized Advantage Estimate, the lambda parameter will control the trade-off between bias and variance. While it is typically kept within the high 0.95–0.99 range, this depends on the quality of the value estimate V(s) being used, and more accurate V(s) can allow for greater reliance on it when calculating the Advantage.</li><li><strong>Time-Horizon</strong>: The “best” time horizon is determined primarily by the reward structure of the environment itself. The time-horizon should be long enough to allow the agent to likely receive some meaningful reward within it, but not too long as to allow large amounts of variance into the signal. In cases where the environment has high-variance in the expected rewards, utilizing a smaller time-horizon will allow for a more consistent learning signal in spite of that stochasticity.</li></ul><p>With all the tweaking and tuning that often goes into the process, it can sometimes feel overwhelming, and like black magic, but hopefully, the information presented above can help contribute, even in a small way, to ensure that Deep Reinforcement Learning is a little more interpretable to those practicing it.</p><p><strong>If you have questions about the bias-variance trade-off in RL, or if you are an RL researcher and have additional insight (or corrections) to share, please feel free to comment below!</strong></p><p><em>Thanks to </em><a href="https://medium.com/u/ab956b47915a"><em>Marwan &#39;Moe&#39; Mattar</em></a><em> for the helpful feedback when reviewing a draft of this post.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=79cf1e83d565" width="1" height="1" alt=""><hr><p><a href="https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565">Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning</a> was originally published in <a href="https://blog.mlreview.com">ML Review</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>