<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1909.08191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.02649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.11909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.12450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.15670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.12142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.11873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.04083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.08868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.05985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.14074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.14904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.15829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.03699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.07819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.08176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.03471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.05800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1906.11898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.02849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17145" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01370">
<title>Class Interference of Deep Neural Networks. (arXiv:2211.01370v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01370</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing and telling similar objects apart is even hard for human beings.
In this paper, we show that there is a phenomenon of class interference with
all deep neural networks. Class interference represents the learning difficulty
in data, and it constitutes the largest percentage of generalization errors by
deep networks. To understand class interference, we propose cross-class tests,
class ego directions and interference models. We show how to use these
definitions to study minima flatness and class interference of a trained model.
We also show how to detect class interference during training through label
dancing pattern and class dancing notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_D/0/1/0/all/0/1&quot;&gt;Dongcui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hengshuai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bei Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01373">
<title>Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators. (arXiv:2211.01373v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2211.01373</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior knowledge about the imaging physics provides a mechanistic forward
operator that plays an important role in image reconstruction, although myriad
sources of possible errors in the operator could negatively impact the
reconstruction solutions. In this work, we propose to embed the traditional
mechanistic forward operator inside a neural function, and focus on modeling
and correcting its unknown errors in an interpretable manner. This is achieved
by a conditional generative model that transforms a given mechanistic operator
with unknown errors, arising from a latent space of self-organizing clusters of
potential sources of error generation. Once learned, the generative model can
be used in place of a fixed forward operator in any traditional
optimization-based reconstruction process where, together with the inverse
solution, the error in prior mechanistic forward operator can be minimized and
the potential source of error uncovered. We apply the presented method to the
reconstruction of heart electrical potential from body surface potential. In
controlled simulation experiments and in-vivo real data experiments, we
demonstrate that the presented method allowed reduction of errors in the
physics-based forward operator and thereby delivered inverse reconstruction of
heart-surface potential with increased accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toloubidokhti_M/0/1/0/all/0/1&quot;&gt;Maryam Toloubidokhti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Nilesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gyawali_P/0/1/0/all/0/1&quot;&gt;Prashnna K. Gyawali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zenger_B/0/1/0/all/0/1&quot;&gt;Brian Zenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Good_W/0/1/0/all/0/1&quot;&gt;Wilson W. Good&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+MacLeod_R/0/1/0/all/0/1&quot;&gt;Rob S. MacLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01407">
<title>On the Informativeness of Supervision Signals. (arXiv:2211.01407v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01407</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning transferable representations by training a classifier is a
well-established technique in deep learning (e.g., ImageNet pretraining), but
it remains an open theoretical question why this kind of task-specific
pre-training should result in &apos;&apos;good&apos;&apos; representations that actually capture
the underlying structure of the data. We conduct an information-theoretic
analysis of several commonly-used supervision signals from contrastive learning
and classification to determine how they contribute to representation learning
performance and how the dynamics of learning are affected by training
parameters such as the number of labels, classes, and dimensions in the
training dataset. We validate these results empirically in a series of
simulations and conduct a cost-benefit analysis to establish a tradeoff curve
that enables users to optimize the cost of supervising representation learning
on their own datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1&quot;&gt;Ilia Sucholutsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1&quot;&gt;Raja Marjieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1&quot;&gt;Nori Jacoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01413">
<title>XAI-Increment: A Novel Approach Leveraging LIME Explanations for Improved Incremental Learning. (arXiv:2211.01413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01413</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability of neural network prediction is essential to understand
feature importance and gain interpretable insight into neural network
performance. In this work, model explanations are fed back to the feed-forward
training to help the model generalize better. To this extent, a custom weighted
loss where the weights are generated by considering the Euclidean distances
between true LIME (Local Interpretable Model-Agnostic Explanations)
explanations and model-predicted LIME explanations is proposed. Also, in
practical training scenarios, developing a solution that can help the model
learn sequentially without losing information on previous data distribution is
imperative due to the unavailability of all the training data at once. Thus,
the framework known as XAI-Increment incorporates the custom weighted loss
developed with elastic weight consolidation (EWC), to maintain performance in
sequential testing sets. Finally, the training procedure involving the custom
weighted loss shows around 1% accuracy improvement compared to the traditional
loss based training for the keyword spotting task on the Google Speech Commands
dataset and also shows low loss of information when coupled with EWC in the
incremental learning setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1&quot;&gt;Arnab Neelim Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_N/0/1/0/all/0/1&quot;&gt;Niall Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Anand Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1&quot;&gt;Ashutosh Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1&quot;&gt;Avik Santra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01427">
<title>TextCraft: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Text. (arXiv:2211.01427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01427</link>
<description rdf:parseType="Literal">&lt;p&gt;Language is one of the primary means by which we describe the 3D world around
us. While rapid progress has been made in text-to-2D-image synthesis, similar
progress in text-to-3D-shape synthesis has been hindered by the lack of paired
(text, shape) data. Moreover, extant methods for text-to-shape generation have
limited shape diversity and fidelity. We introduce TextCraft, a method to
address these limitations by producing high-fidelity and diverse 3D shapes
without the need for (text, shape) pairs for training. TextCraft achieves this
by using CLIP and using a multi-resolution approach by first generating in a
low-dimensional latent space and then upscaling to a higher resolution,
improving the fidelity of the generated shape. To improve shape diversity, we
use a discrete latent space which is modelled using a bidirectional transformer
conditioned on the interchangeable image-text embedding space induced by CLIP.
Moreover, we present a novel variant of classifier-free guidance, which further
improves the accuracy-diversity trade-off. Finally, we perform extensive
experiments that demonstrate that TextCraft outperforms state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1&quot;&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Rao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1&quot;&gt;Vivian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1&quot;&gt;Hooman Shayani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Hosein Khasahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1&quot;&gt;Srinath Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1&quot;&gt;Daniel Ritchie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01441">
<title>eXplainable AI for Quantum Machine Learning. (arXiv:2211.01441v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2211.01441</link>
<description rdf:parseType="Literal">&lt;p&gt;Parametrized Quantum Circuits (PQCs) enable a novel method for machine
learning (ML). However, from a computational point of view they present a
challenge to existing eXplainable AI (xAI) methods. On the one hand,
measurements on quantum circuits introduce probabilistic errors which impact
the convergence of these methods. On the other hand, the phase space of a
quantum circuit expands exponentially with the number of qubits, complicating
efforts to execute xAI methods in polynomial time. In this paper we will
discuss the performance of established xAI methods, such as Baseline SHAP and
Integrated Gradients. Using the internal mechanics of PQCs we study ways to
speed up their computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Steinmuller_P/0/1/0/all/0/1&quot;&gt;Patrick Steinm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schulz_T/0/1/0/all/0/1&quot;&gt;Tobias Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Graf_F/0/1/0/all/0/1&quot;&gt;Ferdinand Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Herr_D/0/1/0/all/0/1&quot;&gt;Daniel Herr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01471">
<title>Dual Generator Offline Reinforcement Learning. (arXiv:2211.01471v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01471</link>
<description rdf:parseType="Literal">&lt;p&gt;In offline RL, constraining the learned policy to remain close to the data is
essential to prevent the policy from outputting out-of-distribution (OOD)
actions with erroneously overestimated values. In principle, generative
adversarial networks (GAN) can provide an elegant solution to do so, with the
discriminator directly providing a probability that quantifies distributional
shift. However, in practice, GAN-based offline RL methods have not performed as
well as alternative approaches, perhaps because the generator is trained to
both fool the discriminator and maximize return -- two objectives that can be
at odds with each other. In this paper, we show that the issue of conflicting
objectives can be resolved by training two generators: one that maximizes
return, with the other capturing the ``remainder&apos;&apos; of the data distribution in
the offline dataset, such that the mixture of the two is close to the behavior
policy. We show that not only does having two generators enable an effective
GAN-based offline RL method, but also approximates a support constraint, where
the policy does not need to match the entire data distribution, but only the
slice of the data that leads to high long term performance. We name our method
DASCO, for Dual-Generator Adversarial Support Constrained Offline RL. On
benchmark tasks that require learning from sub-optimal data, DASCO
significantly outperforms prior methods that enforce distribution constraint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1&quot;&gt;Yevgen Chebotar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01482">
<title>RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01482</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing metrics for evaluating the quality of automatically generated
questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
predicted questions, providing a high score when there is a considerable
lexical overlap or semantic similarity between the candidate and the reference
questions. This approach has two major shortcomings. First, we need expensive
human-provided reference questions. Second, it penalises valid questions that
may not have high lexical or semantic similarity to the reference questions. In
this paper, we propose a new metric, RQUGE, based on the answerability of the
candidate question given the context. The metric consists of a
question-answering and a span scorer module, in which we use pre-trained models
from the existing literature, and therefore, our metric can be used without
further training. We show that RQUGE has a higher correlation with human
judgment without relying on the reference question. RQUGE is shown to be
significantly more robust to several adversarial corruptions. Additionally, we
illustrate that we can significantly improve the performance of QA models on
out-of-domain datasets by fine-tuning on the synthetic data generated by a
question generation model and re-ranked by RQUGE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1&quot;&gt;Alireza Mohammadshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1&quot;&gt;Thomas Scialom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1&quot;&gt;Majid Yazdani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanki_P/0/1/0/all/0/1&quot;&gt;Pouya Yanki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1&quot;&gt;Angela Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1&quot;&gt;James Henderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1&quot;&gt;Marzieh Saeidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01496">
<title>Max Markov Chain. (arXiv:2211.01496v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01496</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Max Markov Chain (MMC), a novel representation
for a useful subset of High-order Markov Chains (HMCs) with sparse correlations
among the states. MMC is parsimony while retaining the expressiveness of HMCs.
Even though parameter optimization is generally intractable as with HMC
approximate models, it has an analytical solution, better sample efficiency,
and the desired spatial and computational advantages over HMCs and approximate
HMCs. Simultaneously, efficient approximate solutions exist for this type of
chains as we show empirically, which allow MMCs to scale to large domains where
HMCs and approximate HMCs would struggle to perform. We compare MMC with HMC,
first-order Markov chain, and an approximate HMC model in synthetic domains
with various data types to demonstrate that MMC is a valuable alternative for
modeling stochastic processes and has many potential applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucklew_M/0/1/0/all/0/1&quot;&gt;Mitchell Bucklew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01500">
<title>Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity. (arXiv:2211.01500v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01500</link>
<description rdf:parseType="Literal">&lt;p&gt;A simple gripper can solve more complex manipulation tasks if it can utilize
the external environment such as pushing the object against the table or a
vertical wall, known as &quot;Extrinsic Dexterity.&quot; Previous work in extrinsic
dexterity usually has careful assumptions about contacts which impose
restrictions on robot design, robot motions, and the variations of the physical
parameters. In this work, we develop a system based on reinforcement learning
(RL) to address these limitations. We study the task of &quot;Occluded Grasping&quot;
which aims to grasp the object in configurations that are initially occluded;
the robot needs to move the object into a configuration from which these grasps
can be achieved. We present a system with model-free RL that successfully
achieves this task using a simple gripper with extrinsic dexterity. The policy
learns emergent behaviors of pushing the object against the wall to rotate and
then grasp it without additional reward terms on extrinsic dexterity. We
discuss important components of the system including the design of the RL
problem, multi-grasp training and selection, and policy generalization with
automatic curriculum. Most importantly, the policy trained in simulation is
zero-shot transferred to a physical robot. It demonstrates dynamic and
contact-rich motions with a simple gripper that generalizes across objects with
various size, density, surface friction, and shape with a 78% success rate.
Videos can be found at https://sites.google.com/view/grasp-ungraspable/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01515">
<title>MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2211.01515</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Multiscale Audio Spectrogram Transformer (MAST) for audio
classification, which brings the concept of multiscale feature hierarchies to
the Audio Spectrogram Transformer (AST). Given an input audio spectrogram we
first patchify and project it into an initial temporal resolution and embedding
dimension, post which the multiple stages in MAST progressively expand the
embedding dimension while reducing the temporal resolution of the input. We use
a pyramid structure that allows early layers of MAST operating at a high
temporal resolution but low embedding space to model simple low-level acoustic
information and deeper temporally coarse layers to model high-level acoustic
information with high-dimensional embeddings. We also extend our approach to
present a new Self-Supervised Learning (SSL) method called SS-MAST, which
calculates a symmetric contrastive loss between latent representations from a
student and a teacher encoder. In practice, MAST significantly outperforms AST
by an average accuracy of 3.4% across 8 speech and non-speech tasks from the
LAPE Benchmark. Moreover, SS-MAST achieves an absolute average improvement of
2.6% over SSAST for both AST and MAST encoders. We make all our codes available
on GitHub at the time of publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1&quot;&gt;Ashish Seth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1&quot;&gt;S. Umesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01519">
<title>SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2211.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new Self-Supervised Learning (SSL) approach to pre-train
encoders on unlabeled audio data that reduces the need for large amounts of
labeled data for audio and speech classification. Our primary aim is to learn
audio representations that can generalize across a large variety of speech and
non-speech tasks in a low-resource un-labeled audio pre-training setting.
Inspired by the recent success of clustering and contrasting learning paradigms
for SSL-based speech representation learning, we propose SLICER (Symmetrical
Learning of Instance and Cluster-level Efficient Representations), which brings
together the best of both clustering and contrasting learning paradigms. We use
a symmetric loss between latent representations from student and teacher
encoders and simultaneously solve instance and cluster-level contrastive
learning tasks. We obtain cluster representations online by just projecting the
input spectrogram into an output subspace with dimensions equal to the number
of clusters. In addition, we propose a novel mel-spectrogram augmentation
procedure, k-mix, based on mixup, which does not require labels and aids
unsupervised representation learning for audio. Overall, SLICER achieves
state-of-the-art results on the LAPE Benchmark \cite{9868132}, significantly
outperforming DeLoRes-M and other prior approaches, which are pre-trained on
$10\times$ larger of unsupervised data. We will make all our codes available on
GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1&quot;&gt;Ashish Seth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1&quot;&gt;S. Umesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01527">
<title>Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments. (arXiv:2211.01527v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01527</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach for autonomous sensor control for information
gathering under partially observable, dynamic and sparsely sampled
environments. We consider the problem of controlling a sensor that makes
partial observations in some space of interest such that it maximizes
information about entities present in that space. We describe our approach for
the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to
search for and track unknown, dynamic signals in the environment. To this end,
we develop and demonstrate enhancements of the Deep Anticipatory Network (DAN)
Reinforcement Learning (RL) framework that uses prediction and information-gain
rewards to learn information-maximization policies in reward-sparse
environments. We also extend this problem to situations in which taking samples
from the actual RF spectrum/field is limited and expensive, and propose a
model-based version of the original RL algorithm that fine-tunes the controller
using a model of the environment that is iteratively improved from limited
samples taken from the RF field. Our approach was thoroughly validated by
testing against baseline expert-designed controllers in simulated RF
environments of different complexity, using different rewards schemes and
evaluation metrics. The results show that our system outperforms the standard
DAN architecture and is more flexible and robust than several hand-coded
agents. We also show that our approach is adaptable to non-stationary
environments where the agent has to learn to adapt to changes from the emitting
sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burns_J/0/1/0/all/0/1&quot;&gt;J. Brian Burns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_A/0/1/0/all/0/1&quot;&gt;Aravind Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sequeira_P/0/1/0/all/0/1&quot;&gt;Pedro Sequeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadhu_V/0/1/0/all/0/1&quot;&gt;Vidyasagar Sadhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01528">
<title>Fair and Optimal Classification via Transports to Wasserstein-Barycenter. (arXiv:2211.01528v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01528</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in automated decision-making systems has gained increasing attention
as their applications expand to real-world high-stakes domains. To facilitate
the design of fair ML systems, it is essential to understand the potential
trade-offs between fairness and predictive power, and the construction of the
optimal predictor under a given fairness constraint. In this paper, for general
classification problems under the group fairness criterion of demographic
parity (DP), we precisely characterize the trade-off between DP and
classification accuracy, referred to as the minimum cost of fairness. Our
insight comes from the key observation that finding the optimal fair classifier
is equivalent to solving a Wasserstein-barycenter problem under $\ell_1$-norm
restricted to the vertices of the probability simplex. Inspired by our
characterization, we provide a construction of an optimal fair classifier
achieving this minimum cost via the composition of the Bayes regressor and
optimal transports from its output distributions to the barycenter. Our
construction naturally leads to an algorithm for post-processing any
pre-trained predictor to satisfy DP fairness, complemented with finite sample
guarantees. Experiments on real-world datasets verify and demonstrate the
effectiveness of our approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1&quot;&gt;Ruicheng Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Han Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01535">
<title>Reliable Malware Analysis and Detection using Topology Data Analysis. (arXiv:2211.01535v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.01535</link>
<description rdf:parseType="Literal">&lt;p&gt;Increasingly, malwares are becoming complex and they are spreading on
networks targeting different infrastructures and personal-end devices to
collect, modify, and destroy victim information. Malware behaviors are
polymorphic, metamorphic, persistent, able to hide to bypass detectors and
adapt to new environments, and even leverage machine learning techniques to
better damage targets. Thus, it makes them difficult to analyze and detect with
traditional endpoint detection and response, intrusion detection and prevention
systems. To defend against malwares, recent work has proposed different
techniques based on signatures and machine learning. In this paper, we propose
to use an algebraic topological approach called topological-based data analysis
(TDA) to efficiently analyze and detect complex malware patterns. Next, we
compare the different TDA techniques (i.e., persistence homology, tomato, TDA
Mapper) and existing techniques (i.e., PCA, UMAP, t-SNE) using different
classifiers including random forest, decision tree, xgboost, and lightgbm. We
also propose some recommendations to deploy the best-identified models for
malware detection at scale. Results show that TDA Mapper (combined with PCA) is
better for clustering and for identifying hidden relationships between malware
clusters compared to PCA. Persistent diagrams are better to identify
overlapping malware clusters with low execution time compared to UMAP and
t-SNE. For malware detection, malware analysts can use Random Forest and
Decision Tree with t-SNE and Persistent Diagram to achieve better performance
and robustness on noised data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tidjon_L/0/1/0/all/0/1&quot;&gt;Lionel Nganyewou Tidjon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01548">
<title>INGREX: An Interactive Explanation Framework for Graph Neural Networks. (arXiv:2211.01548v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01548</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are widely used in many modern applications,
necessitating explanations for their decisions. However, the complexity of GNNs
makes it difficult to explain predictions. Even though several methods have
been proposed lately, they can only provide simple and static explanations,
which are difficult for users to understand in many scenarios. Therefore, we
introduce INGREX, an interactive explanation framework for GNNs designed to aid
users in comprehending model predictions. Our framework is implemented based on
multiple explanation algorithms and advanced libraries. We demonstrate our
framework in three scenarios covering common demands for GNN explanations to
present its effectiveness and helpfulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Tien-Cuong Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Van-Duc Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wen-Syan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sang Kyun Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01551">
<title>Crime Prediction using Machine Learning with a Novel Crime Dataset. (arXiv:2211.01551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01551</link>
<description rdf:parseType="Literal">&lt;p&gt;Crime is an unlawful act that carries legal repercussions. Bangladesh has a
high crime rate due to poverty, population growth, and many other
socio-economic issues. For law enforcement agencies, understanding crime
patterns is essential for preventing future criminal activity. For this
purpose, these agencies need structured crime database. This paper introduces a
novel crime dataset that contains temporal, geographic, weather, and
demographic data about 6574 crime incidents of Bangladesh. We manually gather
crime news articles of a seven year time span from a daily newspaper archive.
We extract basic features from these raw text. Using these basic features, we
then consult standard service-providers of geo-location and weather data in
order to garner these information related to the collected crime incidents.
Furthermore, we collect demographic information from Bangladesh National Census
data. All these information are combined that results in a standard machine
learning dataset. Together, 36 features are engineered for the crime prediction
task. Five supervised machine learning classification algorithms are then
evaluated on this newly built dataset and satisfactory results are achieved. We
also conduct exploratory analysis on various aspects the dataset. This dataset
is expected to serve as the foundation for crime incidence prediction systems
for Bangladesh and other countries. The findings of this study will help law
enforcement agencies to forecast and contain crime as well as to ensure optimal
resource allocation for crime patrol and prevention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shohan_F/0/1/0/all/0/1&quot;&gt;Faisal Tareque Shohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akash_A/0/1/0/all/0/1&quot;&gt;Abu Ubaida Akash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Muhammad Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mohammad Shafiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01559">
<title>The ProfessionAl Go annotation datasEt (PAGE). (arXiv:2211.01559v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01559</link>
<description rdf:parseType="Literal">&lt;p&gt;The game of Go has been highly under-researched due to the lack of game
records and analysis tools. In recent years, the increasing number of
professional competitions and the advent of AlphaZero-based algorithms provide
an excellent opportunity for analyzing human Go games on a large scale. In this
paper, we present the ProfessionAl Go annotation datasEt (PAGE), containing
98,525 games played by 2,007 professional players and spans over 70 years. The
dataset includes rich AI analysis results for each move. Moreover, PAGE
provides detailed metadata for every player and game after manual cleaning and
labeling. Beyond the preliminary analysis of the dataset, we provide sample
tasks that benefit from our dataset to demonstrate the potential application of
PAGE in multiple research directions. To the best of our knowledge, PAGE is the
first dataset with extensive annotation in the game of Go. This work is an
extended version of [1] where we perform a more detailed description, analysis,
and application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Danni Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyue Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01568">
<title>Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01568</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are now part of a powerful new paradigm in machine
learning. These models learn a wide range of capabilities from training on
large unsupervised text corpora. In many applications, these capabilities are
then fine-tuned through additional training on specialized data to improve
performance in that setting. In this paper, we augment these models with an
epinet: a small additional network architecture that helps to estimate model
uncertainty and form an epistemic neural network (ENN). ENNs are neural
networks that can know what they don&apos;t know. We show that, using an epinet to
prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same
performance while using 2x less data. We also investigate performance in
synthetic neural network generative models designed to build understanding. In
each setting, using an epinet outperforms heuristic active learning schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Asghari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1&quot;&gt;Nat McAleese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1&quot;&gt;John Aslanides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1&quot;&gt;Geoffrey Irving&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01572">
<title>FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01572</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is an emerging learning paradigm where multiple clients
collaboratively train a machine learning model in a privacy-preserving manner.
Personalized federated learning extends this paradigm to overcome heterogeneity
across clients by learning personalized models. Recently, there have been some
initial attempts to apply Transformers to federated learning. However, the
impacts of federated learning algorithms on self-attention have not yet been
studied. This paper investigates this relationship and reveals that federated
averaging algorithms actually have a negative impact on self-attention where
there is data heterogeneity. These impacts limit the capabilities of the
Transformer model in federated learning settings. Based on this, we propose
FedTP, a novel Transformer-based federated learning framework that learns
personalized self-attention for each client while aggregating the other
parameters among the clients. Instead of using a vanilla personalization
mechanism that maintains personalized self-attention layers of each client
locally, we develop a learn-to-personalize mechanism to further encourage the
cooperation among clients and to increase the scablability and generalization
of FedTP. Specifically, the learn-to-personalize is realized by learning a
hypernetwork on the server that outputs the personalized projection matrices of
self-attention layers to generate client-wise queries, keys and values.
Furthermore, we present the generalization bound for FedTP with the
learn-to-personalize mechanism. Notably, FedTP offers a convenient environment
for performing a range of image and language tasks using the same federated
network architecture - all of which benefit from Transformer personalization.
Extensive experiments verify that FedTP with the learn-to-personalize mechanism
yields state-of-the-art performance in non-IID scenarios. Our code is available
online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiangnan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Weiping Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Ye Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01576">
<title>Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning. (arXiv:2211.01576v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01576</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots planning long-horizon behavior in complex environments must be able to
quickly reason about the impact of the environment&apos;s geometry on what plans are
feasible, i.e., whether there exist action parameter values that satisfy all
constraints on a candidate plan. In tasks involving articulated and movable
obstacles, typical Task and Motion Planning (TAMP) algorithms spend most of
their runtime attempting to solve unsolvable constraint satisfaction problems
imposed by infeasible plan skeletons. We developed a novel Transformer-based
architecture, PIGINet, that predicts plan feasibility based on the initial
state, goal, and candidate plans, fusing image and text embeddings with state
features. The model sorts the plan skeletons produced by a TAMP planner
according to the predicted satisfiability likelihoods. We evaluate the runtime
of our learning-enabled TAMP algorithm on several distributions of kitchen
rearrangement problems, comparing its performance to that of non-learning
baselines and algorithm ablations. Our experiments show that PIGINet
substantially improves planning efficiency, cutting down runtime by 80% on
average on pick-and-place problems with articulated obstacles. It also achieves
zero-shot generalization to problems with unseen object categories thanks to
its visual encoding of objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhutian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrett_C/0/1/0/all/0/1&quot;&gt;Caelan Reed Garrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1&quot;&gt;Dieter Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01600">
<title>nerf2nerf: Pairwise Registration of Neural Radiance Fields. (arXiv:2211.01600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01600</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a &apos;&apos;surface field&apos;&apos; -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goli_L/0/1/0/all/0/1&quot;&gt;Lily Goli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1&quot;&gt;Daniel Rebain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1&quot;&gt;Sara Sabour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Animesh Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01604">
<title>Meta-PDE: Learning to Solve PDEs Quickly Without a Mesh. (arXiv:2211.01604v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial differential equations (PDEs) are often computationally challenging
to solve, and in many settings many related PDEs must be be solved either at
every timestep or for a variety of candidate boundary conditions, parameters,
or geometric domains. We present a meta-learning based method which learns to
rapidly solve problems from a distribution of related PDEs. We use
meta-learning (MAML and LEAP) to identify initializations for a neural network
representation of the PDE solution such that a residual of the PDE can be
quickly minimized on a novel task. We apply our meta-solving approach to a
nonlinear Poisson&apos;s equation, 1D Burgers&apos; equation, and hyperelasticity
equations with varying parameters, geometries, and boundary conditions. The
resulting Meta-PDE method finds qualitatively accurate solutions to most
problems within a few gradient steps; for the nonlinear Poisson and
hyper-elasticity equation this results in an intermediate accuracy
approximation up to an order of magnitude faster than a baseline finite element
analysis (FEA) solver with equivalent accuracy. In comparison to other learned
solvers and surrogate models, this meta-learning approach can be trained
without supervision from expensive ground-truth data, does not require a mesh,
and can even be used when the geometry and topology varies between tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tian Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beatson_A/0/1/0/all/0/1&quot;&gt;Alex Beatson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oktay_D/0/1/0/all/0/1&quot;&gt;Deniz Oktay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1&quot;&gt;Nick McGreivy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1&quot;&gt;Ryan P. Adams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01634">
<title>P4P: Conflict-Aware Motion Prediction for Planning in Autonomous Driving. (arXiv:2211.01634v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01634</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is crucial in enabling safe motion planning for autonomous
vehicles in interactive scenarios. It allows the planner to identify potential
conflicts with other traffic agents and generate safe plans. Existing motion
predictors often focus on reducing prediction errors, yet it remains an open
question on how well they help identify the conflicts for the planner. In this
paper, we evaluate state-of-the-art predictors through novel conflict-related
metrics, such as the success rate of identifying conflicts. Surprisingly, the
predictors suffer from a low success rate and thus lead to a large percentage
of collisions when we test the prediction-planning system in an interactive
simulator. To fill the gap, we propose a simple but effective alternative that
combines a physics-based trajectory generator and a learning-based relation
predictor to identify conflicts and infer conflict relations. We demonstrate
that our predictor, P4P, achieves superior performance over existing
learning-based predictors in realistic interactive driving scenarios from Waymo
Open Motion Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1&quot;&gt;Brian C. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01635">
<title>Revisiting Grammatical Error Correction Evaluation and Beyond. (arXiv:2211.01635v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01635</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1&quot;&gt;Peiyuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuebo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01642">
<title>Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively. (arXiv:2211.01642v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01642</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained language models have achieved impressive results on a
wide range of downstream tasks recently. However, fine-tuning an extremely
large-scale pre-trained language model on limited target datasets is often
plagued by overfitting and representation degradation. In this paper, we
propose a Dynamic Parameter Selection (DPS) algorithm for the large-scale
pre-trained models during fine-tuning, which adaptively selects a more
promising subnetwork to perform staging updates based on gradients of
back-propagation. Experiments on the GLUE benchmark show that DPS outperforms
previous fine-tuning methods in terms of overall performance and stability, and
consistently achieves better results with variable pre-trained language models.
In addition, DPS brings a large magnitude of improvement in out-of-domain
transferring experiments and low-resource scenarios, which shows that it can
maintain stable general contextual features and reduce the representation
collapse. We release our code at https://github.com/ZhangHaojie077/DPS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haojie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongjin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01644">
<title>StereoPose: Category-Level 6D Transparent Object Pose Estimation from Stereo Images via Back-View NOCS. (arXiv:2211.01644v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01644</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing methods for category-level pose estimation rely on object point
clouds. However, when considering transparent objects, depth cameras are
usually not able to capture meaningful data, resulting in point clouds with
severe artifacts. Without a high-quality point cloud, existing methods are not
applicable to challenging transparent objects. To tackle this problem, we
present StereoPose, a novel stereo image framework for category-level object
pose estimation, ideally suited for transparent objects. For a robust
estimation from pure stereo images, we develop a pipeline that decouples
category-level pose estimation into object size estimation, initial pose
estimation, and pose refinement. StereoPose then estimates object pose based on
representation in the normalized object coordinate space~(NOCS). To address the
issue of image content aliasing, we further define a back-view NOCS map for the
transparent object. The back-view NOCS aims to reduce the network learning
ambiguity caused by content aliasing, and leverage informative cues on the back
of the transparent object for more accurate pose estimation. To further improve
the performance of the stereo framework, StereoPose is equipped with a parallax
attention module for stereo feature fusion and an epipolar loss for improving
the stereo-view consistency of network predictions. Extensive experiments on
the public TOD dataset demonstrate the superiority of the proposed StereoPose
framework for category-level 6D transparent object pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Stephen James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_C/0/1/0/all/0/1&quot;&gt;Congying Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01656">
<title>GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs). (arXiv:2211.01656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01656</link>
<description rdf:parseType="Literal">&lt;p&gt;TREs are widely, and increasingly used to support statistical analysis of
sensitive data across a range of sectors (e.g., health, police, tax and
education) as they enable secure and transparent research whilst protecting
data confidentiality. There is an increasing desire from academia and industry
to train AI models in TREs. The field of AI is developing quickly with
applications including spotting human errors, streamlining processes, task
automation and decision support. These complex AI models require more
information to describe and reproduce, increasing the possibility that
sensitive personal data can be inferred from such descriptions. TREs do not
have mature processes and controls against these risks. This is a complex
topic, and it is unreasonable to expect all TREs to be aware of all risks or
that TRE researchers have addressed these risks in AI-specific training.
GRAIMATTER has developed a draft set of usable recommendations for TREs to
guard against the additional risks when disclosing trained AI models from TREs.
The development of these recommendations has been funded by the GRAIMATTER UKRI
DARE UK sprint research project. This version of our recommendations was
published at the end of the project in September 2022. During the course of the
project, we have identified many areas for future investigations to expand and
test these recommendations in practice. Therefore, we expect that this document
will evolve over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jefferson_E/0/1/0/all/0/1&quot;&gt;Emily Jefferson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liley_J/0/1/0/all/0/1&quot;&gt;James Liley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malone_M/0/1/0/all/0/1&quot;&gt;Maeve Malone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reel_S/0/1/0/all/0/1&quot;&gt;Smarti Reel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crespi_Boixader_A/0/1/0/all/0/1&quot;&gt;Alba Crespi-Boixader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerasidou_X/0/1/0/all/0/1&quot;&gt;Xaroula Kerasidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tava_F/0/1/0/all/0/1&quot;&gt;Francesco Tava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_A/0/1/0/all/0/1&quot;&gt;Andrew McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preen_R/0/1/0/all/0/1&quot;&gt;Richard Preen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanco_Justicia_A/0/1/0/all/0/1&quot;&gt;Alberto Blanco-Justicia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansouri_Benssassi_E/0/1/0/all/0/1&quot;&gt;Esma Mansouri-Benssassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domingo_Ferrer_J/0/1/0/all/0/1&quot;&gt;Josep Domingo-Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beggs_J/0/1/0/all/0/1&quot;&gt;Jillian Beggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuter_A/0/1/0/all/0/1&quot;&gt;Antony Chuter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1&quot;&gt;Christian Cole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritchie_F/0/1/0/all/0/1&quot;&gt;Felix Ritchie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daly_A/0/1/0/all/0/1&quot;&gt;Angela Daly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_S/0/1/0/all/0/1&quot;&gt;Simon Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jim Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01675">
<title>Spam Review Detection Using Deep Learning. (arXiv:2211.01675v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01675</link>
<description rdf:parseType="Literal">&lt;p&gt;A robust and reliable system of detecting spam reviews is a crying need in
todays world in order to purchase products without being cheated from online
sites. In many online sites, there are options for posting reviews, and thus
creating scopes for fake paid reviews or untruthful reviews. These concocted
reviews can mislead the general public and put them in a perplexity whether to
believe the review or not. Prominent machine learning techniques have been
introduced to solve the problem of spam review detection. The majority of
current research has concentrated on supervised learning methods, which require
labeled data - an inadequacy when it comes to online review. Our focus in this
article is to detect any deceptive text reviews. In order to achieve that we
have worked with both labeled and unlabeled data and proposed deep learning
methods for spam review detection which includes Multi-Layer Perceptron (MLP),
Convolutional Neural Network (CNN) and a variant of Recurrent Neural Network
(RNN) that is Long Short-Term Memory (LSTM). We have also applied some
traditional machine learning classifiers such as Nave Bayes (NB), K Nearest
Neighbor (KNN) and Support Vector Machine (SVM) to detect spam reviews and
finally, we have shown the performance comparison for both traditional and deep
learning classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1&quot;&gt;G. M. Shahariar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Swapnil Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_F/0/1/0/all/0/1&quot;&gt;Faiza Omar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1&quot;&gt;Faisal Muhammad Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1&quot;&gt;Samiha Binte Hassan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01676">
<title>Repeatable random permutation set. (arXiv:2211.01676v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01676</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on Dempster-Shafer evidence theory (DST), random permutation set (RPS)
is proposed by replacing combinatorial number with permutation number and
therefore incorporating order information. Besides, RPS could take DST as a
special case when all items occur in the same order. However, the repetition of
items is not allowed in RPS. To address this issue, we propose repeatable
random permutation set (R2PS) which takes the repetition of items into
consideration. The right and left junctional sum combination rules are proposed
and their properties including consistency, pseudo-Matthew effect and
associativity are researched. Based on these properties, a decision support
system application is simulated to show the effectiveness of R2PS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01698">
<title>Scaling up the self-optimization model by means of on-the-fly computation of weights. (arXiv:2211.01698v1 [nlin.AO])</title>
<link>http://arxiv.org/abs/2211.01698</link>
<description rdf:parseType="Literal">&lt;p&gt;The Self-Optimization (SO) model is a useful computational model for
investigating self-organization in &quot;soft&quot; Artificial life (ALife) as it has
been shown to be general enough to model various complex adaptive systems. So
far, existing work has been done on relatively small network sizes, precluding
the investigation of novel phenomena that might emerge from the complexity
arising from large numbers of nodes interacting in interconnected networks.
This work introduces a novel implementation of the SO model that scales as
$\mathcal{O}\left(N^{2}\right)$ with respect to the number of nodes $N$, and
demonstrates the applicability of the SO model to networks with system sizes
several orders of magnitude higher than previously was investigated. Removing
the prohibitive computational cost of the naive $\mathcal{O}\left(N^{3}\right)$
algorithm, our on-the-fly computation paves the way for investigating
substantially larger system sizes, allowing for more variety and complexity in
future studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Weber_N/0/1/0/all/0/1&quot;&gt;Natalya Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Koch_W/0/1/0/all/0/1&quot;&gt;Werner Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Froese_T/0/1/0/all/0/1&quot;&gt;Tom Froese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01730">
<title>Feedback is Good, Active Feedback is Better: Block Attention Active Feedback Codes. (arXiv:2211.01730v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2211.01730</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network (DNN)-assisted channel coding designs, such as
low-complexity neural decoders for existing codes, or end-to-end
neural-network-based auto-encoder designs are gaining interest recently due to
their improved performance and flexibility; particularly for communication
scenarios in which high-performing structured code designs do not exist.
Communication in the presence of feedback is one such communication scenario,
and practical code design for feedback channels has remained an open challenge
in coding theory for many decades. Recently, DNN-based designs have shown
impressive results in exploiting feedback. In particular, generalized block
attention feedback (GBAF) codes, which utilizes the popular transformer
architecture, achieved significant improvement in terms of the block error rate
(BLER) performance. However, previous works have focused mainly on passive
feedback, where the transmitter observes a noisy version of the signal at the
receiver. In this work, we show that GBAF codes can also be used for channels
with active feedback. We implement a pair of transformer architectures, at the
transmitter and the receiver, which interact with each other sequentially, and
achieve a new state-of-the-art BLER performance, especially in the low SNR
regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1&quot;&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghazanfari_A/0/1/0/all/0/1&quot;&gt;Amin Ghazanfari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1&quot;&gt;Alberto Perotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_B/0/1/0/all/0/1&quot;&gt;Branislav Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01736">
<title>Exploring the State-of-the-Art Language Modeling Methods and Data Augmentation Techniques for Multilingual Clause-Level Morphology. (arXiv:2211.01736v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01736</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the KUIS-AI NLP team&apos;s submission for the 1$^{st}$
Shared Task on Multilingual Clause-level Morphology (MRL2022). We present our
work on all three parts of the shared task: inflection, reinflection, and
analysis. We mainly explore two approaches: Transformer models in combination
with data augmentation, and exploiting the state-of-the-art language modeling
techniques for morphological analysis. Data augmentation leads a remarkable
performance improvement for most of the languages in the inflection task.
Prefix-tuning on pretrained mGPT model helps us to adapt reinflection and
analysis tasks in a low-data setting. Additionally, we used pipeline
architectures using publicly available open source lemmatization tools and
monolingual BERT-based morphological feature classifiers for reinflection and
analysis tasks, respectively. While Transformer architectures with data
augmentation and pipeline architectures achieved the best results for
inflection and reinflection tasks, pipelines and prefix-tuning on mGPT received
the highest results for the analysis task. Our methods achieved first place in
each of the three tasks and outperforms mT5-baseline with ~89\% for inflection,
~80\% for reinflection and ~12\% for analysis. Our code
https://github.com/emrecanacikgoz/mrl2022 is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acikgoz_E/0/1/0/all/0/1&quot;&gt;Emre Can Acikgoz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chubakov_T/0/1/0/all/0/1&quot;&gt;Tilek Chubakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kural_M/0/1/0/all/0/1&quot;&gt;M&amp;#xfc;ge Kural&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde G&amp;#xfc;l &amp;#x15e;ahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1&quot;&gt;Deniz Yuret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01745">
<title>Task Tree Retrieval for Robotic Cooking. (arXiv:2211.01745v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2211.01745</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotics is used to foster creativity. Humans can perform jobs in their
unique manner, depending on the circumstances. This situation applies to food
cooking. Robotic technology in the kitchen can speed up the process and reduce
its workload. However, the potential of robotics in the kitchen is still
unrealized. In this essay, the idea of FOON, a structural knowledge
representation built on insights from human manipulations, is introduced. To
reduce the failure rate and ensure that the task is effectively completed,
three different algorithms have been implemented where weighted values have
been assigned to the manipulations depending on the success rates of motion.
This knowledge representation was created using videos of open-sourced recipes
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondalapati_S/0/1/0/all/0/1&quot;&gt;Sandeep Bondalapati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01751">
<title>Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2211.01751</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming models are an essential component of real-time speech enhancement
tools. The streaming regime constrains speech enhancement models to use only a
tiny context of future information, thus, the low-latency streaming setup is
generally assumed to be challenging and has a significant negative effect on
the model quality. However, due to the sequential nature of streaming
generation, it provides a natural possibility for autoregression, i.e., using
previous predictions when making current ones. In this paper, we present a
simple, yet effective trick for training of autoregressive low-latency speech
enhancement models. We demonstrate that the proposed technique leads to stable
improvement across different architectures and training scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreev_P/0/1/0/all/0/1&quot;&gt;Pavel Andreev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaev_N/0/1/0/all/0/1&quot;&gt;Nicholas Babaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saginbaev_A/0/1/0/all/0/1&quot;&gt;Azat Saginbaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchekotov_I/0/1/0/all/0/1&quot;&gt;Ivan Shchekotov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01759">
<title>Resource-aware Deep Learning for Wireless Fingerprinting Localization. (arXiv:2211.01759v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2211.01759</link>
<description rdf:parseType="Literal">&lt;p&gt;Location based services, already popular with end users, are now inevitably
becoming part of new wireless infrastructures and emerging business processes.
The increasingly popular Deep Learning (DL) artificial intelligence methods
perform very well in wireless fingerprinting localization based on extensive
indoor radio measurement data. However, with the increasing complexity these
methods become computationally very intensive and energy hungry, both for their
training and subsequent operation. Considering only mobile users, estimated to
exceed 7.4 billion by the end of 2025, and assuming that the networks serving
these users will need to perform only one localization per user per hour on
average, the machine learning models used for the calculation would need to
perform $65 \times 10^{12}$ predictions per year. Add to this equation tens of
billions of other connected devices and applications that rely heavily on more
frequent location updates, and it becomes apparent that localization will
contribute significantly to carbon emissions unless more energy-efficient
models are developed and used. In this Chapter, we discuss the latest results
and trends in wireless localization and look at paths towards achieving more
sustainable AI. We then elaborate on a methodology for computing DL model
complexity, energy consumption and carbon footprint and show on a concrete
example how to develop a more resource-aware model for fingerprinting. We
finally compare relevant works in terms of complexity and training CO$_2$
footprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerar_G/0/1/0/all/0/1&quot;&gt;Gregor Cerar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertalanic_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Bertalani&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuna_C/0/1/0/all/0/1&quot;&gt;Carolina Fortuna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01761">
<title>PromptEHR: Conditional Electronic Healthcare Records Generation with Prompt Learning. (arXiv:2211.01761v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01761</link>
<description rdf:parseType="Literal">&lt;p&gt;Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is
challenging due to privacy concerns, which hinders the use of ML for healthcare
applications. Synthetic EHRs generation bypasses the need to share sensitive
real patient records. However, existing methods generate single-modal EHRs by
unconditional generation or by longitudinal inference, which falls short of low
flexibility and makes unrealistic EHRs. In this work, we propose to formulate
EHRs generation as a text-to-text translation task by language models (LMs),
which suffices to highly flexible event imputation during generation. We also
design prompt learning to control the generation conditioned by numerical and
categorical demographic features. We evaluate synthetic EHRs quality by two
perplexity measures accounting for their longitudinal pattern (longitudinal
imputation perplexity, lpl) and the connections cross modalities
(cross-modality imputation perplexity, mpl). Moreover, we utilize two
adversaries: membership and attribute inference attacks for privacy-preserving
evaluation. Experiments on MIMIC-III data demonstrate the superiority of our
methods on realistic EHRs generation (53.1\% decrease of lpl and 45.3\%
decrease of mpl on average compared to the best baselines) with low privacy
risks. Software is available at https://github.com/RyanWangZf/PromptEHR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01786">
<title>Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01786</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are publicly
available at https://github.com/bigscience-workshop/xmtf.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Thomas Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1&quot;&gt;Lintang Sutawika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Adam Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1&quot;&gt;Stella Biderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1&quot;&gt;Teven Le Scao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1&quot;&gt;M Saiful Bari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1&quot;&gt;Zheng-Xin Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1&quot;&gt;Hailey Schoelkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiangru Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1&quot;&gt;Alham Fikri Aji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almubarak_K/0/1/0/all/0/1&quot;&gt;Khalid Almubarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1&quot;&gt;Zaid Alyafeai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1&quot;&gt;Albert Webson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01806">
<title>BATT: Backdoor Attack with Transformation-based Triggers. (arXiv:2211.01806v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.01806</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor
adversaries intend to maliciously control the predictions of attacked DNNs by
injecting hidden backdoors that can be activated by adversary-specified trigger
patterns during the training process. One recent research revealed that most of
the existing attacks failed in the real physical world since the trigger
contained in the digitized test samples may be different from that of the one
used for training. Accordingly, users can adopt spatial transformations as the
image pre-processing to deactivate hidden backdoors. In this paper, we explore
the previous findings from another side. We exploit classical spatial
transformations (i.e. rotation and translation) with the specific parameter as
trigger patterns to design a simple yet effective poisoning-based backdoor
attack. For example, only images rotated to a particular angle can activate the
embedded backdoor of attacked DNNs. Extensive experiments are conducted,
verifying the effectiveness of our attack under both digital and physical
settings and its resistance to existing backdoor defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01809">
<title>Manipulation of individual judgments in the quantitative pairwise comparisons method. (arXiv:2211.01809v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01809</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-making methods very often use the technique of comparing
alternatives in pairs. In this approach, experts are asked to compare different
options, and then a quantitative ranking is created from the results obtained.
It is commonly believed that experts (decision-makers) are honest in their
judgments. In our work, we consider a scenario in which experts are vulnerable
to bribery. For this purpose, we define a framework that allows us to determine
the intended manipulation and present three algorithms for achieving the
intended goal. Analyzing these algorithms may provide clues to help defend
against such attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strada_M/0/1/0/all/0/1&quot;&gt;M. Strada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulakowski_K/0/1/0/all/0/1&quot;&gt;K. Ku&amp;#x142;akowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01817">
<title>Liability regimes in the age of AI: a use-case driven analysis of the burden of proof. (arXiv:2211.01817v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01817</link>
<description rdf:parseType="Literal">&lt;p&gt;New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concerns about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, lead to considerable difficulties
when it comes to proving causation. This paper presents three case studies, as
well as the methodology to reach them, that illustrate these difficulties.
Specifically, we address the cases of cleaning robots, delivery drones and
robots in education. The outcome of the proposed analysis suggests the need to
revise liability regimes to alleviate the burden of proof on victims in cases
involving AI technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llorca_D/0/1/0/all/0/1&quot;&gt;David Fern&amp;#xe1;ndez Llorca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charisi_V/0/1/0/all/0/1&quot;&gt;Vicky Charisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamon_R/0/1/0/all/0/1&quot;&gt;Ronan Hamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_I/0/1/0/all/0/1&quot;&gt;Ignacio S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_E/0/1/0/all/0/1&quot;&gt;Emilia G&amp;#xf3;mez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01830">
<title>Ranking-based Group Identification via Factorized Attention on Social Tripartite Graph. (arXiv:2211.01830v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2211.01830</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the proliferation of social media, a growing number of users search
for and join group activities in their daily life. This develops a need for the
study on the ranking-based group identification (RGI) task, i.e., recommending
groups to users. The major challenge in this task is how to effectively and
efficiently leverage both the item interaction and group participation of
users&apos; online behaviors. Though recent developments of Graph Neural Networks
(GNNs) succeed in simultaneously aggregating both social and user-item
interaction, they however fail to comprehensively resolve this RGI task. In
this paper, we propose a novel GNN-based framework named Contextualized
Factorized Attention for Group identification (CFAG). We devise tripartite
graph convolution layers to aggregate information from different types of
neighborhoods among users, groups, and items. To cope with the data sparsity
issue, we devise a novel propagation augmentation (PA) layer, which is based on
our proposed factorized attention mechanism. PA layers efficiently learn the
relatedness of non-neighbor nodes to improve the information propagation to
users. Experimental results on three benchmark datasets verify the superiority
of CFAG. Additional detailed investigations are conducted to demonstrate the
effectiveness of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingdai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liangwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01839">
<title>HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2211.01839</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szatkowski_F/0/1/0/all/0/1&quot;&gt;Filip Szatkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piczak_K/0/1/0/all/0/1&quot;&gt;Karol J. Piczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01842">
<title>Towards Discovering Neural Architectures from Scratch. (arXiv:2211.01842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of neural architectures from scratch is the long-standing goal
of Neural Architecture Search (NAS). Searching over a wide spectrum of neural
architectures can facilitate the discovery of previously unconsidered but
well-performing architectures. In this work, we take a large step towards
discovering neural architectures from scratch by expressing architectures
algebraically. This algebraic view leads to a more general method for designing
search spaces, which allows us to compactly represent search spaces that are
100s of orders of magnitude larger than common spaces from the literature.
Further, we propose a Bayesian Optimization strategy to efficiently search over
such huge spaces, and demonstrate empirically that both our search space design
and our search strategy can be superior to existing baselines. We open source
our algebraic NAS approach and provide APIs for PyTorch and TensorFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1&quot;&gt;Simon Schrodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1&quot;&gt;Danny Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1&quot;&gt;Rhea Sukthanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01845">
<title>Reinforcement Learning based Cyberattack Model for Adaptive Traffic Signal Controller in Connected Transportation Systems. (arXiv:2211.01845v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.01845</link>
<description rdf:parseType="Literal">&lt;p&gt;In a connected transportation system, adaptive traffic signal controllers
(ATSC) utilize real-time vehicle trajectory data received from vehicles through
wireless connectivity (i.e., connected vehicles) to regulate green time.
However, this wirelessly connected ATSC increases cyber-attack surfaces and
increases their vulnerability to various cyber-attack modes, which can be
leveraged to induce significant congestion in a roadway network. An attacker
may receive financial benefits to create such a congestion for a specific
roadway. One such mode is a &apos;sybil&apos; attack in which an attacker creates fake
vehicles in the network by generating fake Basic Safety Messages (BSMs)
imitating actual connected vehicles following roadway traffic rules. The
ultimate goal of an attacker will be to block a route(s) by generating fake or
&apos;sybil&apos; vehicles at a rate such that the signal timing and phasing changes
occur without flagging any abrupt change in number of vehicles. Because of the
highly non-linear and unpredictable nature of vehicle arrival rates and the
ATSC algorithm, it is difficult to find an optimal rate of sybil vehicles,
which will be injected from different approaches of an intersection. Thus, it
is necessary to develop an intelligent cyber-attack model to prove the
existence of such attacks. In this study, a reinforcement learning based
cyber-attack model is developed for a waiting time-based ATSC. Specifically, an
RL agent is trained to learn an optimal rate of sybil vehicle injection to
create congestion for an approach(s). Our analyses revealed that the RL agent
can learn an optimal policy for creating an intelligent attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irfan_M/0/1/0/all/0/1&quot;&gt;Muhammad Sami Irfan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkison_T/0/1/0/all/0/1&quot;&gt;Travis Atkison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hainen_A/0/1/0/all/0/1&quot;&gt;Alexander Hainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01847">
<title>Seeing the Unseen: Errors and Bias in Visual Datasets. (arXiv:2211.01847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01847</link>
<description rdf:parseType="Literal">&lt;p&gt;From face recognition in smartphones to automatic routing on self-driving
cars, machine vision algorithms lie in the core of these features. These
systems solve image based tasks by identifying and understanding objects,
subsequently making decisions from these information. However, errors in
datasets are usually induced or even magnified in algorithms, at times
resulting in issues such as recognising black people as gorillas and
misrepresenting ethnicities in search results. This paper tracks the errors in
datasets and their impacts, revealing that a flawed dataset could be a result
of limited categories, incomprehensive sourcing and poor classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongrui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01873">
<title>Port-metriplectic neural networks: thermodynamics-informed machine learning of complex physical systems. (arXiv:2211.01873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01873</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop inductive biases for the machine learning of complex physical
systems based on the port-Hamiltonian formalism. To satisfy by construction the
principles of thermodynamics in the learned physics (conservation of energy,
non-negative entropy production), we modify accordingly the port-Hamiltonian
formalism so as to achieve a port-metriplectic one. We show that the
constructed networks are able to learn the physics of complex systems by parts,
thus alleviating the burden associated to the experimental characterization and
posterior learning process of this kind of systems. Predictions can be done,
however, at the scale of the complete system. Examples are shown on the
performance of the proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Q/0/1/0/all/0/1&quot;&gt;Quercus Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1&quot;&gt;Alberto Bad&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1&quot;&gt;Francisco Chinesta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1&quot;&gt;El&amp;#xed;as Cueto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01910">
<title>Large Language Models Are Human-Level Prompt Engineers. (arXiv:2211.01910v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01910</link>
<description rdf:parseType="Literal">&lt;p&gt;By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the &quot;program,&quot; optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yongchao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresanu_A/0/1/0/all/0/1&quot;&gt;Andrei Ioan Muresanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Ziwen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paster_K/0/1/0/all/0/1&quot;&gt;Keiran Paster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitis_S/0/1/0/all/0/1&quot;&gt;Silviu Pitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Harris Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01914">
<title>FedGen: Generalizable Federated Learning. (arXiv:2211.01914v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01914</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing federated learning models that follow the standard risk minimization
paradigm of machine learning often fail to generalize in the presence of
spurious correlations in the training data. In many real-world distributed
settings, spurious correlations exist due to biases and data sampling issues on
distributed devices or clients that can erroneously influence models. Current
generalization approaches are designed for centralized training and attempt to
identify features that have an invariant causal relationship with the target,
thereby reducing the effect of spurious features. However, such invariant risk
minimization approaches rely on apriori knowledge of training data
distributions which is hard to obtain in many applications. In this work, we
present a generalizable federated learning framework called FedGen, which
allows clients to identify and distinguish between spurious and invariant
features in a collaborative manner without prior knowledge of training
distributions. We evaluate our approach on real-world datasets from different
domains and show that FedGen results in models that achieve significantly
better generalization than current federated learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkateswaran_P/0/1/0/all/0/1&quot;&gt;Praveen Venkateswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isahagian_V/0/1/0/all/0/1&quot;&gt;Vatche Isahagian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthusamy_V/0/1/0/all/0/1&quot;&gt;Vinod Muthusamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatasubramanian_N/0/1/0/all/0/1&quot;&gt;Nalini Venkatasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01915">
<title>Uncertainty Quantification for Rule-Based Models. (arXiv:2211.01915v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2211.01915</link>
<description rdf:parseType="Literal">&lt;p&gt;Rule-based classification models described in the language of logic directly
predict boolean values, rather than modeling a probability and translating it
into a prediction as done in statistical models. The vast majority of existing
uncertainty quantification approaches rely on models providing continuous
output not available to rule-based models. In this work, we propose an
uncertainty quantification framework in the form of a meta-model that takes any
binary classifier with binary output as a black box and estimates the
prediction accuracy of that base model at a given input along with a level of
confidence on that estimation. The confidence is based on how well that input
region is explored and is designed to work in any OOD scenario. We demonstrate
the usefulness of this uncertainty model by building an abstaining classifier
powered by it and observing its performance in various scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yusik Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01917">
<title>Expanding Accurate Person Recognition to New Altitudes and Ranges: The BRIAR Dataset. (arXiv:2211.01917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01917</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition technology has advanced significantly in recent years due
largely to the availability of large and increasingly complex training datasets
for use in deep learning models. These datasets, however, typically comprise
images scraped from news sites or social media platforms and, therefore, have
limited utility in more advanced security, forensics, and military
applications. These applications require lower resolution, longer ranges, and
elevated viewpoints. To meet these critical needs, we collected and curated the
first and second subsets of a large multi-modal biometric dataset designed for
use in the research and development (R&amp;amp;D) of biometric recognition technologies
under extremely challenging conditions. Thus far, the dataset includes more
than 350,000 still images and over 1,300 hours of video footage of
approximately 1,000 subjects. To collect this data, we used Nikon DSLR cameras,
a variety of commercial surveillance cameras, specialized long-rage R&amp;amp;D
cameras, and Group 1 and Group 2 UAV platforms. The goal is to support the
development of algorithms capable of accurately recognizing people at ranges up
to 1,000 m and from high angles of elevation. These advances will include
improvements to the state of the art in face recognition and will support new
research in the area of whole-body recognition using methods based on gait and
anthropometry. This paper describes methods used to collect and curate the
dataset, and the dataset&apos;s characteristics at the current stage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornett_D/0/1/0/all/0/1&quot;&gt;David Cornett III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brogan_J/0/1/0/all/0/1&quot;&gt;Joel Brogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_N/0/1/0/all/0/1&quot;&gt;Nell Barber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aykac_D/0/1/0/all/0/1&quot;&gt;Deniz Aykac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baird_S/0/1/0/all/0/1&quot;&gt;Seth Baird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burchfield_N/0/1/0/all/0/1&quot;&gt;Nick Burchfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dukes_C/0/1/0/all/0/1&quot;&gt;Carl Dukes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_A/0/1/0/all/0/1&quot;&gt;Andrew Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrell_R/0/1/0/all/0/1&quot;&gt;Regina Ferrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goddard_J/0/1/0/all/0/1&quot;&gt;Jim Goddard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_G/0/1/0/all/0/1&quot;&gt;Gavin Jager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1&quot;&gt;Matt Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_B/0/1/0/all/0/1&quot;&gt;Bart Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_C/0/1/0/all/0/1&quot;&gt;Christi Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shelley_I/0/1/0/all/0/1&quot;&gt;Ian Shelley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_N/0/1/0/all/0/1&quot;&gt;Nisha Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stockwell_B/0/1/0/all/0/1&quot;&gt;Brandon Stockwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1&quot;&gt;Leanne Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yohe_M/0/1/0/all/0/1&quot;&gt;Matt Yohe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Robert Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolvin_S/0/1/0/all/0/1&quot;&gt;Scott Dolvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Villalobos_H/0/1/0/all/0/1&quot;&gt;Hector J. Santos-Villalobos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolme_D/0/1/0/all/0/1&quot;&gt;David S. Bolme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01939">
<title>Empirical Analysis of Model Selection for Heterogenous Causal Effect Estimation. (arXiv:2211.01939v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01939</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of model selection in causal inference, specifically for
the case of conditional average treatment effect (CATE) estimation under binary
treatments. Unlike model selection in machine learning, we cannot use the
technique of cross-validation here as we do not observe the counterfactual
potential outcome for any data point. Hence, we need to design model selection
techniques that do not explicitly rely on counterfactual data. As an
alternative to cross-validation, there have been a variety of proxy metrics
proposed in the literature, that depend on auxiliary nuisance models also
estimated from the data (propensity score model, outcome regression model).
However, the effectiveness of these metrics has only been studied on synthetic
datasets as we can observe the counterfactual data for them. We conduct an
extensive empirical analysis to judge the performance of these metrics, where
we utilize the latest advances in generative modeling to incorporate multiple
realistic datasets. We evaluate 9 metrics on 144 datasets for selecting between
415 estimators per dataset, including datasets that closely mimic real-world
datasets. Further, we use the latest techniques from AutoML to ensure
consistent hyperparameter selection for nuisance models for a fair comparison
across metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1&quot;&gt;Divyat Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1&quot;&gt;Ioannis Mitliagkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neal_B/0/1/0/all/0/1&quot;&gt;Brady Neal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01948">
<title>Efficiently Trained Mongolian Text-to-Speech System Based On FullConv. (arXiv:2211.01948v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01948</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent Neural Networks (RNNs) have become the standard modeling technique
for sequence data, and are used in a number of novel text-to-speech models.
However, training a TTS model including RNN components has certain requirements
for GPU performance and takes a long time. In contrast, studies have shown that
CNN-based sequence synthesis technology can greatly reduce training time in
text-to-speech models while ensuring a certain performance due to its high
parallelism. We propose a new text-to-speech system based on deep convolutional
neural networks that does not employ any RNN components (recurrent units). At
the same time, we improve the generality and robustness of our model through a
series of data augmentation methods such as Time Warping, Frequency Mask, and
Time Mask. The final experimental results show that the TTS model using only
the CNN component can reduce the training time compared to the classic TTS
models such as Tacotron while ensuring the quality of the synthesized speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;ZiQi Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01957">
<title>Sub-network Multi-objective Evolutionary Algorithm for Filter Pruning. (arXiv:2211.01957v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2211.01957</link>
<description rdf:parseType="Literal">&lt;p&gt;Filter pruning is a common method to achieve model compression and
acceleration in deep neural networks (DNNs).Some research regarded filter
pruning as a combinatorial optimization problem and thus used evolutionary
algorithms (EA) to prune filters of DNNs. However, it is difficult to find a
satisfactory compromise solution in a reasonable time due to the complexity of
solution space searching. To solve this problem, we first formulate a
multi-objective optimization problem based on a sub-network of the full model
and propose a Sub-network Multiobjective Evolutionary Algorithm (SMOEA) for
filter pruning. By progressively pruning the convolutional layers in groups,
SMOEA can obtain a lightweight pruned result with better
performance.Experiments on VGG-14 model for CIFAR-10 verify the effectiveness
of the proposed SMOEA. Specifically, the accuracy of the pruned model with
16.56% parameters decreases by 0.28% only, which is better than the widely used
popular filter pruning criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weize Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaowu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01959">
<title>An agent-based approach to procedural city generation incorporating Land Use and Transport Interaction models. (arXiv:2211.01959v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2211.01959</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply the knowledge of urban settings established with the study of Land
Use and Transport Interaction (LUTI) models to develop reward functions for an
agent-based system capable of planning realistic artificial cities. The system
aims to replicate in the micro scale the main components of real settlements,
such as zoning and accessibility in a road network. Moreover, we propose a
novel representation for the agent&apos;s environment that efficiently combines the
road graph with a discrete model for the land. Our system starts from an empty
map consisting only of the road network graph, and the agent incrementally
expands it by building new sites while distinguishing land uses between
residential, commercial, industrial, and recreational.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1&quot;&gt;Luiz Fernando Silva Eug&amp;#xea;nio dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aranha_C/0/1/0/all/0/1&quot;&gt;Claus Aranha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Ponce de Leon F de Carvalho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01962">
<title>A Posterior Sampling Framework for Interactive Decision Making. (arXiv:2211.01962v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01962</link>
<description rdf:parseType="Literal">&lt;p&gt;We study sample efficient reinforcement learning (RL) under the general
framework of interactive decision making, which includes Markov decision
process (MDP), partially observable Markov decision process (POMDP), and
predictive state representation (PSR) as special cases. Toward finding the
minimum assumption that empowers sample efficient learning, we propose a novel
complexity measure, generalized eluder coefficient (GEC), which characterizes
the fundamental tradeoff between exploration and exploitation in online
interactive decision making. In specific, GEC captures the hardness of
exploration by comparing the error of predicting the performance of the updated
policy with the in-sample training error evaluated on the historical data. We
show that RL problems with low GEC form a remarkably rich class, which subsumes
low Bellman eluder dimension problems, bilinear class, low witness rank
problems, PO-bilinear class, and generalized regular PSR, where generalized
regular PSR, a new tractable PSR class identified by us, includes nearly all
known tractable POMDPs. Furthermore, in terms of algorithm design, we propose a
generic posterior sampling algorithm, which can be implemented in both
model-free and model-based fashion, under both fully observable and partially
observable settings. The proposed algorithm modifies the standard posterior
sampling algorithm in two aspects: (i) we use an optimistic prior distribution
that biases towards hypotheses with higher values and (ii) a loglikelihood
function is set to be the empirical loss evaluated on the historical data,
where the choice of loss function supports both model-free and model-based
learning. We prove that the proposed algorithm is sample efficient by
establishing a sublinear regret upper bound in terms of GEC. In summary, we
provide a new and unified understanding of both fully observable and partially
observable RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Han Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sirui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01964">
<title>Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis. (arXiv:2211.01964v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01964</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding paralinguistic properties is a challenging task as there are only a
few hours of training data available for domains such as emotional speech. One
solution to this problem is to pretrain a general self-supervised speech
representation model on large amounts of unlabeled speech. This pretrained
model is then finetuned to a specific task. Paralinguistic properties however
have notoriously high class variance, making the finetuning ineffective. In
this work, we propose a two step approach to this. First we improve the
embedding space, then we train an adapter to bridge the gap from the embedding
space to a classification task. In order to improve the class invariance we use
a combination of contrastive and non-contrastive losses to explicitly optimize
for class invariant, yet discriminative features. Our approach consistently
outperforms baselines that are finetuned end-to-end on multiple tasks and
surpasses a benchmark on state-of-the-art emotion classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lux_F/0/1/0/all/0/1&quot;&gt;Florian Lux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Ching-Yi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1&quot;&gt;Ngoc Thang Vu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01969">
<title>Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing. (arXiv:2211.01969v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2211.01969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a framework for jointly grounding objects that follow
certain semantic relationship constraints given in a scene graph. A typical
natural scene contains several objects, often exhibiting visual relationships
of varied complexities between them. These inter-object relationships provide
strong contextual cues toward improving grounding performance compared to a
traditional object query-only-based localization task. A scene graph is an
efficient and structured way to represent all the objects and their semantic
relationships in the image. In an attempt towards bridging these two modalities
representing scenes and utilizing contextual information for improving object
localization, we rigorously study the problem of grounding scene graphs on
natural images. To this end, we propose a novel graph neural network-based
approach referred to as Visio-Lingual Message PAssing Graph Neural Network
(VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object
proposals as nodes and an edge between a pair of nodes representing a plausible
relation between them. Then a three-step inter-graph and intra-graph message
passing is performed to learn the context-dependent representation of the
proposals and query objects. These object representations are used to score the
proposals to generate object localization. The proposed method significantly
outperforms the baselines on four public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1&quot;&gt;Aditay Tripathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Anand Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Anirban Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01970">
<title>AI enhanced finite element multiscale modelling and structural uncertainty analysis of a functionally graded porous beam. (arXiv:2211.01970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01970</link>
<description rdf:parseType="Literal">&lt;p&gt;The local geometrical randomness of metal foams brings complexities to the
performance prediction of porous structures. Although the relative density is
commonly deemed as the key factor, the stochasticity of internal cell sizes and
shapes has an apparent effect on the porous structural behaviour but the
corresponding measurement is challenging. To address this issue, we are aimed
to develop an assessment strategy for efficiently examining the foam properties
by combining multiscale modelling and deep learning. The multiscale modelling
is based on the finite element (FE) simulation employing representative volume
elements (RVEs) with random cellular morphologies, mimicking the typical
features of closed-cell Aluminium foams. A deep learning database is
constructed for training the designed convolutional neural networks (CNNs) to
establish a direct link between the mesoscopic porosity characteristics and the
effective Youngs modulus of foams. The error range of CNN models leads to an
uncertain mechanical performance, which is further evaluated in a structural
uncertainty analysis on the FG porous three-layer beam consisting of two thin
high-density layers and a thick low-density one, where the imprecise CNN
predicted moduli are represented as triangular fuzzy numbers in double
parametric form. The uncertain beam bending deflections under a mid-span point
load are calculated with the aid of Timoshenko beam theory and the Ritz method.
Our findings suggest the success in training CNN models to estimate RVE modulus
using images with an average error of 5.92%. The evaluation of FG porous
structures can be significantly simplified with the proposed method and
connects to the mesoscopic cellular morphologies without establishing the
mechanics model for local foams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Da Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emami_N/0/1/0/all/0/1&quot;&gt;Nima Emami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1&quot;&gt;Shahed Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosendahl_P/0/1/0/all/0/1&quot;&gt;Philipp L. Rosendahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bai-Xiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jens Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01978">
<title>PEMP: Leveraging Physics Properties to Enhance Molecular Property Prediction. (arXiv:2211.01978v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2211.01978</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular property prediction is essential for drug discovery. In recent
years, deep learning methods have been introduced to this area and achieved
state-of-the-art performances. However, most of existing methods ignore the
intrinsic relations between molecular properties which can be utilized to
improve the performances of corresponding prediction tasks. In this paper, we
propose a new approach, namely Physics properties Enhanced Molecular Property
prediction (PEMP), to utilize relations between molecular properties revealed
by previous physics theory and physical chemistry studies. Specifically, we
enhance the training of the chemical and physiological property predictors with
related physics property prediction tasks. We design two different methods for
PEMP, respectively based on multi-task learning and transfer learning. Both
methods include a model-agnostic molecule representation module and a property
prediction module. In our implementation, we adopt both the state-of-the-art
molecule embedding models under the supervised learning paradigm and the
pretraining paradigm as the molecule representation module of PEMP,
respectively. Experimental results on public benchmark MoleculeNet show that
the proposed methods have the ability to outperform corresponding
state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yimeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Weizhi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Ying Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01979">
<title>Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters. (arXiv:2211.01979v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01979</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapter-tuning is a paradigm that transfers a pretrained language model to
downstream tasks by adding and tuning a small number of new parameters.
Previously proposed adapter architectures are all feed-forward neural networks.
In this paper, we investigate the effectiveness of using tiny-attention --
i.e., attention with extremely small per-head dimensionality -- as adapters.
Our tiny-attention adapter learns to modify the hidden states at each position
directly conditioned on the hidden states at all the other positions, which is
missed by the previously proposed adapters. Moreover, we view its multiple
attention heads as a mixture of experts and propose to average their weights
during deployment, which further reduces its inference computation cost. On the
GLUE benchmark, our tiny-attention adapter outperforms the other
parameter-efficient transfer learning methods as well as full fine-tuning while
only updating 0.05% of the parameters. On the FewGLUE benchmark, its
performance is comparable to that of GPT-3 and PET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hongyuan Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01981">
<title>Topic Taxonomy Expansion via Hierarchy-Aware Topic Phrase Generation. (arXiv:2211.01981v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2211.01981</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic taxonomies display hierarchical topic structures of a text corpus and
provide topical knowledge to enhance various NLP applications. To dynamically
incorporate new topic information, several recent studies have tried to expand
(or complete) a topic taxonomy by inserting emerging topics identified in a set
of new documents. However, existing methods focus only on frequent terms in
documents and the local topic-subtopic relations in a taxonomy, which leads to
limited topic term coverage and fails to model the global topic hierarchy. In
this work, we propose a novel framework for topic taxonomy expansion, named
TopicExpan, which directly generates topic-related terms belonging to new
topics. Specifically, TopicExpan leverages the hierarchical relation structure
surrounding a new topic and the textual content of an input document for topic
term generation. This approach encourages newly-inserted topics to further
cover important but less frequent terms as well as to keep their relation
consistency within the taxonomy. Experimental results on two real-world text
corpora show that TopicExpan significantly outperforms other baseline methods
in terms of the quality of output taxonomies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiaming Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Susik Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hwanjo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01984">
<title>Sybil-Proof Diffusion Auction in Social Networks. (arXiv:2211.01984v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2211.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;A diffusion auction is a market to sell commodities over a social network,
where the challenge is to incentivize existing buyers to invite their neighbors
in the network to join the market. Existing mechanisms have been designed to
solve the challenge in various settings, aiming at desirable properties such as
non-deficiency, incentive compatibility and social welfare maximization. Since
the mechanisms are employed in dynamic networks with ever-changing structures,
buyers could easily generate fake nodes in the network to manipulate the
mechanisms for their own benefits, which is commonly known as the Sybil attack.
We observe that strategic agents may gain an unfair advantage in existing
mechanisms through such attacks. To resist this potential attack, we propose
two diffusion auction mechanisms, the Sybil tax mechanism (STM) and the Sybil
cluster mechanism (SCM), to achieve both Sybil-proofness and incentive
compatibility in the single-item setting. Our proposal provides the first
mechanisms to protect the interests of buyers against Sybil attacks with a mild
sacrifice of social welfare and revenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongyin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaotie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dengji Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01994">
<title>lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01994</link>
<description rdf:parseType="Literal">&lt;p&gt;We present lilGym, a new benchmark for language-conditioned reinforcement
learning in visual environments. lilGym is based on 2,661 highly-compositional
human-written natural language statements grounded in an interactive visual
environment. We annotate all statements with executable Python programs
representing their meaning to enable exact reward computation in every possible
world state. Each statement is paired with multiple start states and reward
functions to form thousands of distinct Markov Decision Processes of varying
difficulty. We experiment with lilGym with different models and learning
regimes. Our results and analysis show that while existing methods are able to
achieve non-trivial performance, lilGym forms a challenging open problem.
lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1&quot;&gt;Anne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1&quot;&gt;Kiant&amp;#xe9; Brantley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1&quot;&gt;Noriyuki Kojima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02016">
<title>Oracle Inequalities for Model Selection in Offline Reinforcement Learning. (arXiv:2211.02016v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.02016</link>
<description rdf:parseType="Literal">&lt;p&gt;In offline reinforcement learning (RL), a learner leverages prior logged data
to learn a good policy without interacting with the environment. A major
challenge in applying such methods in practice is the lack of both
theoretically principled and practical tools for model selection and
evaluation. To address this, we study the problem of model selection in offline
RL with value function approximation. The learner is given a nested sequence of
model classes to minimize squared Bellman error and must select among these to
achieve a balance between approximation and estimation error of the classes. We
propose the first model selection algorithm for offline RL that achieves
minimax rate-optimal oracle inequalities up to logarithmic factors. The
algorithm, ModBE, takes as input a collection of candidate model classes and a
generic base offline RL algorithm. By successively eliminating model classes
using a novel one-sided generalization test, ModBE returns a policy with regret
scaling with the complexity of the minimally complete model class. In addition
to its theoretical guarantees, it is conceptually simple and computationally
efficient, amounting to solving a series of square loss regression problems and
then comparing relative square loss between classes. We conclude with several
numerical simulations showing it is capable of reliably selecting a good model
class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jonathan N. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02024">
<title>fMRI from EEG is only Deep Learning away: the use of interpretable DL to unravel EEG-fMRI relationships. (arXiv:2211.02024v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2211.02024</link>
<description rdf:parseType="Literal">&lt;p&gt;The access to activity of subcortical structures offers unique opportunity
for building intention dependent brain-computer interfaces, renders abundant
options for exploring a broad range of cognitive phenomena in the realm of
affective neuroscience including complex decision making processes and the
eternal free-will dilemma and facilitates diagnostics of a range of
neurological deceases. So far this was possible only using bulky, expensive and
immobile fMRI equipment. Here we present an interpretable domain grounded
solution to recover the activity of several subcortical regions from the
multichannel EEG data and demonstrate up to 60% correlation between the actual
subcortical blood oxygenation level dependent sBOLD signal and its EEG-derived
twin. Then, using the novel and theoretically justified weight interpretation
methodology we recover individual spatial and time-frequency patterns of scalp
EEG predictive of the hemodynamic signal in the subcortical nuclei. The
described results not only pave the road towards wearable subcortical activity
scanners but also showcase an automatic knowledge discovery process facilitated
by deep learning technology in combination with an interpretable domain
constrained architecture and the appropriate downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kovalev_A/0/1/0/all/0/1&quot;&gt;Alexander Kovalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mikheev_I/0/1/0/all/0/1&quot;&gt;Ilia Mikheev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ossadtchi_A/0/1/0/all/0/1&quot;&gt;Alexei Ossadtchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1909.08191">
<title>Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space. (arXiv:1909.08191v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1909.08191</link>
<description rdf:parseType="Literal">&lt;p&gt;The trends of open science have enabled several open scholarly datasets which
include millions of papers and authors. Managing, exploring, and utilizing such
large and complicated datasets effectively are challenging. In recent years,
the knowledge graph has emerged as a universal data format for representing
knowledge about heterogeneous entities and their relationships. The knowledge
graph can be modeled by knowledge graph embedding methods, which represent
entities and relations as embedding vectors in semantic space, then model the
interactions between these embedding vectors. However, the semantic structures
in the knowledge graph embedding space are not well-studied, thus knowledge
graph embedding methods are usually only used for knowledge graph completion
but not data representation and analysis. In this paper, we propose to analyze
these semantic structures based on the well-studied word embedding space and
use them to support data exploration. We also define the semantic queries,
which are algebraic operations between the embedding vectors in the knowledge
graph embedding space, to solve queries such as similarity and analogy between
the entities on the original datasets. We then design a general framework for
data exploration by semantic queries and discuss the solution to some
traditional scholarly data exploration tasks. We also propose some new
interesting tasks that can be solved based on the uncanny semantic structures
of the embedding space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Hung Nghiep Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1&quot;&gt;Atsuhiro Takasu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.02649">
<title>A step towards a reinforcement learning de novo genome assembler. (arXiv:2102.02649v3 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2102.02649</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of reinforcement learning has proven to be very promising for solving
complex activities without human supervision during their learning process.
However, their successful applications are predominantly focused on fictional
and entertainment problems - such as games. Based on the above, this work aims
to shed light on the application of reinforcement learning to solve this
relevant real-world problem, the genome assembly. By expanding the only
approach found in the literature that addresses this problem, we carefully
explored the aspects of intelligent agent learning, performed by the Q-learning
algorithm, to understand its suitability to be applied in scenarios whose
characteristics are more similar to those faced by real genome projects. The
improvements proposed here include changing the previously proposed reward
system and including state space exploration optimization strategies based on
dynamic pruning and mutual collaboration with evolutionary computing. These
investigations were tried on 23 new environments with larger inputs than those
used previously. All these environments are freely available on the internet
for the evolution of this research by the scientific community. The results
suggest consistent performance progress using the proposed improvements,
however, they also demonstrate the limitations of them, especially related to
the high dimensionality of state and action spaces. We also present, later, the
paths that can be traced to tackle genome assembly efficiently in real
scenarios considering recent, successfully reinforcement learning applications
- including deep reinforcement learning - from other domains dealing with
high-dimensional inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Padovani_K/0/1/0/all/0/1&quot;&gt;Kleber Padovani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xavier_R/0/1/0/all/0/1&quot;&gt;Roberto Xavier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Carvalho_A/0/1/0/all/0/1&quot;&gt;Andre Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reali_A/0/1/0/all/0/1&quot;&gt;Anna Reali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chateau_A/0/1/0/all/0/1&quot;&gt;Annie Chateau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alves_R/0/1/0/all/0/1&quot;&gt;Ronnie Alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.11909">
<title>Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2103.11909</link>
<description rdf:parseType="Literal">&lt;p&gt;Employing paraphrasing tools to conceal plagiarized text is a severe threat
to academic integrity. To enable the detection of machine-paraphrased text, we
evaluate the effectiveness of five pre-trained word embedding models combined
with machine learning classifiers and state-of-the-art neural language models.
We analyze preprints of research papers, graduation theses, and Wikipedia
articles, which we paraphrased using different configurations of the tools
SpinBot and SpinnerChief. The best performing technique, Longformer, achieved
an average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for
SpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and
F1=65.6% for SpinnerChief cases. We show that the automated classification
alleviates shortcomings of widely-used text-matching systems, such as Turnitin
and PlagScan. To facilitate future research, all data, code, and two web
applications showcasing our contributions are openly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1&quot;&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1&quot;&gt;Terry Ruas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Folt&amp;#xfd;nek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1&quot;&gt;Norman Meuschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1&quot;&gt;Bela Gipp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.12450">
<title>Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2103.12450</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of language models such as BERT allows for high-quality text
paraphrasing. This is a problem to academic integrity, as it is difficult to
differentiate between original and machine-generated content. We propose a
benchmark consisting of paraphrased articles using recent language models
relying on the Transformer architecture. Our contribution fosters future
research of paraphrase detection systems as it offers a large collection of
aligned original and paraphrased documents, a study regarding its structure,
classification experiments with state-of-the-art systems, and we make our
findings publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1&quot;&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1&quot;&gt;Terry Ruas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1&quot;&gt;Norman Meuschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1&quot;&gt;Bela Gipp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.15670">
<title>On the Adversarial Robustness of Vision Transformers. (arXiv:2103.15670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.15670</link>
<description rdf:parseType="Literal">&lt;p&gt;Following the success in advancing natural language processing and
understanding, transformers are expected to bring revolutionary changes to
computer vision. This work provides a comprehensive study on the robustness of
vision transformers (ViTs) against adversarial perturbations. Tested on various
white-box and transfer attack settings, we find that ViTs possess better
adversarial robustness when compared with MLP-Mixer and convolutional neural
networks (CNNs) including ConvNeXt, and this observation also holds for
certified robustness. Through frequency analysis and feature visualization, we
summarize the following main observations contributing to the improved
robustness of ViTs: 1) Features learned by ViTs contain less high-frequency
patterns that have spurious correlation, which helps explain why ViTs are less
sensitive to high-frequency perturbations than CNNs and MLP-Mixer, and there is
a high correlation between how much the model learns high-frequency features
and its robustness against different frequency-based perturbations. 2)
Introducing convolutional or tokens-to-token blocks for learning high-frequency
features in ViTs can improve classification accuracy but at the cost of
adversarial robustness. 3) Modern CNN designs that borrow techniques from ViTs
including activation function, layer norm, larger kernel size to imitate the
global attention, and patchify the images as inputs, etc., could help bridge
the performance gap between ViTs and CNNs not only in terms of performance, but
also certified and empirical adversarial robustness. Moreover, we show
adversarial training is also applicable to ViT for training robust models, and
sharpness-aware minimization can also help improve robustness, while
pre-training with clean images on larger datasets does not significantly
improve adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.12142">
<title>IQ-Learn: Inverse soft-Q Learning for Imitation. (arXiv:2106.12142v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.12142</link>
<description rdf:parseType="Literal">&lt;p&gt;In many sequential decision-making problems (e.g., robotics control, game
playing, sequential prediction), human or expert data is available containing
useful information about the task. However, imitation learning (IL) from a
small amount of expert data can be challenging in high-dimensional environments
with complex dynamics. Behavioral cloning is a simple method that is widely
used due to its simplicity of implementation and stable convergence but doesn&apos;t
utilize any information involving the environment&apos;s dynamics. Many existing
methods that exploit dynamics information are difficult to train in practice
due to an adversarial optimization process over reward and policy approximators
or biased, high variance gradient estimators. We introduce a method for
dynamics-aware IL which avoids adversarial training by learning a single
Q-function, implicitly representing both reward and policy. On standard
benchmarks, the implicitly learned rewards show a high positive correlation
with the ground-truth rewards, illustrating our method can also be used for
inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning
(IQ-Learn) obtains state-of-the-art results in offline and online imitation
learning settings, significantly outperforming existing methods both in the
number of required environment interactions and scalability in high-dimensional
spaces, often by more than 3x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1&quot;&gt;Divyansh Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Shuvam Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1&quot;&gt;Chris Cundy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.11873">
<title>When Do Contrastive Learning Signals Help Spatio-Temporal Graph Forecasting?. (arXiv:2108.11873v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2108.11873</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are modern tools for spatio-temporal graph (STG)
forecasting. Though successful, we argue that data scarcity is a key factor
limiting their recent improvements. Meanwhile, contrastive learning has been an
effective method for providing self-supervision signals and addressing data
scarcity in various domains. In view of this, one may ask: can we leverage the
additional signals from contrastive learning to alleviate data scarcity, so as
to benefit STG forecasting? To answer this question, we present the first
systematic exploration on incorporating contrastive learning into STG
forecasting. Specifically, we first elaborate two potential schemes for
integrating contrastive learning. We then propose two feasible and efficient
designs of contrastive tasks that are performed on the node or graph level. The
empirical study on STG benchmarks demonstrates that integrating graph-level
contrast with the joint learning scheme achieves the best performance. In
addition, we introduce four augmentations for STG data, which perturb the data
in terms of graph structure, time domain, and frequency domain. Experimental
results reveal that the model is not sensitive to the proposed augmentations&apos;
semantics. Lastly, we extend the classic contrastive loss via a rule-based
strategy that filters out the most semantically similar negatives, yielding
performance gains. We also provide explanations and insights based on the above
experimental findings. Code is available at https://github.com/liuxu77/STGCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.04083">
<title>User Tampering in Reinforcement Learning Recommender Systems. (arXiv:2109.04083v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2109.04083</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides novel formal methods and empirical demonstrations of a
particular safety concern in reinforcement learning (RL)-based recommendation
algorithms. We call this safety concern `user tampering&apos; -- a phenomenon
whereby an RL-based recommender system might manipulate a media user&apos;s opinions
via its recommendations as part of a policy to increase long-term user
engagement. We then apply techniques from causal modelling to analyse the
leading approaches in the literature for implementing scalable RL-based
recommenders, and we observe that the current approaches permit user tampering.
Additionally, we review the existing mitigation strategies for reward tampering
problems and show that they do not transfer well to the user tampering
phenomenon found in the recommendation context. Furthermore, we provide a
simulation study of a media RL-based recommendation problem constrained to the
recommendation of political content. We show that a Q-learning algorithm
consistently learns to exploit its opportunities to polarise simulated users
with its early recommendations in order to have more consistent success with
later recommendations catering to that polarisation. This latter contribution
calls for urgency in designing safer RL-based recommenders; the former suggests
that creating such safe recommenders will require a fundamental shift in design
away from the approaches we have seen in the recent literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_C/0/1/0/all/0/1&quot;&gt;Charles Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasirzadeh_A/0/1/0/all/0/1&quot;&gt;Atoosa Kasirzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.08868">
<title>Clean-label Backdoor Attack against Deep Hashing based Retrieval. (arXiv:2109.08868v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.08868</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep hashing has become a popular method in large-scale image retrieval due
to its computational and storage efficiency. However, recent works raise the
security concerns of deep hashing. Although existing works focus on the
vulnerability of deep hashing in terms of adversarial perturbations, we
identify a more pressing threat, backdoor attack, when the attacker has access
to the training data. A backdoored deep hashing model behaves normally on
original query images, while returning the images with the target label when
the trigger presents, which makes the attack hard to be detected. In this
paper, we uncover this security concern by utilizing clean-label data
poisoning. To the best of our knowledge, this is the first attempt at the
backdoor attack against deep hashing models. To craft the poisoned images, we
first generate the targeted adversarial patch as the backdoor trigger.
Furthermore, we propose the confusing perturbations to disturb the hashing code
learning, such that the hashing model can learn more about the trigger. The
confusing perturbations are imperceptible and generated by dispersing the
images with the target label in the Hamming space. We have conducted extensive
experiments to verify the efficacy of our backdoor attack under various
settings. For instance, it can achieve 63% targeted mean average precision on
ImageNet under 48 bits code length with only 40 poisoned images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kuofeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jiawang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongxian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13751">
<title>StereoSpike: Depth Learning with a Spiking Neural Network. (arXiv:2109.13751v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13751</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation is an important computer vision task, useful in particular
for navigation in autonomous vehicles, or for object manipulation in robotics.
Here we solved it using an end-to-end neuromorphic approach, combining two
event-based cameras and a Spiking Neural Network (SNN) with a slightly modified
U-Net-like encoder-decoder architecture, that we named StereoSpike. More
specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It
provides a depth ground-truth, which was used to train StereoSpike in a
supervised manner, using surrogate gradient descent. We propose a novel readout
paradigm to obtain a dense analog prediction -- the depth of each pixel -- from
the spikes of the decoder. We demonstrate that this architecture generalizes
very well, even better than its non-spiking counterparts, leading to
state-of-the-art test accuracy. To the best of our knowledge, it is the first
time that such a large-scale regression problem is solved by a fully spiking
network. Finally, we show that low firing rates (&amp;lt;10%) can be obtained via
regularization, with a minimal cost in accuracy. This means that StereoSpike
could be efficiently implemented on neuromorphic chips, opening the door for
low power and real time embedded systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rancon_U/0/1/0/all/0/1&quot;&gt;Ulysse Ran&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuadrado_Anibarro_J/0/1/0/all/0/1&quot;&gt;Javier Cuadrado-Anibarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cottereau_B/0/1/0/all/0/1&quot;&gt;Benoit R. Cottereau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.05985">
<title>A Categorical Semantics of Fuzzy Concepts in Conceptual Spaces. (arXiv:2110.05985v2 [math.CT] UPDATED)</title>
<link>http://arxiv.org/abs/2110.05985</link>
<description rdf:parseType="Literal">&lt;p&gt;We define a symmetric monoidal category modelling fuzzy concepts and fuzzy
conceptual reasoning within G\&quot;ardenfors&apos; framework of conceptual (convex)
spaces. We propose log-concave functions as models of fuzzy concepts, showing
that these are the most general choice satisfying a criterion due to
G\&quot;ardenfors and which are well-behaved compositionally. We then generalise
these to define the category of log-concave probabilistic channels between
convex spaces, which allows one to model fuzzy reasoning with noisy inputs, and
provides a novel example of a Markov category.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tull_S/0/1/0/all/0/1&quot;&gt;Sean Tull&lt;/a&gt; (Cambridge Quantum Computing)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.14074">
<title>Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee. (arXiv:2110.14074v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.14074</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing literature of Federated Learning (FL) has recently inspired
Federated Reinforcement Learning (FRL) to encourage multiple agents to
federatively build a better decision-making policy without sharing raw
trajectories. Despite its promising applications, existing works on FRL fail to
I) provide theoretical analysis on its convergence, and II) account for random
system failures and adversarial attacks. Towards this end, we propose the first
FRL framework the convergence of which is guaranteed and tolerant to less than
half of the participating agents being random system failures or adversarial
attackers. We prove that the sample efficiency of the proposed framework is
guaranteed to improve with the number of agents and is able to account for such
potential failures or attacks. All theoretical results are empirically verified
on various RL benchmark tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Flint Xiaofeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yining Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Wei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheston Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1&quot;&gt;Bryan Kian Hsiang Low&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.14904">
<title>MERCURY: Accelerating DNN Training By Exploiting Input Similarity. (arXiv:2110.14904v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2110.14904</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNN) are computationally intensive to train. It
consists of a large number of multidimensional dot products between many
weights and input vectors. However, there can be significant similarity among
input vectors. If one input vector is similar to another, its computations with
the weights are similar to those of the other and, therefore, can be skipped by
reusing the already-computed results. We propose a novel scheme, called
MERCURY, to exploit input similarity during DNN training in a hardware
accelerator. MERCURY uses Random Projection with Quantization (RPQ) to convert
an input vector to a bit sequence, called Signature. A cache (MCACHE) stores
signatures of recent input vectors along with the computed results. If the
Signature of a new input vector matches that of an already existing vector in
the MCACHE, the two vectors are found to have similarities. Therefore, the
already-computed result is reused for the new vector. To the best of our
knowledge, MERCURY is the first work that exploits input similarity using RPQ
for accelerating DNN training in hardware. The paper presents a detailed
design, workflow, and implementation of the MERCURY. Our experimental
evaluation with twelve different deep learning models shows that MERCURY saves
a significant number of computations and speeds up the model training by an
average of 1.97X with an accuracy similar to the baseline system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janfaza_V/0/1/0/all/0/1&quot;&gt;Vahid Janfaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_K/0/1/0/all/0/1&quot;&gt;Kevin Weston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razavi_M/0/1/0/all/0/1&quot;&gt;Moein Razavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1&quot;&gt;Shantanu Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmud_F/0/1/0/all/0/1&quot;&gt;Farabi Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilty_A/0/1/0/all/0/1&quot;&gt;Alex Hilty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muzahid_A/0/1/0/all/0/1&quot;&gt;Abdullah Muzahid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.15829">
<title>Holistic Deep Learning. (arXiv:2110.15829v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.15829</link>
<description rdf:parseType="Literal">&lt;p&gt;There is much interest in deep learning to solve challenges in applying
neural network models in real-world environments. In particular, three areas
have received considerable attention: adversarial robustness, parameter
sparsity, and output stability. Despite numerous attempts to solve these
problems independently, little work simultaneously addresses the challenges. In
this paper, we address the problem of constructing holistic deep learning
models by proposing a novel formulation that solves these issues in
combination. Real-world experiments on both tabular and MNIST datasets show
that our formulation can simultaneously improve the accuracy, robustness,
stability, and sparsity over traditional deep learning models among many
others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1&quot;&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carballo_K/0/1/0/all/0/1&quot;&gt;Kimberly Villalobos Carballo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Michael Lingzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paskov_A/0/1/0/all/0/1&quot;&gt;Alex Paskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paskov_I/0/1/0/all/0/1&quot;&gt;Ivan Paskov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.03699">
<title>A space of goals: the cognitive geometry of informationally bounded agents. (arXiv:2111.03699v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2111.03699</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, Euclidean geometry is treated by scientists as a priori and
objective. However, when we take the position of an agent, the problem of
selecting a best route should also factor in the abilities of the agent, its
embodiment and particularly its cognitive effort. In this paper we consider
geometry in terms of travel between states within a world by incorporating
information processing costs with the appropriate spatial distances. This
induces a geometry that increasingly differs from the original geometry of the
given world as information costs become increasingly important. We visualise
this &quot;cognitive geometry&quot; by projecting it onto 2- and 3-dimensional spaces
showing distinct distortions reflecting the emergence of epistemic and
information-saving strategies as well as pivot states. The analogies between
traditional cost-based geometries and those induced by additional informational
costs invite a generalisation of the notion of geodesics as cheapest routes
towards the notion of infodesics. In this perspective, the concept of
infodesics is inspired by the property of geodesics that, travelling from a
given start location to a given goal location along a geodesic, not only the
goal, but all points along the way are visited at optimal cost from the start.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Archer_K/0/1/0/all/0/1&quot;&gt;Karen Archer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volpi_N/0/1/0/all/0/1&quot;&gt;Nicola Catenacci Volpi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broker_F/0/1/0/all/0/1&quot;&gt;Franziska Br&amp;#xf6;ker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polani_D/0/1/0/all/0/1&quot;&gt;Daniel Polani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.07819">
<title>Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2111.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;A drastic rise in potentially life-threatening misinformation has been a
by-product of the COVID-19 pandemic. Computational support to identify false
information within the massive body of data on the topic is crucial to prevent
harm. Researchers proposed many methods for flagging online misinformation
related to COVID-19. However, these methods predominantly target specific
content types (e.g., news) or platforms (e.g., Twitter). The methods&apos;
capabilities to generalize were largely unclear so far. We evaluate fifteen
Transformer-based models on five COVID-19 misinformation datasets that include
social media posts, news articles, and scientific papers to fill this gap. We
show tokenizers and models tailored to COVID-19 data do not provide a
significant advantage over general-purpose ones. Our study provides a realistic
assessment of models for detecting COVID-19 misinformation. We expect that
evaluating a broad spectrum of datasets and models will benefit future research
in developing misinformation detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1&quot;&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_N/0/1/0/all/0/1&quot;&gt;Nischal Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1&quot;&gt;Terry Ruas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1&quot;&gt;Norman Meuschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1&quot;&gt;Tirthankar Ghosal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1&quot;&gt;Bela Gipp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08588">
<title>Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning. (arXiv:2112.08588v7 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08588</link>
<description rdf:parseType="Literal">&lt;p&gt;A hallmark of intelligence is the ability to autonomously learn new flexible,
cognitive behaviors. Here we define cognitive behaviors as those that require
working memory, that is, behaviors where the appropriate action depends not
just on immediate stimuli (as in simple reflexive stimulus-response
associations), but on memorized contextual information. Such cognitive,
memory-dependent behaviors are by definition meta-learning (``learning to
learn&apos;&apos;) tasks. In typical meta-learning experiments, agents are trained with
an external, human-designed algorithm to acquire one specific cognitive task.
By contrast, animals are able to pick up new cognitive tasks (including tasks
never seen during evolution) automatically, from stimuli and rewards alone,
through the operation of their own evolved internal machinery. Can we harness
this process to generate artificial agents with such abilities? Here we evolve
neural networks, endowed with plastic connections and neuromodulation, over a
sizable set of simple meta-learning tasks based on a framework from
computational neuroscience. The resulting evolved networks can automatically
modify their own connectivity to acquire a novel simple cognitive task, never
seen during evolution, from stimuli and rewards alone, through the spontaneous
operation of their evolved neural organization and plasticity system. We
suggest that attending to the multiplicity of loops involved in natural
learning may provide useful insight into the emergence of intelligent behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miconi_T/0/1/0/all/0/1&quot;&gt;Thomas Miconi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08766">
<title>CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking. (arXiv:2112.08766v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08766</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has been the dominant approach to training dense
retrieval models. In this work, we investigate the impact of ranking context -
an often overlooked aspect of learning dense retrieval models. In particular,
we examine the effect of its constituent parts: jointly scoring a large number
of negatives per query, using retrieved (query-specific) instead of random
negatives, and a fully list-wise loss. To incorporate these factors into
training, we introduce Contextual Document Embedding Reranking (CODER), a
highly efficient retrieval framework. When reranking, it incurs only a
negligible computational overhead on top of a first-stage method at run time
(delay per query in the order of milliseconds), allowing it to be easily
combined with any state-of-the-art dual encoder method. After fine-tuning
through CODER, which is a lightweight and fast process, models can also be used
as stand-alone retrievers. Evaluating CODER in a large set of experiments on
the MS~MARCO and TripClick collections, we show that the contextual reranking
of precomputed document embeddings leads to a significant improvement in
retrieval performance. This improvement becomes even more pronounced when more
relevance information per query is available, shown in the TripClick
collection, where we establish new state-of-the-art results by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1&quot;&gt;George Zerveas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1&quot;&gt;Navid Rekabsaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1&quot;&gt;Carsten Eickhoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09130">
<title>Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review. (arXiv:2201.09130v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09130</link>
<description rdf:parseType="Literal">&lt;p&gt;Death by suicide is the seventh leading death cause worldwide. The recent
advancement in Artificial Intelligence (AI), specifically AI applications in
image and voice processing, has created a promising opportunity to
revolutionize suicide risk assessment. Subsequently, we have witnessed
fast-growing literature of research that applies AI to extract audiovisual
non-verbal cues for mental illness assessment. However, the majority of the
recent works focus on depression, despite the evident difference between
depression symptoms and suicidal behavior and non-verbal cues. This paper
reviews recent works that study suicide ideation and suicide behavior detection
through audiovisual feature analysis, mainly suicidal voice/speech acoustic
features analysis and suicidal visual cues. Automatic suicide assessment is a
promising research direction that is still in the early stages. Accordingly,
there is a lack of large datasets that can be used to train machine learning
and deep learning models proven to be effective in other, similar tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhelim_S/0/1/0/all/0/1&quot;&gt;Sahraoui Dhelim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1&quot;&gt;Huansheng Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugent_C/0/1/0/all/0/1&quot;&gt;Chris Nugent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.08176">
<title>Bias and unfairness in machine learning models: a systematic literature review. (arXiv:2202.08176v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.08176</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the difficulties of artificial intelligence is to ensure that model
decisions are fair and free of bias. In research, datasets, metrics,
techniques, and tools are applied to detect and mitigate algorithmic unfairness
and bias. This study aims to examine existing knowledge on bias and unfairness
in Machine Learning models, identifying mitigation methods, fairness metrics,
and supporting tools. A Systematic Literature Review found 40 eligible articles
published between 2017 and 2022 in the Scopus, IEEE Xplore, Web of Science, and
Google Scholar knowledge bases. The results show numerous bias and unfairness
detection and mitigation approaches for ML technologies, with clearly defined
metrics in the literature, and varied metrics can be highlighted. We recommend
further research to define the techniques and metrics that should be employed
in each case to standardize and ensure the impartiality of the machine learning
model, thus, allowing the most appropriate metric to detect bias and unfairness
in a given context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagano_T/0/1/0/all/0/1&quot;&gt;Tiago Palma Pagano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loureiro_R/0/1/0/all/0/1&quot;&gt;Rafael Bessa Loureiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisboa_F/0/1/0/all/0/1&quot;&gt;Fernanda Vit&amp;#xf3;ria Nascimento Lisboa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_G/0/1/0/all/0/1&quot;&gt;Gustavo Oliveira Ramos Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peixoto_R/0/1/0/all/0/1&quot;&gt;Rodrigo Matos Peixoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guimaraes_G/0/1/0/all/0/1&quot;&gt;Guilherme Arag&amp;#xe3;o de Sousa Guimar&amp;#xe3;es&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1&quot;&gt;Lucas Lisboa dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_M/0/1/0/all/0/1&quot;&gt;Maira Matos Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_M/0/1/0/all/0/1&quot;&gt;Marco Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_E/0/1/0/all/0/1&quot;&gt;Ewerton Lopes Silva de Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkler_I/0/1/0/all/0/1&quot;&gt;Ingrid Winkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1&quot;&gt;Erick Giovani Sperandio Nascimento&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.03471">
<title>DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2204.03471</link>
<description rdf:parseType="Literal">&lt;p&gt;Adopting reinforcement learning (RL) for traffic signal control (TSC) is
increasingly popular, and RL has become a promising solution for traffic signal
control. However, several challenges still need to be overcome. Firstly, most
RL methods use fixed action duration and select the green phase for the next
state, which makes the phase duration less dynamic and flexible. Secondly, the
phase sequence of RL methods can be arbitrary, affecting the real-world
deployment which may require a cyclical phase structure. Lastly, the average
travel time and throughput are not fair metrics to evaluate TSC performance. To
address these challenges, we propose a multi-level traffic signal control
framework, DynLight, which uses an optimization method Max-QueueLength (M-QL)
to determine the phase and uses a deep Q-network to determine the duration of
the corresponding phase. Based on DynLight, we further propose DynLight-C which
adopts a well-trained deep Q-network of DynLight and replace M-QL with a
cyclical control policy that actuates a set of phases in fixed cyclical order
to realize cyclical phase structure. Comprehensive experiments on multiple
real-world datasets demonstrate that DynLight achieves a new state-of-the-art.
Furthermore, the deep Q-network of DynLight can learn well on determining the
phase duration and DynLight-C demonstrates high performance for deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shubin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jianming Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10189">
<title>Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10189</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve psychotherapy
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Baihan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1&quot;&gt;Djallel Bouneffouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1&quot;&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tejwani_R/0/1/0/all/0/1&quot;&gt;Ravi Tejwani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03770">
<title>Transformer-Empowered 6G Intelligent Networks: From Massive MIMO Processing to Semantic Communication. (arXiv:2205.03770v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;It is anticipated that 6G wireless networks will accelerate the convergence
of the physical and cyber worlds and enable a paradigm-shift in the way we
deploy and exploit communication networks. Machine learning, in particular deep
learning (DL), is expected to be one of the key technological enablers of 6G by
offering a new paradigm for the design and optimization of networks with a high
level of intelligence. In this article, we introduce an emerging DL
architecture, known as the transformer, and discuss its potential impact on 6G
network design. We first discuss the differences between the transformer and
classical DL architectures, and emphasize the transformer&apos;s self-attention
mechanism and strong representation capabilities, which make it particularly
appealing for tackling various challenges in wireless network design.
Specifically, we propose transformer-based solutions for various massive
multiple-input multiple-output (MIMO) and semantic communication problems, and
show their superiority compared to other architectures. Finally, we discuss key
challenges and open issues in transformer-based solutions, and identify future
research directions for their deployment in intelligent 6G networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dezhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1&quot;&gt;H. Vincent Poor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11930">
<title>The Authenticity Gap in Human Evaluation. (arXiv:2205.11930v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11930</link>
<description rdf:parseType="Literal">&lt;p&gt;Human ratings are the gold standard in NLG evaluation. The standard protocol
is to collect ratings of generated text, average across annotators, and rank
NLG systems by their average scores. However, little consideration has been
given as to whether this approach faithfully captures human preferences.
Analyzing this standard protocol through the lens of utility theory in
economics, we identify the implicit assumptions it makes about annotators.
These assumptions are often violated in practice, in which case annotator
ratings cease to reflect their preferences. The most egregious violations come
from using Likert scales, which provably reverse the direction of the true
preference in certain cases. We suggest improvements to the standard protocol
to make it more theoretically sound, but even in its improved form, it cannot
be used to evaluate open-ended tasks like story generation. For the latter, we
propose a new human evaluation protocol called $\textit{system-level
probabilistic assessment}$ (SPA). When human evaluation of stories is done with
SPA, we can recover the ordering of GPT-3 models by size, with statistically
significant results. However, when human evaluation is done with the standard
protocol, less than half of the expected preferences can be recovered (e.g.,
there is no significant difference between $\texttt{curie}$ and
$\texttt{davinci}$, despite using a highly powered test).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1&quot;&gt;Kawin Ethayarajh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1&quot;&gt;Dan Jurafsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13255">
<title>Active Labeling: Streaming Stochastic Gradients. (arXiv:2205.13255v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13255</link>
<description rdf:parseType="Literal">&lt;p&gt;The workhorse of machine learning is stochastic gradient descent. To access
stochastic gradients, it is common to consider iteratively input/output pairs
of a training dataset. Interestingly, it appears that one does not need full
supervision to access stochastic gradients, which is the main motivation of
this paper. After formalizing the &quot;active labeling&quot; problem, which focuses on
active learning with partial supervision, we provide a streaming technique that
provably minimizes the ratio of generalization error over the number of
samples. We illustrate our technique in depth for robust regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1&quot;&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1&quot;&gt;Vianney Perchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13561">
<title>Physics-Guided Hierarchical Reward Mechanism for Learning-Based Object Grasping. (arXiv:2205.13561v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13561</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based grasping can afford real-time motion planning of
multi-fingered robotics hands thanks to its high computational efficiency.
However, it needs to explore large search spaces during its learning process.
The search space causes low learning efficiency, which has been the main
barrier to its practical adoption. In addition, the generalizability of the
trained policy is limited unless they are identical or similar to the trained
objects. In this work, we develop a novel Physics-Guided Deep Reinforcement
Learning with a Hierarchical Reward Mechanism to improve the learning
efficiency and generalizability for learning-based autonomous grasping. Unlike
conventional observation-based grasp learning, physics-informed metrics are
utilized to convey correlations between features associated with hand
structures and objects to improve learning efficiency and outcomes. Further, a
hierarchical reward mechanism is developed to enable the robot to learn the
grasping task in a prioritized way. It is validated in grasping tasks with a
MICO robot arm in both simulation and physical experiments. The results show
that our method outperformed the standard Deep Reinforcement learning method in
task performance by 48% and learning efficiency by 40%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1&quot;&gt;Yunsik Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Lingfeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_M/0/1/0/all/0/1&quot;&gt;Michael Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiucai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03216">
<title>Data Governance in the Age of Large-Scale Data-Driven Language Technology. (arXiv:2206.03216v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03216</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent emergence and adoption of Machine Learning technology, and
specifically of Large Language Models, has drawn attention to the need for
systematic and transparent management of language data. This work proposes an
approach to global language data governance that attempts to organize data
management amongst stakeholders, values, and rights. Our proposal is informed
by prior work on distributed governance that accounts for human values and
grounded by an international research collaboration that brings together
researchers and practitioners from 60 countries. The framework we present is a
multi-party international governance structure focused on language data, and
incorporating technical and organizational tools needed to support its work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1&quot;&gt;Yacine Jernite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1&quot;&gt;Stella Biderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1&quot;&gt;Anna Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1&quot;&gt;Maraim Masoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danchev_V/0/1/0/all/0/1&quot;&gt;Valentin Danchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Samson Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1&quot;&gt;Alexandra Sasha Luccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1&quot;&gt;Nishant Subramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupont_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;rard Dupont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1&quot;&gt;Jesse Dodge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1&quot;&gt;Zeerak Talat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_I/0/1/0/all/0/1&quot;&gt;Isaac Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikpoor_S/0/1/0/all/0/1&quot;&gt;Somaieh Nikpoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohberg_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Frohberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1&quot;&gt;Aaron Gokaslan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1&quot;&gt;Peter Henderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1&quot;&gt;Rishi Bommasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Margaret Mitchell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.05800">
<title>Long-Horizon Manipulation Planning with Functional Object-Oriented Networks. (arXiv:2207.05800v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2207.05800</link>
<description rdf:parseType="Literal">&lt;p&gt;Following work on joint object-action representation, functional
object-oriented networks (FOON) were introduced as a knowledge representation
for robots. A FOON contains symbolic (high-level) concepts useful to a robot&apos;s
understanding of tasks and its environment for object-level planning. Prior to
this work, little has been done to show how plans acquired from FOON can be
executed by a robot, as the concepts in a FOON are too abstract for immediate
execution. We propose a hierarchical task planning approach that translates a
FOON graph into a PDDL-based representation of domain knowledge for
manipulation planning. As a result of this process, a manipulation plan can be
acquired, which can be executed by a robot from start to end, leveraging the
use of action contexts and skills as dynamic movement primitives (DMPs). We
demonstrate the entire pipeline from planning to execution using CoppeliaSim
and show how learned action contexts can be extended to never-before-seen
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1&quot;&gt;David Paulius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostini_A/0/1/0/all/0/1&quot;&gt;Alejandro Agostini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongheui Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08581">
<title>Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-Ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy, loss and area under the curve) and reduction of execution
times will be studied with respect to the classical case (the centralized
approach). Different clients will be simulated from the training data, selected
in an unbalanced manner. The results of considering three or ten clients are
exposed and compared between them and against the centralized case. Two
different problems related to intermittent clients are discussed, together with
two approaches to be followed for each of them. Specifically, this type of
problems may occur because in a real scenario some clients may leave the
training, and others enter it, and on the other hand because of client
technical or connectivity problems. Finally, improvements and future work in
the field are proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Judith S&amp;#xe1;inz-Pardo D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro L&amp;#xf3;pez Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00843">
<title>Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards. (arXiv:2208.00843v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00843</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploration with sparse rewards remains a challenging research problem in
reinforcement learning (RL). Especially for sequential object manipulation
tasks, the RL agent always receives negative rewards until completing all
sub-tasks, which results in low exploration efficiency. To solve these tasks
efficiently, we propose a novel self-guided continual RL framework, RelayHER
(RHER). RHER first decomposes a sequential task into new sub-tasks with
increasing complexity and ensures that the simplest sub-task can be learned
quickly by utilizing Hindsight Experience Replay (HER). Secondly, we design a
multi-goal &amp;amp; multi-task network to learn these sub-tasks simultaneously.
Finally, we propose a Self-Guided Exploration Strategy (SGES). With SGES, the
learned sub-task policy will guide the agent to the states that are helpful to
learn more complex sub-task with HER. By this self-guided exploration and relay
policy learning, RHER can solve these sequential tasks efficiently stage by
stage. The experimental results show that RHER significantly outperforms
vanilla-HER in sample-efficiency on five singleobject and five complex
multi-object manipulation tasks (e.g., Push, Insert, ObstaclePush, Stack,
TStack, etc.). The proposed RHER has also been applied to learn a contact-rich
push task on a physical robot from scratch, and the success rate reached 10/10
with only 250 episodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yongle Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_K/0/1/0/all/0/1&quot;&gt;Kun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1&quot;&gt;Erkang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07628">
<title>FALCON: Sound and Complete Neural Semantic Entailment over ALC Ontologies. (arXiv:2208.07628v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07628</link>
<description rdf:parseType="Literal">&lt;p&gt;Many ontologies, i.e., Description Logic (DL) knowledge bases, have been
developed to provide rich knowledge about various domains, and a lot of them
are based on ALC, i.e., a prototypical and expressive DL, or its extensions.
The main task that explores ALC ontologies is to compute semantic entailment.
Symbolic approaches can guarantee sound and complete semantic entailment but
are sensitive to inconsistency and missing information. To this end, we propose
FALCON, a Fuzzy ALC Ontology Neural reasoner. FALCON uses fuzzy logic operators
to generate single model structures for arbitrary ALC ontologies, and uses
multiple model structures to compute semantic entailments. Theoretical results
demonstrate that FALCON is guaranteed to be a sound and complete algorithm for
computing semantic entailments over ALC ontologies. Experimental results show
that FALCON enables not only approximate reasoning (reasoning over incomplete
ontologies) and paraconsistent reasoning (reasoning over inconsistent
ontologies), but also improves machine learning in the biomedical domain by
incorporating background knowledge from ALC ontologies. FALCON is available at
https://github.com/bio-ontology-research-group/FALCON.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinnerichs_T/0/1/0/all/0/1&quot;&gt;Tilman Hinnerichs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehndorf_R/0/1/0/all/0/1&quot;&gt;Robert Hoehndorf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11764">
<title>Taking the Intentional Stance Seriously: A Guide to Progress in Artificial Intelligence. (arXiv:2209.11764v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding claims that researchers have made considerable progress in artificial
intelligence over the last several decades is easy. However, our everyday
interactions with cognitive systems (e.g., Siri, Alexa, DALL-E) quickly move
from intriguing to frustrating. One cause of those frustrations rests in a
mismatch between the expectations we have due to our inherent,
folk-psychological theories and the real limitations we experience with
existing computer programs. The software does not understand that people have
goals, beliefs about how to achieve those goals, and intentions to act
accordingly. One way to align cognitive systems with our expectations is to
imbue them with mental states that mirror those we use to predict and explain
human behavior. This paper discusses these concerns and illustrates the
challenge of following this route by analyzing the mental state &apos;intention.&apos;
That analysis is joined with high-level methodological suggestions that support
progress in this endeavor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bridewell_W/0/1/0/all/0/1&quot;&gt;Will Bridewell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13511">
<title>Phy-Taylor: Physics-Model-Based Deep Neural Networks. (arXiv:2209.13511v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13511</link>
<description rdf:parseType="Literal">&lt;p&gt;Purely data-driven deep neural networks (DNNs) applied to physical
engineering systems can infer relations that violate physics laws, thus leading
to unexpected consequences. To address this challenge, we propose a
physics-model-based DNN framework, called Phy-Taylor, that accelerates learning
compliant representations with physical knowledge. The Phy-Taylor framework
makes two key contributions; it introduces a new architectural
Physics-compatible neural network (PhN), and features a novel compliance
mechanism, we call {\em Physics-guided Neural Network Editing\}. The PhN aims
to directly capture nonlinearities inspired by physical quantities, such as
kinetic energy, potential energy, electrical power, and aerodynamic drag force.
To do so, the PhN augments neural network layers with two key components: (i)
monomials of Taylor series expansion of nonlinear functions capturing physical
knowledge, and (ii) a suppressor for mitigating the influence of noise. The
neural-network editing mechanism further modifies network links and activation
functions consistently with physical knowledge. As an extension, we also
propose a self-correcting Phy-Taylor framework that introduces two additional
capabilities: (i) physics-model-based safety relationship learning, and (ii)
automatic output correction when violations of safety occur. Through
experiments, we show that (by expressing hard-to-learn nonlinearities directly
and by constraining dependencies) Phy-Taylor features considerably fewer
parameters, and a remarkably accelerated training process, while offering
enhanced model robustness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yanbing Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1&quot;&gt;Lui Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Huajie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelzaher_T/0/1/0/all/0/1&quot;&gt;Tarek Abdelzaher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14610">
<title>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. (arXiv:2209.14610v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14610</link>
<description rdf:parseType="Literal">&lt;p&gt;Mathematical reasoning, a core ability of human intelligence, presents unique
challenges for machines in abstract thinking and logical reasoning. Recent
large pre-trained language models such as GPT-3 have achieved remarkable
progress on mathematical reasoning tasks written in text form, such as math
word problems (MWP). However, it is unknown if the models can handle more
complex problems that involve math reasoning over heterogeneous information,
such as tabular data. To fill the gap, we present Tabular Math Word Problems
(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that
require mathematical reasoning on both textual and tabular data. Each question
in TabMWP is aligned with a tabular context, which is presented as an image,
semi-structured text, and a structured table. There are two types of questions:
free-text and multi-choice, and each problem is annotated with gold solutions
to reveal the multi-step reasoning process. We evaluate different pre-trained
models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier
studies suggest, since few-shot GPT-3 relies on the selection of in-context
examples, its performance is unstable and can degrade to near chance. The
unstable issue is more severe when handling complex problems like TabMWP. To
mitigate this, we further propose a novel approach, PromptPG, which utilizes
policy gradient to learn to select in-context examples from a small amount of
training data and then constructs the corresponding prompt for the test
example. Experimental results show that our method outperforms the best
baseline by 5.31% on the accuracy metric and reduces the prediction variance
significantly compared to random selection, which verifies its effectiveness in
the selection of in-context examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Liang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1&quot;&gt;Tanmay Rajpurohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1&quot;&gt;Ashwin Kalyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03568">
<title>How Large Language Models are Transforming Machine-Paraphrased Plagiarism. (arXiv:2210.03568v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03568</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of large language models for text generation poses a
severe threat to academic integrity, as plagiarists can generate realistic
paraphrases indistinguishable from original work. However, the role of large
autoregressive transformers in generating machine-paraphrased plagiarism and
their detection is still developing in the literature. This work explores T5
and GPT-3 for machine-paraphrase generation on scientific articles from arXiv,
student theses, and Wikipedia. We evaluate the detection performance of six
automated solutions and one commercial plagiarism detection software and
perform a human study with 105 participants regarding their detection
performance and the quality of generated examples. Our results suggest that
large models can rewrite text humans have difficulty identifying as
machine-paraphrased (53% mean acc.). Human experts rate the quality of
paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5,
fluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)
achieves a 66% F1-score in detecting paraphrases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1&quot;&gt;Jan Philip Wahle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1&quot;&gt;Terry Ruas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirstein_F/0/1/0/all/0/1&quot;&gt;Frederic Kirstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1&quot;&gt;Bela Gipp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04590">
<title>The Small Solution Hypothesis for MAPF on Strongly Connected Directed Graphs Is True. (arXiv:2210.04590v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;The determination of the computational complexity of multi-agent pathfinding
on directed graphs (diMAPF) has been an open research problem for many years.
While diMAPF has been shown to be polynomial for some special cases, only
recently, it has been established that the problem is NP-hard in general.
Further, it has been proved that diMAPF will be in NP if the short solution
hypothesis for strongly connected directed graphs is correct. In this paper, it
is shown that this hypothesis is indeed true, even when one allows for
synchronous rotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nebel_B/0/1/0/all/0/1&quot;&gt;Bernhard Nebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07729">
<title>Model-Based Imitation Learning for Urban Driving. (arXiv:2210.07729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;An accurate model of the environment and the dynamic agents acting in it
offers great potential for improving motion planning. We present MILE: a
Model-based Imitation LEarning approach to jointly learn a model of the world
and a policy for autonomous driving. Our method leverages 3D geometry as an
inductive bias and learns a highly compact latent space directly from
high-resolution videos of expert demonstrations. Our model is trained on an
offline corpus of urban driving data, without any online interaction with the
environment. MILE improves upon prior state-of-the-art by 31% in driving score
on the CARLA simulator when deployed in a completely new town and new weather
conditions. Our model can predict diverse and plausible states and actions,
that can be interpretably decoded to bird&apos;s-eye view semantic segmentation.
Further, we demonstrate that it can execute complex driving manoeuvres from
plans entirely predicted in imagination. Our approach is the first camera-only
method that models static scene, dynamic scene, and ego-behaviour in an urban
driving environment. The code and model weights are available at
https://github.com/wayveai/mile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Anthony Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Gianluca Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_N/0/1/0/all/0/1&quot;&gt;Nicolas Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1&quot;&gt;Zak Murez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurau_C/0/1/0/all/0/1&quot;&gt;Corina Gurau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1&quot;&gt;Hudson Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1&quot;&gt;Alex Kendall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1&quot;&gt;Jamie Shotton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08471">
<title>Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08471</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Di Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rumei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1&quot;&gt;Minlong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongxin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10678">
<title>Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10678</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11277">
<title>TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition. (arXiv:2210.11277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11277</link>
<description rdf:parseType="Literal">&lt;p&gt;Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiabao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yabin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kui Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11947">
<title>Generalizing over Long Tail Concepts for Medical Term Normalization. (arXiv:2210.11947v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11947</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical term normalization consists in mapping a piece of text to a large
number of output classes. Given the small size of the annotated datasets and
the extremely long tail distribution of the concepts, it is of utmost
importance to develop models that are capable to generalize to scarce or unseen
concepts. An important attribute of most target ontologies is their
hierarchical structure. In this paper we introduce a simple and effective
learning strategy that leverages such information to enhance the
generalizability of both discriminative and generative models. The evaluation
shows that the proposed strategy produces state-of-the-art performance on seen
concepts and consistent improvements on unseen ones, allowing also for
efficient zero-shot knowledge transfer across text typologies and datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1&quot;&gt;Beatrice Portelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1&quot;&gt;Simone Scaboro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1&quot;&gt;Enrico Santus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedghamiz_H/0/1/0/all/0/1&quot;&gt;Hooman Sedghamiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1&quot;&gt;Emmanuele Chersoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1&quot;&gt;Giuseppe Serra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13768">
<title>GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks. (arXiv:2210.13768v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13768</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) have been studied over decades to incorporate
their biological plausibility and leverage their promising energy efficiency.
Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly
adopted to formulate the spiking neuron and evolves into numerous variants with
different biological features. However, most LIF-based neurons support only
single biological feature in different neuronal behaviors, limiting their
expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF,
a unified spiking neuron, to fuse different bio-features in different neuronal
behaviors, enlarging the representation space of spiking neurons. In GLIF,
gating factors, which are exploited to determine the proportion of the fused
bio-features, are learnable during training. Combining all learnable
membrane-related parameters, our method can make spiking neurons different and
constantly changing, thus increasing the heterogeneity and adaptivity of
spiking neurons. Extensive experiments on a variety of datasets demonstrate
that our method obtains superior performance compared with other SNNs by simply
changing their neuronal formulations to GLIF. In particular, we train a spiking
ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on
CIFAR-100, which has advanced the state-of-the-art. Codes are available at
\url{https://github.com/Ikarosy/Gated-LIF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xingting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fanrong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1&quot;&gt;Zitao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13944">
<title>A Survey on Artificial Intelligence for Music Generation: Agents, Domains and Perspectives. (arXiv:2210.13944v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13944</link>
<description rdf:parseType="Literal">&lt;p&gt;Music is one of the Gardner&apos;s intelligences in his theory of multiple
intelligences. How humans perceive and understand music is still being studied
and is crucial to develop artificial intelligence models that imitate such
processes. Music generation with Artificial Intelligence is an emerging field
that is gaining much attention in the recent years. In this paper, we describe
how humans compose music and how new AI systems could imitate such process by
comparing past and recent advances in the field with music composition
techniques. To understand how AI models and algorithms generate music and the
potential applications that might appear in the future, we explore, analyze and
describe the agents that take part of the music generation process: the
datasets, models, interfaces, the users and the generated music. We mention
possible applications that might benefit from this field and we also propose
new trends and future research directions that could be explored in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_C/0/1/0/all/0/1&quot;&gt;Carlos Hernandez-Olivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_J/0/1/0/all/0/1&quot;&gt;Javier Hernandez-Olivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1&quot;&gt;Jose R. Beltran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14431">
<title>$N$-gram Is Back: Residual Learning of Neural Text Generation with $n$-gram Language Model. (arXiv:2210.14431v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14431</link>
<description rdf:parseType="Literal">&lt;p&gt;$N$-gram language models (LM) have been largely superseded by neural LMs as
the latter exhibits better performance. However, we find that $n$-gram models
can achieve satisfactory performance on a large proportion of testing cases,
indicating they have already captured abundant knowledge of the language with
relatively low computational cost. With this observation, we propose to learn a
neural LM that fits the residual between an $n$-gram LM and the real-data
distribution. The combination of $n$-gram and neural LMs not only allows the
neural part to focus on the deeper understanding of language but also provides
a flexible way to customize an LM by switching the underlying $n$-gram model
without changing the neural model. Experimental results on three typical
language tasks (i.e., language modeling, machine translation, and
summarization) demonstrate that our approach attains additional performance
gains over popular standalone neural models consistently. We also show that our
approach allows for effective domain adaptation by simply switching to a
domain-specific $n$-gram model, without any extra training. Our code is
released at https://github.com/ghrua/NgramRes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16508">
<title>Clenshaw Graph Neural Networks. (arXiv:2210.16508v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16508</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Convolutional Networks (GCNs), which use a message-passing paradigm
with stacked convolution layers, are foundational methods for learning graph
representations. Recent GCN models use various residual connection techniques
to alleviate the model degradation problem such as over-smoothing and gradient
vanishing. Existing residual connection techniques, however, fail to make
extensive use of underlying graph structure as in the graph spectral domain,
which is critical for obtaining satisfactory results on heterophilic graphs. In
this paper, we introduce ClenshawGCN, a GNN model that employs the Clenshaw
Summation Algorithm to enhance the expressiveness of the GCN model. ClenshawGCN
equips the standard GCN model with two straightforward residual modules: the
adaptive initial residual connection and the negative second-order residual
connection. We show that by adding these two residual modules, ClenshawGCN
implicitly simulates a polynomial filter under the Chebyshev basis, giving it
at least as much expressive power as polynomial spectral GNNs. In addition, we
conduct comprehensive experiments to demonstrate the superiority of our model
over spatial and spectral GNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16993">
<title>STN: a new tensor network method to identify stimulus category from brain activity pattern. (arXiv:2210.16993v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16993</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural decoding is still a challenge and hot topic in neurocomputing science.
Recently, many studies have shown that brain network pattern containing rich
spatial and temporal structure information, which represented the activation
information of brain under external stimuli. The traditional method is to
extract brain network features directly from the common machine learning
method, then put these features into the classifier, and realize to decode
external stimuli. However, this method cannot effectively extract the
multi-dimensional structural information, which is hidden in the brain network.
The tensor researchers show that the tensor decomposition model can fully mine
unique spatio-temporal structure characteristics in multi-dimensional structure
data. This research proposed a stimulus constrain tensor brain model(STN),
which involved the tensor decomposition idea and stimulus category constraint
information. The model was verified on the real neuroimaging data sets (MEG and
fMRI). The experimental results show that the STN model achieved more 11.06%
and 18.46% compared with others methods on two modal data sets. These results
imply the superiority of extracting discriminative characteristics about STN
model, especially for decoding object stimuli with semantic information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chunyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiacai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17168">
<title>SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking. (arXiv:2210.17168v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17168</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has
widespread applications. Existing systems typically utilize BERT for text
encoding. However, CSC requires the model to account for both phonetic and
graphemic information. To adapt BERT to the CSC task, we propose a token-level
self-distillation contrastive learning method. We employ BERT to encode both
the corrupted and corresponding correct sentence. Then, we use contrastive
learning loss to regularize corrupted tokens&apos; hidden states to be closer to
counterparts in the correct sentence. On three CSC datasets, we confirmed our
method provides a significant improvement above baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaotian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00384">
<title>The future is different: Large pre-trained language models fail in prediction tasks. (arXiv:2211.00384v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00384</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained language models (LPLM) have shown spectacular success when
fine-tuned on downstream supervised tasks. Yet, it is known that their
performance can drastically drop when there is a distribution shift between the
data used during training and that used at inference time. In this paper we
focus on data distributions that naturally change over time and introduce four
new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and
POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display
average performance drops of about 88% (in the best case!) when predicting the
popularity of future posts from sub-reddits whose topic distribution changes
with time. We then introduce a simple methodology that leverages neural
variational dynamic topic models and attention mechanisms to infer temporal
language model representations for regression tasks. Our models display
performance drops of only about 40% in the worst cases (2% in the best ones)
when predicting the popularity of future posts, while using only about 7% of
the total number of parameters of LPLM and providing interpretable
representations that offer insight into real-world events, like the GameStop
short squeeze of 2021
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1&quot;&gt;Kostadin Cvejoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1&quot;&gt;Rams&amp;#xe9;s J. S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Ojeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00732">
<title>Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;Online encyclopedias, such as Wikipedia, have been well-developed and
researched in the last two decades. One can find any attributes or other
information of a wiki item on a wiki page edited by a community of volunteers.
However, the traditional text, images and tables can hardly express some
aspects of an wiki item. For example, when we talk about ``Shiba Inu&apos;&apos;, one may
care more about ``How to feed it&apos;&apos; or ``How to train it not to protect its
food&apos;&apos;. Currently, short-video platforms have become a hallmark in the online
world. Whether you&apos;re on TikTok, Instagram, Kuaishou, or YouTube Shorts,
short-video apps have changed how we consume and create content today. Except
for producing short videos for entertainment, we can find more and more authors
sharing insightful knowledge widely across all walks of life. These short
videos, which we call knowledge videos, can easily express any aspects (e.g.
hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and
they can be systematically analyzed and organized like an online encyclopedia.
In this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia
consisting of items, aspects, and short videos lined to them, which was
extracted from billions of videos of Kuaishou (Kwai), a well-known short-video
platform in China. We first collected items from multiple sources and mined
user-centered aspects from millions of users&apos; queries to build an item-aspect
tree. Then we propose a new task called ``multi-modal item-aspect linking&apos;&apos; as
an expansion of ``entity linking&apos;&apos; to link short videos into item-aspect pairs
and build the whole short-video encyclopedia. Intrinsic evaluations show that
our encyclopedia is of large scale and highly accurate. We also conduct
sufficient extrinsic experiments to show how Kuaipedia can help fundamental
applications such as entity typing and entity linking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haojie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zepeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruiji Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00915">
<title>Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00915</link>
<description rdf:parseType="Literal">&lt;p&gt;Retriever-reader models achieve competitive performance across many different
NLP tasks such as open question answering and dialogue conversations. In this
work, we notice these models easily overfit the top-rank retrieval passages and
standard training fails to reason over the entire retrieval passages. We
introduce a learnable passage mask mechanism which desensitizes the impact from
the top-rank retrieval passages and prevents the model from overfitting.
Controlling the gradient variance with fewer mask candidates and selecting the
mask candidates with one-shot bi-level optimization, our learnable
regularization strategy enforces the answer generation to focus on the entire
retrieval passages. Experiments on different tasks across open question
answering, dialogue conversation, and fact verification show that our method
consistently outperforms its baselines. Extensive experiments and ablation
studies demonstrate that our method can be general, effective, and beneficial
for many NLP tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shujian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chengyue Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xingchao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00924">
<title>SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory. (arXiv:2211.00924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00924</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of talking face generation from speech lies in aligning two
different modal information, audio and video, such that the mouth region
corresponds to input audio. Previous methods either exploit audio-visual
representation learning or leverage intermediate structural information such as
landmarks and 3D models. However, they struggle to synthesize fine details of
the lips varying at the phoneme level as they do not sufficiently provide
visual information of the lips at the video synthesis step. To overcome this
limitation, our work proposes Audio-Lip Memory that brings in visual
information of the mouth region corresponding to input audio and enforces
fine-grained audio-visual coherence. It stores lip motion features from
sequential ground truth images in the value memory and aligns them with
corresponding audio features so that they can be retrieved using audio input at
inference time. Therefore, using the retrieved lip motion features as visual
hints, it can easily correlate audio with visual dynamics in the synthesis
step. By analyzing the memory, we demonstrate that unique lip features are
stored in each memory slot at the phoneme level, capturing subtle lip motion
based on memory addressing. In addition, we introduce visual-visual
synchronization loss which can enhance lip-syncing performance when used along
with audio-visual synchronization loss in our model. Extensive experiments are
performed to verify that our method generates high-quality video with mouth
shapes that best align with the input audio, outperforming previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Joanna Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01334">
<title>MemoNet:Memorizing Representations of All Cross Features Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01334</link>
<description rdf:parseType="Literal">&lt;p&gt;New findings in natural language processing(NLP) demonstrate that the strong
memorization capability contributes a lot to the success of large language
models.This inspires us to explicitly bring an independent memory mechanism
into CTR ranking model to learn and memorize all cross
features&apos;representations. In this paper,we propose multi-Hash Codebook
NETwork(HCNet) as the memory mechanism for efficiently learning and memorizing
representations of all cross features in CTR tasks.HCNet uses multi-hash
codebook as the main memory place and the whole memory procedure consists of
three phases: multi-hash addressing,memory restoring and feature
shrinking.HCNet can be regarded as a general module and can be incorporated
into any current deep CTR model.We also propose a new CTR model named MemoNet
which combines HCNet with a DNN backbone.Extensive experimental results on
three public datasets show that MemoNet reaches superior performance over
state-of-the-art approaches and validate the effectiveness of HCNet as a strong
memory module.Besides, MemoNet shows the prominent feature of big models in
NLP,which means we can enlarge the size of codebook in HCNet to sustainably
obtain performance gains.Our work demonstrates the importance and feasibility
of learning and memorizing representations of all cross features ,which sheds
light on a new promising research direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengtao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junlin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1906.11898">
<title>InsectUp: Crowdsourcing Insect Observations to Assess Demographic Shifts and Improve Classification. (arXiv:1906.11898v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1906.11898</link>
<description rdf:parseType="Literal">&lt;p&gt;Insects play such a crucial role in ecosystems that a shift in demography of
just a few species can have devastating consequences at environmental, social
and economic levels. Despite this, evaluation of insect demography is strongly
limited by the difficulty of collecting census data at sufficient scale. We
propose a method to gather and leverage observations from bystanders, hikers,
and entomology enthusiasts in order to provide researchers with data that could
significantly help anticipate and identify environmental threats. Finally, we
show that there is indeed interest on both sides for such collaboration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giro_Larraz_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Giro-Larraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guille_Escuret_C/0/1/0/all/0/1&quot;&gt;Charles Guille-Escuret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1&quot;&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kegl_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs K&amp;#xe9;gl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.02849">
<title>Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition. (arXiv:2201.02849v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2201.02849</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing the dependencies between joints is critical in skeleton-based
action recognition task. Transformer shows great potential to model the
correlation of important joints. However, the existing Transformer-based
methods cannot capture the correlation of different joints between frames,
which the correlation is very useful since different body parts (such as the
arms and legs in &quot;long jump&quot;) between adjacent frames move together. Focus on
this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is
proposed. The skeleton sequence is divided into several parts, and several
consecutive frames contained in each part are encoded. And then a
spatio-temporal tuples self-attention module is proposed to capture the
relationship of different joints in consecutive frames. In addition, a feature
aggregation module is introduced between non-adjacent frames to enhance the
ability to distinguish similar actions. Compared with the state-of-the-art
methods, our method achieves better performance on two large-scale datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Helei Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Biao Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1&quot;&gt;Bo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17145">
<title>Probability-Dependent Gradient Decay in Large Margin Softmax. (arXiv:2210.17145v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2210.17145</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, Softmax has become a common component in neural
network frameworks. In this paper, a gradient decay hyperparameter is
introduced in Softmax to control the probability-dependent gradient decay rate
during training. By following the theoretical analysis and empirical results of
a variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we
find that the generalization performance depends significantly on the gradient
decay rate as the confidence probability rises, i.e., the gradient decreases
convexly or concavely as the sample probability increases. Moreover,
optimization with the small gradient decay shows a similar curriculum learning
sequence where hard samples are in the spotlight only after easy samples are
convinced sufficiently, and well-separated samples gain a higher gradient to
reduce intra-class distance. Based on the analysis results, we can provide
evidence that the large margin Softmax will affect the local Lipschitz
constraint of the loss function by regulating the probability-dependent
gradient decay rate. This paper provides a new perspective and understanding of
the relationship among concepts of large margin Softmax, local Lipschitz
constraint and curriculum learning by analyzing the gradient decay rate.
Besides, we propose a warm-up strategy to dynamically adjust Softmax loss in
training, where the gradient decay rate increases from over-small to speed up
the convergence rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Linbo Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>