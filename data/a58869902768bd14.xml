<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns:yt="http://www.youtube.com/xml/schemas/2015" xmlns:media="http://search.yahoo.com/mrss/" xmlns="http://www.w3.org/2005/Atom">
 <link rel="self" href="http://www.youtube.com/feeds/videos.xml?channel_id=UCp3Q1Xhsu-TRljsDqp0Kj_A"/>
 <id>yt:channel:UCp3Q1Xhsu-TRljsDqp0Kj_A</id>
 <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
 <title>ISL and Collaborators</title>
 <link rel="alternate" href="https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A"/>
 <author>
  <name>ISL and Collaborators</name>
  <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
 </author>
 <published>2016-09-12T18:40:11+00:00</published>
 <entry>
  <id>yt:video:rj7A2OP7KO4</id>
  <yt:videoId>rj7A2OP7KO4</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Simulating Reality at One Million Frames Per Second (June 2021)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=rj7A2OP7KO4"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2021-06-21T03:02:19+00:00</published>
  <updated>2022-07-26T01:31:31+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Simulating Reality at One Million Frames Per Second (June 2021)</media:title>
   <media:content url="https://www.youtube.com/v/rj7A2OP7KO4?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/rj7A2OP7KO4/hqdefault.jpg" width="480" height="360"/>
   <media:description>Talk at the CVPR Workshop on Autonomous Driving
June 20, 2021
http://cvpr2021.wad.vision/

Correction: 1M fps is the simulation throughput. With training in the loop, the throughput is lower (still more than 100K fps). The numbers are in Table 1 in the megaverse paper.

Papers covered in the talk:

Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning
ICML 2020
http://vladlen.info/papers/sample-factory.pdf
https://sites.google.com/view/sample-factory

Large Batch Simulation for Deep Reinforcement Learning
ICLR 2021
http://vladlen.info/papers/BPS.pdf
https://github.com/shacklettbp/bps-nav

Megaverse: Simulating Embodied Agents at One Million Experiences per Second
ICML 2021
http://vladlen.info/papers/megaverse.pdf
https://www.megaverse.info/</media:description>
   <media:community>
    <media:starRating count="182" average="5.00" min="1" max="5"/>
    <media:statistics views="7845"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:jZZ2-eNW77o</id>
  <yt:videoId>jZZ2-eNW77o</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Evaluating Researchers (May 2021)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=jZZ2-eNW77o"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2021-05-19T12:37:54+00:00</published>
  <updated>2022-07-18T22:43:00+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Evaluating Researchers (May 2021)</media:title>
   <media:content url="https://www.youtube.com/v/jZZ2-eNW77o?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/jZZ2-eNW77o/hqdefault.jpg" width="480" height="360"/>
   <media:description>Talk at the CMU AI Seminar
May 18, 2021
http://www.cs.cmu.edu/~aiseminar/

h-frac paper: https://arxiv.org/abs/2102.03234
h-frac visualization: https://h-frac.org/

CAP paper: http://arxiv.org/abs/2105.08089
CAP rankings: https://cap-measure.org/</media:description>
   <media:community>
    <media:starRating count="130" average="5.00" min="1" max="5"/>
    <media:statistics views="5219"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:P1IcaBn3ej0</id>
  <yt:videoId>P1IcaBn3ej0</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Enhancing Photorealism Enhancement</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=P1IcaBn3ej0"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2021-05-10T20:13:00+00:00</published>
  <updated>2022-07-22T10:55:15+00:00</updated>
  <media:group>
   <media:title>Enhancing Photorealism Enhancement</media:title>
   <media:content url="https://www.youtube.com/v/P1IcaBn3ej0?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/P1IcaBn3ej0/hqdefault.jpg" width="480" height="360"/>
   <media:description>Enhancing Photorealism Enhancement
Stephan R. Richter, Hassan Abu AlHaija, and Vladlen Koltun

Paper: https://arxiv.org/abs/2105.04619
Code and data: https://github.com/isl-org/PhotorealismEnhancement
Project page: https://isl-org.github.io/PhotorealismEnhancement/

We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.</media:description>
   <media:community>
    <media:starRating count="128227" average="5.00" min="1" max="5"/>
    <media:statistics views="4606492"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:yLLhMkctfBY</id>
  <yt:videoId>yLLhMkctfBY</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Further Towards Photorealism (May 2021)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=yLLhMkctfBY"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2021-05-10T20:12:23+00:00</published>
  <updated>2022-05-31T01:00:06+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Further Towards Photorealism (May 2021)</media:title>
   <media:content url="https://www.youtube.com/v/yLLhMkctfBY?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/yLLhMkctfBY/hqdefault.jpg" width="480" height="360"/>
   <media:description>Keynote for Eurographics 2021
May 5, 2021
https://conferences.eg.org/eg2021/

Stable View Synthesis
Paper: http://vladlen.info/papers/SVS.pdf
Video: https://youtu.be/gqgXIY09htI
Code and data: https://github.com/intel-isl/StableViewSynthesis

Enhancing Photorealism Enhancement
Paper: https://arxiv.org/abs/2105.04619
Video: https://youtu.be/P1IcaBn3ej0
Code and data: https://github.com/intel-isl/PhotorealismEnhancement
Project page: https://intel-isl.github.io/PhotorealismEnhancement/</media:description>
   <media:community>
    <media:starRating count="1138" average="5.00" min="1" max="5"/>
    <media:statistics views="58865"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:gqgXIY09htI</id>
  <yt:videoId>gqgXIY09htI</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Stable View Synthesis</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=gqgXIY09htI"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-11-17T04:23:54+00:00</published>
  <updated>2022-06-08T20:37:15+00:00</updated>
  <media:group>
   <media:title>Stable View Synthesis</media:title>
   <media:content url="https://www.youtube.com/v/gqgXIY09htI?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/gqgXIY09htI/hqdefault.jpg" width="480" height="360"/>
   <media:description>Stable View Synthesis
Gernot Riegler and Vladlen Koltun
Computer Vision and Pattern Recognition (CVPR), 2021

Paper: https://arxiv.org/abs/2011.07233
Code: https://github.com/intel-isl/StableViewSynthesis

We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes.</media:description>
   <media:community>
    <media:starRating count="229" average="5.00" min="1" max="5"/>
    <media:statistics views="10359"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:Rd0nBO6--bM</id>
  <yt:videoId>Rd0nBO6--bM</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Towards Photorealism (September 2020)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=Rd0nBO6--bM"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-10-16T13:33:00+00:00</published>
  <updated>2022-05-29T04:33:05+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Towards Photorealism (September 2020)</media:title>
   <media:content url="https://www.youtube.com/v/Rd0nBO6--bM?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/Rd0nBO6--bM/hqdefault.jpg" width="480" height="360"/>
   <media:description>Keynote for the DAGM GCPR / VMV / VCBM joint conference
September 28, 2020
https://www.gcpr-vmv-vcbm-2020.uni-tuebingen.de

Q&amp;A session: https://youtu.be/KVKuHF52jRk?t=5394

FVS paper: http://vladlen.info/papers/FVS.pdf
FVS code: https://github.com/intel-isl/FreeViewSynthesis

NeRF++ paper: http://arxiv.org/abs/2010.07492
NeRF++ code: https://github.com/Kai-46/nerfplusplus</media:description>
   <media:community>
    <media:starRating count="219" average="5.00" min="1" max="5"/>
    <media:statistics views="10223"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:qc8hFLyWDOM</id>
  <yt:videoId>qc8hFLyWDOM</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>OpenBot: Turning Smartphones into Robots</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=qc8hFLyWDOM"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-08-24T16:29:07+00:00</published>
  <updated>2022-06-02T03:08:09+00:00</updated>
  <media:group>
   <media:title>OpenBot: Turning Smartphones into Robots</media:title>
   <media:content url="https://www.youtube.com/v/qc8hFLyWDOM?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/qc8hFLyWDOM/hqdefault.jpg" width="480" height="360"/>
   <media:description>Current robots are either expensive or make significant compromises on sensory richness, computational power, and communication capabilities. We propose to leverage smartphones to equip robots with extensive sensor suites, powerful computational abilities, state-of-the-art communication channels, and access to a thriving software ecosystem. We design a small electric vehicle that costs $50 and serves as a robot body for standard Android smartphones. We develop a software stack that allows smartphones to use this body for mobile operation and demonstrate that the system is sufficiently powerful to support advanced robotics workloads such as person following and real-time autonomous navigation in unstructured environments. Controlled experiments demonstrate that the presented approach is robust across different smartphones and robot bodies.

Paper: https://arxiv.org/abs/2008.10631

Get started at https://www.openbot.org</media:description>
   <media:community>
    <media:starRating count="1009" average="5.00" min="1" max="5"/>
    <media:statistics views="56389"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:JDJPn3ZtfZs</id>
  <yt:videoId>JDJPn3ZtfZs</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Free View Synthesis</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=JDJPn3ZtfZs"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-08-11T13:38:54+00:00</published>
  <updated>2022-07-22T12:20:08+00:00</updated>
  <media:group>
   <media:title>Free View Synthesis</media:title>
   <media:content url="https://www.youtube.com/v/JDJPn3ZtfZs?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/JDJPn3ZtfZs/hqdefault.jpg" width="480" height="360"/>
   <media:description>Free View Synthesis
Gernot Riegler and Vladlen Koltun
European Conference on Computer Vision (ECCV), 2020

Paper: http://vladlen.info/papers/FVS.pdf
Code and data: https://github.com/intel-isl/FreeViewSynthesis

We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.</media:description>
   <media:community>
    <media:starRating count="279" average="5.00" min="1" max="5"/>
    <media:statistics views="13907"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:XmtTjqimW3g</id>
  <yt:videoId>XmtTjqimW3g</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Autonomous Driving: The Way Forward (July 2020)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=XmtTjqimW3g"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-07-13T13:29:33+00:00</published>
  <updated>2022-07-24T06:40:52+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Autonomous Driving: The Way Forward (July 2020)</media:title>
   <media:content url="https://www.youtube.com/v/XmtTjqimW3g?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/XmtTjqimW3g/hqdefault.jpg" width="480" height="360"/>
   <media:description>Talk at the ICML Autonomous Driving Workshop
July 17, 2020
https://sites.google.com/view/aiad2020</media:description>
   <media:community>
    <media:starRating count="87" average="5.00" min="1" max="5"/>
    <media:statistics views="4151"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:GuAsn4E3fP8</id>
  <yt:videoId>GuAsn4E3fP8</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Beyond convolutional networks (June 2020)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=GuAsn4E3fP8"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-06-16T06:25:47+00:00</published>
  <updated>2022-05-21T09:33:59+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Beyond convolutional networks (June 2020)</media:title>
   <media:content url="https://www.youtube.com/v/GuAsn4E3fP8?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i4.ytimg.com/vi/GuAsn4E3fP8/hqdefault.jpg" width="480" height="360"/>
   <media:description>Talk at the DeepVision workshop
June 19, 2020
https://sites.google.com/view/deepvision2020</media:description>
   <media:community>
    <media:starRating count="159" average="5.00" min="1" max="5"/>
    <media:statistics views="5824"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:VWuH9XTL9Ss</id>
  <yt:videoId>VWuH9XTL9Ss</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Vladlen Koltun: Towards machines that see in the real world (June 2020)</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=VWuH9XTL9Ss"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-06-16T02:46:38+00:00</published>
  <updated>2022-10-20T22:01:13+00:00</updated>
  <media:group>
   <media:title>Vladlen Koltun: Towards machines that see in the real world (June 2020)</media:title>
   <media:content url="https://www.youtube.com/v/VWuH9XTL9Ss?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i3.ytimg.com/vi/VWuH9XTL9Ss/hqdefault.jpg" width="480" height="360"/>
   <media:description>Talk at the Machines Can See summit
June 8, 2020
http://machinescansee.com/</media:description>
   <media:community>
    <media:starRating count="39" average="5.00" min="1" max="5"/>
    <media:statistics views="1214"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:PzBK6K5gyyo</id>
  <yt:videoId>PzBK6K5gyyo</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>MSeg: A Composite Dataset for Multi-domain Semantic Segmentation</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=PzBK6K5gyyo"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2020-06-08T19:13:41+00:00</published>
  <updated>2022-01-13T05:38:23+00:00</updated>
  <media:group>
   <media:title>MSeg: A Composite Dataset for Multi-domain Semantic Segmentation</media:title>
   <media:content url="https://www.youtube.com/v/PzBK6K5gyyo?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/PzBK6K5gyyo/hqdefault.jpg" width="480" height="360"/>
   <media:description>MSeg: A Composite Dataset for Multi-domain Semantic Segmentation
John Lambert, Zhuang Liu, Ozan Sener, James Hays, and Vladlen Koltun
Computer Vision and Pattern Recognition (CVPR), 2020

Paper: http://vladlen.info/papers/MSeg.pdf
Code and data: https://github.com/mseg-dataset

We present MSeg, a composite dataset that unifies semantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model’s robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash leaderboard for robust semantic segmentation, with no exposure to WildDash data during training.</media:description>
   <media:community>
    <media:starRating count="103" average="5.00" min="1" max="5"/>
    <media:statistics views="3523"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:D46FzVyL9I8</id>
  <yt:videoId>D46FzVyL9I8</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=D46FzVyL9I8"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2019-12-05T03:23:29+00:00</published>
  <updated>2022-07-28T05:35:25+00:00</updated>
  <media:group>
   <media:title>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</media:title>
   <media:content url="https://www.youtube.com/v/D46FzVyL9I8?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/D46FzVyL9I8/hqdefault.jpg" width="480" height="360"/>
   <media:description>Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer
René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun

Paper: https://arxiv.org/abs/1907.01341
Code: https://github.com/intel-isl/MiDaS

The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.</media:description>
   <media:community>
    <media:starRating count="77" average="5.00" min="1" max="5"/>
    <media:statistics views="4452"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:YeaHVrPLSro</id>
  <yt:videoId>YeaHVrPLSro</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Seeing Motion in the Dark</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=YeaHVrPLSro"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2019-08-29T14:40:26+00:00</published>
  <updated>2022-05-24T05:00:00+00:00</updated>
  <media:group>
   <media:title>Seeing Motion in the Dark</media:title>
   <media:content url="https://www.youtube.com/v/YeaHVrPLSro?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i2.ytimg.com/vi/YeaHVrPLSro/hqdefault.jpg" width="480" height="360"/>
   <media:description>Seeing Motion in the Dark
Chen Chen, Qifeng Chen, Minh Do, and Vladlen Koltun
International Conference on Computer Vision (ICCV), 2019

Paper: http://vladlen.info/papers/DRV.pdf
Code and data: https://github.com/cchen156/Seeing-Motion-in-the-Dark</media:description>
   <media:community>
    <media:starRating count="130" average="5.00" min="1" max="5"/>
    <media:statistics views="7739"/>
   </media:community>
  </media:group>
 </entry>
 <entry>
  <id>yt:video:4MfWa2yZ0Jc</id>
  <yt:videoId>4MfWa2yZ0Jc</yt:videoId>
  <yt:channelId>UCp3Q1Xhsu-TRljsDqp0Kj_A</yt:channelId>
  <title>Does Computer Vision Matter for Action?</title>
  <link rel="alternate" href="https://www.youtube.com/watch?v=4MfWa2yZ0Jc"/>
  <author>
   <name>ISL and Collaborators</name>
   <uri>https://www.youtube.com/channel/UCp3Q1Xhsu-TRljsDqp0Kj_A</uri>
  </author>
  <published>2019-05-23T23:18:43+00:00</published>
  <updated>2022-06-01T05:17:58+00:00</updated>
  <media:group>
   <media:title>Does Computer Vision Matter for Action?</media:title>
   <media:content url="https://www.youtube.com/v/4MfWa2yZ0Jc?version=3" type="application/x-shockwave-flash" width="640" height="390"/>
   <media:thumbnail url="https://i1.ytimg.com/vi/4MfWa2yZ0Jc/hqdefault.jpg" width="480" height="360"/>
   <media:description>Does Computer Vision Matter for Action?
Brady Zhou, Philipp Krähenbühl, and Vladlen Koltun
Science Robotics, 4(30), 2019

Paper (open access): http://arxiv.org/abs/1905.12887
Paper (official journal version): https://robotics.sciencemag.org/content/4/30/eaaw6661
Video (download file): http://vladlen.info/papers/does-vision-matter.mp4
Code: https://github.com/intel-isl/vision-for-action</media:description>
   <media:community>
    <media:starRating count="106" average="5.00" min="1" max="5"/>
    <media:statistics views="6178"/>
   </media:community>
  </media:group>
 </entry>
</feed>
