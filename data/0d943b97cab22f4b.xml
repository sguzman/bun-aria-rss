<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://danielhomola.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://danielhomola.com/" rel="alternate" type="text/html" /><updated>2020-12-24T10:38:57+00:00</updated><id>https://danielhomola.com/feed.xml</id><title type="html">Daniel Homola</title><subtitle>Personal website of Daniel Homola senior data scientist &amp; ML engineer using deep learning and NLP to drag &lt;br&gt; healthcare into the 21st century. I am also interested in NLP, reinforcement learning and applying deep learning to real world problems. </subtitle><author><name> </name></author><entry><title type="html">My startup failed, now what?</title><link href="https://danielhomola.com/startups/learning/my-startup-failed/" rel="alternate" type="text/html" title="My startup failed, now what?" /><published>2020-10-10T00:00:00+01:00</published><updated>2020-10-10T00:00:00+01:00</updated><id>https://danielhomola.com/startups/learning/my-startup-failed</id><content type="html" xml:base="https://danielhomola.com/startups/learning/my-startup-failed/">&lt;h2 id=&quot;the-1-minute-version&quot;&gt;The 1 minute version&lt;/h2&gt;

&lt;p&gt;I was part of the EF12 London cohort in 2019, where I met my co-founder. Together, we pursued an idea that I had for a while:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A privacy-preserving medical-data marketplace and AI platform built around federated deep learning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The purpose of the platform would have been to allow data scientists to train deep learning models on highly sensitive healthcare data without that data ever leaving the hospitals. At the same time, thanks to a novel data monetization strategy and marketplace component, hospitals would have been empowered to make money from the data they are generating.&lt;/p&gt;

&lt;p&gt;We received pre-seed funding, valued at $1M. Then the race for demo day began with frantic product building and non-stop business development. Unfortunately, my co-founder fell out of love with the idea after enduring months of business development hardship. Then we failed to pivot to something we both liked, so we split up and I went on my own way.&lt;/p&gt;

&lt;p&gt;The whole ordeal was an amazing mixture of fun, challenging learning opportunities and uninhibited misery (but mostly fun). Make sure to check out the &lt;a href=&quot;/federeated_learning_platform&quot;&gt;demo of the MVP&lt;/a&gt; I built.&lt;/p&gt;

&lt;p&gt;If you are curious about the details of the story then read on. If you’re in a rush, feel free to skip ahead to the &lt;a href=&quot;#takeaways&quot;&gt;list of things I learned&lt;/a&gt; from the experience.&lt;/p&gt;

&lt;h2 id=&quot;how-to-find-a-co-founder&quot;&gt;How to find a co-founder?&lt;/h2&gt;

&lt;p&gt;Back in early 2019, I left my job at IQVIA to join the London cohort of &lt;a href=&quot;https://joinef.com&quot;&gt;Entrepreneur First&lt;/a&gt;. EF is an amazing incubator / accelerator programme that has given the world the likes of &lt;a href=&quot;https://techcrunch.com/2016/06/20/twitter-is-buying-magic-pony-technology-which-uses-neural-networks-to-improve-images/&quot;&gt;Magic Pony&lt;/a&gt; and &lt;a href=&quot;https://www.joinef.com/companies/&quot;&gt;hundreds of other exciting companies&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;entrepreneur-what&quot;&gt;Entrepreneur what?&lt;/h3&gt;

&lt;p&gt;The main thing that sets EF apart from the dozens of similar programmes is that they focus on the earliest possible stage of companies: sole founders. They take in a hundred talented and ambitious individuals in each cohort (actually almost 200 when you count the other European EF sites: Berlin and Paris).&lt;/p&gt;

&lt;p&gt;Then the good people of EF will&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;make everyone go through a weird, speed-dating meets Love Island type of co-founder finding process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this phase, for weeks on end, you are simultaneously reading the CVs of super interesting cohort members and having ludicrously blue-sky chats with them, about potentially world changing companies the two of you could build. By the end of this process, you should end up with a co-founder.&lt;/p&gt;

&lt;p&gt;EF also gives you a stipend, a place to work and a ton of high quality material on the basics of entrepreneurship and startup building. But I think the real USP of EF, is the access to this pool of pre-screened, highly talented and motivated people,  who all left their (sometimes lucrative) day jobs to become members of the cohort and build companies.&lt;/p&gt;

&lt;h3 id=&quot;from-beers-to-business&quot;&gt;From beers to business&lt;/h3&gt;

&lt;p&gt;As a naturally sceptical person, I wasn’t sure what to think about this process. I did what was expected of me (reading CVs, talking to people, coming up with outlandish ideas) but I didn’t really click with anyone.&lt;/p&gt;

&lt;p&gt;Finally however, after many beers and chats, I found my co-founder, with whom we just had enough in common (interest in healthcare and machine learning) to make our discussions fruitful, but with whom our skills were complimentary enough that we could take advantage of having two people on the team.&lt;/p&gt;

&lt;p&gt;This latter point is in fact quite crucial: too often, we are naturally attracted to people with similar interests, skills and thinking to our own. This is great for friendships, but can be devastating in a startup where&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;you want the co-founders to cover each other’s blind spots.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Both my co-founder and I are “techies” in broad terms. But it was quite clear from the beginning that I am more of a CTO type who loves to build, whereas he liked the process of selling and pitching, which is quite important because that’s pretty much &lt;strong&gt;all you do&lt;/strong&gt; as an early stage startup CEO.&lt;/p&gt;

&lt;p&gt;Fortunately (as this is quite rare AFAIK), we both had a good share of the skills of the other’s role too. This really helped with both the technical and business discussions between the two of us.&lt;/p&gt;

&lt;h2 id=&quot;do-you-have-a-1b-idea&quot;&gt;Do you have a $1B idea?&lt;/h2&gt;

&lt;p&gt;Something we all learned fairly quickly in EF was the economics of venture capital. It’s really quite simple and goes something like this:&lt;/p&gt;

&lt;h3 id=&quot;venture-capitalism-101&quot;&gt;Venture capitalism 101&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There’s a huge amount of excess money slushing around in the global financial system.&lt;/li&gt;
  &lt;li&gt;Most investors stratify their investment portfolios according to some risk profile. This usually means that they are happy to put 1-2% of their capital in high-risk ventures. Funding highly risky tech startups is one of these high-risk venture types. Another would be to buy crypto for instance.&lt;/li&gt;
  &lt;li&gt;VC firms utilise this and go out every $latex x$ years to &lt;em&gt;raise a round&lt;/em&gt;, i.e. pool together capital from wealthy individuals and investment funds.&lt;/li&gt;
  &lt;li&gt;Whatever they raise is the fund that can be allocated by the partners of the VC firm, as they see fit.&lt;/li&gt;
  &lt;li&gt;Most VCs work on a 2/20 rule: they will take 2% management fee each year, plus 20% of any profit they make.
    &lt;ul&gt;
      &lt;li&gt;It’s worth noting here, that 2% of a billion-dollar fund is still a few million for each partner each year, even if none of their funded companies exit or do particularly well. Yeah, being a VC is quite lucrative.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;VCs promise extraordinary returns to the investors. Certainly well above anything that’s attainable with reasonable risk profile on the market. How can they deliver?
    &lt;ol&gt;
      &lt;li&gt;To achieve this they can either fund companies where they expect that most of them will grow 100%.&lt;/li&gt;
      &lt;li&gt;Or fund companies where none of them will grow a 100%, because most will grow 0%, and potentially one or two will grow 1,000% or 10,000%.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Companies from the first group are called successful medium sized business. The latter ones are called unicorns: startups in hyper-growth mode, overtaking and disrupting a whole industry like AirBnB or Uber did for example.&lt;/p&gt;

&lt;h3 id=&quot;not-all-ideas-are-created-equal&quot;&gt;Not all ideas are created equal&lt;/h3&gt;

&lt;p&gt;Even though probably both investment models could be equally successful, &lt;strong&gt;VCs want to invest in high impact firms&lt;/strong&gt; that will change the world. Think Google, Facebook, Twitter.&lt;/p&gt;

&lt;p&gt;I leave it to your judgement whether this is a healthy and sustainable business model. I will certainly write more about this in the future, but I will say this: VCs serve a super important function in the global innovation market and without them, the world would be different in countless ways.&lt;/p&gt;

&lt;p&gt;The corollary of all the above is that if you go down the VC backed startup building path, you need an idea that can become a 1 billion-dollar company. &lt;strong&gt;Yes, that’s a billion with a B.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;start-me-up&quot;&gt;Start me up!&lt;/h2&gt;

&lt;p&gt;We spent weeks ideating with my co-founder, but after dozens of discussions we came back to the idea I arrived to EF with. It was the only one that seemed simultaneously ambitious and high impact enough that a VC might take a liking to it, while also having the potential to grow into a multi billion-dollar company. Also, neither of us hated it, which is quite important, I heard.&lt;/p&gt;

&lt;h3 id=&quot;the-idea&quot;&gt;The idea&lt;/h3&gt;

&lt;p&gt;I had this idea for a company for months before applying to EF:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The world’s first, privacy-centered medical-data marketplace and machine learning platform, powered by federated deep learning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I couldn’t wait to tell all the people at the cohort about it, so I did. And they all made the exact same face you just did when you read the sentence above.&lt;/p&gt;

&lt;p&gt;The problem with deep tech ideas (besides not fitting on a napkin) is that for the most of us not well-versed in the particular field, they are often indistinguishable from vaporware and snake-oil.&lt;/p&gt;

&lt;p&gt;You really have to know a lot about healthcare informatics, medical machine learning, federated deep learning and also the financial incentive structure of the US healthcare industry to properly evaluate that one sentence above.&lt;/p&gt;

&lt;p&gt;If you do that, it will not only make a lot of sense but you’ll find that it’s offering one of the very few viable long-term solutions to the impossible trade-off healthcare faces in the 21st century:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The patients’ natural right to data privacy is in direct conflict with data pooling and with how much we can learn from this data with current AI algorithms.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is bad news, because without the deployment and proliferation of AI, &lt;a href=&quot;https://www.goodreads.com/book/show/40915762-deep-medicine&quot;&gt;healthcare is doomed&lt;/a&gt;. Another indication that the above idea might not be the worst ever is that Google Ventures already backed an &lt;a href=&quot;https://owkin.com/federated-learning/&quot;&gt;amazing team&lt;/a&gt; to do exactly this. Yeah, finding that out wasn’t fun…&lt;/p&gt;

&lt;h3 id=&quot;the-idea-explained&quot;&gt;The idea explained&lt;/h3&gt;

&lt;p&gt;Probably it’s easiest to explain the idea for our startup by enumerating some of the relevant problems that plague healthcare currently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hospitals and outpatient clinics generate vast amounts of incredibly rich and detailed data about patients.  We’ll call these institutions providers.&lt;/li&gt;
  &lt;li&gt;This generated patient data is extremely sensitive, and people are rightly concerned about who gets to use or analyse it.&lt;/li&gt;
  &lt;li&gt;Pharmaceutical companies, biotech and medtech AI startups are all hungry for this data and could develop amazing new drugs, therapies, diagnostic tools, algorithms by leveraging it. We’ll call these companies vendors.&lt;/li&gt;
  &lt;li&gt;Providers are often cash-stripped and as a consequence struggling to provide the highest quality of care for their patients.&lt;/li&gt;
  &lt;li&gt;Vendors have ample resources (money) and a clear need to access this data. Furthermore, by providing this data to them securely, the world could undoubtedly become a better and healthier place through their innovation.&lt;/li&gt;
  &lt;li&gt;Even if vendors had access to this data, deploying AI models at scale, in a live hospital setting is still one of the biggest unsolved challenges of digital healthcare.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you write down these six simple facts about today’s healthcare systems, it really doesn’t take long for the light bulb to start flashing.&lt;/p&gt;

&lt;p&gt;We need to build a marketplace where providers can sell access to their data without compromising patient privacy and where vendors can get access to this data to learn from it and train models on it. Furthermore, once vendors have a model trained, they need to be able to easily deploy it to network of hospitals, where those models could start to generate useful (and potentially life-saving) predictions.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;But how could anyone use sensitive data without actually having a look at it, thereby immediately compromising patient privacy?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The answer is quite remarkably simple, and it’s been powering all sorts of technologies (like predictive texting on your phone) for years. It’s called &lt;strong&gt;federated learning&lt;/strong&gt; and it’s first real-world, large scale application came from Google (as far as I know) following &lt;a href=&quot;https://arxiv.org/abs/1511.03575&quot;&gt;this paper&lt;/a&gt;. Then, &lt;a href=&quot;https://arxiv.org/abs/1610.05492&quot;&gt;many&lt;/a&gt; - &lt;a href=&quot;https://arxiv.org/abs/1806.00582&quot;&gt;more&lt;/a&gt; - &lt;a href=&quot;https://arxiv.org/abs/1902.01046&quot;&gt;followed&lt;/a&gt; and amazing open source libraries were released like &lt;a href=&quot;https://github.com/OpenMined/PySyft&quot;&gt;PySyft&lt;/a&gt; by &lt;a href=&quot;https://www.openmined.org/&quot;&gt;OpenMined&lt;/a&gt;, or &lt;a href=&quot;https://github.com/FederatedAI/FATE&quot;&gt;FATE&lt;/a&gt; by &lt;a href=&quot;https://www.wikiwand.com/en/WeBank_(China)&quot;&gt;WeBank&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Federated learning’s simplest explanation can be boiled down to a single sentence:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Instead of moving the data to the model, &lt;strong&gt;let’s move the model to data.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Suffice to say that this idea has found applications in several security and privacy conscious industries, but its impact on healthcare will truly be transformational. Have a look at these two recently published Nature articles to see what federated learning holds for the future of &lt;a href=&quot;https://www.nature.com/articles/s41598-020-69250-1&quot;&gt;multi-institutional medical studies&lt;/a&gt;  and &lt;a href=&quot;https://www.nature.com/articles/s41746-020-00323-1&quot;&gt;digital health applications&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re curious about the technical details and implementation of our idea, &lt;strong&gt;make sure to check out&lt;/strong&gt; &lt;a href=&quot;/federeated_learning_platform&quot;&gt;&lt;strong&gt;this page&lt;/strong&gt;&lt;/a&gt; about the MVP I built, or watch my demo of it below.&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/KZBNBdD1yVI&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;h3 id=&quot;to-recap&quot;&gt;To recap&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;We wanted to build a platform to enable researchers and data scientists of pharma and biotech companies to access highly granular and information rich patient data from US hospitals to train models on it,&lt;/li&gt;
  &lt;li&gt;without the data ever leaving the four walls of those hospitals and without the researchers ever being able to actually look at the data.&lt;/li&gt;
  &lt;li&gt;We wanted to leverage federated deep learning and a federated data query engine to achieve this.&lt;/li&gt;
  &lt;li&gt;The best part? The hospitals would have made money out of the mountains of data they are sitting on, without ever compromising patient privacy, while simultaneously enabling vendors to innovate, research and develop therapies.&lt;/li&gt;
  &lt;li&gt;Furthermore the learned algorithms could have been deployed back to the federated hospital network and put to good use.&lt;/li&gt;
  &lt;li&gt;We wanted to charge vendors according to the well-known &lt;a href=&quot;https://stripe.com/en-gb/atlas/guides/business-of-saas#the-fundamental-equation-of-saas&quot;&gt;SaaS business model&lt;/a&gt; and share a sizeable fraction of our revenue with the hospitals so they become financially incentivised to partner with us.&lt;/li&gt;
  &lt;li&gt;How? Every time a hospital’s data would have been used to train a model, they would have got compensated based on the number of patients and data granularity they provided for the study.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s a flowchart of the smallest possible incarnation of our platform with just two hospitals connected to the federated data network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/fedml_overview.png&quot;&gt;&lt;img src=&quot;/assets/images/fedml_overview.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-rollercoaster-of-founder-life&quot;&gt;The rollercoaster of founder life&lt;/h2&gt;

&lt;p&gt;The EF programme is structured according to a fairly well thought out schedule that gives a nice framework for working on your idea.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the first few weeks you should get data about the market’s reaction to your idea. This means you’re constantly on the phone (sometimes for 6-8 hours straight), talking to people, who are often not that interested, but you begged your way in or forced them some other way.&lt;/li&gt;
  &lt;li&gt;You’re doing all of this to basically &lt;a href=&quot;http://momtestbook.com/&quot;&gt;“Mom test” your idea&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;This is a neat little trick to get around the problem of people being too nice. Yes, that’s actually a problem, as they’ll outright lie in your face and say that they like your idea and would even pay for it. Only then to completely disappear after the call.&lt;/li&gt;
      &lt;li&gt;The reason for this is quite simple: no one likes to be the a-hole who breaks the soul of budding entrepreneurs. Contrary to popular belief, people on average are fairly nice that way, so they’ll naturally try to say something good about your idea, even if it’s objectively terrible and they knew from the second minute, they’ll never ever use it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In this early phase, it’s not a big deal if people are hanging up on you. You should just have as many calls as possible, be resilient and accept that no one cares about your idea besides you. Keep your head down, collect data and try to find out if there’s anyone out there even remotely interested in buying what you have to offer.&lt;/li&gt;
  &lt;li&gt;In the next phase of your VC fuelled company building, you are trying to produce some form of &lt;em&gt;“traction”&lt;/em&gt; to prove to your future investors (the EF Investment Committee (IC) in our case), that in fact, your idea is a viable business.&lt;/li&gt;
  &lt;li&gt;Traction can be many things, depending on the complexity of your product. Here are a few types of traction in decreasing order of impressiveness:
    &lt;ul&gt;
      &lt;li&gt;Signed up users / companies with their card details already taken, ready for doing business with you.&lt;/li&gt;
      &lt;li&gt;Signed contract with another business for a (hopefully) paid trial. Contract for an unpaid trial is the next best thing here.&lt;/li&gt;
      &lt;li&gt;Letter of intent from another company or institution. This is usually a document with purposefully vague and legally non-binding language in which your future client (you wish) says that they don’t think your idea is totally crazy and (if the stars align and the director wakes up in the right good mood) they might even consider using it one day. Maybe…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Then you take all this evidence that you’ve gathered through hundreds of interviews and thousands of emails and present it to the EF Investment Committee. If they like your idea, you get £80k for a 10% stake in your company. You get to work on it more till you reach the maturity to pitch your company at demo day to dozens and dozens of great VCs with the intention of raising a seed round (~£1-2M) with one of them.&lt;/li&gt;
  &lt;li&gt;If you get seed funded (usually 4-6 months after demo day) then hurray, your VC fuelled company building journey can finally begin. Unfortunately, the odds are still &lt;a href=&quot;https://medium.com/journal-of-empirical-entrepreneurship/dissecting-startup-failure-by-stage-34bb70354a36&quot;&gt;heavily stacked against you&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;There’s only about a 20% chance you’ll make it to the next funding round. Quite annoyingly, you’ll need several of those in the coming years to keep your unicorn alive - they are hungry beasts.&lt;/li&gt;
      &lt;li&gt;There’s a 97% chance that you’ll fail to exit, i.e. fail to turn your years and years of gruelling work into hard earned millions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hm… That was depressing to learn, but don’t worry, it’ll get worse.&lt;/p&gt;

&lt;h3 id=&quot;the-up&quot;&gt;The up&lt;/h3&gt;

&lt;p&gt;As anyone who’s ever tried will tell you, building a company is like being on a rollercoaster ride. Sometimes you have days that feel like none of the days in your previous office jobs. You feel amazing, fulfilled and working on something truly meaningful (at least to you).&lt;/p&gt;

&lt;p&gt;Even better, sometimes things just work out. Like when we started to ask huge and well-known US research hospitals if they would partner with us (two random unfunded guys from London) to build the world’s first federated machine learning platform for healthcare and they just said “yeah sure”.&lt;/p&gt;

&lt;p&gt;Or when the second and third meetings started to occur with these teams, and the term “letter of intent” started to fly around a lot. We felt invincible. Even if it took more than a hundred Zoom calls and thousands of emails sent out, we thought we are clearly making progress with business development here.&lt;/p&gt;

&lt;h3 id=&quot;the-top&quot;&gt;The top&lt;/h3&gt;

&lt;p&gt;Then the rollercoaster continued and took us to this amazing top. Armed with our letters of intent and signed project proposals with these hospitals, EF’s IC said they’d invest in us and just like that, we got the pre-seed funding.&lt;/p&gt;

&lt;p&gt;Roughly 30% of the initial EF cohort make it this far so we felt great. Even though we knew the work is just about to get a whole lot more challenging and crazy, there’s a moment when you have your first little success and you start quietly wondering to yourself:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Maybe we’ll be among the 1% of startups that actually make it… I know it’s  unlikely, but maybe this will all just work out fine!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I was also super excited that I get to finally build again after months of not coding. Don’t get me wrong, conducting BD calls, writing marketing materials, technical documents, grants, drafting SOWs and LOIs was extremely educational too and they are necessary and unavoidable parts of every company formation. Nonetheless, I was itching to actually get back to the thing I love doing and I’m reasonably good at.&lt;/p&gt;

&lt;h3 id=&quot;the-down&quot;&gt;The down&lt;/h3&gt;

&lt;p&gt;After the pre-seed investment, I was 100% on building and my co-founder was 100% on business development. We only had 2.5 months till demo day to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build an MVP of the highly complex product we envisioned.&lt;/li&gt;
  &lt;li&gt;Get extremely conservative and risk averse pharma companies and hospitals to sign up with our marketplace.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our plan seemed straightforward. I am going to take care of the first one and my co-founder is doing the second. Even though we knew the second is probably dependent on the first being ready, we agreed to work as hard as we can and try to have our first customers by demo day in late September 2019.&lt;/p&gt;

&lt;p&gt;However, the cracks started to show quite early on… The business development process went from painfully hard (pre-IC) to near impossibly hard (post-IC). Many people told us that letters of intent are worth the same as toilet paper, and we should try to convert those into signed contracts for paid or unpaid trials ASAP, so that’s what we tried to do.&lt;/p&gt;

&lt;p&gt;Despite my co-founder’s heroic efforts however, after months of negotiations, meetings, project proposals and more meetings with US hospitals, &lt;strong&gt;every single lead of ours simply dried up&lt;/strong&gt;. Emails went unanswered. Crucial final meetings with the key stakeholders did not happen.&lt;/p&gt;

&lt;p&gt;When the discussions turned real, and when hospitals realised that for this fancy-sounding research project they’d need to actually provision resources for us so we can integrate our software into their IT systems, they went cold turkey on us.&lt;/p&gt;

&lt;h3 id=&quot;the-small-up-before&quot;&gt;The small up before..&lt;/h3&gt;

&lt;p&gt;We realised that we made our own lives impossible by trying to aggressively innovate on two fronts at once:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We wanted to build a federated ML platform for deep learning models in a notoriously conservative industry, leveraging highly sensitive data with tons of regulations around it.&lt;/li&gt;
  &lt;li&gt;We wanted to create a marketplace, therefore we had to find not just one customer type but two: providers and vendors too.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Even just one of these goals would have been insanely hard for a two-person team to pull off in 6 months, but doing them together was clearly way too much. So we pivoted and said:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s sell our federated ML platform to pharma companies. Let them use it with their existing hospital relationships however they see fit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This wasn’t the worst idea to be honest. Pharma companies already have longstanding relationships with hospitals and outpatient clinics, as these connections are essential for recruiting patients for new clinical trials.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Healthcare&lt;/strong&gt; - as we found out to our own detriment - &lt;strong&gt;is built on trust and slowly evolving long-term partnerships&lt;/strong&gt;. There’s very little place for disruptive technologies, especially pushed by small companies. Due to the heavily regulated and risk averse nature of healthcare, you can have the best tech product and it’ll still be extremely hard to sell (this was confirmed to us by numerous companies on the market).&lt;/p&gt;

&lt;p&gt;So we thought, let’s sell the technology to the innovation leads and heads of data science of large pharma companies. Then they will take care of getting the tech to the data, i.e. inside the hospitals.&lt;/p&gt;

&lt;h3 id=&quot;the-crash&quot;&gt;The crash&lt;/h3&gt;

&lt;p&gt;This hasn’t really changed the MVP I had to build, only the way we’d demo it to users potentially, so I proceeded further with full steam.&lt;/p&gt;

&lt;p&gt;My co-founder in the meanwhile was trying to line up as many demo opportunities as he could, so we would have a good chance of converting at least one or two of those into an unpaid trial by demo day. Then we’d have our hard-won &lt;em&gt;“traction”&lt;/em&gt; in our hands for our discussions with VCs.&lt;/p&gt;

&lt;p&gt;I finished &lt;a href=&quot;/federeated_learning_platform&quot;&gt;the MVP&lt;/a&gt; and we started to demo it to large pharma companies. Quite miraculously, none of them thought it was terrible. In fact, some of them even murmured something positive about it during the call.&lt;/p&gt;

&lt;p&gt;But crucially, &lt;strong&gt;none of them jumped out of their chair singing Hail Mary&lt;/strong&gt;, thanking us for finally solving their long standing hair-on-fire problem. Instead, they all said, they’ll get back to us after they had some internal discussions.&lt;/p&gt;

&lt;p&gt;Although sales cycles in healthcare are notoriously long, and selling to pharma as a tiny startup is near impossible, we knew we missed the mark. We could simply feel after these calls that we aren’t solving any of these teams’ top 3 problems. What we offered was definitely in the top 10 or 15, but not even close to top 3.&lt;/p&gt;

&lt;p&gt;And then, to make things worse (or better?)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;through a series of honest chats with my co-founder, I found out that his heart isn’t really in federated learning anymore.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Turns out, it’s a pretty big problem when the CEO of a company is not emotionally invested in and hyped up about the very thing the company is selling. It’s especially bad if this happens literally weeks before he’s supposed to stand in front of a hundred VCs and convince them that our business is the one that’s going to change the world and which is worth their £1M investment.&lt;/p&gt;

&lt;h3 id=&quot;operation-salvage&quot;&gt;Operation Salvage&lt;/h3&gt;

&lt;p&gt;I totally empathised with the frustration of my co-founder after he struggled for months to sell without much to show for it… However it felt rather bad to give up on the idea at the very last minute.&lt;/p&gt;

&lt;p&gt;Granted, our traction was practically non-existent at that point, but I suspected from the beginning that selling this product will be incredible hard and might potentially take years. We also knew for a fact that selling our product isn’t impossible, as we have found &lt;a href=&quot;https://trinetx.com/&quot;&gt;companies that managed to get into US hospitals&lt;/a&gt; and others that &lt;a href=&quot;https://owkin.com/&quot;&gt;worked successfully on something similar to our idea&lt;/a&gt; (although both with incredible funding behind their backs). So personally I wasn’t disappointed, and I wasn’t that surprised with our lack of progress. But clearly my co-founder was and it was him who had to muster the energy day after day to try and sell this thing.&lt;/p&gt;

&lt;p&gt;It was pretty clear to me that just like you cannot make someone love another human being, you cannot make someone feel passion for something for which they simply don’t. So reluctantly, I agreed to move on and use the rest of our pre-seed funding to build something else for the next demo day (which was scheduled for 6 months later).&lt;/p&gt;

&lt;p&gt;We told our plans to EF (who were really nice and supportive about it), then proceeded to conduct one of the most painful 4 weeks of my life. We brainstormed in the Google Startup Campus, the British Library and countless pubs and cafes of London for days on end. We were desperately trying to come up with an idea which we both felt good about, meaning we would be willing to dedicate a couple years of our lives to it.&lt;/p&gt;

&lt;p&gt;Whiteboard sessions came and went, beers and many more coffees were had. We tried for hours each day, but one of the two following scenarios kept happening:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Either founder A hated the idea founder B just pitched or,&lt;/li&gt;
  &lt;li&gt;we both quickly realised that founder B’s idea was genuinely terrible, so we just laughed (or cried).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-end&quot;&gt;The end&lt;/h3&gt;

&lt;p&gt;After weeks of this, I proposed to leave the company and sell my shares to my co-founder. Again, EF was incredibly nice about this and signed all the necessary contract modifications that allowed me a clean exit. Then, on a rainy October day, six months after joining EF,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;my co-founder and I said goodbye in a typical London pub and that was the end of my first startup.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As I walked towards the tube, lots of thoughts and feelings were rushing through my head… None of them particularly positive or pleasant, but I knew I made the right decision.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;I probably learned more about myself and the world in those six months than in the preceding 2.5 years in my typical 9 to 5 corporate data-science job. I couldn’t possibly summarise all of it here and some (or most) of it might be completely trivial to some readers. So here’s a list of my top learnings.&lt;/p&gt;

&lt;h3 id=&quot;what-did-i-learn&quot;&gt;What did I learn?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product market fit is (almost) everything&lt;/strong&gt;. It’s a simple sounding but pretty complex idea. Here’s a &lt;a href=&quot;https://a16z.com/2017/02/18/12-things-about-product-market-fit/&quot;&gt;good intro&lt;/a&gt; to it, and here’s the &lt;a href=&quot;https://defmacro.substack.com/p/go-to-market-strategy-for-engineers&quot;&gt;best step by step guide&lt;/a&gt; I’ve found so far about how to achieve it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ideas are cheap.&lt;/strong&gt; Execution is all that matters. It’s been repeated ad-nauseam in startup circles but I think it’s mostly true so worth noting. Although, some successful people &lt;a href=&quot;https://www.indiehackers.com/podcast/170-vincent-woo-of-coderpad&quot;&gt;challenge this&lt;/a&gt; and advocate the exact opposite. The truth (as usual) is probably somewhere in the middle, but definitely closer to the “ideas are cheap” side.
    &lt;ul&gt;
      &lt;li&gt;Related tip: never be afraid to talk about your idea. It’s the only way to find funding and co-founders (and most likely you’ll need at least one of those).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The tale of two ideas:&lt;/strong&gt; imagine you have two ideas, X and Y. Let’s say, idea X solves a problem, which for a 100 people is so serious, they’d sell their kidneys (both in fact) to get it fixed. Idea Y solves a problem for a 10,000 people and they’d be willing to pay (they say) $20 for it. Which one you should pursue? Before EF, I would have chosen idea Y in a heartbeat. Today, I’d kill to have an idea that is like X. Always choose idea X.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Finding a great co-founder is exceptionally hard&lt;/strong&gt;, although I definitely lucked out with mine on many fronts. In general, and contrary to popular startup wisdom, I’d prioritize the following qualities above charisma, genius-level IQ, qualifications or anything else:
    &lt;ul&gt;
      &lt;li&gt;integrity &amp;amp; honesty&lt;/li&gt;
      &lt;li&gt;intellectual rigour &amp;amp; logical consistency&lt;/li&gt;
      &lt;li&gt;good (&lt;a href=&quot;https://basecamp.com/books/calm&quot;&gt;but not crazy&lt;/a&gt;) work ethic &amp;amp; resilience &amp;amp; patience&lt;/li&gt;
      &lt;li&gt;sense of humour &amp;amp; being able to laugh at oneself&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Creating a great team&lt;/strong&gt; of first 10 employees is even harder than finding a co-founder (from what I heard) and it’s probably the most important thing any founder will do after securing funding, as it will most likely determine the culture of the entire company for many years to come.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sales, marketing and UX are super important&lt;/strong&gt;. Probably more so than your code (especially at the beginning).
    &lt;ul&gt;
      &lt;li&gt;Related tip: don’t be the dumb tech guy that builds it, expecting the users to just show up and start using and loving it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://momtestbook.com/&quot;&gt;&lt;strong&gt;Mom test&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;everything&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Being able to evaluate business ideas critically is a fundamental skill for founders. The good news, that it can be learned. Also, a &lt;a href=&quot;https://www.defmacro.org/2019/03/26/startup-checklist.html&quot;&gt;good checklist&lt;/a&gt; helps.&lt;/li&gt;
  &lt;li&gt;Spending time on finding out what is it that you’d work on even if you didn’t get paid for it, is an amazingly useful way of focusing your life and career.&lt;/li&gt;
  &lt;li&gt;Some &lt;strong&gt;healthcare related takeaways&lt;/strong&gt; I wish I knew before starting:
    &lt;ul&gt;
      &lt;li&gt;Healthcare is conservative, built on trust and slowly evolving but longstanding relationships and partnerships.&lt;/li&gt;
      &lt;li&gt;As a not surprising consequence of this, the average age of founders of unicorn healthcare startups is &lt;a href=&quot;https://medium.com/@alitamaseb/land-of-the-super-founders-a-data-driven-approach-to-uncover-the-secrets-of-billion-dollar-a69ebe3f0f45&quot;&gt;significantly higher&lt;/a&gt; than of tech unicorns.&lt;/li&gt;
      &lt;li&gt;Also, healthcare seems to be the only industry where having a PhD and many years of relevant experience is a prerequisite of being successful. In most other industries a BSc and/or an MBA seems to be enough.&lt;/li&gt;
      &lt;li&gt;If you are planning to build and sell any tech related product in healthcare, be prepared for extremely long sales cycles, business partners who proceed with extreme caution and who care a lot about the optics of your company.&lt;/li&gt;
      &lt;li&gt;Therefore, all in all, healthcare is not best suited for tiny startups, created by starry eyed 20-30 something founders with tiny professional networks and little work experience in the field.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-do-i-miss&quot;&gt;What do I miss?&lt;/h3&gt;

&lt;p&gt;Being a founder of your own deep tech company feels amazing on most days. Here are some of the things I miss:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The autonomy, responsibility and fast pace is infectious and invigorating. Especially coming from a traditional healthcare behemoth like IQVIA.&lt;/li&gt;
  &lt;li&gt;Living in a “Come on, we can do this!” mindset and being surrounded by people who think that way.&lt;/li&gt;
  &lt;li&gt;Being surrounded by motivated, ambitious, highly educated and driven people who chose to veer off the traditional corporate track is amazing. Again, in my opinion this is definitely the USP of EF.&lt;/li&gt;
  &lt;li&gt;Being able to determine the work schedule and culture of your team.&lt;/li&gt;
  &lt;li&gt;Being able to just spend a whole day on learning about something, because that’s what the business calls for.&lt;/li&gt;
  &lt;li&gt;Talking to other founders and potential customers teaches you that the world is so much more complex, interesting and nuanced than it seems as an employee in a small vertical of a single industry. There are hundreds of industries, all with their own inefficiencies, suboptimal solutions, knowledge gaps, tech hurdles. Learning about these is fascinating and broadens you as a person and as a professional.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;will-i-do-it-again&quot;&gt;Will I do it again?&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Absolutely!&lt;/strong&gt; Although I might take a different funding route.&lt;/p&gt;

&lt;p&gt;But that’s a topic for another post…&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="startups" /><category term="learning" /><category term="deep learning" /><category term="federated learning" /><summary type="html">The story of our healthcare AI startup and how I didn't become a millionaire</summary></entry><entry><title type="html">Science Flask is released</title><link href="https://danielhomola.com/phd/science-flask-is-released/" rel="alternate" type="text/html" title="Science Flask is released" /><published>2017-04-30T22:16:00+01:00</published><updated>2017-04-30T22:16:00+01:00</updated><id>https://danielhomola.com/phd/science-flask-is-released</id><content type="html" xml:base="https://danielhomola.com/phd/science-flask-is-released/">&lt;h2 id=&quot;why-use-scienceflask&quot;&gt;Why use ScienceFlask?&lt;/h2&gt;

&lt;p&gt;Building &lt;a href=&quot;/corrmapper&quot;&gt;CorrMapper&lt;/a&gt; was one of the hardest things I’ve ever done. I had no idea how many pieces I will need to fit together to turn my my bioinformatics pipeline into a functioning web-app.&lt;/p&gt;

&lt;p&gt;I learned a lot from it, and I wanted to save the pain and the steep learning curve for my fellow scientist colleagues, who might not want to spend a full week of their life on trying to get an upload form to work properly. Yeah.. those were fun times..&lt;/p&gt;

&lt;p&gt;Here’s the simple idea behind Science Flask:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/science_flask_pipeline.png&quot;&gt;&lt;img src=&quot;/assets/images/science_flask_pipeline.png&quot; alt=&quot;Science Flask flowchart&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Notice how everything in blue is non-specific to any scientific app. So then why are we keep re-developing it? Ideally, scientists need not to work on anything else than the green bit. Then they can plug that into Science Flask with a few hours of work and have their tool online in a day or two, instead of weeks.&lt;/p&gt;

&lt;p&gt;Here’s the &lt;a href=&quot;https://github.com/danielhomola/science_flask&quot;&gt;GitHub repo&lt;/a&gt; with some &lt;a href=&quot;https://github.com/danielhomola/science_flask/blob/master/README.md&quot;&gt;lengthy docs&lt;/a&gt; about the structure of the project. I also wrote a &lt;a href=&quot;https://github.com/danielhomola/science_flask/blob/master/deployment.md&quot;&gt;step-by-step guide to deploy your Science Flask app on an AWS&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;

&lt;p&gt;Science Flask comes batteries included.&lt;/p&gt;
&lt;h3 id=&quot;user-management&quot;&gt;User management&lt;/h3&gt;

&lt;p&gt;User’s are only allowed to register with a valid academic email address. This is to ensure that your tool is mainly used for academic and research purposes and not for commercial uses. Furthermore it comes with all the rest of it: email addresses are confirmed, users can change passwords, get password reset request if they forgot it, etc.&lt;/p&gt;

&lt;p&gt;Thanks Flask-Security, you can also assign roles to different users and easily build custom user management logic. For example you might decide that certain users can only use a part of the application, while other users can access all features.&lt;/p&gt;

&lt;h3 id=&quot;sql-database&quot;&gt;SQL database&lt;/h3&gt;

&lt;p&gt;All user, study and analysis data is stored in an SQLite by defaul. This can be changed to MySQL or Postgre SQL easily and the same code will work, thanks to &lt;code&gt;SQLAlchemy&lt;/code&gt;. Thanks to Flask-Migrate if you change your app’s model, you can easily upgrade your database even when your app is deployed.&lt;/p&gt;

&lt;h3 id=&quot;admin-panel&quot;&gt;Admin panel&lt;/h3&gt;

&lt;p&gt;The model (database tables and relations between them) of your app can be easily edited online, from anywhere using CRUD operations. Thanks to Flask-Admin, setting up an admin user who can edit users, and other databases is as simple as modifying 2 lines in the config file.&lt;/p&gt;

&lt;h3 id=&quot;upload-form&quot;&gt;Upload form&lt;/h3&gt;

&lt;p&gt;Getting the data from the user sounds super simple but you’d be surprised how long does it take to get a decent upload page. Also it’s very easy to build complex form logic from the bricks Science- Flask provides.&lt;/p&gt;

&lt;h3 id=&quot;profile-page&quot;&gt;Profile page&lt;/h3&gt;

&lt;p&gt;This collects the uploaded studies of each user and let’s them submit analysis on their data.&lt;/p&gt;

&lt;h3 id=&quot;analysis-form&quot;&gt;Analysis form&lt;/h3&gt;

&lt;p&gt;Just like with the upload form, you can build custom logic to ensure you get the parameters from the user just right. The analysis job is then submitted to the backend. This uses &lt;code&gt;Celery&lt;/code&gt;. Once the analysis is ready, the user is notified in email. Then they can download or check out their results online.&lt;/p&gt;

&lt;h3 id=&quot;logging&quot;&gt;Logging&lt;/h3&gt;

&lt;p&gt;All errors and warning messages are sent to the admins via email. All analysis exceptions and errors could be catched so that the program crashes gracefully, letting the user know what happened.&lt;/p&gt;

&lt;h3 id=&quot;runs-on-bootstrapcss&quot;&gt;Runs on Bootstrap.css&lt;/h3&gt;

&lt;p&gt;Modern, mobile friendly, responsive. Bootstrap makes writing good looking HTML pages dead easy.&lt;/p&gt;

&lt;h3 id=&quot;tool-tips-and-tours&quot;&gt;Tool tips and tours&lt;/h3&gt;

&lt;p&gt;Explain to the user how your application works with interactive tours (available on all the above listed pages) and tooltips.&lt;/p&gt;

&lt;h3 id=&quot;python3&quot;&gt;Python3&lt;/h3&gt;

&lt;p&gt;The whole project is written in Python3.5 (because it’s 2017).&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;I hope it’ll be useful for someone. I definitely would have loved something like this when I started develop CorrMapper. Also if you have an idea to improve it, then please contribute to the project!&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="phd" /><category term="flask" /><category term="python" /><category term="science-flask" /><summary type="html">An extensible modular web-app template for online scientific research tools</summary></entry><entry><title type="html">CorrMapper is finished</title><link href="https://danielhomola.com/phd/corrmapper-is-finished/" rel="alternate" type="text/html" title="CorrMapper is finished" /><published>2017-04-30T00:00:00+01:00</published><updated>2017-04-30T00:00:00+01:00</updated><id>https://danielhomola.com/phd/corrmapper-is-finished</id><content type="html" xml:base="https://danielhomola.com/phd/corrmapper-is-finished/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;CorrMapper was the main project of my &lt;a href=&quot;/assets/DanielHomola_PhD.pdf&quot;&gt;PhD&lt;/a&gt;. It is an online research tool for the integration and visualisation of complex biomedical and omics datasets.&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/ZNwc5aCFonI&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;why-corrmapper-is-needed&quot;&gt;Why CorrMapper is needed?&lt;/h2&gt;

&lt;p&gt;CorrMapper could be best understood by thinking of the problems researchers with complex biomedical datasets face these days.&lt;/p&gt;

&lt;h3 id=&quot;analytical-platforms-are-getting-a-lot-cheaper&quot;&gt;Analytical platforms are getting a lot cheaper&lt;/h3&gt;

&lt;p&gt;Most of the &lt;strong&gt;analytical platforms&lt;/strong&gt; we use today in life sciences and medical research &lt;strong&gt;are getting cheaper each year&lt;/strong&gt;. Some of them are getting cheaper at a ridiculous rate. &lt;/p&gt;

&lt;p&gt;The sequencing and assembly of the first human genomes cost hundreds of millions of dollars ($3 billion by the US government funded project 1 and $300,000,000 by the private Celera initiative) and took 11 and 3 years respectively.&lt;/p&gt;

&lt;p&gt;Yet today, less than 15 years later, we are about to pass the $1000 price point in human genome sequencing, with the Illumina HiSeq X Ten, which will be capable of sequencing 18,000 human genomes per year, to the gold standard of 30× coverage.&lt;/p&gt;

&lt;h3 id=&quot;rise-of-multi-omics-studies&quot;&gt;Rise of multi-omics studies&lt;/h3&gt;

&lt;p&gt;Usually, when products get cheaper two things happen:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;more people start using them,&lt;/li&gt;
  &lt;li&gt;people use more of them. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If this happens to several analytical platform simultaneously, then researchers with the same budget will suddenly be able to design studies which utilise multiple platforms at once. This is precisely what happened in the past 10 years in life sciences and &lt;strong&gt;&lt;em&gt;multi-omics&lt;/em&gt; studies are becoming a lot more popular and affordable&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Multi-omics just means that we have more than one omics dataset, where omics is the terminology used in life sciences to collectively refer to the data coming from genomics, transcriptomics, metagenomics, metabolomics, etc.&lt;/p&gt;

&lt;p&gt;Multi-omics studies have great potential as they allow us to examine the biology behind a disease from multiple viewpoints, each analytical platform opening a new window to the underlying biochemical processes.  &lt;/p&gt;

&lt;p&gt;For example the change of gene expression in colon cancer is just as important as the changes in epigenomic markers, or the gut microbiome which cannot be ignored neither, as there seems to be a complex, multi-level interplay between the bugs in our gut and our health.&lt;/p&gt;

&lt;h3 id=&quot;multi-omics-studies-have-too-many-features&quot;&gt;Multi-omics studies have too many features&lt;/h3&gt;

&lt;p&gt;How do we relate these disparate datasets and combine them so that their complimentary information could be harnessed to expand our biomedical knowledge?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Modern omics datasets are extremely feature rich&lt;/strong&gt; and in multi-omics studies this complexity is compounded by a second or even third dataset. Many of these features however, might be completely irrelevant to the studied biological problem, or redundant in the context of others (multicollinearity). We wouldn’t expect for example to find all 25000 human genes to be involved in breast cancer, or all urinary metabolites as candidate biomarkers for liver failure.&lt;/p&gt;

&lt;h3 id=&quot;multi-omics-studies-are-hard-to-visualise&quot;&gt;Multi-omics studies are hard to visualise&lt;/h3&gt;

&lt;p&gt;Learning from such feature rich datasets inevitably incurs an increased &lt;strong&gt;computational cost&lt;/strong&gt;. It also increases the chance of &lt;strong&gt;over- fitting the noise&lt;/strong&gt; in our data, while reducing the predictive power of our models. Finally, the correlation networks arising from these high-throughput datasets are often hard to interpret and explore due to their density and lack of interactive tools.&lt;/p&gt;

&lt;h3 id=&quot;clinical-metadata-presents-additional-complexity&quot;&gt;Clinical metadata presents additional complexity&lt;/h3&gt;

&lt;p&gt;Finally, biomedical studies will have &lt;strong&gt;increasing amounts of metadata&lt;/strong&gt; attached to the actual omics measurements, making the stratification of patients easier than ever before. This is largely due to the explosion of digital/wearable health gadgets and the radical improvement in the digitization of healthcare records.&lt;/p&gt;

&lt;h2 id=&quot;how-does-corrmapper-work&quot;&gt;How does CorrMapper work?&lt;/h2&gt;

&lt;p&gt;CorrMapper attempts the near impossible and address several of these problems at once.&lt;/p&gt;

&lt;h3 id=&quot;corrmappers-pipeline&quot;&gt;CorrMapper’s pipeline&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/corrmapper_pipeline.png&quot;&gt;&lt;img src=&quot;/assets/images/corrmapper_pipeline.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;interactive-metadata-explorer&quot;&gt;Interactive metadata explorer&lt;/h3&gt;

&lt;p&gt;CorrMapper provides a&lt;strong&gt; completely automatically generated metadata explorer&lt;/strong&gt; which allows researchers to explore and stratify their patient cohort with an &lt;strong&gt;interactive dashboard&lt;/strong&gt; which seamlessly integrates metadata with up to two omics datasets.&lt;/p&gt;

&lt;h3 id=&quot;advanced-feature-selection&quot;&gt;Advanced feature selection&lt;/h3&gt;

&lt;p&gt;Several &lt;strong&gt;cutting edge feature selection algorithms&lt;/strong&gt; have been built into CorrMapper to allow researchers to focus their attention to only those features which have the most discriminatory power with respect to a metadata variable, for example cancer vs. control. This not only decreases the computational cost of subsequent analysis steps but also&lt;strong&gt; helps with the interpretation of the data&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;estimation-of-sparse-covariance-structures&quot;&gt;Estimation of sparse covariance structures&lt;/h3&gt;

&lt;p&gt;The selected features are then used to estimate a sparse inverse covariance matrix using the graphical lasso algorithm. This is useful because under Gaussian assumptions this inverse covarience matrix will only be zero if the row and column variables of the given cell are conditionally independent given all other features in the matrix.&lt;/p&gt;

&lt;p&gt;In plain English, we can work out which variables are &lt;strong&gt;conditionally independent&lt;/strong&gt; (having removed all the confounding effects of the others). This is hugely important because without this, the complexity of biological systems would almost certainly guarantee that we will see a lot of spurious and confounded correlations in our analysis. Based on the inverse covariance matrix we can draw a &lt;strong&gt;network of correlated variables&lt;/strong&gt;, see below.&lt;/p&gt;

&lt;h3 id=&quot;robust-estimation-of-statistical-significance&quot;&gt;Robust estimation of statistical significance&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;edges of the network represent Spearman rank correlations&lt;/strong&gt; for which p values are estimated using 10000 permutations. The &lt;em&gt;p-values&lt;/em&gt; are made more precise using a Generalized Pareto Distribution based method. Finally the p values are corrected for multiple testing using one of the user selected methods.&lt;/p&gt;

&lt;h3 id=&quot;highly-interactive-visualisation-of-correlation-networks&quot;&gt;Highly interactive visualisation of correlation networks&lt;/h3&gt;

&lt;p&gt;The resulting &lt;strong&gt;heatmap and networks of correlations&lt;/strong&gt; are then simultaneously visualised and &lt;strong&gt;interlinked&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If the uploaded datasets have genomic features, this allows CorrMapper to use a more appropriate &lt;strong&gt;genomic network visualisation&lt;/strong&gt;, where the features are laid out in clockwise fashion along the genome of the given species.&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="phd" /><category term="python" /><category term="feature selection" /><category term="data integration" /><category term="data visualisation" /><summary type="html">A biomedical data integration and visualisation platform leveraging advanced feature selection and covariance estimation techniques.</summary></entry><entry><title type="html">Newton’s method with 10 lines of Python</title><link href="https://danielhomola.com/learning/newtons-method-with-10-lines-of-python/" rel="alternate" type="text/html" title="Newton’s method with 10 lines of Python" /><published>2016-02-09T14:51:00+00:00</published><updated>2016-02-09T14:51:00+00:00</updated><id>https://danielhomola.com/learning/newtons-method-with-10-lines-of-python</id><content type="html" xml:base="https://danielhomola.com/learning/newtons-method-with-10-lines-of-python/">&lt;h2 id=&quot;problem-setting&quot;&gt;Problem setting&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;Newton's method, which is an old numerical approximation technique that could be used to find the roots of complex polynomials and any differentiable function. We'll code it up in 10 lines of Python in this post.&lt;/p&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;Let's say we have a complicated polynomial:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex f(x)=6x^5-5x^4-4x^3+3x^2 $&lt;/p&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;and we want to find its roots. Unfortunately we know from the &lt;a href=&quot;https://www.wikiwand.com/en/Galois_theory&quot;&gt;Galois theory&lt;/a&gt; that there is no formula for a 5th degree polynomial so we'll have to use numeric methods. Wait a second. What's going on? We all learned the quadratic formula in school, and there are formulas for cubic and quartic polynomials, but Galois proved that no such &quot;root-finding&quot; formula exist for fifth or higher degree polynomials, that uses only the usual algebraic operations (addition, subtraction, multiplication, division) and application of radicals (square roots, cube roots, etc).&lt;/p&gt;

&lt;h3 id=&quot;bit-more-maths-context&quot;&gt;Bit more maths context&lt;/h3&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;Some &lt;a href=&quot;https://www.reddit.com/user/pqnelson&quot;&gt;nice&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/user/KalROFL&quot;&gt;guys&lt;/a&gt; pointed out on reddit that I didn't quite get the theory right. Sorry about that, I'm no mathematician by any means. It turns out that this polynomial could be factored into $latex x^2(x-1)(6x^2 + x - 3)$ and solved with traditional cubic formula.&lt;/p&gt;
&lt;p&gt;Also the theorem I referred to is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem&quot;&gt;Abel-Ruffini Theorem&lt;/a&gt; and it only applies to the solution to the general polynomial of degree five or greater. Nonetheless the example is still valid, and demonstrates how would you apply Newton's method, to any polynomial, so let's crack on.&lt;/p&gt;

&lt;h3 id=&quot;simplest-way-to-solve-it&quot;&gt;Simplest way to solve it&lt;/h3&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;So in these cases we have to resort to numeric linear approximation. A simple way would be to use the &lt;a href=&quot;https://www.wikiwand.com/en/Intermediate_value_theorem&quot;&gt;intermediate value theorem&lt;/a&gt;, which states that if $latex f(x)$ is continuous on $latex [a,b]$ and $latex f(a) &amp;lt; y&amp;lt; f(b)$, then there is an $latex x$ between $latex a$ and $latex b$ so that $latex f(x)=y$.&lt;/p&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;We could exploit this by looking for an $latex x_1$ where $latex f(x_1)&amp;gt;0$ and an $latex x_2$ where $latex f(x_2)&amp;lt;0$, and then we could be certain that $latex f(x)=0$ must be between $latex x_1$ and $latex x_2$. So we could  check $latex x_3=\frac{x_2-x_1}{2}$, and find that $latex f(x_3)$ will be positive, then continue with$latex x_4=\frac{x_2-x_3}{2}$ which would be negative and our proposed $latex x_n$ would become closer and closer to $latex f(x_n)=0$. This method however can be pretty slow, so Newton devised a better way to speed things up (when it works).&lt;/p&gt;

&lt;h3 id=&quot;how-newton-solved-it&quot;&gt;How Newton solved it&lt;/h3&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;If you look at the figure below, you'll see the plot of our polynomial. It has three roots at 0, 1, and somewhere in-between. So how do we find these?&lt;/p&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;In Newton's method we take a random point $latex f(x_0)$, then draw a tangent line through $latex x_0, f(x_0)$, using the derivative $latex f'(x_0)$. The point $latex x_1$ where this tangent line crosses the $latex x$ axis will become the next proposal we check. We calculate the tangent line at $latex f'(x_1)$ and find $latex x_2$. We carry on, and as we do $latex |f(x_n)| \to 0$, or in other words we can make our approximation as close to zero as we want, provided we are willing to continue with the number crunching. Using the fact that the slope of tangent (by the definition of derivatives) at $latex x_n$ is $latex f'(x_n)$ we can derive the formula for $latex x_{n+1}$, i.e. where the tangent crosses the $latex x$ axis:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/newton_polynomial.png&quot; alt=&quot;Newton's method&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p style=&quot;text-align: justify;&quot;&gt;With all this in mind it's easy to write an algorithm that approximates $f(x)=0$ with arbitrary error $latex \varepsilon$. Obviously in this example depending on where we start $latex x_0$ we might find different roots.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;newtons_method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Root is at: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'f(x) at root is: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There you go, we’are done in 10 lines (9 without the blank line), even less without the print statements.&lt;/p&gt;

&lt;h3 id=&quot;in-use&quot;&gt;In use&lt;/h3&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;So in order to use this, we need two functions, $latex f(x)$ and $latex f'(x)$. For this polynomial these are:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;Now we can simply find the three roots with:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;x0s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;newtons_method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Root is at:  0
# f(x) at root is:  0
# Root is at:  0.628668078167
# f(x) at root is:  -1.37853879978e-06
# Root is at:  1
# f(x) at root is:  0&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;All of the above code, and some additional comparison test with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scipy.optimize.newton&lt;/code&gt; method can be found in this &lt;a href=&quot;https://gist.github.com/danielhomola/de1726bfc7330b2c9b2a&quot;&gt;Gist&lt;/a&gt;. And don’t forget, if you find it too much trouble differentiating your functions, just use &lt;a href=&quot;http://live.sympy.org/&quot;&gt;SymPy&lt;/a&gt;, I wrote about it &lt;a href=&quot;http://danielhomola.com/2016/02/06/solving-real-world-problems-with-sympy/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Newton’s method is pretty powerful but there could be problems with the speed of convergence, and awfully wrong initial guesses might make it not even converge ever, see &lt;a href=&quot;https://www.wikiwand.com/en/Newton's_method#/Practical_considerations&quot;&gt;here&lt;/a&gt;. Nonetheless I hope you found this relatively useful.. Let me know in the comments.&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="learning" /><category term="newton's method" /><category term="python" /><category term="optimization" /><summary type="html">One of the oldest tricks in the book of numerical optimization</summary></entry><entry><title type="html">Solving “real world” problems with SymPy</title><link href="https://danielhomola.com/learning/solving-real-world-problems-with-sympy/" rel="alternate" type="text/html" title="Solving “real world” problems with SymPy" /><published>2016-02-06T18:51:00+00:00</published><updated>2016-02-06T18:51:00+00:00</updated><id>https://danielhomola.com/learning/solving-real-world-problems-with-sympy</id><content type="html" xml:base="https://danielhomola.com/learning/solving-real-world-problems-with-sympy/">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sympy.org/en/index.html&quot;&gt;SymPy&lt;/a&gt; is an amazing library for symbolic mathematics in Python. It’s like &lt;a href=&quot;http://www.wolfram.com/mathematica/&quot;&gt;Mathematica&lt;/a&gt;, and its &lt;a href=&quot;http://live.sympy.org/&quot;&gt;online shell&lt;/a&gt; version along with &lt;a href=&quot;http://www.sympygamma.com/&quot;&gt;SymPy Gamma&lt;/a&gt;  is pretty much like&lt;a href=&quot;https://www.wolframalpha.com/&quot;&gt; Wolfram Alpha&lt;/a&gt; (WA).&lt;/p&gt;

&lt;h3 id=&quot;problem-setting&quot;&gt;Problem setting&lt;/h3&gt;
&lt;p&gt;In this dummy example we’ll go to an imaginary gallery and figure out how far we should stand from a huge painting on the wall so we have the widest possible viewing angle.&lt;/p&gt;

&lt;p&gt;If you have a look at the image below you’ll see, that the bottom of the painting is 3 meters above our level of eyesight and the top of the painting is 15 meters above it. You can imagine that if you were standing right below the painting you wouldn’t see much of it. Same thing would happen if you stood very far away. But where exactly should you stand to get the best viewing angle? I took this example from &lt;a href=&quot;https://mooculus.osu.edu/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/sympy.jpg&quot;&gt;&lt;img src=&quot;/assets/images/sympy.jpg&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;So we are looking for the maximum $latex \theta$ as we are moving closer to or further away from the painting, effectively varying $latex x$. Basic geometry tells us that we can find this angle as the difference of two related angles $latex \alpha-\beta$, and trigonometry tells us that&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;$latex f(x) = \theta = \arctan \left(\frac{15}{x}\right)-\arctan \left(\frac{3}{x}\right) $.&lt;/p&gt;

&lt;h3 id=&quot;on-paper&quot;&gt;On paper&lt;/h3&gt;
&lt;p&gt;We want to find the maximum of this function with respect to $latex x$. We need to differentiate it, find its critical points and check at which of those critical points the second derivative is negative (i.e. there we’ll have the max of $latex f(x)$. We could do this manually remembering that&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;$latex \frac{d}{dx}\arctan x=\frac{1}{1+x^2},$&lt;/p&gt;

&lt;p&gt;but what’s the fun in that? :) &lt;/p&gt;

&lt;h3 id=&quot;using-sympy&quot;&gt;Using SymPy&lt;/h3&gt;
&lt;p&gt;So let’s type a few lines of code into &lt;a href=&quot;http://live.sympy.org/&quot;&gt;SymPy’s online shell&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;critical&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;critical&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;critical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;critical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;breaking-it-down&quot;&gt;Breaking it down&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;In the first line we define $latex f(x)$.
    &lt;ul&gt;
      &lt;li&gt;The cool thing here is that this is done symbolically, as we would do it on a piece of paper.  &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In the 2nd line we differentiate this function.
    &lt;ul&gt;
      &lt;li&gt;This is done as it would be done on paper. If you now type $latex df$, you’ll get the derivative:&lt;/li&gt;
      &lt;li&gt;$latex f’(x) = -\frac{15}{x^2 \left( 1+\frac{255}{x^2}\right)}+\frac{3}{x^2 \left( 1+\frac{9}{x^2}\right)}$&lt;/li&gt;
      &lt;li&gt;Even if I had looked up $latex \frac{d}{dx} \text{arctan}(x)$, it would have taken a bit of time for me to get here to be honest..&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In the third line we get the second derivative,&lt;/li&gt;
  &lt;li&gt;In the 4th we find the critical points of $latex f(x)$.
    &lt;ul&gt;
      &lt;li&gt;Again everything is symbolic, nothing would make sense in pure Python, but it works beautifully in SymPy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In the 5th line we print the critical points.
    &lt;ul&gt;
      &lt;li&gt;We get $latex -3\sqrt{5}$ and $latex 3\sqrt{5}$.&lt;/li&gt;
      &lt;li&gt;Remember, we are looking for $latex x$, which is a distance, so we should be suspicious(to say the least) about the negative value, but we still need to check that $latex f(x)$ actually has a local maximum at $latex 3\sqrt{5}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;That’s exactly what the 6th line does.
    &lt;ul&gt;
      &lt;li&gt;We substitute in the second critical point into the second derivative, and indeed we get a negative value, confirming that $latex f(x)$ has a local maximum at $latex 3\sqrt{5}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In the 7th line we use the $latex N()$ function to print out the numerical value of this expression, which is $latex \approx$ 6.7 meters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;So we should stand about 6.7 meters away from the painting to get the maximum possible viewing angle.&lt;/p&gt;

&lt;p&gt;There you go, in less than 10 lines we did some optimization symbolically in Python. Actually we could have done this in about three lines, sacrificing a bit of clarity, but that’s never a good trade-off..&lt;/p&gt;

&lt;p&gt;As I said, this is barely scratching the surface of what SymPy can do for you, but if you were after an alternative of Wolfram Alpha for symbolic mathematics, I think you’ll find it extremely helpful.&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="learning" /><category term="optimization" /><category term="python" /><category term="sympy" /><summary type="html">Using symbolic maths in Python.</summary></entry><entry><title type="html">MIFS</title><link href="https://danielhomola.com/feature%20selection/phd/mifs-parallelized-mutual-information-based-feature-selection-module/" rel="alternate" type="text/html" title="MIFS" /><published>2016-01-31T15:54:00+00:00</published><updated>2016-01-31T15:54:00+00:00</updated><id>https://danielhomola.com/feature%20selection/phd/mifs-parallelized-mutual-information-based-feature-selection-module</id><content type="html" xml:base="https://danielhomola.com/feature%20selection/phd/mifs-parallelized-mutual-information-based-feature-selection-module/">&lt;h2 id=&quot;quick-summary&quot;&gt;Quick summary&lt;/h2&gt;

&lt;p&gt;I wrapped up three mutual information based feature selection methods in a scikit-learn like module. You can find it on my &lt;a href=&quot;https://github.com/danielhomola/mifs&quot;&gt;GitHub&lt;/a&gt;. It is very easy to use, you can run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;example.py&lt;/code&gt; or import it into your project and apply it to your data like any other scikit-learn method.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mifs&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# load X and y
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'my_X_table.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'my_y_vector.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# define MI_FS feature selection method
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mifs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MutualInformationFeatureSelector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# find all relevant features
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# check selected features
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;support_&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# check ranking of features
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranking_&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# call transform() on X to filter it down to selected features
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X_filtered&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;mutual-information-based-filter-methods&quot;&gt;Mutual information based filter methods&lt;/h3&gt;

&lt;p&gt;In the past twenty years, a large variety of information theory based filter methods were developed. Filter methods represent a subclass of feature selection algorithms which are classifier independent and capture the discriminating power of each feature by calculating some sort of &lt;em&gt;relevance&lt;/em&gt; statistics with respect to the outcome variable. These statistics are used in a heuristic scoring criterion which acts as a proxy measure for classification accuracy. Therefore filter methods can rank the features by their relevance computationally cheaply, without the need of training classifiers on the data.&lt;/p&gt;

&lt;p&gt;Information theory based filter methods assess the mutual information between the $latex m$ features of $latex X^{m \times n}$ and $latex y$. To follow the conventional notation of introductory texts in information theory however, $latex X$ will have a different meaning in this post.&lt;/p&gt;

&lt;p&gt;Information theory is concerned about quantifying the uncertainty present in distributions, by measuring their &lt;strong&gt;entropy&lt;/strong&gt;. The entropy of the distribution of variable $latex X$, denoted as $latex H(X)$, is defined as:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex H(X) = -\sum_{x \in \mathcal{X}}p(x)\log p(x), $&lt;/p&gt;

&lt;p&gt;where $latex x$ denotes a possible value of variable $latex X$, that it can take from a set of values $latex \mathcal{X}$, and $latex p(x)$ is the distribution of $latex X$. This equation quantifies the uncertainty in $latex X$, and for discrete variables it could be computed by estimating $latex p(X)$ as the fraction of observations taking on value $latex x$ from the total $latex N$: $latex \bar{p}(x)=\frac{\#x}{N}.$&lt;/p&gt;

&lt;p&gt;If $latex p(x)$ peaks around a certain value, than the entropy of it will be low, while if it is uniform, meaning all events in $latex X$ are equally likely, it will be high. Furthermore, &lt;strong&gt;conditional entropy&lt;/strong&gt; of two distributions could be defined as:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex H(X|Y)=-\sum_{y \in \mathcal{Y}}p(y) \sum_{x \in \mathcal{X}}p(x|y)\log p(x|y),$&lt;/p&gt;

&lt;p&gt;which represents the amount of uncertainty remaining in$latex X$ after we have seen $latex Y$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mutual information&lt;/strong&gt; (Shannon 1948) between $latex X$ and $latex Y$ is then defined as:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex I(X;Y) = H(X) - H(X|Y)$&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}p(xy)\log\frac{p(xy)}{p(x)p(y)}$&lt;/p&gt;

&lt;p&gt;In this difference the first term represents the uncertainty &lt;em&gt;before&lt;/em&gt; $latex Y$ is known, while the second term captures the uncertainty &lt;em&gt;after&lt;/em&gt; $latex Y$ is known. Thus mutual information could also be thought of, as the amount of uncertainty in $latex X$ that is removed by knowing $latex Y$. Mutual information is symmetric $latex I(X;Y)=I(Y;X)$ and is zero if and only if $latex X$ and $latex Y$ are statistically independent.&lt;/p&gt;

&lt;p&gt;Building on the these concepts, and given we have three discrete random variables $latex X,Y,Z$, the &lt;strong&gt;c&lt;/strong&gt;&lt;strong&gt;onditional mutual information &lt;/strong&gt;could be defined as:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex I(X;Y|Z)=H(X|Z)-H(X|Y,Z)=I(X;Y,Z)-I(X;Z),$&lt;/p&gt;

&lt;p&gt;where $latex I(X;Y,Z)$ is the &lt;strong&gt;joint mutual information&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As described earlier, filter methods attempt to rank the features based on statistical &lt;em&gt;relevance&lt;/em&gt; measures. Therefore the simplest information theoretic filter techniques simply calculate the mutual information between all features and $latex y$ independently, rank them, and select the $latex k$ best ones. This is called Mutual Information Maximisation and it has been used in many early algorithms (Lewis 1992). This method is known to be suboptimal however, when the features are interdependent, as it will end up selecting highly correlated thus &lt;em&gt;redundant &lt;/em&gt;features.&lt;/p&gt;

&lt;p&gt;Using the Joint Mutual Information (JMI) Moody Moody 1999) and Meyer &lt;em&gt;et al.&lt;/em&gt; (Meyer 2008) proposed a method that focuses on increasing the complementary information between the selected features:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex J_{jmi}(X_i) = \sum_{X_j \in S}I(X_i X_j;Y),$&lt;/p&gt;

&lt;p&gt;where $latex J_{jmi}(X_i)$ is the JMI score for feature $latex i$ under consideration, and $latex X_j \in S$ represents all features that were selected in previous iterations of the algorithm. This selection criterion ensures, that the candidate feature $latex X_i$ has to be &lt;em&gt;complementary&lt;/em&gt; with all the previously selected features $latex X_j$ in order to be added to $latex \bar{S}$.&lt;/p&gt;

&lt;p&gt;Brown &lt;em&gt;et al. (&lt;/em&gt;Brown 2012) in their extensive review, systematically benchmarked 17 information theoretic filter methods including the widely used Mutual Information Feature Selection (Battiti 1994) and Max-Relevance Min-Redundancy (Peng 2005) algorithms. They performed a large empirical study to rank these methods by their accuracy, stability and flexibility, and the JMI criterion based feature selection methods were picked as overall winners.&lt;/p&gt;

&lt;h3 id=&quot;how-to-select-features-using-mutual-information&quot;&gt;How to select features using mutual information?&lt;/h3&gt;

&lt;p&gt;OK, see we have these mathematical concepts of entropy and mutual information and joint mutual information. They are very useful as they don’t really assume anything about our data, yet they can somehow capture if two random variables have any sort of similarity or “connection”. But how could we use these concepts to actually perform feature selection?&lt;/p&gt;

&lt;p&gt;Let’s introduce some notation. We have a matrix of data $latex X^{n \times p}$ and an outcome variable $latex y$, which could be discrete or continuous. We want to select a set of features $latex S$ from $latex X$, such that $latex |S| &amp;lt;&amp;lt; p$. Let’s call the set of all features $latex F, |F|=p$. We do this btw to understand which features govern the system mostly and/or to reduce the chance of over-fitting the noise in our data.&lt;/p&gt;

&lt;p&gt;There are two problems to address with mutual information based feature selection:&lt;/p&gt;

&lt;ol style=&quot;text-align: justify;&quot;&gt;
	&lt;li&gt;As the number of features in our dataset ($latex p$) grows, the possible combinations to consider for $latex S$ grows exponentially and becomes intractable even for a few dozen features.&lt;/li&gt;
	&lt;li&gt;Although it is simple to compute entropy and consequently mutual information for discrete random variables, most of the time we have continuous measurements in real life datasets. Rounding them to the nearest integer, might seem tempting but it introduces a bias into our MI estimates. Using binning or histogram based methods are better but still suffer from the same bias issue.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;the-first-problem&quot;&gt;The first problem&lt;/h4&gt;

&lt;p&gt;To overcome the first problem we could calculate the MI between each feature and our outcome variable and select an arbitrary amount of the top-scoring ones. This would be a univariate approach, and require us to assume that all of our features are completely independent from each other. As we have discussed in our earlier &lt;a href=&quot;/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method/&quot; target=&quot;_blank&quot;&gt;post about Boruta&lt;/a&gt;, this is rarely the scenario we find ourselves in.&lt;/p&gt;

&lt;p&gt;A better way would be to calculate the MI between each feature and $latex y$, $latex \forall f_i \in F \text{compute} I(f_i;y)$. Then select $latex f_i$ with the largest $latex I(f_i;y)$, remove it from $latex F$ and add it to $latex S$.&lt;/p&gt;

&lt;p&gt;Then in each consecutive round we would perform a &lt;strong&gt;greedy search&lt;/strong&gt; and find the feature $latex f_i \in F$, which has the maximum joint mutual information with the previously selected features $latex f_i \in S$ and $latex y$:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex \arg\max_{f_i \in F-S}(\sum_{s \in S} (I(f_i, f_s; y)))$&lt;/p&gt;

&lt;p&gt;This is the selection criteria of the &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.4424&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot;&gt;JMI&lt;/a&gt; method and it &lt;a href=&quot;http://people.ee.duke.edu/~lcarin/brown12a.pdf&quot;&gt;was show&lt;/a&gt; to perform best out of 17 information theory based filter methods. As with every greedy search algorithm it can happen that we end up with suboptimal set of $latex S$.&lt;/p&gt;

&lt;p&gt;A recent alteration of this criterion is the &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0957417415004674&quot; target=&quot;_blank&quot;&gt;Joint Mutual Information Maximisation&lt;/a&gt; (JMIM) which uses the maximum of the minimum of the JMI:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;$latex \arg\max_{f_i \in F-S}(\min_{s \in S} (I(f_i, f_s; y))).$&lt;/p&gt;

&lt;p&gt;Beyond these two methods, I also implemented a third MI based filter method the &lt;a href=&quot;http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf&quot; target=&quot;_blank&quot;&gt;Minimum Redundancy Maximum Relevance&lt;/a&gt; algorithm, which is quite famous  and heavily used in life sciences.&lt;/p&gt;

&lt;h4 id=&quot;the-second-problem&quot;&gt;The second problem&lt;/h4&gt;

&lt;p&gt;To overcome the second problem I used the well established &lt;a href=&quot;http://arxiv.org/pdf/cond-mat/0305641.pdf&quot; target=&quot;_blank&quot;&gt;kNN based&lt;/a&gt; MI&lt;a href=&quot;http://rspa.royalsocietypublishing.org/content/464/2093/1203.full-text.pdf&quot; target=&quot;_blank&quot;&gt; estimation methods&lt;/a&gt; in the case when both $latex X$ and $latex y$ are continuous. I used the excellent sklearn based Python implementation of &lt;a href=&quot;https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429&quot; target=&quot;_blank&quot;&gt;Gael Varoquaux&lt;/a&gt; on GitHub to get started.&lt;/p&gt;

&lt;p&gt;Most of the times in life sciences however, we have continuous measurements in $latex X$ and a discrete $latex y$ denoting some class membership like “treatment” vs “control”. None of the previously described MI estimating procedures apply to this scenario and until recently there was no accepted way to deal with this (at least to the best of my knowledge).&lt;/p&gt;

&lt;p&gt;Luckily an extension of the above described kNN method &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357&quot;&gt;got published&lt;/a&gt; recently which deals with this problem. I coded up the method in Python and compared the MI estimates I got from it, with the Matlab implementation that came with the paper. The numbers matched up nicely and the Python version was 5-10 times faster, thanks to the fantastic kNN algorithm in scikit-learn. So the mifs module can be used to select features from a continuous $latex X$ matrix against a categorical or a discrete outcome variable, using mutual information based metrics.&lt;/p&gt;

&lt;h3 id=&quot;going-parallel&quot;&gt;Going parallel&lt;/h3&gt;

&lt;p&gt;Since these filter methods spend most of the CPU time on calculating MI between columns of $latex X$ and $latex y$, which is embarrassingly parallel, I used joblib to make the mifs module run in parallel. It utilizes all the cores the user has and this can easily cut the time spent on feature selection in half or even make it shorter.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Hopefully this easy to use implementation will make these information theory based feature selection methods more appealing to researchers. Please let me know if I cocked something up or if you think I could improve this work. I’m planning to adopt a chapter from my late stage PhD report into another blog post and show some benchmarking results where I compare the MI based filter methods, the Boruta method and other better established FS methods, so stay tuned..&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="feature selection" /><category term="phd" /><category term="JMI" /><category term="JMIM" /><category term="MRMR" /><category term="mutual information" /><category term="python" /><summary type="html">A Python package for parallelized Mutual Information based Feature Selection</summary></entry><entry><title type="html">Linear algebra notes</title><link href="https://danielhomola.com/learning/linear-algebra-notes-and-latex/" rel="alternate" type="text/html" title="Linear algebra notes" /><published>2015-09-25T12:16:00+01:00</published><updated>2015-09-25T12:16:00+01:00</updated><id>https://danielhomola.com/learning/linear-algebra-notes-and-latex</id><content type="html" xml:base="https://danielhomola.com/learning/linear-algebra-notes-and-latex/">&lt;h2 id=&quot;taking-notes&quot;&gt;Taking notes&lt;/h2&gt;

&lt;p&gt;I wanted to take a linear algebra course that focuses mostly on the computation side and applications of the field. I also wanted to learn LaTeX properly so writing my thesis will be less painful (as if that’s possible).&lt;/p&gt;

&lt;p&gt;So, I did both, and wrote a 70-something page long document from my notes of the &lt;a href=&quot;http://www.ulaff.net/&quot;&gt;Linear Algebra Foundations and Frontiers&lt;/a&gt; course by The University of Texas at Austin. It still isn’t completely finished and I’m sure there are tons of typos in it but here it is:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/laff_notes.pdf&quot; target=&quot;_blank&quot; class=&quot;btn btn--primary btn--block&quot;&gt;&lt;i class=&quot;fas fa-file-alt&quot; style=&quot;padding-right:20px&quot;&gt;&lt;/i&gt;LAFF notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It helped me a lot to write things down and understand linalg concepts more in depth, hope it helps you too as quick reference or learning aid.&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="learning" /><category term="linear algebra" /><category term="LAFF" /><category term="MOOC" /><summary type="html">My notes for the LAFF course, written in LaTeX</summary></entry><entry><title type="html">BorutaPy</title><link href="https://danielhomola.com/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method/" rel="alternate" type="text/html" title="BorutaPy" /><published>2015-05-08T18:25:00+01:00</published><updated>2015-05-08T18:25:00+01:00</updated><id>https://danielhomola.com/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method</id><content type="html" xml:base="https://danielhomola.com/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method/">&lt;h2 id=&quot;quick-summary&quot;&gt;Quick summary&lt;/h2&gt;

&lt;p&gt;There’s a pretty clever all-relevant feature selection method, which was conceived by Witold R. Rudnicki and developed by Miron B. Kursa at the ICM UW. Here is the &lt;a title=&quot;Boruta&quot; href=&quot;https://www.jstatsoft.org/article/view/v036i11&quot; target=&quot;_blank&quot;&gt;publication&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While working on my PhD project I read their paper, really liked the method, but didn’t quite like how slow it was. It’s based on R’s Random Forest implementation which runs only on a single core.&lt;/p&gt;

&lt;p&gt;On the other hand, I knew that the core dev team working at scikit-learn on the Random Forest Classifier has made an incredible job at optimizing its performance making it the &lt;a href=&quot;http://www.slideshare.net/glouppe/accelerating-random-forests-in-scikitlearn&quot; target=&quot;_blank&quot;&gt;fastest&lt;/a&gt; implementation currently available. So I thought I’d re-implement the algorithm in Python. It runs pretty fast now, and has a scikit-learn like interface, so you can call fit(), transform() and fit_transform() on it. You can check it out on my &lt;a title=&quot;BorutaPy&quot; href=&quot;https://github.com/danielhomola/boruta_py&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Miron kindly agreed to benchmark it against his original version so it might get modified, but in the meanwhile please feel free to use it and let me know what you think!&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;whats-feature-selection&quot;&gt;What’s feature selection?&lt;/h3&gt;

&lt;p&gt;In many data analysis and modelling projects we end up with hundreds or thousands of collected features. Even worse, sometimes we have way more features than samples. It is a setting that is becoming increasingly common, at least in biomedical sciences for sure. This is called a small n large p situation, which pretty much means you’re left with regularized (preferably lasso) regression techniques if you want to avoid massively over-fitting your data.&lt;/p&gt;

&lt;p&gt;But fortunately most of the time not all your variables are interesting or relevant to the stuff you’re trying to understand/model. So you can try to devise clever ways to select the important ones and incorporate only those into your model, this is called feature selection and there are loads of published methods floating around.&lt;/p&gt;

&lt;h3 id=&quot;doing-the-simple-way&quot;&gt;Doing the simple way&lt;/h3&gt;

&lt;p&gt;Probably the easiest way to go about feature selection, is to do some univariate test on each of your features relating it to the predicted outcome, and select the ones that do pretty well based on some obscenely arbitrary measure or some simple statistical test.&lt;/p&gt;

&lt;p&gt;This is simple and quick, but also univariate, which means it’s probably only useful if you can guarantee that all of your features are completely independent. Unless you are modelling some simulated toy dataset this is never the case.&lt;/p&gt;

&lt;p&gt;In fact, one of the main reasons doing science is so incredibly hard is because of the many intricate unknown relationships between your measured variables and other confounders you haven’t even thought of. OK, so what else?&lt;/p&gt;

&lt;h3 id=&quot;going-multivariate&quot;&gt;Going multivariate&lt;/h3&gt;

&lt;p&gt;Well, you can do feature selection using a multivariate model, like Support Vector Machine. First you fit it to your data and get the weights it assigns to each of your features. Then you can recursively get rid of features in each round which didn’t do too well, until you reach some predefined end point when you stop.&lt;/p&gt;

&lt;p&gt;This is definitely slower, but it is also presumably better as it’s multivariate meaning it will attempt to weigh in the various relationships between your predictive variables and select a set of features that together explain a lot of the variance in your data. On the other hand it is quite arbitrary when you stop, or how many features you get rid off at each round. I hear you say, you can do an exhaustive grid search on these hyper-parameters, or include some cross-validation to make sure you stop at the right point.&lt;/p&gt;

&lt;p&gt;But even then, this method essentially is maximizing a regressor’s or classifier’s performance by selecting an exceedingly pruned version of your input data matrix. This might be a good a thing, but it can also throw away a number of important features.&lt;/p&gt;

&lt;h3 id=&quot;how-is-boruta-different&quot;&gt;How is Boruta different?&lt;/h3&gt;

&lt;p&gt;Boruta is an all-relevant feature selection method. It tries to capture all the important, interesting features you might have in your dataset with respect to an outcome variable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Boruta is an all relevant feature selection method, while most other are minimal optimal; this means it tries to find all features carrying information usable for prediction, rather than finding a possibly compact subset of features on which some classifier has a minimal error.&lt;br /&gt;Miron B. Kursa&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This makes it really well suited for biomedical data analysis, where we regularly collect measurements of thousands of features (genes, proteins, metabolites, microbiomes in your gut, etc), but we have absolutely no clue about which one is important in relation to our outcome variable, or where should we cut off the decreasing “importance function” of these.&lt;/p&gt;

&lt;p&gt;As with so many great algorithms the idea behind Boruta is really simple. First, we duplicate our dataset, and shuffle the values in each column, these are called shadow features.&lt;/p&gt;

&lt;p&gt;Then we train a classifier on our dataset, such that we get importances for each of our features. Tree ensemble methods such as Random Forest, Gradient Boosted Trees, and the Extra Trees Classifiers are really great not only because they can capture non-linear highly intricate relationships between your predictors, but also because they tend to handle the small n large p situation rather well.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why should you care? For a start, when you try to understand the phenomenon that made your data, you should care about all factors that contribute to it, not just the bluntest signs of it in context of your methodology (yes, minimal optimal set of features by definition depends on your classifier choice).&lt;br /&gt;Miron B. Kursa&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is quite a lot of discussion about whether these methods can truly overfit your training data, but generally it is accepted that even if they do, this happens much later with them than with many other machine learning algorithms.&lt;/p&gt;

&lt;p&gt;Back to the Boruta. So we train one of these ensemble methods on our merged training data and shadow features (see the image below). Then we get the relative importance of each feature from the ensemble method: higher means better or more important.&lt;/p&gt;

&lt;p&gt;Then we check for each of our real features if they have higher importance than the best of the shadow features. If they do, we record this in a vector (these are called a hits) and continue with another iteration. After a predefined set of iterations we end up with a table of these hits.&lt;/p&gt;

&lt;p&gt;At every iteration we check if a given feature is doing better then expected than random chance. We do this by simply comparing the number of times a feature did better than the shadow features using a binomial distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/boruta.png&quot; alt=&quot;image-center&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the case of feature #1 in the table below, out of 3 runs it did better than the best of shadow features 3 times. So we calculate a p-value  using the binomial distribution, k=3, n=3, p=0.5. As we do this for thousands of features we need to correct for multiple testing. The original method uses the rather conservative Bonferroni correction for this. We say a feature is confirmed to be important if its corrected p-value is lower than 0.01.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/boruta2.png&quot; alt=&quot;image-center&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then we remove its column from the original data matrix and continue with another iteration.&lt;/p&gt;

&lt;p&gt;Conversely if a feature hasn’t been recorded as a hit in say 15 iterations, we reject it and also remove it from the original matrix. After a set number of iterations (or if all the features have been either confirmed or rejected) we stop.&lt;/p&gt;

&lt;p&gt;For all the parameters and options please check the &lt;a href=&quot;https://github.com/danielhomola/boruta_py&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt; site and the documentation for more details.&lt;/p&gt;

&lt;h2 id=&quot;python-implementation&quot;&gt;Python implementation&lt;/h2&gt;

&lt;p&gt;The Python version requires &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy, scipy, scikit-learn&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bottleneck&lt;/code&gt;, so you’ll need to install these before trying it, but then you can simply apply it to your dataset like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;boruta_py&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boruta_py&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# load X and y
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'my_X_table.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'my_y_vector.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# define random forest classifier, with utilising all cores and
# sampling in proportion to y labels
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'auto'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# define Boruta feature selection method
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boruta_py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BorutaPy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'auto'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# find all relevant features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# check selected features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;support_&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# check ranking of features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranking_&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# call transform() on X to filter it down to selected features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_filtered&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat_selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Again, all of the parameters, attributes and methods are documented with docstrings so grab a copy or fork it and let me know what you think!&lt;/p&gt;</content><author><name>danielhomola</name></author><category term="feature selection" /><category term="phd" /><category term="boruta" /><category term="all relevant" /><category term="python" /><summary type="html">An all relevant feature selection method based on Random Forest estimators</summary></entry></feed>