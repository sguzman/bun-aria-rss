<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by samim on Medium]]></title>
        <description><![CDATA[Stories by samim on Medium]]></description>
        <link>https://medium.com/@samim?source=rss-f3c8148878e1------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*8klK3U-gH-9-AlhBeKUZig.jpeg</url>
            <title>Stories by samim on Medium</title>
            <link>https://medium.com/@samim?source=rss-f3c8148878e1------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 05 Nov 2022 16:32:30 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@samim/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[MokMok]]></title>
            <link>https://medium.com/@samim/mokmok-266f3b85a1d5?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/266f3b85a1d5</guid>
            <category><![CDATA[videogames]]></category>
            <category><![CDATA[music]]></category>
            <category><![CDATA[art]]></category>
            <category><![CDATA[game-development]]></category>
            <category><![CDATA[games]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Tue, 01 Aug 2017 20:17:28 GMT</pubDate>
            <atom:updated>2017-08-02T07:14:32.213Z</atom:updated>
            <content:encoded><![CDATA[<h4>A Music Adventure Game</h4><p><a href="http://mokmok.com/"><strong>MokMok</strong></a> is a music adventure game for PC &amp; Mac. Mok´s are <strong>living musical instruments.</strong> They move to the beat they play and together create ever-changing interactive songs. Accompany them on a funky journey and experience music like never before. <a href="http://store.steampowered.com/app/671480/MokMok/"><strong>MokMok Episode One is out now</strong></a><strong>!</strong></p><blockquote><em>MokMok is made by </em><strong><em>2Beats</em></strong><em>, lead by </em><a href="https://twitter.com/marc_lauper"><strong><em>@marc_lauper</em></strong></a><strong><em> &amp; </em></strong><a href="https://twitter.com/samim"><strong><em>@samim</em></strong></a><strong><em>.</em></strong></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D-PTpyuRitRGiZ0q0_pDuw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ljehelWKt9to2p36d4gnKA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uZVRHY3CIu0uQIXMmPrsmA.png" /></figure><h3><strong>MokMok: </strong>Play with Music</h3><p>The <strong>MokMok</strong> project started with a dream: Making music should be as easy and fun as playing a game. A music <em>album</em> should be an immersive journey, in which you could experience and create funky music casually. We dreamt of a new hybrid of instrument and game, which lets you <strong>Play with Music.</strong></p><p>The result is <strong>MokMok - A Music Adventure Game. </strong>In MokMok you are a visitor on an ancient alien world. This bizarre place is inhabited by Mok’s - living musical instruments. During the games journey you’ll discover and collect many different Mok’s, learn to play music with them, perform mystical rituals and uncover the secrets of MokMok. <a href="https://www.youtube.com/watch?v=8AOiBGQKHNA"><strong>Watch the Trailer</strong></a><strong>:</strong></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F8AOiBGQKHNA%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D8AOiBGQKHNA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F8AOiBGQKHNA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/7908709d4a7ad10bbd94c530e486d591/href">https://medium.com/media/7908709d4a7ad10bbd94c530e486d591/href</a></iframe><h3>Meet the Moks</h3><p>Mok’s are creatures that love to make music. Each Mok plays a unique sound pattern and jumps, vibrates or blows to the rhythm of its beat. Together, the Mok’s perform songs. By playing with them, ever-changing music is generated. This is the soundtrack for your adventures in the world of Mok.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ulWPj4p1HgRjWUC3MO4mmA.jpeg" /></figure><h3>Experiments</h3><p>MokMok explores a range of experimental music and gaming techniques, attempting to imagine a new kind of playful music experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qBG9HTzH4tRRzwszDD59WQ.jpeg" /></figure><p><strong>Synesthesia: </strong>In MokMok the visuals and music are tightly coupled, to create a <a href="https://en.wikipedia.org/wiki/Synesthesia">synesthetic</a> experience. The animations of the Mok’s are driven by the sound they play: Kick-Drum Mok’s jump, String Mok’s vibrate and Horn Mok’s blow. This let’s you <em>see</em> musical rhythm in 3D and play with it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WFtGIiaCNNkzlDNbALuJ_g.png" /></figure><p><strong>Spatial Music: </strong>All music in MokMok lives in 3D space - mimicking how sound behaves in nature. The closer a Mok is to the player, the louder its sound gets. The music dynamically changes, depending on where you and the Mok’s move to. This let’s you playfully remix music in countless ways.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eNkozpVVuBf-YBjx6hcHqQ.jpeg" /></figure><p><strong>MokAI:</strong> Each Mok is driven by a <a href="http://www.gamasutra.com/blogs/ChrisSimpson/20140717/221339/Behavior_trees_for_AI_How_they_work.php">behaviour tree</a> based game A.I system. This simple, yet effective approach defines the objectives, character traits and music of the Mok’s. Excited Mok’s play upbeat tunes, scared Mok’s play quite tunes and at the end of a quest, a group of Mok’s play a celebratory song.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KszbWcD2Xipksaz7CPAnnQ.jpeg" /></figure><p><strong>Interactive Music: </strong>In MokMok there are many ways to influence what music the Mok’s play. Through simple interactions, you can make Mok’s follow you, change their musical patterns and perform funky rituals. Think of it as exploring how to play a living musical instrument.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*qUhSkBskcL7N9x56ymKomg.gif" /></figure><p><strong>Procedural Generation:</strong> The bizarre world of MokMok was made with a mix of classic <em>manual</em> design processes, along side procedural generation techniques. Using <a href="https://www.sidefx.com/">Houdini</a>, many design variations were generated and then explored. Experience one of our finest worlds in <a href="http://store.steampowered.com/app/671480/MokMok/"><strong>MokMok Episode 1</strong></a><strong>.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*efAhn8P3AtGT0cJKNkRB4w.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*zHzhF09ohsmn82e9yb3wQA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pBuh6AJGHMHIQpVsaRSHsA.jpeg" /><figcaption><strong>Episodic Content: </strong>Here is an EP 2 Sneak Peak.</figcaption></figure><h3>The Story of MokMok</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jvhsjHnk2aK1G-ro8k8PXg.jpeg" /></figure><p>The development of MokMok has been a fascinating and emotional journey. It is a story of friendship, experimentation and learning. The release of MokMok is a joyous event for team 2Beats. We want to deeply thank all the great people and organisations that made MokMok possible!</p><p>MokMok is made by <strong>2Beats</strong>, lead by <a href="https://twitter.com/marc_lauper"><strong>@marc_lauper</strong></a><strong> &amp; </strong><a href="https://twitter.com/samim"><strong>@samim</strong></a><strong>. <br></strong>Development Support:<strong> </strong><a href="https://twitter.com/mikehergaarden"><strong>@mikehergaarden</strong></a> <strong>&amp;</strong> <strong>Jos Hoebe.</strong><br>Financial Support:<strong> </strong><a href="https://prohelvetia.ch/en/"><strong>Pro Helvetia</strong></a> <strong>&amp;</strong> <a href="https://www.suisa.ch/en/home.html"><strong>Suisa</strong></a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FnPldBgQnKEFJ60jnp90hw.jpeg" /></figure><h3><a href="http://store.steampowered.com/app/671480/MokMok/">MokMok Episode One is available now on Steam</a>, for PC &amp; Mac. Get it now and dance with the Mok’s!</h3><h3>Visit <a href="http://mokmok.com">MokMok.com</a>, share this with your friends and sign up to our <a href="https://docs.google.com/forms/d/e/1FAIpQLSeUXN1JyLJrC6-Rp4qGDNLpn6PuDtS93l5jUhv3Z87NEjZpvw/viewform?formkey=dHlYeHJLeHFiZl80eVNtSnhjWFdtdFE6MQ#gid=0">mailing list</a>. May the Mok be with you.</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/160/1*PoW1nNAsUHb7cHxGSzrA_Q.png" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=266f3b85a1d5" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Assisted Writing]]></title>
            <link>https://medium.com/@samim/assisted-writing-7adea9aed19?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/7adea9aed19</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[natural-language]]></category>
            <category><![CDATA[writing]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[word-processing]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Wed, 07 Jun 2017 18:14:51 GMT</pubDate>
            <atom:updated>2017-06-08T22:55:20.419Z</atom:updated>
            <content:encoded><![CDATA[<h4>Reimagining Word Processing</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*R1kWE3my6Avll0DmEOI22w.jpeg" /></figure><p>Writing is a ancient art form. From Papyrus to MS-Word, the tools we use to write have defined <em>how</em> and <em>what</em> we write. Today, <strong>assisted writing</strong> <strong>tools</strong> are using machine learning techniques to understand, manipulate and generate human language. The implications are profound.</p><p><strong>This experiment tries to re-imagine word-processing software. It explores new forms of writing, that allow authors to shift their focus from creation to curation, and write more joyfully.</strong></p><h3>Writing Text</h3><p>Humans have been writing text, assisted by machines for a very long time. From the first commercial <a href="https://en.wikipedia.org/wiki/Typewriter">typewriters</a> in the 1860&#39;s to today’s laptops, <strong>the act of writing has changed dramatically - yet essentially stayed the same: </strong>We use our fingers and a keyboard to translate our imagination into matter. The human does the <em>creative</em> work, the machine prints words.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KXfPqQ9jt_vMDsr1izVeUQ.jpeg" /></figure><p>In the 1960s, researchers such as <a href="https://en.wikipedia.org/wiki/Douglas_Engelbart"><strong>Douglas Engelbart</strong></a><strong> </strong>and<strong> </strong><a href="https://en.wikipedia.org/wiki/Ted_Nelson"><strong>Ted Nelson</strong></a><strong> </strong>started to <a href="https://en.wikipedia.org/wiki/The_Mother_of_All_Demos">experiment</a> with ideas such as <strong>digital writing tools, </strong>the <a href="https://en.wikipedia.org/wiki/Computer_mouse">mouse</a><strong> </strong>and<strong> </strong><a href="https://en.wikipedia.org/wiki/Hypertext">hypertext</a>. A central goal was to find out how computer systems could augment the human ability to read, write and think. These experiments laid the foundation for a new generation of writing tools.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o5jWbVdUv52zbpJMfwqKIA.jpeg" /><figcaption><strong>“Thought Vectors in Concept Spaces.” — Douglas Engelbart.</strong></figcaption></figure><p>With the advent of personal computing, digital writing tools went mainstream and became the PCs first killer-app: <a href="https://en.wikipedia.org/wiki/Word_processor"><strong>Word Processors</strong></a>. Still using typewriter metaphors, software such as <a href="https://archive.org/details/jot_0.53_ted_nelson">Nelson’s JOT </a>(1972), <a href="https://en.wikipedia.org/wiki/Bravo_(software)">Xerox Parc’s Bravo</a> (1974), <a href="https://en.wikipedia.org/wiki/MacWrite">Apple’s MacWrite</a> (1984) and <a href="https://en.wikipedia.org/wiki/Microsoft_Word">Microsoft’s Word</a> (1989) quickly solidified the contemporary concept of digital text editing. As MS-Word™ conquered the world, <strong><em>how</em> and <em>what</em> we write changed for ever.</strong></p><blockquote><strong>“The medium is the message” </strong>-Marshall McLuhan</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aq75lWOCsO32gJRP02pszQ.jpeg" /><figcaption><strong>The Evolution of Word Processors: </strong><a href="https://en.wikipedia.org/wiki/Bravo_(software)"><strong>Xerox Parc Bravo</strong></a><strong> (1974) | </strong><a href="https://en.wikipedia.org/wiki/MacWrite"><strong>MacWrite</strong></a><strong> (1984) | </strong><a href="https://en.wikipedia.org/wiki/Microsoft_Word"><strong>Microsoft Word</strong></a><strong> (1989)</strong></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/486/1*3aPNVLAQq5OG1qzhUz5rsg.jpeg" /><figcaption><strong>A representation of </strong><a href="https://en.wikipedia.org/wiki/Writer%27s_block"><strong>writers-block</strong></a><strong> by </strong><a href="https://en.wikipedia.org/wiki/Leonid_Pasternak"><strong>Leonid Pasternak</strong></a><strong> (1862–1945)</strong></figcaption></figure><h3>Re-imagining Word Processing</h3><p>For all the advanced features digital word processors have given authors, current tools do not address many fundamental questions: <em>How do i write good story? How do i find inspiration &amp; overcome </em><a href="https://en.wikipedia.org/wiki/Writer%27s_block"><em>writers-block</em></a><em>?</em> In these areas, writing has stayed the same for aeons: The human does the creative work, the machine prints words.</p><p><strong>Today</strong>, new techniques are redefining our relationship with our writing tools. Since the early days of <a href="https://en.wikipedia.org/wiki/Autocorrection">Auto-Correct</a>, <strong>computer assisted writing </strong>has evolved into a active research field. Academia and Industry have embraced a range of <strong>Machine Learning</strong> and<strong> Natural Language Processing</strong> techniques, to <strong>understand, manipulate and generate human language.</strong></p><p>The implications of these highly accessible capabilities for word processors design are profound. It is becoming possible to imagine a very different kind of writing software:<strong> a tool that actively inspires and guides the writing process, allows authors to widen the frame of reference, shift their focus from creation to curation, and write more joyfully.</strong></p><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> | <a href="http://samim.io">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UounJisiRdamB-jnBa7ljg.jpeg" /></figure><h3>10 Assisted Word Processing Capabilities</h3><p>The following capabilities are building blocks for designing new types of word processing software. Each capability references a research project or open-source project to illustrate it’s concept. While the result quality of some projects is still not production ready, it is a useful indicator of where things are going. The shared goal is to mobilize the intelligence of authors and enable new playful writing experiences, assisted by machines.</p><h4>01. Recommend</h4><p><strong><em>Recommend</em></strong> addresses the problem of context. It <a href="https://arxiv.org/abs/1609.02116">recommends</a> related texts (books etc.) on the fly, based on the text the author is currently writing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o1rwAcICuXzsKDwwdkPfZg.jpeg" /></figure><h4>02. Summarise</h4><p><strong><em>Summarise</em></strong> addresses the problem of text length and reading times. It automatically <a href="https://www.quora.com/Has-Deep-Learning-been-applied-to-automatic-text-summarization-successfully">summarises text</a> using <a href="https://arxiv.org/abs/1512.01712">neural networks</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f6sKc_8v9eEgzReGzDdP4Q.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZdKqdxxW4VymOw0fceCNbA.jpeg" /><figcaption><a href="https://arxiv.org/abs/1512.01712"><strong>Generating News Headlines with Recurrent Neural Networks</strong></a></figcaption></figure><h4>03. Simplify</h4><p><strong><em>Simplify</em></strong> addresses the problem of text complexity. It automatically <a href="https://arxiv.org/abs/1704.02312">simplifies sentences</a> using neural networks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F5gOPYV6AyAV0pBnl_vsGw.jpeg" /></figure><h4>04. Morph</h4><p><strong><em>Morph</em></strong> takes two (or more) text-passages and blends in-between them. A neural net fills in the blanks, <a href="https://arxiv.org/abs/1511.06349">generating sentences from a continuous space</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*73cjWSK4XMXBVR2fxACSLg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/751/1*89vegjxK67Ujo4RM2Oi5MQ.jpeg" /><figcaption><a href="https://arxiv.org/abs/1511.06349"><strong>Generating Sentences from a Continuous Space</strong></a></figcaption></figure><h4>05. Transfer</h4><p><strong><em>Transfer</em></strong> addresses the problem of content and style. Thanks to Neural Nets, we can <a href="https://github.com/tokestermw/tensorflow-shakespeare">translate our texts into the style of any famous writer</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uxB_b1XXB5wtwsamcXK2JA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/755/1*JtpWCtA0nHD0uhmwqNT0FA.jpeg" /><figcaption><a href="http://visual.cs.ucl.ac.uk/pubs/handwriting/"><strong>My Text in Your Handwriting — Generating Handwriting</strong></a><strong>: Style: Frida Kahlo.</strong></figcaption></figure><h4>06. Predict</h4><p><strong><em>Predict</em></strong><em> </em>addresses the problem of flow<em>. </em>It let’s us write much faster. Think of it as a <a href="https://en.wikipedia.org/wiki/Predictive_text"><em>predictive text interface</em></a><em>, or</em> <a href="https://www.robinsloan.com/notes/writing-with-the-machine/">“Autocomplete”, powered by a neural net</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zadyv0yJdQbCLdhbpuEwWg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*F4IIi6TtxBS_yl31XZ4Cvg.gif" /><figcaption><a href="https://www.robinsloan.com/notes/writing-with-the-machine/"><strong>“Autocomplete” powered by a neural network</strong></a></figcaption></figure><h4>07. Generate</h4><p><strong><em>Generate</em></strong> addresses the problem of <a href="https://en.wikipedia.org/wiki/Writer%27s_block">writer’s block</a>. It uses machine learning to generate <em>original</em> texts. Topic &amp; Style are selectable (<a href="https://arxiv.org/abs/1610.09889">poetry</a>, <a href="http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP221.pdf">lyrics</a>, etc..)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RgU0WqGim6NeZt1MDieoPQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ho124nc2N-DWWZlzQDJBgA.jpeg" /><figcaption><a href="https://arxiv.org/abs/1610.09889"><strong>Chinese Poetry Generation with Planning based Neural Network</strong></a></figcaption></figure><h4>08. Translate</h4><p><strong>Translate</strong> addresses the problem of modality. By translating between media types we can, for example <a href="https://arxiv.org/abs/1606.07493v5">generate stories from jumbled images</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ppcd9OwB1u7AoALdxrIF5A.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aloBvnMXkkQjiQ4mzfwUsA.jpeg" /><figcaption><a href="https://arxiv.org/abs/1606.07493v5"><strong>Sort Story: Sorting Jumbled Images and Captions into Stories</strong></a></figcaption></figure><h4>09. Collaborate</h4><p><strong><em>Collaborate</em></strong> addresses the pain of writing, alone or in teams. It puts a <a href="https://projects.csail.mit.edu/soylent/">crowd inside our word processors</a> and let’s us easily <a href="https://arxiv.org/abs/1611.02682">crowdsourcing complex work</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LNevwmrCB8pNoB3ek2mukg.jpeg" /></figure><h4>10. Interface</h4><p><strong><em>Interface</em></strong> addresses the problem of input output. Good speech recognition and <a href="http://research.baidu.com/deep-voice-2-multi-speaker-neural-text-speech/">text-to-speech system that can master hundreds of accents</a> are here.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wbC2xdB_Y1BIkFUhZU5mCw.jpeg" /></figure><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fw.soundcloud.com%2Fplayer%2F%3Furl%3Dhttps%253A%252F%252Fapi.soundcloud.com%252Ftracks%252F318661840%26show_artwork%3Dtrue&amp;display_name=SoundCloud&amp;url=https%3A%2F%2Fsoundcloud.com%2Fuser-535691776%2Fdialog&amp;image=https%3A%2F%2Fsoundcloud.com%2Fimages%2Ffb_placeholder.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=soundcloud" width="800" height="166" frameborder="0" scrolling="no"><a href="https://medium.com/media/023c3e65534b57d1c16e5aa3a487a2a4/href">https://medium.com/media/023c3e65534b57d1c16e5aa3a487a2a4/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HVxbZkg-CheIkK6jK9k-Vg.jpeg" /></figure><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FohmajJTcpNk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DohmajJTcpNk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FohmajJTcpNk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/bee5b30bf53e8f2668f05c2b0b872d82/href">https://medium.com/media/bee5b30bf53e8f2668f05c2b0b872d82/href</a></iframe><h3>And more…</h3><p>The presented capabilities only scratch the surface of what is currently possible. Re-imagining word processors with machine learning is a very large canvas, transcending traditional notions of reading and writing. One can easily imagine new forms of “writing”, such as auto e-book generation or auto re-enactment by virtual actors of the text one has just written.</p><h3>Final Thoughts</h3><p>By combining the presented capabilities, we arrive at a very different view of what a word processors can be: a tool that actively inspires and guides the writing process, allows authors to widen the frame of reference, shift their focus from creation to curation, and <strong>write more joyfully.</strong></p><p>The coming generation of intelligent content read-writers (<em>word-processors</em>) will be able to understand, manipulate and generate human language in many new, seemingly magical ways. Such writing tools will mobilize the intelligence of authors and enable them to <strong>playfully write the next great human story, assisted by machines.</strong></p><blockquote>“Writing is easy. All you have to do is cross out the wrong words.” ― <a href="http://www.goodreads.com/author/show/1244.Mark_Twain"><strong>Mark Twain</strong></a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wrUp1v-rWDiGrDHY1M3Obg.jpeg" /></figure><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> | <a href="http://samim.io">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/1*uYcsrIR-wFS74jiR282Asg.gif" /></figure><h3>Follow Up Discussion</h3><h3>vakibs on Twitter</h3><p>@samim Also see @TheTedNelson arguing about why we still don&#39;t have a fully functional &quot;cut and paste&quot; for text editing. https://t.co/uD6uxT9xaZ</p><h3>samim on Twitter</h3><p>Augmenting Literacy: The Role of Expertise in Digital Writing&quot;: https://t.co/17pYw4aPnl</p><h3>Patrick Hebron on Twitter</h3><p>@samim I&#39;d like to see more draft management / smart versioning and UI around section folding / ToC management. Help the divergent thinker!</p><h3>James Ryan on Twitter</h3><p>@samim great article! here&#39;s some related work out of USC: https://t.co/1G4v6t7eUw</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7adea9aed19" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Musical Novelty Search]]></title>
            <link>https://medium.com/@samim/musical-novelty-search-2177c2a249cc?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/2177c2a249cc</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[evolution]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[music]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Mon, 29 May 2017 13:52:11 GMT</pubDate>
            <atom:updated>2017-05-30T17:18:44.490Z</atom:updated>
            <content:encoded><![CDATA[<h4>Evolutionary Algorithms + Ableton Live</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nrISIxgLegOu23J9HjwCxg.jpeg" /></figure><p>Making music with computer tools is delightful. Musical ideas can be explored quickly and composing songs is easy. Yet for many, these tools are overwhelming: An ocean of settings can be tweaked and it is often unclear, which changes lead to a great song. <strong>This experiment investigates how to use evolutionary algorithm and novelty search to help musicians find musical inspiration in Ableton Live.</strong></p><h3>Making Music With Computers</h3><p>In the past 30 years, the art of making music has dramatically changed. As computers gradually conquered most aspects of the creative process, <a href="https://en.wikipedia.org/wiki/Digital_audio_workstation">Digital Audio Workstation</a> (DAW) Software has become the primary tool to produce music of all sorts: from Techno &amp; Hiphop to Classical, Jazz &amp; Doomcore.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gYmsP2QIilBvkaPIT5Bc8A.jpeg" /></figure><p>Today, <a href="https://www.ableton.com/"><strong>Ableton Live</strong></a> is as a leading DAW. It enables anyone even with just minimal training to compose professional sounding music. Simply by loading a few audio-samples or a synthesiser and then <strong>manually exploring possible combinations and settings, </strong>musical ideas start to emerge within seconds.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*JZo3mO4K-PY3Nqn_nWZ99A.jpeg" /><figcaption>Ableton Live Clip View: <em>Play on Pad1 on Channel1 + Drum1 on Channel3 + Melody6 etc.</em></figcaption></figure><p>While manual exploration is enjoyable for small collections, imagine dealing with 1000 audio-samples or synthesiser settings. As the amount of total possible combinations <a href="https://en.wikipedia.org/wiki/Combinatorial_explosion">explodes</a>, manually searching for desired outcomes becomes very time consuming and thus, most <strong>musical ideas remain hidden.</strong></p><h3>Evolution &amp; Novelty Search</h3><p>Using computational methods to find new music is intriguing: A algorithm could quickly search through countless sample &amp; synth setting combinations, and give us a expansive, browsable map of truly novel musical ideas.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2kpd1gySE1aKCL6MS4YvPg.jpeg" /><figcaption>The basic concept of a Evolutionary Algorithm</figcaption></figure><p><a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm"><strong>Evolutionary Algorithms</strong></a> (EA) have a long history of being used for such interactive music generation experiments. They are a family of optimisation algorithms inspired by the principle of <strong>Darwinian natural selection</strong>. EA’s generate many possible solutions for a problem and then tests how well the solutions solve the problem, using a given <em>fitness function</em>. The fittest solutions <em>procreate</em>.</p><p>When thinking about using a Evolutionary Algorithm to search through musical combinations, a key question is how to define the fitness function — or <strong>how to measure what is good music.</strong> A few years ago <a href="https://twitter.com/kenneth0stanley?lang=en"><strong>Kenneth Stanley</strong></a><strong> -</strong> a computer science professor at the University of Central Florida - published a series of papers outlining a concept which is relevant to this question: <a href="http://eplex.cs.ucf.edu/noveltysearch/userspage/"><strong>Novelty Search</strong></a><strong>, </strong>a evolutionary search strategy which <strong>rewards behaving <em>differently</em> instead of <em>progress</em> to some goal.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JupV4VGkX-ZfStOKZZ3ttw.jpeg" /><figcaption>Evolving Paintings by <a href="http://picbreeder.org/">PicBreeder.org</a></figcaption></figure><p><strong>In a search for novelty there is no pressure to be <em>better</em></strong><em> —</em><strong><em> </em></strong>it simply rewards those solutions that are different. Novelty Search and a group of related approaches are under active development since years and have found application in <a href="http://picbreeder.org/">painting</a>, <a href="https://www.youtube.com/watch?v=Gjoh3lexiRQ">robotics</a> &amp; <a href="http://julian.togelius.com/Liapis2014Constrained.pdf">games</a>. Watch Stanley’s fascinating talk “<a href="https://www.youtube.com/watch?v=dXQPL9GooyI"><strong>The Myth of the Objective</strong></a>” for further infos on the topic.</p><blockquote><strong>“Surprisingly, sometimes <em>not looking for the goal</em> leads to finding the goal more quickly and consistently” — </strong><em>Kenneth Stanley.</em></blockquote><h3>Experiment: Novelty Search Live</h3><p><a href="https://github.com/samim23/Novelty-Search-Live/"><strong>Novelty Search Live</strong></a> is a open-source tool that helps musicians find musical inspiration in Ableton Live. It uses a evolutionary algorithm to continuously evolve new audio clip and synth parameter combinations, guided exclusively by Novelty Search. Finally is takes the thousands of musical ideas it has discovered and generates a interactively browsable map with <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"><em>t-SNE</em></a>.</p><p>While this experiment is just a quick proof of concept, it hints at the option of a <strong>musical inspiration assistant</strong> which guides the creative process.</p><iframe src="https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DgcX5lez_Q9o&amp;src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FgcX5lez_Q9o&amp;type=text%2Fhtml&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/186ff43f4f94a7a6526eea31eaf17315/href">https://medium.com/media/186ff43f4f94a7a6526eea31eaf17315/href</a></iframe><h4><strong>Novelty Search Live — Features:</strong></h4><ul><li>Evolve Clip Combinations, Device &amp; Synths Parameters in realtime.</li><li>Explore possible combination of a live set &amp; discover new musical ideas.</li><li>Play with a interactive map (T-SNE) of generated combinations.</li></ul><h4>&gt;&gt; <a href="https://github.com/samim23/Novelty-Search-Live/">Get Novelty Search Live on GitHub</a> &lt;&lt;</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fw.soundcloud.com%2Fplayer%2F%3Furl%3Dhttp%253A%252F%252Fapi.soundcloud.com%252Ftracks%252F324937944%26show_artwork%3Dtrue&amp;url=https%3A%2F%2Fsoundcloud.com%2Fsamim%2Fnovelty-search-1&amp;image=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000224752249-h3ac17-t500x500.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=soundcloud" width="800" height="166" frameborder="0" scrolling="no"><a href="https://medium.com/media/ae1b4f1bb7a41301091f9db71cfb8416/href">https://medium.com/media/ae1b4f1bb7a41301091f9db71cfb8416/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fw.soundcloud.com%2Fplayer%2F%3Furl%3Dhttp%253A%252F%252Fapi.soundcloud.com%252Ftracks%252F324948993%26show_artwork%3Dtrue&amp;url=https%3A%2F%2Fsoundcloud.com%2Fsamim%2Fnovelty-search-2&amp;image=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000224760426-8viavm-t500x500.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=soundcloud" width="800" height="166" frameborder="0" scrolling="no"><a href="https://medium.com/media/6842eeb7bdfa406becca896dd23a447a/href">https://medium.com/media/6842eeb7bdfa406becca896dd23a447a/href</a></iframe><h4>How does it work?</h4><ol><li>Control Ableton Live from Python code via the <a href="https://github.com/ideoforms/pylive">PyLive</a> library.</li><li>Capture infos (clips/parameters/status) from Live &amp; turn into Vector.</li><li>Evolve Vector with <a href="https://github.com/deap">Deap</a> (Distributed Evolutionary Algorithms in Python) (this should be replaced with a <a href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network">CPPN</a> like <a href="https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies">NEAT</a> for better results)</li><li>Use Novelty Search as fitness function, run for X generations.</li><li>Send evolved vector to Ableton Live and play new music combination.</li><li>Loop 3–5 until user interrupt.</li><li>Show interactive T-SNE map of all evolved solutions.</li></ol><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FuBstLHqWjj4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DuBstLHqWjj4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FuBstLHqWjj4%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/75f80dd97e1842137eb65470a0ab1ab7/href">https://medium.com/media/75f80dd97e1842137eb65470a0ab1ab7/href</a></iframe><h3>Surprise, Curiosity, Motivation, Quality &amp; Diversity</h3><p>While Novelty Search is useful for musical discovery, it is clearly only part of a bigger puzzle. In the past years, researchers have come up with many improvements, extensions and new ideas going beyond pure novelty search. <strong>Here is a selection of related research, worth checking out:</strong></p><ul><li><a href="http://www.autogamedesign.eu/surprise-search"><strong>Surprise Search</strong></a><strong>: </strong>“a novel algorithm that takes inspiration from the notion of surprise for unconventional discovery”.</li><li><a href="http://antoniosliapis.com/research/novsearch.php"><strong>Constrained Novelty Search</strong></a>: “A Study on Game Content Generation”</li><li><a href="http://journal.frontiersin.org/article/10.3389/frobt.2016.00040/full"><strong>Quality Diversity</strong></a><strong>:</strong> “A New Frontier for Evolutionary Computation”.</li><li><a href="https://pathak22.github.io/noreward-rl/"><strong>Curiosity-driven Exploration by Self-supervised Prediction.</strong></a></li><li><a href="http://scikit-learn.org/stable/modules/outlier_detection.html"><strong>Novelty and Outlier Detection</strong></a><strong>:</strong> In scikit-learn.</li><li><a href="http://www.araya.org/archives/1306"><strong>Recurrent generative auto-encoders and novelty search.</strong></a></li><li><a href="http://journal.frontiersin.org/article/10.3389/fnhum.2017.00145/full"><strong>The Emerging Neuroscience of Intrinsic Motivation</strong></a>: A New Frontier in Self-Determination Research.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3slKhzgPQrFc5PXr37u8Vg.png" /><figcaption><a href="http://www.autogamedesign.eu/surprise-search">Surprise Search</a> system diagram</figcaption></figure><h3>Final Thoughts</h3><p><em>Novelty Search Live</em> has transformed the way i compose music. Compared with manual processes, finding inspiration with computational assistance is joyful and often leads to unforeseeable outcomes. Combined with ideas such as surprise search and interactive training, a true <strong>musical inspiration assistant </strong>may become a reality soon. Such an assistant would allow us to compose high quality, yet unheard music much faster and inform us if what we are doing has been done a million times before. As such approaches have applications beyond just music, it is easy to imagine a world where there is an inspiration assistant for any thinkable creative processes.</p><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> | <a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PTyD4hY_Z_3DTh9jeaOuxA.jpeg" /><figcaption>Map of Evolved Musical Ideas with Novelty-Search-Live</figcaption></figure><h3>Follow up Discussion</h3><h3>Kenneth Stanley on Twitter</h3><p>@samim Great application of novelty search to music! I really enjoyed the clips. Music is a good domain for this type of exploration.</p><h3>Kenneth Stanley on Twitter</h3><p>Nice example of applying novelty search to music! https://t.co/9ZwQQ9UZqk</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2177c2a249cc" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Human Pose Detection]]></title>
            <link>https://medium.com/@samim/human-pose-detection-51268e95ddc2?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/51268e95ddc2</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Mon, 22 May 2017 23:01:55 GMT</pubDate>
            <atom:updated>2017-05-23T20:21:08.641Z</atom:updated>
            <content:encoded><![CDATA[<h4>Mining Body Language from Videos</h4><p>From Gene Kelly’s Step-Dance to Bruce Lee’s Kung-Fu — iconic movement has made history. Communicating through <a href="https://en.wikipedia.org/wiki/Body_language">Body Language</a> is an ancient art form, currently evolving in fascinating ways: <strong>Computationally detecting human body language is becoming effective and accessible. </strong>This experiment explores enabling technologies, applications &amp; implications.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/916/1*2lETIHaA48xDPnxlhTvWqg.jpeg" /><figcaption>Human Pose Estimation, using OpenPose. Footage by Boston Dynamics.</figcaption></figure><p><strong>For over 20 years</strong>, <a href="https://en.wikipedia.org/wiki/Motion_capture">Motion Capture</a> has enabled us to record actions of humans and then use that information to animate a digital character or analyse poses. While movie makers and game developers embraced such technologies — it until recently required expensive equipment which captured only few aspects of the overall performance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/694/1*o74wXStwInealT7VjK-wrQ.jpeg" /><figcaption>Human Pose Estimation. Image by OpenPose</figcaption></figure><p><strong>Today</strong>, a new generation of machine learning based systems is making it possible to <strong>detect human body language directly from images. </strong>A growing number of research papers and open-source libraries addresses key aspects: <em>Body, Hand, Face, Gaze Tracking. Identity, Gender, Age, Emotion and Muscle strain Detection. Action Classification &amp; Prediction. </em>We now can...</p><blockquote><strong>Imagine a world where every camera is a realtime body language detector — and every video can be analysed.</strong></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/928/1*LVMs2B1yuCDHCGzDkExOow.jpeg" /><figcaption><a href="https://en.wikipedia.org/wiki/The_Ministry_of_Silly_Walks">Ministry of Silly Walks</a> — John Cleese — <a href="https://en.wikipedia.org/wiki/Monty_Python">Monty Python</a> — processed with OpenPose.</figcaption></figure><h3>Experiment: Human Pose Detection in Videos</h3><p>Cinema and online video sites are a vast source of recorded human performances. Any imaginable movement has been discovered and perfected: walks, dances, gestures, drama, love and fight scenes. As the new generation of body tracking tools enables us to “mine” body language data from any video, we can now easily “steal” motion from famous movies and then use that data to drive characters in AR/VR — to name just one example.</p><p><strong>The following video is made using the OpenPose library to detect human body poses in movie scenes and video clips.</strong></p><iframe src="https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D9veagUn1KFY&amp;src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F9veagUn1KFY&amp;type=text%2Fhtml&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/759e86ce3b475ebe58df1564419dc6fe/href">https://medium.com/media/759e86ce3b475ebe58df1564419dc6fe/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/890/1*AD9waikeC5dyg_p9I4wHbg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Sn4iLt-jRLOQ17yRnY_Eqw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yfMVs6SOgFpQGDc9LeSKGg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*z-M0EE3B2VdWmIGaYHM9sA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/920/1*b3usXach-i5LHz6-j6ZLTA.jpeg" /></figure><p>The video tests OpenPose on diverse sources, including sport games, James Brown’s dance routines and Kung-Fu scenes. The Library detected a wide range of footage robustly — failing infrequently in delightfully comedic ways.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FYGPQWlHUhlAGtjJkf_BuA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zlqbgFqsE97S-_BmsykLDA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1bQaCi8OdZ3oY3z1H8iTcQ.jpeg" /><figcaption>Cloning Yoga &amp; Tai-Chi Class Videos is exceptionally easy todo.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F9jlOpSQegs52EyKMhByQQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fi0MtXvEynN2r1n8efpzpQ.jpeg" /><figcaption>Stick Figures &amp; Skeletons.</figcaption></figure><h3>OpenPose</h3><p>All experiment videos were processes with <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a> - a open-source <strong>library for real-time multi-person keypoint detection</strong> — <a href="https://arxiv.org/abs/1611.08050">authored</a> by <a href="https://www.linkedin.com/in/gineshidalgo/">Gines Hidalgo</a>, <a href="http://www.andrew.cmu.edu/user/zhecao">Zhe Cao</a>, <a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>, <a href="https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;hl=en">Shih-En Wei</a>, <a href="http://www.cs.cmu.edu/~hanbyulj/">Hanbyul Joo</a> and <a href="http://www.cs.cmu.edu/~yaser/">Yaser Sheikh</a>. It enables the detection of 18 body keypoints from images and is invariant to the number of detected people. Even though the library is in rapid development, it works reliably out of the box and is fun to use.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K-PHx6CcfjJRnuMP5W51Eg.jpeg" /></figure><p>OpenPose uses a interesting pipeline to achieve it’s robust performance. The paper “<a href="https://arxiv.org/abs/1611.08050">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a>” gives a overview of the inner workings of the System. Finally, this:</p><h4><strong>“Hands &amp; Face Estimation — Coming Soon!”</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*WRvs7kp-Q3ib_0rY-LLNew.gif" /><figcaption>Hands &amp; Face Estimation Image — by OpenPose</figcaption></figure><h3>Body Language?</h3><p>OpenPose does not model the entire spectrum of human body language. Today’s systems are still struggling with hard challenges and are limited in scope, yet development is moving very fast. Combined with components such as <a href="https://github.com/oarriaga/face_classification"><strong>Face, Gender and age classification</strong></a><strong>, </strong><a href="https://arxiv.org/abs/1611.08860v2"><strong>Gaze Estimation</strong></a><strong>, </strong><a href="https://github.com/ShuangLI59/person_search"><strong>Person Identification</strong></a><strong>, </strong><a href="https://arxiv.org/abs/1705.02445v1"><strong>motion prediction</strong></a><strong> and </strong><a href="https://github.com/search?utf8=%E2%9C%93&amp;q=emotion+detection&amp;type="><strong>emotion detection</strong></a><strong>, </strong>we are gradually arriving at a computational perspective of human body language.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/435/1*Lyg8qUARKe3k2s8D1V-Ahg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/599/1*hfOQU5UDCGEjGOuIIfCY0A.jpeg" /><figcaption><a href="https://github.com/oarriaga/face_classification">Real-time face detection and emotion/gender classification</a> | <a href="https://github.com/ShuangLI59/person_search">Person Identification and Search</a></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7_7IwVnC_9tBpUv50wPJBQ.jpeg" /></figure><h3>Applications</h3><p><strong>The list of possible applications is long and growing. Here is a summery of fields, where human body language detection might find heavy use:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*It9I-C3I3mdVIIE4-CRSaA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/698/1*ipvpFKUJyMDt1O5u70-xQA.jpeg" /><figcaption><strong>Human Computer Interaction: </strong><a href="http://resources.mpi-inf.mpg.de/coactivationclustering/bachynskyi2015.pdf">Novel Input Methods with Muscle Coactivation Clustering</a><strong> — Games and VR: </strong><a href="https://www.youtube.com/watch?v=MSTge5IDxF4">Real-time Mocap and VR in UDK</a></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BqddVOS2LyRQiU-KKMQhaA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FXr7B_GlBsY8anA6UM1xww.jpeg" /><figcaption><strong>Surveillance: </strong><a href="http://www.arabianbusiness.com/world-s-first-robocop-joins-dubai-police-force-674837.html">“Robocop” joins Dubai Police </a><strong>| </strong><a href="http://www.mirror.co.uk/news/uk-news/saatchis-intelligent-billboard-can-tell-6125060"><strong>Advertising</strong></a><strong>: </strong><a href="http://www.mirror.co.uk/news/uk-news/saatchis-intelligent-billboard-can-tell-6125060">Intelligent billboard can tell if you are smiling</a></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SAezIbd-3PT0LAV_-AIXEg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xqABNEL6hYWJw8BpfR_jFQ.jpeg" /><figcaption><strong>Military, Science, Art, Magic and Comedy</strong></figcaption></figure><h4>Pantomim, Butoh and Gnawa Performances — as seen by OpenPose:</h4><iframe src="https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DZxa9ZyEXTlo&amp;src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FZxa9ZyEXTlo&amp;type=text%2Fhtml&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/46577139778e0f3c94507d0375029364/href">https://medium.com/media/46577139778e0f3c94507d0375029364/href</a></iframe><h3>Final Thoughts</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Eug-PdynzO6m5nkRrj-0EQ.jpeg" /></figure><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Orm9_-Ocu2dZa1lGEfMVoQ.jpeg" /><figcaption>OpenPose meets a yoga master…</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/623/1*wZ8cOGzrOtJxDuh3C02wpg.jpeg" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=51268e95ddc2" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Don’t Worry Be Happy]]></title>
            <link>https://medium.com/@samim/dont-worry-be-happy-415b96c5b12e?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/415b96c5b12e</guid>
            <category><![CDATA[writing]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Thu, 02 Jun 2016 21:00:22 GMT</pubDate>
            <atom:updated>2016-06-03T08:48:38.819Z</atom:updated>
            <content:encoded><![CDATA[<h4>A Emotion Enforcing Text Editor</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*0PKXOWFolRv3lj3cvu9TVw.gif" /></figure><p>Writing is a magical medium. It allows us to think clearly and communicate quickly. Writing has instigated revolutions, ended empires and started love stories. How and What we write, is heavily influenced by our mental state. <strong>Don’t worry be happy is a emotion enforcing text editor, only usable if you are happy*.</strong></p><h3>State of Mind</h3><p>Writing is a ancient practice. Recently <a href="https://en.wikipedia.org/wiki/Writing#Tools_and_materials">discovered</a> neolithic writings have been dated to the 6th millennium BC. From Papyrus to MS Word: Tools seek to empower authors to write — and <em>be</em> <em>more creative, more of the time</em>. In the digital age, a tsunami of tools is transforming how we write. Yet, a primary factor of the creative processes is often <em>overlooked</em>:<strong> our mental state.</strong></p><h3>samim on Twitter</h3><p>Which factor is most important to be creative?</p><p>Most writers know this situation: You are sitting in-front of a blank page, and don’t know how to start. Overcoming such mental-blocks is a key ingredient of any successful writing strategy. <strong>Mental States are a important factor for creativity, </strong>that heavily influence actions and outcomes: <em>try writing a happy story or song, when you are in sad state.</em></p><p>Mental States are modulated<strong> </strong>by habits, environments, practices such as mediation and usage of tools. In this context it is interesting to ask: “How do writing tools impact my mental state?” or simply: “<strong>How does using MS Word make me feel and why do angry emails and comments exist?</strong>”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*_1y_fq5sMzgWtm0JUehBbw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*N-j1TqzDA-LiCxTVaymnug.gif" /></figure><h3>Experiment</h3><p><strong>Don’t Worry be Happy</strong> is a Emotion Enforcing Text Editor. It uses your web-cam to track your face and tries to read your emotions. You have a limited amount of time to write a story. If you are not happy<strong>*</strong>, all progress is lost. Once your done, share your story. For extra fun, check out the image feature and hardcore mode. Most importantly: <strong>Be Happy* :-)</strong></p><h3><a href="https://dontworrybehappy.today/">https://DontWorryBeHappy.today</a></h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FolzOV6M1g48%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DolzOV6M1g48&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FolzOV6M1g48%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/be841b7155e45b3b673830b8a1228217/href">https://medium.com/media/be841b7155e45b3b673830b8a1228217/href</a></iframe><p><strong>Don’t Worry Be Happy</strong> is a experiment, exploring how to build creative systems that actively influence our mental state. It playfully promotes awareness and enforces mental-states during the creative process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/474/1*PxHXkGqFYBUCnxU6AYe7CQ.gif" /></figure><p>Thanks to the availability of large datasets and machine learning techniques, <strong>emotion tracking is making progress.</strong> State-of-the-art systems track a wide range of emotions with great accuracy. While some <a href="https://github.com/isseu/mood-recognition-neural-networks">systems</a> are <a href="https://github.com/mihaelacr/pydeeplearn">open source</a>, using them interactively on a website as service is hard.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/954/1*pcX-4_w_K59S5a0Hhga0tw.gif" /></figure><p>Don’t worry be happy uses the <a href="https://github.com/auduno/clmtrackr"><strong>clmtrackr</strong></a> <strong>javascript library </strong>for fitting facial models to faces in videos — in your browser. It tracks your face and outputs coordinate positions. Finally, the current emotion is estimated across four categories: <strong>angry, sad, surprised and happy. </strong>Under the hood, it uses an implementation of <a href="http://dl.acm.org/citation.cfm?id=1938021">this paper</a>.</p><h3>Happy*</h3><p><a href="https://en.wikipedia.org/wiki/Happiness">Happiness</a> is a mental-state that many people experience regularly, but is hard to define. Don’t worry be happy uses a specific, naive definition of Happy*. At times it is inaccurate, transforming the writing process into facial gymnastics. Nevertheless, writing with Don’t Worry Be Happy for even 2min has a tangible impact on your mental-state: <strong>You start to smile and laugh.</strong></p><blockquote><strong>“What i like about laughter, is that that when people laugh, they can change their minds”</strong> — <a href="https://twitter.com/DalaiLama/">@DalaiLama</a></blockquote><p>A string of scientific <a href="http://www.ncbi.nlm.nih.gov/pubmed/24682001">studies</a> are <a href="http://www.huffingtonpost.com/2014/04/22/laughter-and-memory_n_5192086.html">revealing</a> the positive impacts of laughter: Improved memory capabilities, higher calorie burn rate, lower cortisol and stress levels, improved <a href="http://www.scientificamerican.com/article/why-laughter-may-be-the-best-pain-medicine/">pain tolerance</a>. Yet, <strong>arguing that MS Word, Gmail or facebook promote laughter and happiness, is a joke.</strong></p><h3>Emotional Media</h3><p>“Emotional Media” is a general propose design pattern, with a wide range of use-cases. An emerging generation of applications is enabling us to actively modulation our mental-states - and then use this data to optimise creative processes. <strong>The following two work-in-progress experiments, explore the possibilities of the Emotional Media pattern further:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nH_KgUhSq3ZihGno32SKjg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tQLDZaMi0AgExg7YmCB96w.jpeg" /><figcaption><strong>— 1. HappyDay </strong>— convolutional neural network based <a href="https://github.com/mihaelacr/pydeeplearn">emotion detection system</a> that runs all day. Data is browsable and correlated with data such as music and food choices. — <strong>2. HappyMirror:</strong> HappyMirror is a emotion enforcing make-up mirror. The mirror checks your emotions. If your not happy, the mirror turns black. <a href="https://dontworrybehappy.today/mirror.html"><strong>Check it out online</strong></a><strong>!</strong></figcaption></figure><h3>10 Emotional Media Examples</h3><p><a href="http://emotivemodeler.media.mit.edu/"><strong>EmotiveModeler</strong></a> — An Emotive Form Design CAD Tool</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fa05T9Pxtn1Q%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Da05T9Pxtn1Q&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fa05T9Pxtn1Q%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/2f196237739213935bfcf25bc9010495/href">https://medium.com/media/2f196237739213935bfcf25bc9010495/href</a></iframe><p><a href="https://www.eecs.tufts.edu/~byukse01/yukselCHI2016.pdf"><strong>Learn Piano with BACh — </strong>Adaptive Learning Interface that Adjusts Task Difficulty based on Brain State</a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F153867393&amp;url=https%3A%2F%2Fvimeo.com%2F153867393&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F554271910_640.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=vimeo" width="640" height="360" frameborder="0" scrolling="no"><a href="https://medium.com/media/12f86f8ef2ac96da59961caf43f5e775/href">https://medium.com/media/12f86f8ef2ac96da59961caf43f5e775/href</a></iframe><p><a href="http://www.themostdangerouswritingapp.com/"><strong>The most dangerous writing app — </strong></a>Hats off to this project for inspiration!</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FHBrZj8tzs8c%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DHBrZj8tzs8c&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FHBrZj8tzs8c%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/b9d2ddc50ea38fdfb5756625dab9f6bd/href">https://medium.com/media/b9d2ddc50ea38fdfb5756625dab9f6bd/href</a></iframe><p><a href="http://www.affectiva.com/"><strong>Affectiva</strong></a> — Emotion Detection as Service.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FmFrSFMnskI4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DmFrSFMnskI4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmFrSFMnskI4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/294bc2a36158d87f8d058e4e73db72fb/href">https://medium.com/media/294bc2a36158d87f8d058e4e73db72fb/href</a></iframe><p><a href="http://www.enteractive.space/alice/"><strong>Alice —</strong> Read emotion from person’s drawing</a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FrIo-ciPCkzI%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DrIo-ciPCkzI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FrIo-ciPCkzI%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/e91c2d895e2457c99fd19853c8710c4b/href">https://medium.com/media/e91c2d895e2457c99fd19853c8710c4b/href</a></iframe><p><a href="http://emosaic.de/"><strong>Emosaic —</strong> text emotion visualizer</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*PeNAYUPCIWntFtQaODEYGg.jpeg" /></figure><p><a href="http://sensilab.monash.edu/project/thoughtforms/"><strong>Thoughtforms — </strong>Use brain activity to build 3D forms that are 3D printed.</a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzgjAumaYxo8%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzgjAumaYxo8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FzgjAumaYxo8%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/522eb0178ba0be05dd2b269545d893c1/href">https://medium.com/media/522eb0178ba0be05dd2b269545d893c1/href</a></iframe><p><a href="https://www.youtube.com/watch?v=ohmajJTcpNk"><strong>Face2Face: Real-time Face Capture and Reenactment of RGB Videos</strong></a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FohmajJTcpNk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DohmajJTcpNk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FohmajJTcpNk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/bee5b30bf53e8f2668f05c2b0b872d82/href">https://medium.com/media/bee5b30bf53e8f2668f05c2b0b872d82/href</a></iframe><p><a href="http://www.extremetech.com/extreme/189259-real-time-emotion-detection-with-google-glass-an-awesome-creepy-taste-of-the-future-of-wearable-computers"><strong>Real-time emotion detection with Google Glass</strong></a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FSuc5B79qjfE%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DSuc5B79qjfE&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FSuc5B79qjfE%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f506d8cd2c40a3b214fa5b64cfaf3db1/href">https://medium.com/media/f506d8cd2c40a3b214fa5b64cfaf3db1/href</a></iframe><p><strong>Laughing Yoga</strong></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F3dpNS2l7jNs%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D3dpNS2l7jNs&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F3dpNS2l7jNs%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/84e03247b4783f0cf6dd0fb698caf281/href">https://medium.com/media/84e03247b4783f0cf6dd0fb698caf281/href</a></iframe><h3>Final Thoughts</h3><p>In a world where cameras and sensors are everywhere, mental-state detection and behaviour prediction are the frontier. <a href="https://medium.com/@brianroemmele/has-apple-lost-its-way-with-ai-cde76172a630#.argep6cr2">Apple</a>, <a href="https://cloud.google.com/vision/">Google</a>, <a href="https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds">Facebook</a>, <a href="http://www.wired.com/insights/2014/08/armed-ready-data-weaponized/">Military</a> and <a href="http://www.marketingweek.com/2016/01/22/how-emotion-tracking-can-help-brands-create-marketing-that-resonates/">Advertising</a> are investing. While ethical concerns around mental-state and behaviour change systems are substantial, Emotive Media holds great potential for the design of systems that transparently support our well being. A central question is: “<strong>How do we apply AI to reduce anxiety in work, supporting confidence &amp; enjoyment, allowing us to be more creative?</strong>”</p><blockquote>Get in touch here: <a href="https://twitter.com/samim">@samim</a> | <a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pwgTrTBOGfaWM4kdK94LtQ.jpeg" /><figcaption><a href="http://www.dexigner.com/news/24464">Emotional Robots</a> — <strong>angry, surprised, happy and sad</strong></figcaption></figure><h3>Follow up Discussion</h3><h3>Ken Liu on Twitter</h3><p>@samim the more I think about this the more devious it is.</p><h3>Nicholas Guttenberg on Twitter</h3><p>@samim can you do a version of this that tries to learn the latent vectors of a text generator that will make the person happy?</p><h3>ⒶfterMath on Twitter</h3><p>@samim was thinking this would be good as an AR app, that tells you which people to avoid cos the don&#39;t make you happy :)</p><h3>hardmaru on Twitter</h3><p>@samim This demo is really well done! Reminds me of Smile Training for Japanese Workers.https://youtu.be/ReC86fy1pJQ</p><h3>hal on Twitter</h3><p>@samim this is cool, but it seriously weirds me out</p><h3>Alex J. Champandard on Twitter</h3><p>@halhod @samim &quot;Be employed or else...&quot; ?</p><h3>hal on Twitter</h3><p>@alexjc @samim or even &quot;be &#39;creative&#39;/&#39;productive&#39; or else &quot;</p><h3>Kaixhin on Twitter</h3><p>@samim Trying to appear happy to prevent my work being deleted is stressful D: My resting face is just serious.</p><h3>Leon Eckert on Twitter</h3><p>@samim here is all the documentation of it I could find:https://youtu.be/_bk2DKBoA9w</p><h3>Harald K. on Twitter</h3><p>@samim great for certain boardrooms, teambuilding events, revival meetings &amp; other venues where negative affect will not be tolerated ;)</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=415b96c5b12e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Assisted Drawing]]></title>
            <link>https://medium.com/@samim/assisted-drawing-7b26c81daf2d?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/7b26c81daf2d</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[drawing]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Sun, 20 Dec 2015 12:04:32 GMT</pubDate>
            <atom:updated>2015-12-21T13:30:28.483Z</atom:updated>
            <content:encoded><![CDATA[<h4>Exploring Augmented Creativity</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YeZYkDCPBX5RKQBhFND2hQ.jpeg" /></figure><p>Seeing drawings that were made over 20&#39;000 years ago has a haunting effect. Human history has been profoundly shaped by our ability to create visual content - assisted by tools. This experiment and report explore<strong> </strong>a<strong> new generation of creative tools, that help us draw.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/1*Ia4RKNhFuon3XRbkXchWPg.jpeg" /></figure><p><strong>Drawing</strong> is one of the oldest forms of human expression. According to Wikipedia it is a “<em>visual art in which a person uses various instruments to mark paper or another medium</em>”. <strong>Drawing helps us to communicate. </strong>The history of drawing is intertwined with the evolution of tools. From Rocks to Photoshop: <strong>Tools help us to change perspective and draw <em>differently</em></strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5e15e2GPovswrQPEQ5v8lA.jpeg" /></figure><h3>Experiment: Classifying Drawings</h3><p>This experiment started with an exploration of the research project “<a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/"><em>How Do Humans Sketch Objects?</em></a>” by <strong>Mathias Eitz, James Hays and Marc Alexa.</strong> The researchers collected <strong>20,000 sketches across 250 categories </strong>(741h drawing time)<strong> </strong>and<strong> </strong>build a system that <strong>classifies drawings in realtime</strong>. For the following video, I asked a professional Illustrator to test the system:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F149471168&amp;url=https%3A%2F%2Fvimeo.com%2F149471168&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F548691327_1280.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=vimeo" width="1280" height="720" frameborder="0" scrolling="no"><a href="https://medium.com/media/3b9f01ad089825c36e40ad681da769ca/href">https://medium.com/media/3b9f01ad089825c36e40ad681da769ca/href</a></iframe><p>A primary aim of this experiment was to explore how humans draw <em>differently,</em> when a machine continuously is trying to guess what is being drawn. To intensifying the user/machine feedback-loop, a Text2Speech system was added. Illustrator:<strong> </strong><a href="http://mami.space/"><strong>KedamaMi</strong></a><strong> </strong>- Music:<strong> </strong><a href="https://archive.org/details/OscarWoods-dontSellItDontGiveItAway"><strong>Oscar Woods</strong></a>.</p><blockquote>Entertaining side-note: Youtube’s A.Is classified this video as “copyright infringing in over 150 countries”, due to the used song. It was composed in 1937 by <a href="https://en.wikipedia.org/wiki/Oscar_%22Buddy%22_Woods">Oscar Woods</a> and is now Public Domain, according to <a href="https://archive.org/details/OscarWoods-dontSellItDontGiveItAway">Archive.org</a>.</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iZL1hqajBgBqTVJXND6Mbg.jpeg" /></figure><h3>Experiment: Memorability Prediction</h3><p>This experiment started with an exploration of the research project “<a href="http://memorability.csail.mit.edu/index.html"><strong>Understanding and Predicting Image Memorability at a Large Scale</strong></a><strong>” </strong>by Researchers from MIT’s Artificial Intelligence Laboratory (CSAIL). Their system can <strong>predict how memorable or forgettable an image is.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*l0QH6EQZqO9IC9ogRemE0A.jpeg" /></figure><p>As the current system is trained on photos, performance on drawings is poor. You can try it out with your own photos, by using their <a href="http://memorability.csail.mit.edu/demo.html">online app</a>. This experiment serves as pointer to a future, where machine assistants can tweak drawings (&amp; photos) on-the-fly, to make them more <em>memorable</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dwpl7jdj0tuzmLJQByUaag.jpeg" /><figcaption>Caves in Lascaux, France</figcaption></figure><h3>Research</h3><p>Augmenting human skills with machines has a long history. Our ability to draw has continuously been expanded with new techniques. While we are still feeling the aftershocks of tools like Photoshop &amp; AutoCad, A.I assisted drawing tools are emerging that radically redefine creative processes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2l-oqDsa_KC7Mb2MH3ohSw.png" /></figure><p>Research in “Assisted Drawing” technology dates back to the early 1960ties, where <a href="https://en.wikipedia.org/wiki/Douglas_Engelbart">Douglas C. Engelbart</a> famously called for the <a href="http://www.dougengelbart.org/pubs/augment-3906.html">Augmentation of the Human Intellect</a>. In recent years, research has substantially accelerated: A wide range of experiments are taking place in machine learning, human computer interaction, psychology, design and art.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/1*gHbM5xtCPr83FcLgVfoVlw.jpeg" /></figure><h3>20 Assisted Drawing Examples</h3><p>The following is a collection of 20 Assisted Drawing Projects. It is in no way complete and aims simply at sparking a debate about vision &amp; opportunity.</p><h4>01. Freehand Drawing with Realtime Guidance</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fhzq70CN48o4%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dhzq70CN48o4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fhzq70CN48o4%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/097bdc9fe0882ca8644ae0b0a4d35d50/href">https://medium.com/media/097bdc9fe0882ca8644ae0b0a4d35d50/href</a></iframe><h4>02. Reinforcement Learning Drawing Agents</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FDjmYmWeXVRI%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DDjmYmWeXVRI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FDjmYmWeXVRI%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f3cb31e5ab3f24bb46f2c96e0409ef94/href">https://medium.com/media/f3cb31e5ab3f24bb46f2c96e0409ef94/href</a></iframe><h4>03. Assisted Creativity for Novices</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FI_LBT-WWy-s%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DI_LBT-WWy-s&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FI_LBT-WWy-s%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/21968fe552ae3093368aa8b5615d6732/href">https://medium.com/media/21968fe552ae3093368aa8b5615d6732/href</a></iframe><h4>04. Style Transfer</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F139123754%3Fh%3D1ee2252f60%26app_id%3D122963&amp;dntp=1&amp;display_name=Vimeo&amp;url=https%3A%2F%2Fvimeo.com%2F139123754&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F534800911-ae578ccf0cb94c642e13631729b199d64213ca5c90ea7e67510b2471be44b7a7-d_640&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo" width="640" height="336" frameborder="0" scrolling="no"><a href="https://medium.com/media/555a329357a1b85ea3774eeda89ad0c0/href">https://medium.com/media/555a329357a1b85ea3774eeda89ad0c0/href</a></iframe><h4>05. Exploring Latent Spaces</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FH1amMPKl798%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DH1amMPKl798&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FH1amMPKl798%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/1ad2754d5f36e1534c897bfa70af453f/href">https://medium.com/media/1ad2754d5f36e1534c897bfa70af453f/href</a></iframe><h4>06. 3D painting simulation at the bristle level</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F-foZ64bIxEw%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D-foZ64bIxEw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-foZ64bIxEw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/78b8b07a88095c011a813fc03b72f441/href">https://medium.com/media/78b8b07a88095c011a813fc03b72f441/href</a></iframe><h4>07. Autocomplete hand-drawn animations</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fw0YmWiy6sA4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dw0YmWiy6sA4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fw0YmWiy6sA4%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/a29e7f971c2c36f2ed77d0dab82a29af/href">https://medium.com/media/a29e7f971c2c36f2ed77d0dab82a29af/href</a></iframe><h4>08. Computer Assisted Animation of Line and Paint</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FOKl9mpGMCiA%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DOKl9mpGMCiA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FOKl9mpGMCiA%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/24f6afa4ce5e6099d0f64e93348dc17a/href">https://medium.com/media/24f6afa4ce5e6099d0f64e93348dc17a/href</a></iframe><h4>09. Animating drawings with face recognition</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FIINyowbMqJI%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIINyowbMqJI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FIINyowbMqJI%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/b1a393f7198d9e934ed1753a8ec881b8/href">https://medium.com/media/b1a393f7198d9e934ed1753a8ec881b8/href</a></iframe><h4>10. Drawing in Virtual Reality</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FuFWw6hGIKmc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DuFWw6hGIKmc&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FuFWw6hGIKmc%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/7aa32543e4175faed07df818c4a7bcfc/href">https://medium.com/media/7aa32543e4175faed07df818c4a7bcfc/href</a></iframe><h4>11. Drawing with Games</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fz4MEH1TRz70%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dz4MEH1TRz70&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fz4MEH1TRz70%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/333dc8b8a90a5193c88999bfd429370d/href">https://medium.com/media/333dc8b8a90a5193c88999bfd429370d/href</a></iframe><h4>12. Handwriting Beautification</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FEtfCenXsSgQ%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEtfCenXsSgQ&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FEtfCenXsSgQ%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/a07338a55b68964449ca1f48941da6b7/href">https://medium.com/media/a07338a55b68964449ca1f48941da6b7/href</a></iframe><h4>13. Robotic Handwriting Assistance</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FpEfD5HkFi58%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DpEfD5HkFi58&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FpEfD5HkFi58%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/32c563edf72f4dc913f73b70eb8f490e/href">https://medium.com/media/32c563edf72f4dc913f73b70eb8f490e/href</a></iframe><h4>14. Generating Handwriting</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F-yX1SYeDHbg%3Fstart%3D1218%26feature%3Doembed%26start%3D1218&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D-yX1SYeDHbg%26feature%3Dyoutu.be%26t%3D1218&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-yX1SYeDHbg%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ca2d6495138e4f2506f8866ce825f24/href">https://medium.com/media/8ca2d6495138e4f2506f8866ce825f24/href</a></iframe><h4>15. Live stream of Drawing Robot controlled by Users.</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FFx-FkVdG9pM%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DFx-FkVdG9pM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FFx-FkVdG9pM%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/8795c03749759a17a22324bd58b54bc1/href">https://medium.com/media/8795c03749759a17a22324bd58b54bc1/href</a></iframe><h4>16. Drawing Physical Stuff</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fuj2XNpvvYvI%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Duj2XNpvvYvI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fuj2XNpvvYvI%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/20b6fc71f3683e18f3f357ac8a3ef2bb/href">https://medium.com/media/20b6fc71f3683e18f3f357ac8a3ef2bb/href</a></iframe><h4>17. Memorability Prediction</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FASagSK-Fu4g%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DASagSK-Fu4g&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FASagSK-Fu4g%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/3bf05135c3153dbca5a1115eb0733a95/href">https://medium.com/media/3bf05135c3153dbca5a1115eb0733a95/href</a></iframe><h4>18. Adaptive Drawing</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FKUDh6sCtm8s%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DKUDh6sCtm8s&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FKUDh6sCtm8s%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/db42f555164bbfc820dc049589cca8f1/href">https://medium.com/media/db42f555164bbfc820dc049589cca8f1/href</a></iframe><h4>19. Live stream of Human Drawing in-front of 3 million people.</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fm57h9xWdGb8%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dm57h9xWdGb8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fm57h9xWdGb8%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/1ba5651df6d83ec4e01757c8639f1f9c/href">https://medium.com/media/1ba5651df6d83ec4e01757c8639f1f9c/href</a></iframe><h4>20. Drawing with Elephants</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FfoahTqz7On4%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DfoahTqz7On4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FfoahTqz7On4%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/370eaba25a4e951e067bfaf3e2d54f31/href">https://medium.com/media/370eaba25a4e951e067bfaf3e2d54f31/href</a></iframe><p><strong>Thanks to</strong> <a href="https://twitter.com/zachlieberman">@zachlieberman</a> <a href="https://twitter.com/graphific">@graphific</a> <a href="https://twitter.com/genekogan">@genekogan</a> <a href="https://twitter.com/hardmaru">@hardmaru</a> <a href="https://twitter.com/kastnerkyle">@kastnerkyle</a> <a href="https://twitter.com/alexjc">@alexjc</a> <a href="https://twitter.com/Udibr">@udibr</a> <a href="https://twitter.com/AlecRad">@alecrad</a> <a href="https://twitter.com/worrydream">@worrydream</a> <a href="https://twitter.com/KedamaMi">@KedamaMi</a> <a href="https://twitter.com/nervous_system">@nervous_system</a> <a href="https://twitter.com/inconvergent">@inconvergent</a> <a href="https://twitter.com/auriea">@auriea</a> and many more for inspiration!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dwpl7jdj0tuzmLJQByUaag.jpeg" /></figure><h3>Final Thoughts</h3><p><a href="https://en.wikipedia.org/wiki/Le_Corbusier">Le Corbusier</a> once said:<strong> “</strong><em>I prefer drawing to talking. Drawing is faster, and leaves less room for lies</em>”. The advent of powerful assisted drawing tools is enabling us to draw <em>very differently </em>and explore yet unseen perspectives. The implications of <strong><em>A.Is augmenting human creativity</em></strong><em> </em>are profound &amp; exiting!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/923/1*kuft_Z2fI3Nt8S2w9uacbw.png" /><figcaption>Image by <strong>Roelof Pieters</strong>: <a href="http://www.slideshare.net/roelofp/creative-ai-multimodality-looking-ahead">Creative AI &amp; Multimodality</a></figcaption></figure><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>samim on Twitter</h3><p>&quot;With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.&quot; - John von Neumann pic.twitter.com/HW3U6XhGk1</p><h3>Supplement</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ftR85KfYd_71-fo2NH8OJA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zbhR5le7mV3R6x73W-Elcg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GoSxmag88iDwfa-u-LUG1Q.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lMlMDTyeOPjIHP22bTtzgw.gif" /></figure><h3>Follow Up Discussion</h3><h3>Maks Del Mar on Twitter</h3><p>So in drawing, I am forced to think concretely - but in assisted drawing, an extra element: I am being disrupted, dislodged from my habits</p><h3>₥ Plummer-Fernandez on Twitter</h3><p>@samim so good! i made a similar game but to draw faces aided by face detection https://vimeo.com/69694262</p><h3>Chris Olah on Twitter</h3><p>A lovely collection of videos on computer-assisted drawing: https://medium.com/@samim/assisted-drawing-7b26c81daf2d#.1dp2lspxr ...</p><h3>Ehud Ben-Reuven on Twitter</h3><p>@samim how about using it for active learning: the last tag from every drawing session is a new label added to the training set</p><h3>vakibs on Twitter</h3><p>@samim Something cool you missed in the post. http://www.autodeskresearch.com/publications/draco ...</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7b26c81daf2d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Adversarial Machines]]></title>
            <link>https://medium.com/@samim/adversarial-machines-998d8362e996?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/998d8362e996</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[copyright]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Mon, 07 Dec 2015 15:12:17 GMT</pubDate>
            <atom:updated>2015-12-17T20:23:16.416Z</atom:updated>
            <content:encoded><![CDATA[<h4>Fooling A.Is (and turn everyone into a Manga)</h4><p><strong>Adversarial A.Is</strong> are a common sci-fi theme: Robot VS Robot. In recent years, real adversarial examples have emerged. This experiment explores how to <strong>generate images to fool A.Is <em>(and turn everyone into manga).</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-degb_Avfq5TR0_0CJhn9A.png" /></figure><h3>Convolutional Neural Networks</h3><p>At the heart of many modern computer vision systems are <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"><strong>Convolutional Neural Networks</strong></a>. On <em>some</em> vision tasks, <strong>CNNs</strong> have surpassed human performance. Industries such as Web-Services, Research, Transport, Medical, Manufacturing, Defence and Intelligence rely on them every day.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1018/1*kVm3hjja6tUAh9tiSoSC0A.png" /></figure><p><strong>Convolutional Nets </strong>are commonly used to classify images. The network is shown an image of a <em>pipe</em> and classifies it as “<em>pipe</em>”. Generalist networks are able to classify 1000+ classes of objects with amazing precision and speed.</p><h3><strong>Fooling Neural Networks</strong></h3><p>A series of published research papers has produced evidence that <strong>Convolutional Neural Networks can be fooled. </strong>Images can be manipulated, so that image recognition networks are likely to miss-classify them. These manipulations look like noise, almost invisible to humans.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vp2hctXzXImYq6BHDpbr-g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0R7EasNbUPLk9M-w4Rf5iQ.png" /><figcaption>Image by Christian Szegedy (Google) et al. NOTE: “Noise” is used for imagination. “Imperceptible changes” more fitting.</figcaption></figure><p>This problem has stirred controversy in the Machine Learning community, with some hailing it as a “deep flaw” of deep neural networks and others promoting a more cautious interpretation. Researchers are actively exploring the reasons for adversarial examples. <strong>Ian Goodfellow</strong> gives a great overview in his recent talk:<strong> </strong><a href="https://www.youtube.com/watch?v=Pq4A2mPCB0Y"><strong>‘Do Statistical Models Understand the World? (Video)’</strong></a></p><blockquote><strong>Research Papers: </strong><a href="http://arxiv.org/pdf/1412.1897v4.pdf">Deep Neural Networks are Easily Fooled</a> (<a href="http://www.evolvingai.org/fooling"><strong>code</strong></a><strong> / </strong><a href="https://www.youtube.com/watch?v=M2IebCN9Ht4"><strong>video</strong></a>): <a href="http://arxiv.org/pdf/1510.05328v3.pdf">Exploring the space of Adversarial Images</a> : <a href="http://arxiv.org/pdf/1511.05122v3.pdf">Adversarial manipulation of deep representations </a>: <a href="http://arxiv.org/pdf/1412.6572v3.pdf">Explaining and harnessing adversarial examples</a> : <a href="http://arxiv.org/pdf/1312.6199v4.pdf">Intriguing properties of neural networks</a> : <a href="https://karpathy.github.io/2015/03/30/breaking-convnets/">Breaking Linear Classifiers on ImageNet</a> : <a href="http://arxiv.org/abs/1511.04508">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</a> : <a href="http://arxiv.org/abs/1511.07528">The Limitations of Deep Learning in Adversarial Settings</a></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*-9LAwwDPElBj3qs7VAHoYg.png" /></figure><h3>Experiment: Generating Adversarial Images</h3><p>This experiment started with an exploration of the recently published paper <a href="http://arxiv.org/abs/1510.05328"><strong>Exploring the space of adversarial images</strong></a> by <a href="https://github.com/tabacof">Pedro Tabacof</a> &amp; <a href="http://eduardovalle.com/">Eduardo Valle</a> of University of Campinas in Brazil. The paper investigates adversarial examples and hints that <strong>most current</strong> <strong>CNN</strong> <strong>classifiers are vulnerable</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dN_GB6jQqOtSH-rMkKO0cw.png" /><figcaption>Adversarial Noise examples. (Image by Pedro Tabacof, Eduardo Valle)</figcaption></figure><p>Alongside the paper, they released <a href="https://github.com/tabacof/adversarial"><strong>open-source code</strong></a><strong> </strong>that enables anyone to generate adversarial images easily.</p><p><strong>The experiments aim was to find a way to demo this library. All explored scenarios were rejected, as outcomes are highly uncertain. Here is a sample of rejected ideas:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uMdKkq7qQSBZ9MMLJVp2dg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oJGKkbvc7psJaPY3tUs7WQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EVbpB3jGnjYEj49Mz-YVuw.png" /><figcaption>Selection of rejected ideas. WARNING: COMEDY INTENDED</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*9SOcfunKZmCoKvmNgSZ7JQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*Zn5bz6vqE1GnaVqsMCo80Q.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*1HB8iuvoqCdEgoLmvNkMSA.gif" /><figcaption>Rejected Cartoon / Don Hertzfeldt</figcaption></figure><h3>Experiment: Generating Adversarial Mangas</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*_n3GueGSrWcPN7_RWCt54Q.gif" /><figcaption>Generative Manga</figcaption></figure><p>Recently a new <strong>pre-trained CNN model</strong> was <a href="https://github.com/rezoo/illustration2vec">released</a>: <a href="http://illustration2vec.net/papers/illustration2vec-main.pdf"><strong>Illustration2Vec: </strong>A Semantic Vector Representation of Illustrations</a>.<strong> </strong>Masaki Saito(Tohoku University) and Yusuke Matsui (University of Tokyo) trained a model on large amounts of Manga Images.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3oMsHHTChV8BWaFd9VLj5w.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fMiI_vCbfcoX04t1RCFRxw.png" /><figcaption>Selection of training data (<em>presumably copyrighted content</em>) / Tag Prediction</figcaption></figure><p>The <a href="http://illustration2vec.net/">Illustration2Vec model</a> has the ability to <a href="http://demo.illustration2vec.net/"><strong>predict copyright</strong></a> tags. One could say, <em>It has memories of copyrighted content</em>. A fascinating way to explore convolutional networks is <strong>deepdream</strong>. This experiment <em>dreams</em> with the Illustration2Vec model and <strong>turns everyone into a Manga.</strong></p><p><strong>Questions raised: </strong>Are the generated images copyright infringing? Can the copyright detection bots of large manga sites (or disney) be fooled easily?</p><h4>A selection of generated manga</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/863/1*2LHLqsDjly4JTo1j3Dj21w.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/648/1*2kJAJRWth5hUyTWjmWZgAw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*N-a4Sz2Eo9-lKspWWS5Q9w.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*oVXpeGPWCO62uzoDKoVE8Q.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qOHKP76r8w44_PUqVF3i5Q.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/955/1*UyHk2GWZgQALMITXlF7SQQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/660/1*R4-jVgk3-tNgEHBUs9lbSA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1002/1*tJ9-P5ENc68qiFBiKp0hDg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*HnK1KondBJOJKmJ5TVwbrA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-3W6Qbt2uyQXH6MSYHsXwA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ZA6IblUAv0qxbkiqVIw93A.jpeg" /></figure><h3>Final Thoughts</h3><p>Adversarial Examples are a fascinating area of ongoing research. They highlight limitations of current systems and raise a number of interesting questions. While industries are racing to include visual intelligence systems in mission-critical infrastructure, looking at <em>edge-cases</em> and exploring solutions is a productive path. As the <a href="https://en.wikipedia.org/wiki/Surrealist">surrealist</a> Belgian painter <a href="https://en.wikipedia.org/wiki/Ren%C3%A9_Magritte"><strong>René Magritte</strong></a> said in “<a href="https://en.wikipedia.org/wiki/The_Treachery_of_Images"><em>The Treachery of Images</em></a>”: <strong>“<em>Ceci n’est pas une pipe</em>”</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*-9LAwwDPElBj3qs7VAHoYg.png" /></figure><h3>Interview</h3><p>The following is a interview with <a href="http://www.papernot.fr/"><strong>Nicolas Papernot</strong></a><strong>, </strong>a machine learning researcher who recently published <a href="http://arxiv.org/pdf/1511.04508v1.pdf">two</a> <a href="http://arxiv.org/pdf/1511.07528v1.pdf">papers</a> on adversarial examples:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*KlaRASm4ndC8pDdWmFf5XA.jpeg" /><figcaption>Nicolas Papernot</figcaption></figure><p><strong>Q: What is your core research interest in the “adversarial examples” space?</strong></p><p><em>I do research in a lab focused on security. Our end-goal is to identify vulnerabilities in deep neural networks to better understand their attack surface and defend them. Our first paper explores attacks while the second one defenses. Our algorithms were designed to reduce the number of input features that we perturb so that they can be applied on various datasets (spam, authentication, etc.).</em></p><p><strong>Q: In your recent publication you introduce a “a defensive mechanism to reduce the effectiveness of adversarial samples on DNNs”. Could you explain your approach in simple terms?</strong></p><p><em>Our paper proposes to provide additional information at training on samples. This Information takes the form of probabilities and gives us insight on the various classes. To extract probabilities we do a 1st training of the network. Then we do a 2nd training with the same architecture, including these probabilities. This gives more robustness and smoothness.</em></p><p><em>The consequence are: </em><strong><em>1.</em></strong><em> Derivatives very small (amplitude of jacobian components). </em><strong><em>2.</em></strong><em> average minimum perturbation (number of input dimensions) to leave source class and achieve target class increases by 500–800% in tests.</em></p><p><strong>Q: Do you think we will “solve” adversarial problems in the near future or are the problems deeper?</strong></p><p><em>It is a tough problem because it is closely linked to how we train our networks. </em><a href="http://arxiv.org/pdf/1511.04508v1.pdf"><em>Distillation</em></a><em> as a defense is a good first step. New defenses will probably involve additional tricks at training.</em></p><p><strong>Q: Part of your research was “sponsored by the Army Research Laboratory”. How serious do you think the implications/risks of adversarial examples are for society at the current stage?</strong></p><blockquote><em>“</em><strong><em>Note that my opinions are mine and not the opinion of the Army Research Laboratory (but I acknowledge their generous support).</em></strong><em> The implications of adversarial examples are very serious for any company in the industry. If someone has potentially an incentive to benefit from the misbehaviors (e.g. classification) then there is a risk. The consequences can be bad: cars, spam, authentication, malware, network intrusion, fraud detection come to mind.”</em></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/265/1*xfef1Z1l_dmzSzrVrBC0-Q.png" /><figcaption>Generated Fingerprint with DeepTexture</figcaption></figure><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>Follow Up Discussion</h3><h3>Zachary Lipton on Twitter</h3><p>@samim Nice bit on adversarial examples. One nit: Misleading to call perturbations noise as they&#39;re deliberately &amp; deterministically chosen</p><h3>samim on Twitter</h3><p>@zacharylipton good point! Struggled abit between accuracy and accessibility. Would you be able to give me a clarifying quote to include?</p><h3>Zachary Lipton on Twitter</h3><p>@samim I think you could reasonably point out that these changes do look like noise.</p><h3>samim on Twitter</h3><p>@zacharylipton will tweak the post now based on your advice! Interview with researcher of 2 new papers on the topic will be added nxt week</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=998d8362e996" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Industrial DeepDreaming]]></title>
            <link>https://medium.com/@samim/industrial-deepdreaming-8623ab396a48?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/8623ab396a48</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[music-video]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Wed, 18 Nov 2015 21:48:20 GMT</pubDate>
            <atom:updated>2015-11-19T17:00:20.544Z</atom:updated>
            <content:encoded><![CDATA[<h4>Music Video for Band “Years &amp; Years”</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/540/1*QbaPgip4Miq8a7-Q8ciqWg.gif" /></figure><p>Shortly after the release of “<a href="https://en.wikipedia.org/wiki/DeepDream">DeepDream</a>” in 2015, <a href="https://twitter.com/graphific/">Roelof Pieters</a> and I independently produced the first <a href="https://vimeo.com/samim23/deepdream">two</a> <a href="https://www.youtube.com/watch?v=oyxSerkkP4o">videos</a> exploring animated dreaming. The outputs were fascinating. It lead to the release of <a href="https://github.com/samim23/DeepDreamAnim">custom</a> <a href="https://github.com/graphific/DeepDreamVideo">tools</a> and started a collaboration.</p><p>The videos &amp; tools were highly popular online, resulting in an avalanche of requests from directors, agencies and artists — all looking to use deepdream. A particularly interesting request came from director <a href="https://vimeo.com/crysopia">Brian Harrison</a>, asking us to <strong>dream a music video for the Band “Years &amp; Years”</strong> (<a href="http://www.interscope.com/">Interscope</a>).</p><h4>Years &amp; Years — Desire (DeepDream Music Video)</h4><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FNYVg-V8-7q0%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNYVg-V8-7q0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNYVg-V8-7q0%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/550d71cce3354e40b15945e25c4804fc/href">https://medium.com/media/550d71cce3354e40b15945e25c4804fc/href</a></iframe><p>Note: Production was done mainly with muted audio due to sanity reasons.</p><h3>Challenges</h3><p>During the production process, numerous challenges had to be overcome. Applying deepdream to 2K footage, building <a href="https://graphific.github.io/posts/building-a-deep-learning-dream-machine/">custom</a> <a href="https://graphific.github.io/posts/running-a-deep-learning-dream-machine/">hardware</a> rigs, designing tools for rapid iteration (deepUI), exploring hyper-parameters and collaborating with directors &amp; corporates— All under time pressure.</p><p>The final video is the worlds first <em>industrial scale</em> neural-network based music video. It gives us a glimpse of a future where humans and machines collaborate as equal creative partners. We call this new paradigm <strong><em>creativeAI</em></strong><em>.</em></p><blockquote><strong>Note to trolls:</strong> There are many videos online using deepdream, yes we know (many of them are made using our open source tools). <em>Industrial scale</em> means the first video to be pushed into the global pop market by a major institution.</blockquote><h3>Impressions</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/540/1*ICh8nVeKGFneoSoeYvduvA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*lqhdUBXw5xttQlOMIyiMcQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OIDQAZZ1-XIO_owwNMZ4Mw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dYTp3w3BLwmjGiQ3wzLvkg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LXZmH4UQcoIuyDdb7eluTA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hhZuTWruEofmEJkfGwZCA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DbFemdCAJNUucuBt24VdqQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D_wlS0_VZqFhMF_n8mjgNg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PJp9Vcx4OYXU3BKS9ICOdg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mpcA_avXPjh1Poj9mi4fTg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YBxEWekAQjsYEMAVMW-oxQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*T2rKtv3WUywCoc4CIIBVMw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hiWnn9YJrUobZ9jPJ4FqQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Dk4AcNMVQ0x8NT9xn2_w7g.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7DVtTrcmMNkTFz7ef9vKVQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D-QJh51Bs1kGAgTbAZrIEg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fJ_JE7lLq-FqJR7pQ5KWQg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gYTYdJgn4isogZnUgKEMWA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ydZl2lgiap0q36ooPar38Q.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Oy38FUKmMaH89FFpDXAo1w.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*41kag-79OTsNAqsR15O2Kw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HvN8uTaFcCl-UH0F_k83Sw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rv-yS1MFn-BmwuH3c4GFvg.jpeg" /></figure><h3>Final Thoughts</h3><p>In many ways, deepdream has brought the power of creativeAI technologies into the consciousness of the general public. As research and development of such systems is moving forward with breathtaking speed, creative expression and creative industries are bound to be deeply transformed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*slMwEui7kg144-0MdZeRHQ.gif" /></figure><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>samim on Twitter</h3><p>Writing a blog post at 5am about generative neuralnet systems in-front of 5 massive Tokyo sky-scrappers is strange. #CyberPunk</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8623ab396a48" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Regressing Images]]></title>
            <link>https://medium.com/@samim/regressing-images-a80bacd0f862?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/a80bacd0f862</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Sat, 07 Nov 2015 23:04:33 GMT</pubDate>
            <atom:updated>2015-11-08T09:12:13.769Z</atom:updated>
            <content:encoded><![CDATA[<h4>Regressing 24 Hours in New Orleans</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6CciZsbzlGp-ZSLdo2GIJA.gif" /></figure><p>Regression is a widely applied technique in machine learning. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight. <a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a> is a statistical process for estimating the relationships among variables. Lets have some fun with it ;-)</p><h3>Experiment</h3><p>A while back, <a href="https://twitter.com/karpathy"><strong>Andrej Karpathy</strong></a> released <a href="https://github.com/karpathy/convnetjs"><strong>convnetjs</strong></a>, a javascript based neural network library. It contains a <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html">demo</a> which Andrej describes like this:</p><blockquote>The demo treats the pixels of an image as a learning problem: it takes the (x,y) position on a grid and learns to predict the color at that point using regression to (r,g,b). It’s a bit like compression, since the image information is encoded in the weights of the network, but <strong>almost</strong> certainly not of practical kind :)</blockquote><p><strong>This experiment test a regression based approach for video stylisation.</strong> The following video was generated using <a href="https://github.com/Newmu/stylize"><strong>Stylize</strong></a><strong> </strong>by<strong> </strong><a href="https://twitter.com/AlecRad"><strong>Alec Radford</strong></a><strong>.</strong> Alec extends Andrej’s implementation and uses a <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forest</a> Regressor. This experiment extends it to work on video. The source video is by <a href="https://www.youtube.com/channel/UCTqEu1wZDBju2tHkNP1dwzQ"><strong>JacksGap</strong></a><strong>.</strong></p><h4>Regressing 24h in New Orleans</h4><p><em>If the youtube video is blocked, </em><strong><em>try </em></strong><a href="https://vimeo.com/145008957"><strong><em>vimeo</em></strong></a></p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F5d_g0UPoV6Y%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D5d_g0UPoV6Y&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F5d_g0UPoV6Y%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/3c735fb62b2253d4de13971b52caec83/href">https://medium.com/media/3c735fb62b2253d4de13971b52caec83/href</a></iframe><h4>Selection of results</h4><p>Heavy loading times, sorry!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JDJSfJQbfCz9tNfm3qCtUQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*caWIUL8xWwJnZ1wh5guSMQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zc18L3RbjYP6edWnFJMFvw.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LJvd_9DvfRID-DQRYP6WHQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FPjwbRWXthDr6uT4fi0ybw.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uKhc-9peBdUkKTZ6p4x7lg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OefTwp6Ceo8TX44uenvgRg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Pwaf6m7vap6C5cmkqnNeQA.gif" /></figure><h4>Selection of results: convnetJS</h4><p>With bottleneck layer / dropout for extra fun.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/391/1*zQF7KCtdzvxOLn_a8uJMeQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/393/1*Pbz0s6prdWQxE3vzvc1fHA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/394/1*kp020Zwp5Gu3vwu-M4brTg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/360/1*G1zuD_-XL3DTUHhcoY8agA.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/399/1*SnI_GeB3BWC82n0EWOcAaw.gif" /></figure><h3>Final thoughts</h3><p>Using machine learning techniques in unusual ways is fun, generating new image styles addictive. Realtime processing might be possible using GPUs (all CPU so far). A VR version could be fascinating ;-) All results in this experiment are reproducible with the linked open-source software. If setups are to cumbersome for you, try <a href="https://twitter.com/quasimondo">Mario Klingemann</a>’s related “<a href="https://twitter.com/Lowpolybot"><em>Lowpoly Bot</em></a><em>”.</em></p><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>Source Video</h3><p>Special thanks to JackGap for the awesome source video! :-)</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FpHhW588NiBU%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DpHhW588NiBU&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FpHhW588NiBU%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/44105dbcd827bb3a7d14393ba5bd553d/href">https://medium.com/media/44105dbcd827bb3a7d14393ba5bd553d/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/393/1*kf1P_Gh-bJ94jVByuhM97A.gif" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a80bacd0f862" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Generating Stories about Images]]></title>
            <link>https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed?source=rss-f3c8148878e1------2</link>
            <guid isPermaLink="false">https://medium.com/p/d163ba41e4ed</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[samim]]></dc:creator>
            <pubDate>Thu, 05 Nov 2015 21:52:08 GMT</pubDate>
            <atom:updated>2015-12-12T18:38:02.204Z</atom:updated>
            <content:encoded><![CDATA[<h4>Recurrent neural network for generating stories about images</h4><p>Stories are a fundamental human tool that we use to communicate thought. Creating a stories about a image is a difficult task that many struggle with. New machine-learning experiments are enabling us to generate stories based on the content of images.<strong> </strong>This experiment explores <strong>how to generate <em>little romantic stories about images</em> (incl. guest star <em>Taylor Swift).</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rUzuHpOqB8q1cRjsl65rmw.jpeg" /></figure><h3>neural-storyteller</h3><p><strong>neural-storyteller</strong> is a recently published experiment by <a href="https://github.com/ryankiros">Ryan Kiros</a> (University of Toronto). It combines <em>recurrent neural networks (RNN), </em><a href="http://arxiv.org/pdf/1506.06726v1.pdf">skip-thoughts vectors</a> and other techniques to generate <em>little story about images</em>. Neural-storyteller’s<strong> </strong>outputs are <em>creative and </em>often <em>comedic. </em>It is <a href="https://github.com/ryankiros/neural-storyteller">open-source</a>.</p><h3>Experiment</h3><p>This experiment started by running 5000 randomly selected web-images through neural-storyteller and experimenting with hyper-parameters. <strong>neural-storyteller </strong>comes with 2 pre-trained models: One trained on 14 million passages of <a href="http://www.cs.toronto.edu/~mbweb/">romance novels</a>, the other trained on<em> Taylor Swift Lyrics</em>. Inputs and outputs were manually filtered and recombined into two videos.</p><h4>Generating Romance</h4><p>Using Romantic Novel Model. Voices generated with a Text-to-Speech.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FpKDzKzDnwtQ%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DpKDzKzDnwtQ&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FpKDzKzDnwtQ%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/802b75b6a55dc7a09004bb2d37c1cdf3/href">https://medium.com/media/802b75b6a55dc7a09004bb2d37c1cdf3/href</a></iframe><h4>Generating Taylor Swift</h4><p>Using Taylor Swift Model. Combined with a well known Swift instrumental.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcersRTtjFcU%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcersRTtjFcU&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FcersRTtjFcU%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0c18cbef8e777d5b10ee0eaa43f4a041/href">https://medium.com/media/0c18cbef8e777d5b10ee0eaa43f4a041/href</a></iframe><h3>How does it work?</h3><ol><li>Train a recurrent neural network (<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a>) decoder on romance novels.</li><li>Each passage from a novel is mapped to a <a href="http://arxiv.org/pdf/1506.06726v1.pdf">skip-thought vector</a>.</li><li>Conditions RNN on skip-thought vector &amp; generate the encoded passage.</li><li>Train a visual-semantic embedding between <a href="http://mscoco.org/">COCO</a> images and captions. <em>Captions and images are mapped into a common vector space.</em></li><li>After training, embed new images and retrieve captions.</li></ol><h3>A selection of generated stories</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aVVktwEOz_BjX8eFu3ewHQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*brmI0US0cs00qwg4y0lMvg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CCF_gTs16d_FmosXZAb_UA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BCC0mvK6eMFWbkUC-fZYWw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZLXqCT1_K7R5L7wZT6VFEQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KqmeCjamtIz227I-aKLqQg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eeEGqvq20wRxSC1D2QKuEQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fVJkGqSwdqveRSWmL9ff4g.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WQzbzkvLb7GKjkM6T1wOJw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nCLRnwgQef3Fv98ctv3mfQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c8eKdZpvqCFcRjq_0dHozA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*B2gtOWUWUl4_9cvk4btg8g.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*63kjGm-IX1mBUnHJR3tIkw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iXcpxBIth0IgXGhU-pmWhA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RvhRVuSw85ymK1K2Po9aag.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H--j4jWVw-iR0YK5Ed8MPw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zQlAprPoLmbZlxNeTVARgw.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-gBgp67CDBWJZeeNFR-BDg.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8vG3p8ra8qMIjkewD3pPOQ.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DmKmcYzGjGhFYf0m77ROqQ.jpeg" /></figure><h3>Final thoughts</h3><p>neural-storyteller gives us a fascinating glimpse into the future of storytelling. Even though these technologies are not fully mature yet, the art of storytelling is bound to change. In the near future, <strong>authors will be</strong> <strong>training</strong> custom models, <strong>combining</strong> styles across genres and <strong>generating</strong> text with images &amp; sounds. Exploring this exiting new medium is rewarding!</p><blockquote>Get in touch here: <a href="https://twitter.com/samim">twitter.com/samim</a> |<a href="http://samim.io/">http://samim.io</a></blockquote><blockquote><a href="https://tinyletter.com/samim">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>Follow up discussions</h3><h3>Hendrik on Twitter</h3><p>@samim Another great experiment! Though I think the future of storytelling will be interactive machine learning &amp; aided associative thinking</p><h3>yoav goldberg on Twitter</h3><p>@samim public-service reminder: it is the readers who put meaning into the images and the associated generated verse. not the algorithms.</p><h3>samim on Twitter</h3><p>@yoavgo exactly! thats the fun bit :-)</p><h3>Mario Klingemann on Twitter</h3><p>@samim I wonder when authors are starting to sue for &quot;stealing&quot; samples? How many consecutive words constitute plagiarism?</p><h3>samim on Twitter</h3><p>https://t.co/X5nFXBoJIs</p><h3>samim on Twitter</h3><p>@azeem requested running this photo through neural-storyteller, there u go ;-) cc @mpshanahan @AndrewYNg @ylecun pic.twitter.com/cKrn98jezP</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d163ba41e4ed" width="1" height="1" alt="">]]></content:encoded>
        </item>
    </channel>
</rss>