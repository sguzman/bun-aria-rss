<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/topic/robotics.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Sat, 05 Nov 2022 16:01:19 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Video Friday: Uncrewed</title><link>https://spectrum.ieee.org/video-friday-uncrewed</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-hawk-helicopter-with-the-darpa-logo-on-it-flies-over-the-desert-with-no-crew-on-board.jpg?id=32042319&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><p>Sikorsky, a Lockheed Martin company, and the Defense Advanced Research Projects Agency (DARPA) have successfully demonstrated to the U.S. Army for the first time how an uninhabited Black Hawk helicopter flying autonomously can safely and reliably perform internal and external cargo-resupply missions and a rescue operation.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d9abae4d109ee3fce8ed2b965fe420f3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dYcq_pzLsjA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.lockheedmartin.com/en-us/news/features/2022/sikorsky-and-darpa-demonstrate-future-battlefield-logistics-missions-with-autonomous-utility-helicopter.html">Lockheed Martin</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Taking inspiration from nature, SEAS researchers designed a new type of soft, robotic gripper that uses a collection of thin tentacles to entangle and ensnare objects, similar to how jellyfish collect stunned prey. Alone, individual tentacles, or filaments, are weak. But together, the collection of filaments can grasp and securely hold heavy and oddly shaped objects. The gripper relies on simple inflation to wrap around objects and doesn’t require sensing, planning, or feedback control.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="12c40ac113bfff56ebc0c521ccb697ad" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SayuM8E_WaQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.seas.harvard.edu/news/2022/10/tentacle-robot-can-gently-grasp-fragile-objects">Harvard</a> ]</p><div class="horizontal-rule"></div><p>Agility Robotics’ Digit does not have bird legs. Birds have robot legs.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="20ad7b3f4a398a0743393a474cf20afb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9B2CVZe0jFE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>At TRI, we are developing robotic capabilities with the goal of improving the quality of everyday life for all. To reach this goal, we define “challenge tasks” that are exciting to work on, that drive our development towards general purpose robot capabilities, and that allow for rigorous quantitative testing.</em><br/><em>Autonomous order fulfillment in grocery stores is a particularly good way to drive our development of mobile manipulation capabilities because it encompasses a host of difficult challenges for robots, including perceiving and manipulating a large variety of objects, navigating an ever-changing environment, and reacting to unexpected circumstances.</em></blockquote><p class="shortcode-media shortcode-media-vimeo">
<iframe class="rm-shortcode" data-rm-shortcode-id="615ff17c43a4e64d0acf1de39f3aaaee" frameborder="0" height="480" scrolling="no" src="https://player.vimeo.com/video/764216312" width="100%"></iframe>
</p><p>[ <a href="https://medium.com/toyotaresearch/why-we-make-our-robots-shop-for-groceries-73ea8108caae">TRI</a> ]</p><p>Thanks, Lukas!</p><div class="horizontal-rule"></div><blockquote><em>On Halloween don’t come empty-handed to the MAB robotics’ basement. This is a spooky season, you’d better have a treat for the honey badger legged robot.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1270de418fab51b0644eb0ecc72fa94a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UmIsr2h27BM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.mabrobotics.pl/">MAB Robotics</a> ]</p><p>Thanks, Jakub!</p><div class="horizontal-rule"></div><p>The most important skill in humanoid robotics is knowing how to shove your robot in just the right way.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="96ffc6189a1be97e81d55457612edf02" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/_htcbZz_jd0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://robots.ihmc.us/">IHMC</a> ]</p><div class="horizontal-rule"></div><p>If this is a humanlike workspace, I need to take Pilates or something.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f32da978c858122b2d9fe191f70650e3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/HN8osJJZH1k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://apptronik.com/our-work/">Apptronik</a> ]</p><div class="horizontal-rule"></div><p>A Spooky Lab Tour of KIMLAB!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cabd836f241c7138b5f38e04dd4238a5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5O2HC9aiIYU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://publish.illinois.edu/kimlab2020/">KIMLAB</a> ]</p><div class="horizontal-rule"></div><p>I know I say this every time, but I still cannot believe that this is a commercial system.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="26addb6aa355883564510bf5c179d185" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3oYw035gYyk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tevel-tech.com/">Tevel</a> ]</p><div class="horizontal-rule"></div><p>Amazon has a prototype autonomous mobile robot, or AMR, for transporting oversize packages through warehouses. Its name is Bluebell.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9ee65dca91695f456ae759254d5c48d2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Kget9r-0kGs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.amazon.science/research-areas/robotics">Amazon</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Using GPT3, Ameca answers user-submitted questions for you in the first installment of Ask Ameca!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8597176660fbe24a35986fed01b37092" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fQ2JpkFBAOs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.engineeredarts.co.uk/robot/ameca/">Engineered Arts</a> ]</p><div class="horizontal-rule"></div><blockquote><em>If insects can discern up from down while flying without fancy accelerometers, could we develop drones to do the same? In a new article published in </em>Nature<em>, scientists from Delft University of Technology (Netherlands) and Aix-Marseille University (France) describe how insects detect gravity. And how we could perhaps copy from nature.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="5d78a286f8cd3fcff5ed03433c1c631f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ugY0RTMjH1s?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tudelft.nl/en/2022/tu-delft/how-flying-insects-and-drones-can-discern-up-from-down">TU Delft</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We show a new method to handle fabric using a robot. Our approach relies on a finger-tip-size electroadhesive skin to lift fabric up. A pinch-type grasp is then used to securely hold the separated sheet of fabric, enabling easy manipulation thereafter.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6426d340885a8f385b3f292c08309ab6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wBoLqbNxwtc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ieeexplore.ieee.org/document/9636095">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We present FLEX-SDK: an open-source software development kit that allows creating a social robot from two simple tablet screens. FLEX-SDK involves tools for designing the robot face and its facial expressions, creating screens for input/output interactions, controlling the robot through a Wizard-of-Oz interface, and scripting autonomous interactions through a simple text-based programming interface.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b4b3f502734dbfeb4090567916d07752" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/H19xNKsx6uo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://dl.acm.org/doi/10.1145/3526113.3545707">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote><em>D’Manus is a 10 DoF, low-cost, reliable prehensile hand.  It is fully 3D printable, and features integrated large-area ReSkin sensing.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4564bde91a7c22a414e41e11a5f345d0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/oiNdePCi_5k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://sites.google.com/view/roboticsbenchmarks/platforms/dmanus">D'Manus</a> ]</p><div class="horizontal-rule"></div><p>10,000 cheese sticks per hour.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="951e43b0722a104f71e0c8d346a9e09b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/KZOr5k2zSx8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kuka.com/food-processing-automation">Kuka</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We present UltraBat, an interactive 3D side-scrolling game inspired by Flappy Bird, in which the game character, a bat, is physically levitated in mid-air using ultrasound. Players aim to navigate the bat through a stalagmite tunnel that scrolls to one side as the bat travels, which is implemented using a pin-array display to create a shape-changing passage.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9346d97f664d0e4149cbabbed5b4b5d5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/jMYAQzzQ_PI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://dl.acm.org/doi/10.1145/3526114.3561344">UltraBat</a> ]</p><div class="horizontal-rule"></div><blockquote><em>The next generation of robots will rely on machine learning in one way or another. However, when machine learning algorithms (or their results) are deployed on robots in the real world, studying their safety is important. In this talk, I will summarize the findings of our recent review paper “Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning”.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cd41edfc17b2195a2457607d6485d088" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/g6eHhvHMSy8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dynsyslab.org/vision-news/">UofT</a> ]</p><div class="horizontal-rule"></div><blockquote><em>On October 20, 2022, Kimberly Hambuchen of NASA talked to Robotics students as a speaker in the Undergraduate Robotics Pathways & Careers Speaker Series, which aims to answer the question: “What can I do with a robotics degree?”</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="149910571bbe34a5d07981b8881c4e27" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6mYRJf2Wbcs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotics.umich.edu/academics/undergraduate/robotics-pathways-speaker-series/">Michigan Robotics</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 04 Nov 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-uncrewed</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-hawk-helicopter-with-the-darpa-logo-on-it-flies-over-the-desert-with-no-crew-on-board.jpg?id=32042319&amp;width=980"></media:content></item><item><title>Can Autonomous Weapons Be Compatible With International Humanitarian Law?</title><link>https://spectrum.ieee.org/autonomous-weapons-law</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-green-connected-lines-the-silhouette-of-heads-and-a-globe.jpg?id=31988636&width=1200&height=800&coordinates=0%2C150%2C0%2C150"/><br/><br/><p><em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em></p><p>
	The real world is anything but binary. It is fuzzy and indistinct, with lots of options and potential outcomes, full of complexity and nuance. Our societies create laws and cultural norms to provide and maintain some semblance of order, but such structures are often open to interpretation, and they shift and evolve over time.
</p><p>
	This fuzziness can be challenging for any autonomous system navigating the uncertainty of a human world—such as Alexa reacting to the wrong conversations, or self-driving cars being stymied by 
	<a href="https://www.archyde.com/tesla-hits-a-white-truck-again-with-two-passengers-rushing-to-icu-tesla-tesla-electric-car/" rel="noopener noreferrer" target="_blank">white trucks</a> and<a href="https://www.theverge.com/2021/5/14/22436584/waymo-driverless-stuck-traffic-roadside-assistance-video" rel="noopener noreferrer" target="_blank"> orange traffic cones</a>. But not having clarity on "right or wrong" is especially problematic when considering autonomous weapons systems (AWS).
</p><p>
	International Humanitarian Law (IHL) is the body of laws that govern international military conflicts, and they provide rules about how weapons should be used. The fundamentals of IHL were developed before the widespread use of personal computers, satellites, the Internet, and social media, and before private data became a commodity that could be accessed remotely and often without a person’s knowledge or consent. 
	<a href="https://www.stopkillerrobots.org/a-global-push/member-organisations/" rel="noopener noreferrer" target="_blank">Many groups</a> are concerned that the existing laws don’t cover the myriad issues that recent and emerging technologies have created, and the International Committee of the Red Cross, the <a href="https://www.icrc.org/en/mandate-and-mission" rel="noopener noreferrer" target="_blank">watchdog of IHL</a>, has <a href="https://www.icrc.org/en/document/icrc-position-autonomous-weapon-systems" rel="noopener noreferrer" target="_blank">recommended new, legally binding rules </a>to cover AWS.
</p><p>
<a href="https://documents-dds-ny.un.org/doc/UNDOC/GEN/G19/343/64/PDF/G1934364.pdf?OpenElement" rel="noopener noreferrer" target="_blank">Ethical principles</a> have been developed to help address gaps between changing cultural norms and technologies and established laws, but such principles also tend to be vague and difficult to translate into legal code. For example, even if everyone agrees on an ethical principle like minimizing bias in an autonomous system, how would that be programmed? Who would determine whether an algorithmic bias has been sufficiently “minimized” for the system to be deployed?
</p><p>
	All countries involved in the autonomous weapons systems (AWS) debate at the United Nations have stated that AWS must follow international law. However, they don’t agree on what these laws and ethics mean in practice, and there’s additional disagreement over whether some AWS capabilities must be preemptively banned in order to ensure that IHL is honored.
</p><h3>IHL, Emerging Technology, and AWS</h3><p>
	Much of the disagreement at the United Nations stems from the uncertainty surrounding the technology and how the technology will evolve in the future. Though existing weapons systems have some autonomous capabilities, and though there have been reports of 
	<a href="https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d" rel="noopener noreferrer" target="_blank">AWS being used in Libya</a> and <a href="https://www.csis.org/analysis/russia-probably-has-not-used-ai-enabled-weapons-ukraine-could-change" rel="noopener noreferrer" target="_blank">questions about AWS being used</a> in Ukraine, the extent to which AI and autonomy will change warfare remains unknown. Even when IHL mandates already exist, it’s unclear that AWS will be able to follow them: For example, can a machine be trained to reliably recognize when a combatant is injured or surrendering? Is it possible for a machine to learn the difference between a civilian and a combatant dressed as a civilian?
</p><p>
	Cyberthreats pose new risks to national security, and the ability of companies and governments to collect personal data is already a controversial legal and ethical issue. These risks are only exacerbated when paired with AWS, which could be biased, hacked, trained on bad data, or otherwise compromised as a result of weak regulations surrounding emerging technologies.
</p><p>
	Moreover, for AI systems to work, they typically need to be trained on huge data sets. But military conflict and battlefields can be chaotic and unpredictable, and large, reliable data sets may not exist. AWS may also be subject to greater adversarial manipulation, which, essentially, involves tricking the system into misunderstanding the situation–something that can be as easy to do as 
	<a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google" rel="noopener noreferrer" target="_blank">placing a sticker on or near an object</a>. Is it possible for AWS algorithms to receive sufficient training and supervision to ensure they won’t violate international laws, and who makes that decision?
</p><p>
	AWS are complex, with various people and organizations involved at different stages of development, and communication between designers and users of the systems may not exist. Additionally, the algorithms and AI software used in AWS may not have originally been intended for military use, or they may have been intended for the military but not for weapons specifically. To ensure the safety and reliability of AWS, new standards for testing, evaluation, verification, and validation are needed. And if an automated weapons system acts inappropriately or unexpectedly and causes unintended harm, will it be clear who is at fault?
</p><h3>Nonmilitary Use of AWS</h3><p>
	While certain international laws cover human rights issues during a war, separate laws cover human rights issues in all other circumstances. Simply prohibiting a weapons system from being used during wartime does not guarantee that the system can’t be used outside of military combat. For example, 
	<a href="https://theconversation.com/tear-gas-and-pepper-spray-are-chemical-weapons-so-why-can-police-use-them-140364" rel="noopener noreferrer" target="_blank">tear gas</a> has been classified as a chemical weapon and banned in warfare since 1925, but it remains legal for law enforcement to use for riot control.
</p><p>
	If new international laws are developed to regulate the wartime use of AI and autonomy in weapons systems, human rights violations committed outside of the scope of a military action could—and likely would—still occur. The latter could include actions by private security companies, police, border control agencies, and nonstate armed groups.
</p><p>
	Ultimately, in order to ensure that laws, policy, and ethics are well adapted to the new technologies of AWS—and that AWS are designed to better abide by international laws and norms—policymakers need to have a stronger understanding of the technical capabilities and limitations of the weapons, and how the weapons might be used.
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the 
	<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
</p>
<p>
	We’ve put together a <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in the <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
</p></div>]]></description><pubDate>Thu, 03 Nov 2022 13:24:56 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-law</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-green-connected-lines-the-silhouette-of-heads-and-a-globe.jpg?id=31988636&amp;width=980"></media:content></item><item><title>What Does “Human Control” Over Autonomous Systems Mean?</title><link>https://spectrum.ieee.org/autonomous-weapons-control</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conceptual-illustration-of-two-humanoid-heads-facing-away-from-each-other-with-a-pattern-of-nested-squares.jpg?id=32029419&width=1200&height=800&coordinates=0%2C227%2C0%2C228"/><br/><br/><p><em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em></p><p>
<a href="https://spectrum.ieee.org/how-the-boeing-737-max-disaster-looks-to-a-software-developer" target="_self">Two Boeing 737 Max planes crashed</a> in 2018 and 2019 due to sensor failures that led to autopilot malfunctions that two human pilots were unable to overcome. Also in 2018, an Uber autonomous vehicle struck and killed a pedestrian in Arizona, even though a person in the car was supposed to be overseeing the system. These examples highlight many of the issues that arise when considering what “human control” over an autonomous system really means.
</p><p>
	The development of these autonomous technologies occurred within enormously complex bureaucratic frameworks. A huge number of people were involved—in engineering a number of autonomous capabilities to function within a single system, in determining how the systems would respond to an unknown or emergency situation, and in training people to oversee the systems. A failure in any of these steps could, and did, lead to a catastrophic failure in which the people overseeing the system weren’t able to prevent it from causing unintended harm.
</p><hr/><p>
	These examples underscore the basic human psychology that developers need to understand in order to design and test autonomous systems. Humans are <a href="https://www.bbc.com/future/article/20211019-why-we-place-too-much-trust-in-machines" rel="noopener noreferrer" target="_blank">prone to over-trusting machines</a> and become increasingly complacent the more they use a system and nothing bad happens. Humans are notoriously bad at maintaining the level of focus necessary to catch an error in such situations, typically <a href="https://slate.com/technology/2018/03/safety-drivers-attention-spans-might-slow-self-driving-car-progress.html" rel="noopener noreferrer" target="_blank">losing focus after about 20 minutes</a>. And the human response to an emergency situation can be <a href="https://www.tudelft.nl/en/2019/lr/unpredictable-training-better-prepares-pilots-for-emergencies/" rel="noopener noreferrer" target="_blank">unpredictable</a>.
</p><p>
	Ultimately, “human control” is hard to define and has become a controversial issue in discussions about autonomous weapons systems, with many similar phrases used in international debates, including “meaningful human control,” “human responsibility,” and “appropriate human judgment.” But regardless of the phrase that’s used, the problem remains the same: Simply assigning a human the task of overseeing an AWS may not prevent the system from doing something it shouldn’t, and it’s not clear who would be at fault.
</p><h3>Responsibility and Accountability</h3><p>
	Autonomous weapons systems can process data at speeds that far exceed a human’s cognitive capabilities, which means a human involved will need to know when to trust the data and when to question it.
</p><p>
	In the examples above, people were directly overseeing a single commercial system. In the very near future, a single soldier might be expected to monitor an entire swarm of hundreds of weaponized drones, with testing already taking place for soldiers. Each drone may be detecting and processing data in real time. If a human can’t keep up with a single autonomous system, they certainly wouldn’t be able to keep up with the data coming in from a swarm. Additional autonomous systems may thus be added to filter and package the data, introducing even more potential points of failure. Among other issues, this raises legal concerns, given that responsibility and accountability could quickly become unclear if the system behaves unexpectedly only after it’s been deployed.</p><h3>Human-Machine Teams</h3><p>
	Artificial intelligence often relies on machine learning, which can turn AI-based systems into black boxes, with the AI taking unexpected actions and leaving its designers and users uncertain as to why it did what it did. It remains unclear how humans working with AWS will respond to their machine partners or what type of training will be necessary to ensure the human understands the capabilities and limitations of the system. Human-machine teaming also presents challenges both in terms of training people to use the system and of developing a better understanding of the trust dynamic between humans and AWS. While the human-robot handoff may be a technical challenge in many fields, it quickly becomes a question of international humanitarian law if the handoff doesn’t go smoothly for a weapons system.
</p><p>
	Ensuring responsibility and accountability for AWS is a general point of agreement among those involved in the international debate. But without sufficient understanding of human psychology or how human-machine teams should work, is it reasonable to expect the human to be responsible and accountable for any unintended consequences of the system’s deployment?
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank"> “<u>Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</u></a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
</p><p>
We’ve put together a<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank"> series of questions</a> on <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">the Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other technical realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal.<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank"> Public comments will be open through 7 December 2022</a>.
</p></div>]]></description><pubDate>Thu, 03 Nov 2022 13:20:08 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-control</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conceptual-illustration-of-two-humanoid-heads-facing-away-from-each-other-with-a-pattern-of-nested-squares.jpg?id=32029419&amp;width=980"></media:content></item><item><title>How Can We Make Sure Autonomous Weapons Are Used Responsibly?</title><link>https://spectrum.ieee.org/autonomous-weapons-trust</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-a-light-shining-through-overlapping-rectangles-of-different-sizes.jpg?id=31988677&width=1200&height=800&coordinates=0%2C145%2C0%2C146"/><br/><br/><p>
<em><em>This article is part of our <a href="https://spectrum.ieee.org/collections/autonomous-weapons-challenges/" target="_self">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">these questions</a>.</em></em>
</p><p>
	International discussions about autonomous weapons systems (AWS) often focus on a fundamental question: Is it legal for a machine to make the decision to take a human life? But woven into this question is another fundamental issue: Can an automated weapons system be trusted to do what it’s expected to do?
</p><p>
	If the technical challenges of developing and using AWS can’t be addressed, then the answer to both questions is likely “no.”
</p><hr/><h3>AI Challenges Are Magnified When Applied to Weapons </h3><p>
	Many of the known issues with AI and machine learning become even more problematic when associated with weapons. For example, AI systems could help process data from images far faster than human analysts can, and the majority of the results would be accurate. But the algorithms used for this functionality are known to introduce or exacerbate issues of bias and discrimination, targeting certain demographics more than others. Given that, is it reasonable to use image-recognition software to help humans identify potential targets?
</p><p>
	But concerns about the technical abilities of AWS extend beyond object recognition and algorithmic bias. Autonomy in weapons systems requires a slew of technologies, including sensors, communications, and onboard computing power, each of which poses its own challenges for developers. These components are often designed and programmed by different organizations, and it can be hard to predict how the components will function together within the system, as well as how they’ll react to a variety of real-world situations and adversaries.
</p><h3>Testing for Assurance and Risk</h3><p>
	It’s also not at all clear how militaries can test these systems to ensure the AWS will do what’s expected and comply with International Humanitarian Law. And yet militaries typically want weapons to be tested and proven to act consistently, legally, and without harming their own soldiers before the systems are deployed. If commanders don’t trust a weapons system, they likely won’t use it. But standardized testing is especially complicated for an AI program that can learn from its interactions in the field—in fact, such standardized testing for AWS simply doesn’t exist.
</p><p>
	We know how software updates can alter how a system behaves and may introduce bugs that cause a system to behave erratically. But an automated weapons system powered by AI may also update its behavior based on real-world experience, and changes to the AWS behavior could be much harder for users to track. New information that the system accesses in the field could even trigger it to start to shift away from its original goals.
</p><p>
	Similarly, cyberattacks and adversarial attacks pose a known threat, which developers try to guard against. But if an attack is successful, what would testing look like to identify that the system has been hacked, and how would a user know to implement such tests?
</p><h3>Physical Challenges of Autonomous Weapons</h3><p>
	Though recent advancements in artificial intelligence have led to greater concern about the use of AWS, the technical challenges of autonomy in weapons systems extends beyond AI. Physical challenges already exist for conventional weapons and for nonweaponized autonomous systems, but these same problems are further exacerbated and complicated in AWS.
</p><p>
	For example, many autonomous systems are getting smaller, even as their computational needs grow, including navigation, data acquisition and analysis, and decision making—and potentially all while out of communication with commanders. Can the automated weapons system maintain the necessary and legal functionality throughout the mission, even if communication is lost? How is data protected if the system falls into enemy hands?
</p><p>
	Issues similar to these may also arise with other autonomous systems, but the consequences of failure are magnified with AWS, and extra features will likely be necessary to ensure that, for example, a weaponized autonomous vehicle in the battlefield doesn’t violate International Humanitarian Law or mistake a friendly vehicle for an enemy target. Because these problems are so new, weapons developers and lawmakers will need to work with and learn from experts in the robotics space to be able to solve the technical challenges and create useful policy.
</p><p>
	There are many technical advances that will contribute to various types of weapons systems. Some will prove far more difficult to develop than expected, while others will likely be developed faster. That means AWS development won’t be a leap from conventional weapons systems to full autonomy, but will instead make incremental steps as new autonomous capabilities are developed. This could lead to a slippery slope where it’s unclear if a line has been crossed from acceptable use of technology to unacceptable. Perhaps the solution is to look at specific robotic and autonomous technologies as they’re developed and ask ourselves whether society would want a weapons system with this capability, or if action should be taken to prevent that from happening.
</p><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	We want your feedback! To help bring clarity to these AWS discussions, the 
		<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance. Last year, the expert group published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of <em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
	</p>
We’ve put together a 
	<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in <a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">the Challenges document</a> that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other technical realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
</div>]]></description><pubDate>Thu, 03 Nov 2022 13:16:52 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-trust</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-a-light-shining-through-overlapping-rectangles-of-different-sizes.jpg?id=31988677&amp;width=980"></media:content></item><item><title>How Can We Talk About Autonomous Weapons?</title><link>https://spectrum.ieee.org/autonomous-weapons-challenges</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-illustration-of-overlapping-circles-and-squares.jpg?id=31988673&width=1200&height=800&coordinates=0%2C116%2C0%2C117"/><br/><br/><p>
<em>This article is part of our <a href="/collections/autonomous-weapons-challenges/" target="_blank">Autonomous Weapons Challenges</a> series. The IEEE Standards Association is <a href="https://saforms.ieee.org/autonomous-weapons-participation/" target="_blank">looking for your feedback</a> on this topic, and has invites you to answer <a href="https://saforms.ieee.org/autonomous-weapons-participation/" target="_blank">these questions</a>.</em></p><p>
	Lethal autonomous weapons systems can sound terrifying, but autonomy in weapons systems is far more nuanced and complicated than a simple debate between “good or bad” and “ethical or unethical.” In order to address the legal and ethical issues that an autonomous weapons system (AWS) can raise, it’s important to look at the many technical challenges that arise along the full spectrum of autonomy. A group of experts convened by 
	<a href="https://standards.ieee.org/" target="_blank">the IEEE Standards Association</a> is working on this, but they need your help.
</p><p>
	Weapons systems can be built with a range of autonomous capabilities. They might be self-driving tanks, surveillance drones with AI-enabled image recognition, unmanned underwater vehicles that operate in swarms, loitering munitions with advanced target recognition—the list goes on. Some autonomous capabilities are less controversial, while others trigger intense debate over the legality and ethics of the capability. Some capabilities have existed for decades, while others are still hypothetical and may never be developed.
</p><p>
	All of this can make autonomous weapons systems difficult to talk about, and doing so has proven to be incredibly challenging over the years. Answering even the most seemingly straightforward questions, such as whether an AWS is lethal or not, can get surprisingly complicated.
</p><hr/><p>
	To date, international discussions have largely focused on the legal, ethical, and moral issues that arise with the prospect of lethal AWS, with limited consideration of the technical challenges. At the United Nations, these discussions have taken place within the
	<a href="https://www.un.org/disarmament/the-convention-on-certain-conventional-weapons/background-on-laws-in-the-ccw/" rel="noopener noreferrer" target="_blank"> Convention on Certain Conventional Weapons</a>. After nearly a decade, though, the U.N. has yet to come up with a new treaty or regulations to cover AWS. In early discussions at the CCW and other international forums, participants often talked past each other: One person might consider a “fully autonomous weapons system” to include capabilities that are only slightly more advanced than today’s drones, while another might use the term as a synonym for the Terminator.
</p><p>
	Discussions advanced to the point that in 2019, member states at the CCW agreed on a set of 
	<a href="https://documents-dds-ny.un.org/doc/UNDOC/GEN/G19/343/64/PDF/G1934364.pdf?OpenElement" rel="noopener noreferrer" target="_blank">11 guiding principles regarding lethal AWS</a>. But these principles are nonbinding, and it’s unclear how the technical community can implement them. At the most recent meeting of the CCW in July, delegates repeatedly pushed for more nuanced discussions and understanding of the various technical issues that arise throughout the life cycle of an AWS.
</p><p>
	To help bring clarity to these and other discussions, the 
	<a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a> convened an expert group in 2020, to consider the ethical and technical challenges of translating AWS principles into practice and what that might mean for future development and governance.
</p><p>
	Last year, the expert group, which I lead, published its findings in a report entitled “<a href="https://standards.ieee.org/wp-content/uploads/import/documents/other/ethical-technical-challenges-autonomous-weapons-systems.pdf" rel="noopener noreferrer" target="_blank">Ethical and Technical Challenges in the Development, Use, and Governance of Autonomous Weapons Systems</a>.” In the document, we identified over 60 challenges of autonomous weapons systems, organized into 10 categories:
</p><ol>
<li>Establishing common language</li>
<li>Enabling effective human control</li>
<li>Determining legal obligations</li>
<li>Ensuring robustness</li>
<li>Testing and evaluating</li>
<li>Assessing risk</li>
<li>Addressing operational constraints</li>
<li>Collecting and curating data</li>
<li>Aligning procurement practices</li>
<li>Addressing nonmilitary use</li>
</ol><p>
	It’s not surprising that “establishing common language” is the first category. As mentioned, when the debates around AWS first began, the focus was on 
	<em>lethal</em> autonomous weapons systems, and that’s often still where people focus. Yet determining whether or not an AWS is lethal turns out to be harder than one might expect.
</p><p>
	Consider a drone that does autonomous surveillance and carries a remote-controlled weapon. It uses artificial intelligence to navigate to and identify targets, while a human makes the final decision about whether or not to launch an attack. Just the fact that the weapon and autonomous capabilities are within the same system suggests this could be considered a lethal AWS.
</p><p>
	Additionally, a human may not be capable of monitoring all of the data the drone is collecting in real time in order to identify and verify the target, or the human may over-trust the system (a 
	<a href="https://www.bbc.com/future/article/20211019-why-we-place-too-much-trust-in-machines" rel="noopener noreferrer" target="_blank">common problem</a> when humans work with machines). Even if the human makes the decision to launch an attack against the target that the AWS has identified, it’s not clear how much “meaningful control” the human truly has. (“<a href="https://blogs.icrc.org/law-and-policy/2018/08/15/autonomous-weapons-operationalizing-meaningful-human-control/" rel="noopener noreferrer" target="_blank">Meaningful human control</a>” is another phrase that has been hotly debated.)
</p><p>
	This problem of definitions isn’t just an issue that comes up when policymakers at the U.N. discuss AWS. AI developers also have different definitions for commonly used concepts, including “bias,” “transparency,” “trust,” “autonomy,” and “artificial intelligence.” In many instances, the ultimate question may not be, Can we establish technical definitions for these terms? but rather, How do we address the fact that there may never be consistent definitions and agreement on these terms? Because, of course, one of the most important questions for all of the AWS challenges is not whether we technically 
	<em>can</em> address this, but even if there is a technical solution, <em>should</em> we build and deploy the system?
</p><p>
	Identifying the challenges was just the first stage of the work for the IEEE-SA expert group. We also concluded that there are three critical perspectives from which a new group of experts will be considering these challenges in more depth:
</p><ul>
<li><strong><a href="https://spectrum.ieee.org/how-can-we-make-sure-that-aws-are-used-responsibly" target="_blank">Assurance and safety</a></strong>, which looks at the technical challenges of ensuring the system behaves the way it’s expected to.<br/>
</li>
<li><strong><a href="https://spectrum.ieee.org/the-challenge-of-human-control-over-autonomous-weapons" target="_blank">Human–machine teaming</a></strong>, which considers how the human and the machine will interact to enable reasonable and realistic human control, responsibility, and accountability.</li>
<li><strong><a href="https://spectrum.ieee.org/autonomous-weapon-systems-and-the-laws-of-war" target="_blank">Law, policy, and ethics</a></strong>, which considers the legal, political, and ethical implications of the issues raised throughout the Challenges document.</li>
</ul><div class="ieee-sidebar-large">
<h3>What Do You Think?</h3>
<p>
	This is where we want your feedback! Many of the AWS challenges are similar to those arising in other fields that are developing autonomous systems. We expect and hope that IEEE members and readers of 
		<em>IEEE Spectrum</em> will have insights from their own fields that can inform the discussion around AWS technologies.
	</p>
<p>
	We’ve put together a 
		<a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">series of questions</a> in the Challenges document that we hope you’ll answer, to help us better understand how people in other fields are addressing these issues. Autonomous capabilities will increasingly be applied to weapons systems, much as they are being applied in other realms, and we hope that by looking at the challenges in more detail, we can help establish effective technical solutions, while contributing to discussions about what can and should be legally acceptable. Your feedback will help us move toward this ultimate goal. <a href="https://saforms.ieee.org/autonomous-weapons-participation/" rel="noopener noreferrer" target="_blank">Public comments will be open through 7 December 2022</a>.
	</p>
</div><p>
<em>The independent group of experts who authored the report for the IEEE Standards Associate includes <a href="https://www.linkedin.com/in/emmanuelbloch/?locale=en_US" rel="noopener noreferrer" target="_blank">Emmanuel Bloch</a>, <a href="https://www.linkedin.com/in/aconn23/" rel="noopener noreferrer" target="_blank">Ariel Conn</a>, <a href="https://www.linkedin.com/in/denise-garcia-13054016b/" rel="noopener noreferrer" target="_blank">Denise Garcia</a>, <a href="https://www.linkedin.com/in/amandeep-gill-33052416/" rel="noopener noreferrer" target="_blank">Amandeep Gill</a>, <a href="https://www.linkedin.com/in/ashley-soulstice-llorens/" rel="noopener noreferrer" target="_blank">Ashley Llorens</a>, <a href="https://www.linkedin.com/in/martnoorma/" rel="noopener noreferrer" target="_blank">Mart Noorma</a>, and <a href="https://www.linkedin.com/in/heather-m-roff-95466713/" rel="noopener noreferrer" target="_blank">Heather Roff</a></em><em>.</em>
</p>]]></description><pubDate>Thu, 03 Nov 2022 13:10:04 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-weapons-challenges</guid><category>Autonomous weapons</category><category>Aws</category><category>Robotics</category><dc:creator>Ariel Conn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/abstract-illustration-of-overlapping-circles-and-squares.jpg?id=31988673&amp;width=980"></media:content></item><item><title>Innovative Shins Turn Quadrupedal Robot Biped</title><link>https://spectrum.ieee.org/quadrupedal-robot-shins-turns-biped</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-video-clip-showing-a-quadrupedal-robot-with-shin-extensions-standing-up-on-its-hind-legs.gif?id=32019322&width=1200&height=800&coordinates=0%2C1%2C0%2C2"/><br/><br/><p>
	The word <em>quadruped</em> means, technically, “four feet.” Roboticists tend to apply the term to anything that uses four limbs to walk, differentiating it from bipedal robots, which walk on two limbs instead. But there’s a huge, blurry crossover there, in both robotics and biology, where you find animals (and occasionally robots) that can transition from quadruped to biped when they need to (for example) manipulate something.
</p><p>
	If you look at quadrupedal robots simply as robots with four limbs rather than robots with four feet, they start to seem much more versatile, but that transition can be a tricky one. At the <a href="https://iros2022.org/" target="_blank">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems</a> (IROS 2022) in Kyoto, Japan, last week, researchers from <a href="https://www.wpi.edu/" target="_blank">Worcester Polytechnic Institute (WPI), in Massachusetts</a>, and ShanghaiTech University <a href="https://chenaah.github.io/multimodal/" target="_blank">presented</a> a generalizable method whereby an off-the-shelf quadruped robot can turn into a biped with some clever software and a tiny bit of mechanical modification.
</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bb5c78fbb0670171a354b39c52329e2e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Xf-fKWNRLcc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	We’ve seen robots that can transition from quadruped to biped before, but they’re almost always designed very deliberately to be able to do this, and they pay a penalty in weight, complexity, and cost. What’s unique about this research is that it’s intended to be applied to any quadruped at all—with some very minor hardware, your quadruped can become a biped, too.
</p><p>
	The mechanical side of this bipedalization is a 3D-printed stick that gets installed onto the shin of each of the quadruped’s hind legs. This provides additional support so that the robot can stand and walk robustly—without the shin attachments, the robot wouldn’t be statically stable. This is especially useful as the robot stands up, since its center of mass is fully supported during that process. The video shows this working on what looks like a <a href="https://robots.ieee.org/robots/minicheetah/" target="_blank">Mini Cheetah</a> robot, but again, the platform really doesn’t matter as long as it meets some basic requirements.
</p><p class="pull-quote">“We [seek] to reap the benefits from two worlds: stability and speed from quadrupeds, manipulability and a gain in operational height from bipeds.”<br/>—Andre Rosendo, Worcester Polytechnic Institute</p><p>
	Once the robot is upright, walking comes from a policy that’s trained first in simulation and then transferred onto the real robot. This isn’t trivial, because the controller is trying to get the robot to both walk and not fall over, which is a bit of a contradiction, but the best performing policy was able to get the robot to walk for several meters. It’s important to remember that this is a robot that was not designed to walk bipedally at all, so in some sense you’ve got software struggling to get hardware to work in a way that it isn’t supposed to and certainly isn’t optimized for. Perhaps if this kind of thing catches on, quadruped designers might be given incentives to build a little extra flexibility into their platforms to make them more adaptable.
</p><p>
	For more on this research, <em>IEEE Spectrum</em> spoke with <a href="https://www.wpi.edu/people/faculty/arosendo" rel="noopener noreferrer" target="_blank">Andre Rosendo, who is now a professor at WPI</a>.
</p><p><strong>Fundamentally is there a difference between a four-legged robot and a four-limbed robot?</strong>
</p><p>
<strong>Andre Rosendo: </strong>As seen in nature, quadruped locomotion enables higher speeds, and the robot is noticeably faster when moving with four legs. That said, the benefits related to manipulability seen in this animal transition from four to two legs (for example, <a href="https://humanorigins.si.edu/evidence/human-fossils/species/australopithecus-afarensis" target="_blank">Australopithecus</a> using their hands to bring food to their mouths) are also true to robots. We are currently developing a “variant end effector” for the forelimbs to allow this quadruped robot to become a “two-arm manipulator” when standing, handling objects and operating environments.
</p><p>
<strong>Why did you decide on this particular system to enable the bipedal transition?</strong>
</p><p>
<strong>Rosendo: </strong>We noticed that it is quite easy to adapt the hind legs of a quadruped robot with a fixed structure, with very little drop in performance. Although not as aesthetically pleasing as an active structure, advances in materials nowadays allow us to use a small carbon-fiber link protruding from the leg to mimic the same passive stability that our feet give us (known in legged locomotion as the <a href="https://en.wikipedia.org/wiki/Support_polygon" target="_blank">polygon of stability</a>). An active retractable system, on the other hand, would add a tiny motor to the leg, increasing the <a href="https://www.britannica.com/science/moment-of-inertia" target="_blank">moment of inertia</a> of that leg during locomotion, affecting performance negatively.
</p><p>
<strong>What are the limitations to the walking performance of this system?</strong>
</p><p>
<strong>Rosendo: </strong>We trained the robot in a simulated environment, and the walking gait, after being transferred to the real world, is stable, albeit slow. Bipedal robots usually have more degrees of freedom in their legs to allow a more dynamic and adaptive locomotion, but in our case, we are focusing on the multimodal aspect to reap the benefits from two worlds: stability and speed from quadrupeds, manipulability and a gain in operational height from bipeds.
</p><p>
<strong>What are you working on next?</strong>
</p><p>
<strong>Rosendo: </strong>Our next steps...will be on the development of the manipulability of this robot. More specifically, we have been asking ourselves the question: “Now that we can stand up, what can we do that other robots cannot?”, and we already have some preliminary results on climbing to places that are higher than the center of gravity of the robot itself. After mechanical changes on the forelimbs, we will better evaluate complex handling that might require both hands at the same time, which is rare in current mobile robots.
</p><div class="horizontal-rule">
</div><p><em>Multi-Modal Legged Locomotion Framework with Automated Residual Reinforcement Learning</em>, by Chen Yu and Andre Rosendo from ShanghaiTech University, was presented this week at IROS 2022 in Kyoto, Japan. More details are available <a href="https://chenaah.github.io/multimodal/" rel="noopener noreferrer" target="_blank">on Github</a>.</p>]]></description><pubDate>Wed, 02 Nov 2022 16:16:56 +0000</pubDate><guid>https://spectrum.ieee.org/quadrupedal-robot-shins-turns-biped</guid><category>Quadruped robots</category><category>Bipedal robots</category><category>Iros</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/a-video-clip-showing-a-quadrupedal-robot-with-shin-extensions-standing-up-on-its-hind-legs.gif?id=32019322&amp;width=980"></media:content></item><item><title>Eat This Drone</title><link>https://spectrum.ieee.org/edible-robots</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-drone-airplane-sits-on-a-lab-bench-it-has-a-normal-propeller-and-motor-and-tail-but-its-wings-are-made-of-rice-crackers-cover.jpg?id=32019267&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Drones have the potential to be very useful in disaster scenarios by transporting food and water to people in need. But whenever you ask a drone to transport anything, anywhere, the bulk of what gets moved is the drone itself. Most delivery drones can carry only about 30 percent of their mass as payload, because most of their mass is both critical, like wings, and comes in the form of things that are essentially useless to the end user, like wings.</p><p>At the IEEE/RSJ International Conference on Intelligent Robots and Systems (<a href="https://iros2022.org/" target="_blank">IROS</a>) conference in Kyoto last week, researchers from the <a href="https://www.epfl.ch/labs/lis/" target="_blank">Swiss Federal Institute of Technology Lausanne (EPFL)</a> presented a paper describing a drone that can boost its payload of food from 30 percent to 50 percent of its mass. It does so with the ingenious use of wings made from rice cakes that contain the caloric equivalent of an average, if unbalanced, breakfast. For anyone interested in digesting the paper, it is titled “Towards Edible Drones for Rescue Missions: Design and Flight of Nutritional Wings,” by Bokeon Kwak, Jun Shintake, Lu Zhang, and Dario Floreano from EPFL.</p><hr/><p>The reason why this drone exists is to work toward the effective and efficient delivery of food to someone who, for whatever reason, really, really needs food and is not in a position to gain access to it in any other way. The idea is that you could fly this drone directly to them and keep them going for an extra day or two. You obviously won’t get the drone back afterward (because its wings will have been eaten off), but that’s a small price to pay for potentially keeping someone alive via the delivery of vital calories.<br/></p><p>The researchers designed the wing of this partially edible drone out of compressed puffed rice (rice cakes or rice cookies, depending on whom you ask) because of the foodstuff’s similarity to expanded polypropylene (EPP) foam. EPP foam is something that’s commonly used as wing material in drones because it’s strong and lightweight; puffed rice shares those qualities. Though it’s not quite as strong as the EPP, it’s not bad. And it’s also affordable, accessible, and easy to laser cut. The puffed rice also has a respectable calorie density—at 3,870 kilocalorie per kilogram, rice cakes aren’t as good as something like chocolate, but they’re about on par with pasta, just with a much lower density.</p><p>Out of the box, the rice cakes are round, so the first step in fabricating the wing is to laser cut them into hexagons to make them easier to stick together. The glue is just gelatin, and after it all dries, the wing is packaged in plastic and tape to make sure that it doesn’t break down in wet or humid environments. It’s a process that’s fast, simple, and cheap.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="34ff74f7de5643c072335f12a54277bf" data-rm-shortcode-name="rebelmouse-image" id="4db39" loading="lazy" src="https://spectrum.ieee.org/media-library/image.gif?id=32019265&width=980"/>
</p><p>The size of the wing is actually driven not by flight requirements but by nutrition requirements. In this case, a wingspan of about 70 centimeters results in enough rice cake and gelatin glue to deliver 300 kcal, or the equivalent of one breakfast serving, with 80 grams remaining for a payload of vitamins or water or something like that. The formula the researchers came up with to calculate the design of this avian appetite quencher assumes that the rest of the drone is not edible, because it isn’t. The structure and tail surfaces are made of carbon fiber and foam.</p><p>While this is just a prototype, the half-edible drone does actually fly, achieving speeds of about 10 meters per second with the addition of a motor, some servos to actuate the tail surfaces for control, and a small battery. The next step is to figure out a way to make as many of those nonedible pieces out of edible materials instead, as well as finding a way to carry a payload (like water) in an edible container.</p><p>For a bit more about this drone, we spoke with the lead author of the paper, Bokeon Kwak.</p><p><strong>It sounds like your selection of edible wing material was primarily optimized for its mechanical properties and low weight. Are there other options that could work if the goal was to instead optimize for calories while still maintaining functionality?</strong></p><p><strong>Bokeon Kwak: </strong>As you pointed out, achieving sufficient mechanical properties while maintaining low weight (with food materials) was the foremost design criteria in designing the edible wing. We can expand the design criteria to contain higher calories by using fat-based material—for example, edible wax; fat has higher calories per gram than proteins and carbohydrates. On the other hand, containing more calories also implies the increase of structural weight, which is a price we need to pay toward higher calories. This aspect also requires further study to find a sweet spot! </p><p><strong>What does the drone taste like?</strong></p><p><strong>Kwak:</strong> The edible wing tastes like a crunchy rice crisp cookie with a little touch of raw gelatin (which worked as an edible glue to hold the rice cookies as a flat plate shape). No artificial flavor has been added yet. </p><p><strong>Would there be any significant advantages to making the wing into a more complex shape, for example with an airfoil cross-section instead of a flat plate?</strong></p><p><strong>Kwak:</strong> Making a well-streamlined airfoil (instead of a flat plate) is actually our next goal to achieve more efficient aerodynamic properties, such as lower drag and higher lift. These advantages let an edible drone carry more payload (which is useful to carry water) and have prolonged flight time and distance. Our team is testing 3D food printing and molding to create such an edible wing, including material characterization to make sure the edible wing has sufficient mechanical properties (for example, higher Young’s modulus, low density). </p><p><strong>What else will you be working on next?</strong></p><p><strong>Kwak:</strong> Other structural components such as wing control surfaces (such as an aileron or a rudder) will be made of edible material by 3D food printing or molding. Other things that will be considered are an edible/water-resistant coating on the surface of the edible wing, and degradation testing of the edible wing upon time (and water exposure).</p><div class="horizontal-rule"></div><p>This drone is just one application of a broader European research initiative called <a href="https://www.robofood.org/" rel="noopener noreferrer" target="_blank">RoboFood</a>, which seeks to develop edible robots in a way that maximizes both performance and nutritional value. Edible sensing, actuation, and computation are all parts of this project, and the researchers (<a href="https://www.epfl.ch/labs/lis/" rel="noopener noreferrer" target="_blank">led by Dario Floreano at EPFL</a>) can now start to focus on some of those more challenging edible components. </p>]]></description><pubDate>Tue, 01 Nov 2022 16:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/edible-robots</guid><category>Drones</category><category>Iros 2022</category><category>Epfl</category><category>Robotics</category><category>Edible robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-drone-airplane-sits-on-a-lab-bench-it-has-a-normal-propeller-and-motor-and-tail-but-its-wings-are-made-of-rice-crackers-cover.jpg?id=32019267&amp;width=980"></media:content></item><item><title>Video Monday: IROS 2022 Award Winners</title><link>https://spectrum.ieee.org/robot-videos-iros-award-winners</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-large-conference-center-in-japan-on-a-sunny-day.jpg?id=32023205&width=1200&height=800&coordinates=170%2C0%2C171%2C0"/><br/><br/><p>IROS 2022 took place in Kyoto last week, bringing together thousands of roboticists from around the world to share all the latest awesome research they’ve been working on. We’ve got a bunch of stuff to bring you from the conference, but while we work on that (and recover from some monster jetlag), here are the presentation videos of all of the <a href="https://iros2022.org/2022/10/30/award-winners/" target="_blank">IROS 2022 award-winning papers</a>. This is the some of the best, most impactful robotics research presented this year. Congratulations to all of the winners!</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best Paper Award</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="22786bbd78e15453cc7a7f1e5014b8a3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UTMT2WAUlRw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“SpeedFolding: Learning Efficient Bimanual Folding of Garment,” by Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten Kroeger, and Ken Goldberg from the University of California, Berkeley, and the Karlsruhe Institute of Technology.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84508" target="_blank">https://events.infovaya.com/presentation?id=84508</a>
</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best Student Paper Award – Sponsored by ABB</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="f549e7147d656ae45477c82c2cbb343f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6M7SWa1lxcM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“FAR Planner: Fast, Attemptable Route Planner Using Dynamic Visibility Update,” by Fan Yang, Chao Cao, Hongbiao Zhu, Jean Oh, and Ji Zhang from Carnegie Mellon University and the Harbin Institute of Technology.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84511" rel="noopener noreferrer" target="_blank">https://events.infovaya.com/presentation?id=84511</a></p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Cognitive Robotics – Sponsored by KROS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="14c74b9de5b2bd6358ace305fa635b8a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/qFObMpOboCg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Gesture2Vec: Clustering Gestures Using Representation Learning Methods for Co-Speech Gesture Generation,” by Payam Jome Yazdian, Mo Chen, and Angelica Lim from Simon Fraser University.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=90186" target="_blank">https://events.infovaya.com/presentation?id=90186</a>
</p><div class="horizontal-rule">
</div><h3>IROS 2022 Best RoboCup Paper Award – Sponsored by RoboCup Federation</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="ef451ccb67e56b4a509e175340c38ac8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/mNy1cloWrP0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“RCareWorld: A Human-Centric Simulation World for Caregiving Robots,” by Ruolin Ye, Wenqiang Xu, Haoyuan Fu, Rajat Kumar Jenamani, Vy Nguyen, Cewu Lu, Katherine Dimitropoulou, and Tapomayukh Bhattacharjee from Cornell University, Shanghai Jiao Tong University, and Columbia University.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=84520" target="_blank">https://events.infovaya.com/presentation?id=84520</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Robot Mechanisms and Design – Sponsored by ROBOTIS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="b42d5ffc61aff7acc0b3c83690b90d0d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/F9CE_CdK3Oo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Aerial Grasping and the Velocity Sufficiency Region,” by Tony G. Chen, Kenneth Hoffmann, JunEn Low, Keiko Nagami, David Lentink, and Mark Cutkosky from Stanford University and Wageningen University & Research.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85675" target="_blank">https://events.infovaya.com/presentation?id=85675</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Entertainment and Amusement Paper Award – Sponsored by JTCF</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="e7819d2c975c638e060cde9242927457" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2DCkyE0l0aI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Robot Learning to Paint From Demonstrations,” by Younghyo Park, Seunghun Jeon, and Taeyoon Lee from Seoul National University, KAIST, and Naver Labs.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85681" target="_blank">https://events.infovaya.com/presentation?id=85681</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Safety, Security, and Rescue Robotics in memory of Motohiro Kisoi – Sponsored by IRS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="705dface359e7a7a41e6b3571d19ec77" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/O-AQHSVig7U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Power-Based Safety Layer for Aerial Vehicles in Physical Interaction Using Lyapunov Exponents,” by Eugenio Cuniato, Nicholas Lawrance, Marco Tognon, and Roland Siegwart from ETH Zurich and CSIRO.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86266" target="_blank">https://events.infovaya.com/presentation?id=86266</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Agri-Robotics – Sponsored by YANMAR</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="3b2f882c126aef5f8454837e22e0eecd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DfIN8Br7xFo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Explicitly Incorporating Spatial Information to Recurrent Networks for Agriculture,” by Claus Smitt, Michael Allan Halstead, Alireza Ahmadi, and Christopher Steven McCool from the University of Bonn.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86839" target="_blank">https://events.infovaya.com/presentation?id=86839</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award on Mobile Manipulation – Sponsored by OMRON Sinic X Corp.</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="abfeb764a51be62c241e25d02507d356" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/7KNHtBwkt64?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Robot Learning of Mobile Manipulation With Reachability Behavior Priors,” by Snehal Jauhri, Jan Peters, and Georgia Chalvatzaki from TU Darmstadt.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86827" target="_blank">https://events.infovaya.com/presentation?id=86827</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Application Paper Award – Sponsored by ICROS</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="f57802c306105adcf47dcebd71446a84" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YQaLRtOtQf8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Soft Tissue Characterisation Using a Novel Robotic Medical Percussion Device with Acoustic Analysis and Neural Network,” by Pilar Zhang Qiu, Yongxuan Tan, Oliver Thompson, Bennet Cobley, and Thrishantha Nanayakkara from Imperial College London.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=86287" target="_blank">https://events.infovaya.com/presentation?id=86287</a>
</p><div class="horizontal-rule">
</div><h3>IROS Best Paper Award for Industrial Robotics Research for Applications – Sponsored by Mujin Inc.</h3><br/><span class="rm-shortcode" data-rm-shortcode-id="9579d11d44c465ddf325e85b5e85239a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DGqvREtBnS4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p>
	“Absolute Position Detection in 7-Phase Sensorless Electric Stepper Motor,” by Vincent Groenhuis, Gijs Rolff, Koen Bosman, Leon Abelmann, and Stefano Stramigioli from the University of Twente, IMS BV, and Eye on Air.</p><p>
	Read more: <a href="https://events.infovaya.com/presentation?id=85705" target="_blank">https://events.infovaya.com/presentation?id=85705</a>
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Mon, 31 Oct 2022 20:56:31 +0000</pubDate><guid>https://spectrum.ieee.org/robot-videos-iros-award-winners</guid><category>Robotics research</category><category>Robots</category><category>Iros</category><dc:creator>Erico Guizzo</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-large-conference-center-in-japan-on-a-sunny-day.jpg?id=32023205&amp;width=980"></media:content></item><item><title>Video Friday: Swarm Control</title><link>https://spectrum.ieee.org/video-friday-swarm-control</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-from-a-low-angle-looking-upwards-of-a-human-in-a-futuristic-suit-gesturing-at-night-while-hundreds-of-illuminated-drones.png?id=31976554&width=1200&height=800&coordinates=352%2C0%2C353%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote><em>Imagine being able to control a swarm of drones with just your hands or gestures. In this video, we explore a future concept-of-operations for swarm management and how large groups of robots and drones will be able to interact and work together. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="32da93d8ff33eccbec7194d4c1ef8f57" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/plrS-dbwXr8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dronisos.com/">Dronisos</a> ]</p><div class="horizontal-rule"></div><p>There’s a new Mini Pupper on Kickstarter, now with ROS 2!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c0e7e0ddbf2343b068514e940f473cb3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/LeT10vdBSJM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kickstarter.com/projects/336477435/mini-pupper-2-open-source-ros2-robot-kit-for-dreamers/">Kickstarter</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Researchers created a method for magnetically programming materials to make cubes that are very picky about who they connect with, enabling more scalable self-assembly.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6a270915c408d3a1d2bd724c9a8d1825" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/tWDWDf08XhE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Paper at IROS next week!</p><p>[ <a href="https://hcie.csail.mit.edu/research/selective/selective.html">MIT CSAIL</a> ]</p><p>Thanks, Rachel!</p><div class="horizontal-rule"></div><blockquote><em>This summer, we held a contest seeking ideas robots inspired by nature, that could help the world. And then we made the winning idea into a real working prototype! The winner this year was “Gillbert” by Eleanor Mackinstosh, a robotic fish that filters microplastics using its gills.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f8a711c63e56cb5c4640dc5491b57ad4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ld15OYvvgfk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.naturalroboticscontest.com/">Natural Robotics Contest</a> ]</p><p>Thanks, Rob!</p><div class="horizontal-rule"></div><p>I’ve never seen a real centaur climb up onto a block while carrying a payload, but I bet it would look almost exactly like Centauro doing it.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d934152faa811b0cb06f52798f4f7927" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YaPbRVBxj8o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2210.06803">Paper</a> ]</p><p>Thanks, Ioannis!</p><div class="horizontal-rule"></div><blockquote><em>Enjoy our favorite obstacle avoidance highlights from the Skydio community! They make showcasing the intellect of our software sublimely easy. The power of autonomous cinematography is displayed best by our incredible Skydians!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="febb4121fcb663ff8fd4110e4c620ece" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ui89kE0p5a8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>That last clip is especially impressive, since if you look closely, you can see the drone avoiding a wire while flying directly toward the setting sun.</p><p>[ <a href="https://www.skydio.com/">Skydio</a> ]</p><div class="horizontal-rule"></div><p>Somehow I missed this adorable little robot of questionable usefulness from Sony.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1171a34937c4e7a7af3d3b799988e859" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/1GMRq-tOHLo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>Meet poiq, your future buddy robot. Its AI gets smarter and more individualized through questions and conversations with users. Sony is reimagining communication and connection, and developing one-of-a-kind friendships between humans and robots in the process.</em></blockquote><p>[ <a href="https://poiq.sony.jp/">Sony</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Spot’s got permission to dance! Check out this dance created for the “BTS Yet To Come in BUSAN” concert.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="50b1b79cf11030b381893621d202d37f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yEW-9SbXahI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><p>Awawa, awawa...</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="98d08b1e0b15b0fcee0d0a3f889d2666" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xesJ7O2g3H8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.icd.cs.tut.ac.jp/index.php/portfolio/pokebo-cube/">ICD Lab</a> ]</p><div class="horizontal-rule"></div><p>Ascento, on patrol.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c74fc6685349250d2a7db4d3d0be190d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/POUaZlzlBAE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.ascento.ch/">Ascento Robotics</a> ]</p><div class="horizontal-rule"></div><p>Here’s what happens if you grab a Wing delivery drone’s cable and start running.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c7e8ec910fa0d51672d7a031635e624b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/snLOdFxrvUo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://wing.com/">Wing</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Detecting an overheating motor can be the difference between a $1,000 repair or a $50,000 replacement. As a result, routine thermal inspections are a major part of predictive maintenance operations, but collecting this valuable information frequently is still a challenge in many facilities. Agile mobile robots like Spot are transforming condition monitoring with dynamic sensing, so industrial teams can make the most of their predictive maintenance programs. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b53a09248dba05c2cb79d42d4d5a63b2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-XUVulOQX9c?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/solutions/inspection/thermal">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Robotnik is specialized in the development of industrial robotic applications based in mobile robots and mobile manipulators. Here [are] some AMR developed and manufactured by us.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9ed311af8c3039b3803cd5d4b6d056d5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/X9qRKnegxUY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotnik.eu/">Robotnik</a> ]</p><div class="horizontal-rule"></div><p>How many robot dogs does it take to explore a football field? Fewer than it would if they weren’t working together, that’s for sure.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="bd9b5b95ba8c99274bd077bb17daefcd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/CNh8KziM_XA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.deeprobotics.cn/en/">Deep Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>During Summer 2022 our group demoed ANYmal and Spot carrying out in the context of construction progress monitoring at Costain’s Gatwick Airport Train Station site. This was the final demo of the MEMMO Horizon Europe Project.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3cbba96397f55dab6bc5b7618543b668" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-zfE7Dyon8o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ori.ox.ac.uk/labs/drs/">Oxford</a> ]</p><div class="horizontal-rule"></div><p>Lex Fridman interviews Kate Darling.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e6a299b3fa628663276f9d9bdb0e13ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ZFntEFXKDHM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://lexfridman.com/kate-darling-2/">Lex Fridman</a> ]</p><div class="horizontal-rule"></div><p>In this week’s CMU RI Seminar, Nidhi Kalra from The RAND Corporation answers the question, “What (else) can you do with a robotics degree?”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4b0d770924db9968db842768145830be" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9dyfUAJyxPI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.ri.cmu.edu/event/ri-seminar-nidhi-kalra-rand-corporation-senior-information-scientist-2022-10-14/">CMU RI</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 21 Oct 2022 15:06:26 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-swarm-control</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-photo-from-a-low-angle-looking-upwards-of-a-human-in-a-futuristic-suit-gesturing-at-night-while-hundreds-of-illuminated-drones.png?id=31976554&amp;width=980"></media:content></item><item><title>Who Will Fix Hubble and Chandra?</title><link>https://spectrum.ieee.org/elon-musk-spacex-hubble</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/the-hubble-space-telescope-with-clouds-in-the-background-seen-during-a-space-shuttle-servicing-mission-in-2002.jpg?id=31972878&width=1200&height=800&coordinates=19%2C0%2C19%2C0"/><br/><br/><p>
<a href="https://spectrum.ieee.org/spacex-starlink-dish-network" target="_self">Elon Musk</a>, step aside. You may be the richest rich man in the space business, but you’re not first. Musk’s <a href="https://spectrum.ieee.org/search/?q=spacex" target="_self">SpaceX</a> corporation is a powerful force, with its weekly launches and visions of colonizing Mars. But if you want a broader view of how wealthy entrepreneurs have shaped space exploration, you might want to look at <a href="https://en.wikipedia.org/wiki/George_Ellery_Hale" target="_blank">George Ellery Hale</a>, <a href="https://en.wikipedia.org/wiki/James_Lick" target="_blank">James Lick</a>, <a href="https://en.wikipedia.org/wiki/William_Johnson_McDonald" target="_blank">William McDonald</a> or—remember this name—<a href="https://en.wikipedia.org/wiki/John_Daggett_Hooker" target="_blank">John D. Hooker</a>.
</p><p>
	All this comes up now because SpaceX, joining forces with the billionaire 
	<a href="https://polarisprogram.com/team/" rel="noopener noreferrer" target="_blank">Jared Isaacman</a>, has made what sounds at first like a novel <a href="https://www.nasa.gov/feature/goddard/2022/nasa-spacex-to-study-hubble-telescope-reboost-possibility" target="_blank">proposal</a> to NASA: It would like to see if one of the company’s Dragon spacecraft can be sent to service the fabled, invaluable (and aging) <a href="https://www.nasa.gov/mission_pages/hubble/main/index.html" rel="noopener noreferrer" target="_blank">Hubble Space Telescope</a>, last repaired in 2009.
</p><p>
	Private companies going to the rescue of one of NASA’s crown jewels? NASA’s mantra in recent years has been to let 
	<a href="https://spectrum.ieee.org/private-space-stations" target="_self">private enterprise</a> handle the day-to-day of space operations—communications satellites, getting astronauts to the space station, and so forth—while pure science, the stuff that makes history but not necessarily money, remains the province of government. Might that model change?
</p><p>
	“We’re working on crazy ideas all the time,” said 
	<a href="https://science.nasa.gov/about-us/leadership/Thomas-Zurbuchen" rel="noopener noreferrer" target="_blank">Thomas Zurbuchen</a>, NASA’s space science chief. "Frankly, that’s what we’re supposed to do.”
</p><p>
	It’s only a six-month feasibility study for now; no money will change hands between business and NASA. But Isaacman, who made his fortune in 
	<a href="https://www.shift4.com/" rel="noopener noreferrer" target="_blank">payment-management software</a> before turning to space, suggested that if a Hubble mission happens, it may lead to other things. “Alongside NASA, exploration is one of many objectives for the commercial space industry,” he said on a media teleconference. “And probably one of the greatest exploration assets of all time is the Hubble Space Telescope.”
</p><p>
	So it’s possible that at some point in the future, there may be a SpaceX Dragon, perhaps with Isaacman as a crew member, setting out to grapple the Hubble, boost it into a higher orbit, maybe even replace some worn-out components to lengthen its life.
</p><p>
	Aerospace companies say privately mounted repair sounds like a good idea. So good that they’ve proposed it already.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="The Chandra X-ray telescope, photographed by space shuttle astronauts after they deployed it in July 1999." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9ae81d7d302196c0c852dc9d2f3f7385" data-rm-shortcode-name="rebelmouse-image" id="ecbfc" loading="lazy" src="https://spectrum.ieee.org/media-library/the-chandra-x-ray-telescope-photographed-by-space-shuttle-astronauts-after-they-deployed-it-in-july-1999.jpg?id=31972890&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The Chandra X-ray telescope, as photographed by space-shuttle astronauts after they deployed it in July 1999. It is attached to a booster that moved it into an orbit 10,000 by 100,000 kilometers from Earth.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">NASA</small>
</p><p>
<a href="https://www.northropgrumman.com/" rel="noopener noreferrer" target="_blank">Northrop Grumman</a>, one of the United States’ largest aerospace contractors, has quietly suggested to NASA that it might service one of the Hubble’s sister telescopes, the <a href="https://www.nasa.gov/mission_pages/chandra/main/index.html" rel="noopener noreferrer" target="_blank">Chandra X-ray Observatory</a>. Chandra was launched into Earth orbit by the space shuttle Columbia in 1999 (Hubble was launched from the shuttle Discovery in 1990), and the two often complement each other, observing the same celestial phenomena at different wavelengths.
</p><p>
	As in the case of the SpaceX/Hubble proposal, Northrop Grumman’s Chandra study is at an early stage. But there are a few major differences. For one, Chandra was assembled by TRW, a company that has since been bought by Northrop Grumman. And another company subsidiary, 
	<a href="https://www.northropgrumman.com/space/space-logistics-services/" rel="noopener noreferrer" target="_blank">SpaceLogistics</a>, has been sending what it calls <a href="https://www.northropgrumman.com/space/space-logistics-services/" rel="noopener noreferrer" target="_blank">Mission Extension Vehicles</a> (MEVs) to service aging Intelsat communications satellites since 2020. Two of these robotic craft have launched so far. The MEVs act like space tugs, docking with their target satellites to provide them with attitude control and propulsion if their own systems are failing or running out of fuel. SpaceLogistics says it is developing a next-generation rescue craft, which it calls a Mission Robotic Vehicle, equipped with an articulated arm to add, relocate, or possibly repair components on orbit.
</p><p>
	“We want to see if we can apply this to space-science missions,” says 
	<a href="https://materials.ucsb.edu/events/dr-jonathan-w-arenberg-northrop-grumman" rel="noopener noreferrer" target="_blank">Jon Arenberg</a>, Northrop Grumman’s chief mission architect for science and robotic exploration, who worked on Chandra and, later, the <a href="https://spectrum.ieee.org/collections/james-webb-telescope/" target="_self">James Webb Space Telescope</a>. He says a major issue for servicing is the exacting specifications needed for NASA’s major observatories; Chandra, for example, records the extremely short wavelengths of X-ray radiation (0.01–10 nanometers).
</p><p>
	“We need to preserve the scientific integrity of the spacecraft,” he says. “That’s an absolute.”
</p><p>
	But so far, the company says, a mission seems possible. NASA managers have listened receptively. And Northrop Grumman says a servicing mission could be flown for a fraction of the cost of a new telescope.
</p><div class="rm-embed embed-media"><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/JUnA88PQY3k" title="YouTube video player" width="560">
</iframe></div><p>
	New telescopes need not be government projects. In fact, NASA’s chief economist, 
	<a href="https://www.nasa.gov/about/alex-macdonald-chief-economist-office-of-the-administrator/" rel="noopener noreferrer" target="_blank">Alexander MacDonald</a>, argues that almost all of America’s greatest observatories were privately funded until Cold War politics made government the major player in space exploration. That’s why this story began with names from the 19th and 20th centuries—Hale, Lick, and McDonald—to which we should add <a href="https://en.wikipedia.org/wiki/Charles_Yerkes" target="_blank">Charles Yerkes</a> and, more recently, <a href="https://en.wikipedia.org/wiki/William_Myron_Keck" target="_blank">William Keck</a>. These were arguably the Elon Musks of their times—entrepreneurs who made millions in oil, iron, or real estate before funding the United States’ largest telescopes. (Hale’s father manufactured elevators—highly profitable in the rebuilding after the Great Chicago Fire of 1871.) The most ambitious observatories, MacDonald calculated for his book <em><a href="https://yalebooks.yale.edu/book/9780300219326/the-long-space-age/" rel="noopener noreferrer" target="_blank">The Long Space Age</a></em>, were about as expensive back then as some of NASA’s modern planetary probes. None of them had very much to do with government.
</p><p>
	To be sure, government will remain a major player in space for a long time. “NASA pays the cost, predominantly, of the development of new commercial crew vehicles, SpaceX’s Dragon being one,” MacDonald says. “And now that those capabilities exist, private individuals can also pay to utilize those capabilities.” Isaacman doesn’t have to build a spacecraft; he can hire one that SpaceX originally built for NASA.
</p><p>“I think that creates a much more diverse and potentially interesting space-exploration future than we have been considering for some time,” MacDonald says.</p><p>
	So put these pieces together: Private enterprise has been a driver of space science since the 1800s. Private companies are already conducting on-orbit satellite rescues. NASA hasn’t said no to the idea of private missions to service its orbiting observatories.
</p><p>
	And why does John D. Hooker’s name matter? In 1906, he agreed to put up US $45,000 (about $1.4 million today) to make the mirror for a 
	<a href="https://www.mtwilson.edu/building-the-100-inch-telescope/" rel="noopener noreferrer" target="_blank">100-inch reflecting telescope at Mount Wilson, Calif</a>. One astronomer made the Hooker Telescope famous by using it to determine that the universe, full of galaxies, was expanding.
</p><p>
	The astronomer’s name was 
	<a href="https://en.wikipedia.org/wiki/Edwin_Hubble" target="_blank">Edwin Hubble</a>. We’ve come full circle.
</p>]]></description><pubDate>Thu, 20 Oct 2022 18:06:36 +0000</pubDate><guid>https://spectrum.ieee.org/elon-musk-spacex-hubble</guid><category>Elon musk</category><category>Spacex</category><category>Nasa</category><category>Northrop grumman</category><category>Hubble space telescope</category><category>Chandra x-ray observatory</category><category>Service robotics</category><dc:creator>Ned Potter</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/the-hubble-space-telescope-with-clouds-in-the-background-seen-during-a-space-shuttle-servicing-mission-in-2002.jpg?id=31972878&amp;width=980"></media:content></item><item><title>Goalkeeping Robot Dog Tends Its Net Like a Pro</title><link>https://spectrum.ieee.org/football-robot-mini-cheetah</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/animated-gif-of-a-human-kicking-a-soft-football-soccer-ball-at-a-small-quadruped-robot-which-leaps-to-successfully-block-the.gif?id=31924013&width=1200&height=800&coordinates=117%2C0%2C118%2C0"/><br/><br/><p>The best professional football goalkeepers in the English Premiere League (we’re talking about the sport called soccer in North America) are able to save <a href="https://scores.nbcsports.com/epl/player_leaders.asp?category=107" rel="noopener noreferrer" target="_blank">almost, but not quite, 80 percent of shots taken on goal</a>. This is very good. But it’s not nearly as good as the 87.5 percent of shots that a 9-kilogram quadrupedal robot can block: In its tiny goal, and versus tiny children taking tiny shots, <a href="https://robots.ieee.org/robots/minicheetah/" target="_blank">Mini Cheetah</a> turns out to be an excellent goalie.</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bf3226393fe9a40eb601854c472834d3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iX6OgG67-ZQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>What’s the point of this? Well, it’s fun! Also, this is a challenging problem, because it involves highly dynamic locomotion with object manipulation—specifically, manipulating a fast-moving ball in any direction except for into the goal. Teaching the robot to move its body dynamically while also making sure that its feet (or face) end up where they need to be in time to block the ball is basically two problems combined into one. The trick here is combining the right locomotion controller with a planner for the end-effector trajectory that can find the best way to get Mini Cheetah in front of the ball for the save—all in the less than a second that it takes for the ball to travel to the goal.</p><p>The approach to solving this was to train Mini Cheetah on a set of useful goalkeeping skills: sidestepping for intercepts near the robot and close to the ground, diving to reach the lower corners of the goal, and jumping to cover the top of the goal and the upper corners. The idea (or hope?) is that all of these skills are recoverable and that the robot will end up making a safe landing on its feet afterward. But as with human goalies, that’s a secondary concern behind making a successful save. A reference motion for each skill is manually programmed in, and then the system is trained up in simulation before being transferred directly to the robot. Intercepting the ball involves the system choosing which skill will get a piece of the robot to intersect the ball’s trajectory in the most stable and energy-efficient way.</p><p>The goal that Mini Cheetah is defending is 1.5 meters wide and 0.9 meters high, and the ball (size 3) is kicked from about 4 meters away. The ball is tracked externally. The robot’s performance here is pretty impressive for such a little robot, but we should keep it in context:</p><blockquote>We show that our system can be used to directly transfer dynamic maneuvers and goalkeeping skills learned in simulation to a real quadrupedal robot, with an 87.5 [percent] successful interception rate of random shots in the real world. We note that human soccer goalkeepers average around a 69 [percent] save rate. Although this is against professional players shooting towards regulation sized goals, we hope this paper takes us one step closer to enabling robotic soccer players to compete with humans in the near future.</blockquote><p>If you think about it, the sport of football is basically a bunch of discrete skills that can be chained together around the trajectory of a ball in support of a high-level goal. And the researchers say that “the proposed framework can be extended to other scenarios, such as multiskill soccer-ball kicking.” This group has already done some early work on shooting, and it’ll be fun to see what they come up with next.</p><p>And also, watch your back, English Premiere League goalies: Mini Cheetah is coming for you.</p><em>Creating a Dynamic Quadrupedal Robotic Goalkeeper with Reinforcement Learning</em>, by Xiaoyu Huang, Zhongyu Li, Yanzhen Xiang, Yiming Ni, Yufeng Chi, Yunhao Li, Lizhi Yang, Xue Bin Peng, and Koushil Sreenath from <a href="https://hybrid-robotics.berkeley.edu/" target="_blank">the University of California, Berkeley's Hybrid Robotics Lab</a>, is <a href="https://arxiv.org/abs/2210.04435" rel="noopener noreferrer" target="_blank">available on arXiv</a>.]]></description><pubDate>Sun, 16 Oct 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/football-robot-mini-cheetah</guid><category>Mini cheetah</category><category>Legged robots</category><category>Soccer robots</category><category>Football robots</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/animated-gif-of-a-human-kicking-a-soft-football-soccer-ball-at-a-small-quadruped-robot-which-leaps-to-successfully-block-the.gif?id=31924013&amp;width=980"></media:content></item><item><title>Video Friday: Venus Aerobot</title><link>https://spectrum.ieee.org/video-friday-venus-aerobot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-silver-balloon-with-a-robotic-payload-drifts-high-above-a-desert-landscape.jpg?id=31944712&width=1200&height=800&coordinates=312%2C0%2C313%2C0"/><br/><br/><p>Your weekly selection of awesome robot videos</p><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.</p><h5><a href="https://roscon.ros.org/2022/">ROSCon 2022</a>: 19–21 October 2022, KYOTO, JAPAN</h5><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today's videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote><em>In July 2022, technologists from NASA’s Jet Propulsion Laboratory in Southern California and Near Space Corporation in Tillamook, Ore., carried out two successful flights of an aerial robotic balloon, or aerobot, over Nevada’s Black Rock Desert. The prototype is a scaled-down version of an aerobot that could one day take to Venus’s skies, exploring an atmospheric region too low for orbiters.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="95ad4fa082e63318a5cb4d1246e5ccb4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ug6B2hnNk3I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.jpl.nasa.gov/news/jpls-venus-aerial-robotic-balloon-prototype-aces-test-flights">JPL</a> ]</p><div class="horizontal-rule"></div><blockquote><em>UC Berkeley researchers debut EMBUR (EMerita BUrrowing Robot), a unique robot inspired by Pacific mole crabs. Hannah Stuart, assistant professor of mechanical engineering, and Ph.D. student Laura Treers demonstrate one of the first legged robots that can self-burrow vertically, the way Pacific mole crabs bury themselves in beach sand.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="52a678d81fbd379f6853e1eb95c60c22" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Yya-KHE092A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://engineering.berkeley.edu/news/2022/10/digging-deep/">UC Berkeley</a> ]</p><div class="horizontal-rule"></div><p>A full-length documentary on one of the bravest robots to ever roll the Mars? I will watch the crap out of this.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="10c19f9c7a4b6e3b198d53b8940f19d1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/W4t58Yruhds?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>In theaters 4 November and streaming on Amazon Prime Video 23 November.</p><p>[ <a href="https://press.amazonstudios.com/us/en/original-movies/good-night-oppy">Amazon Prime Video</a> ]</p><div class="horizontal-rule"></div><blockquote><em>The Sapien 6M intelligent manipulator offers speed, dexterity, precision, and strength in a compact, lightweight package. With six degrees of freedom, an optimized strength-to-weight ratio, embedded intelligence, and a sleek hardware design that can withstand extreme temperatures and environmental conditions (IP66), the Sapien robotic arm can be used for a variety of complex outdoor and indoor applications.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4f2375b47023ecac4db8182efb1fb164" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yDrda-M93gY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.sarcos.com/products/sapien-6m-robotic-arm/">Sarcos</a> ]</p><div class="horizontal-rule"></div><p>Nothing new from Spot in this video, but it’s always cool to watch it go through doors.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="64ea985597168f882abec8a67e1b35b6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/gRrcYqOUIgA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotics.me.utexas.edu/">NRG</a> ]</p><div class="horizontal-rule"></div><blockquote><em>For the harvest of small tomatoes, the qb SoftHand Industry adapts naturally to the shape and gently exerts the appropriate force to be able to perform the assigned task.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f5073a4b5f65a66e35b324825fcf5ed9" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/RgTWN8uOL80?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://qbrobotics.com/product/qb-softhand-industry/">QB Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>The next generation of LoCoBot is here. The LoCoBot is a ROS research rover for mapping, navigation, and manipulation (optional) that enables researchers, educators, and students alike to focus on high-level code development instead of hardware and building out lower-level code.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9f003d6c723b1ecacbeeecfe53485ec8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/HW8RRbBEO7Y?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.trossenrobotics.com/locobot-wx250.aspx">Trossen</a> ]</p><div class="horizontal-rule"></div><p>If you’re at IROS and have some spare time in Tokyo, the Maid Robot Cafe is now open in Akihabara.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="0ba68e9afc0fd5b0a8814b4c52a18ea1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3JoxgdLsh5g?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotstart.info/2022/10/07/masiro-maidcafe.html">RobotStart</a> ]</p><div class="horizontal-rule"></div><p>This week’s CMU RI Seminar is from Ankur Mehta at UCLA, on “Towards $1 Robots.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="29ba06f21a24346359992b7aeeb2a28b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/slONyD2rJ9U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>Robots are pretty great—they can make some hard tasks easy, some dangerous tasks safe, or some unthinkable tasks possible.  And they’re just plain fun to boot.  But how many robots have you interacted with recently?  And where do you think that puts you compared to the rest of the world’s people? In contrast to computation, automating physical interactions continues to be limited in scope and breadth.  I’d like to change that.  But in particular, I’d like to do so in a way that’s accessible to everyone, everywhere.  In our lab, we work to lower barriers to robotics design, creation, and operation through material and mechanism design, computational tools, and mathematical analysis. We hope that with our efforts, everyone will be soon able to enjoy the benefits of robotics to work, to learn, and to play.</em></blockquote><p>[ <a href="https://www.ri.cmu.edu/event/ri-seminar-ankur-mehta-ucla-assistant-professor-samueli-fellow-2022-10-07/">CMU RI</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 14 Oct 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-venus-aerobot</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-silver-balloon-with-a-robotic-payload-drifts-high-above-a-desert-landscape.jpg?id=31944712&amp;width=980"></media:content></item><item><title>Robots and AI Could Optimize Lithium-Ion Batteries</title><link>https://spectrum.ieee.org/lithium-ion-battery</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/battery-icon-with-lightning-bolts-on-sides.jpg?id=31888618&width=1200&height=800&coordinates=0%2C80%2C0%2C80"/><br/><br/><p>Cutting-edge electronics, including electric vehicles and the latest smartphones, all depend on batteries whose chemistries are still largely developed manually by trial and error. Now, a new study reveals that artificial intelligence can direct robots in rapidly finding advanced new battery formulations. A team of scientists detailed <a href="https://www.nature.com/articles/s41467-022-32938-1" rel="noopener noreferrer" target="_blank">their findings</a> online 27 September in the journal <em>Nature</em><em> Communications</em>.</p><p>The conventional techniques for developing new batteries can take years because researchers have to experiment with many possible components. This is complicated by the need to achieve multiple competing goals, such as longer life, greater capacity, faster charging, and improved safety.</p><p>“The kind of lithium-ion battery you might find in a Tesla EV may have one primary salt—typically lithium hexafluorophosphate—as well as two or three liquid solvents in which the salt is dissolved and one or two additives that are secret,” says  Jay Whitacre, an energy technologist at Carnegie Mellon University who was co-senior author of the <em>Nature <em>Communications</em></em> paper. “There are many compelling potential combinations of all these components, potentially with multiple salts, five or six or more solvents, multiple additives, which can be incredibly complicated to rifle through.”</p><p>In the new study, researchers sought to accelerate battery development by coupling a robotics platform named Clio with an AI called Dragonfly in order to find the best combination of battery components in an autonomous manner.</p><p>“It’s like putting peanut butter and chocolate together,” Whitacre says. “I’m the experimentalist who has always wanted to find a way to mix up chemicals for batteries in an automated way,” whereas study co-senior author Venkat Viswanathan “is the computer-modeling machine-learning person who wanted to take people out of the loop.”</p><p>In the new study, the system autonomously experimented with lithium hexafluorophosphate salt and the solvents ethylene carbonate, ethyl-methyl carbonate, and dimethyl carbonate. (In a lithium-ion battery, a salt dissolves in one or more solvents to form a liquid electrolyte. Lithium ions move from one electrolyte to another to carry electric charge.)</p><p>The robotic system used pumps to inject various combinations of solvents into pouches with a lithium nickel manganese cobalt oxide cathode and a graphite anode. “There wasn’t a person telling the system what to do; the system decided what to do,” Whitacre says.</p><p>In 42 experiments over two working days, the system autonomously identified six electrolytes that enable faster charging than a conventional electrolyte composition. This approach hit upon the new chemistry six times as fast as it would likely have taken to discover it via a random search.</p><p>The researchers note their system likely performs more experimental measurements per day than an average human operator and uses about 30 percent as many lab materials. In the future, they suggest their system may prove 20 to 1,000 times as efficient as people doing this work.</p><p>The sole goal of these experiments was a faster-charging battery. However, the scientists note this system can also pursue multiple objectives simultaneously.</p><p>“As we dive more and more into this project, we’re aiming at true exploration and discovery with more complicated possible combinations of electrolytes placed into many test cells to see what does and does not work,” Whitacre says.</p>]]></description><pubDate>Tue, 11 Oct 2022 13:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/lithium-ion-battery</guid><category>Robotics</category><category>Artificial intelligence</category><category>Electric vehicles</category><category>Smartphones</category><category>Mobile devices</category><category>Electrolytes</category><category>Tesla</category><category>Lithium-ion batteries</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/battery-icon-with-lightning-bolts-on-sides.jpg?id=31888618&amp;width=980"></media:content></item><item><title>Video Friday: RobOctoberfest</title><link>https://spectrum.ieee.org/video-friday-roboctoberfest</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=31877848&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote><em>Pouring the perfect Octoberfest beer requires a humanlike sense of touch and dexterity, precision and force control. Franka Production 3 has it all!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="00a06859234674364664033c282f5499" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/jMtClHViUNQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.franka.de/">Franka Emika</a> ]</p><div class="horizontal-rule"></div><p>First time, I’m sure.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fa125eb35f188857761e438025f6d588" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/rh9d_azyg2M?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.techunited.nl/en/soccer_robots">Tech United</a> ]</p><div class="horizontal-rule"></div><blockquote><em>A team led by the University of California, San Diego, has developed a new system of algorithms that enables four-legged robots to walk and run on challenging terrain while avoiding both static and moving obstacles. The work brings researchers a step closer to building robots that can perform search-and-rescue missions or collect information in places that are too dangerous or difficult for humans.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="626e9f5956b31162610421e8fcb8ad3d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GKbTklHrq60?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://jacobsschool.ucsd.edu/news/release/3511">UCSD</a> ]</p><p>Thanks, Liezel!</p><div class="horizontal-rule"></div><p>SusGrip is a new smart gripper from Apicoo Robotics with a compact size and long stroke. The gripping motion is parallel, making it is easy and intuitive to use. An absolute encoder is equipped so users can instantly use the gripper without calibration.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="26111301b044edaae73f0a1a13a09f53" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/LW-d16ReVDM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://apicoorobotics.com/">Apicoo Robotics</a> ]</p><p>Thanks, Vo Gia Loc!</p><div class="horizontal-rule"></div><p>This is both brilliant and, somehow, terrifying.</p><blockquote style="margin-left: 20px;"></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fedbfbda892bc0efa86bf874d91b6095" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/rvyToxdR9Dc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://chiamingchang.com/gazingcarproject.html">Gazing Car</a> ]</p><div class="horizontal-rule"></div><p>Robots away!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f39630f862c2544bf644f5af6aaa2a49" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/RuJHrGR9rco?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://reconrobotics.com/">Recon Robotics</a> ]</p><div class="horizontal-rule"></div><p>Robot friends, as it turns out, are very expensive.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="52123992cb3f8e924dd6566878dfe387" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5jWduVA6nyo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://publish.illinois.edu/robodesign/">RoboDesign Lab</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We present a “Kinegami” algorithm that converts a Denavit-Hartenberg specification into a single-sheet crease pattern for an equivalent serial robot mechanism by composing origami modules from a “Tubular Origami Catalogue.”</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3a179126cb55cbb8af2afa4ce5310791" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/IT58JeMoAr0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://kodlab.seas.upenn.edu/">Kod*lab</a> ]</p><div class="horizontal-rule"></div><p>PIBOT2 makes flying a (fake) airplane look easy.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="450751aae72245f4ad37d6f732dc10c0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/uNDbZu60hgw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://unmanned.kaist.ac.kr/">USRG</a> ]</p><div class="horizontal-rule"></div><blockquote><em>GITAI USA Inc. wishes God Speed for JAXA Astronaut Dr. Koichi Wakata, Expedition 68 Crews, and SpaceX Falcon 9!!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="46a5a6c6b2fe1b4fe97e32a4028ae941" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/rZr26qsLRqE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://gitai.tech/">GITAI</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We’re excited to announce that Reachy gained mobility with a new mobile base, but also [has] improved software and hardware. Pollen Robotics’ engineers developed new features including VR teleoperation compatibility and the Lidar technology to ensure robot safety.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a690976b1f73915616e8926649de061c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/F0RPIV3n-ms?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.pollen-robotics.com/">Pollen Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>As Starship robots carry out their deliveries, they naturally might need to cross a road. The robots are equipped with a suite of sensors and cameras which are constantly understanding the environment around them. This means they only cross roads when it is deemed safe to do so. Have a look at this technology in action.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ffc6e87f52f8c6fed2e2298ab5268b1e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6MUpHBUzvJ4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.starship.xyz/">Starship</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Mauser Packaging Solutions needed a material-handling automation solution that would meet uptime and throughput requirements. By making the switch to OTTO 1500 AMRs, Mauser increased uptime, improved worker safety and enhanced throughput by 600%.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2ea1efd4109c4a25a9645acf52e81b2d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iAlRpeaZwxQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ottomotors.com/resources/videos/mauser">OTTO</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Discover the Navigation2 applications for ROS2 allowing TIAGo robot to navigate autonomously in indoor spaces and with mapping.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="da39cdfddbf4ff9b38f6c9ac59d9cbf4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/x_zBMk7s7w8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://pal-robotics.com/robots/tiago/">PALRobotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>In one of our first pilots for C.H. Robinson customers, we autonomously moved 1M+ pounds of freight for Constellation Brands from Dallas to Houston. Pilots like these will shape the future development of the Waymo Driver and expansion of our Waymo Via trucking solution.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="584ece30718af3d5794905a7b73d61be" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/t-ZpZuEmE7A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://waymo.com/">Waymo</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We have seen great advances in artificial intelligence in recent years. To HAI Distinguished Fellow Peter Norvig, as an AI practitioner and educator, this raises two questions: First, what do learners need to know about AI (and machine learning and data science) today, and how can they best learn that? Second, how can the technologies of AI be used to facilitate learning in all subjects?</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7d5c0799447217af8374fc1c4f78cb4f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/LiDHiRVVxk8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://hai.stanford.edu/events/upcoming-events">Stanford HAI</a> ]</p><div class="horizontal-rule"></div><p>This Maryland Robotics Center seminar is from UC Berkeley’s Hannah Stuart, on “Embodying Dexterity: Contact models for the design of robotic grasping and manipulation.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="355985dbcf4bb229446a3a750e616e15" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/g6OBYpMMeqg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>For robots to perform helpful manual tasks, they must be able to physically interact with the real world. In this talk, I will introduce how my lab is developing tools for “messy” or adversarial contact conditions—granular/rocky media, fluids, human interaction—to support the design of more capable systems.</em></blockquote><p>[ <a href="https://robotics.umd.edu/event/17501/mrc-seminar-embodying-dexterity-contact-models-for-the-design-of-robotic-grasping-and-manipulation">UMD</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 07 Oct 2022 15:47:51 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-roboctoberfest</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/image.png?id=31877848&amp;width=980"></media:content></item><item><title>Weaponized Robots Letter Calls for Policy, Tech Fixes</title><link>https://spectrum.ieee.org/robots-with-weapons-industry-initiative</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/boston-dynamics-spot-robot-set-against-a-mocha-colored-background-containing-logos-of-several-robotics-companies-and-the-text-of.jpg?id=31877429&width=1200&height=800&coordinates=0%2C182%2C0%2C182"/><br/><br/><p>
	Yesterday, six companies that build or support sophisticated mobile robots (led by Boston Dynamics) 
	<a href="https://www.bostondynamics.com/open-letter-opposing-weaponization-general-purpose-robots" rel="noopener noreferrer" target="_blank">published an open letter to the robotics community and industry pledging to not weaponize their general-purpose robots</a>. Signed by Agility Robotics, ANYbotics, Clearpath Robotics, Open Robotics, and Unitree, in addition to Boston Dynamics, the letter seeks to ensure that the companies’ robots are used safely and ethically, in a way that helps rather than harms.
</p><p>
	Boston Dynamics and others find themselves in an increasingly difficult position. These companies are striving to make useful, general-purpose robots, and that means making them useful for anything—even things that they should not be used for. There have been a bunch of high-profile examples of robot misuse recently (that we’re not going to link to), and the companies building the robots being misused have taken it personally—as they should, because those misused robots are very easy to identify. Plus any misperceptions that these misused robots could be autonomous (though they aren’t) makes an implied false connection between the actions of the robot and the ethics of the company.
</p><p>
<div class="rblad-ieee_in_content"></div>
</p><p>
	The letter is just one step, and not the first for any of these companies, toward friendlier, safer robots. But it’s going to take a lot of difficult, complicated steps, and there isn’t even an obvious path forward: As we heard from many of these folks a couple of years ago, <a href="https://spectrum.ieee.org/when-robots-enter-the-world-who-is-responsible-for-them" target="_self">it’s hard to maintain responsibility for robots out in the world</a>.
</p><p>
	With this complexity in mind, we spoke with <a href="https://www.linkedin.com/in/brendanschulman/" rel="noopener noreferrer" target="_blank">Brendan Schulman, vice president of policy and government relations at Boston Dynamics</a>, to better understand what this letter means.
</p><hr/><div class="ieee-sidebar-medium">
<h3>An Open Letter to the Robotics Industry and our Communities,<br/>
General Purpose Robots Should Not Be Weaponized
	</h3>
<p>
	We are some of the world’s leading companies dedicated to introducing new generations of advanced mobile robotics to society. These new generations of robots are more accessible, easier to operate, more autonomous, affordable, and adaptable than previous generations, and capable of navigating into locations previously inaccessible to automated or remotely-controlled technologies. We believe that advanced mobile robots will provide great benefit to society as co-workers in industry and companions in our homes.
	</p>
<p>
	As with any new technology offering new capabilities, the emergence of advanced mobile robots offers the possibility of misuse. Untrustworthy people could use them to invade civil rights or to threaten, harm, or intimidate others. One area of particular concern is weaponization. We believe that adding weapons to robots that are remotely or autonomously operated, widely available to the public, and capable of navigating to previously inaccessible locations where people live and work, raises new risks of harm and serious ethical issues. Weaponized applications of these newly-capable robots will also harm public trust in the technology in ways that damage the tremendous benefits they will bring to society. For these reasons, we do not support the weaponization of our advanced-mobility general-purpose robots. For those of us who have spoken on this issue in the past, and those engaging for the first time, we now feel renewed urgency in light of the increasing public concern in recent months caused by a small number of people who have visibly publicized their makeshift efforts to weaponize commercially available robots.
	</p>
<p>
	We pledge that we will not weaponize our advanced-mobility, general-purpose robots or the software we develop that enables advanced robotics and we will not support others to do so. When possible, we will carefully review our customers’ intended applications to avoid potential weaponization. We also pledge to explore the development of technological features that could mitigate or reduce these risks. To be clear, we are not taking issue with existing technologies that nations and their government agencies use to defend themselves and uphold their laws.
	</p>
<p>
	We understand that our commitment alone is not enough to fully address these risks, and therefore we call on policymakers to work with us to promote safe use of these robots and to prohibit their misuse. We also call on every organization, developer, researcher, and user in the robotics community to make similar pledges not to build, authorize, support, or enable the attachment of weaponry to such robots. We are convinced that the benefits for humanity of these technologies strongly outweigh the risk of misuse, and we are excited about a bright future in which humans and robots work side by side to tackle some of the world’s challenges.
	</p>
<p>
	Signed,
	</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="c09b5d5d4b42818a4afe2d4a883b9eba" data-rm-shortcode-name="rebelmouse-image" id="707a8" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31877427&width=980"/>
</p>
</div><p>
<strong><em>IEEE Spectrum</em>: There have been some similar letters in the past where the robotics industry has advocated against weaponizing robots. Why is this letter different?</strong>
</p><p>
<strong>Brendan Schulman: </strong>This is the first time that companies which are developing and selling general-purpose robots—that are increasingly being used in workplaces and communities—have gotten together to say something about this issue. I think that makes it distinct from other efforts that have been going on in a broader robotics context for a number of years. It’s not about military robots per se, and that’s made quite clear in the letter. We’re not taking issue with weapons systems that are already governed by an international legal framework.
</p><p class="pull-quote">“We’re calling on policymakers to do something about this issue. We acknowledge that our commitment alone is not enough to address the ethics and public-trust concerns.”<br/>
	—Brendan Schulman, Boston Dynamics
</p><p>
	The focus of the letter is on these new, widely accessible general-purpose commercial robots, where in some cases we’ve lately seen people potentially misusing them by weaponizing them. And that’s the issue for us, because that’s an ethical concern as well as the risk of a loss of public trust in robotics—that the public will begin to feel that all of these companies developing these highly advanced mobile robots are just one step away from deploying weapons in our communities, when in fact the whole point is to create robots that help people and do good things.</p><p>
<strong>Before publishing the letter signed by these six companies, did you contact anyone else? Did any companies decline to participate?</strong>
</p><p>
<strong>Schulman:</strong> These are the companies that happened to be discussing this issue for the last few months, just among themselves. We decided to get together and publish this letter after a summer of a number of prominent stunts involving people weaponizing commercially available robots. We thought it was time to say something about that.
</p><p>
	We invite everyone to join us, and we certainly welcome a broader group—not just industry, but across academia and government.
</p><p>
<strong>What do you hope this letter will accomplish?</strong>
</p><p>
<strong>Schulman: </strong>It’s important to keep in mind that we’re calling on policymakers to do something about this issue. We acknowledge that our commitment alone is not enough to address the ethics and public-trust concerns. And we really want to figure out what types of policy solutions could be put in place to address this risk, as well as what kinds of technology solutions might exist or might be developed.
</p><p>
	In the drone space, for example, where I worked previously, one of the technology-based solutions to public accountability concerns for drones was Remote ID, which targeted the anonymity of drone operations. There may be solutions like that in the general-purpose robotics space as well, and this letter is committing to exploring whether there are such solutions and how to develop and implement them. But it’s an open question.
</p><p>
	I would like to think that we’ve gone a step further with this letter, both in bringing together the broader industry, but also really reaching out to policymakers as well as technologists to say, “work with us to help solve these challenges.”
</p><p><div class="rblad-ieee_in_content"></div></p><p>
<strong>The letter focuses on “weaponized” robots. What does “weaponized” mean, exactly?</strong>
</p><p>
<strong>Schulman: </strong>This is part of the discussion we’ve been having in the industry and with other stakeholders.
</p><p>
	One of the examples that has come up in discussions is about a year ago, 
	<a href="https://dronedj.com/2021/10/12/video-shows-a-peruvian-cop-drone-rescuing-an-sol-pigeon/" rel="noopener noreferrer" target="_blank">a drone in Peru was equipped with a knife and used to free a pigeon that got tangled in power lines</a>. That sounds like a weaponized robot, but it was used for a beneficial purpose, and I don’t think any of us would object to that. So there’s a nuanced and detailed discussion to be had, and some of us are beginning to have that conversation, and we hope to continue it with policymakers: how do we define weapon, and how do we define what a robot is in terms of the capabilities that we are concerned about when it comes to misuse.
</p><p>
	Our goal is to arrive at something more specific than just this initial letter, which is really just the beginning of the conversation. A lot of these definitional questions and parameters for what kinds of weapons and robots would be included in a policy-based prohibition is exactly the discussion we want everyone to have, including the broader robotics community, academia, government, civil-rights stakeholders, and law-enforcement agencies. This is a discussion that doesn’t start or end with industry alone.
</p><p class="pull-quote">“What doesn’t exist is a framework for how advanced commercial robots are being used in society, and how we might impose limitations on their misuse, specifically weaponization.”<br/>
	—Brendan Schulman, Boston Dynamics
</p><p>
	For Boston Dynamics, I can say that we prohibit and would condemn any device on the robot that is designed to harm humans. I think that’s a pretty broad category; obviously there are weapons known as “less-lethal” weapons, but those are still designed to harm or incapacitate. And our company would prohibit those. When you start talking about devices that could be used to harm someone, but that are really designed to do something else, then you start to have a more difficult line to draw. But I think in this early era of advanced mobile robots, it will be pretty clear.
	<br/>
</p><p>
<strong>The letter is very specific that it does not support the weaponization of general-purpose robots. Does that leave companies that sign this letter free to develop robots for other purposes that may include weaponization?</strong>
</p><p>
<strong>Schulman: </strong>What we’re focusing on with this coalition are the kinds of robots that are more affordable, adaptable, accessible, and easy to operate. That is a new thing, and that’s the kind of new thing that we have increasingly seen raising concerns among members of the public, among government stakeholders, within the media, and within the robotics and academic communities as well.
</p><p>
	What’s not new is that militaries have long had what could be described as weaponized mobile robots. Those weapons systems are not what the letter is about. If a defense contractor is developing a weapons system for the military that is to be used by the military under existing legal doctrines and conventions on warfare, and it’s subject to those safeguards rather than out there for public purchase and use, that’s absolutely in a different category—that whole development, procurement, and use process exists under a framework that’s been developed for decades, and certainly there are ongoing ethics discussions in that context too. What doesn’t exist is a framework for how advanced commercial robots are being used in society, and how we might impose limitations on their misuse, specifically weaponization.
</p><p>
	But, I think there needs to be a nuanced discussion about what kinds of constraints we might ask policymakers to impose, and what we do about products that potentially cross over from civilian to military use. It’s a complicated question that needs to be discussed more broadly.
</p><p><div class="rblad-ieee_in_content"></div></p><p>
<strong>What happens next? How do you hope technology and policy can be leveraged to make progress toward the goals of this letter?</strong>
</p><p>
<strong>Schulman: </strong>Let’s start with technology. This is a call for the exploration of potential solutions, which could be anything from a remote identification and accountability mechanism, as we’ve seen lately on consumer aerial robots, to things like data logging or payload detection or perhaps remote disabling technologies. It’s something that we are starting to explore, and we very much invite ideas for other technology-based solutions that might help address this concern.
</p><p>
	Policymakers are likely to focus on proscribed conduct, which would potentially prohibit the weaponization of a robot. You’d have to define what kinds of weapons, and what kinds of robotic technologies these prohibitions would apply to, so it would require some thoughtfulness. There have been bills introduced over the years that have prohibited weaponized drones, and I think that other robots could follow a similar pattern in that over time; policymakers and other stakeholders figure out what kind of conduct is the real concern, and what legal provisions would prohibit that.
</p><p>
	We’d love to continue discussing these issues with policymakers. Again, this is really just the beginning, and in some cases, members of this coalition have been in discussions with policymakers about this for a while. I think the letter demonstrates that there is a broad sentiment by the advanced mobile robotic industry, across three continents, that something should be done. But certainly the robotics community is far broader than just us, and it would be great to hear more ideas. I hope the letter will spur these additional conversations and perhaps speed up the process of trying to reach solutions.
</p>]]></description><pubDate>Fri, 07 Oct 2022 15:39:57 +0000</pubDate><guid>https://spectrum.ieee.org/robots-with-weapons-industry-initiative</guid><category>Boston dynamics</category><category>Weaponized robots</category><category>Legged robots</category><category>Agility robotics</category><category>Unitree</category><category>Open robotics</category><category>Clearpath robotics</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/boston-dynamics-spot-robot-set-against-a-mocha-colored-background-containing-logos-of-several-robotics-companies-and-the-text-of.jpg?id=31877429&amp;width=980"></media:content></item><item><title>IHMC’s Nadia Is a Versatile Humanoid Teammate</title><link>https://spectrum.ieee.org/ihmc-nadia-humanoid-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/several-different-views-of-a-headless-black-humanoid-robot-demonstrating-its-flexibility.jpg?id=31870017&width=1200&height=800&coordinates=0%2C208%2C0%2C209"/><br/><br/><p><a href="http://robots.ihmc.us/" target="_blank">The Florida Institute for Human & Machine Cognition (IHMC)</a> is well known in bipedal robotics circles for teaching very complex humanoid robots to walk. Since 2015, IHMC has been home to a <a href="https://spectrum.ieee.org/atlas-drc-robot-is-75-percent-new-completely-unplugged" target="_blank">Boston Dynamics Atlas (the DRC version)</a> as well as a <a href="https://spectrum.ieee.org/update-nasa-valkyrie-robot" target="_blank">NASA Valkyrie</a>, and significant progress has been made on advancing these platforms toward reliable mobility and manipulation. But fundamentally, we’re talking about some very old hardware here. And there just aren’t a lot of good replacement options (available to researchers, anyway) when it comes to humanoids with human-comparable strength, speed, and flexibility.</p><p>Several years ago, IHMC decided that it was high time to build its own robot from scratch, and in 2019, <a href="https://spectrum.ieee.org/ihmc-developing-new-gymnastinspired-humanoid-robot" target="_blank">we saw some very cool plastic concepts of Nadia</a>—a humanoid designed from the ground up to perform useful tasks at human speed in human environments. After 16 (!) experimental plastic versions, <a href="http://robots.ihmc.us/nadia" target="_blank">Nadia is now a real robot</a>, and it already looks pretty impressive.</p><hr/><p class="pull-quote">For a long time, we’ve been pushing up against the limits of the hardware with DRC Atlas and Valkyrie. We haven’t been able to fully explore the limits of our software and our controls. With Nadia, it’s the other way around, which is really exciting from a research perspective. <br/>—<a href="https://www.linkedin.com/in/rjgriffin42/" rel="noopener noreferrer" target="_blank">Robert Griffin, IHMC</a></p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="ded99ccad6a183902e3d0a76c9a33d67" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/_mQJw8VhZ7w?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>Designed to be essentially the next generation of the DRC Atlas and Valkyrie, Nadia is faster, more flexible, and robust enough to make an excellent research platform. It’s a hybrid of electric and hydraulic actuators: 7 degrees-of-freedom (DoF) electric arms and a 3 DoF electric pelvis, coupled with a 2 DoF hydraulic torso and 5 DoF hydraulic legs. The hydraulics are integrated smart actuators, which we’ve <a href="https://spectrum.ieee.org/ihmc-developing-new-gymnastinspired-humanoid-robot" target="_blank">covered in the past</a>. Nadia’s joints have been arranged to maximize range of motion, meaning that it has a dense manipulation workspace in front of itself (where it really matters) as well as highly mobile legs. Carbon fiber shells covering most of the robot allow for safe contact with the environment. </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two images show a human using a virtual reality system and controllers to control the Nadia robot in manipulating small objects on a table." class="rm-shortcode" data-rm-shortcode-id="b7c8c12071226dc7368ab7b51753a16b" data-rm-shortcode-name="rebelmouse-image" id="7f53a" loading="lazy" src="https://spectrum.ieee.org/media-library/two-images-show-a-human-using-a-virtual-reality-system-and-controllers-to-control-the-nadia-robot-in-manipulating-small-objects.jpg?id=31863530&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Teleoperation through immersive virtual reality helps Nadia perform complex tasks.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IHMC</small></p><p>That large manipulation workspace is particularly relevant because there’s a lot of emphasis on immersive virtual reality teleoperation with Nadia. Not that autonomy isn’t important, but the concept behind Nadia has been shaped (at least in part) by IHMC’s experience at the <a href="http://robots.ihmc.us/drc" target="_blank">DARPA Robotics Challenge Finals</a>: Ideally, you want the robot to manage everything it can on its own, while having a human able to take more direct control over complex tasks. While it’s not the fantasy of a robot being fully autonomous, the fact is that in the near term, this approach is an immediately viable way of getting robots to reliably do useful things in unstructured environments. Overall, the goal with Nadia is to operate as close to human speed as possible when doing urban exploration and manipulation tasks. And if that involves a human directly teleoperating the robot because that’s the best way of doing things, Nadia is designed to handle it.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two humanoid robots stand next to each other; Nadia on the left is crossing its legs much more than DRC Atlas on the right" class="rm-shortcode" data-rm-shortcode-id="099b13188cff4db8740a5c9649e448f5" data-rm-shortcode-name="rebelmouse-image" id="4710b" loading="lazy" src="https://spectrum.ieee.org/media-library/two-humanoid-robots-stand-next-to-each-other-nadia-on-the-left-is-crossing-its-legs-much-more-than-drc-atlas-on-the-right.jpg?id=31863522&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Nadia demonstrates how much more flexible its hips and legs are relative to DRC Atlas.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IHMC</small></p><p>For more details about Nadia, we spoke with <a href="https://www.linkedin.com/in/rjgriffin42/" target="_blank">Robert Griffin</a>, a research scientist at IHMC.</p><p><strong></strong><strong>How is Nadia unique among humanoid robots?</strong><br/></p><p><strong>Robert Griffin:</strong> I think the big thing that we’re trying to bring to the table with Nadia is the really high range of motion of a lot of the joints. And it’s not just the range of motion that differentiates Nadia from many other humanoid robots out there, it’s also speed and power. Nadia has much better power-to-weight than the DRC Atlas, making it significantly faster, which improves its general operational speed as well as its stability.</p><p><strong>Can you put your goals with Nadia into context in the humanoid robotics space?</strong></p><p><strong>Griffin: </strong>We’re trying to make Nadia a robot that can function in unstructured environments where you don’t know what it needs to do ahead of time. So, instead of having a set sequence or number of tasks, we’re trying to build up a robot in hardware, software, and interfaces that enable a human-machine team to go into an unknown environment, explore it, map it, and then do meaningful tasks.</p><p><strong>IHMC has a reputation for helping complex humanoid robots like DRC Atlas and Valkyrie with their mobility. How has all of that experience made its way into Nadia’s design?</strong></p><p><strong>Griffin: </strong>You’re right, and we’ve been partnering with <a href="https://www.nasa.gov/centers/johnson/home/index.html" target="_blank">NASA JSC</a> since the DRC—when you see Valkyrie walking now, it’s using IHMC software and controllers. When it comes to applying some of that knowledge to Nadia, we’ve paid special attention to things like Nadia’s range of motion in its legs. Unlike a lot of humanoid platforms, Nadia has really good hip roll (or ab/adduction), which means that it can do really wide steps and also really narrow steps where the feet overlap each other. We think that’s going to help increase mobility by helping with step adjustment and reaching challenging footholds.</p><p>Beyond the hardware, we were also able to use our simulation tools and control stack to really help us specify what the hardware design needed to be in order to accomplish some of these tasks. We weren’t able to optimize all of the joints on the robot—we pretty much stuck with a <a href="https://spectrum.ieee.org/ihmc-developing-new-gymnastinspired-humanoid-robot" target="_self">single hydraulic piston</a> in the legs, for now, but we hope that our future designs will be better integrated and optimized. We do have full ankle actuation, unlike a lot of humanoid robots. Our approach, from the beginning, was to look for the necessary control authority with Nadia’s feet to be able to do precise foot placement, and use that for relatively quick reactive mobility.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-float-right rm-resized-container-35" data-rm-resized-container="35%" style="float: right;">
<img alt="A man uses a boxing glove on the end of a wooden stick to shove a humanoid robot" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="d63a32cdd1fc88a13a9f5049b17f8c1d" data-rm-shortcode-name="rebelmouse-image" id="a1a7c" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-uses-a-boxing-glove-on-the-end-of-a-wooden-stick-to-shove-a-humanoid-robot.jpg?id=31863517&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Robert Griffin gives Nadia a helpful shove, for science.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">IHMC</small></p><p><strong>As you went from concept to hardware with Nadia, what compromises did you have to make?</strong></p><p><strong>Griffin: </strong>We’re not quite as flexible as a typical human, although we’re not very far off. I was surprised that we lost some range of motion in the mechanisms just because of the hydraulic hose routing, especially around the hips, where we wanted to avoid having big loops of hoses hanging off the robot. We eventually want to take Nadia into the real world, and we don’t want the hoses getting snagged on things.</p><p>Nadia is not quite as fast as a human in some of the joints, and in some of the joints it’s faster. We’re also not quite as compact as we wanted to be originally, because, well, robots are really hard. We’re hoping to get it a little more compact in the future. And we missed our weight margin by a little bit—it’s about 90 kilograms right now, without the hydraulic pump or battery on board. </p><p><strong>Why did you decide to focus on how a human can directly control Nadia in VR, as opposed to more emphasis on autonomy?</strong></p><p><strong>Griffin: </strong>Autonomy works really, really well when you know what the operating conditions are, and what you’re trying to accomplish. Not that it’s easy to automate that, but it’s feasible. We’ve experimented with some autonomous planning and manipulation, and we found that it’s a little limiting in that it doesn’t take advantage of the full capabilities of our platform. And so what we’re looking at doing is trying to automate just key parts of the system—things like footstep planning and grasp planning, which can be automated pretty well. But we also need the ability to interact with novel environments, which is why we’re focusing on developing ways to team with humans through interfaces like virtual reality. With the cognitive capabilities of a human combined with the manipulation capabilities of the robot, we believe you get a ton of versatility. </p><p><strong>How much of a difference has it made to controlling Nadia in VR, that the hardware has humanlike flexibility and a humanlike workspace?</strong></p><p><strong>Griffin: </strong>For manipulation, it’s been huge. We don’t want the user to be operating the robot and feeling like they’re limited in what they can do because of the robot’s constraints. A lot of the manipulation that we’ve been able to do with Nadia we could never have done with our other platforms.</p><p>There are definitely trade-offs—the arms are more designed for speed than for strength, which is going to be a little bit limiting, but we’re hoping to improve that long-term. I think that designing the robot—and especially the control system and software architecture—with the idea of having a human operating it has opened up a lot of capability that we hope will let us rapidly iterate along many different paths.</p><p><strong>What are some things that Nadia will be uniquely capable of?</strong></p><p><strong>Griffin: </strong>We’re hoping that the robot will be uniquely capable of multicontact locomotion—that’s a direction that we’re really hoping to move in, where we’re not just relying on our feet to move through the world. But I don’t necessarily want to say that it’ll be better at that than other robots are, because I don’t know everything that other robots can do. </p><p><strong>Are there practical tasks that you hope Nadia will be able to do at some point in the future?</strong></p><p><strong>Griffin: </strong>Things like disaster response, nuclear remediation, bomb disposal—these tasks are largely done by people right now, because we don’t have robots that are actually capable of doing them adequately despite the amount of time that roboticists have been working on it. I think that by partnering excellent robotic engineering with really incredible operating interfaces and human cognition, these tasks will start to become feasible to do by a robot.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A headless humanoid robot with outstretched arms stands in a lab; on its torso are stickers forming a comical face " class="rm-shortcode" data-rm-shortcode-id="9809398f42490f67b37042f802b7cf74" data-rm-shortcode-name="rebelmouse-image" id="0aedf" loading="lazy" src="https://spectrum.ieee.org/media-library/a-headless-humanoid-robot-with-outstretched-arms-stands-in-a-lab-on-its-torso-are-stickers-forming-a-comical-face.jpg?id=31863505&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Who needs a real head when you have googly eyes and a shark mouth?</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IHMC</small></p><p>Even though Nadia is brand new, IHMC already has a bunch of upgrades planned for the robot. The most obvious one is cutting that tether by putting batteries and a custom hydraulic pump into Nadia’s torso. Also on the road map are higher torque arms and lighter, more streamlined legs. In the meantime, the software has a lot of catching up to do to allow Nadia to reach its full potential. And if we know IHMC, that’s going to be an incredibly exciting thing to see.</p>]]></description><pubDate>Wed, 05 Oct 2022 19:16:52 +0000</pubDate><guid>https://spectrum.ieee.org/ihmc-nadia-humanoid-robot</guid><category>Humanoid robots</category><category>Ihmc</category><category>Bipedal robots</category><category>Nadia</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/several-different-views-of-a-headless-black-humanoid-robot-demonstrating-its-flexibility.jpg?id=31870017&amp;width=980"></media:content></item><item><title>What Robotics Experts Think of Tesla’s Optimus Robot</title><link>https://spectrum.ieee.org/robotics-experts-tesla-bot-optimus</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/tesla-s-optimus-robot-waves-at-audience-from-the-stage.jpg?id=31857078&width=1200&height=800&coordinates=0%2C37%2C0%2C38"/><br/><br/><p>Last Friday, 30 September, <a href="https://spectrum.ieee.org/tesla-optimus-robot" target="_blank">Tesla introduced several prototypes of its new humanoid robot, Optimus</a>. After a year of speculation based on little more than <a href="https://spectrum.ieee.org/elon-musk-robot" target="_blank">a person in a robot suit combined with some optimistic assertions made by Tesla CEO Elon Musk</a>, many roboticists tuned in to the event live stream (or attended in person) to see what Tesla’s approach to humanoid robotics would turn out to be.</p><p>Reactions across the robotics community were diverse. Because robotics requires expertise in many different aspects of both software and hardware, getting a good sense of the present context of Tesla’s robot as well as its future potential means finding perspectives from a multitude of robotics experts, including people working in industry and academia and everywhere in between. And by scouring the Internet over the weekend, we found as many expert commenters as we could. Together, they offer the most detailed and nuanced understanding of Optimus we’re likely to get outside of Tesla itself.</p><hr/><p class="rm-anchors" id="top"> These robotics experts posted their thoughts on Twitter, LinkedIn, Substack, and elsewhere, and with their permission, we’ve collected them for you below. Many of these folks wrote a lot more than we have room to include, but you can follow the link in a particular expert’s name to see their complete commentary.</p><h3>Experts on Optimus</h3><br/><ul><li><a href="#georgia-chalvatzaki">Georgia Chalvatzaki</a> — Assistant Professor, Technische Universität Darmstadt</li><li><a href="#kate-darling">Kate Darling</a> — Research Specialist, MIT Media Lab</li><li><a href="#animesh-garg">Animesh Garg</a> — Assistant Professor, University of Toronto</li><li><a href="https://spectrum.ieee.org/r/entryeditor/2658380182#animesh-garg" target="_self"></a><a href="https://spectrum.ieee.org/r/entryeditor/2658380182#animesh-garg" target="_self"></a><a href="#ryan-gariepy">Ryan Gariepy</a> — CTO, Clearpath Robotics & OTTO Motors</li><li><a href="#keerthana-gopalakrishnan">Keerthana Gopalakrishnan</a> — Roboticist, Google Brain</li><li><a href="https://spectrum.ieee.org/r/entryeditor/2658380182#keerthana-gopalakrishnan" target="_self"></a><a href="#dennis-hong">Dennis Hong</a> — Professor, UCLA</li><li><a href="#christian-hubicki">Christian Hubicki</a> — Assistant Professor, Florida State University</li></ul><h3></h3><br/><ul><li><a href="#will-jackson">Will Jackson</a> — Founder and CEO, Engineered Arts</li><li><a href="#gary-marcus">Gary Marcus</a> — Author, <em>Rebooting AI</em></li><li><a href="#brandon-rohrer">Brandon Rohrer</a> — Machine Learning Engineer, LinkedIn</li><li><a href="#siddhartha-srinivasa">Siddhartha Srinivasa</a> — Director of Robotics and AI, Amazon</li><li><a href="#mikell-taylor">Mikell Taylor</a> — Principal Technical Program Manager, Amazon Robotics</li><li><a href="#cynthia-yeung">Cynthia Yeung</a> — Founding Board Member, Women in Robotics</li></ul><h3></h3><br/><p>
	Click the link under each section to jump back up to the list of experts.</p><p>
<br/>
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Georgia Chalvatzaki" class="rm-shortcode" data-rm-shortcode-id="d9ae0cc979b5793242204a6da7df6c63" data-rm-shortcode-name="rebelmouse-image" id="4bca5" loading="lazy" src="https://spectrum.ieee.org/media-library/georgia-chalvatzaki.jpg?id=31856994&width=980"/><h3 class="rm-anchors" id="georgia-chalvatzaki"><a href="https://twitter.com/georgiachal/status/1576161102110420992" rel="noopener noreferrer" target="_blank">Georgia Chalvatzaki<br/>
</a>Assistant Professor, Technische Universität Darmstadt</h3><p>
	Looking at the Tesla Bot as a roboticist, I am impressed by what the engineers achieved for this prototype in a year. However, the behaviors demonstrated are less impressive than that of Honda’s Asimo from 20 years ago.
</p><p>
	What excites me is the idea of cheap and accessible hardware! The electric motors with the battery support could make a very good tool for academic research. It takes way more to solve manipulation, but Academia looks forward to getting your hardware!
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Kate Darling" class="rm-shortcode" data-rm-shortcode-id="e521d4d82ad7a54e907c33e5beffbe6c" data-rm-shortcode-name="rebelmouse-image" id="4bb67" loading="lazy" src="https://spectrum.ieee.org/media-library/kate-darling.jpg?id=31857019&width=980"/><h3 class="rm-anchors" id="kate-darling"><a href="https://twitter.com/grok_/status/1576266024261128192" rel="noopener noreferrer" target="_blank">Kate Darling<br/>
</a>Research Specialist, MIT Media Lab</h3><p>
	The sense I’m getting is that Optimus isn’t as bad as people thought, but also, nobody [in the robotics community] is very impressed or surprised by any of the tech.
</p><p class="caption">
	[
	<a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Animesh Garg" class="rm-shortcode" data-rm-shortcode-id="aa2820305a095e6496cef6b119d16b1d" data-rm-shortcode-name="rebelmouse-image" id="6a5f7" loading="lazy" src="https://spectrum.ieee.org/media-library/animesh-garg.jpg?id=31857020&width=980"/><h3 class="rm-anchors" id="animesh-garg"><a href="https://animeshgarg.substack.com/p/tesla-ai-day-2022-optimus" target="_blank">Animesh Garg</a><br/>
Assistant Professor, University of Toronto</h3><p>
	The Optimus effort is, in a sense, no different from 
	<a href="https://asimo.honda.com/" rel="noopener noreferrer" target="_blank">Asimo</a>, <a href="https://www.bostondynamics.com/atlas" rel="noopener noreferrer" target="_blank">Atlas</a>, <a href="https://global.toyota/en/detail/19666346" rel="noopener noreferrer" target="_blank">T-HR3</a>, or <a href="https://agilityrobotics.com/news/2019/meet-digit-the-newest-robot-from-agility-robotics" rel="noopener noreferrer" target="_blank">Digit</a>. All of them are backed by expert roboticists with years of design experience. However, the Tesla Robotics team has done a commendable job in fast iteration time for the design of the robot from the ground up, [following the path from] a good experimental study on actuator requirements, to designing new actuators, then creating the integrated system. The current locomotion stack is not using any machine learning, but rather trajectory optimization using reference controllers. This is a sensible design choice to get up and running, but would definitely need redoing for longer-term operation.
</p><p>
	The hand also seems exciting, with a metallic cable-driven system with four fingers and a thumb. In the brief demo, they showed it had a reasonably high loading capacity (holding plant-watering water cans, and lifting bars of aluminum in the factory). However, due to the cable-driven design, the system will have slower response time, harder to do learning-based control, and no backdrivability in autonomous mode. This will make general-purpose autonomous manipulation slightly challenging.
</p><p>
	Robots, in the short term, have use cases in places where they do not necessarily need to interact with humans and the implications of failure are less severe. The community needs to find a revenue-positive pathway to support this development. And this could come from behind-the-scenes use cases for robot manipulation, in warehouses, retail stores, food preparation, and manufacturing. While automation-based solutions are still being pursued, it remains to be seen how a general-purpose hardware-based solution would stack up. We would need to look to low-volume, high-variability products which require quick adaptation.
</p><p>
	Tesla would benefit a lot from collaborating with the community. Tesla already has the community taking notice, and by being more open, Elon would only empower more people to work on the problem—which is, after all, “beneficial to humanity” —which Elon claims is their driving principle. Designing the whole stack of hardware, simulation, and data infrastructure is requiring Tesla to reinvent the wheel on many fronts.
</p><p>
	Overall, the current design is a very good first step. Interest in building such systems is welcome because Tesla and Elon Musk’s involvement in the problem brings attention, talent, and resources to the problem, setting in motion a flywheel of progress. This effort should be lauded with cautious optimism by the community, for the compass points in the right direction, and Elon brings with him the heft of Tesla engineers as we trek through the AI/Robotics jungle.
</p><p class="caption">
	[
	<a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br/><img alt="Ryan Gariepy" class="rm-shortcode" data-rm-shortcode-id="2df41b550b8220d431718c848d2168bf" data-rm-shortcode-name="rebelmouse-image" id="5c7d6" loading="lazy" src="https://spectrum.ieee.org/media-library/ryan-gariepy.jpg?id=31857021&width=980"/><h3 class="rm-anchors" id="ryan-gariepy"><a href="https://www.linkedin.com/feed/update/urn:li:activity:6982004775533887488/" rel="noopener noreferrer" target="_blank">Ryan Gariepy<br/>
</a>CTO, Clearpath Robotics & OTTO Motors</h3><p>
	Great credit to the engineering team who pulled it off, of course, but I’m not seeing anything particularly impressive here which we can attribute specifically to Elon or Tesla. Specifically, there were lots of arguments ahead of the unveil that one or more of the “Tesla FSD stack/data/EV experience” was going to let them leapfrog all of the other companies in the space and I didn’t see any of that.
</p><p>
	There’s also lots of credit going to Elon for saying that “it’s going to cost 20K,” which reminds me of all the 3D lidar companies that have been saying “our lidar will only cost [US] $100...if you pre-order 100K+ units.”</p><p>
	In short, I’d bet that any decent university or corporate robotics lab with a similar budget and an active PR team would be able to pull this off.
</p><p class="caption">
	[
	<a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Keerthana Gopalakrishnan" class="rm-shortcode" data-rm-shortcode-id="074d7da16678affa07f40465058f291b" data-rm-shortcode-name="rebelmouse-image" id="fa515" loading="lazy" src="https://spectrum.ieee.org/media-library/keerthana-gopalakrishnan.jpg?id=31857023&width=980"/><h3 class="rm-anchors" id="keerthana-gopalakrishnan"><a href="https://twitter.com/keerthanpg/status/1576227115761926144?s=46&t=10KkYNDyNatBBM64f63arw" rel="noopener noreferrer" target="_blank">Keerthana Gopalakrishnan<br/>
</a>Roboticist, Google Brain</h3><p>
	Optimus reveal: Mind blown with the velocity of the team and the very sleek hardware design elements. Yet to see autonomy. Surprised Tesla went full Boston Dynamics mode with classical control/planning when it’s been around for a while…
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Dennis Hong" class="rm-shortcode" data-rm-shortcode-id="b60b346912293cfae8bf24c528eb8d1b" data-rm-shortcode-name="rebelmouse-image" id="5c1a8" loading="lazy" src="https://spectrum.ieee.org/media-library/dennis-hong.jpg?id=31857024&width=980"/><h3 class="rm-anchors" id="dennis-hong"><a href="https://twitter.com/DennisHongRobot/status/1576304407389487105" rel="noopener noreferrer" target="_blank">Dennis Hong<br/>
</a>Professor, UCLA</h3><p>
	The energy and excitement at AI Day 2 was amazing. “AI Day” is actually a recruitment event, and in that sense I believe the event was a big success.
</p><p>
	I am aware of critics who say that the prototype had nothing new that they haven’t seen elsewhere, and that there are other, more impressive humanoids. There are also people who have doubts about the aggressive timeline Elon had proposed, and I do not necessarily disagree with them.
</p><p>
	That being said, I am a true believer of the future with humanoid robots and their eventual applications; that they will be used in our everyday lives “one day” and make our lives better. And for that to happen, we need to start somewhere and project Optimus is just that.
</p><p>
	What was most impressive to me was what the Optimus team was able to accomplish in such a short period of time. If you are in this field, you would agree, too. The prototype they have created will serve as an excellent beginning platform for them to learn from and to build upon.
</p><p>
	I would say this is their good first step towards something big—if Tesla truly commits to put its resources, time, and efforts into it long term. The company has great engineers, and with the newly recruited talents, I am even more excited to see what they’ll be able to accomplish next.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br/><img alt="Christian Hubicki" class="rm-shortcode" data-rm-shortcode-id="fecc0ce40b91f2e1955cdc715d381f4c" data-rm-shortcode-name="rebelmouse-image" id="a23e0" loading="lazy" src="https://spectrum.ieee.org/media-library/christian-hubicki.jpg?id=31857025&width=980"/><h3 class="rm-anchors" id="christian-hubicki"><a href="https://twitter.com/chubicki/status/1576087846405828608" rel="noopener noreferrer" target="_blank">Christian Hubicki<br/>
</a>Assistant Professor, Florida State University</h3><p>
	Am I blown away? No. Am I laughing? No.
</p><p>
	First, the team did a good job. They came a long way in about a year(?), going from zero-to-robot from the ground up. Also, doing a live demo without a tether (safety-catch rope) is braver than people know.
</p><p>
	Let’s talk walking. I told my lab today that I expected it to walk on stage, and it did, but I was a bit surprised at how they did it. It <em>seems</em> to use a method called zero-moment point to maintain balance. It’s been used in various forms since the 1990s. You see ZMP in the bent knees and how it shifts its weight over to its next foot before taking a step. It’s pretty safe, but not mind-blowing in 2022. Also, people don’t walk quite like this. We are more efficient—we stick our foot out, fall, catch, repeat.
</p><p>
	There’s a portion showing off the vision, manipulation, and “smarts.” It’s hard for me to glean the methods here because it’s a cut video, so I don’t have much to say. I don’t know how much is pre-canned vs. online planning. I will also let everyone in on a secret about humanoids: It’s all about reliability. How often does it fall down? You can’t tell from a cool video—or even a live demo.
</p><p>
	Musk mentioned a [US] $20K price point, which would be a big deal. The cheapest human-size humanoids I’m aware of are in the $150K range (a long way from the olden days of $1M price tags)… But I’ll believe the price when I can actually buy one.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Will Jackson" class="rm-shortcode" data-rm-shortcode-id="3a61e9fb33d7c013026db50c5437929a" data-rm-shortcode-name="rebelmouse-image" id="e48c2" loading="lazy" src="https://spectrum.ieee.org/media-library/will-jackson.jpg?id=31857027&width=980"/><h3 class="rm-anchors" id="will-jackson"><a href="https://www.linkedin.com/feed/update/urn:li:activity:6981994999450116096/" rel="noopener noreferrer" target="_blank">Will Jackson<br/>
</a>Founder and CEO, Engineered Arts</h3><p>[I was] fortunate enough to have been at the Optimus unveil in Palo Alto last night. Had a chance to check out the design and talk with many of the engineers involved.<br/></p><p>It’s generally an old-school series chain of actuators, excepting wrists and ankles, which are differential roll/pitch. Nothing novel in the kinematics. No mechanical energy storage, parallel springs, etc.—it’s not going to be efficient unless they change that.<br/></p><p>Two main classes [of actuators]: rotary and linear. Rotary is an integrated strain wave gear reduction (harmonic drive). Linear actuators are more interesting, integrated inverted roller screw drive. Playing with a drive, it’s not inherently very transparent—you certainly wouldn’t get a free swinging knee or hip joint. All the actuators look like they need and use active force loops. This looks nasty to me: It will complicate the control, reduce efficiency, and raise complexity. If they really want to get to [US] $20,000 a unit, this is not the way to go.<br/></p><p>Hands: One novel feature here is a clutch on the finger flex/extend. Playing with an actual hand, it felt like it worked quite nicely to decouple the finger from the drive; this will have advantages. The design of the hands leaves very little room for a compliant (soft) layer, and bare metal hands are a terrible idea—try picking up a glass of water with two metal spoons and you will know what I mean. No finger ab/adduction, only two DOFs in the thumb—no chance of doing anything human-level dexterous here.<br/></p><p>[Tesla has] a large team of highly capable engineers and will iterate quickly to better designs—if they can find a leader for the mechanical architecture with better ideas than they are currently showing.<br/></p><p>The elephant in the room is the application ideas, which are frankly ridiculous. I am amazed that Musk can address an audience so rapturously enamored with the idea of a humanoid and totally fail to recognize that their desire to interact with a robot is the killer application. Did he think they were applauding because finally the world will have a humanoid robot that can lift a pipe in a car factory?<br/></p><p>Summary: An extraordinarily brave live demonstration of a herculean effort that sadly lacks novelty and imagination. Hopefully we will see a course correction by the time of next year’s event.</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Gary Marcus" class="rm-shortcode" data-rm-shortcode-id="29900e925735192e44986110dbc4b51b" data-rm-shortcode-name="rebelmouse-image" id="65a41" loading="lazy" src="https://spectrum.ieee.org/media-library/gary-marcus.jpg?id=31857028&width=980"/><h3 class="rm-anchors" id="gary-marcus"><a href="https://garymarcus.substack.com/p/sub-optimal" rel="noopener noreferrer" target="_blank">Gary Marcus<br/>
</a>Author, <em>Rebooting AI</em></h3><p>
	The Optimus demo turned out to be a bit of a dud.
</p><p>
	The challenge for Tesla isn’t really so much the mere fact that Boston Dynamics robots are ahead (or that Agility Robotics is also doing similar work). With enough investment, Tesla might 
	<em>in principle</em> catch up. If Musk really wants to win the robotics race, he has the resources to do so. (Though he clearly has not invested nearly enough so far.)
</p><p>
	What I didn’t see last night was 
	<em>vision</em>.
</p><p>
	I mean this in two different senses.
</p><p>
	First, there was no clearly outlined vision for what Optimus would do, nor much justification for why Tesla is building the robot in this specific way. There was no decisive justification for why to use a humanoid robot (rather than, for example, just an arm), no clarity about the first big application, no clear go-to-market strategy, and no clear product differentiator.
</p><p>
	Second, there was very little vision for how Tesla would build the 
	<em>cognitive</em> part of the AI they will need, beyond the basics of motor control (which Boston Dynamics already does so well), nor much recognition about <em>why</em> robotics is so hard in the real world.
</p><p>
	For me, the most worrisome part of last night’s presentation was not the lack of a world-beating demo, but a lack of recognition of what would even be required.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br><img alt="Brandon Rohrer" class="rm-shortcode" data-rm-shortcode-id="abada70213d5c3b1abefb5ba5a9a6d2b" data-rm-shortcode-name="rebelmouse-image" id="9a726" loading="lazy" src="https://spectrum.ieee.org/media-library/brandon-rohrer.jpg?id=31857029&width=980"/><h3 class="rm-anchors" id="brandon-rohrer"><a href="https://twitter.com/_brohrer_/status/1576368293719736320" rel="noopener noreferrer" target="_blank">Brandon Rohrer<br/>
</a>Machine Learning Engineer, LinkedIn</h3><p>
	Huge kudos to the Tesla team for putting the Optimus prototype together this quickly. Clearly a world-class engineering team at the top of their game. A lot of nights and weekends there. But it’s only about 5 percent of the way to what’s being sold.
</p><p>
	The upper body actuators have harmonic drives and the leg actuators are screw driven. Backdrivability can be added to some actuators if you also include torque sensors and close a very fast control loop around them. But it doesn’t come without adding cost and complexity to your design. Low backdrivability leaves a robot rigid to external forces. When you try to compensate for it through software control loops, you can get a telltale wobble. If you watch Optimus’s hands as it walks, you can see this wobble is in the 4-hertz range.
</p><p>
	We were also teased with the almost finished, “almost production-ready” model. It can wiggle its fingers, but the backdrivability limitations have not been addressed. These are all tractable problems, but they’re hard. Probably not 12 months out.
</p><p>
	I’m delighted to see such an ambitious robot project in the world! But it doesn’t help the field of robotics to over-promise. We already tried this in the ’80s, and it took us 30 years to recover.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Siddhartha Srinivasa" class="rm-shortcode" data-rm-shortcode-id="02a52d4a3e700609fa78dfb58a00db20" data-rm-shortcode-name="rebelmouse-image" id="edc3a" loading="lazy" src="https://spectrum.ieee.org/media-library/siddhartha-srinivasa.jpg?id=31857030&width=980"/><h3 class="rm-anchors" id="siddhartha-srinivasa"><a href="https://twitter.com/siddhss5/status/1576569786686877697" rel="noopener noreferrer" target="_blank">Siddhartha Srinivasa<br/>
</a>Director of Robotics and AI, Amazon</h3><p>
	My take on the Tesla Bot: You’re not as good as you think you are. You’re also not as bad as they think you are.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Mikell Taylor" class="rm-shortcode" data-rm-shortcode-id="a7286234b44878ad6bf08dde630cf5e1" data-rm-shortcode-name="rebelmouse-image" id="9b6a3" loading="lazy" src="https://spectrum.ieee.org/media-library/mikell-taylor.jpg?id=31857032&width=980"/><h3 class="rm-anchors" id="mikell-taylor"><a href="https://twitter.com/MikellTaylor/status/1576554770659409921" rel="noopener noreferrer" target="_blank">Mikell Taylor<br/>
</a>Principal Technical Program Manager, Amazon Robotics</h3><p>
	There is no “doing it first” with an all-purpose humanoid robot that can 1:1 replace people. That’s simply not where the tech is now and it’s not where it’s going to be in the next 10 years, I will say with complete confidence.
</p><p>
	Those of us in the industry were watching to see if Tesla somehow knew something we didn’t know. And…nope. They did not. The tech isn’t there for anyone. And that’s why the more generous takes some of us try to make for the sake of the engineering team are “you did as well as you could and you’re figuring out the state of the art quickly.” It’s not that team’s fault. There was no way to achieve an impossible goal.
</p><p>
	Here’s the thing. What humanoid robotics needs is a realistic vision. A “here is exactly where this, a humanoid form factor, is needed instead of literally anything else” vision that acknowledges the realities of the technology. But that’s not what it was. It was “today we have this, in a year we’ll have sci-fi.” And that’s just not going to happen. Frankly, it’s disappointingly un-visionary. “Everyone else just hasn’t engineered hard enough” is not a vision. It’s ignorance.
</p><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div><h3></h3><br><img alt="Cynthia Yeung" class="rm-shortcode" data-rm-shortcode-id="e3028a4995b4ff02397904229ca3e8d7" data-rm-shortcode-name="rebelmouse-image" id="9d4e1" loading="lazy" src="https://spectrum.ieee.org/media-library/cynthia-yeung.jpg?id=31857033&width=980"/><h3 class="rm-anchors" id="cynthia-yeung"><a href="https://twitter.com/ctwy" rel="noopener noreferrer" target="_blank">Cynthia Yeung<br/></a>Founding Board Member, Women in Robotics</h3><p>
	Things I liked:
</p><ul>
<li>Sharing the spotlight with engineers</li>
<li>Cool actuators and simulators</li>
<li>Expression of humility toward the end when [Musk] admits they may be “barking up the wrong tree” in terms of technical approach</li>
</ul><p>
	Things I didn’t:
</p><ul>
<li>Musk, referring to competitors’ robots, is claiming that Optimus will be autonomous and cost < [US] $20k (<a href="https://en.wikipedia.org/wiki/BOM" target="_blank">BOM</a> or retail price?). From a product perspective, <$20k <a href="https://en.wikipedia.org/wiki/Capital_expenditure" target="_blank">capex</a> with zero recurring fees isn’t a great revenue model when all the robotics startups are moving toward <a href="https://en.wikipedia.org/wiki/Robot_as_a_service" target="_blank">RaaS</a> models.</li>
<li>Battery details: Tesla claims [2.3 kWh] will be enough for a full day’s work. Agility Robotics’ Digit batteries last 3 hours for light duty. This doesn’t pass the sniff test.</li>
<li>A lot of talk about joints and actuators. I think Musk seems to be enamored of the Boston Dynamics approach toward robots (form over function) as opposed to what a lot of other folks are working on (function informs form). Look, actuators are cool and all, but that’s not the hardest part about creating useful robots!</li>
<li>The AI part seems to be conspicuously missing from Tesla AI Day.</li>
<li>I love that Tesla has decided to create a five-fingered hand as opposed to a two- or three-finger pincer or vacuum-based pick-and-place approach. There’s a reason why all the warehouse startups don’t use handlike manipulation mechanisms.</li>
<li>Another Tesla robotics engineer is talking about controls in the real world and state estimation. This is coming off as a grad student/TA doing a class presentation for a bunch of undergrads. Feels very 101. None of this is cutting edge. Hire some Ph.D.s and go to some robotics conferences, Tesla: IROS 2022 is coming up in a few weeks!</li>
</ul><p class="caption">
	[ <a href="#top">BACK TO TOP</a> ↑ ]
</p><div class="horizontal-rule">
</div></br></br></br></br></br></br></br></br></br></br></br></br></br></br>]]></description><pubDate>Tue, 04 Oct 2022 14:32:59 +0000</pubDate><guid>https://spectrum.ieee.org/robotics-experts-tesla-bot-optimus</guid><category>Tesla</category><category>Tesla bot</category><category>Optimus robot</category><category>Home robots</category><category>Humanoid robots</category><dc:creator>Erico Guizzo</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/tesla-s-optimus-robot-waves-at-audience-from-the-stage.jpg?id=31857078&amp;width=980"></media:content></item><item><title>For Better or Worse, Tesla Bot Is Exactly What We Expected</title><link>https://spectrum.ieee.org/tesla-optimus-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-humanoid-robot-with-metal-and-wires-exposed-stands-on-stage.jpg?id=31851352&width=1200&height=800&coordinates=0%2C36%2C0%2C37"/><br/><br/><p>
	At the end of Tesla’s 2021 AI Day last August, 
	<a href="https://spectrum.ieee.org/elon-musk-robot" target="_self">Elon Musk introduced a concept for “Tesla Bot,”</a> an electromechanically actuated, autonomous bipedal “general purpose” humanoid robot. Musk suggested that a prototype of Tesla Bot (also called “Optimus”) would be complete within the next year. After a lot of hype, a prototype of Tesla Bot was indeed unveiled last night at Tesla’s 2022 AI Day. And as it turns out, the hype was just that—hype.
</p><p>
	While there’s absolutely nothing 
	<em>wrong</em> with the humanoid robot that Musk very briefly demonstrated on stage, there’s nothing uniquely <em>right</em>, either. We were hoping for (if not necessarily expecting) more from Tesla. And while the robot isn’t exactly a disappointment, there’s very little to suggest that it disrupts robotics the way that SpaceX did for rockets or Tesla did for electric cars.
</p><p>
	You can watch the entire 3+ hour live stream archived on YouTube 
	<a href="https://www.youtube.com/watch?v=ODSJsviD_SU" rel="noopener noreferrer" target="_blank">here</a> (which also includes car stuff and whatnot), but we’re just going to focus on the most interesting bits about Tesla Bot/Optimus.
</p><hr/><h2>Setting Expectations<br/>
</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Elon Musk on stage with two Tesla engineers." class="rm-shortcode" data-rm-shortcode-id="6f5cb4220433b1450d86b022c985b25c" data-rm-shortcode-name="rebelmouse-image" id="23ad0" loading="lazy" src="https://spectrum.ieee.org/media-library/elon-musk-on-stage-with-two-tesla-engineers.png?id=31851307&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Before revealing the robot, Musk attempted to set reasonable expectations for the prototype.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	These quotes are all from Musk.
</p><blockquote>
	“I do want to set some expectations with respect to our Optimus robot… Last year was just a person in a robot suit, but we’ve come a long way, and compared to that, it’s going to be very impressive.”
</blockquote><p>
	It’s far, far too late for Musk to be attempting to set reasonable expectations for this robot (or Tesla’s robotics program in general). Most roboticists know better than to use humans when setting expectations for humanoid robots, because disappointment is inevitable. And trying to save it at the literal last minute by saying “compared to not having a robot at all, our robot will be very impressive,” while true, is not going to fix things.
</p><blockquote>
<em>“I think there’s some potential that what we’re doing here at Tesla could make a meaningful contribution to AGI.”</em>
</blockquote><p>
	Yeah, I’m not touching that.
</p><p>
<div class="rblad-ieee_in_content"></div>
</p><p>
	Right before the robot was brought on stage, one of the engineers made clear that this was going to be the first time that the robot would be walking untethered and unsupported. If true, that’s bonkers, because why the heck would you wait until 
	<em>this moment</em> to give that a try? I’m not particularly impressed, just confused.
</p><p>
	For some context on what you’re about to see, a brief callback to a year ago last August, when I predicted what was in store for 2022:
</p><blockquote>
<em>It’s possible, even likely, that Tesla will build some sort of Tesla Bot by sometime next year, as Musk says. I think that it won’t look all that much like the concept images in this presentation. I think that it’ll be able to stand up, and perhaps walk. Maybe withstand a shove or two and do some basic object recognition and grasping. And I think after that, progress will be slow. But the hard part is not </em><em><strong>building </strong></em><em>a robot, it’s getting that robot to do </em><em><strong>useful stuff</strong></em><em>, and I think Musk is way out of his depth here.</em>
</blockquote><h2>Tesla Bot Development Platform Demo</h2><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="39d188961e8e294502919847dd38d9f3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/uncvPWe008I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	I’m reminded of the 2015 DARPA Robotics Challenge, because 
	<a href="https://spectrum.ieee.org/darpa-robotics-challenge-drc-finals-know-your-robots" target="_self">many of the humanoid platforms looked similar to the way Tesla Bot looks</a>. I guess there’s only so much you can do with a mostly naked electromechanical humanoid in terms of form factor, but at first glance there’s nothing particularly innovative or futuristic about Tesla’s design. If anything, the robot’s movement is not quite up to DRC standards, since it looks like it would have trouble with any kind of accidental contact or even a bit of nonlevel floor (and Musk suggested as much).
</p><p>
	On stage, the robot did very little. It walked successfully, but not very dynamically. The “moves” it made may well have been entirely scripted, so we don’t know to what extent the robot can balance on its own. I’m glad it didn’t fall on its face, but if it had, I wouldn’t have been surprised or judged it too harshly.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The humanoid robot stands near a desk and extends its arm to grab a watering can." class="rm-shortcode" data-rm-shortcode-id="3682f5765e596d1c73522b30193378af" data-rm-shortcode-name="rebelmouse-image" id="91e18" loading="lazy" src="https://spectrum.ieee.org/media-library/the-humanoid-robot-stands-near-a-desk-and-extends-its-arm-to-grab-a-watering-can.png?id=31851310&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla showed videos of the robot watering plants, carrying a box, and picking up a metal bar at a factory. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	After the very brief live demo, Musk showed some video clips of the prototype robot doing other things (starting at 19:30 in the live stream). These clips included the robot walking while carrying a box of unspecified weight and placing it on a table, and grasping a watering can. The watering can was somewhat impressive, because gripping that narrow handle looks tricky.
</p><p class="pull-quote">
	“The robot can actually do a lot more than we’ve just showed you. We just didn’t want it to fall on its face.”
	<br/>
	—Elon Musk
</p><p>
	However, despite the added footage from the robot’s sensors, we have no idea how this was actually done—whether it was autonomous or not, or how many tries it took to get right. There’s also a clip of a robot picking up an object and attempting to place it in a bin, but the video cuts right before the placement is successful. This makes me think that we’re seeing carefully curated best-case scenarios for performance.
</p><blockquote>
	That was our rough development robot, using semi-off-the-shelf actuators, but we’ve gone a step farther than that already. We actually have an Optimus bot with fully Tesla-designed actuators, battery pack, control system, everything—it wasn’t quite ready to walk, but we wanted to show you something that’s fairly close to what will go into production.
</blockquote><h2>Tesla Bot Latest Generation Demo</h2><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="01c1820322b73c6bb8405cc72c1856ce" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/q_tgzTQK_hc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	This looks a bit more like the concept that Tesla showed last year, although obviously it’s less functional than the other prototype we saw. It’s tempting to project the capabilities of the first robot onto the second robot, but it would be premature to do so.
</p><blockquote>
<em>Here you’re seeing Optimus with the degrees of freedom that we expect to have in the Optimus production unit, which is the ability to move all the fingers independently and opposable thumbs, so that it’s able to operate tools and do useful things. </em>
</blockquote><h2>Cost and Competition</h2><h3>How Much Does Tesla’s Robot Cost?</h3><br/><p>
	Musk expects the robot to cost less than a car: “much less than $20,000,” he said at the event.
</p><blockquote>
<em>Our goal is to make a useful humanoid robot as quickly as possible. We’ve designed it using the same discipline that we use in designing the car, which is to design it for manufacturing, such that it’s possible to make the robot at high volume with low cost and high reliability. That’s incredibly important.… <br/><br/>Optimus is designed to be an extremely capable robot, but made in very high volume, ultimately millions of units. And it is expected to cost much less than a car—much less than [US] $20,000 would be my guess.<br/>
</em>
</blockquote><p>
	I generally agree with Musk here, in that historically, humanoid robots were not designed for manufacturability. This is changing, however, and I think that other companies likely have a head start over Tesla in manufacturability now. But it’s entirely possible that Tesla will be able to rapidly catch up if it’s able to leverage all that car-building expertise into robot building somehow. It’s not a given that it’ll work that way, but it’s a good idea, potentially a big advantage.</p><p>As for the production volume and cost, I have no idea what “expected” means. This line got some applause, but as far as I’m concerned, these numbers are basically meaningless at the moment.
</p><h3>​What Will the Tesla Robot Be Used for?</h3><br><p>
	Musk said the robot will be able to operate tools and do useful things like carrying and manipulating objects in factories and other settings.
</p><p>
	Just like last year, he’s implying that the robot will be able to carry out useful tasks because it has the necessary degrees of freedom. But of course the hardware is only the first step toward operating tools and doing useful things, and the software is, I would argue, much harder and far more time consuming, and Tesla seems to have barely started work on that side of things.</p></br><p>
<div class="rblad-ieee_in_content"></div>
</p><blockquote>
<em>You’ve all seen very impressive humanoid robot demonstrations, and that’s great, but what are they missing? They’re missing a brain—they don’t have the intelligence to navigate the world by themselves. </em>
</blockquote><p>
	I’m not exactly sure whom Musk is throwing shade at, but there are only a couple of companies that would probably qualify as having “very impressive humanoid robot demonstrations.” And those companies do, in fact, have robots that broadly have the kind of intelligence that allows them to navigate at least some of the world by themselves, much better than we have seen from Optimus at this point. If Musk is saying that those robots are insufficiently autonomous or world-aware, then okay, but so far Tesla has not done better, and doing better will be a lot of work.
</p><blockquote>
<em>The team has put in an incredible amount of work, seven days a week, to get to the demonstration today. I’m super proud, and they’ve really done a great job.</em>
</blockquote><p>
	While the actual achievements here have been mercilessly overshadowed by the hype surrounding them, this is truly an astonishing amount of work to be done in such a short time, and Tesla’s robotics team should be proud of what they’ve accomplished. And while there will inevitably be comparisons to other companies with humanoid robots, it’s critical to remember the context here: Tesla has made this happen in something like eight months. It’s nuts.
</p><blockquote>
<em>There’s still a lot of work to be done to refine Optimus and improve it, and that’s really why we’re holding this event—to convince some of the most talented people in the world to join Tesla and help make it a reality, help bring it to fruition at scale so that it can help millions of people. </em>
</blockquote><p>
	I can see the appeal of Tesla for someone who wants to start a robotics career, since you’d get to work on a rapidly evolving hardware platform backed by what I can only assume are virtually unlimited resources.
</p><blockquote>
<em>…This means a future of abundance, a future where there is no poverty, where you can have whatever you want in terms of products and services. It really is a fundamental transformation of civilization as we know it.</em>
</blockquote><p>
	Maybe just, like, get your robot to reliably and affordably do A Single Useful Thing, first?
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The images showing the evolution of the robot." class="rm-shortcode" data-rm-shortcode-id="5868f1430ca97eecbb9b5e653c07b9a6" data-rm-shortcode-name="rebelmouse-image" id="8f96f" loading="lazy" src="https://spectrum.ieee.org/media-library/the-images-showing-the-evolution-of-the-robot.png?id=31851313&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Three versions of the Optimus design: Concept, Development Platform, and Latest Generation. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	Musk takes a break after this, and we get some actual specific information from a series of Tesla robotics team members about the latest generation Optimus.
</p><h2>Optimus Hardware</h2><ul>
<li>28 degrees of freedom</li>
<li>11 additional degrees of freedom in each hand</li>
<li>2.3 kilowatt-hour, 52-volt battery pack, “perfect for about a full day of work”</li>
</ul><p>
	We’ll come back to the hands, but that battery really stands out for being able to power the robot for an entire day(ish). Again, we have to point out that until Tesla actually demonstrates this, it’s not all that meaningful, but Tesla does know a heck of a lot about power systems and batteries, and I’m guessing that the company will be able to deliver on this.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Computer generated image showing the robot on the ground after a fall." class="rm-shortcode" data-rm-shortcode-id="e59f1d8ff5496a215b27f843f3306fd9" data-rm-shortcode-name="rebelmouse-image" id="b3160" loading="lazy" src="https://spectrum.ieee.org/media-library/computer-generated-image-showing-the-robot-on-the-ground-after-a-fall.png?id=31851322&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla is using simulations to design the robot’s structure so that it can suffer minimal damage after a fall.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	I appreciate that Tesla is thinking very early about how to structure its robot to be able to fall down safely and get up again with only superficial damage—although the company doesn’t seem to be taking advantage of any kind of protective motion for fall mitigation, which is an active area of research elsewhere. And what is not mentioned in this context is the safety of others. I’m glad the robot won’t get damaged all that much when it falls, but can Tesla say the same for whoever might be standing next to it?
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Six images showing the robot's actuators." class="rm-shortcode" data-rm-shortcode-id="c235bcc540c0a617dbfb782ebce7e4c0" data-rm-shortcode-name="rebelmouse-image" id="515eb" loading="lazy" src="https://spectrum.ieee.org/media-library/six-images-showing-the-robot-s-actuators.png?id=31851323&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Optimus will use six different actuators: three rotary and three linear units.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	Tesla’s custom actuators seem very reasonable. Not special, particularly, but Tesla has to make its own actuators if it needs a lot of them, which it supposedly will. I’d expect these to be totally decent considering the level of mechanical expertise Tesla has, but as far as I can tell nothing here is crazy small or cheap or efficient or powerful or anything like that. And it’s very hard to tell from these slides and from the presentation just how well the actuators are going to work, especially for dynamic motions. The robot’s software has a lot of catching up to do first.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Computer generated image showing the robot's hand." class="rm-shortcode" data-rm-shortcode-id="cd2a434449b0a323ab089975d7b5df1f" data-rm-shortcode-name="rebelmouse-image" id="a4fb2" loading="lazy" src="https://spectrum.ieee.org/media-library/computer-generated-image-showing-the-robot-s-hand.png?id=31851339&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Optimus will feature a bioinspired hand design with cable-driven actuators.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	Each hand has six cable-driven actuators for fingers and thumb (with springs to provide the opening force), which Tesla chose for simplicity and to minimize part count. This is perhaps a little surprising, since cable drives typically aren’t as durable and can be more finicky to keep calibrated. The five-finger hand is necessary, Tesla says, because Optimus will be working with human tools in human environments. And that’s certainly one perspective, although it’s a big trade-off in complexity. The hand is designed to carry a 9-kilogram bag.
</p><h2>Optimus Software</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Tesla engineer stands near Optimus prototype on stage." class="rm-shortcode" data-rm-shortcode-id="7942b73a6d6cf5b2901256f818314f70" data-rm-shortcode-name="rebelmouse-image" id="4bd3a" loading="lazy" src="https://spectrum.ieee.org/media-library/tesla-engineer-stands-near-optimus-prototype-on-stage.png?id=31851341&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla is using software components developed for its vehicles and porting them to the robot’s environment.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	Software! The following quote comes from 
	<a href="https://www.linkedin.com/in/milankovac/" rel="noopener noreferrer" target="_blank">Milan Kovac</a>, who’s on the autonomy team.
</p><blockquote>
<em>All those cool things we showed earlier in the videos were possible in just a few months, thanks to the amazing work we’ve done on Autopilot over the past few years. Most of those components ported quite easily over to the bot’s environment. If you think about it, we’re just moving from a robot on wheels to a robot on legs. Some of the components are similar, and some others required more heavy lifting. </em>
</blockquote><p>
	I still fundamentally disagree with the implied “humanoid robots are just cars with legs” thing, but it’s impressive that they were able to port much at all—I was highly skeptical of that last year, but I’m more optimistic now, and being able to generalize between platforms (on some level) could be huge for both Tesla and for autonomous systems more generally. I’d like more details on what was easy, and what was not.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Images showing how the robot sees the world through cameras and computer generated views." class="rm-shortcode" data-rm-shortcode-id="34e71d0c157cb3f93c78afa789ad3202" data-rm-shortcode-name="rebelmouse-image" id="56dd7" loading="lazy" src="https://spectrum.ieee.org/media-library/images-showing-how-the-robot-sees-the-world-through-cameras-and-computer-generated-views.png?id=31851342&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla showed how sensing used in its vehicles can help the Optimus robot navigate.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	What we’re seeing above, though, is one of the reasons I was skeptical. That occupancy grid (where the robot’s sensors are detecting potential obstacles) on the bottom is very car-ish, in that the priority is to make absolutely sure that the robot stays very far away from anything it could conceivably run into.
</p><p>
	By itself, this won’t transfer well to a humanoid robot that needs to directly interact with objects to do useful tasks. I’m sure there are lots of ways to adapt the Tesla car’s obstacle-avoidance system, but that’s the question: How hard is that transfer, and is it better than using a solution developed specifically for mobile manipulators?
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Tesla engineer on stage next to Optimus prototype." class="rm-shortcode" data-rm-shortcode-id="fd7d7a62940c9cf92af59401309597cb" data-rm-shortcode-name="rebelmouse-image" id="5ac5e" loading="lazy" src="https://spectrum.ieee.org/media-library/tesla-engineer-on-stage-next-to-optimus-prototype.png?id=31851349&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla explained the challenges of dynamic walking in humanoid robots, and its approach to motion planning.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	The next part of the presentation focused on some motion planning and state-estimation stuff that was very basic, as far as I could make out. There’s nothing wrong with the basics, but it’s slightly weird that Tesla spent so much time on this. I guess it’s important context for most of the people watching, but the team sort of talked about it like they’d discovered how to do all of this stuff themselves, which I hope they didn’t, because again, very, very basic stuff that other humanoid robots have been doing for a very long time.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Slide shown by Tesla engineer to explain the robot's Motion Control." class="rm-shortcode" data-rm-shortcode-id="5351d3a2d227cdd24702f3a72085099c" data-rm-shortcode-name="rebelmouse-image" id="3174c" loading="lazy" src="https://spectrum.ieee.org/media-library/slide-shown-by-tesla-engineer-to-explain-the-robot-s-motion-control.png?id=31851351&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla adopted a traditional approach to motion control, based on a model of the robot and state estimation.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	One more quote from 
	<a href="https://www.linkedin.com/in/milankovac/" target="_blank">Milan Kovac</a>:
</p><blockquote>
<em>Within the next few weeks, we’re going to start focusing on a real use case at one of our factories. We’re really going to try to nail this down, and iron out all of the elements needed to deploy this product in the real world. I’m pretty sure we can get this done within the next few months or years, and make this product a reality and change the entire economy.</em>
</blockquote><p>
	Ignoring that last bit about changing the entire economy, and possibly also ignoring the time frame because “next few months or years” is not particularly meaningful, the push to make Tesla Bot useful is another substantial advantage that Tesla has. Unlike most companies working on humanoid robots, Tesla is potentially its own biggest customer, at least initially, and having these in-house practical tasks for the robot to train on could really help accelerate development.
</p><p class="pull-quote">
	“Optimus is designed to be an extremely capable robot, but made in very high volume, ultimately millions of units. And it is expected to cost much less than a car—much less than [US] $20,000 would be my guess.”
	<br/>
	—Elon Musk
</p><p>
	However, I’m having trouble imagining what Tesla Bot would actually 
	<strong>do</strong> in a factory that would be uniquely useful and not done better by a nonhumanoid robot. I’m very interested to see what Tesla comes up with here, and whether the company can make it happen in months (or years). I suspect it’s going to be much more difficult than it’s suggesting it will be, especially as it gets to 90 percent of where it wants to be and start trying to crack that last 10 percent that’s necessary for something reliable.
</p><p>
<span></span>This was the end of the formal presentation about Optimus, but there was a Q&A at the end with Musk where he gave some additional information about the robot side of things. He also gave some additional noninformation, which is worth including just in case you haven’t yet had enough eye rolling for one day.
</p><h2>Audience Q&A</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Elon Musk stands on stage with the Tesla AI team." class="rm-shortcode" data-rm-shortcode-id="c07fde9e7c6a986b25c10333adf1d915" data-rm-shortcode-name="rebelmouse-image" id="2bfd5" loading="lazy" src="https://spectrum.ieee.org/media-library/elon-musk-stands-on-stage-with-the-tesla-ai-team.png?id=31851353&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Musk expects Optimus to cost less than a car, “much less than [US] $20,00 dollars would be my guess,” he said.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><blockquote>
<em>Our goal with Optimus is to have a robot that’s maximally useful as quickly as possible. There are a lot of ways to solve the various problems of a humanoid robot, and we’re probably not barking up the right tree on all the technical solutions. We’re open to evolving the technical solutions that you see here over time. But we had to pick something. We’re trying to follow the goal of fastest pathway to a useful robot that can be made at volume. And we’re going to test the robot internally in our factory to see how useful it is, because you have to close the loop on reality to confirm that the robot is, in fact, useful.</em>
</blockquote><p>
	This is a variation on the minimum-viable-product idea, although it seems to be more from the perspective of making a generalist robot, which is somewhat at odds with something minimally viable. It’s good that Musk views the hardware as something in flux, and that he’s framed everything within a plan for volume production. This isn’t the only way to do it—you can first build a useful robot and then figure out how to make it cheaper, but Tesla’s approach could get the company to production faster. If, that is, it is able to confirm that the robot is in fact useful. I’m still not convinced that it will be, at least not on a time scale that will satisfy Musk.
</p><blockquote>
<em>I think we’ll want to have really fun versions of Optimus. Optimus can be utilitarian, and do tasks, but it can also be like a friend, and a buddy, and hang out with you. I’m sure people will think of great uses for this robot. Once you have the core intelligence and actuators figured out, you can put all sorts of costumes, I guess, on the robot.</em>
</blockquote><p>
<div class="rblad-ieee_in_content"></div>
</p><p>
	While Musk seems to be mostly joking here, the whole “it’s going to be your friend” is really not a good perspective to bring to a robot like this, in my opinion. Or probably any robot, at all honestly.
</p><blockquote>
<em>We want over time for Optimus to be the kind of android that you see in sci-fi, like in Star Trek: The Next Generation, like Data. But obviously we could program the robot to be less robotlike and more friendly, and it can obviously learn to emulate humans and feel very natural.</em>
</blockquote><p>
	Less robotlike and more friendly than a human pretending to be a robot trying to be a human? Good luck with that.
</p><blockquote>
<em>We’re going to start Optimus with very simple tasks in the factory, like maybe carrying a part from one place to another, or loading a part into a conventional robot cell. We’ll start with how do we make it useful at all, and then gradually expand the number of situations in which it’s useful. I think the number of situations where Optimus is useful will grow exponentially.</em>
</blockquote><p>
	I think it’s more likely that in the short-to-medium term, Tesla will struggle to find situations where Optimus is uniquely useful in an efficient and cost-effective way.
</p><blockquote>
<em>In terms of when people can order one, I think it’s not that far away. I don’t know, I’d say within three years, probably not more than five years. </em>
</blockquote><p>
	Uh. Maybe as a research platform?
</p><blockquote>
<em>I think Optimus is going to be incredible in five years. In 10 years, mind-blowing. I’m really interested to see that happen, and I hope you are too.</em>
</blockquote><p>
	Despite my skepticism on the time frame here, five years is a long time for any robot, and 10 years is basically forever. I’m also really interested to see these things happen, although Musk’s definitions of “incredible” and “mind-blowing” may be much different than mine. But we’ll see, won’t we?
</p><h2>What’s Next for Tesla Bot?</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Elon Musk with the Tesla AI team on stage." class="rm-shortcode" data-rm-shortcode-id="e60812eea3ae614cdd856560ee5de8a9" data-rm-shortcode-name="rebelmouse-image" id="e9e95" loading="lazy" src="https://spectrum.ieee.org/media-library/elon-musk-with-the-tesla-ai-team-on-stage.jpg?id=31851355&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Tesla’s AI Day serves as a recruitment event for the company. “There’s still a lot of work to be done to refine Optimus and improve it, and that’s really why we’re holding this event—to convince some of the most talented people in the world to join Tesla,” Musk said.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tesla</small>
</p><p>
	I think Elon Musk now has a 
	<a href="https://spectrum.ieee.org/elon-musk-robot" target="_self">somewhat better idea of what he’s doing with Tesla Bot</a>. The excessive hype is still there, but now that the company has actually built something, Musk seems to have a much better idea of how hard it actually is.
</p><p>
	Things are only going to get more difficult from here.
</p><p>
	Most of what we saw in the presentation was hardware. And hardware is important and a necessary first step, but software is arguably a much more significant challenge when it comes to making robotics useful in the real world. Understanding and interacting with the environment, reasoning and decision-making, the ability to learn and be taught new tasks—these are all necessary pieces of the puzzle of a useful robot that Tesla is trying to put together, but they’re all also extremely difficult, cutting-edge problems, despite the enormous amount of work that the research community has put into them.
</p><p>
	And so far, we (still) have very little indication that Tesla is going to be any better at tackling this stuff than anyone else. There doesn’t appear to be anything all that special or exciting from Tesla that provides any unique foundation for Musk’s vision in a way that’s likely to allow the company to outpace other companies working on similar things. I’ll reiterate what I said a year ago: The hard part is not 
	<em>building</em> a robot, it’s getting that robot to <em>do useful stuff</em>.
</p><p class="pull-quote">
	“I think Optimus is going to be incredible in five years. In 10 years, mind-blowing. I’m really interested to see that happen, and I hope you are too.”
	<br/>
	—Elon Musk
</p><p>
	I could, of course, be wrong. Tesla likely has more resources to throw at this problem than almost anyone else. Maybe the automotive software will translate much better and faster than I think it will. There could be a whole bunch of simple but valuable use cases in Tesla’s own factories that will provide critical stepping-stones for Optimus. Tesla’s battery and manufacturing expertise could have an outsized influence on the affordability, reliability, and success of the robot. The company’s basic approach to planning and control could become a reliable foundation that will help the system mature faster. And the team is obviously very talented and willing to work extremely hard, which could be the difference between modest success and slow failure.
</p><p>
	Honestly, I would love to be wrong. We’re just starting to see some realistic possibilities with commercial legged and humanoid robots. There are lots of problems to solve, but also lots of potential, and Tesla finding success would be a huge confidence boost in commercial humanoids broadly. We can also hope that all of the resources that Tesla is putting toward Optimus will either directly or indirectly assist other folks working on humanoid robots, if Tesla is willing to share some of what it learns. But as of today, this is all just hoping, and it’s on Tesla to actually make it happen.
</p>]]></description><pubDate>Sat, 01 Oct 2022 18:50:51 +0000</pubDate><guid>https://spectrum.ieee.org/tesla-optimus-robot</guid><category>Humanoid robots</category><category>Tesla</category><category>Optimus robot</category><category>Elon musk</category><category>Robotics</category><category>Tesla</category><category>Tesla bot</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-humanoid-robot-with-metal-and-wires-exposed-stands-on-stage.jpg?id=31851352&amp;width=980"></media:content></item><item><title>Video Friday: StickBot</title><link>https://spectrum.ieee.org/video-friday-stickbot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-image-of-a-robot-made-of-a-small-sticks-tied-together-with-a-tangle-of-colorful-wires-batteries-actuators-and-electronics.jpg?id=31846789&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><p>From Devin Carroll, who brought us <a href="https://spectrum.ieee.org/robots-made-of-ice-could-build-and-repair-themselves-on-other-planets" target="_blank">a robot made of ice</a>, is a robot made of sticks.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="022562c53d4a9dd447d70948fb81b47c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/M12iLYQ-P7E?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://penntoday.upenn.edu/news/grasp-lab-stickbot-robot-made-sticks">UPenn</a> ]</p><div class="horizontal-rule"></div><p>Amazon Astro can now check on your pets for you. Not sure how the pets feel about that, though.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="97aad65320d558c88b41fa491190b5d2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/mM1K7QRhpVY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.amazon.com/Introducing-Amazon-Astro/dp/B078NSDFSB">Amazon</a> ]</p><div class="horizontal-rule"></div><p>Soft robot hugs for everyone!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b6b2f3ab3a551411cbb28a69ab921030" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/G8ZYgPRV5LY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2111.09354">Paper</a> ]</p><div class="horizontal-rule"></div><p>Scythe’s upgraded M.52 autonomous robotic mower can now handle more complex obstacles and terrain, with big enough batteries to behead grass all day long.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a5489f7af40a9f0905db03a507a6a271" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wqwDldk2zH8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.scytherobotics.com/mower">Scythe</a> ]</p><p>Thanks, Jack!</p><div class="horizontal-rule"></div><blockquote><em>Agility CEO Damion Shelton and CTO Jonathan Hurst discuss artificial intelligence and its role in robot control. They also discuss the capability of robot learning paired with physics-based locomotion, Cassie setting a new world record using learned policies for control, and an exploration of the future of robotics through Dall-E.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="96a96e49f19eac49bc8d750b844b5bc3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ty2Fxllg26Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>That new version of Digit is looking slick!</p><p>[ <a href="https://agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><p>Intel gives an update on RealSense at a recent ROS Industrial meeting, and the part you’ll probably want to listen to starts at 3:50.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b531bdbf2b62b3419327d0db014e046c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/MyRyuDdnAvs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://rosindustrial.org/news/2022/9/22/fall-2022-ros-i-community-meeting-happenings">ROS-I</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Local navigation and locomotion of legged robots are commonly split into separate modules. In this work, we propose to combine them by training an end-to-end policy with deep reinforcement learning. Training a policy in this way opens up a larger set of possible solutions, which allows the robot to learn more complex behaviors.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9c5b96a29d441cf65f0b4e469b9d956b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Xoe8a_2t24U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>That box climbing, right?</p><p>[ <a href="https://rsl.ethz.ch/">RSL</a> ]</p><div class="horizontal-rule"></div><p>Neura Robotics is building a new humanoid. Most of this video is CG, but since there does appear to be a physical robot at the very end (albeit one that doesn’t do much), we’ll let it slide.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="5c4a625bc314f33442e1bf03e80a44f5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SGR-FTNeMAI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://neura-robotics.com/meet-4ne-1">Neura Robotics</a> ]</p><div class="horizontal-rule"></div><p>Dino Robotics will help you teach your robot to make hard-boiled eggs, which will make it a better chef than I am.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="498160819639d3f5a6908467622b1129" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AIj6rRRCT7g?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dino-robotics.com/">Dino Robotics</a> ]</p><div class="horizontal-rule"></div><p>You know what sucks for robots? Lidar in blowing snow.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="69f9f3bbcb2445c9a6a60fe890be63f1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/sro_zsfMIRA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2209.03226">Paper</a> ]</p><div class="horizontal-rule"></div><p><a href="https://spectrum.ieee.org/senator-bans-funding-for-beerbots-that-dont-exist" target="_blank">This research is banned in the United States</a>.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f20a88540da57df58242a7efe29f0c52" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SJFpNP0esJk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.shadowrobot.com/">Shadow</a> ]</p><div class="horizontal-rule"></div><blockquote><em>We often get asked about  how Starship robots navigate around the community and those within, so we wanted to give a little insight—and some tips on what to do if you come across one on your journey. Have a look at how our robots navigate around various obstacles throughout their delivery journeys.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="64ab560c9a68e72018c683373a6b440a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/eNHsehDSWzk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.starship.xyz/">Starship</a> ]</p><div class="horizontal-rule"></div><blockquote><em>AIIRA’s vision is to create new AI-driven, predictive digital twins for modeling plants, and deploy them to increase the resiliency of the nation’s agricultural systems.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="0c3e41f9bbcd4490ed576d8709cfb132" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SM90T18_Wxw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://labs.ri.cmu.edu/aiira/">AIIRA</a> ]</p><div class="horizontal-rule"></div><blockquote><em>On 22 September 2022, Ryan Eustice of Toyota Research Institute talked to robotics students as a speaker in the Undergraduate Robotics Pathways & Careers Speaker Series, which aims to answer the question “What can I do with a robotics degree?”</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="66e332be3f1a2099008d771c2b64b85e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/F-5PmFVrXmk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotics.umich.edu/academics/undergraduate/robotics-pathways-speaker-series/">Michigan Robotics</a> ]</p><div class="horizontal-rule"></div><p>This Maryland Robotics Center Seminar is by Michael T. Tolley at University of California, San Diego, on biologically inspired soft mobile robots.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2028d6fa9dd45286f0862912e389e502" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/RnzAgCbitxg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://robotics.umd.edu/event/17495/maryland-robotics-center-seminar-biologically-inspired-soft-mobile-robots">UMD</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 30 Sep 2022 15:10:51 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-stickbot</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-image-of-a-robot-made-of-a-small-sticks-tied-together-with-a-tangle-of-colorful-wires-batteries-actuators-and-electronics.jpg?id=31846789&amp;width=980"></media:content></item><item><title>Robo-Ostrich Sprints to 100-meter World Record</title><link>https://spectrum.ieee.org/bipedal-robot-world-record-speed</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-robot-with-two-orange-ostrich-like-legs-and-no-torso-sprints-along-a-running-track.jpg?id=31833360&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>For a robot that shares a leg design with the fastest-running bird on the planet, we haven’t ever really gotten a sense of how fast <a href="https://spectrum.ieee.org/agility-robotics-introduces-cassie-a-dynamic-and-talented-robot-delivery-ostrich" target="_self">Agility Robotics’ Cassie</a> is actually able to move. <a href="https://www.youtube.com/watch?v=a_YGPbWJO5g" rel="noopener noreferrer" target="_blank">Oregon State University’s Cassie successfully ran a 5k last year</a>, but it was the sort of gait that we’ve come to expect from humanoid robots—more of a jog, really, with measured steps that didn’t inspire a lot of confidence in higher speeds. Turns out, Cassie was just holding back, because she’s just sprinted her way to <a href="https://today.oregonstate.edu/news/bipedal-robot-developed-oregon-state-achieves-guinness-world-record-100-meters" rel="noopener noreferrer" target="_blank">a Guinness World Record for fastest 100-meter run by a bipedal robot</a>.</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="44ce5ed9ae8e165b932b0311254b93a8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DdojWYOK0Nc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>Cassie’s average speed was just over 4 meters per second, completing the 100 meters in 24.73 seconds. And for a conventional<sup>1</sup> bipedal robot, that is <em>fast</em>. Moreover, her top speed was certainly higher than 4 m/s, since <a href="https://www.guinnessworldrecords.com/world-records/629600-fastest-100-m-by-a-bipedal-robot" target="_blank">the record attempt required a standing start</a> (along with a return to the starting point without falling over). This is also by far the most ostrichlike I’ve ever seen Cassie move, with a springy birdlike gait. With a feathery costume on, the robot would be a dead ringer for the real bird, and it would give Cassie something to aspire to, since a real ostrich can run the 100-meter in 5 seconds flat. </p><p>This was not an autonomous run, since this version of Cassie has no external sensors, and there was a human with a remote doing the steering. OSU’s Dynamic Robotics Laboratory has been working on <a href="https://www.youtube.com/watch?v=Wb0tIWBrjmc" target="_blank">this kind of dynamic movement</a> for a while, but the sprinting in particular required some extra training in the form of gait optimization in simulation. And according to the researchers, one of the most difficult challenges was actually getting Cassie to reach a sprint from a standing start and then slow down to a stop on the other end without borking herself.</p><p>“This may be the first bipedal robot to learn to run, but it won’t be the last,” Agility Robotics’ Jonathan Hurst said. “I believe control approaches like this are going to be a huge part of the future of robotics. The exciting part of this race is the potential. Using learned policies for robot control is a very new field, and this 100-meter dash is showing better performance than other control methods. I think progress is going to accelerate from here.”</p><p>I certainly hope that this won’t be the last bipedal robot to learn to run, because I would pay money to attend a live bipedal robot race.</p><div class="horizontal-rule"></div><p><sup>1</sup>Arguably, the fastest bipedal legged robot was probably the <a href="https://www.youtube.com/watch?v=LTIpdtv_AK8" rel="noopener noreferrer" target="_blank">OutRunner</a>—depending on what you decide counts as “legged” and “bipedal,” although it would not have qualified for this particular record due to its difficulty with starting and stopping.</p>]]></description><pubDate>Wed, 28 Sep 2022 16:36:30 +0000</pubDate><guid>https://spectrum.ieee.org/bipedal-robot-world-record-speed</guid><category>Agility robotics</category><category>Cassie</category><category>Bipedal robots</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-robot-with-two-orange-ostrich-like-legs-and-no-torso-sprints-along-a-running-track.jpg?id=31833360&amp;width=980"></media:content></item><item><title>iRobot Crams Mop and Vacuum Into Newest Roomba</title><link>https://spectrum.ieee.org/irobot-roomba-combo-j7-vacuum</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-round-black-robot-vacuum-with-a-mopping-pad-that-can-move-from-below-the-robot-to-above-the-robot-and-out-of-the-way.jpg?id=31828592&width=1200&height=800&coordinates=32%2C0%2C32%2C0"/><br/><br/><p>
	Robots tend to do best when you optimize them for a single, specific task. This is especially true for home robots, which need to be low cost(ish) as well as robust enough to be effective in whatever home they find themselves in. iRobot has had this formula pretty well nailed down with its family of vacuuming robots for nearly two decades, but they’ve also had another family of floor care robots that have been somewhat neglected recently: 
	<a href="https://www.irobot.com/en_US/braava.html" target="_blank">mopping robots</a>.
</p><p>
	Today, 
	<a href="https://www.irobot.com/en_US/roomba-combo-j7plus-robot-vacuum-and-mop/C755020.html" target="_blank">iRobot is announcing the US $1,100 Roomba Combo j7+</a>, which stuffs both a dry vacuum and a wet mop into the body of a Roomba j7. While very much not the first or only combo floor-cleaning robot on the market, the Combo j7+ uses a unique and very satisfying mechanical system to make sure that your carpets stay clean and dry while giving your hard floors the moist buffing that they so desperately need.
</p><hr/><p>
	While iRobot is now best known for its vacuums, a decade ago 
	<a href="https://spectrum.ieee.org/irobot-scooba-450-is-the-most-effective-way-yet-to-avoid-cleaning-your-floors" target="_self">Scooba floor cleaning robots</a> were right up there with Roombas as a focus for the company, featuring tanks for cleaning solution and dirty water and combining vacuuming and scrubbing for non-carpeted floors. <a href="https://www.youtube.com/watch?v=_g-na7d-hEs" rel="noopener noreferrer" target="_blank">They were impressive robots</a>, but they were quite expensive and relatively labor-intensive for users, since you needed to fill and empty them with every cycle.
</p><p>
	iRobot eventually phased out <a href="https://spectrum.ieee.org/review-irobot-scooba-230" target="_self">the Scooba</a> for the Braava, <a href="https://spectrum.ieee.org/irobot-braava-jet-mopping-robot" target="_blank">which used mopping pads</a> instead of a vacuuming system and was way cheaper. But Braavas aren’t vacuums, meaning that they work best when the floor that they’re supposed to clean is vacuumed first. <a href="https://spectrum.ieee.org/irobot-completely-redesigns-its-floor-care-robots-with-new-m6-and-s9" target="_self">You can coordinate a Braava with a Roomba to do exactly that</a>, but it’s perhaps not the most elegant way of doing things, even if it does allow you to keep your robots well-optimized for single tasks. And the Braava is showing its age, without any of the <a href="https://spectrum.ieee.org/irobot-roomba-j7" target="_self">clever sensing that the newest Roombas use to intelligently navigate around your home and life</a>.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="92307296c7de4e090d0c73d3baf60985" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/S6V8kGU28Ng?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	The Combo j7+ essentially combines the mopping capabilities of a Braava with the vacuuming smarts of a Roomba j7. Frankly, I have no idea why iRobot didn’t name this thing something a bit more distinctive, because the regular j7 Roomba still exists, and the “+” simply refers to the fact that it comes with a self-emptying Clean Base. The “Combo j7+”, meanwhile, has completely different hardware taking up the back half of the robot, and isn’t (as the name sort of implies) a regular j7 with an add-on or something like that. Anyway, for brevity, I’m just going to call this new combo Roomba the j7C until iRobot comes up with something better.
</p><p>
	Now, the j7C is absolutely not the first mop/vacuum robot out there, but the way most others tackle the transition from hard floors (mop appropriate) and carpet (not mop appropriate) is to raise the mopping attachment up into the body of the robot a couple of millimeters when on carpet to keep the mop from dragging across the carpet and making it wet and gross. This is certainly better than 
	<em>not</em> lifting the mop up at all, but iRobot says that it still “paints” wet drippy drops all over the place. Not ideal.
</p><p class="pull-quote">
	“Why would someone sell a carpet-painting robot that applies mud to your carpet in a systematic fashion? It’s a fundamentally flawed concept that doesn’t work. iRobot, being who we are, said “here’s an impossible challenge, let’s go do it because it should be fun.” 
	<br/>
	—Colin Angle, iRobot CEO
	<br/>
</p><p>
	iRobot’s solution is, honestly, more complex than I would have thought to be practical for a Roomba—the mop pad is attached to the robot through two actuated metal arms, which can move the entire thing from the bottom of the robot to the top, placing the body of the robot in between any droppy drips and the carpet:
	<br/>
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Animated gif showing operation of mopping pad, moving on arms from beneath the robot to above" class="rm-shortcode" data-rm-shortcode-id="b1f206fb135757f566ea9996f54a3320" data-rm-shortcode-name="rebelmouse-image" id="16b04" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-gif-showing-operation-of-mopping-pad-moving-on-arms-from-beneath-the-robot-to-above.gif?id=31828618&width=980"/>
</p><p>
	Cool, right? There are belt drives in there to move the arms, making sure that the motion is both smooth enough and powerful enough to exert adequate pressure on the floor when the pad is under the robot. And if you look closely, you’ll see the skins on the sides of the robot open out slightly to give the arms space to move up and down.
</p><p>
	The j7C vacuums and mops in one single pass. On hard floors, water (or cleaning solution) is continuously sprayed underneath the robot from ports just behind the vacuuming system, and the mopping pad wipes it right up. When the robot detects carpet (which it does through ultrasonic sensors, not visually like the animation suggests), it pauses to lift the pad up, and then vacuums the carpet just like a regular j7 Roomba. You can remove and clean the pad of course, which is made easier to do since you don’t have to get under the robot to do it.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Underneath view of the robot, showing the cleaning system of rolling rubber brushes and a mopping pad behind them" class="rm-shortcode" data-rm-shortcode-id="ef651ca2caf54f52535fa1a18f98efb1" data-rm-shortcode-name="rebelmouse-image" id="7c69a" loading="lazy" src="https://spectrum.ieee.org/media-library/underneath-view-of-the-robot-showing-the-cleaning-system-of-rolling-rubber-brushes-and-a-mopping-pad-behind-them.jpg?id=31828620&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Just aft of the brushes, three black nozzles dispense water in advance of the mopping pad.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">iRobot</small>
</p><p>
	Since all of this happens in the same footprint as the original j7, there are some compromises to make room for the mopping system. This mostly happens with the bin, which now accommodates a water reservoir, taking up some of the space where you’d otherwise find dirt. It’s not a huge deal, though, because Roombas with the automatic Clean Base (including the j7C) will empty their own bins when they get full and then resume vacuuming where they left off. So, that may happen an extra time or two during the j7C’s cleaning cycle, but it’s not something the user has to worry about.
</p><p>
	What the user does have to worry about, unfortunately, is the water reservoir. You access it by removing the j7C’s bin, and then you can fill the reservoir at a sink. One fill is enough for a thousand square feet of coverage on eco-mode, and there’s also normal mode and a double-pass mode that you can select in the app for dirtier floors.
</p><p>
	This water-filling process is easy, but it’s a user-dependent step, which sadly breaks the magic of the automatic dirt-emptying Roombas where you basically don’t have to think about them for weeks at a time. Water isn’t required, at least, and if the robot detects an empty reservoir, it’ll just default to vacuuming everything instead of mopping. Other companies have approached this problem with docks that include water reservoirs able to automatically refill a robot multiple times, and I have some faith that iRobot is already working on a way of doing this more elegantly.
</p><p>
	Personally, I’m hoping for a water shuttle robot: some little bot with a small tank that zips back and forth between the dock and, say, a water dispenser hooked into the toilet fill line in your bathroom to provide refills on demand.
</p><h3>Q&A With iRobot CEO Colin Angle</h3><br/><p>
	We asked iRobot CEO Colin Angle to explain why the company feels that this is the right approach to a hybrid mopping and vacuuming robot, and how the heck it came up with this system in the first place.
</p><p>
<strong>One of the greatest things about the Roomba is that it does one thing very well. Often with robots, trying to make more of a generalist robot results in significant compromises. What made you decide to shift from dedicated vacuuming and mopping robots, to one robot that does both tasks?</strong>
</p><p>
<strong>Angle: </strong>The short answer is that we finally figured out how to do it. We’re not the first company with a two-in-one robot on the market. What took us so long? Well, it took us this long to solve the problem.
</p><p>
	There’s a very interesting history of mopping robots at iRobot, where we had the Scooba that put down water, scrubbed, and vacuumed the water up again. And then we started getting into mopping pads with the Scooba 230 and the Braava. Although we had a lot of skepticism about the idea of capturing the water and dirt in the pad and we didn’t know how to do it at the beginning, it proved to be a very successful strategy. Then the question was, could we figure out a mechanism that would allow edge-to-edge cleaning with a mop on a Roomba platform?
</p><p>
	Our early attempts were not successful, and that purist view of keeping the vacuuming and mopping robots separate held for a long time, but we recognized that the convenience of true on-the-fly switching would be a real and tangible customer benefit and would allow us to save on costs because we wouldn’t be duplicating much of the hardware across two separate robots.”
</p><p>
<strong>So how did you end up at the solution of moving the mopping pad all the way from the bottom of the robot to the top?</strong>
</p><p>
<strong>Angle: </strong>It’s certainly not where we went at first! In the annals of iRobot history are dozens of flawed ideas around how we could do this, actually dating back to some of the original designs for the original Roomba, because it was always something we’d been thinking about. The idea of having something on the bottom and pulling a plastic screen over the mopping pad got real consideration, but there are some bad failure modes there.
</p><p>
	The guy who came up with this is one of our principal engineers, who has been with iRobot maybe the longest besides me at this point. I convinced him to join the company from [the Jet Propulsion Laboratory]. He’s the one who said, “Well, why don’t we just use a belt drive and arms?” and everyone looked at him like, “Are you insane?” And so he built it, and proved to us that it could work, which is his normal way of convincing us that he’s right and we’re wrong. And it’s brilliant! It sounds like a crazy approach to solving the problem, but when you see it, it makes sense.
</p><p>
	iRobot is also announcing some hefty software updates in the form of iRobot OS 5.0. The j7 Roombas have front-facing cameras that are able to do all sorts of things, and last we checked, that included identifying and avoiding four different classes of floor-dwelling objects. iRobot OS 5.0 brings that number up to 80 [!], and obstacles now trigger different behaviors besides avoidance.</p><p>Litter boxes and pet bowls, for example, can be given special attention because they tend to be dirtier areas. Same with toilets, dishwashers, and ovens. With a voice assistant, you can now also yell at your robot to skip the room that you’re in, and it’ll come back to it later, which is great for those of us who feel like our robots actively seek us out whenever they have cleaning to do.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The robot mopping a hardwood floor with scattered cat toys around it and a cat in the background" class="rm-shortcode" data-rm-shortcode-id="96f1dd41525f2cbe4fc289e339aa3bf2" data-rm-shortcode-name="rebelmouse-image" id="2de29" loading="lazy" src="https://spectrum.ieee.org/media-library/the-robot-mopping-a-hardwood-floor-with-scattered-cat-toys-around-it-and-a-cat-in-the-background.jpg?id=31828632&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">iRobot</small>
</p><p>
	iRobot told us that its j7 vacuums have received the TÜV SÜD Cyber Security Mark, a stringent third-party endorsement of iRobot’s security practices, meaning that iRobot has invested heavily in making sure that the data that it has in its possession is kept safe from external hackers. This is good, for sure, but frankly I don’t get the sense that folks are nearly as worried about their data getting 
	<em>stolen</em> from iRobot by hackers as they are about their data getting <em>intentionally leveraged</em> by iRobot (or iRobot’s future owner) in a way that is contrary to users’ interests, although iRobot has promised that it will never sell your data to other companies. </p><p>Until we have a better idea of <a href="https://spectrum.ieee.org/amazon-irobot-acquisition" target="_blank">what exactly is happening with the Amazon merger</a>, it’s probably best to remain cautious. We did ask Colin Angle what the options are if you want to keep your data completely private while still using your Roomba, and here’s what he told us:
</p><blockquote>
	“It depends on what your comfort level is. You don’t have to turn on mapping, and you certainly don’t have to ever store or share any image from your home. If you want to clean by room, we need to remember where your rooms are. We’re going to remember them as polygonal objects—we’re not going to have any idea what they look like. We are really trying to make sure that we only store that data that the robot actually needs to do the job. If you don’t want to clean by room or build a map, you can still have the robot operate and switch between modes and benefit from the avoidance technologies and do the right things. But there are different levels that will hopefully satisfy most different levels of concern.”
</blockquote><p>
	We also learned that 
	<a href="https://spectrum.ieee.org/irobot-completely-redesigns-its-floor-care-robots-with-new-m6-and-s9" target="_self">iRobot’s top-of-the-line s9, which features a decidedly non-Roomba like square front plus a 3D sensor</a>, is not the direction that iRobot will be moving in. Historically, iRobot has released premium Roombas like the s9, and then the tech in them trickles down into less expensive robots over time. But it sounds like the s9, while still iRobot’s most powerful vacuum, couldn’t justify its fancy and expensive sensor or nonround form factor to the extent that would be necessary to influence future generations of Roombas. </p><p>“This is all a journey,” Angle told us. “With the costs inherent in building the s9 robot, we felt like we could go another way and put more CPU power in and really adopt an architecture around computer vision that would be more flexible than the dense 3D point-cloud sensor in the s9. The technology is moving so fast on the visual understanding and machine learning side, that it’s a better long-term bet to get behind. 3D sensing will come back, but it may come back as depth from vision. And the square front has some advantages—speed of clean is a benefit, but improved mission completion [of round Roombas] is a bigger benefit. I think the architecture of the J series robot is the go-forward architecture.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Top down picture of the robot on its clean base in a hallway" class="rm-shortcode" data-rm-shortcode-id="123bdbba02188a2b1e34fa28b7970da4" data-rm-shortcode-name="rebelmouse-image" id="2b17c" loading="lazy" src="https://spectrum.ieee.org/media-library/top-down-picture-of-the-robot-on-its-clean-base-in-a-hallway.jpg?id=31828629&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">iRobot</small>
</p><p>
	Angle acknowledges that the architecture of the J series, and of the j7C in particular, makes it a high-end robot, and you could buy a Roomba and a Braava together for much less money. But this is how iRobot does things—offering premium robots with new features and capabilities for high prices, and eventually we’ll see the costs come down in the form of more affordable generations of robot. “This is definitely an exciting path forward for us,” says Angle.
</p><p>
	The iRobot Roomba Combo j7+ is available for preorder now for $1,099 in the United States, and will be available in Canada and Europe in early October.
</p>]]></description><pubDate>Tue, 27 Sep 2022 04:08:21 +0000</pubDate><guid>https://spectrum.ieee.org/irobot-roomba-combo-j7-vacuum</guid><category>Irobot</category><category>Roomba</category><category>J7</category><category>Cleaning robots</category><category>Vacuum robots</category><category>Robotics</category><category>Irobot</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-round-black-robot-vacuum-with-a-mopping-pad-that-can-move-from-below-the-robot-to-above-the-robot-and-out-of-the-way.jpg?id=31828592&amp;width=980"></media:content></item><item><title>Flying Robots 3D-Print Structures in Flight</title><link>https://spectrum.ieee.org/drone-3dprint</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-drone-flying-in-the-air-one-printing-a-3d-structure.jpg?id=31802792&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>Flying 3D-printing robots modeled after wasps and birds may one day repair and build structures at remote sites beyond the reach of standard construction teams, a new study finds.<br/></p><p><a href="https://spectrum.ieee.org/robotic-construction-platform-creates-large-buildings-on-demand" target="_self">Construction robots</a> that can 3D-print structures on sites may one day prove faster, safer, and more productive than human teams. However, construction robotics currently focuses mostly on ground-based robots. This approach is limited by the heights a robot can reach, and large-scale systems that require tethering to a power supply are limited in where they can be deployed.</p><p>In the new study, researchers drew inspiration from flying animals that are highly adept at construction. For instance, to incrementally build its nest, a barn swallow can overcome the limited payload it can carry in one flight by typically making some 1,200 trips between where it gets its construction material and its construction site.</p><p class="pull-quote">“When animals want to build something large, they work together in swarms or collectives to do so.”<br/>—Mirko Kovač, Imperial College London</p><p>The new robot fleet the researchers developed, which they call their aerial additive manufacturing system, can collectively and autonomously 3D-print structures while in flight. The fleet consists of two kinds of untethered quadrotor drones—BuilDrones that deposit materials in layers from nozzles, and ScanDrones that use standard optical cameras to continuously map the structures in 3D and monitor their quality.</p><p>“This combination of manufacturing and scanning with flying robots is very new,” says study senior author Mirko Kovač, a roboticist at Imperial College London and at the Swiss Federal Laboratories for Materials Science and Technology in Dübendorf, Switzerland.</p><p>The drones work cooperatively from a single blueprint, adapting to variations in the geometry of the structure in real time as construction progresses. The robots are fully autonomous while flying, but a human supervisor can monitor data from the drones and intervene when necessary.</p><p>A question often asked about this approach is “Can you build something with one drone when one drone can carry relatively little?” Kovač says. The key to this strategy is not just using one drone “but many drones working together, which is what is also seen in the animal kingdom. When animals want to build something large, they work together in swarms or collectives to do so.”</p><p>The researchers developed four different mixtures with which the robots 3D printed. The robots, their software, the materials they build with, and the architecture they end up constructing all need to be designed together, Kovač says, an approach the researchers call “<a href="https://www.nature.com/articles/s42256-020-00258-y" target="_blank">physical artificial intelligence</a>.”</p><p>“We’re not just taking some material and putting it on a robot—the evolution of the material itself can be quite complex, and tailored to being integrable with a robot that has a relatively low payload,” Kovač says.</p><p>In experiments, the drones could manufacture a roughly 2-meter-high, 30-centimeter-wide, 72-layer cylinder from polyurethane insulation foam in 29 minutes. They could also build a 18-cm-high, 33-cm-wide, 28-layer cylinder from a cementlike material in 133 minutes. All in all, they achieved a manufacturing accuracy of 5 millimeters, acceptable within United Kingdom building requirements.</p><p>The scientists note their approach is potentially scalable to large numbers of robots working as a team. Potential applications may include “work at height, or in areas that are inaccessible—for example, facades of buildings, or remote structures that need very fast repair, such as pipelines,” Kovač says. Other potential uses may include construction in hostile locations or after natural disasters, the researchers say.</p><p>“We are now working on case studies with industrial partners to apply our approach to industrial problems,” Kovač says. “We may tailor our drones to one use case or the other—this may include bigger drones, or slightly different designs of drones.”</p><p>Kovač, along with architect Robert Stuart-Smith at University College London and the University of Pennsylvania in Philadelphia and their colleagues, detailed <u><a href="https://www.nature.com/articles/s41586-022-04988-4" target="_blank">their findings</a></u> online 21 September in the journal <em>Nature</em>.</p>]]></description><pubDate>Sun, 25 Sep 2022 21:56:53 +0000</pubDate><guid>https://spectrum.ieee.org/drone-3dprint</guid><category>3d printing</category><category>Construction robots</category><category>Biomimicry</category><category>Biomimetics</category><category>Additive manufacturing</category><category>Drones</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-drone-flying-in-the-air-one-printing-a-3d-structure.jpg?id=31802792&amp;width=980"></media:content></item><item><title>Video Friday: Humans Helping Robots</title><link>https://spectrum.ieee.org/video-friday-humans-helping-robots</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-human-with-two-white-robotic-arms-strapped-to-their-arms.png?id=31804827&width=1200&height=800&coordinates=145%2C0%2C155%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><p>Until robots achieve 100 percent autonomy (HA), humans are going to need to step in from time to time, and Contoro is developing a system for intuitive, remote human intervention.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="571756755c7579c5fcbe7adc449bc615" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/enIiznDZEdk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.contoro.com/">Contoro</a> ]</p><p>Thanks, Youngmok!</p><div class="horizontal-rule"></div><blockquote><em>A one year update of our ongoing project with Ontario Power Generation (OPG) and RMUS Canada to investigate the capabilities of Boston Dynamics’ Spot robot for autonomous inspection and first response in the power sector.  Highlights of the first year of the project, featuring the work of Ph.D. student Christopher Baird, include autonomous elevator riding and autonomous door opening (including proxy card access doors) as part of Autowalks, as well as autonomous firefighting.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c4e22bc84da18776ba3029610ccb930e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ws_V5wNQfts?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://mars.engineering.uoit.ca/">MARS Lab</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Teams involved in DARPA’s Robotic Autonomy in Complex Environments with Resiliency (RACER) program have one experiment under their belts and will focus on even more difficult off-road landscapes at Camp Roberts, California, September 15–27. The program aims to give driverless combat vehicles off-road autonomy while traveling at speeds that keep pace with those driven by people in realistic situations.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="89a0fb3cef0bf1404d942c9d0122c89c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5zf-sUSa5sg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.darpa.mil/news-events/2022-09-16">DARPA</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Tool use has long been a hallmark of human intelligence, as well as a practical problem to solve for a vast array of robotic applications. But machines are still wonky at exerting just the right amount of force to control tools that aren’t rigidly attached to their hands. To manipulate said tools more robustly, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), in collaboration with the Toyota Research Institute (TRI), have designed a system that can grasp tools and apply the appropriate amount of force for a given task, like squeegeeing up liquid or writing out a word with a pen. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3a221ce222d32ff2158b52ed8a64e2c5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/JESNwiFxsEQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://news.mit.edu/2022/soft-robots-grasp-right-amount-force-0922">MIT</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Cornell researchers installed electronic “brains” on solar-powered robots that are 100 to 250 micrometers in size, so the tiny bots can walk autonomously without being externally controlled.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="33913786ace1f5ec26b92ddc67e030bb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/bCjnekohBAY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://news.cornell.edu/stories/2022/09/brains-board-smart-microrobots-walk-autonomously">Cornell</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Researchers at the University of California, San Diego, have developed soft devices containing algae that glow in the dark when experiencing mechanical stress, such as being squished, stretched, twisted or bent. The devices do not require any electronics to light up, making them an ideal choice for building soft robots that explore the deep sea and other dark environments, researchers said.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="102df6add6c39fece627c377d41dd25e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/JhwUB7KIex4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://jacobsschool.ucsd.edu/news/release/3507">UCSD</a> ]</p><p>Thanks, Liezel!</p><div class="horizontal-rule"></div><blockquote><em>Our robotaxi is built to withstand a range of temperatures to ensure that the vehicle, and most importantly, its riders are never too hot or too cold...no matter the weather. Learn more about our thermal testing in the latest episode of Putting Zoox to the Test.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="49b47ed98f1f59ad67d272dd3e36b19f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/G-kZEwDpNuA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://zoox.com/journal/labbot/">Zoox</a> ]</p><p>Thanks, Whitney!</p><div class="horizontal-rule"></div><p>Skydio drones will do an excellent job of keeping you in frame, whatever happens.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="95d541c9d0b86f3c0e868a13755a669d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/i96xojqDHTo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.skydio.com/">Skydio</a> ]</p><div class="horizontal-rule"></div><blockquote><em>With the accelerated urbanization in the world, the development and utilization of the underground space are important for economic and social development and the survival of people’s lives is important for all of us. Zhejiang University Huzhou Research Institute convened a robot team to conduct an underground space unknown environment exploration adventure in Yellow dragon cave. DEEP Robotics participate in this fascinated robot party and try out the underground challenges, also team up with the drone team (air-ground robot) to seek new collaboration.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f28e8fac04d4e6b3b5ed1b11f89fca0c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/0mhB6gx0S2g?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.deeprobotics.cn/en/">Deep Robotics</a> ]</p><div class="horizontal-rule"></div><p>The title of this video is “Ion Propulsion Drone Proves Its Commercial Viability,” but it seems like quite a leap from a 4.5-minute flight to reaching the 15-minute flight with a significant payload that would be required for last-mile delivery.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9b7c165e066ffb59f09a0a674ff7673c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yM_ZcN3dleo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.undefinedtechnologies.com/post/silent-ion-propulsion-drone-proves-its-commercial-viability">Undefined Technologies</a> ]</p><div class="horizontal-rule"></div><p>Welcome to this week’s edition of “How much stuff can you cram onto a Husky?”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ff8b5ebf6e92b8e581b39b658a4d59df" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YkOhs1QDKdk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://clearpathrobotics.com/husky-unmanned-ground-vehicle-robot/">Clearpath</a> ]</p><div class="horizontal-rule"></div><blockquote><em>In the Nanocopter AI challenge the teams demonstrated the AI they developed for Bitcraze AB’s Crazyflie nanocopters to perform vision-based obstacle avoidance at increasing speeds. The drones flew around in our “Cyberzoo,” avoiding a range of obstacles, from walls to poles and artificial plants. The drones were primarily scored on the distance they covered in the limited time but could gain extra points when flying also through gates.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c763d5b3900c0a801eb7cd0cb0db2dc7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/NFeLwGfJ1so?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://2022.imavs.org/">IMAV</a> ]</p><div class="horizontal-rule"></div><p>Watch this drone deliver six eggs to an empty field!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4f39f2424d4f2b4f6903c072a1052eec" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/cRRiNMG1SK0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Sorry, I shouldn’t be so snarky, but I’m still not sold on the whole urban drone delivery of groceries thing.</p><p>[ <a href="https://wing.com/">Wing</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Flexiv is pleased to announce the launch of its ROS 2 driver to bring a better robot development experience for customers.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ab9b8ee5b0fc5db231ee15e92a2066ba" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/d8UBa4hr9b8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.flexiv.com/en/">Flexiv</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Northrop Grumman has been pioneering new capabilities in the undersea domain for more than 50 years. Manta Ray, a new unmanned underwater vehicle, taking its name from the massive “winged” fish, will need to be able to operate on long-duration, long-range missions in ocean environments without need for on-site human logistics support—a unique but important mission needed to address the complex nature of undersea warfare. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1080d5f8fb3b94d44db482f4aa1b911f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/FsJEwvhvVak?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.northropgrumman.com/what-we-do/sea/mission-unlimited-inventing-autonomous-recharging-of-unmanned-underwater-vehicles/">Northrop Grumman</a> ]</p><div class="horizontal-rule"></div><p>Some unique footage from drones that aren’t scared of getting a little wet.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4958d107565907dc6b4a1678c3b79d40" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/vMHnNQpZ3N4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.instagram.com/_blastr/">Blastr</a> ]</p><div class="horizontal-rule"></div><blockquote><em>People tend to overtrust sophisticated computing devices, especially those powered by AI. As these systems become more fully interactive with humans during the performance of day-to-day activities, ethical considerations in deploying these systems must be more carefully investigated. In this talk, we will discuss various forms of human overtrust with respect to these intelligent machines and possible ways to mitigate the impact of bias in our interactions with them.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f239568bfbe2c75a7b6d28b579578f4c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DQ1YbB9D9Gk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://cait.engineering.columbia.edu/events/cait-distinguished-lecture-series-ayanna-howard-good-and-ugly-how-robots-and-ai-influence">Columbia</a> ]</p><div class="horizontal-rule"></div><blockquote><em>The Jet Propulsion Laboratory’s success in landing the low-cost Mars Pathfinder mission in 1997 was viewed as proof that spacecraft could be built more often and for far less money—a radical cultural change NASA termed “Faster, Better, Cheaper.” The next challenge taken on by JPL was to fly two missions to Mars for the price of the single Pathfinder mission. Mars Climate Orbiter and the Mars Polar Lander both made it to the launchpad, on time and on budget, but were lost upon arrival at Mars, resulting in one of the most difficult periods in the history of JPL. “</em>The Breaking Point<em>” tells the story of the demise of these two missions and the abrupt end of NASA’s “Faster, Better, Cheaper” era.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f9dc3aa355ef8279da6d5192d23b8c08" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/kuYDkVRyMkg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.jpl.nasa.gov/who-we-are/documentary-series-jpl-and-the-space-age">JPL</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 23 Sep 2022 18:05:01 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-humans-helping-robots</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-photo-of-a-human-with-two-white-robotic-arms-strapped-to-their-arms.png?id=31804827&amp;width=980"></media:content></item><item><title>Circle of Circuits</title><link>https://spectrum.ieee.org/robocup-2022</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.jpg?id=31727864&width=980"/><br/><br/><p>
	The Big Picture features technology through the lens of photographers.
</p><p>
	Every month, <em>IEEE Spectrum</em> selects the most stunning technology images recently captured by photographers around the world. We choose images that reflect an important advance, or a trend, or that are just mesmerizing to look at. We feature all images on our site, and one also appears on our monthly print edition.
</p><p>
	Enjoy the latest images, and if you have suggestions, leave a comment below.</p><h3>RoboCup Class Picture</h3><br/><img alt="Humanoid robots and their human handlers gather in a circle for a group photo." class="rm-shortcode" data-rm-shortcode-id="6f7604b9c6782dc39e3688b9015516d7" data-rm-shortcode-name="rebelmouse-image" id="ffa46" loading="lazy" src="https://spectrum.ieee.org/media-library/humanoid-robots-and-their-human-handlers-gather-in-a-circle-for-a-group-photo.jpg?id=31704707&width=980"/><p>
	Have you ever been awed by the pageantry of the parade of nations in the opening ceremony of the Olympic Games? Then this photo, featuring more than 100 Nao programmable educational robots, two Pepper humanoid assistive robots, and their human handlers, should leave you similarly amazed. It was taken at the end of this year’s RoboCup 2022 in Bangkok. After two years during which the RoboCup was scuttled by the global pandemic, the organizers were able to bring together 13 robot teams from around the world (with three teams joining in remotely) to participate in the automaton games. The spirit of the gathering was captured in this image, which, according to RoboCup organizers, shows robots with a combined market value of roughly US $1 million.
</p><p class="caption">
	Patrick Göttsch and Thomas Reinhardt
</p><h3></h3><br/><img alt="A satellite containing a cellular base station is shown in orbit." class="rm-shortcode" data-rm-shortcode-id="98d7ff2a24d51c2e93a43c6b89e86837" data-rm-shortcode-name="rebelmouse-image" id="57f47" loading="lazy" src="https://spectrum.ieee.org/media-library/a-satellite-containing-a-cellular-base-station-is-shown-in-orbit.jpg?id=31717983&width=980"/><h3>
<a href="https://spectrum.ieee.org/5g-satellite" target="_blank">Longest-Distance Calls</a>
</h3><p>
	When you’re traveling to faraway destinations, it’s comforting to know that you can remain in contact with the folks back home, no matter how far you roam. Still, it’s surprisingly easy to end up somewhere that has poor cellular reception or none to speak of. That’s because only about 10 percent of the world’s surface is in cellular coverage zones. But in April 2022, a company called Lynk launched Lynk Tower 1, poised to be the world’s first commercial satellite cell tower, into space. The cell tower, pictured here, is said to be the first of four that Lynk plans to launch into orbit this year. Once they’re in place and contracts with terrestrial cellular service providers are set up, the 4 billion people who hardly ever have adequate cellular reception will finally be able to respond in the plural when asked “How many bars you got?”
</p><p class="caption">
	Lynk Global
</p><h3></h3><br/><img alt="Steam rises from a set of specially arranged tubes that yield a cooling effect." class="rm-shortcode" data-rm-shortcode-id="be134545cfa9e54c363d316c1ec9d6b3" data-rm-shortcode-name="rebelmouse-image" id="3bac5" loading="lazy" src="https://spectrum.ieee.org/media-library/steam-rises-from-a-set-of-specially-arranged-tubes-that-yield-a-cooling-effect.jpg?id=31717988&width=980"/><h3>
<a href="https://spectrum.ieee.org/ai-3d-printing-better-ac" target="_blank">Self-Made Manufacturing</a>
</h3><p>
	What’s more 
	<em>meta</em> than using a 3D printer to make parts for a 3D printer? This device looks like a bunch of separate tubes packaged together. But it is actually a single unit that was built that way inside a 3D printer. It is a precision-engineered heat exchanger—optimized to improve the cooling of shielding gas that keeps impurities from fouling the additive manufacturing process that occurs inside an industrial 3D printer. No paper jams here.
</p><p class="caption">
	Hyperganic
</p><h3></h3><br/><img alt="White plastic rectangular patch with four buttons is stuck on the back of a person\u2019s hand. Beads of water on the hand run off the waterproof patch." class="rm-shortcode" data-rm-shortcode-id="59748baebc1a755c56034a987d9b8a7f" data-rm-shortcode-name="rebelmouse-image" id="9ffe8" loading="lazy" src="https://spectrum.ieee.org/media-library/white-plastic-rectangular-patch-with-four-buttons-is-stuck-on-the-back-of-a-person-u2019s-hand-beads-of-water-on-the-hand-run-o.jpg?id=31717990&width=980"/><h3>
<a href="https://spectrum.ieee.org/a-self-powered-waterproof-hmi " target="_blank">None the Worse for Wearable</a>
</h3><p>
	How are we to benefit from the physical and cognitive enhancements that electronic wearables could someday provide if everyday aspects of human life such as breaking a sweat are hazardous to these devices? Not to worry. In a recent paper, a research team at the University of California, Los Angeles, reported that it has the problem licked. They developed a human-machine interface that is impervious to moisture. And, as if being waterproof weren’t enough, the four-button device has been engineered to generate enough electric current to power its own operation when any of the buttons is pressed. So, it can go just about anywhere we go, with no concerns about spills, splashes, sweat, or spent batteries.
</p><p class="caption">
	JUN CHEN RESEARCH GROUP/UCLA
</p>]]></description><pubDate>Wed, 21 Sep 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/robocup-2022</guid><category>3d printer</category><category>Cellular telephony</category><category>Heat exchanger</category><category>Lynk</category><category>Robocup</category><category>Robots</category><category>Satellite</category><category>The big picture</category><dc:creator>Willie Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/31727864/origin.jpg"></media:content></item><item><title>Zero Trust Approach to Systems Security for Autonomous Systems</title><link>https://engineeringresources.spectrum.ieee.org/free/w_defa3180/prgm.cgi</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/zero-trust-approach-to-systems-security-for-autonomous-systems.gif?id=31786872&width=1200&height=800&coordinates=9%2C0%2C10%2C0"/><br/><br/><p>Autonomous systems sit at the intersection of AI, IoT, cloud architectures, and agile software development practices. Various systems are becoming prominent, such as unmanned drones, self-driving cars, automated warehouses, and managing capabilities in smart cities. Little attention has been paid to securing autonomous systems as systems composed of multiple automated components. Various patchwork efforts have focused on individual components.</p> <p>Cloud services are starting to adopt a Zero Trust approach for securing the chain of trust that might traverse multiple systems. It has become imperative to extend a Zero Trust architecture to systems of autonomous systems to protect not only drones, but also industrial equipment, supply chain automation, and smart cities.</p>]]></description><pubDate>Tue, 20 Sep 2022 13:57:26 +0000</pubDate><guid>https://engineeringresources.spectrum.ieee.org/free/w_defa3180/prgm.cgi</guid><category>Type:whitepaper</category><dc:creator>Technology Innovation Institute</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/zero-trust-approach-to-systems-security-for-autonomous-systems.gif?id=31786872&amp;width=980"></media:content></item><item><title>Video Friday: Loona</title><link>https://spectrum.ieee.org/video-friday-loona</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-small-white-robot-with-four-wheels-moving-ears-and-an-expressive-animated-face-reacts-cutely-to-being-pet-by-a-human.gif?id=31723400&width=1200&height=800&coordinates=52%2C0%2C53%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>
	Enjoy today's videos!
</p><hr/><div style="page-break-after: always">
<span style="display:none"> </span>
</div><p>
	Another robotic pet on Kickstarter, another bunting of red flags.
</p><p>
<iframe frameborder="0" height="450" scrolling="no" src="https://www.kickstarter.com/projects/keyitechnology/meet-loona-the-petbot-you-will-fall-in-love/widget/video.html" width="800"> </iframe>
</p><p>
	Let's see, we've got: "she's so playful and affectionate you'll forget she's a robot." "Everything you can dream of in a best friend and more." "Get ready to fall in love!" And that's literally like the first couple of tiles on the Kickstarter post. Look, the hardware seems fine, and there is a lot of expressiveness going on, I just wish they didn't set you up for an inevitable disappointment when after a couple of weeks it becomes apparent that yes, this is just a robotic toy, and will never be your best friend (or more).</p><p>Loona is currently on Kickstarter for about USD $300.
</p><p>
	[ <a href="https://www.kickstarter.com/projects/keyitechnology/meet-loona-the-petbot-you-will-fall-in-love">Kickstarter</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>Inspired by the flexibility and resilience of dragonfly wings, we propose a novel design for a biomimetic drone propeller called Tombo propeller. Here, we report on the design and fabrication process of this biomimetic propeller that can accommodate collisions and recover quickly, while maintaining sufficient thrust force to hover and fly. </em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="ba6c1bc0ca091b43ba199c8d99902f36" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/zjHvukgfJwc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="http://www.jaist.ac.jp/ms/labs/vanho/publications-e.html">JAIST</a> ]
</p><p>
	Thanks Van!
</p><div class="horizontal-rule">
</div><blockquote>
<em>Meet Tom, a software engineer at Boston Dynamics, as he shares insights on programming and testing the practical—and impractical—applications of robotics. Whether Spot is conducting inspections or playing an instrument, learn how we go from code on a computer to actions in the real world.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="a18e39f15e702e221ab77548e98f76cf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TnEj6L-Z2nU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Yeah, but where do I get that awesome shirt?!
</p><p>
	[ <a href="https://www.bostondynamics.com/">Boston Dynamics</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>This Ameca demo couples automated speech recognition with GPT 3 —a large language model that generates meaningful answers—the output is fed to an online TTS service which generates the voice and <a href="https://en.wikipedia.org/wiki/Viseme" target="_blank">visemes</a> for lip sync timing. The team at Engineered Arts Ltd. pose the questions.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="74eae37318b4d42e88571786ffb62b74" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/EWACmFLvpHE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	"Meaningful answers."
</p><p>
	[ <a href="https://www.engineeredarts.co.uk/robot/ameca/">Engineered Arts</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>The ANT project develops a navigation and motion control system for future walking systems for planetary exploration. After successful testing on ramps and rubble fields, the challenge of climbing rough inclines such as craters is being tackled.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c3a9dd0da6db2ee66b68e90084513339" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6UsJl_mVMzw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.dfki.de/web/forschung/projekte-publikationen/projekte-uebersicht/projekt/ant">DFKI</a> ]
</p><div class="horizontal-rule">
</div><p>
	Look, if you’re going to crate-train Spot, at least put some blankets and stuffed animals in there or something.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="f24ca189d87c778e74907f8cdb913026" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/KFk8cTwIwvs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.energy-robotics.com/">Energy Robotics</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>With multitrade layout, all trades’ layouts are set down with a single pass over the floor by Dusty's FieldPrinter system. Trades experience unparalleled clarity and communication with each other, because they can see each others’ installation plans and immediately identify and resolve conflicts. Instead of fighting over the floor and pointing fingers, they start to solve problems together.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="fe0b3deb7c60238a02f9bad692d8f7e8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6HBl5BUiGs4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.dustyrobotics.com/">Dusty Robotics</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>We present QUaRTM—a novel quadcopter design capable of tilting the propellers into the forward flight direction, which reduces the drag area and therefore allows for faster, more agile, and more efficient flight.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="6b7bead0daef3361f186f76676df9d1c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/SaRGTph3AqU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://hiperlab.berkeley.edu/">HiPeRLab</a> ]
</p><div class="horizontal-rule">
</div><p>
	Is there an option in the iRobot app to turn my Roomba into a cake? Because I want cake.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="3a152a817ad0b8771601a6f5b564c4bf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/4JaZSfJIPZs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.irobot.com/en_US/roomba.html">iRobot</a> ]
</p><div class="horizontal-rule">
</div><p>
	Looks like SoftBank is getting into high-density robotic logistics.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="24b8d9344945b1daedc35f1524f75a13" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yD6BuKohzDw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.watch.impress.co.jp/docs/news/1439328.html">Impress</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>GITAI S2 ground test for space debris removal. During this demonstration, a tool changer was also tested to perform several different tasks at OSAM.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bf3c6d141c568d719543a1feecbd48fc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/1Sbqu_UB9SE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://gitai.tech/en/product/s1/">GITAI</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>Recent advances allow for the automation of food preparation in high-throughput environments, yet the successful deployment of these robots requires the planning and execution of quick, robust, and ultimately collision-free behaviors. In this work, we showcase a novel framework for modifying previously generated trajectories of robotic manipulators in highly detailed and dynamic collision environments.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="e003afb806bae46c131ec827dd83b53d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/nmkbya8XBmw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://arxiv.org/abs/2205.01026">Paper</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>The LCT Hospital in South Korea uses “Dr. LCT” for robotic-based orthopedic knee procedures. The system is based on the KUKA LBR Med robotic platform, which is ideally suited for orthopedic surgery with its seven axes, software developed specifically for medical technology, and appropriate safety measures.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="bd23f4341e5c6ac98b569429c5f505f7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GU1GKt0HeHc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.kuka.com/lbr-med">Kuka</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>A year in review. Compilation of 2022 video highlights of the Game Changing Development (GCD) Program. The Game Changing Development Program is a part of NASA’s Space Technology Mission Directorate. The program advances space technologies that may lead to entirely new approaches for the agency’s future space missions and provide solutions to significant national needs.</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="e8f0887e311765355a25149a166eb233" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/bqFZb3wgq8Y?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.nasa.gov/directorates/spacetech/game_changing_development/index.html">NASA</a> ]
</p><div class="horizontal-rule">
</div><p>
	Naomi Wu reviews a Diablo mobile robot (with some really cool customizations of her own), sending it out to run errands in Shenzhen during lockdown.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="33fe8f293a26099877ff04292d102e8a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/cleaXYtY2fM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://www.youtube.com/c/SexyCyborg">Naomi Wu</a> ]
</p><div class="horizontal-rule">
</div><p>
	Roundtable discussion on how teaching automation in schools, colleges, and universities can help shape the workers of tomorrow. ABB Robotics has put together a panel of experts in this field to discuss the challenges and opportunities.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="7d14b20e6024f81d6ac5d53bb28f1ee5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dYLL859BGBA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="https://new.abb.com/products/robotics/industries/robotics-in-education">ABB</a> ]
</p><div class="horizontal-rule">
</div><blockquote>
<em>On 8 September 2022, Mario Santillo of Ford talked to robotics students as the first speaker in the Undergraduate Robotics Pathways & Careers Speaker Series, which aims to answer the question “What can I do with a robotics degree?”</em></blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="84aa98bb247d6d5170d035f016a216ce" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6udKYi-sX5Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ <a href="">Michigan Robotics</a> ]
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Fri, 16 Sep 2022 18:19:52 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-loona</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/a-small-white-robot-with-four-wheels-moving-ears-and-an-expressive-animated-face-reacts-cutely-to-being-pet-by-a-human.gif?id=31723400&amp;width=980"></media:content></item><item><title>Faster, Meaner, Deadlier: The Evolution of “BattleBots”</title><link>https://spectrum.ieee.org/battlebots-tech-evolution</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-green-and-purple-battlebot-hovers-in-the-air-amid-a-large-number-of-sparks-below-it-is-a-purple-and-red-battlebot.jpg?id=31708905&width=1200&height=800&coordinates=0%2C67%2C0%2C68"/><br/><br/><p>Earlier this year, <a href="https://spectrum.ieee.org/u/anouk-wipprecht" target="_blank">friend-of-</a><em><a href="https://spectrum.ieee.org/u/anouk-wipprecht">IEEE Spectrum</a></em> and fashiontech designer <a href="http://www.anoukwipprecht.nl/bio" target="_blank">Anouk Wipprecht</a> gave a peek of what it’s like to be a competitor on “BattleBots,” the 22-year-old robot-combat competition, from the preparation “pit” to the arena. Her team, <a href="https://battlebots.com/robot/ghost-raptor-2021/" target="_blank">Ghostraptor</a>, was knocked out of the regular competition after losing its <a href="https://spectrum.ieee.org/inside-story-of-battlebots" target="_blank">first</a> and <a href="https://spectrum.ieee.org/battlebot-carnage" target="_blank">second</a> fights, though they regained some glory by winning a round in the bonus Golden Bolt tournament, which recently finished airing on <a href="https://www.tbs.com/shows/battlebots/season-6/episode-15/champions-let-the-slugfest-begin" target="_blank">the TBS TV channel</a>.</p><p>This week, <a href="https://battlebots.com/tickets/" target="_blank">tickets went on sale</a> for audience seating for the next season of “BattleBots”; filming will commence in October in Las Vegas. We thought it was a good moment to get a different perspective on the show, so <em>Spectrum</em> asked one of the founders of “BattleBots” and its current executive producer, <a href="https://www.linkedin.com/in/megagreg" target="_blank">Greg Munson</a>, about how two decades’ worth of technological progress has impacted the competition. <br/></p><p><strong>What are the biggest changes you’ve seen, technology-wise, over 20 years or so?</strong></p><p><strong>Greg Munson:</strong> Probably the biggest is battery technology. “BattleBots” premiered on Comedy Central in, I think it was, 2000. Now we’re 22 years later. In the early days, people were using car batteries. Then NiCad packs became very popular. But with the advent of lithium technology, when the battery packs could be different sizes and shapes, that’s when things just took off in terms of power-to-weight ratio. Now you can have these massively spinning disk weapons, or bar weapons, or drum weapons that can literally obliterate the other robot.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A bearded man in a black hat and t-shirt that say Battlebots" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="059a5cf434bfa97bdde2a79a781eebc1" data-rm-shortcode-name="rebelmouse-image" id="04f07" loading="lazy" src="https://spectrum.ieee.org/media-library/a-bearded-man-in-a-black-hat-and-t-shirt-that-say-battlebots.jpg?id=31709031&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Greg Munson</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Gabe Ginsberg/Getty Images</small></p><p>Second is the [improvement in electronic speed control (ESC) circuitry]. We built a robot called Bombmachine back in the day. And besides its giant gel cell batteries, which were probably a third of the [bot’s total] weight, we had this big old Vantex speed controller with a big giant heat sink. The ESC form factors have gotten smaller. They’ve gotten more efficient. They’re able to handle way more amperage through the system, so they don’t blow up. They’ve got more technology built into them, so the team can have a person monitoring things like heat, and they’ll know when to, for instance, shut a weapon down. You see this a lot now on the show where they’re spinning up really fast, going in for a hit. And then they actually back off the weapon. And watchers will think, “Oh, the weapon’s dead.” But no, they’re actually just letting it cool down because the monitor guy has told his driver, “Hey, the weapon’s hot. I’m getting some readings from the ESC. The weapon’s hot. Give me five seconds.” That kind of thing. And that’s a tremendous strategy boon.</p><p><strong>So instead of just one-way remote control, teams are getting telemetry back from the robots now as well?</strong></p><p><strong>Munson: </strong>A lot of that is starting to happen more and more, and teams like <a href="https://battlebots.com/robot/ribbot-2021/" rel="noopener noreferrer" target="_blank">Ribbot</a> are using that. I think they’re influencing other teams to go that route as well, which is great. Just having that extra layer of data during the fight is huge.</p><p class="pull-quote">CAD gives the robots more personality and character, which is perfect for a TV show.</p><p><strong>What other technologies have made a big difference?</strong></p><p><strong>Munson:</strong> CAD is probably just as big of a technology boost since the ’90s compared to now. In the early “BattleBots” era, a lot of teams were using pencil and paper or little wooden prototypes. Only the most elite, fancy teams back then would use some early version of <a href="https://www.solidworks.com/" target="_blank">Solidworks</a> or <a href="https://www.autodesk.com/" target="_blank">Autodesk</a>. We were actually being hit up by the CAD companies to get more builders into designing in CAD. Back in the day, if you’re going to build a robot without CAD, you think very pragmatically and very form-follows-function. So you saw a lot of robots that were boxes with wheels and a weapon on top. That’s something you can easily just draw on a piece of paper and figure out. And now CAD is just a given. High-school students are designing things in CAD. But when you’ve got CAD, you can play around and reshape items, and you can get a robot like <a href="https://battlebots.com/robot/hypershock-2021/" target="_blank">HyperShock</a>—it looks like there’s no right-angled pieces on HyperShock.</p><p>CAD gives the robots more personality and character, which is perfect for a TV show because we want the audience to go, “Hey, <em>that’s</em> HyperShock, my favorite!” Because of the silhouettes, because of the shape, it’s branded, it’s instantly identifiable—as opposed to a silver aluminum box that has no paint.</p><p class="pull-quote">It quickly became obvious that if there’s a battery fire in the pit, with the smoke and whatnot, that’s a no-go.</p><p><strong>When Anouk was writing about being a competitor, she pointed out that there’s quite a strict safety regime teams have to follow, especially with regard to batteries, which are stored and charged in a separate area where competitors have to bring their robots before a fight. How did those rules evolve?</strong></p><p><strong>Munson:</strong> It’s part “necessity is the mother of invention” and part you just know the lithium technology is more volatile. We have a really smart team that helps us do the rules—there are some EEs on there and some mechanical engineers. They know about technology issues even before they hit the awareness of the general public. The warning shots were there from the beginning—lithium technology can burn, and it keeps on burning. We started out with your basic bucket full of sand and special fire extinguishers along the arena side and in the pit where people were fixing the robots. Every row had a bucket of sand and a protocol for disposing of the batteries properly and safely. But it quickly became obvious that if there’s a battery fire in the pit, with the smoke and whatnot, that’s a no-go. So we quickly pivoted away from that [to a separate] battery charging pit.</p><p>We’ve seen batteries just go up, and they don’t happen in the main pit; they happen in the battery pit—which is a huge, huge win for us because that’s a place where we know exactly how to deal with that. There’s staff at the ready to put the fires out and deal with them. We also have a battery cool-down area for after a fight. When the batteries have just discharged massive amounts of energy, they’re hot and some of them are puffing. They get a full inspection. You can’t go back to the pit after your match. You have to go to the battery cool-down area—it’s outside, it’s got fans, it’s cool. A dedicated safety inspector is there inspecting the batteries, making sure they’re not on the verge of causing a fire or puffing in any kind of way. If it’s all good, they let them cool down and stay there for 10, 15 minutes, and then they can go back to the battery-charging tent, take the batteries out and recharge them, and then go back to fixing the robot. If the batteries are not good, they are disposed of properly.</p><p><strong>The technology has become more flexible, but how do you prevent competitors from just converging on a handful of optimal design solutions, and all start looking alike?</strong></p><p><strong>Munson: </strong>That’s a constant struggle. Sometimes we win, and sometimes we lose. A lot of it is in the judging rules, the criteria. We’ve gone through so many iterations of the judging rules because builders love to put either a fork, a series of forks, or a wedge on their bot. Makes total sense because you can scoop the guy up and hit them with your weapon or launch them in the air. So okay, if you’re just wedging the whole fight, is that aggressive? Is that control? Is that damage? And so back in the day, we were probably more strict and ruled that if you all you do is just wedge, we actually count it against you. We’ve loosened up there. Now, if all you do is wedge, it only counts against you just a little bit. But you’ll never win the aggression category if all you’re going to do is wedge.</p><p>Because a wedge can beat everything. We often saw the finals would be between a big gnarly spinner and a wedge. Wedges are a very effective, simple machine that can clean up in robot combat. So we’re tweaking how we count the effectiveness of wedges and our judging guide if the fight goes to judges. Meanwhile, we don’t <em>want</em> it to go to judges. We want to see a knockout. So we demand that you have to have an active weapon. You can’t just have a wedge. It has to be a robust, active weapon that can actually cause damage. You just can’t put a Home Depot drill on the top of your robot and call it a day. That was just something we knew we needed to have to push the sport forward. What seems to be happening is the vertical spinners are now sort of the dominant class.</p><p>We don’t want the robots to be homogenized. That’s one of the reasons why we allow modifications during the actual tournament. Certain fans have gotten mad at us, like, “Why’d you let them add this thing during the middle of the tournament?” Because we <em>want</em> that. We want that spirit of ingenuity and resourcefulness. We want to break any idea of “vertical spinners will always win.” We want to see different kinds of fights because people will get bored otherwise. Even if there’s massive amounts of destruction, which always seems to excite us, if it’s the same kind of destruction over and over again, it starts to be like an explosion in <em>Charlie’s Angels</em> that I’ve seen 100 times, right? A lot of robots are modular now, where they can swap out a vertical spinner for a horizontal undercutter and so on. This will be a constant evolution for our entire history. If you ask me this question 20 years from now, I’m going to still be saying it’s a struggle!</p>]]></description><pubDate>Thu, 15 Sep 2022 16:13:51 +0000</pubDate><guid>https://spectrum.ieee.org/battlebots-tech-evolution</guid><category>Battlebots</category><category>Batteries</category><category>Cad</category><category>Robots</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-green-and-purple-battlebot-hovers-in-the-air-amid-a-large-number-of-sparks-below-it-is-a-purple-and-red-battlebot.jpg?id=31708905&amp;width=980"></media:content></item><item><title>Video Friday: DARPA ANCILLARY</title><link>https://spectrum.ieee.org/video-friday-2658169018</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-rendering-of-a-silvery-fixed-wing-drone-with-three-ducted-fan-propellers-arranged-in-a-triangle-at-its-tail.png?id=31626838&width=1200&height=800&coordinates=72%2C0%2C73%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://clawar.org/clawar2022/">CLAWAR 2022</a>: 12–14 September 2022, AZORES, PORTUGAL</h5><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPrize Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote><em>DARPA’s AdvaNced airCraft Infrastructure-Less Launch And RecoverY X-Plane program, nicknamed ANCILLARY, aims to develop and flight demonstrate critical technologies required for a leap ahead in vertical takeoff and landing (VTOL), low-weight, high-payload, and long-endurance capabilities.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b832808fe22645d4d114c1526de0d73d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/4YpOGM5nEbQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.darpa.mil/news-events/2022-09-07">DARPA</a> ]</p><div class="horizontal-rule"></div><p>Behold the tastiest robot ever, thanks to the 40 kilograms of dark chocolate that it’s made of.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="215f336d14173754c0f353d7e26e0aef" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ZpgegbgIlSM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.instagram.com/amauryguichon/">Amaury Guichon</a> ]</p><div class="horizontal-rule"></div><p>When a video features a robot operating outdoors while being pursued by a human with a laptop on a cart, you know it’s going to be some cutting-edge stuff. In this case, it’s the University of Michigan’s Cassie autonomously navigating based on directions from a hand-drawn map.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="a76ed6321db7294af4984d2f24c11afc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/1S78UwkE0yg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>First, we show Cassie a map with a hand-drawn path, which she needs to follow. Second, she localizes herself into the OpenStreetMap, used as a topological global map. Third, she then converts the drawn path to her own understanding in the OpenStreetMap. Fourth, she determines terrain types such as sidewalks, roads, and grass. Fifth, she decides what categories she should walk on at the moment. Sixth, a multi-layered map is built. Seventh, a reactive CLF planning algorithm is guiding Cassie to walk safely without hitting obstacles. Finally, the planning signal is sent to Cassie’s 20 degree-of-freedom motion controller.</em></blockquote><p>[ <a href="https://github.com/UMich-BipedLab">University of Michigan</a> ]</p><p>Thanks, Bruce!</p><div class="horizontal-rule"></div><p>Apparently Indonesia drone laws are very permissive? Or they are for DJI, anyway.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="678af0555cf665f83ba90c648b796df7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/W7jtv2DlGs0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dji.com/avata">DJI Avata</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Waymo Co-CEO Dmitri Dolgov recently took another rider-only trip around San Francisco. Watch as the Waymo Driver reacts dynamically to other human drivers, cyclists, and pedestrians during the nearly hour-long ride.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="96aa772ba015709aa28ddc0533e05ca6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ospoTAyEdDQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://waymo.com/">Waymo</a> ]</p><div class="horizontal-rule"></div><p>This capacitive sensing skin will keep you from getting whacked by a robot arm.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6b3ca3c394c6d9e12df46e424da979b8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GbRArufI7t4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ieeexplore.ieee.org/document/6386294">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Dexterous Teleoperation combining shadow hand with real-time volumetric telepresence rendering in VR.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d547abbce868c4bdffb26fff7c680d96" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/laE9GHDZVlQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.extendrobotics.com/">Extend Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Breathtaking landscape <a href="https://www.skydio.com/blog/keyframe-challenge-produces-mind-bending-drone-cinematography" target="_blank">aerial cinematography</a> is made easy when using Skydio drone technology! Enjoy some of our favorite scenic landscape moments from the Skydio community.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ad51de27407c066db3cc155016f4b950" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xeQe-IzrbH8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.skydio.com/">Skydio</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Most people think of intelligence as existing in the computer or our brain. Artificial intelligence recognizes faces, understands speech, picks movies, and corrects typos. These tasks are well-suited for computers. But when it comes to roboticists, they are all about physical tasks in the real world. And intelligence is no longer confined to the realm of the bits; the intelligent agent is a robot. Professor Matei Ciocarlie’s Robotic Manipulation and Mobility lab is embodying intelligence in robot hands to solve the problem of physical interaction in our complicated world.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3fa6898a1a048ea07fbf508c519a1dea" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/WXGAf3pqNDc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://roam.me.columbia.edu/">ROAM Lab</a> ]</p><div class="horizontal-rule"></div><blockquote><em>In this episode of our Robot Spotlight series, we showcase a Polaris GEM electric vehicle that has been outfitted with our OutdoorNav autonomy software. Watch the video to learn how it all came together and to find out if the team was able to use the autonomy software to navigate the vehicle through a local shopping plaza and through a Starbucks drive thru.  </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="069490ed44c52e0569c8b343b7f6205e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/QJdJ7nAmhzY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://clearpathrobotics.com/">Clearpath</a> ]</p><div class="horizontal-rule"></div><p>Two research talks from UPenn’s GRASP lab: Nadia Figueroa on Collaborative Human-Aware Robotics, and M. Ani Hsieh on Robots for Climate, Energy, and Stability.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6d34912b1048ba8fa26813ff80542476" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8hZIj9zI92s?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="692548184121f017c922bb358de59eb1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/BC4ssqExJS4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>[ <a href="https://www.grasp.upenn.edu/">GRASP Lab</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 09 Sep 2022 15:55:30 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-2658169018</guid><category>Video friday</category><category>Robotics</category><category>Drones</category><category>Self-driving cars</category><category>Robot hands</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-rendering-of-a-silvery-fixed-wing-drone-with-three-ducted-fan-propellers-arranged-in-a-triangle-at-its-tail.png?id=31626838&amp;width=980"></media:content></item><item><title>Drone Lands on Astonishingly Steep Surfaces</title><link>https://spectrum.ieee.org/drone-landing</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-quadcopter-drone-resting-on-the-steep-roof-of-a-house-with-trees-in-the-background.jpg?id=31515370&width=1200&height=800&coordinates=0%2C44%2C0%2C44"/><br/><br/><p><p>
<em>This article is part of our exclusive <a href="https://spectrum.ieee.org/collections/journal-watch/" rel="noopener noreferrer" target="_self">IEEE Journal Watch series</a> in partnership with IEEE Xplore.</em>
</p></p><p>Drones are able to complete a wide range of useful tasks, but they often require specific conditions in order to land safely. Ideally, most drones need to approach a flat surface at a slow enough speed to avoid bouncing or crashing. One group of researchers in Canada has devised a novel solution to this challenge, which allows drones to land on impressively steep inclines of up to 60 degrees, and at speeds up to 2.75 meters per second.</p><p>Through a series of experiments, the researchers demonstrate how their approach can be used to safely land drones on the steep rooftops of homes. The results are described in a <a href="https://ieeexplore.ieee.org/abstract/document/9779538" rel="noopener noreferrer" target="_blank">study</a> published in the July issue of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369" rel="noopener noreferrer" target="_blank"><em>IEEE Robotics and Automation Letters</em></a>. </p><hr/><p>“At high-impact velocities and on inclined surfaces, commercial multirotors tend to bounce, flip over, or even break upon impact, due to their stiff landing gear,” explains <a href="https://ca.linkedin.com/in/john-bass-62273a16a" target="_blank">John Bass</a>, a Ph.D. student at the <a href="https://www.usherbrooke.ca/" target="_blank">Université de Sherbrooke’s</a> <a href="https://www.createk.co/" target="_blank">Createk design lab</a>, in Québec, who was involved in the study. “Our general approach for landing a quadrotor on a steep incline is to combine two technologies—friction shock absorbers and reverse thrust.”</p><p>Bass’s team first began exploring combinations of friction shock absorbers and reverse thrust through simulations. Based on the simulation data, they then created the landing gear with friction shock absorbers using micro-DC (direct current) motors, springs, and 3D-printed parts.</p><p>“The friction shock absorbers that we integrated on the landing gear slow down the UAV [uncrewed aerial vehicle]’s dynamics enough to successfully exploit reverse thrust,” Bass explains. “[The friction shock absorbers] also promote a continuous contact between the legs and the ground, instead of bouncing indefinitely down the slope as is the case for normal rigid landing gear.”</p><p>The team then put their drone to the test, landing it on roofs and landing pads at various speeds and inclines. Bass notes that because the drone can land on a surface as steep as 60 degrees, it is capable of landing on most North American rooftops, which typically have a pitch of less than 53 degrees. </p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="fa9927dec8c8d3a0e03a5d58383e3336" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/tG1K_63q00Y?rel=0&start=6" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Fast Multirotor Landings on Steep Roofs using Friction Shock Absorbers and Reverse Thrust</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=tG1K_63q00Y&t=6s" target="_blank">www.youtube.com</a>
</small>
</p><p>This capability could be useful for recharging, surveillance, or emergency-landing purposes. The ability to land at high speeds—with less precision and sensing requirements—could be particularly useful  during emergency landings.</p><p>The proposed landing technique may only be feasible for smaller drones, however. Notably, larger and heavier drones dropping down at fast speeds onto a roof could potentially damage the roof. </p><p>The team plans to continue to explore novel ways of landing drones. The lab has previously landed drones on <a href="https://youtu.be/RSn62JKIRks" rel="noopener noreferrer" target="_blank">vertical walls</a> and <a href="https://youtu.be/n7awDhdVkCs" rel="noopener noreferrer" target="_blank">lakes</a>. </p><p>“[Our] goal is to further increase the capabilities of UAVs by landing in other difficult scenarios, such as boats in harsh sea conditions, on icebergs, and on fast-moving ground vehicles,” says <a href="https://ncrn-rcrc.mcgill.ca/people/researchers/alexis-lussier-desbiens" target="_blank">Alexis Lussier Desbiens</a>, a professor of mechanical and robotics engineering at the Createk design lab who was also involved in the study. </p><p>“These scenarios each bring their own new set of challenges, such as a ship’s linear and angular motions when landing on boats, the strong drag force when landing in high wind or at high speed, the turbulent air motion near boats, icebergs, and fast-moving vehicles, and the slippery surfaces of icebergs,” he adds.</p><p><em>This article was updated on 7 September 2022 to clarify that using the landing technique with a larger drone could damage rooftops.</em><br/></p>]]></description><pubDate>Tue, 06 Sep 2022 17:25:26 +0000</pubDate><guid>https://spectrum.ieee.org/drone-landing</guid><category>Drones</category><category>Uavs</category><category>Landing</category><category>Journal watch</category><dc:creator>Michelle Hampson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-quadcopter-drone-resting-on-the-steep-roof-of-a-house-with-trees-in-the-background.jpg?id=31515370&amp;width=980"></media:content></item><item><title>Video Friday: In der Natur</title><link>https://spectrum.ieee.org/video-friday-in-der-natur</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-curiously-dressed-man-wearing-white-and-orange-clothes-and-dark-glasses-walks-an-orange-robot-dog-in-a-white-coat-through-the.png?id=31440100&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p><span style="background-color: initial;">Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span style="background-color: initial;"> for inclusion.</span><br/></p><h5><a href="https://clawar.org/clawar2022/">CLAWAR 2022</a>: 12–14 September 2022, AZORES, PORTUGAL</h5><h5><a href="https://iros2022.org/">IROS 2022</a>: 23–27 October 2022, KYOTO, JAPAN</h5><h5><a href="https://www.xprize.org/prizes/avatar/finals-testing">ANA Avatar XPRIZE Finals</a>: 4–5 November 2022, LOS ANGELES</h5><h5><a href="https://corl2022.org/">CoRL 2022</a>: 14–18 December 2022, AUCKLAND, NEW ZEALAND</h5><p>Enjoy today’s videos!</p><hr/><div style="page-break-after: always"><span style="display:none"> </span></div><p>There’s really nothing I can say to prepare you for this German music video, which features Spot for some reason.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="aae4b1d6de97f6074a9b94a5770db66b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/C1wKQUFuzng?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>I’m told that the music video is about how the idealized version of a forest is somewhat at odds with technology and that bringing your fancy fleece jackets and robots along with you into nature can kind of ruin the experience. I get it. Also, that IR shot of Spot at night is suuuper creepy.</p><p>[ <a href="https://www.deichkind.de/">Deichkind</a> ]</p><p>Thanks, Thilo!</p><div class="horizontal-rule"></div><p>I’m going to assume that KIMLAB is not at all confused about which superhero has what equipment, and instead that Spot is cosplaying that one specific scene in <em>Avengers: Endgame</em>.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="32a1505d403e792e41eee59ae074b3f7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/B1FqiZuX_uc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://publish.illinois.edu/kimlab2020/">KIMLAB</a> ]</p><div class="horizontal-rule"></div><p>Dongwon Son, who’s now at Ph.D. student at <a href="https://www.kaist.ac.kr/en/" target="_blank">Korea Advanced Institute of Science and Technology</a>, wrote in to share this work he did at Samsung Research. Somehow, they know exactly what my desk looks like most of the time.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="94feb56de5eec1b5f5edbbeda9a41eb6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Sj5QDR2Hwd4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://dongwon-son.github.io/">Github</a> ]</p><p>Thanks, Dongwon!</p><div class="horizontal-rule"></div><p>Rethink Robotics and Sawyer: still a thing!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b936f7161215ff4b9d9dcd921de16b5a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PIeYJIsGQ5A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.rethinkrobotics.com/">Rethink</a> ]</p><div class="horizontal-rule"></div><p>The designer of one of the most destructive combat robots ever built gives some tips on how to hit harder.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8a015a63d79906092240e80c6ba6376f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/U1-wVjPSSNE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="http://www.hardcorerobotics.com/">Hardcore Robotics</a> ]</p><div class="horizontal-rule"></div><p>Some satisfying precision syringe filling.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="81b1c192496323fe98fb89ec1177344d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/4lPbt2TpqDM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.flexiv.com/en/">Flexiv</a> ]</p><div class="horizontal-rule"></div><p>When you put the Lockheed Martin Missiles and Fire Control Operations Team together with Boston Dynamics, you get something not nearly as exciting as you were probably expecting.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e0ea8669f89f283506a49c57d34552bf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/nNz0eo2kh0o?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.lockheedmartin.com/en-us/who-we-are/business-areas/missiles-and-fire-control/dallas.html">Lockheed Martin</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Philippines Flying Labs has teamed up with local clinics and public health professionals in <a href="https://en.wikipedia.org/wiki/Tawi-Tawi" target="_blank">Tawi-Tawi</a> to enable demand-driven drone deliveries.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="0f68d4092c395325fa2a6b6c00a0c9a1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5SbiP7bSUw4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://werobotics.org/healthrobotics/">WeRobotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Speaker John Enright, Principal Engineer, Amazon Robotics, tells the story of developing precision autonomy on Proteus, the new cost-effective autonomous mobile robot designed to work safely and efficiently alongside humans in shared, collaborative spaces.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ff0e4481cad0d3eef0e7eb9479d65fc7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/0Hd9nZHolZQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.amazon.science/research-areas/robotics">Amazon Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>In this second episode of #MeetAGoogleResearcher, Drew Calcagno speaks with Kanishka Rao of Google Research and Daniel Ho of Everyday Robots, researchers who helped combine the PaLM-SayCan robotics algorithm with the advanced capabilities of a helper robot.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ad2f0c848b230bc5dad776a6f907af24" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/amSfQOCuPrg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html">Google Research</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Launched in 1977, the twin Voyager probes are NASA’s longest-operating mission and the only spacecraft ever to explore interstellar space. For two decades after launch, the spacecraft were planetary explorers, giving us up-close views of the gas giants Jupiter, Saturn, Uranus, and Neptune. Now, as they reach distances far beyond the hopes of their original designers, the aging spacecraft challenge their team in new ways, requiring creative solutions to keep them operating and sending back science data from the space between the stars. As we celebrate the 45th anniversary of these epic explorers, join Voyager deputy project scientist Linda Spilker and propulsion engineer Todd Barber for a live Q&A.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="9c4da12dae1dacc8311b37e796404489" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ZbiJDvFNmP0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.jpl.nasa.gov/events/voyager-45-years-in-space">JPL</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 02 Sep 2022 17:50:00 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-in-der-natur</guid><category>Video friday</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-curiously-dressed-man-wearing-white-and-orange-clothes-and-dark-glasses-walks-an-orange-robot-dog-in-a-white-coat-through-the.png?id=31440100&amp;width=980"></media:content></item></channel></rss>