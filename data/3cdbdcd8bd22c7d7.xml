<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/topic/tech-history.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Mon, 31 Oct 2022 16:13:26 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Pong Was Boring—And People Loved It</title><link>https://spectrum.ieee.org/pong-video-game</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-boxy-retro-game-console-with-a-crt-screen-with-the-word-pong-on-the-front.jpg?id=31999182&width=1200&height=800&coordinates=0%2C183%2C0%2C184"/><br/><br/><p>
<strong>November marks the</strong> 50th anniversary of 
	<em>Pong</em>. Why should we care?
</p><p>
	For starters, 
	<em>Pong</em> is the first video game that millions of people welcomed into their homes to play on their own televisions. <a href="https://spectrum.ieee.org/pong" target="_self"><em>Pong</em> kick-started a global video-game industry</a> that is now worth upwards of <a href="https://earthweb.com/how-much-is-the-gaming-industry-worth/" rel="noopener noreferrer" target="_blank">US $300 billion</a>. And <em>Pong</em> still has a place in active research, for <a href="https://spectrum.ieee.org/ai-pong-uncertainty" target="_self">training AI algorithms</a>, <a href="https://spectrum.ieee.org/pong-in-a-dish" target="_blank">strengthening neural networks</a>, and developing the <a href="https://spectrum.ieee.org/elon-musk-brain-neuralink" target="_self">brain-machine interface called Neuralink</a>, among other things.
</p><hr/><p>
	And yet as a Gen-Xer born too late to have enjoyed 
	<em>Pong</em> as a child, I have trouble fathoming how anyone could sit in front of a TV watching a square dot—not even a round ball—bounce back and forth across the dark, featureless screen. Was this really fun? To celebrate the half-century persistence of <em>Pong</em>, I set out to discover why so many people love the most boring video game of all time.
</p><h2>What is <em>Pong?</em></h2><p>
	In case you have somehow never encountered 
	<em>Pong</em>, it is simplicity itself—represented on screen as a moving dot, two vertical lines with which to hit the dot, and squared-off digits reflecting the score. The game mimics table tennis, with two people competing against each other to get the ball past their opponent. Players can move their paddle vertically (but not horizontally) to deflect the ball, and the ball can also bounce off the top and bottom of the screen. The ball speeds up the longer you rally, and each hit is accompanied by a satisfying “click” sound. If you miss, your opponent scores a point. The first player to reach 11 points wins.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A bearded white man smiles as he stands in front of a retro game console." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="abb9ec2384c41b0f49126f325550c83e" data-rm-shortcode-name="rebelmouse-image" id="1c49d" loading="lazy" src="https://spectrum.ieee.org/media-library/a-bearded-white-man-smiles-as-he-stands-in-front-of-a-retro-game-console.jpg?id=31999194&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Allan Alcorn, shown here in 2011, designed Pong as an exercise, to learn about how to design video games.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;"><a href="https://commons.wikimedia.org/wiki/File:Al_Alcorn,_Pong,_CHM_2011.jpg" rel="noopener noreferrer" target="_blank">vonguard/Wikipedia</a></small>
</p><p>
	The game was actually created as part of a training exercise to get a young engineer named 
	<a href="https://archive.computerhistory.org/resources/access/text/2012/09/102658257-05-01-acc.pdf" rel="noopener noreferrer" target="_blank">Allan Alcorn</a> acquainted with video-game development. <a href="https://www.si.edu/media/NMAH/NMAH-AC1498_Transcript_NolanBushnell.pdf" rel="noopener noreferrer" target="_blank">Nolan Bushnell </a>and <a href="https://www.youtube.com/watch?v=6arAY7cUa5I&t=6s" rel="noopener noreferrer" target="_blank">Samuel “Ted” Dabney</a> had recently started <a href="https://www.computerhistory.org/revolution/computer-games/16/185" rel="noopener noreferrer" target="_blank">Atari</a>, and they hired Alcorn as their third employee. Alcorn had majored in electrical engineering and computer science at the University of California, Berkeley, but had never played a video game before. At the time, video games were mostly the domain of research laboratories and universities. Under the guise of a contract for General Electric, Bushnell asked Alcorn to design a game similar to ping-pong that could be played on a television.
</p><p>
	Because 
	<em>Pong</em> was developed before the debut of microcontrollers, the circuit board consisted of hardwired, discrete logic components. The arcade version included 66 integrated circuits, a pair of <a href="https://spectrum.ieee.org/chip-hall-of-fame-signetics-ne555" target="_self">555 timers</a>, and a few transistors. (For a deeper dive into <em>Pong</em>’s design, see Hugo Holden’s article, “<a href="https://www.worldphaco.com/uploads/LAWN_TENNIS.pdf" rel="noopener noreferrer" target="_blank">Atari Pong E Circuit Analysis & Lawn Tennis: Building a Digital Video Game with 74 Series TTL IC’s</a>,” Dan Boris’s <a href="http://atarihq.com/danb/files/PongSchematic.pdf" rel="noopener noreferrer" target="_blank">redraw of the circuit diagram</a>, and Ricardo Ramos’s account of <a href="https://ricardoramos.me/pong-board/" rel="noopener noreferrer" target="_blank">building a <em>Pong</em> clone</a>.)
</p><p class="pull-quote">
	To celebrate the half-century persistence of 
	<em>Pong</em>, I set out to discover why so many people love the most boring video game of all time.
</p><p>
	Bushnell understood the economics of pinball machines and other arcade games, and so he decided to add a coin box to the 
	<em>Pong</em> arcade console [pictured at top]. He beta tested it at Andy Capp’s Tavern in Sunnyvale, Calif. According to <em>Pong</em> lore, people lined up for their turn to stuff quarters into the machine and play. Eventually, the manager called Alcorn because the coin box was jammed to overflowing and the game had stopped working.
</p><p>
	Atari sold 
	<a href="https://www.computerhistory.org/revolution/computer-games/16/183" rel="noopener noreferrer" target="_blank">about 35,000 consoles</a> for use in arcades, bars, and restaurants, but the real game changer, as it were, didn’t happen until 1975, when Atari introduced <em>Home Pong</em>, for play on your own TV set. The controller was a brown box with a dial on each side, forcing players to play shoulder to shoulder. It was advertised as working with any television set, but you could only play the one game. Later versions had individual controllers with long cords that allowed players to sit back more comfortably. Hundreds of thousands of <em>Pong</em> sets were distributed through the department store Sears. The game was also one of the original titles included with the <a href="https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-atari-2600" target="_self">Atari 2600</a>, a video-game system introduced in 1977 that let you play different games by swapping out the game cartridge.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A game console for the video game Pong has two knobs and a digital readout that says Tele-Games." class="rm-shortcode" data-rm-shortcode-id="4def229fa141131475c291a823b38a21" data-rm-shortcode-name="rebelmouse-image" id="740c5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-game-console-for-the-video-game-pong-has-two-knobs-and-a-digital-readout-that-says-tele-games.jpg?id=31999216&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Atari’s home version of Pong was released through Sears in 1975.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://commons.wikimedia.org/wiki/File:TeleGames-Atari-Pong.png" rel="noopener noreferrer" target="_blank">Evan Amos/Wikipedia</a></small>
</p><p>
	Meanwhile, Atari had to contend with the problems of 
	<em>Pong</em>’s popularity. Back in 1972, Magnavox had introduced its Odyssey home video-game system, which included a game called <em>Table Tennis</em>. Two years later, it <a href="https://thinksetmag.com/insights/digital-detective-pong" rel="noopener noreferrer" target="_blank">filed a lawsuit</a> against Atari alleging copyright infringement; that suit was settled out of court. Rival companies brought out similar <em>Pong</em>-like games, as did Atari itself. Bushnell wanted a one-player <em>Pong</em>-type game, so he worked with <a href="https://www.retrogamer.net/blog_post/remembering-steve-bristow/" rel="noopener noreferrer" target="_blank">Steve Bristow</a> and <a href="https://www.thehenryford.org/explore/stories-of-innovation/visionaries/steve-wozniak/" rel="noopener noreferrer" target="_blank">Steve Wozniak</a> to develop <a href="https://spectrum.ieee.org/atari-breakout" target="_self"><em>Breakout</em></a>, a brick-breaking game that also proved wildly popular. In 2015, the <a href="https://www.museumofplay.org/" rel="noopener noreferrer" target="_blank">Strong National Museum of Play</a>, in Rochester, N.Y., inducted <em>Pong</em> into its <a href="https://www.museumofplay.org/exhibits/world-video-game-hall-of-fame/inducted-games/" rel="noopener noreferrer" target="_blank">World Video Game Hall of Fame</a>, citing its popularity and influence in launching the video-game industry.
</p><p>
<em>Pong</em> was not the first video game, nor the first table-tennis video game, nor the first home video game, and yet it was the one that introduced millions of children and their parents to the idea of video gaming.
</p><h2>But was <em>Pong</em> fun?</h2><p>
	I first encountered 
	<em>Pong</em> in the early 1980s, on an Atari 2600 at my kindergarten best friend’s house. I didn’t see the magic then, and I still don’t. How was this <em>the</em> game that launched a billion-dollar industry? I recently posed this question to friends of a certain age and was surprised to receive a resounding chorus of cheers for <em>Pong</em>. Yes, <em>Pong</em> really was fun! My curiosity piqued, I followed up with some informal interviews and an entirely unscientific sampling methodology, and I have compiled the top five reasons people seem to love <em>Pong</em>.
</p><p>
	Above all, the 
	<strong>novelty factor</strong> seems to propel 
	<em>Pong</em> to the top of the pantheon of video games. <a href="https://computerhistory.org/profile/david-brock/" rel="noopener noreferrer" target="_blank">David C. Brock</a>, director of curatorial affairs at the Computer History Museum, fondly remembered playing <em>Pong</em> at home and in a restaurant in Pennsylvania. <em>Pong</em> was so new and different that it made an impression, Brock recalls. Pinball may literally have all the bells and whistles, but it was a game from the previous century. <em>Pong</em> was the future, even if that future was a black-and-white television screen.
</p><h3></h3><br/><h2>Top 5 Reasons People Love Pong</h2><p>
1. Novelty
</p><p>
2. Sociability
</p><p>
3. Calming Effect (ASMR)
</p><p>
4. The Challenge
</p><p>
5. Nostalgia
</p><p>
	Connected to the novelty factor is that 
	<em>Pong</em> is <strong>an inherently social game</strong>. No longer were players competing one by one to get the highest score. Now you could compete directly against your friends and family. My University of South Carolina colleague <a href="https://sc.edu/study/colleges_schools/artsandsciences/history/our_people/directory/risk.php" rel="noopener noreferrer" target="_blank">James Risk</a> grew up in Mooresville, Indiana, a small town southwest of Indianapolis. His house was the first in the neighborhood to have a home version of <em>Pong</em>. James’s father would never relinquish his turn, even when he lost, so James and his siblings had to duke it out to be Player 2. Even the family cat would get into the action, chasing the moving dot on the screen.
</p><p>
	Lynn Heidelbaugh, a museum curator in Washington, D.C., remembers watching her older brothers play 
	<em>Pong</em>. She found the game mesmerizing—watching the ball move back and forth, bouncing at angles off the paddles, each hit accompanied by a satisfying chirp. It’s not a stretch to think that <em>Pong</em> induces <strong>autonomous sensory meridian response, or ASMR</strong>, the calming and euphoric feeling triggered by repetitive auditory or visual stimuli. <em>Pong</em>’s distinctive sound was Alcorn’s response to Bushnell’s request for the roar of crowds. Alcorn crafted the sound from tones that already existed in the sync generator.
</p><p>
	Despite the simplicity of the game, 
	<em>Pong</em> was surprisingly <strong>challenging to play</strong>. The controllers were not particularly sensitive, and it was easy to miss a ball you thought you had. For many players, this imperfection made the game addictive, as they kept trying to improve. Another defect (or feature, as Alcorn preferred to think of it) was that the paddles did not move fully to the top or bottom of the screen. This was due to a problem in the circuitry. Alcorn could have fixed it, but it turned out to be a happy accident. Had the paddles reached the edges, really good players could have rallied indefinitely. Instead, the game inevitably ends after a few minutes, only to be started again by gamers intent on winning.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="18155cb8154a8536878b40915cfea3ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/e4VRgY3tkh0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	A final reason people love 
	<em>Pong</em> is <strong>nostalgia</strong>, whether that’s a recalling of childhood memories or a wistfulness for a past they never had. <a href="https://www.eiu.edu/geoscience/faculty.php?id=kjlewandowski&subcat=" rel="noopener noreferrer" target="_blank">Katherine Lewandowski</a>, now a geologist at Eastern Illinois University, didn’t play much <em>Pong</em> growing up, but was reintroduced to the game as an undergraduate at Vanderbilt in the 1990s. She says she was a novice compared to her friends, who had been playing for years, but she still enjoyed a spin at the console at Obie’s Pizza while waiting for their deep dish.
</p><p>
<em>IEEE Spectrum’s </em>special projects editor <a href="https://spectrum.ieee.org/u/stephen-cass" target="_self">Stephen Cass</a> grew up in Ireland. By the time arcade games arrived there, Cass says, <em>Pong</em> was not among the offerings—manufacturers had already moved on to other titles. His strongest <em>Pong</em> memory is thus not of the game but of the 1994 Frank Black song “Whatever Happened to Pong?,” which leveraged the ’90s trend of ’70s nostalgia. A few years later, <em>Pong</em> made a <a href="https://70s-show-diary.tumblr.com/post/118881047312/one-of-the-great-somewhat-reoccurring-plots-of" rel="noopener noreferrer" target="_blank">recurring appearance</a> on season 1 of <em>That ’70s Show</em>, thereby cementing the nostalgia.
</p><p>
	For me, though, 
	<em>Pong</em> doesn’t hold a candle to <em>Frogger</em>. My father, who was an early adopter of almost all things tech, never saw value in video games, so I had to run down the street to my friend’s house to play <em>Frogger</em> on her Atari. I love that pixelated frog and the many ways to die—I was not particularly good at the game and died frequently. To this day, references to <em>Frogger</em> inevitably surface whenever I make ill-timed attempts at jaywalking in the city.
</p><p>
I suspect that every generation of the past half century has its game, the one that introduced them to video games, that triggers waves of nostalgia and fond memories, the old favorite that people are excited to see when they serendipitously encounter it in a pizza parlor or on an emulator in a museum. It could be 
	<em>Pong</em> or <em>Frogger</em> or <em>Pac-Man</em> or <em>Tetris</em> or <em>Super Mario Bros.</em> or <em>Space Invaders</em>—or the next big thing! What game do you think we will be talking about in 50 years?
</p><p>
<em>Part of a </em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><em>continuing series</em></a> <em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em>
</p><p>
<em>An abridged version of this article appears in the November 2022 print issue as “In Praise of Pong.” </em>
</p>]]></description><pubDate>Sun, 30 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/pong-video-game</guid><category>Al alcorn</category><category>Nolan bushnell</category><category>Past forward</category><category>Pong</category><category>Type:departments</category><category>Video games</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-boxy-retro-game-console-with-a-crt-screen-with-the-word-pong-on-the-front.jpg?id=31999182&amp;width=980"></media:content></item><item><title>How Microwave Radar Brought Direct Phone Calls to Millions</title><link>https://spectrum.ieee.org/trans-canada-microwave-system-milestone</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-electrical-tower-against-a-cloudy-sky.jpg?id=31941092&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>Making direct, long-distance phone calls to family and friends is quick and easy today. But when the telephone was invented in 1876, the farthest a call could be made was about 13 kilometers.</p><p>The first of those calls involved the telephone’s inventor, <a href="https://ethw.org/Alexander_Graham_Bell" rel="noopener noreferrer" target="_blank">Alexander Graham Bell</a>, who spoke from Paris, Ont., Canada, with his father, who lived in Brantford, also in Ontario.</p><hr/><p>Intercity telephone networks had been built in Canada in 1881, but the distances between them were limited, and customers needed to be connected to each other by an operator.</p><p>Fast-forward to 1958. To enable direct-dialed, long-distance calls, <a href="https://www.bell.ca/" rel="noopener noreferrer" target="_blank">Bell Canada</a>, headquartered in Montreal, built the Trans-Canada Microwave system. It consisted of 139 relay towers that spanned more than 6,000 kilometers, from Victoria, B.C., to Sydney, N.S. The system used microwave frequencies to achieve a high enough bandwidth and telephone line capacity to conduct the calls. When the system was completed, it was the longest microwave relay network in the world.</p><p>The microwave system was commemorated with an IEEE Milestone during a ceremony on 1 July at the <a href="https://www.telus.com/" rel="noopener noreferrer" target="_blank">Telus</a> headquarters in Vancouver—which is a former Trans-Canada Microwave site. The <a href="https://vancouver.ieee.ca/" rel="noopener noreferrer" target="_blank">IEEE Vancouver Section</a> sponsored the Milestone nomination.</p><p>“No single engineering achievement of the past century has had a greater impact on Canada’s society, economy, and citizens than the completion of the Trans-Canada Microwave system,” says IEEE Senior Member <a href="https://ece.ubc.ca/david-michelson/" rel="noopener noreferrer" target="_blank">David Michelson</a>, <a href="https://www.ieee.ca/en/" rel="noopener noreferrer" target="_blank">IEEE Canada</a>’s historian. “Introducing direct-dialed long-distance telephone services to Canadians immediately transformed the way that they communicated with each other for both business and pleasure.”</p><h2>A growing demand for long-distance calls</h2><p>When telephones were introduced to consumers, Canadians’ usage was among the highest in the world, according to the <a href="https://ethw.org/Milestones:The_Trans-Canada_Microwave_System,_1958" rel="noopener noreferrer" target="_blank">Milestone entry</a> on the <a href="https://ethw.org/Main_Page" rel="noopener noreferrer" target="_blank">Engineering and Technology History Wiki</a>.</p><p>In the early 1900s, seven telecommunications companies provided telephone services in Canada, according to a <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6372537" rel="noopener noreferrer" target="_blank">1956 article</a> in <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6367273" rel="noopener noreferrer" target="_blank"><em>Transactions of the American Institute of Electrical Engineers</em></a>. (AIEE is one of IEEE’s predecessor societies.) Each company operated a small network within its own territory. In 1929 they formed the <a href="https://publications.gc.ca/collections/collection_2019/isde-ised/Co22/Co22-321-1979-eng.pdf" rel="noopener noreferrer" target="_blank">TransCanada Telephone System</a> to expand and maintain the networks. Its telephone systems consisted of copper wires or high-frequency and very-high-frequency radio waves. The country had nine switching centers, where operators connected callers. But the system had limited capacity, suffered from poor voice quality, and was expensive to use.</p><p>After World War II, Canada’s economy boomed, and its population grew. The volume of long-distance calls made during the first postwar decade doubled from the prewar era, according to the ETHW entry.</p><p>To keep up with the demand, TransCanada hired Bell Canada to upgrade the networks. The company’s engineers determined that microwave links could be used to improve the service. During World War II, scientists from Canada, the United Kingdom, and the United States had developed microwave transmitters and receivers for use in radar systems that measure the echo of a pulse of microwave radiation to detect and locate distant objects. Adopting the high operating frequencies used by radar systems increased the bandwidth and capacity of the new communication system, Michelson says.</p><p>The first part of Bell Canada’s intercity network began operating in 1948. Its 23 telephone lines connected Nova Scotia with Prince Edward Island, a distance of 211 km. Four years later, a radio system made up of 46 telephone lines began operating. It connected Halifax with St. John, N.B., a distance of about 310 km.</p><p>In 1953 Bell Canada built a microwave relay system that linked Buffalo, N.Y., to Toronto; Ottawa; Montreal; Quebec City, Que.; and Kingston, Ont.</p><h2>Building Canada’s national telephone and TV network</h2><p>Because of the success of the intercity networks, the <a href="https://www.cbc.ca/" rel="noopener noreferrer" target="_blank">Canadian Broadcasting Corp.</a>, the country’s public broadcaster for both radio and television, decided to build a nationwide relay network to send TV and radio signals from coast to coast. Construction began in March 1955 and was completed in June 1958.</p><p>The CBC system introduced live network television to households. There had been no national TV network with individual stations or live programming.</p><p>To provide the ability to make coast-to-coast calls and access to TV programs nationwide, engineers used Bell Canada’s Northern Electric TD-2 microwave equipment, according to a 1957 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6442806" rel="noopener noreferrer" target="_blank"><em>Electrical Engineering</em> article</a>. The equipment operated at a frequency of 4,000 megahertz, which produced six, two-way radio channels. One of the channels served as a backup in case another one malfunctioned. Each channel could handle two TV programs or 600 telephone connections, according to the 1957 article.</p><p>The TD-2 system divided bandwidth into frequency “slots” so that a channel could be shared among several users at the same time. Because each channel had an effective base bandwidth of 10 MHz and a TV program used 4 MHz, it was possible to house a TV program and a small number of telephone lines on the same channel simultaneously, according to the <em>Electrical Engineering</em> article.</p><p>The new TV network began operating in July 1958, when it ran its first live coast-to-coast broadcast—“A Memo to Champlain”—in honor of Canada Day.</p><p>The Trans-Canada Microwave system was decommissioned in the late 1990s. It has been replaced with satellites and fiber-optic cables.</p><p>Administered by the <a href="http://www.ieee.org/about/history_center/index.html" rel="noopener noreferrer" target="_blank">IEEE History Center</a> and <a href="https://www.ieeefoundation.org/donate_history" rel="noopener noreferrer" target="_blank">supported by donors</a>, the Milestone program recognizes outstanding technical developments around the world.</p><p>The Trans-Canada system’s Milestone plaque, which is displayed in the lobby of the Telus headquarters, reads:</p><p><em>On 1 July 1958, the Trans-Canada Microwave system introduced live network television and direct-dialed long-distance telephone service to Canadians from coast to coast. Comprising 139 towers spanning more than 6,275 kilometres, it was, when completed, the world’s longest such network. Later extended and upgraded, the system had an immense impact on Canada’s society and economy.</em></p><em></em>]]></description><pubDate>Thu, 13 Oct 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/trans-canada-microwave-system-milestone</guid><category>Ieee history</category><category>Ieee milestone</category><category>Telecommunications</category><category>Microwave radar</category><category>Ieee history</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-electrical-tower-against-a-cloudy-sky.jpg?id=31941092&amp;width=980"></media:content></item><item><title>How Ted Hoff Invented the First Microprocessor</title><link>https://spectrum.ieee.org/ted-hoff</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/man-in-white-striped-shirt-with-pocket-protector-seated-in-front-of-oscilloscope-and-other-electronics-equipment.jpg?id=31848732&width=1200&height=800&coordinates=0%2C0%2C0%2C500"/><br/><br/><p><strong>The rays of the rising sun</strong> have barely reached the foothills of Silicon Valley, but Marcian E. (Ted) Hoff Jr. is already up to his elbows in electronic parts, digging through stacks of dusty circuit boards. This is the monthly flea market at Foothill College, and he rarely misses it.</p><p>Ted Hoff is part of electronics industry legend. While a research manager at Intel Corp., then based in Mountain View, he realized that silicon technology had advanced to the point that, with careful engineering, a complete central processor could fit on a chip. Teaming up with Stanley Mazor and Federico Faggin, he created the first commercial microprocessor, <a href="https://spectrum.ieee.org/chip-hall-of-fame-intel-4004-microprocessor" target="_self">the Intel 4004</a>.</p><h3></h3><br/><p>This article was first published as “Marcian E Hoff.” It appeared in the February 1994 issue of <em>IEEE Spectrum. </em>A <a href="https://ieeexplore.ieee.org/document/259493" target="_blank">PDF version</a> is available on IEEE Xplore. The photographs appeared in the original print version.</p><h3></h3><br/><p>But for Hoff, the microprocessor was merely one blip among many along the tracing of his long fascination with electronics. His passion for the field led him from New York City’s used electronics stores to elite university laboratories, through the intense early years of the microprocessor revolution and the tumult of the video game industry, and ultimately to his job today: high-tech private eye.</p><p>Fairly early in his childhood Hoff figured out that the best way to feel less like a kid—and a little more powerful—was to understand how things work. He started his explorations with chemistry. By the age of 12 he had moved on to electronics, building things with parts ordered from an Allied Radio Catalog, a shortwave radio kit, and surplus relays and motors salvaged from the garbage at his father’s employer, General Railway Signal Co., in Rochester, NY. Then in high school, working mostly with second­hand components, he built an oscilloscope, an achievement he parlayed into a technician’s job at General Railway Signal.</p><p>Hoff returned to that job during breaks from his undergraduate studies at <a href="https://www.rpi.edu/" target="_blank">Rensselaer Polytechnic Institute</a>, Troy, N.Y. Several summers began with Hoff entering the General Railway laboratory to find the researchers’ two best oscilloscopes broken. He would repair the state-of-the-art Tektronix 545s, then move on to more interesting stuff, like inventing an audio frequency railroad­train tracking circuit and a lightning protection unit that gave him two patents before he was out of his teens.</p><p>The best thing about the job, Hoff recalled, was the access it gave him to components that were beyond the budgets of most engineering students in the l950s—transistors, for instance, and even the just-introduced power transistor. He did an undergraduate thesis on transistors used as switches, and the cash prize he won for it quickly went for a Heathkit scope of his own.</p><h2>Early Neural Networks</h2><p>Hoff liked the engineering courses at Rensselaer, but not the narrow focus of the college itself. He wanted to broaden his perspective, both intellectually and geographically (he had never been more than a few miles west of Niagara Falls), so chose California’s <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford University</a> for graduate school. While working toward his Ph.D. there, he did research in adaptive systems (which today are called neural networks) and, with his thesis advisor Bernard Widrow, racked up two more patents.</p><p class="pull-quote">“He had a toy train moving back and forth under computer control, balancing a broom­ stick. I saw him as a kooky inventor, a mad scientist.”<br/>—Stanley Mazor<br/></p><p>His Intel colleague Mazor, now training manager at Synopsys Inc., Mountain View, Calif., recalled meeting Hoff in his Stanford laboratory.</p><p>“He had a toy train moving back and forth under computer control, balancing a broomstick,” Mazor said. “I saw him as a kooky inventor, a mad scientist.”</p><p>After getting his degree, Hoff stayed at Stanford for six more years as a postdoctoral researcher, continuing the work on neural networks. At first, his group made the networks trainable by using a device whose resistance changed with the amount and direction of current applied. It consisted of a pencil lead and a piece of copper wire sitting in a copper sulfate and sulfuric acid solution, and they called it a memistor.</p><p class="pull-quote">“One result of all our work on microprocessors that has always pleased me is that we got computers away from those [computer center] people.”<br/>—Ted Hoff</p><p>The group soon acquired an IBM 1620 computer, and Hoff had his first experience in programming—and in bucking the system. He had to deal with officials at the campus computer center who thought all computers should be in one place, run by specialists who handled the boxes of punched cards delivered by researchers. The idea that a researcher should program computer systems interactively was anathema to them.</p><h3>Ted Hoff: Vital Stats</h3><br><p><strong>Name </strong></p><p><strong></strong>Marcian E. (Ted) Hoff Jr.<br/></p><p><strong>Date of birth </strong></p><p>Oct. 28, 1937</p><p><strong>Family </strong></p><p>Wife, Judy; three daughters, Carolyn, Lisa, and Jill</p><p><strong>Education</strong></p><p>BS, 1958, Rensselaer Polytechnic Institute, Troy, N.Y.; MS, 1959, Ph.D., 1962, Stanford University, California, all in electrical engineering</p><p><strong>First job</strong></p><p>Planting cabbages</p><p><strong>First electronics job</strong></p><p>Technician, General Railway Signal Co., Rochester, N.Y.</p><p><strong>Biggest surprise in career</strong></p><p>Media hysteria over the microprocessor</p><p><strong>Patents</strong></p><p>17</p><p><strong>Books recently read</strong></p><p><em>Introduction to Nuclear Reactor Theory</em> by John R. Lamarsh; A Compiler Generator by William M. McKeeman, James J. Horning, and David B. Wortman</p><p><strong>People most respected</strong></p><p>Intel Corp. founders Robert Noyce and Gordon Moore, Intel chief executive officer Andrew Grove</p><p><strong>Favorite restaurants</strong></p><p>Postrio and Bella Voce in San Francisco, Beausejour in Los Altos, Calif.</p><p><strong>Favorite movies</strong></p><p><em>2001</em>,<em> Dr. Strangelove</em></p><p><strong>Motto</strong></p><p>“If it works, it’s aesthetic”</p><p><strong>Leisure activities</strong></p><p>Playing with electronics; attending operas and concerts; going to the theater, body surfing in Hawaii; walking his Alaskan malamutes</p><p><strong>Car</strong></p><p>Porsche 944</p><p><strong>Management creed</strong></p><p>“The best motivation is self-motivation”</p><p><strong>Organizational memberships</strong></p><p>IEEE, Sigma Xi</p><p><strong>Major awards</strong></p><p>Stuart Balantine Medal of the Franklin Institute, IEEE Cledo Brunetti Award, IEEE Centennial Medal, IEEE Fellow</p><h3></h3><br><p>“One result of all our work on microprocessors that has always pleased me,” Hoff told <em>IEEE Spectrum</em>, “is that we got computers away from those people.”<br/></p><p>By 1968 student hostility to the government over the Vietnam War was growing and life for researchers on campus who, like Hoff, relied on government funding was looking as if it might get uncomfortable. Hoff had already been contemplating the possibilities of industrial jobs when he received a telephone call from Robert Noyce, who told him he was starting a new company, <a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank">Intel Corp</a>., and had heard Hoff might be interested in a job. He asked Hoff where the semiconductor integrated circuit business would find its next growth area. “Memories,” Hoff replied.</p><p>That was the answer Noyce had in mind (Intel was launched as a memory manufacturer), and that year he hired Hoff as a member of the technical staff, Intel’s 12th employee. Working on memory technology, Hoff soon received a patent for a cell for use in MOS random-access integrated circuit memory. Moving on to become manager of applications research, he had the first customer contact of his career.</p><p class="pull-quote">“Engineering people tend to have a very haughty attitude toward marketing, but I discovered you learn a tremendous amount if you keep your eyes and ears open in the field.” <br/>—Hoff</p><p>“Engineering people tend to have a very haughty attitude toward marketing,” Hoff said, “but I discovered you learn a tremendous amount if you keep your eyes and ears open in the field. Trying to understand what problems people are trying to solve is very helpful. People back in the lab who don’t have that contact are working at a disadvantage.”</p><h2>From 12 Chips to One Microprocessor</h2><p>One group of customers with whom Hoff made contact were from Busicom Corp., Tokyo. Busicom had hired Intel to develop a set of custom chips for a low-cost calculator and had sent three engineers to Santa Clara to work on the chip designs. Hoff was assigned to look after them, getting them pencils and paper, showing them where the lunchroom was—nothing technical.</p><p>But the technical part of Hoff’s mind has no off-switch, and he quickly concluded that the engineers were going in the wrong direction. Twelve chips, each with more than 3000 transistors and 36 leads, were to handle different elements of the calculator logic and controls, and he surmised the packaging alone would cost more than the targeted retail price of the calculator. Hoff was struck by the complexity of this tiny calculator, compared with the simplicity of the PDP-8 minicomputer he was currently using in another project, and he concluded that a simple computer that could handle the functions of a calculator could be designed with about 1900 transistors. Given Intel’s advanced MOS process, all these, he felt, could fit on a single chip.</p><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br><img alt="Man sitting at patio table with large dog seated next to him. Laptop and coffee mug on table." class="rm-shortcode" data-rm-shortcode-id="6d1418ca3ba18981624e1c2d8a5f483b" data-rm-shortcode-name="rebelmouse-image" id="9b51c" loading="lazy" src="https://spectrum.ieee.org/media-library/man-sitting-at-patio-table-with-large-dog-seated-next-to-him-laptop-and-coffee-mug-on-table.jpg?id=31784380&width=980"/><h3></h3><br><p>The Busicom engineers had no interest in dumping their design in favor of Hoff’s unproved proposal. But Hoff, with Noyce’s blessing, started working on the project. Soon Mazor, then a research engineer at Intel, joined him, and the two pursued Hoff’s ideas, developing a simple instruction set that could be implemented with about 2000 transistors. They showed that the one set of instructions could handle decimal addition, scan a keyboard, maintain a display, and perform other functions that were allocated to separate chips in the Busicom design.</p><p>In October 1969, Hoff, Mazor, and the three Japanese engineers met with Busicom management, visiting from Japan, and described their divergent approaches. Busicom’s managers chose Hoff’s approach, partly, Hoff said, because they understood that the chip could have varied applications beyond that of a calculator. The project was given the internal moniker “4004.”</p><p>Federico Faggin, now president and chief executive officer of Synaptics Inc., San Jose, Calif., was assigned to design the chip, and in nine months came up with working prototypes of a 4-bit, 2300-transistor “microprogrammable computer on a chip.” Busicom received its first shipment of the devices in February 1971.</p><p>Faggin recalled that when he began implementing the microprocessor, Hoff seemed to have lost interest in the project, and rarely interacted with him. Hoff was already working on his next project, the preliminary design of an 8-bit microprogrammable computer for Computer Terminals Corp., San Antonio, Texas, which, architected by Computer Terminals, was named the 8008. Hoff always “had to do very cutting-edge work,” Faggin told <em>Spectrum</em>. “I could see a tension in him to always be at the forefront of what was happening.”</p><p>In those early Intel days, Mazor recalled that Hoff had a number of ideas for projects, many of which, though not commercially successful, proved prescient: a RAM chip that would act like a digital camera and capture an image in memory, a video game with moving spaceships, a device for programming erasable programmable ROMs, and computer-aided design tools intended for logic simulation.</p><p class="pull-quote">The Intel marketing department they estimated that sales [of microprocessors] might total only 2000 chips a year.</p><p>Meanwhile, the microprocessor revolution was gearing up, albeit slowly. Hoff joined Faggin as a microprocessor evangelist, trying to convince people that general-purpose one chip computers made sense. Hoff said his toughest sell was to the Intel marketing department.</p><p>“They were rather hostile to the idea,” he recalled, for several reasons. First, they felt that all the chips Intel could make would go for several years to one company, so there was little point in marketing them to others. Second, they told Hoff, ‘‘We have diode salesman out there struggling like crazy to sell memories, and you want them to sell computers? You’re crazy.” And finally, they estimated that sales might total only 2000 chips a year.</p><p>But word went out. In May 1971 an article in <em>Datamation</em> magazine mentioned the product, and the following November Intel produced its first ad for the 4004 CPU and placed it in <em>Electronic News</em>. By 1972 stories about the miracle of what began being called the microprocessor started appearing regularly in the press, and Intel’s competitors followed its lead by launching microprocessor products of their own.</p><p class="pull-quote">Hoff never even considered patenting the microprocessor. To him the invention seemed to be obvious.</p><p>One step Hoff did not take at that time was apply for a patent, even though he had already successfully patented several inventions. (Later, with Mazor and Faggin he filed for and was granted a patent for a “memory system for a multi-chip digital computer.”)</p><p>Looking back, Hoff recalled that he never even considered patenting the microprocessor in those days. To him the invention seemed to be obvious, and obviousness was considered grounds for rejecting a patent application (though, Hoff said bitterly, the patent office currently seems to ignore that rule). It was obvious to Hoff that if in one year a computer could be built with 1000 circuits on100 chips, and if in the following year those 1000 circuits could be put onto10 chips, eventually those 1000 circuits could be con­ structed on one chip.</p><p>Instead of patenting, Hoff in March 1970 published an article in the proceedings of the 1970 IEEE International Convention that stated: “An entirely new approach to design of very small computers is made possible by the vast circuit complexity possible with MOS technology. With from 1000 to 6000 MOS devices per chip, an entire central processor may be fabricated on a single chip.”</p><p>But in December 1970, an independent inventor outside the cliquish semiconductor industry, Gilbert Hyatt, filed for a patent on a processor and mentioned that it was to be made on a single chip. In 1990, after numerous appeals and extensions, Hyatt was granted that patent and began collecting royalties from many microprocessor manufacturers. Currently, though history traces today’s microprocessor back to Hoff, Mazor, and Faggin, the legal rights to the invention belong to Hyatt.</p><h2>The Invention of the Codec</h2><p>While the microprocessor has proved to be his most celebrated achievement, Hoff does not view it as his biggest technical breakthrough. That designation he reserves for the single-chip analog-to-digital/ digital-to-analog coder/decoder (codec).</p><p>“Now that work was an exciting technical challenge,” Hoff recollected with some glee, “because there were so many who said it couldn’t be done.”</p><p>The project was kicked off by Noyce, who spotted the telephone industry as ripe for new technology, and urged Hoff to find an important product for that market. Studying telephone communications, Hoff and several other researchers saw that digitized voice transmission, then being used between central offices, depended on the use of complex expensive codecs that tied into electromechanical switches.</p><p>”We thought,” Hoff told <em>Spectrum</em>, “we could integrate this, the analog-to-digital conversion, on a chip, and then use these circuits as the basis for switching.”</p><p>Besides reducing the cost of the systems to the telephone company, such chips would enable companies to build small branch exchanges that handled switching electronically.</p><p>Hoff and his group developed a multiplexed approach to conversion in which a single converter is shared by the transmit and receive channels. They also established a number of other techniques for conversion and decoding that Hoff saw as not being obvious and for which he received patents.</p><p>With that project’s completion in 1980, after six years of effort, and its transfer to Intel’s manufacturing facility in Chandler, Ariz., Hoff became an Intel Fellow, free to pursue whatever technology interested him. What interested him was returning to his work on adaptive structures, combining the concepts he had wrestled with at Stanford with the power of the microprocessor in the service of speech recognition. After a year he built a recognition system that Intel marketed for several years.</p><p>A prime customer for the system was the automotive industry. Its inspectors used the systems to help them check out a car as it finally left the assembly line. When an inspector noted out loud various problems that needed fixing, the system would prompt him for further information, and log his responses in a computer.</p><h3></h3><br><div class="rblad-ieee_in_content"></div><h3></h3><br/><h2>From Intel to Atari</h2><p>
	Though his position as an Intel Fellow gave Hoff a fair amount of freedom, he found himself getting bored. Intel’s success in microprocessors by 1983 had turned it into a chip supplier, and other companies were designing the chips into systems.
</p><p>
	“I had always been more interested in systems than in chips,” Hoff said, “and I had been at Intel for 14 years, at a time when the average stay at a company in Silicon Valley was three years. I was overdue for a move.”
</p><p>
	Again, Hoff had not gone beyond thinking about leaving Intel when a new job came to him. <a href="https://spectrum.ieee.org/pong" target="_self">Atari Inc</a>., Sunnyvale, Calif., then a booming video game company owned by Warner Communications Inc. and a major user of microprocessors, was looking for a vice president of corporate technology. In February 1983, after discussing the scope of the ideas that Atari researchers were pursuing, Hoff latched onto the opportunity.
</p><p>
	Intel from the start had a structured, highly controlled culture. At Atari, chaos reigned.
</p><p>
	Intel from the start had a structured, highly controlled culture. At Atari, chaos reigned. Under Hoff were research laboratories in Sunnyvale, Los Angeles, and Grass Valley, Calif.; Cambridge, Mass.; and New York City. Researchers were working on picture telephones, electronic aids for joggers, computer controls that gave tactile feedback, graphical environments akin to today’s virtual reality, digital sound synthesis, advanced personal computers, and software distribution via FM sidebands.
</p><p>
	But Hoff had barely had time to learn about all the research projects under way before the video game business took a well-publicized plunge. Without solid internal controls, Atari was unable to determine how well its games were selling at the retail point, and distributors were returning hundreds of thousands of cartridges and game machines. Hoff began receiving orders for staff cuts monthly.
</p><p>
	“It would have been one thing if I had known I had to cut back to, say, one-quarter the size of my group,” he told <em>Spectrum</em>. “But when every month you find you have to cut another chunk, morale really drops.”
</p><p>
	In July 1984, while Hoff was at his 30th high school reunion, Warner sold Atari to Jack Tramiel. Hoff then had to choose between convincing Tramiel that he could play a role in a narrowly focused company uninterested in funding futuristic research, and allowing Warner to buy out his contract. He chose the latter.
</p><p>
	Looking back, most of the people who were at Atari in those days now view them darkly. But Hoff recalls his year there as an enjoyable and ultimately useful experience. “Maybe I look at it more positively than I should,” he said, “but it turned out to be a good transition for me, and the life I have now is a very nice one.”
</p><p class="pull-quote">
	“Whenever you are working on one problem, there is always another problem over here that seems more interesting.”<br/>
	—Hoff
</p><p>
	He now spends half his time as a consultant and half pursuing technical projects of his own devising—a read­out device for machine tools, various types of frame grabbers, pattern recognition, and techniques for analog-to-digital conversion. This variegated schedule is perfect for him. He has always felt himself to be a generalist, and has had trouble focusing on just one technology.
</p><p>
	“It’s easy for me to get distracted,” he said. “Whenever you are working on one problem, there is always another problem over here that seems more interesting. But now it is more likely that my own projects get delayed, rather than things critical to other people and their employment.”
</p><p>
	Faggin for one is not surprised that such independent work appeals to Hoff. “He never was the gregarious type,” Faggin said. “He liked introverted work, the thinking, the figuring out of new things. That is what he is good at. I always was impressed how he was able to visualize an architecture for a new IC, practically on the spot.”
</p><p class="pull-quote">
	“He comes up with idea after idea, situation after situation. I think if he wanted to, Ted could sit down and crank out a patent a month.”<br/>
	—Gary Summers
</p><p>
	Said Gary Summers, president and chief executive officer of Teklicon Inc., Mountain View, the consulting firm that employs Hoff today: “He comes up with idea after idea, situation after situation. I think if he wanted to, Ted could sit down and crank out a patent a month.”
</p><p>
	“There is no doubt in my mind that he is a genius,” Mazor stated. Summers readily concurred.
</p><p>
	Hoff’s first project after Atari was a voice­controlled music synthesizer, which gave off the sound of a selected instrument when someone sang into it. Hoff’s biggest contribution to the project was a system that ensured that the emerging notes would be in tune, or at least harmonically complement the tune, even when the singer strayed off key. He scored another patent for this system, and the gadget was sold briefly through the Sharper Image catalog, but never became a big success.
</p><p>
	Hoff still contributes occasionally to product designs. At Teklicon, however, where he is vice president and chief technical officer, most of his consulting is done for lawyers. Hoff has a unique combination of long experience with electronic design and long-standing pack rat habits. His home workshop contains about eight personal computers of different makes and vintages, five oscilloscopes, including a vintage Tektronix 545 scope, 15000 ICs inventoried and filed, and shelves loaded with IC data books dating right back to the 1960s.
</p><p class="pull-quote">
	“If my washing machine breaks down, I call the repairman. Most clever engineers would buy the replacement gear and install it. Ted is capable of analyzing the reason the gear failed in the first place, redesigning a better gear from basic principles, carving it out of wood, casting it at his home, and dynamically balancing it on his lathe before installing it.”<br/>
	—Mazor
</p><p>
	When a lawyer shows him a patent disclosure, even one decades old, he can determine whether or not it could then have been “reduced to practice” and whether it provided sufficient information to allow “one of ordinary skill in the art” to practice the invention. Then he can build a model proving his conclusion, using vintage components from his collection, and demonstrate the model in court as an expert witness. This model-building can get very basic. On <em>Spectrum</em>’s visit, Rochelle salt crystals that Hoff attempted to grow for a recent court demonstration littered his workshop floor, next to metal-working equipment that he uses to build cases for his models.
</p><p>
	Hoff sees this ability to get down to basics as one of his strengths. “I relate things to fundamental principles,” he said. “People who don’t question the assumptions made going into a problem often end up solving the wrong problem.”
</p><p>
	Mazor said, “If my washing machine breaks down, I call the repairman. Most clever engineers would buy the replacement gear and install it. Ted is capable of analyzing the reason the gear failed in the first place, redesigning a better gear from basic principles, carving it out of wood, casting it at his home, and dynamically balancing it on his lathe before installing it.”
</p><p>
	Doing legal detective work appeals to Hoff for another reason: it gives him an excuse to hunt for interesting “antique” components at flea markets and electronics stores.
</p><p>
	Hoff cannot discuss the specifics of patent cases he has been involved with. Several recently were in the video game area; others have involved various IC companies. In a number of cases, Hoff was confident that his side was right, and his side still lost, so he felt little surprise when the microprocessor patent was granted to Hyatt. (After the award was made, though, he did sit down with Hyatt’s patent application and attempted to design a working microprocessor based on Hyatt’s disclosures. He found several incongruities—like a clock rate only suited to bipolar technology with logic that could only be rendered in MOS technology, and logic that required far too many transistors to put on a chip, proving in his mind that the award was incorrect.)
</p><p>
	Seeing someone else get credit for the microprocessor, particularly in recent media reports, “is irritating,” Hoff told <em>Spectrum</em>, “but I’m not going to let it bother me, because I know what I did, I know what all the other people on our project did, and I know what kind of company Intel is. And I know that I was where the action was.”<span class="ieee-end-mark"></span>
</p><p>
<em>Editor’s note: Hoff retired from Teklicon in 2007. He currently serves as a judge for the </em><a href="https://www.invent.org/collegiate-inventors/judging" rel="noopener noreferrer" target="_blank"><em>Collegiate Inventors Competition</em></a><em>, held annually by </em><a href="https://www.invent.org/" rel="noopener noreferrer" target="_blank"><em>the National Inventors Hall of Fame</em></a><em>. These days, his main technical interests surround energy, water, and climate change.</em>
</p></br></br></br></br></br></br>]]></description><pubDate>Sat, 08 Oct 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ted-hoff</guid><category>Intel 4004</category><category>Ted hoff</category><category>Microprocessor</category><dc:creator>Tekla S. Perry</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/man-in-white-striped-shirt-with-pocket-protector-seated-in-front-of-oscilloscope-and-other-electronics-equipment.jpg?id=31848732&amp;width=980"></media:content></item><item><title>The Electric Purple Snake-Oil Machine</title><link>https://spectrum.ieee.org/violet-ray</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-antique-medical-apparatus-consisting-of-a-black-wand-various-glass-tools-to-plug-into-the-wand-and-a-power-cord.jpg?id=31822581&width=1200&height=800&coordinates=0%2C150%2C0%2C150"/><br/><br/><p>
<strong>The violet ray machine </strong>has an awesome name that conjures up images of cartoon supervillains taking out Gotham, but its <em>actual</em> history is even odder—and it includes a superhero, not a villain.
</p><p>
	The technology underpinning the machine begins with none other than 
	<a href="https://ethw.org/Nikola_Tesla" rel="noopener noreferrer" target="_blank">Nikola Tesla</a> and his <a href="https://handwiki.org/wiki/Engineering:History_of_the_Tesla_coil" rel="noopener noreferrer" target="_blank">eponymous coil</a>. After Tesla and others made some refinements to the device, an influential clairvoyant named Edgar Cayce popularized violet ray machines for treating just about every kind of ailment—rheumatism and nervous conditions, acne and baldness, gonorrhea and prostate troubles, brain fog and writer’s cramp. Even Wonder Woman had her own health-restoring Purple Ray device. During the first half of the 20th century, a number of companies manufactured and sold the machines, which became ubiquitous for a time. And yet the scientific basis for the healing effects of violet rays was scant. So what accounted for their popularity?
</p><hr/><h2>The cutting-edge tech of the violet ray machine</h2><p>
	Violet ray machines employ a Tesla coil, also known as a resonance transformer, to produce a high-frequency, low-current beam, which is then applied to the skin. Nikola Tesla kicked off this line of invention after traveling to Paris during the summer of 1889 to attend the 
	<a href="https://en.wikipedia.org/wiki/Exposition_Universelle_(1889)" rel="noopener noreferrer" target="_blank">Exposition Universelle</a>. There he learned of Heinrich Hertz’s electromagnetic discoveries. Intrigued, Tesla returned to New York City to run some experiments of his own. The result was the Tesla coil, which he envisioned being used for wireless lighting and power. In April 1891, he applied for a <a href="https://patents.google.com/patent/US454622A/en?oq=454622" rel="noopener noreferrer" target="_blank">U.S. patent</a> for a “System of Electric Lighting,” which he received two months later. It would be the first in a series of related patents that spanned more than a decade.
</p><p>
	In May of that year, Tesla unveiled his wondrous invention to members of the American Institute of Electrical Engineers, during a 
	<a href="https://books.google.com/books?id=7YPbAAAAMAAJ&pg=PA145#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank">lecture</a> on his “Experiments with Alternate Currents of Very High Frequency and Their Application to Methods of Artificial Illumination.” He continued to test different circuit configurations and patented some (but not all) of his improvements, such as a “Means for Generating Electric Currents,” <a href="https://patents.google.com/patent/US514168A/en" rel="noopener noreferrer" target="_blank">U.S. Patent No. 514,168</a>. After more years of tinkering, Tesla perfected his resonance transformer and was granted <a href="https://patents.google.com/patent/US1119732A/en?oq=1119732" rel="noopener noreferrer" target="_blank">U.S. Patent No. 1,119,732</a> for an “Apparatus for Transmitting Electrical Energy” on 1 December 1914.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An old black and white photo showing a man sitting next to a large electrical apparatus that is emitting sparks." class="rm-shortcode" data-rm-shortcode-id="a3cb2a38509dbe855371a23276cb8951" data-rm-shortcode-name="rebelmouse-image" id="09a11" loading="lazy" src="https://spectrum.ieee.org/media-library/an-old-black-and-white-photo-showing-a-man-sitting-next-to-a-large-electrical-apparatus-that-is-emitting-sparks.jpg?id=31822607&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Nikola Tesla envisioned his eponymous coil being used for wireless lighting and power. It was also at the heart of the violet ray machine. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Stocktrek Images/Getty Images</small>
</p><p>
	Tesla promoted the 
	<a href="https://ieeexplore.ieee.org/document/771079" target="_blank">medical use</a> of the electromagnetic spectrum, suggesting to physicians that different voltages and currents could be used to treat a variety of conditions. His endorsement came at a time when trained doctors as well as shrewd hucksters were already experimenting with <a href="https://spectrum.ieee.org/this-1850s-medical-device-was-said-to-cure-toothache-gangrene-and-ennui" target="_self">electrotherapy</a> and <a href="https://spectrum.ieee.org/weve-been-killing-deadly-germs-with-uv-light-for-more-than-a-century" target="_self">ultraviolet light</a> to help patients or to make a buck, depending on your perspective.<br/>
</p><p>
	The market was perfectly primed for the violet ray machine, in other words. Tesla himself never commercialized a medical device based around his coil, but others did. The French physician and electrophysiologist 
	<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828069" target="_blank">Jacques-Arsène d’Arsonval</a> modified Tesla’s design to make the device safer for human use. It was further improved by another French doctor and electrotherapy researcher, <a href="https://en.wikipedia.org/wiki/Paul_Oudin" rel="noopener noreferrer" target="_blank">Paul Marie Oudin</a>. In 1893, Oudin crafted the first working prototype of what eventually became the violet ray machine. Four years later, Frederick Strong developed an American version.
</p><p class="pull-quote">
	An influential clairvoyant named Edgar Cayce popularized violet ray machines for treating just about every kind of ailment—rheumatism and nervous conditions, acne and baldness, gonorrhea and prostate troubles, brain fog and writer’s cramp.
</p><p>
	Another charismatic individual gets credit for popularizing the device: the psychic 
	<a href="https://psi-encyclopedia.spr.ac.uk/articles/edgar-cayce" target="_blank">Edgar Cayce</a>. As a young adult, Cayce reportedly lost his voice for over a year. No doctor could cure him, and in desperation he underwent hypnosis. He not only regained the ability to speak, he also began suggesting medical advice and homeopathic remedies. Cayce, who claimed to have had visions from childhood, became a professional clairvoyant, and for the next 40 years he dispensed his wisdom through <a href="https://www.edgarcayce.org/edgar-cayce/his-life/" target="_blank">psychic readings</a>. Out of more than 14,000 recorded readings, Cayce mentioned the violet ray machine almost 900 times. In case you doubt his status as an influencer, Cayce counted Thomas Edison, composer George Gershwin, and U.S. president Woodrow Wilson among his clients.<br/>
</p><h2>Was there nothing the violet ray machine couldn’t cure?</h2><h3></h3><br/><p>The popularity of violet ray machines exploded after 1915, once all of the components for a portable device could be easily manufactured. They could be plugged into a lamp or wall socket or wired to a battery—remember that most homes and businesses in the early 20th century were not yet electrified, and so most manufacturers offered both alternating and direct current options. The machine’s handheld wand consisted of a Tesla coil wrapped in an insulating material, such as Bakelite. The coil produced 1 to 2 kilovolts, which charged a condenser, and then discharged at a rate between 4 to 10 kilohertz when passed over the skin. A voltage selector controlled the intensity of the spark, creating anything from a mild sensation to something quite intense. This video shows the sparks coming from an antique machine:</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="9218b56f53f61d6c0c01b85cd8d96085" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/5c8mrnW2evI?rel=0&start=535" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>Glass electrodes—partially evacuated glass tubes known as Geissler tubes—could be inserted into the wand. These came in different shapes depending on their intended use. For example, a rake-shaped attachment worked to massage the scalp, while a narrow tube could be inserted into the mouth, nose, or another orifice. The high voltage ionized the gas within the glass tube, creating the purple glow that gave the device its name.<br/></p><p>
	Numerous manufacturers sprang up to produce the portable machines, including Detroit’s 
	<a href="https://americanhistory.si.edu/collections/search/object/nmah_736741" rel="noopener noreferrer" target="_blank">Renulife Electric Co</a>. Founded by inventor James Henry Eastman in 1917, Renulife sold different models for different uses. According to <a href="https://www.sindecusemuseum.org/blog/violet-ray" rel="noopener noreferrer" target="_blank">company literature</a>, Model M was its most popular general-purpose product, while Model D was for dentistry, and the tricked-out Model R [pictured at top] had finer regulation of current and a built-in ozone generator to help with head and lung congestion.
</p><p class="pull-quote">
	In 1917, editors at the 
	<i>Journal of the American Medical Association</i> reported that a violet ray generator certainly couldn’t treat “practically every ailment known to mankind,” as one manufacturer had claimed.
</p><p>
<a href="https://medicalhistory.uwo.ca/teaching_modules/eletrotherapy/Branston_Violet_Ray_Directions.pdf" rel="noopener noreferrer" target="_blank">Instructions</a> for the violet ray machines manufactured by Charles A. Branston Ltd. contain an alphabetical list of disorders that could be treated, from abscess to writer’s cramp, with dozens of other ailments in between. Like the Renulife products, the Branston machines also came in <a href="https://www.cppdigitallibrary.org/items/show/4433" rel="noopener noreferrer" target="_blank">different flavors</a>. The Branston machine’s high-frequency mode had germicidal effects and purportedly could be used to cure infections as well as relieve pain. Sinusoidal mode was used to gently massage away nervousness and paralysis. Ozone mode was for inhaling, to treat lung disorders. The Branston devices ranged in price from US $30 for the Model 5B (high-frequency mode only) to $100 for the Model 29 (which had all three modes).
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A page from a pamphlet showing the potential uses of an electrotherapeutic machine. " class="rm-shortcode" data-rm-shortcode-id="ef56a9a7f67c1f587c7f8591eaa6db2b" data-rm-shortcode-name="rebelmouse-image" id="9c209" loading="lazy" src="https://spectrum.ieee.org/media-library/a-page-from-a-pamphlet-showing-the-potential-uses-of-an-electrotherapeutic-machine.jpg?id=31822651&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The violet ray machines made by Charles A. Branston Ltd. had different modes for treating a wide variety of ailments.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Historical Medical Library/College of Physicians of Philadelphia</small>
</p><p>
	During the first half of the 20th century, manufacturers marketed the machines to doctors and consumers alike. By the time Wonder Woman debuted in her own comic book in June 1942, the violet ray machine was a well-known household technology. So it wasn’t too surprising that the superhero had a machine of her own.
</p><p>
	In the very first issue, Wonder Woman’s future love interest, Steve Trevor, is grievously injured in a plane crash. Seeking to cure his wounds, Diana works tirelessly for five days to complete her Purple Ray machine—but she’s too late. Trevor has died. Undeterred, Diana bathes her patient in the glowing light of the machine. The result might have embarrassed even the admen who wrote the promotional copy for Branston’s products: Wonder Woman’s Purple Ray 
	<a href="https://wonder-woman.fandom.com/wiki/Purple_Ray?file=Purple_Ray_WWv1-01.png" rel="noopener noreferrer" target="_blank">brings Trevor back to life</a>.
</p><h2>Science frowns on the violet ray machine </h2><p>
	Despite their popularity, the machines didn’t fare quite as well within the medical establishment. In 1917, editors at the 
	<em>Journal of the American Medical Association</em> reported that a violet ray generator certainly couldn’t treat “practically every ailment known to mankind,” as one manufacturer had claimed. Although the devices emitted a violet color, they were not in fact emitting ultraviolet light, or at least not in amounts that would be beneficial. In 1951, a Maryland district court ruled against a company named Master Appliances in a libel suit. The charge was misbranding, and the court found that the device was not an effective treatment nor capable of producing the claimed results. At the time, Master Appliances was one of the last manufacturers of violet ray machines in the United States, and the ruling effectively ended production in this country.
</p><p>
	And yet you can still buy violet ray machines today—both the antique variety and its 
	<a href="https://laserskinsolutions.com.au/skin-problems/what-is-high-frequency-and-why-do-we-use-it/" rel="noopener noreferrer" target="_blank">modern equivalent</a>. Today’s units are mainly marketed to aestheticians or sold for home use, and some dermatologists are not ready to categorically dismiss their benefits. Although they probably won’t cure indigestion or gray hair, the high frequency can dry out the skin and ozone does kill bacteria, so the machines may help treat acne and other skin conditions. Plus, there’s the placebo effect. As with all consumer electronics for which outrageous claims are made, let the buyer beware.
</p><p>
<em>Part of a </em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><em>continuing series</em></a> <em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em>
</p><p>
<em>An abridged version of this article appears in the October 2022 print issue.</em>
</p>]]></description><pubDate>Fri, 30 Sep 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/violet-ray</guid><category>Past forward</category><category>History of medical devices</category><category>Type:departments</category><category>Violet ray machine</category><category>Tesla coils</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-antique-medical-apparatus-consisting-of-a-black-wand-various-glass-tools-to-plug-into-the-wand-and-a-power-cord.jpg?id=31822581&amp;width=980"></media:content></item><item><title>Take a Trip Through Switzerland’s Museum of Consumer Electronics</title><link>https://spectrum.ieee.org/switzerlands-museum-of-consumer-electronics</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-old-computer-in-a-display-box-surrounded-by-other-old-computers.jpg?id=31728837&width=1200&height=800&coordinates=0%2C59%2C0%2C60"/><br/><br/><p>
	For more than a decade <a href="https://enter.ch/home/visit/?lang=en" rel="noopener noreferrer" target="_blank">Museum ENTER</a>, in Solothurn, Switzerland, has been a place where history buffs can explore and learn about the development and growth of computer and consumer electronics in Switzerland and the rest of the world. On display are computers, calculators, floppy disks, phonographs, radios, video game consoles, and related objects.
</p><p>
	Thanks to a new four-year partnership between the museum and the <a href="https://ieee.ch/" rel="noopener noreferrer" target="_blank">IEEE Switzerland Section</a>, IEEE members may visit the facility for free. They also can donate their time to help create exhibits; translate pamphlets, display cards, and other written media; and present science, technology, engineering, and math workshops.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="room full of historical audio radios" class="rm-shortcode" data-rm-shortcode-id="d7d959df925b5fdc0d2ff30436a34ce2" data-rm-shortcode-name="rebelmouse-image" id="87029" loading="lazy" src="https://spectrum.ieee.org/media-library/room-full-of-historical-audio-radios.jpg?id=31728919&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The technology on display includes televisions and radios from the 1950s.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">ENTER Museum</small></p><h2>Collections of calculators, radios, telephones, and televisions</h2><p>
	ENTER started as the private collection of Swiss entrepreneur Felix Kunz, who had been amassing computers and other electronics since the mid-1970s. Kunz and Peter Regenass—a collector of calculators—opened the museum in 2011 near the Solothurn train station.
</p><p>
	The museum’s collection focuses on the history of technology made in Switzerland by companies including <a href="https://en.wikipedia.org/wiki/Bolex" rel="noopener noreferrer" target="_blank">Bolex</a>, <a href="https://en.wikipedia.org/wiki/Crypto_AG" rel="noopener noreferrer" target="_blank">Crypto AG</a>, and <a href="https://www.cryptomuseum.com/manuf/gretag/index.htm" rel="noopener noreferrer" target="_blank">Gretag</a>. The technology on display includes early telegraphs, telephones, televisions, and radios.
</p><p>
	There are 300 mechanical calculators from Regenass’s collection. One of the mechanical calculators, Curta, looks like a pepper mill and has more than 700 parts.
</p><p>
	The museum also has several <em>Volksempfängers</em>, the early radio models used by the Nazis to spread propaganda.
</p><p>
	Visitors can check out the collection of working <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> computers, which the museum claims is the largest in Europe.
</p><h2>Free admission, discounts, and STEM education courses </h2><p>
	The IEEE Switzerland Section began its partnership with the museum last year, when the <a href="https://www.facebook.com/EPFLieeeSB" rel="noopener noreferrer" target="_blank">student branch</a> at the IEEE <a href="https://www.epfl.ch/en/" rel="noopener noreferrer" target="_blank">EPFL</a> hosted a <a href="https://www.youtube.com/watch?v=C9FnbAatUhA" rel="noopener noreferrer" target="_blank">presentation</a> there, says IEEE Senior Member <a href="https://www.linkedin.com/in/mathieu-c-303b1920/" rel="noopener noreferrer" target="_blank">Mathieu Coustans</a>, the Switzerland Section’s treasurer.
</p><p>
	In May, the section and the museum organized a <a href="https://enter.ch/wp-content/uploads/2022/06/Leserbeitrag_100JahreRadio.pdf" rel="noopener noreferrer" target="_blank">workshop celebrating 100 years of radio broadcasting in Switzerland</a>. IEEE members presented on the topic in French, Coustans says, and then translated the presentations to English.
</p><p>
	Based on the success of both events, he says, the section and the museum began to discuss how else they could collaborate.
</p><p>
	The two organizations discovered they have “many of the same goals,” says IEEE Member <a href="https://www.linkedin.com/in/violettavitacca/" rel="noopener noreferrer" target="_blank">Violetta Vitacca</a>, chief executive of the museum. They both aim to inspire the next generation of engineers, promote the history of technology, and bring together engineers from academia and industry to collaborate. The section and museum decided to create a long-term partnership to help each other succeed.
</p><p>
	In addition to the free visits, IEEE members receive a 10 percent discount on services offered by the museum, including digitizing books and other materials and repairing broken equipment such as radios and vintage record players. Members can donate historical artifacts too. In addition, IEEE groups are welcome to host conferences and section meetings at the facility.
</p><p>
	The IEEE Switzerland Section as well as members of student branches and the local <a href="https://www.ieee.org/communities/life-members/groups.html" rel="noopener noreferrer" target="_blank">IEEE Life Members Affinity Group</a> have agreed to speak at events held at the museum and teach STEM classes there.
</p><p>
	“The museum is a space where both professional engineers and young people can network and learn from each other,” Vitacca says. “I think this partnership is a win-win for both IEEE and the museum.”
</p><p>
	She says she hopes that “collaborating with IEEE will help Museum ENTER gain an international reputation.”
</p><p>
	The perks of the collaboration will become “especially attractive with the opening of the brand-new Museum ENTER building” next year, says IEEE Senior Member Hugo Wyss, chair of the Switzerland Section, who led the partnership effort.
</p><h2>Exhibits on gaming, inventors, and startups</h2><p>
	The museum is set to move in May to a larger building in the village of Derendingen. When it reopens there in November, these are some new additions visitors can look forward to:
</p><ul>
<li>Audio guides, display cards, and pamphlets in German, English, and French.</li>
<li>“The Academy,” which aims to inspire the next generation of engineers, offering workshops, lectures, and other events, as well as access to a technical library.</li>
<li>A data digitization laboratory where collectors and electronics enthusiasts can convert vintage media carriers, records, and film.</li>
<li>A public-gathering piazza with an attached café and meeting rooms.</li>
</ul><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Electronic in foreground with a group of children watching adult use something in his hands in background" class="rm-shortcode" data-rm-shortcode-id="28cd54c8d9fc95e0a3c25ff98504f5f7" data-rm-shortcode-name="rebelmouse-image" id="3d02b" loading="lazy" src="https://spectrum.ieee.org/media-library/electronic-in-foreground-with-a-group-of-children-watching-adult-use-something-in-his-hands-in-background.jpg?id=31728924&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The museum offers STEM workshops.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."> ENTER Museum</small></p><p>
	In addition, these eight permanent exhibits will be available, the museum says:
</p><ul>
<li><strong>Game Area.</strong> A display featuring innovations that have driven the rise of gaming and high-performance computing.</li>
<li><strong>Hall of Brands.</strong> A showcase of technologies from well-known companies.</li>
<li><strong>Now.</strong> Current technology highlighted in the news.</li>
<li><strong>Show of Pioneers.</strong> A look at the inventors of popular consumer and computer electronics.</li>
<li><strong>Switzerland Connected. </strong>A showcase for the country’s former and current accelerators, startups, and schools.</li>
<li><strong>Time Travel. </strong>A retrospective look at 150 years of technology.</li>
<li><strong>Typology of Technology. </strong>Applications such as optical and magnetic recording used for music and film.</li>
</ul><p>
	The museum also plans to curate special exhibitions.
</p><p>
	“We are going from being simply a museum with an extensive collection to being a center for networking, education, and innovation,” Vitacca says. “That’s why it’s important for the museum to collaborate with IEEE. Our offerings are not only unique in Switzerland but also across Europe. IEEE is a great partner for us to help get the word out about what we do.”
</p>]]></description><pubDate>Fri, 16 Sep 2022 18:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/switzerlands-museum-of-consumer-electronics</guid><category>Ieee news</category><category>Ieee member news</category><category>Ieee history</category><category>Museum</category><category>Consumer electronics</category><category>History of technology</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-old-computer-in-a-display-box-surrounded-by-other-old-computers.jpg?id=31728837&amp;width=980"></media:content></item><item><title>In 1961, the First Robot Arm Punched In</title><link>https://spectrum.ieee.org/unimation-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-robot-arm-sitting-on-a-rectangular-base-with-control-panels-at-the-back.jpg?id=31198107&width=1200&height=800&coordinates=0%2C125%2C0%2C125"/><br/><br/><p><strong>When they met </strong>at a cocktail party in 1956, <a href="https://spectrum.ieee.org/george-devol-a-life-devoted-to-invention-and-robots" target="_self">George Devol</a> and <a href="https://www.automate.org/a3-content/joseph-engelberger-about" target="_blank">Joseph Engelberger</a> instantly bonded over their love of science fiction, happily chatting about Isaac Asimov and his theories on robotics. Then the conversation turned more serious.  </p><p>Two years earlier, Devol had filed a <a href="https://patents.google.com/patent/US2988237A" target="_blank">patent</a> for a general-purpose manipulator for performing repetitive tasks, and he was looking for a financial backer. Engelberger was curious about how to apply flexible machines for factory automation. By the end of the evening, a plan was in motion that would change the world of manufacturing.  </p><hr/><h2>How the first industrial robot got its start </h2><p>Devol and Engelberger seemed an unlikely pair. Devol was an inventor and entrepreneur who had founded several companies to commercialize his <a href="https://gizmodo.com/five-little-known-inventions-from-deceased-robot-maker-5831586" target="_blank">various inventions</a>. After graduating from high school in 1932, he had started United Cinephone, to improve sound quality on the new talking motion pictures. He quickly moved on to other inventions, including the <a href="https://pdfpiw.uspto.gov/.piw?PageNum=0&docid=02243310" target="_blank">Phantom Doorman</a>, an automated door-opening system; a proximity-controlled laundry press, the patent for which was withheld by the U.S. government through World War II; and the <a href="https://www.thehenryford.org/collections-and-research/digital-collections/artifact/373945/" target="_blank">Speedy Weeny</a>, a vending machine that dispensed hot dogs cooked by microwaves.  </p><p>Meanwhile, Engelberger was pursuing a more traditional corporate career. He had earned a bachelor’s degree in physics in 1946 and a master’s in electrical engineering in 1949, both from Columbia University. When he met Devol, he was working as an engineer for Manning, Maxwell, and Moore, a company that specialized in <span></span>safety valves, pressure gauges, and other industrial control equipment. But that soon changed.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of two men at a bar being served drinks by a robotic arm " class="rm-shortcode" data-rm-shortcode-id="fff4b26db63b63e840310e88f44708d2" data-rm-shortcode-name="rebelmouse-image" id="85d44" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-two-men-at-a-bar-being-served-drinks-by-a-robotic-arm.jpg?id=31198187&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A robot serves drinks to Unimation cofounders Joseph Engelberger [left] and George Devol.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The Henry Ford</small></p><p>Shortly after the cocktail party, the two men founded Unimation, the name a portmanteau of “universal” and “automation,” which Devol had coined when filing his patent. The pair needed funding. A year later, Engelberger founded Consolidated Controls Corp. in an attempt to keep his former team from Manning, Maxwell, and Moore together, after MM&M liquidated the aerospace division that Engelberger had led. Engelberger then convinced Norman Schafler, CEO of Condec Corp. (the parent company of Consolidated Controls), to finance the development of Devol’s invention. Condec eventually purchased Unimation in 1960 and bought Devol’s patent when it was finally issued the next year.</p><p>While Devol’s patent was pending, a team of <span></span>six men set out to turn his idea into a reality. They decided initially to build the mechanical arm with five degrees of freedom. This later evolved into a six-axis machine that emulated the human shoulder, elbow, and wrist. It had a self-contained hydraulic supply operating at 6.9 megapascals (1,000 pounds per square inch). The men had to invent both the programming controls and memory systems, as well as the digital optical encoders for determining shaft positions. Weighing in at about 1,360 kilograms (3,000 pounds), the prototype was a beast, but it could pick up parts of up to 45 kg (100 pounds); later models could handle objects five times as heavy. The Unimate was both squatter and wider than the average worker, measuring 1.6 meters high, 1.2 meters wide, and 1.5 meters deep. </p><p>This video from <a href="https://www.thehenryford.org/" target="_blank">The Henry Ford</a>, in Dearborn, Mich., where the Unimate pictured at top resides, shows the robot in action. </p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="0c0587beb887ea18a70841740e8380a3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/-Xl2c91pWGc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">First Industrial Robot Ever Installed on an Assembly Line | Innovation Nation</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=-Xl2c91pWGc" target="_blank">www.youtube.com</a>
</small>
</p><p>Although programmable, the original Unimate <a href="https://blog.robotiq.com/the-history-of-robot-programming-languages" target="_blank">did not have a programming language</a>. Instead, the user would move the machine head to the desired location, and a position detector would record the spot. The user would repeat this process to record a sequence of events that the Unimate could play back. When Unimation acquired <a href="https://ethw.org/Oral-History:Victor_Scheinman" target="_blank">Victor Scheinman</a>’s Vicarm robot, it also acquired the robot’s programming language, <a href="https://ieeexplore.ieee.org/abstract/document/762620" target="_blank">VAL</a>. Beginning in 1973, VAL became a part of Unimation’s industrial robots.<br/></p><h2>How the Unimate fit into GM’s automation plans </h2><p>As Devol and the team worked out the technical details, Engelberger tried to find a market and customers for the new robot. He visited 15 automobile factories and 20 other manufacturers, trying to assess the industrial needs for a robotic arm. General Motors was the first company to bite.  </p><p>On a fateful day in 1961, the Unimate pictured at top took its place on the assembly line of General Motors’ Ternstedt plant, in Trenton, N.J. It took over the dirty, dreary, and dangerous job of unloading the finished castings from a die-cast press. Its debut was not particularly glamorous or impressive, but it earned the Unimate a place in history as the first successful application of an industrial robot. </p><p>According to Johanna Wallén’s “<span></span><a href="http://www.diva-portal.org/smash/get/diva2:316930/fulltext01.pdf" target="_blank">The History of the Industrial Robot</a>,” GM paid US $18,000 (about $180,000 today) for the first Unimate, which was a tremendous discount off the estimated $65,000 it took to produce the machine. Engelberger accepted the price because he recognized the value in being able to point to GM as a client. He also had faith that more orders would come in. By the early 1970s, the list price for a Unimate was $35,000. Unimation <a href="https://www.nytimes.com/1982/03/21/business/he-brought-the-robot-to-life.html" target="_blank">didn’t turn a profit</a> until <span></span>1975, 19 years after its founding. </p><p class="pull-quote">In 1961, GM paid $18,000 for its first Unimate, a tremendous discount off the estimated $65,000 it took to produce the machine.</p><p>While Devol was the technical genius, Engelberger had a flair for marketing. At the time, many workers worried that their jobs would be replaced by robots, so Engelberger took pains to make the Unimate seem personable. The robot killed it on “The Tonight Show With Johnny Carson” by opening a can of beer and conducting the band. Engelberger used a slightly different pitch when addressing executives. A spoof résumé for the Unimate said it could work 24 hours per day and was a fast learner that never forgets (except on command), never demands a wage increase, and remains equable, despite abuse. </p><p>Workers at GM’s Trenton plant didn’t initially object to the Unimate. After all, unloading the die-cast machine was a terrible job that no one wanted to do. In 1967, GM installed the first two spot-welding robots at its Norwood, Ohio, plant, also without much controversy. But when the company launched a spot-welding line of 28 robots in 1970 at its Lordstown, Ohio, plant, workers protested. </p><p>The Lordstown plant is often viewed as a textbook example of factory workers revolting against automation. The new line of robots was just part of a bigger plan to ratchet up productivity. GM had chosen Lordstown to produce its first subcompact car, the Chevrolet Vega. In 1971, the General Motors Assembly Division retooled the plant to increase efficiency, resulting in the layoffs of at least 700 workers, the reassignment of hundreds more, and the elimination of 300 to 500 line jobs considered too easy for human workers. They also increased line speeds by 67 percent, from 60 cars per hour to 101 cars per hour, with the aim of producing nearly 400,000 Vegas in 1972.  </p><p>To meet production quotas, management established mandatory overtime with shifts running 11 to 12 hours including weekends. When employees objected, supervisors reacted by firing or laying off the most militant. Workers filed over 16,000 grievances and began staging strikes, blocking the front gates, and threatening supervisors. In March 1972, a strike completely shut down the plant for 22 days. </p><p>It seems likely that the experience at Lordstown tainted Unimation’s reputation in the United States. In any event, the company was slow to find a market in its home country. </p><h2>Unimation helped spark the industrial robotics revolution in Japan  </h2><p>Unimation had much <a href="https://robotics.kawasaki.com/en1/anniversary/history/history_01.html" target="_blank">better success in Japan</a>. In 1966, Engelberger held a multihour question-and-answer session with hundreds of leaders in Japanese industry, in hopes of finding a technical partner for manufacturing its robots in Japan. Interest was intense, and Kawasaki Aircraft Industries aggressively fought for the deal, eventually winning over Engelberger. In June 1968, Kawasaki Aircraft opened its Office for Promoting Domestic Production of Industrial Robots, and the following October it announced a partnership with Unimation. The first Kawasaki-Unimate 2000 was completed in 1969 and cost 12 million yen per unit. </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a large robotic arm on a massive base that is almost as tall as the 4 men standing behind it.\u00a0" class="rm-shortcode" data-rm-shortcode-id="b7a3d3d9b0a0865d0b7731ddcd2be8a1" data-rm-shortcode-name="rebelmouse-image" id="522b5" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-large-robotic-arm-on-a-massive-base-that-is-almost-as-tall-as-the-4-men-standing-behind-it-u00a0.jpg?id=31199575&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Unimate was designed to automate the “dirty, dreary, and dangerous” jobs that human workers didn’t want to do.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The Henry Ford</small></p><p>Similar to the U.S. manufacturers that first deployed robots for the dirty, dangerous, and dreary jobs, Japanese companies used the Unimate to tackle the “3K” jobs: <em>kitsui</em> (difficult), <em>kitanai</em> (dirty), and <em>kiken</em> (dangerous). They targeted welding positions, which they were struggling to fill. Working day and night, a Kawasaki-Unimate welder did the work of 20 human welders. Still, it took a decade to be widely adopted. </p><p class="pull-quote">By the mid-1980s, Japan had become a robot kingdom, boasting nearly 70 percent of all the robots in the world. </p><p>In the 1970s, Unimation expanded into human-scale robotic arms when it acquired Vicarm, which it rebranded the Programmable Universal Machine for Assembly, or PUMA. The company worked with General Motors to develop the PUMA for light assembly work. In contrast to the hulking Unimate, the PUMA was designed to manipulate small items, such as installing lightbulbs. According to Dick Beecher, director of robotics and artificial intelligence at General Motors during this time, about 95 percent of the parts in an automobile weigh less than 1.5 kg, and GM had thousands of human workers doing “light assembly.” It was a growth market for robots.  </p><p><span></span>In 1983, Condec sold Unimation to Western Electric for $107 million. Engelberger shifted his focus to service robots, seeing an opportunity for automating home and hospital health care. Devol continued to invent but moved away from robotics. Together, the two men are often referred to as the father (Engelberger) and grandfather (Devol) of industrial robots. </p><p>Although the Unimate debuted in the United States, it built its reputation in Japan and helped propel the use of industrial robots there. By the mid-1980s, Japan had become a robot kingdom, boasting nearly 70 percent of all the robots in the world. Robots have since spread across the globe and transformed many industries—but the spark was a chance conversation over cocktails. </p><p><em>Part of a </em><a href="https://spectrum.ieee.org/search/?q=past+forward" target="_self"><em>continuing series</em></a><em> </em><em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em> </p><p><em>An abridged version of this article appears in the September 2022 print issue as “Unimate Punches In.”</em></p>]]></description><pubDate>Tue, 30 Aug 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/unimation-robot</guid><category>Joseph engelberger</category><category>Past forward</category><category>Unimate</category><category>George devol</category><category>Industrial robot</category><category>Type:departments</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-robot-arm-sitting-on-a-rectangular-base-with-control-panels-at-the-back.jpg?id=31198107&amp;width=980"></media:content></item><item><title>The Godfather of South Korea’s Chip Industry</title><link>https://spectrum.ieee.org/kim-choong-ki</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-in-a-dark-suit-bald-with-some-grey-hair-leans-against-a-shiny-blue-wall-in-which-he-is-reflected.png?id=31216838&width=1200&height=800&coordinates=0%2C203%2C0%2C203"/><br/><br/><p>
	They were called “Kim’s Mafia.” Kim Choong-Ki himself wouldn’t have put it that way. But it was true what semiconductor engineers in South Korea whispered about his former students: They were everywhere.
</p><p>
	Starting in the mid-1980s, as chip manufacturing in the country accelerated, engineers who had studied under Kim at 
	<a href="https://www.kaist.ac.kr/en/" target="_blank">Korea Advanced Institute of Science and Technology</a> (KAIST) assumed top posts in the industry as well as coveted positions teaching or researching semiconductors at universities and government institutes. By the beginning of the 21st century, South Korea had become a dominant power in the global semiconductor market, meeting more than 60 percent of international demand for memory chips alone. Around the world, many of Kim’s protégés were lauded for their brilliant success in transforming the economy of a nation that had just started assembling radio sets in 1959 and was fabricating outdated memory chips in the early ’80s.
</p><hr/><p>
	That success can be traced in part to Kim, now an emeritus professor at KAIST. Of average height, with gray hair since his mid-30s, he was the first professor in South Korea to systematically teach semiconductor engineering. From 1975, when the nation had barely begun producing its first transistors, to 2008, when he retired from teaching, Kim trained more than 100 students, effectively creating the first two generations of South Korean semiconductor experts.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A crowd of more than two dozen people seated and standing atop a rock formation. A large white sign is at center." class="rm-shortcode" data-rm-shortcode-id="dd2f48de8c71c758be94a613d3c278fd" data-rm-shortcode-name="rebelmouse-image" id="65ad3" loading="lazy" src="https://spectrum.ieee.org/media-library/a-crowd-of-more-than-two-dozen-people-seated-and-standing-atop-a-rock-formation-a-large-white-sign-is-at-center.png?id=31216852&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Kim and his former students and their families celebrate his 60th birthday on the summit of South Korea’s Mount Deokyu.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chang Hae-Ja</small>
</p><p>
	The Samsung Welfare Foundation recognized Kim’s influence when it 
	<a href="http://www.hoamfoundation.org/eng/award/part_view.asp?idx=10" target="_blank">awarded him its prestigious Ho-Am Prize in 1993</a> for “building a solid foundation for Korea’s semiconductor industry.” Since then, he has been revered in the South Korean media as the industry’s “godfather.” Yet even today, Kim remains largely unknown outside of South Korea’s chip community. Who, then, is this inconspicuous semiconductor “Mafia” boss?
</p><h2>A Start in Camera Chips<br/>
</h2><p>
	Kim Choong-Ki was born in Seoul in 1942, when Korea was a colony of the Japanese Empire. His mother taught elementary school; his father, Kim Byung-Woon, was a textile engineer for 
	<a href="http://www.kyungbang.co.kr/eng/company/comp_hist.asp?leftselect=3" target="_blank">Kyungbang</a>, Korea’s iconic manufacturer of yarns and fabrics. The elder Kim had helped build the company’s first spinning factory, and his engineering savvy and consequent renown impressed his son. “He made a daily tour of the factory,” the younger Kim recalls. “He told me that he could detect which machines were in trouble and why, just by listening to them.” Such lessons planted the seed of an ethos that would drive Kim Choong-Ki’s career—what he came to call the “engineer’s mind.”
</p><p>
	Growing up, Kim Choong-Ki was a model South Korean student: bookish, obedient, and silent. Although his family pressed him to join his father in the textile industry, he instead chose to pursue electrical engineering. He studied at Seoul National University and then at Columbia University, in New York City, where he earned his doctorate under 
	<a href="https://ieeexplore.ieee.org/author/37272530500" target="_blank">Edward S. Yang</a>, a specialist in transistor theory. Shortly after, in the summer of 1970, Fairchild Camera and Instrument hired Kim to work in its research and development laboratory in Palo Alto, Calif.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A young man in a jacket and tie stands before a pillared building with dozens of steps leading up to it." class="rm-shortcode" data-rm-shortcode-id="e18e5ef96f164ac42efefec0e8e32262" data-rm-shortcode-name="rebelmouse-image" id="db5be" loading="lazy" src="https://spectrum.ieee.org/media-library/a-young-man-in-a-jacket-and-tie-stands-before-a-pillared-building-with-dozens-of-steps-leading-up-to-it.png?id=31218218&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Kim, shown on the Columbia campus, studied for his Ph.D. at the university under Edward S. Yang, a specialist in transistor theory. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chang Hae-Ja</small>
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A woman with short grey hair is seated at a table. Behind her stand an older man in a hat and young man." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="e589f40bf4f1d9a971469aadccf08b0a" data-rm-shortcode-name="rebelmouse-image" id="88d46" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-with-short-grey-hair-is-seated-at-a-table-behind-her-stand-an-older-man-in-a-hat-and-young-man.png?id=31216878&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Kim’s mother and father, a renowned Korean textile engineer, visit him in Palo Alto, Calif., in 1972.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Chang Hae-Ja</small>
</p><p>
	Since World War II, Fairchild Camera had been the world’s leading developer of imaging equipment, including radar cameras, radio compasses, and X-ray machines. In 1957, the company launched the division Fairchild Semiconductor to fabricate transistors and integrated circuits from silicon, then an innovative move, as most semiconductor devices at the time used germanium. The venture spawned dozens of products, including the first silicon integrated circuit, thus fueling the rise of Silicon Valley. As a newcomer to Fairchild’s R&D lab, Kim was put to work on one of these new kinds of chips: the charge-coupled device.
</p><p>
	Just the year before, in 1969, George E. Smith and Willard Boyle at Bell Laboratories 
	<a href="https://ieeexplore.ieee.org/document/6768140" target="_blank">proposed the idea of the CCD</a>, for which they would later win a Nobel Prize. But it was Kim and his colleagues at Fairchild who realized the first CCD devices that evolved into commercial products widely used in digital photography, radiography, and astronomy. Kim became so proficient in CCD technology that other engineers at the company regularly dropped by his office at the end of the day to pick his brain. “Soon they began to call me Professor CCD,” he remembers.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A young man in a jacket and tie stands in front of a pale brick wall with lettering attached." class="rm-shortcode" data-rm-shortcode-id="4713a1bfc7179181ffafe819904be5e0" data-rm-shortcode-name="rebelmouse-image" id="74e66" loading="lazy" src="https://spectrum.ieee.org/media-library/a-young-man-in-a-jacket-and-tie-stands-in-front-of-a-pale-brick-wall-with-lettering-attached.png?id=31218403&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Kim’s colleagues at Fairchild Semiconductor’s research and development laboratories called him “Professor CCD.”</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chang Hae-Ja</small>
</p><p>
	 Among other inventions, Kim helped develop a 
	<a href="https://ieeexplore.ieee.org/document/1451152" target="_blank">CCD area image sensor that greatly improved low-light detection</a> and the first <a href="https://ieeexplore.ieee.org/document/6219610" target="_blank">two-phase CCD linear image sensor</a>—which, he reported, guaranteed “the ease of use and the high quality of image reproduction.” “Fairchild’s—or better call them Choong-Ki’s—CCDs made possible the wide applications in high-resolution cameras,” Columbia’s Yang says. Without these functional devices, he adds, “there would be no Nobel Prize for the CCD.”
</p><p>
	Kim’s time at Fairchild transformed him as much as it did camera technology. His schooling in South Korea and at Columbia had primarily emphasized book learning and theory. But his experience at Fairchild solidified his belief, first inspired by his father, that a true “engineer’s mind” requires practical skill as much as theoretical knowledge. In addition to performing experiments, he made a habit of reading internal technical reports and memos that he found at the company library, some of which he later brought to KAIST and used as teaching material.
</p><p>
	At Fairchild, Kim also learned how to communicate with and lead other engineers. When he started there, he was soft-spoken and introverted, but his mentors at Fairchild encouraged him to express himself confidently and clearly. Later, the converted Kim would become the “loudest-speaking” professor at KAIST, according to several fellow faculty members, and they say his absence made the whole campus seem quiet.
</p><p>
	Kim rose quickly within Fairchild’s hierarchy. But just five years into his tenure, he returned to South Korea. His beloved father had died, and, as the eldest son, he felt a heavy responsibility to care for his widowed mother. Racial discrimination he experienced at Fairchild had also hurt his pride. Most important, however, he had found an ideal place to work back home.
</p><p>
	Then called KAIS (the “T” was added in 1981), Kim’s new employer was the first science and technology university in South Korea and remains one of the most prestigious. The South Korean government had established the institute in 1971 with financing from the United States Agency for International Development and had invited 
	<a href="https://ieeexplore.ieee.org/document/6370209" target="_blank">Frederick E. Terman</a>, the legendary dean of Stanford University’s school of engineering and a “father” of Silicon Valley, to draw up the blueprint for its direction. Terman stressed that KAIS should aim to “satisfy the needs of Korean industry and Korean industrial establishments for highly trained and innovative specialists, rather than to add to the world’s store of basic knowledge.” It was the perfect place for Kim to spread his newfound philosophy of the “engineer’s mind.”
</p><h2>South Korea’s Founding Lab<br/>
</h2><p>
	Kim’s laboratory at KAIS attracted scores of ambitious master’s and doctoral candidates from almost the moment he arrived in the spring of 1975. The primary reason for the lab’s popularity was obvious: South Korean students were hungry to learn about semiconductors. The government touted the importance of these devices, as did electronics companies like GoldStar and Samsung, which needed them to manufacture their radios, televisions, microwaves, and watches. But the industry had yet to mass-produce its own chips beyond basic integrated circuits such as CMOS watch chips, in large part due to a lack of semiconductor specialists. For 20 years, until the mid-1990s, joining Kim’s lab was essentially the only way for aspiring semiconductor engineers in South Korea to get hands-on training; KAIS was the only university in the country that had able teachers and proper facilities, including clean rooms for assembling high-quality chips.
</p><p>
	But it wasn’t KAIST’s virtual monopoly on semiconductor training that made Kim a mentor without peer. He introduced a style of teaching and of mastering engineering that was new to South Korea. For instance, his conviction that an “engineer’s mind” requires equal parts theory and application at first puzzled his students, who regarded engineering as chiefly a scholarly discipline. Although they were proficient in mathematics and well read, most of them had never carried out any serious work in design and construction.
</p><p>
	Therefore, one of the first lessons Kim taught his students was how to use their hands. Before they embarked on their own projects, he put them to work cleaning the lab, repairing and upgrading equipment, and tracking down necessary parts. In this way, they learned how to solve problems for themselves and how to improvise in situations for which no textbook had prepared them. Their view of what it means to be an engineer changed profoundly and permanently. Many of them confess they still repeat Kim’s dicta to this day. For example: “Don’t choose the subjects that others have already thrown into the trash can.” And: “Scientists consider 
	<em>why</em> first, but we engineers must think <em>how</em> first.” And: “Wrong decision is better than slow decision.”
</p><p>
	Kim’s former students remember him as kind, humorous, nonauthoritarian, meticulous, and hardworking. But they also say he was strict and could be hot tempered and even terrifying, especially when he thought they were being lazy or sloppy. Legend has it that some of his students entered the lab via a ladder from the rooftop to bypass Kim’s office. One of his biggest grievances was when students failed to properly balance theory and practice. “Make it yourself; then we will start a discussion,” he scolded those who focused too much on intellectual study. On the other hand, he said, “Why don’t you use something malleable within the hard nut on your neck?” as a reproach to those who spent too much time building things, implying that they should also use their brains.
</p><h3>Meet Kim’s Mafia</h3><br/><p>Many of Kim Choong-Ki’s former students helped lead the rise of semiconductor engineering in South Korean through prominent roles in industry, government, and academia. Here are some of the standouts.</p><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="f6889df775be3956a2003f8670c69911" data-rm-shortcode-name="rebelmouse-image" id="e450d" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254629&width=980"/><p><strong>Kyung Chong-Min</strong>: As Kim’s first doctoral candidate at KAIST, Kyung became a professor at his alma mater at the age of 30. He is best known for designing microprocessor chips in the early 1990s that were fully compatible with Intel 80386 and 80486 chips. He also established and operated two centers for chip design. </p><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="3e48b9bec1113162ce6e6795c5af340b" data-rm-shortcode-name="rebelmouse-image" id="e0050" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254624&width=980"/><p><strong></strong><strong>Kwon Oh-Hyun:</strong> Kwon received a master’s degree under Kim and his doctoral degree from Stanford University. At Samsung Electronics, he developed 64-megabit DRAM in the early 1990s and contributed to the development of the company’s System LSI division in the 2000s. He served as CEO and vice-chairman of the company during most of the 2010s. </p><h3></h3><br/><div class="horizontal-rule">
</div><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="9b4083b44637cf08b8ab606f491b69f1" data-rm-shortcode-name="rebelmouse-image" id="dc929" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254597&width=980"/><p>
<strong></strong><strong>Lim Hyung-Kyu:</strong> Lim studied PMOS devices under Kim and later became Samsung’s first overseas scholarship student (at the University of Florida). He was best known for developing NAND flash memory in the early 1990s and was often called “Mr. NAND Flash.” After retiring from Samsung, he served as vice-chairman of the SK conglomerate.
</p><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="e6d73b22a27d00db318b7336d3d2d8b3" data-rm-shortcode-name="rebelmouse-image" id="3cb90" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254590&width=980"/><p><strong></strong><strong>Suh Kang-Deog:</strong> Suh received both master’s and doctoral degrees under Kim. At Samsung Electronics, he participated in various memory projects including NAND flash. In 2006 he was elected a Samsung Fellow, the company’s highest honor, for his contributions and in expectation of his future work.  </p><h3></h3><br/><div class="horizontal-rule">
</div><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="d8106f61fe728d8946c478c69c246fb7" data-rm-shortcode-name="rebelmouse-image" id="b9b5b" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254571&width=980"/><p><strong></strong><strong>Cho Byung-Jin:</strong> Cho has authored more than 300 technical papers on various subjects. He briefly worked at Hyundai Electronics (now Hynix) and then became a star engineering professor at the National University of Singapore for 10 years. He returned to KAIST in 2007 and pioneered research in graphene and thermoelectric devices.</p><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="88d71368f4ec4414324a283038243e64" data-rm-shortcode-name="rebelmouse-image" id="d7338" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254565&width=980"/><p><strong>Ha Yong-Min:</strong> Ha is the master of TFT-LCD and OLED technologies at LG Display, having worked on the subject from his graduate years under Kim to the present. Thanks to his efforts, LG Display became a major supplier of TFT-LCD and OLED panels for medium and small electronic devices, including those marketed by Apple, HP, Dell, and Lenovo.</p><h3></h3><br/><div class="horizontal-rule">
</div><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="a1c5f6313c8f09af31ef8385e8e3c499" data-rm-shortcode-name="rebelmouse-image" id="415ac" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254561&width=980"/><p><strong>Park Sung-Kye:</strong> Park, sometimes called the “treasure of Hynix,” has developed almost all types of memory chips, including highly efficient and speedier DRAMs and the smallest NAND flash-memory cell. He was also in charge of the development of the 96-layer 3-D NAND flash memory in the late 2010s.</p><h3></h3><br/><img class="rm-shortcode" data-rm-shortcode-id="df3c6f293741bb3692c6c0b599d69436" data-rm-shortcode-name="rebelmouse-image" id="68045" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31254749&width=980"/><p><strong>Chung Han:</strong> Chung is perhaps the most successful engineer-turned-startup-entrepreneur among Kim’s former students. His i3system develops sophisticated infrared image sensors for both defense and commercial markets. Thanks to his more than 30 years of effort, South Korea became the seventh nation in the world to mass-produce infrared image sensors.</p><p>
	 Kim influenced not only his own students but also countless others through his openness. He cooperated with and even shared laboratory space with other KAIST professors, and he liked to visit other departments and universities to give seminars or simply to gain new ideas and perspectives—behavior that was, and still is, very unusual in South Korean academic culture. In his autobiography, 
	<a href="https://cgt.ucsd.edu/fellows/past/chin.html" target="_blank">Chin Dae-Je</a>, who developed 16-megabit DRAM at Samsung in 1989 and later served as South Korea’s minister of information and technology, recounts seeking out Kim’s tutelage when Chin was a graduate student at Seoul National University in the mid-1970s. “There was an intense spirit of competition” between SNU and KAIST, recalls Chin, whose alma matter labeled him a “problem student” for studying with a rival professor.
</p><p>
	Kim’s collegiality extended beyond academia to industry and government . In the early 1980s, during a sabbatical, he led semiconductor research and development at the government-funded 
	<a href="https://www.etri.re.kr/eng/main/main.etri" target="_blank">Korea Institute of Electronics Technology</a>, which developed both 32-kilobit and 64-kilobit ROM under his directorship. His popular semiconductor workshops at KAIST inspired GoldStar (LG since 1995), Hyundai Electronics (Hynix since 2001), and Samsung to sponsor their own training programs at KAIST in the 1990s. Kim’s close partnership with these companies also helped launch other pioneering mostly-industry-funded initiatives at KAIST, including the Center for High-Performance Integrated Systems and the <a href="https://www.idec.or.kr/" target="_blank">Integrated-Circuit Design Education Center,</a> both directed by Kim’s former student Kyung Chong-Min. And the semiconductor industry, in turn, benefited from the ever more highly trained workforce emerging from Kim’s orbit.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="More than three dozen people stand in in four rows. A bald man is at the center of the front row." class="rm-shortcode" data-rm-shortcode-id="946f78f5532ae5ad8a9d75fea834c37f" data-rm-shortcode-name="rebelmouse-image" id="d5149" loading="lazy" src="https://spectrum.ieee.org/media-library/more-than-three-dozen-people-stand-in-in-four-rows-a-bald-man-is-at-the-center-of-the-front-row.png?id=31217297&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Kim [front row, orange tie] also served as director of Korea’s Center for Electro-Optics, a government-sponsored research institute formed to develop technologies for thermal imaging, fiber optics, and lasers.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chang Hae-Ja</small>
</p><h2>The Evolution of South Korea’s Semiconductor Industry<br/>
</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Two men in academic robes and mortar caps face the camera. Others in similar dress or in suits appear the background facing in different directions." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="fd8c7300f5c7aed0c4a54493bf8cac62" data-rm-shortcode-name="rebelmouse-image" id="b6bf8" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-in-academic-robes-and-mortar-caps-face-the-camera-others-in-similar-dress-or-in-suits-appear-the-background-facing-in-d.png?id=31255772&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Chung Jin-Yong [right], a former student of Kim [left], graduated from KAIST in 1976 and later developed DRAM for Hynix.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Chang Hae-Ja</small>
</p><p>
	Kim’s lab at KAIST evolved in parallel with the growth of the semiconductor sector in South Korea, which can be divided into three periods. During the first period, beginning in the mid-1960s, the government led the charge by enacting laws and drawing up plans for industry development, establishing research institutes, and pressing companies and universities to pay more attention to semiconductor technology. Samsung and other electronics companies wouldn’t get serious about manufacturing semiconductor devices until the early 1980s. So when Kim started his lab, almost a decade prior, he was training engineers to meet the industry’s 
	<em>future</em> needs.
</p><p>
	His first group of students worked primarily on the design and fabrication of semiconductors using PMOS, NMOS, and CMOS technologies that, while not cutting edge by global standards, were quite advanced for the South Korea of the time. Because there were few industry jobs, many alumni of Kim’s lab took positions at government research institutes, where they developed state-of-the-art experimental chips. An exception was Lim Hyung-Kyu, one of Kim’s first master’s candidates, whom Samsung sent to study at KAIST in 1976. Lim would go on to lead the development of various memory devices at Samsung, most importantly NAND flash memory in the 1990s.
</p><p>
	The second period started in 1983, when Samsung declared that it would pursue semiconductors aggressively, starting with DRAM. The move drove rival conglomerates such as Hyundai and GoldStar to do likewise. As a result, the South Korean chip industry rapidly expanded. KAIST and other universities provided the necessary manpower, and the government reduced its role. In Kim’s lab, students began to explore emerging technologies—including polysilicon thin-film transistors (for LCD panels), infrared sensors (for military use), and rapid thermal processing (which increased efficiency and reduced costs of semiconductor production)—and published their results in prestigious international journals.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Seven men in academic robes and three others in suits stand shoulder to shoulder." class="rm-shortcode" data-rm-shortcode-id="ef2079f969f113eabcaff186a43c3920" data-rm-shortcode-name="rebelmouse-image" id="7e6fc" loading="lazy" src="https://spectrum.ieee.org/media-library/seven-men-in-academic-robes-and-three-others-in-suits-stand-shoulder-to-shoulder.png?id=31217167&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">KAIST engineering professors Kim [center, gray robe] and Kwon Young-Se [right, blue hood] pose with master’s graduates in 1982. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chang Hae-Ja</small>
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A bespectacled man in a suit smiles while holding a smartphone." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="c9b370dd80d4f2c3f1f871bb1000c979" data-rm-shortcode-name="rebelmouse-image" id="198c5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-bespectacled-man-in-a-suit-smiles-while-holding-a-smartphone.jpg?id=31246169&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Kim’s former master’s student, Kwon Oh-Hyun, rose to become vice chairman and CEO of Samsung Electronics. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Saul Loeb/AFP/Getty Images</small>
</p><p>
	 KAIST graduates flocked to Samsung, GoldStar/LG, and Hyundai/Hynix. As government influence declined, some alums from the first period who had worked at government research institutes also took corporate jobs. At the same time, more and more of Kim’s former students accepted university professorships. After leaving Kim’s lab in 1991, for instance, Cho Byung-Jin spent four years developing DRAM and flash memory at Hyundai before becoming a star professor at the National University of Singapore and later at KAIST. Kyung Chong-Min, Kim’s first doctoral candidate, joined KAIST’s faculty in 1983; by the time he retired in 2018, Kyung had trained more semiconductor specialists than Kim himself.
</p><p>
	During the third period, from 2000 on, industry seized the helm of semiconductor development. Academia churned out more specialists as well as significant research, with minimal contribution from government. Alumni of Kim’s lab continued to lead semiconductor engineering, some of them rising to become high-ranking executives. For example, 
	<a href="https://en.wikipedia.org/wiki/Kwon_Oh-hyun" target="_blank">Kwon Oh-Hyun</a>, who received his master’s degree from KAIST in 1977, served as CEO at Samsung Electronics for most of the 2010s, when the company dominated the world market in not only memory but also mobile phones, TVs, and home appliances.
</p><div class="ieee-sidebar-medium">
<h3>Kim Choong-Ki in Quotes</h3>
<p>
	“If you repeat others’ ideas, you will never overcome them but just follow their buttocks.”
	</p>
<p>
	“Be prepared for the worst case.”
	</p>
<p>
	“If you work harder, everyone in your laboratory will be more comfortable. If you are comfortable, everyone in your laboratory will have to work harder.”
	</p>
<p>
	“The economic development of Korea was dependent on reverse engineering and following [the maps of] advanced countries…. We now have to change our educational policy and teach our students how to draw maps.”
	</p>
<p>
	“Make it yourself; then we will start a discussion.”
	</p>
<p>
	“Why don’t you use something malleable within the hard nut on your neck?”
	</p>
<p>
	“Don’t choose the subjects that others have already thrown into the trash can.”
	</p>
<p>
	“Scientists consider 
		<em>why</em> first, but we engineers must think <em>how</em> first.”
	</p>
<p>
	“Wrong decision is better than slow decision.”
	</p>
</div><p>
	Other alums played key roles in semiconductor research and development. Ha Yong-Min at LG Display mastered TFT-LCD and OLED screens for tablets, notebook computers, and cellphones; Park Sung-Kye, sometimes called the “treasure of Hynix,” developed most of the company’s memory products. In academia, meanwhile, Kim had become a model to emulate. Many of his trainees adopted his methods and principles in teaching and mentoring their own students to become leaders in the field, ensuring a steady supply of highly skilled semiconductor engineers for generations to come.
</p><p>
	In the spring of 2007, less than a year before Kim turned 65—the compulsory retirement age in South Korean academia—KAIST elected him as one of its first distinguished professors, thus extending his tenure for life. Besides the Ho-Am Prize, he has garnered numerous other awards over the years, including the Order of Civil Merit for “outstanding meritorious services…in the interest of improving citizens’ welfare and promoting national development.” And in 2019, he was named a Person of Distinguished Service to Science and Technology, one of the nation’s highest honors.
</p><h2>Legend and Legacy</h2><p>
	For young semiconductor engineers in South Korea today, Kim Choong-Ki is a legend—the great unsung hero behind their nation’s ascendancy in chip production. But its dominance in the world market is now under threat. Although South Korea has competed furiously with Taiwan in recent decades, its most formidable challenger in the future will likely be China, whose ambitious 
	<a href="https://www.uschamber.com/assets/archived/images/final_made_in_china_2025_report_full.pdf" rel="noopener noreferrer" target="_blank">Made in China 2025</a> plan prioritizes semiconductor development. Since 2000, the country has been a major importer of South Korean chips. But China’s recent heavy investment in semiconductors and the availability of highly educated Chinese engineers—including semiconductor specialists trained in the United States, Japan, and South Korea—means that Chinese semiconductor companies could soon become major global competitors.
</p><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="An elderly man with a fringe of white hair sits in a red leather chair." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9c290ce389037902651db1fda69df772" data-rm-shortcode-name="rebelmouse-image" id="1fabb" loading="lazy" src="https://spectrum.ieee.org/media-library/an-elderly-man-with-a-fringe-of-white-hair-sits-in-a-red-leather-chair.png?id=31218981&width=980" style="max-width: 100%"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Korea Academy of Science and Technology  </small>
</p>
<h3>Kim Choong-Ki</h3>
<p>
<strong>Date of birth: </strong>1 October 1942
	</p>
<p>
<strong>Birthplace: </strong>Seoul
	</p>
<p>
<strong>Height:</strong> 170 centimeters
	</p>
<p>
<strong>Family: </strong>Wife, (Chang) Hae-Ja; sons, Ho-Sun and Ho-Jung
	</p>
<p>
<strong>Education: </strong>B.E., Seoul National University, 1965; M.E., Columbia University, 1967; Ph.D., Columbia University 1970
	</p>
<p>
<strong>First employer: </strong>Fairchild Camera and Instrument Corp.
	</p>
<p>
<strong>Current job: </strong>Distinguished Professor and Professor Emeritus at KAIST
	</p>
<p>
<strong>Students: </strong>117 (78 master’s, 39 doctoral)
	</p>
<p>
<strong>Patents: </strong>15 (3 in the United States, 12 in South Korea)
	</p>
<p>
<strong>Biggest surprise in career: </strong>Appointment to vice president of KAIST in 1995
	</p>
<p>
<strong>Hero: </strong>His father, Kim Byung-Woon
	</p>
<p>
<strong>Favorite periodical: </strong><em>TIME </em>magazine
	</p>
<p>
<strong>Favorite kind of music: </strong>Classical
	</p>
<p>
<strong>Favorite movie: </strong><em>Ode to My Father</em>, 2014 (South Korean film)
	</p>
<p>
<strong>Leisure: </strong>Walking
	</p>
<p>
<strong>Languages spoken: </strong>English and Korean
	</p>
<p>
<strong>Car: </strong>Hyundai Genesis
	</p>
<p>
<strong>Organizational memberships: </strong>IEEE, Korean Institute of Electrical Engineers, The Korean Academy of Science and Technology
	</p>
<p>
<strong>Major awards: </strong>Ho-Am Prize by the Samsung Welfare Foundation (1993), Moran Medal (1997) and election as Person of Distinguished Service to Science and Technology (2019) by the South Korean government, IEEE Fellow
	</p>
</div><p>
	Compounding the problem, the South Korean government has neglected its role in supporting chip development in the 21st century. Nearly 50 years after Kim began educating its first semiconductor engineers, the industry again faces a significant workforce shortage. Experts estimate that 
	<a href="https://www.joongang.co.kr/article/25078089#home" target="_blank">several thousand new engineering specialists are needed each year</a>, but the country produces only a few hundred. Yet despite companies’ pleas for more workers and universities’ calls for policies that advance academic education and research, the government has done little.
</p><p>
	Toward the end of his career, Kim had become concerned with the limitations of the kind of “engineer’s mind” that had taken root in South Korea. “The economic development of Korea was dependent on reverse engineering and following advanced countries,” he said in an interview in 1997. That fast-follower approach, he added, relied on an educational system that taught students “how to read maps”—to identify a known product goal and plot a course for achieving it. “And who made the maps? Advanced countries.” He thus concluded, “We now have to change our educational policy and teach our students how to draw maps.”
</p><p>
	Kim himself may not have fully realized this ambitious vision of cultivating a country of creative-minded engineers, capable of pioneering truly groundbreaking technologies that might secure his country’s leadership on the world stage. But hopefully his successors have taken his advice to heart. The future of South Korea depends on it. 
	<span class="ieee-end-mark"></span>
</p><p>
<em>To read more, see “</em><a href="https://www.tandfonline.com/doi/abs/10.1080/19378629.2019.1647218?journalCode=test20" target="_blank">Transfer of ‘Engineer’s Mind’: Kim Choong-Ki and the Semiconductor Industry in South Korea</a>,”<em> Engineering Studies 11:2 (2019), 83-108.</em>
</p><p><em>This article appears in the October 2022 print issue.</em></p>]]></description><pubDate>Sat, 27 Aug 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/kim-choong-ki</guid><category>Semiconductors</category><category>South korea</category><category>Kaist</category><category>Samsung</category><category>Lg</category><category>Hynix</category><category>Engineering education</category><dc:creator>Dong-Won Kim</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-man-in-a-dark-suit-bald-with-some-grey-hair-leans-against-a-shiny-blue-wall-in-which-he-is-reflected.png?id=31216838&amp;width=980"></media:content></item><item><title>Clarivate: Innovators to Watch 2022 Report</title><link>https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;campaignid=7014N000002CbR8&amp;utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;utm_source=ieee&amp;utm_medium=</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=27150766&width=980"/><br/><br/><p>Since introducing the Top 100 Global Innovators list in 2012, each year Clarivate identifies the institutions and companies that sit at the very top of the global innovation ecosystem.<u></u><u></u></p><p><a href="https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&campaignid=7014N000002CbR8&utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&utm_source=ieee&utm_medium=" rel="noopener noreferrer" target="_blank">Download this free whitepaper now!</a></p><p>With each year's list arises the core question of which organizations could be candidates for entry in future years. In this report, we identify those potential future recipients using an overlay analysis focused on the fastest risers.</p><p>In Innovators to watch 2022 report, we identify 37 potential future recipients using an overlay analysis focused on fastest risers.</p><p>To read more on the updated selection process and see what companies and organizations are headed for the Top 100 Global Innovators, read the Innovators to watch 2022.</p>]]></description><pubDate>Thu, 25 Aug 2022 19:15:46 +0000</pubDate><guid>https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;campaignid=7014N000002CbR8&amp;utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;utm_source=ieee&amp;utm_medium=</guid><category>Innovation</category><category>Type:whitepaper</category><category>Clarivate</category><dc:creator>Clarivate</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/27150766/origin.png"></media:content></item><item><title>Will a Baby Born Today Grow up to Live Like the Jetsons?</title><link>https://spectrum.ieee.org/the-jetsons</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-blue-and-yellow-lunch-box-decorated-with-cartoon-characters-from-the-future.jpg?id=30262353&width=1200&height=800&coordinates=0%2C87%2C0%2C87"/><br/><br/><p><a href="https://en.wikipedia.org/wiki/The_Jetsons" rel="noopener noreferrer" target="_blank"><strong>“The Jetsons”</strong></a><strong> premiered</strong> on ABC on 23 September 1962. Set 100 years in the future, the animated comedy series followed the Jetson family—George and Jane and their kids, Judy and Elroy—as they went about their futurist yet surprisingly relatable lives in Orbit City, in a tech-laden house on adjustable columns, commuting by flying car and gliding along moving sidewalks, even in their own home. Although the series ran for just one season, the Jetsons and their wacky space-age world have remained pop-culture touchstones ever since, thanks to near-continuous syndication, two later seasons in the 1980s, and several TV specials and movies. </p><p>And, of course, merchandise. A year after the premiere of “The Jetsons,” Aladdin Industries manufactured the steel lunch box pictured above. Plain, dome-style lunch boxes had long been used by factory workers, but this one, adorned with the Jetson family, Astro the dog, and Rosey the robot maid, was intended for the grade-school set.</p><hr/><p>Food technology is a recurring theme on the show. In Episode 1, the <a href="https://thejetsons.fandom.com/wiki/Foodarackacycle" target="_blank">Foodarackacycle</a>, a push-button meal dispenser, badly malfunctions, leading to the hiring of Rosey, the robot maid with a Brooklyn accent and a heart of gold. In a later episode, Jane Jetson uses the (rotary) Dial-a-Meal to order up a complete breakfast, including burnt toast, in pill form—yet coffee remains a liquid served in a cup. And in the opening credits, Elroy zooms off to school in a space pod but toting a solidly 20th-century lunch box.  </p><p>If the Jetsons’ food tech was a push button too far, many of the other gadgets that they used are commonplace today: video calls, e-readers and tablets, smart watches. I am still waiting for my jet pack and flying car. For a deep dive into the culture and technology of “The Jetsons,” check out Matt Novak’s <a href="https://paleofuture.com/" target="_blank">Paleofuture</a> blog. Back in 2012, for the show’s <a href="https://paleofuture.com/blog/2012/9/19/50-years-of-the-jetsons-why-the-show-still-matters" target="_blank">50th anniversary</a>, he dissected all 24 episodes of the original series. Not all of the episode breakdowns are still available, but <a href="https://paleofuture.com/blog/2012/9/24/recapping-the-jetsons-episode-01-rosey-the-robot" target="_blank">his discourse on Episode 1</a> is worth a read.  </p><p>“The Jetsons” was the first program broadcast in color on ABC. Unfortunately, only viewers in a few select markets—Chicago, Detroit, Los Angeles, New York, and San Francisco—could actually see the show in color. Elsewhere, it aired in black and white. The network had been slow to jump on the color-TV bandwagon, not quite convinced the technology was here to stay. After all, RCA had just started turning a profit on color televisions the previous year, and only 3 percent of Americans owned color televisions at the time.</p><p class="pull-quote">The series ran for just one season, and yet the Jetsons and their wacky space-age world have remained pop-culture touchstones ever since.</p><p>As a cultural historian who trained as an electrical engineer, I’m impressed by how pervasively “The Jetsons” has seeped into society. While researching this piece, I came across an academic article that used the show in a mock court case, another one that invoked George Jetson and the tragedy of the commons to analyze the problem of waste management, and countless news articles that name-checked the Jetsons in their headlines to draw attention to new inventions. In 2007 <em>Forbes </em>did a <a href="https://www.forbes.com/2007/12/10/largest-fictional-companies-oped-books-fict1507-cx_mn_de_1211company_slide.html" target="_blank">ranking of the top 25 fictional companies</a>; Spacely Space Sprockets, where George Jetson worked, came in dead last, with estimated annual sales of US $1.3 billion. The lunch box pictured at top is in the collection of the <a href="https://americanhistory.si.edu/collections/search/object/nmah_1250108" target="_blank">Smithsonian’s National Museum of American History</a>. </p><p>“The Jetsons” clearly has staying power. I peg much of its enduring popularity to the fact that it was in almost nonstop syndication for decades as part of the Saturday morning cartoon lineup. That’s where I first saw the show, over and over again, until a second season of 41 episodes was added in 1985. Ten more episodes came out in 1987, followed by a smattering of movies, TV specials, and direct-to-video or -DVD releases. So many kids grew up watching “The Jetsons” from the 1960s through the 1980s that the show has become a handy shortcut for talking about futuristic technology. </p><p>And in the Jetsons’ world, that futuristic technology might occasionally backfire, but it is never menacing or threatening. Automation has finally delivered on its labor-saving promise, and George works just 3 hours a day, 3 days a week. Everyone is living their best lives. What’s not to love?</p><h2>Happy Birthday, George Jetson!</h2><p>And now for the impetus for this month’s column: According to devoted “Jetsons” fans and Internet sleuths, George Jetson was born right about now—as in July or August 2022. It’s a little difficult to pinpoint the exact birth date of a fictional cartoon character of the future who was introduced almost 60 years ago yet won’t reach adulthood for a couple more decades. But here is what we know.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The cardboard cover of a board game called \u201cThe Jetsons Game\u201d shows a cartoon family of four plus two pets in a spacecraft." class="rm-shortcode" data-rm-shortcode-id="ac4eb6afaf5217404d4697d9d836510b" data-rm-shortcode-name="rebelmouse-image" id="2e56d" loading="lazy" src="https://spectrum.ieee.org/media-library/the-cardboard-cover-of-a-board-game-called-u201cthe-jetsons-game-u201d-shows-a-cartoon-family-of-four-plus-two-pets-in-a-spacec.jpg?id=30262441&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This board game came out in 1985, with the second season of “The Jetsons” and more than two decades after the original show aired.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The Strong, Rochester, N.Y.</small></p><p>Speculations about George’s birthday saw an uptick last November, with various <a href="https://knowyourmeme.com/memes/george-jetson-is-almost-here" target="_blank">wikis and memes</a> suggesting that it falls on either 31 July or 27 August 2022. Although George never celebrates his birthday in any episode, fans combed the Hanna-Barbera canon to arrive at the date, even if it meant doing some mathematical gymnastics to get there.<br/></p><p class="pull-quote">So many kids grew up watching “The Jetsons” that the show has become a handy shortcut for talking about futuristic technology. </p><p>According to various Internet sources, the original promotional materials for “The Jetsons” described its setting as exactly 100 years in the future—September 2062, in other words. From the opening credits, we learn that George is a happily married, middle-aged father of two. His son, Elroy, is in elementary school, while his daughter, Judy, attends Orbit High. </p><p>George’s age is revealed in the episode “Test Pilot,” in which George’s boss sends him to the doctor for an annual checkup. (George needs this physical for insurance coverage; even in the second half of the 21st century, people don’t have universal health care.) Due to a mix-up at the lab, George thinks his death is imminent, so he agrees to engage in high-risk behavior—namely, testing the survivability of the Spacely Lifejacket, a supposedly indestructible garment. George’s doctor eventually reveals the mix-up and tells George that he will live to 150. George, wearing the lifejacket and about to be fired upon by missiles, screams, “I got 110 good years ahead of me!” Do the math: He must be 40 years old. So happy birthday, Mr. Jetson.  </p><p>That also means the high-tech world inhabited by the Jetsons is just 40 years away. Will food replicators cook our meals by then? Maybe. Will our flying cars fold up into briefcases? Maybe not. </p><p><em>Part of a </em><a href="https://spectrum.ieee.org/search/?q=past+forward" target="_self"><em>continuing series</em></a><em> </em><em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em> </p><p><em>An abridged version of this article appears in the August 2022 print issue as “Lunch Box of the Future.”</em></p>]]></description><pubDate>Fri, 29 Jul 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/the-jetsons</guid><category>Past forward</category><category>Pop culture</category><category>Space age</category><category>The jetsons</category><category>Tv shows</category><category>Type:departments</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-blue-and-yellow-lunch-box-decorated-with-cartoon-characters-from-the-future.jpg?id=30262353&amp;width=980"></media:content></item><item><title>The Birth of Random-Access Memory</title><link>https://spectrum.ieee.org/the-birth-of-ram</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/five-gray-towers-with-switch-boards-and-other-tools.jpg?id=30193855&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Whether you’re streaming a movie on<a href="https://about.netflix.com/en" rel="noopener noreferrer" target="_blank"> Netflix</a>, playing a video game, or just looking at digital photos, your computer is regularly dipping into its memory for instructions. Without random-access memory, a computer today can’t even boot up.</p><p>Over the years, memory has been made up of vacuum tubes, glass tubes filled with mercury and, most recently, semiconductors.</p><hr/><p>But the first computers didn’t have any reprogrammable memory at all. Until the late 1940s, every time a machine needed to change tasks, it had to be physically reprogrammed and rewired, according to the<a href="https://www.scienceandindustrymuseum.org.uk/objects-and-stories/baby-and-modern-computing" rel="noopener noreferrer" target="_blank"> Science and Industry Museum</a> in Manchester, England.</p><p>The first electronic digital computer capable of storing instructions and data in a read/write memory was the Manchester Small Scale Experimental Machine, known as the Manchester “Baby.” It successfully ran a program from memory in June 1948.</p><p>Computing pioneers<a href="https://en.wikipedia.org/wiki/Frederic_C._Williams" rel="noopener noreferrer" target="_blank"> Frederic C. Williams</a>,<a href="https://en.wikipedia.org/wiki/Tom_Kilburn" rel="noopener noreferrer" target="_blank"> Tom Kilburn</a>, and<a href="https://en.wikipedia.org/wiki/Geoff_Tootill" rel="noopener noreferrer" target="_blank"> Geoff Tootill</a> developed and built the machine and its storage system—the Williams-Kilburn tube—at the<a href="https://www.manchester.ac.uk/" rel="noopener noreferrer" target="_blank"> University of Manchester</a>.</p><p>“The Baby was very limited in what it could do, but it was the first-ever real-life demonstration of electronic stored-program computing, the fast and flexible approach used in nearly all computers today,” said <a href="https://www.research.manchester.ac.uk/portal/James.Sumner.html" target="_blank">James Sumner</a>, a lecturer on the history of technology at the University of Manchester, in an interview with the<a href="https://www.manchestereveningnews.co.uk/news/greater-manchester-news/bringing-up-baby-how-1948-14819127" rel="noopener noreferrer" target="_blank"> <em>Manchester Evening News</em></a>.</p><p>The IEEE commemorated Baby as an<a href="http://ieeemilestones.ethw.org/Main_Page" rel="noopener noreferrer" target="_blank"> IEEE Milestone</a> during a ceremony held on 21 June at the university.</p><p><strong>How Baby Came to Remember</strong></p><p>After World War II, research groups around the world began investigating ways to build computers that could perform multiple tasks from memory. One such researcher was British engineer F.C. Williams, a radar pioneer who worked at the<a href="https://en.wikipedia.org/wiki/Telecommunications_Research_Establishment" rel="noopener noreferrer" target="_blank"> Telecommunications Research Establishment</a> (TRE), in Malvern, England.</p><p>Williams had an impressive background in radar systems and electronics research. He helped develop the “identification: friend or foe” system, which used radar pulses to distinguish Allied aircraft during the war.</p><p>Because of his expertise, in 1945 the TRE tasked Williams with editing and contributing content to a series of books on radar techniques. As part of his research, he traveled to<a href="https://en.wikipedia.org/wiki/Bell_Labs" rel="noopener noreferrer" target="_blank"> Bell Labs</a> in Murray Hill, N.J., to learn about work being done to remove ground echoes from the radar traces on CRTs. Williams came up with the idea of using two CRTs, and storing the radar trace by passing it back and forth between the two. Williams returned to the TRE and began to investigate the idea, realizing that the approach also could be used to store digital data, with just one CRT. Kilburn, a scientific officer at the TRE, joined Williams in his research.</p><p class="pull-quote">“The Baby was the first-ever real-life demonstration of electronic stored-program computing, and the fast and flexible approach is used in nearly all computers today.”</p><p><span></span>A CRT uses an electron gun to send a focused beam of electrons toward a phosphor-laden screen. The phosphors glow where the beam strikes; the glow eventually fades until struck again by the electron beam. To store digital data, Williams and Kilburn used a more powerful electron beam. When it hit the screen, it knocked a few electrons aside, briefly creating a positively charged spot surrounded by a negative halo. Reading the data involved writing to each data spot on that plate and decoding the pattern of current generated in a nearby metal plate—which would depend on whether there was something written at that spot previously.</p><p>It turned out that the electron charges leaked away over time (just as phosphors on a TV screen fade) and didn’t allow the tube to keep storing data, according to<a href="https://ethw.org/Milestones:Small_Scale_Experimental_Machine,_1948-1951" target="_blank"> an entry about the Milestone</a> on the<a href="https://ethw.org/Main_Page" target="_blank"> Engineering and Technology History Wiki</a>. To maintain the charge, the electron beam had to repeatedly read the data stored on the phosphor and regenerate the associated charge pattern. Such refreshing is also used in the DRAM present in today’s computers.</p><p>In 1946, the men demonstrated a device that could store 1 bit. It is now called<a href="https://en.wikipedia.org/wiki/Williams_tube" target="_blank"> the <em>Williams-Kilburn tube</em></a>; sometimes just the <em>Williams tube</em>.</p><p> Also in 1946, Williams joined the University of Manchester as chair of its electrotechnology department. The TRE temporarily assigned Kilburn to work with him there, and the two continued their research at the university’s<a href="https://en.wikipedia.org/wiki/Computing_Machine_Laboratory" target="_blank"> Computing Machine Laboratory</a>. A year later Williams recruited computer scientist Tootill to join the team. And in 1947 they successfully stored 2,048 bits using a Williams-Kilburn tube.</p><p><strong>Building the Prototype</strong></p><p>To test the reliability of the Williams-Kilburn tube, in 1948 Tilburn and Tootill, with guidance from Computing Machine Laboratory founder<a href="https://en.wikipedia.org/wiki/Max_Newman" rel="noopener noreferrer" target="_blank"> Max Newman</a> and computer scientist<a href="https://spectrum.ieee.org/turing-and-the-test-of-time" target="_self"> Alan Turing</a>, built a small-scale experimental machine. It took them six months, using surplus parts from WWII-era code-breaking machines. And the Manchester Baby was born.</p><p>The Baby took up an entire room in the laboratory building. It was 5 meters long, 2 meters tall, and weighed almost a tonne. The computer consisted of metal racks, hundreds of valves and vacuum tubes, and a panel of vertically mounted hand-operated switches. Users entered programs into memory, bit by bit, via the switches, and read the output directly off the face of the Williams-Kilburn tube.</p><p>On 21 June 1948, Baby ran its first program. Written by Kilburn to find the highest factor of an integer, it consisted of 17 instructions. The machine ran through 3.5 million calculations in 53 minutes before getting the correct answer.</p><p>By 1953, 17 pioneering computer design groups worldwide had adopted the Williams-Kilburn RAM technology.</p><p>Administered by the<a href="http://www.ieee.org/about/history_center/index.html" rel="noopener noreferrer" target="_blank"> IEEE History Center</a> and<a href="https://www.ieeefoundation.org/donate_history" rel="noopener noreferrer" target="_blank"> supported by donors</a>, the Milestone program recognizes outstanding technical developments around the world. The IEEE U.K. and Ireland Section sponsored the nomination for the Baby. The Baby Milestone plaque, which is to be displayed outside on the Coupland 1 building at the University of Manchester, reads:</p><p> “<em>At this site on 21 June 1948 the ‘Baby’ became the first computer to execute a program stored in addressable read-write electronic memory. ‘Baby’ validated Williams-Kilburn tube random-access memories, later widely used, and led to the 1949 Manchester Mark I which pioneered index registers. In February 1951, Ferranti Ltd.’s commercial derivative became the first electronic computer marketed as a standard product delivered to a customer.”</em></p>]]></description><pubDate>Thu, 21 Jul 2022 16:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/the-birth-of-ram</guid><category>Ieee history</category><category>Ieee history center</category><category>Manchester baby computer</category><category>Computer history</category><category>Random access memory</category><category>Ieee milestone</category><category>Computers</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/five-gray-towers-with-switch-boards-and-other-tools.jpg?id=30193855&amp;width=980"></media:content></item><item><title>Landsat Proved the Power of Remote Sensing</title><link>https://spectrum.ieee.org/landsat</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-satellite-image-shows-vegetation-in-red-tones-and-urban-and-rocky-areas-in-grays-and-whites.jpg?id=29985802&width=1200&height=800&coordinates=0%2C123%2C0%2C124"/><br/><br/><p><strong>On 18 September 1969</strong>, U.S. President Richard Nixon <a href="https://2009-2017.state.gov/p/io/potusunga/207305.htm" target="_blank">addressed</a> the General Assembly of the United Nations. It was a difficult time in global politics, and much of his speech focused on the war in Vietnam, disputes in the Middle East, and strategic arms control. Toward the end, though, the speech took a curious and hopeful turn, as Nixon rhapsodized about the unifying potential of international cooperation in space exploration. As an example, he noted the United States was in the process of developing new satellites to survey Earth’s natural resources.  </p><p>Three years later, on 23 July 1972, NASA launched what would be the first Earth Resources Technology Satellite (ERTS). It gave scientists, land managers, policymakers, and others an unprecedented view of their planet. The program has since launched eight more satellites. Renamed the Landsat program in 1975, it is now celebrating its 50th anniversary of imaging the Earth. </p><h2>Landsat’s winning tech was the Multispectral Scanner System </h2><p>ERTS was the first of its kind. On board were two different imaging instruments. The first was a television-style camera system (called a Return Beam Vidicon, or RBV) built by the Radio Corporation of America. Everyone assumed the camera would be the workhorse of the mission.  </p><p>The second instrument was a highly experimental Multispectral Scanner System (MSS) designed by <a href="https://landsat.gsfc.nasa.gov/article/virginia-t-norwood-the-mother-of-landsat/" target="_blank">Virginia T. Norwood</a> and built by Hughes Aircraft Co. MSS capitalized on fiber optics to collect data at four different spectral bands (visible green and red, plus two bands of near-infrared). <a href="https://landsat.gsfc.nasa.gov/article/remembering-paul-lowman/" target="_blank">NASA geologist Paul Lowman</a>, who reviewed the original, unsolicited proposal for the MSS, had no faith that the technology would work. RBV had been used on previous space missions and weather satellites. MSS had been successfully tested on Earth, but could it work while zooming through space, sending out binary data in real time? </p><p>As it turned out, the RBV failed on 5 August 1972 after collecting only 1,690 images. The MSS, though, operated until 6 January 1978, outliving its design life by four and a half years and recording more than 300,000 images. Lowman later conceded that he had been “dead wrong.” MSS was clearly the winning technology. </p><p>The first ERTS satellite (retroactively known as <a href="https://landsat.gsfc.nasa.gov/satellites/landsat-1/" target="_blank">Landsat 1</a>) orbited the Earth about 14 times a day, imaging the globe every 18 days. <a href="https://www.usgs.gov/landsat-legacy/fifty-years-exploration-and-innovation-how-landsat-launched-remote-sensing-era" target="_blank">Ground stations in California and Alaska</a> received the data and then sent the tapes to the Goddard Space Flight Center in Maryland for processing. Two days after the launch, Goddard received its first MSS image, showing the Dallas–Fort Worth area. (In the false-color image, shown at top, reds are vegetation and grays and whites are urban or rocky land.) ERTS had a spatial resolution of 80 meters. Landsat data is considered <a href="https://eos.com/blog/satellite-data-what-spatial-resolution-is-enough-for-you/" target="_blank">low to medium resolution</a>—absolutely fine for looking at large areas of land, but not exactly spyware. Today’s commercial high-resolution satellites can have a resolution of 30 centimeters. </p><p>As Landsat 1 orbited the Earth in a north-to-south direction, the MSS’s oscillating mirror repeatedly scanned a 185-kilometer-wide swath (corresponding to a 12° field of view) from west to east. The light from the mirror passed through filters and onto the 24-element array of detectors—six for each of the MSS’s four spectral bands—yielding 24 channels of video data. A multiplexer processed this data, and an analog-to-digital converter changed it to a pulse-code modulated signal. The satellite then transmitted the signal to a ground station; if no station was within range, it stored the data on magnetic tape for later transmission. </p><p>Under NASA’s supervision, more than 300 researchers investigated how to use this abundance of new data—for biology, geology, land use, agricultural developing, and mining, among other things. </p><h2>The Soviet grain deal gave Landsat a new use: predicting crop yields worldwide </h2><p>The U.S. Department of Agriculture was one of the earliest boosters for Landsat. Beginning in the mid-1960s, the USDA had worked with NASA and labs at Purdue University and the University of Michigan to develop new ways to identify crops from high-altitude aircraft and satellites. In 1970 and 1971, corn blight ravaged the Midwest and the South, providing a real-world test to see if multispectral scanners flown in aircraft could classify the stages of fungus infestation. The experiment showed <a href="https://www.nass.usda.gov/Education_and_Outreach/Reports,_Presentations_and_Conferences/GIS_Reports/Results%20of%20the%201971%20Corn%20Blight%20Watch%20Experiment.pdf" target="_blank">promising results</a> for the emerging MSS technology. </p><p>Just as corn blight was devastating crops in the United States, a long, cold winter was destroying Soviet wheat from the Ukraine and Volga River valley, totaling half of the world’s grain loss that year. The Kremlin, trying to avoid famine at home, sent representatives to the United States to purchase American wheat, corn, oats, rye, and soybeans. The Soviets were capitalizing on a recent change in U.S. agricultural policy that removed restrictions on grain sales to the Soviet Union and China. The U.S. government did not yet know about the Soviet’s crop failure and the ripple effects it would have on the global market, and farmers were happy to sell. Within a few months of the sales, however, global food prices soared by 50 percent, and U.S. farmers estimated a profit loss of $462 million (about $3.2 billion in today’s dollars). </p><p>As the historian Brian Jirout argues in his 2017 article “<a href="https://spacehistory101.com/downloads/quest-vol-24-4/" target="_blank">Farming from Space: Landsat and the Development of Agricultural Surveillance During the Cold War</a>,” the Soviet grain deal created the urgency and anxiety necessary for the USDA to propose the Large Area Crop Inventory Experiment, to predict both domestic and foreign crop yields. The experiment would use the satellite’s MSS to collect agricultural data on a global scale, using the same protocol as the corn blight experiment. The experiment’s success pushed NASA to move up Landsat 2’s launch date. That satellite went up on 22 January 1975 carrying the same RBV and MSS sensor technology as the original. </p><p>To calibrate the MSS imagery, NASA and the USDA needed to have crop reports, soil data, and meteorological information about conditions on the ground. After several months of negotiation, the United States and the Soviet Union signed the Agreement on Cooperation in Agriculture on 19 June 1973, legitimizing the agricultural goals for the project. The two nations also cooperated on applying remote sensing to geology and hydrology in the hopes of allaying international concerns over American espionage.  </p><h2>Landsat provided imagery for the CIA’s War on Drugs and the Gulf War </h2><p>As the Landsat technology matured, both the Carter and Reagan administrations moved to privatize the program. Unfortunately, the <a href="https://www.science.org/doi/10.1126/science.216.4541.40" target="_blank">estimated costs for running the program</a> (including satellites, launch vehicles, communication stations, and image processing) amounted to $1 billion to $10 billion over 10 years, while the estimated annual revenue was only about $6 million. Landsat couldn’t survive without government subsidies. Launches for Landsat 6 and 7 were paused, and program managers warned that image collecting would cease.  </p><p>Meanwhile, Landsat 5 stayed the course, imaging the Earth from March 1984 to June 2013—an astounding 29 years. Jirout notes that without the satellite’s incredible longevity, “there’s a good chance the Landsat archive would have a significant data gap.”  </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A model of a satellite that's suspended from wires and has solar panels, instrumentation, and other equipment extending from its body." class="rm-shortcode" data-rm-shortcode-id="cba706c4d005e800888016a65e91d317" data-rm-shortcode-name="rebelmouse-image" id="c9461" loading="lazy" src="https://spectrum.ieee.org/media-library/a-model-of-a-satellite-that-s-suspended-from-wires-and-has-solar-panels-instrumentation-and-other-equipment-extending-from-its.jpg?id=29985917&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Landsat 5 (a model of which is shown here) operated for 29 years, making it one of the most long-lived satellites of all time.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">SSPL/Getty Images</small></p><p>Government contracts ended up saving the Landsat program, and perhaps unsurprisingly, they came from the intelligence and military sectors. As part of its War on Drugs, the CIA used Landsat imagery to reveal an increase of acreage committed to poppy in Afghanistan in 1985 and the resulting expanded opium trade. The Defense Mapping Agency also regularly used Landsat data for terrain analysis in Kuwait during the Gulf War with Operations Desert Shield and Storm. After the dissolution of the Soviet Union, the U.S. Department of Defense used Landsat to image the 15 new nations. </p><p>As the program marks its 50th anniversary, let’s reflect on Landsat’s place in history. Many of the troubling themes that Nixon raised in his 1969 UN address continue to echo today. Countries still engage in conflicts and border disputes, if not outright war, and arms control remains a contentious subject. Landsat has been an unexpected, mostly peaceful, constant. </p><p>Landsat’s lasting legacy is the treasure trove of continuous global imaging data it has produced. Freely available to all, the data now has a historic dimension, allowing scientists to compare how land has changed over decades. Whether it is comparing damage due to storms, pollution caused by factories, or the spread of deserts and urban sprawl, Landsat provides a baseline for measuring Earth’s resources. </p><p>Although <a href="https://spectrum.ieee.org/commercial-satellite-imagery" target="_self">Earth-imaging satellites</a> have become far more common, especially in the past decade, Landsat is still going strong. Last November, the Landsat Archive added its <a href="https://www.usgs.gov/landsat-missions/news/landsat-archive-adds-its-10-millionth-image" target="_blank">10 millionth scene</a>, an image of the Dead Sea. This video, prepared by the U.S. Geological Survey, which administers the archive, shows how the Dead Sea has shrunk over nearly 50 years:  </p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="b32acef70dae5b874e529a44834405c2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Ivehqq4MXBQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Image of the Week - Landsat's 10 Millionth Scene</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=Ivehqq4MXBQ" target="_blank">www.youtube.com</a>
</small>
</p><p><span style="background-color: initial;">The latest satellite, Landsat 9, launched on 27 September of last year and is collecting up to 750 scenes per day. T</span><span style="background-color: initial;">he Landsat Next mission is already in the planning stages</span><span style="background-color: initial;">.</span> While scientists consider future projects, historians <span style="background-color: initial;">are</span> thinking about how to preserve the history of space <span style="background-color: initial;">projects like Landsat</span> as well as newer efforts<span style="background-color: initial;">. </span><a href="https://www.toboldlypreserve.space/about.html" target="_blank">To Boldly Preserve</a><span style="background-color: initial;"> is an organization of nearly 100 historians, archivists, and museum curators who are working with government officials and </span><span style="background-color: initial;">representative</span> of the space industry to capture contemporary events and ensure they can be accessed in the future. <span style="background-color: initial;">Here’s to the next 50 years of space exploration.</span><br/></p><p><em>Part of a </em><a href="https://spectrum.ieee.org/tag/Past+Forward" target="_self"><em>continuing series</em></a><em> looking at photographs of historical artifacts that embrace the boundless potential of technology.</em> </p><p><em>An abridged version of this article appears in the July 2022 print issue as </em><em>“Pictures of a Planet."</em></p>]]></description><pubDate>Wed, 29 Jun 2022 15:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/landsat</guid><category>Landsat</category><category>Earth imaging</category><category>Remote sensing</category><category>History of the satellite</category><category>Past forward</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-satellite-image-shows-vegetation-in-red-tones-and-urban-and-rocky-areas-in-grays-and-whites.jpg?id=29985802&amp;width=980"></media:content></item><item><title>Explosive Power Beats Even Moore’s Law</title><link>https://spectrum.ieee.org/nuclear-bomb</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-nuclear-explosion-with-islands-in-the-foreground.jpg?id=30007111&width=1200&height=800&coordinates=114%2C0%2C114%2C0"/><br/><br/><p>
<strong>The rising number </strong>of components on a microchip is the go-to example of roaring innovation. Intel’s first microprocessor, <a href="https://spectrum.ieee.org/chip-hall-of-fame-intel-4004-microprocessor" target="_self">the 4004</a>, released in 1971, had 2,300 transistors; half a century later the highest count <a href="https://spectrum.ieee.org/nanosheets-ibms-path-to-5nanometer-transistors" target="_self">surpasses 50 billion,</a> for the Apple M1 Max—<a href="https://en.wikipedia.org/wiki/Moore%27s_law" target="_blank">an increase of seven orders of magnitude</a>. Most other technical advances have lagged behind: During the entire 20th century, maximum travel speeds rose less than tenfold, from about <a href="https://www.railwaywondersoftheworld.com/locomotive-speed-records.html" target="_blank">100 kilometers per </a>hour for express trains to <a href="https://en.wikipedia.org/wiki/Cruise_(aeronautics)#:~:text=The%20typical%20cruising%20airspeed%20for,%3B%20547%E2%80%93575%20mph)." target="_blank">900 km/h</a> for cruising jetliners. Skyscrapers got only <a href="https://www.emporis.com/statistics/history-of-worlds-tallest-buildings" target="_blank">2.4 times as tall</a>, from the <a href="https://en.wikipedia.org/wiki/Singer_Building#:~:text=With%20a%20roof%20height%20of,one%2Dsixth%20of%20that%20area." target="_blank">Singer Building</a> (187 meters) to the <a href="https://en.wikipedia.org/wiki/Petronas_Towers" target="_blank">Petronas Towers</a> (452 meters).
</p><p>
	But there is one accomplishment that, unfortunately, has seen even higher gains since 1945: the destructive power of explosives.
</p><hr/><p>
	Modern explosives date to the 19th century, with trinitrotoluene (
	<a href="https://en.wikipedia.org/wiki/TNT" target="_blank">TNT</a>) and <a href="https://en.wikipedia.org/wiki/Dynamite" target="_blank">dynamite</a> in the 1860s, followed by <a href="https://en.wikipedia.org/wiki/RDX" target="_blank">RDX</a> (Royal Demolition Explosive), patented in 1898. During the Second World War, explosive power rained on European and Japanese cities in the form of mass-scale bombing, and by the war’s end, in 1945, the most powerful explosive weapon was the Nazi <a href="https://en.wikipedia.org/wiki/V-2_rocket" target="_blank">V-2</a> rocket. It carried 910 kilograms of <a href="https://en.wikipedia.org/wiki/Amatol" target="_blank">amatol</a>—a blend of TNT and ammonium nitrate—and had an explosive energy of about 3.5 gigajoules.
</p><p>
	The increase in explosive power, over 16 years, matches what Moore’s Law has accomplished in the 50 years since 1970
	<br/>
</p><p>
	And then came an entirely new class of explosives, those exploiting nuclear fission and fusion. The bomb that exploded over 
	<a href="https://atomicarchive.com/resources/documents/pdfs/00313791.pdf" target="_blank">Hiroshima</a> on 7 August 1945 released 63 GJ of energy, half of it as the blast wave, about a third as thermal radiation. The Nagasaki bomb, dropped two days later, released about 105 GJ. But these first two bombs were tiny when compared to what came later. The most powerful U.S. hydrogen (or fusion) bomb, tested in 1954, was equivalent to <a href="https://en.wikipedia.org/wiki/Castle_Bravo" target="_blank">15 megatons of TNT</a> (63 petajoules). This was far surpassed on 30 October 1961, when the Soviet Union tested the RDS-220 bomb above Novaya Zemlya in the Arctic Ocean. Fifty-nine years later, in August 2020, Rosatom (Russia’s atomic energy agency) released a 40-minute-long film that claimed that the bomb, nicknamed the <em>tsar bomba</em>—the emperor’s bomb—had had a yield of <a href="https://en.wikipedia.org/wiki/Tsar_Bomba" target="_blank">50 megatons</a>.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/10193323?602891">
<script src="https://public.flourish.studio/resources/embed.js"> </script>
</div><p>
	In this 
	<a href="https://www.livescience.com/tsar-bomba-secret-test-footage-declassified.html" target="_blank">remarkable video</a>, the antiquated analog instrumentation provides a strange contrast with the weapon’s immense destructive power. The bomb—hung beneath the belly of a Tu-95 bomber—was dropped by parachute from a height of 10.5 kilometers and detonated 4 km above the ground. The explosion released 210 PJ of energy, three orders of magnitude more than the Nagasaki bomb, creating a mushroom cloud of 60-65 km in diameter and a flash visible from nearly 1,000 km away. And soon afterward Nikita Khrushchev, the Soviet premier, claimed that his country had built but not tested a bomb twice as powerful.
</p><p>
	The last V-2 attack on London came on 
	<a href="https://en.wikipedia.org/wiki/March_1945#:~:text=Soviet%20military%20commander-,March%2027%2C%201945%20(Tuesday),(present%2Dday%20Montenegro)." target="_blank">27 March 1945</a>, less than six weeks before the Nazi surrender. By the Novaya Zemlya test in 1961, the maximum explosive energy of weapons had risen by seven orders of magnitude, to more than 200 PJ. That increase, over 16 years, matches what Moore’s Law has accomplished in the 50 years since 1970. It is a reminder of the terrible priorities of modern civilization.
</p><p>
<em>This article appears in the July 2022 print issue as “A Moore’s Law—for Bombs.”</em>
</p>]]></description><pubDate>Tue, 28 Jun 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/nuclear-bomb</guid><category>Explosive power</category><category>Atom bomb</category><category>H-bomb</category><category>V-2 rocket</category><category>Nuclear bomb</category><category>Nuclear weapons</category><category>Energy</category><dc:creator>Vaclav Smil</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-nuclear-explosion-with-islands-in-the-foreground.jpg?id=30007111&amp;width=980"></media:content></item><item><title>The Fatal Flaw of the Pulse Oximeter</title><link>https://spectrum.ieee.org/pulse-oximeters-encode-racial-bias-with-clear-consequences-for-covid-19-patients</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-hospital-patient-with-a-pulse-oximeter-on-their-finger.jpg?id=30012999&width=1200&height=800&coordinates=0%2C141%2C0%2C142"/><br/><br/><p>If someone is seeking medical care, the color of their skin shouldn’t matter. But, according to new research, pulse oximeters’ performance and accuracy apparently hinges on it. Inaccurate blood-oxygen measurements, in other words, made by pulse oximeters have had <a href="https://journals.lww.com/ajnonline/fulltext/2021/04000/pulse_oximetry_may_be_inaccurate_in_patients_with.9.aspx" target="_blank">clear consequences for people of color</a> during the COVID-19 pandemic. </p><p>“That device ended up being essentially a gatekeeper for how we treat a lot of these patients,” said Dr. <a href="https://www.bcm.edu/people-search/tianshi-wu-65121" rel="noopener noreferrer" target="_blank">Tianshi David Wu</a>, an assistant professor of medicine at Baylor College of Medicine, in Houston, and one of the authors of the study.</p><hr/><p>For decades, scientists have found that pulse oximeters, devices that estimate blood-oxygen saturation, can be affected by a person’s skin color. In 2021, the FDA <a href="https://www.fda.gov/medical-devices/safety-communications/pulse-oximeter-accuracy-and-limitations-fda-safety-communication" rel="noopener noreferrer" target="_blank">issued a warning</a> about this limitation of pulse oximeters. The agency says it plans to hold a <a href="https://www.fda.gov/medical-devices/safety-communications/pulse-oximeter-accuracy-and-limitations-fda-safety-communication" target="_blank">meeting on pulse oximeters</a> later this year. Because low oxygen saturation, called <a href="https://www.mayoclinic.org/symptoms/hypoxemia/basics/definition/sym-20050930" target="_blank">hypoxemia</a>, is a common symptom of COVID-19, low blood-oxygen levels qualify patients to receive certain medications. In the first study to examine this issue among COVID-19 patients, published in <em><a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2792653#:~:text=Conclusions%20and%20Relevance%20The%20results,or%20unrecognized%20eligibility%20for%20COVID%2D" rel="noopener noreferrer" target="_blank">JAMA Internal Medicine</a> </em>in May, researchers found that the inaccurate measurements resulted in a “systemic failure,” delaying care for many Black and Hispanic patients, and in some cases, preventing them from receiving proper medications. The study adds a growing sense of urgency to an issue raised decades ago. </p><p class="pull-quote">“We found that in Black and Hispanic patients, there was a significant delay in identifying severe COVID compared to white patients.”<br/>—Dr. Ashraf Fawzy, Johns Hopkins University</p><p><a href="https://spectrum.ieee.org/should-you-trust-apples-new-blood-oxygen-sensor" target="_self">Pulse oximeters</a> work by passing light through part of the body, usually a finger. These devices infer a patient's blood-oxygen saturation (that is, the percentage of hemoglobin carrying oxygen) from the absorption of light by <a href="https://www.mountsinai.org/health-library/tests/hemoglobin#:~:text=Hemoglobin%20is%20a%20protein%20in,is%20exchanged%20for%20carbon%20dioxide." target="_blank">hemoglobin</a>, the pigment in blood that carries oxygen. In theory, pulse oximeters shouldn’t be affected by anything other than the levels of oxygen in the blood. But research has shown otherwise. </p><p>“If you have melanin, which is the pigment that's responsible for skin color…that could potentially affect the transmittance of the light going through the skin,” said <a href="https://cbee.umbc.edu/govind-rao/" rel="noopener noreferrer" target="_blank">Govind Rao</a>, a professor of engineering and director of the Center for Advanced Sensor Technology at the University of Maryland, Baltimore County, who was not involved in the study. </p><p>To examine how patients with COVID-19 were affected by this flaw in pulse oximeters, researchers used data from over 7,000 COVID-19 patients in the <a href="https://www.hopkinsmedicine.org/about/johnshopkins-health-system-corp.html" target="_blank">Johns Hopkins hospital system</a>, which includes five hospitals, between March 2020 and November 2021. In the first part of the study, researchers compared blood-oxygen saturation for the 1,216 patients who had measurements taken using both a pulse oximeter and arterial blood-gas analysis, which determines the same measure using a direct analysis of blood. The researchers found that the pulse oximeter overestimated blood-oxygen saturation by an average of 1.7 percent for Asian patients, 1.2 percent for Black patients, and 1.1 percent for Hispanic patients. </p><p>Then, the researchers used these results to create a statistical model to estimate what the arterial blood-gas measurements would be for patients with only pulse-oximeter measurements. Because arterial blood gas requires a needle to be inserted into an artery to collect the blood, most patients only have a pulse-oximeter measurement. </p><p>To qualify for COVID-19 treatment with <a href="https://www.covid19treatmentguidelines.nih.gov/therapies/antiviral-therapy/remdesivir/" target="_blank">remdesivir</a>, an antiviral drug, and <a href="https://medlineplus.gov/druginfo/meds/a682792.html" target="_blank">dexamethasone</a>, a steroid, patients had to have a blood-oxygen saturation of 94 percent or less. Based on the researchers’ model, nearly 30 percent of the 6,673 patients about whom they had enough information to predict their arterial blood-gas measurements met this cutoff. Many of these patients, most of whom were Black or Hispanic, had their treatment delayed for between 5 and 7 hours, with Black patients being delayed on average 1 hour more than white patients. </p><p>“We found that in Black and Hispanic patients, there was a significant delay in identifying severe COVID compared to white patients,” said Dr. <a href="https://www.hopkinsmedicine.org/profiles/details/ashraf-fawzy" rel="noopener noreferrer" target="_blank">Ashraf Fawzy</a>, assistant professor of medicine at Johns Hopkins University and an author of the study. </p><p>There were 451 patients who never qualified for treatments but that the researchers predicted likely should have; 55 percent were Black, while 27 percent were Hispanic. </p><p>The study “shows how urgent it is to move away from pulse [oximeters],” said Rao, and to find alternatives ways of measuring blood-oxygen saturation. </p><p>Studies finding that skin color can affect pulse oximeters go back <a href="https://scholar.google.com/scholar?q=pulse+oximeter+racial+bias&hl=en&as_sdt=0%2C22&as_ylo=1970&as_yhi=1990" target="_blank">as far as the 1980s</a>. Despite knowledge of the issue, there are few ways of addressing it. Wu says increasing awareness helps, and that it also may be helpful to do more arterial blood-gas analyses.</p><p>A long-term solution will require changing the technology, either by using a different method entirely or having devices that can better adjust results to account for differences in skin color. One technological alternative is having devices that measure oxygen diffusing across the skin, called transdermal measurement, which Rao’s lab is working on developing. </p><p>The researchers said one limitation of their study involved the way patients race was self-identified—meaning a wide range of skin pigmentation could be represented in each of the sample groups, depending on how each patient self-identified. The researchers also did not measure how delaying or denying treatment affected the patients clinically, for instance how likely they were to die, how sick they were, or how long they were sick. The researchers are currently working on a study examining these additional questions and factors. </p><p>Although the problem of the racial bias of pulse oximeters has no immediate solution, said the researchers, they are confident the primary hurdle is not technological. </p><p>“We do believe that technology exists to fix this problem, and that would ultimately be the most equitable solution for everybody,” said Wu. </p>]]></description><pubDate>Fri, 24 Jun 2022 13:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/pulse-oximeters-encode-racial-bias-with-clear-consequences-for-covid-19-patients</guid><category>Bias</category><category>Racial diversity</category><category>Pulse oximetry</category><category>Product design</category><category>Medical technology</category><dc:creator>Rebecca Sohn</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-hospital-patient-with-a-pulse-oximeter-on-their-finger.jpg?id=30012999&amp;width=980"></media:content></item><item><title>The Magnet That Made the Modern World</title><link>https://spectrum.ieee.org/the-men-who-made-the-magnet-that-made-the-modern-world</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-cube-made-up-of-silver-spheres.jpg?id=30007520&width=1200&height=800&coordinates=0%2C106%2C0%2C107"/><br/><br/><p>
	For sheer drama and resonance, few tech breakthroughs can match the invention of the neodymium-iron-boron permanent magnet in the early 1980s. It’s one of the great stories of corporate intrigue: General Motors in the United States and Sumitomo in Japan independently conceived the technology and then worked in secret, racing to commercialize the technology, and without even being aware of the other’s efforts. The two project leaders—<a href="https://en.wikipedia.org/wiki/Masato_Sagawa" target="_blank">Masato Sagawa</a> of Sumitomo and <a href="https://ethw.org/John_J._Croat" target="_blank">John Croat</a> of GM—surprised each other by announcing their results at the same conference in Pittsburgh in 1983.
</p><p>
	Up for grabs was a market potentially worth billions of dollars. The best permanent magnets at the time, samarium-cobalt, were strong and reliable but expensive. They were used in electric motors, generators, audio speakers, hard-disk drives, and other high-volume products. Today, some 
	<a href="https://link.springer.com/article/10.1007/s40831-018-0162-8" rel="noopener noreferrer" target="_blank">95 percent</a> of permanent magnets are neodymium-iron-boron. The global market for these magnets is expected to reach <a href="https://borates.today/boron-magnets-prize/" rel="noopener noreferrer" target="_blank">US $20 billion a year</a> within a couple of years, as the automobile industry shifts toward electric vehicles and as utilities turn increasingly to wind turbines to meet growing demand.
</p><p>
<a href="https://ieeetv.ieee.org/channels/ieee_awards/masato-sagawa-john-j-croat-ieee-medal-for-environmental-and-safety-technologies#:~:text=IEEE%20Awards-,John%20J.,generators%2C%20and%20other%20devices.%22" target="_blank">IEEE recently honored</a> Sagawa and Croat by awarding them its <a href="https://ethw.org/IEEE_Medal_for_Environmental_and_Safety_Technologies" target="_blank">Medal for Environmental and Safety Technologies</a> at the 2022 Vision, Innovation, and Challenges Summit. <em>IEEE Spectrum</em> spoke with the two inventors, including an hourlong interview with both of them (only the second time the two have been interviewed together). They revealed their reasons for zeroing in on the rare-earth element neodymium, the major challenges they faced in making a commercial magnet out of it, the extraordinary intellectual-property deal that allowed both GM and Sumitomo to market their magnets worldwide, and their opinions on whether there will ever be a successful permanent magnet that does not use rare-earth elements.<a name="top"></a>
</p><p><em>Editor's note: a video of the interview is available <a href="https://ieeetv.ieee.org/channels/ieee_awards/masato-sagawa-john-croat-interview-with-glenn-zorpette-ieee-vic-summit" target="_blank">here</a>.</em></p><p>
	John Croat and Masato Sagawa on…
</p><ul>
<li><a href="#metals">How they settled on neodymium, iron, and boron</a>
</li>
<li><a href="#magnet">The major hurdles to producing a commercially viable magnet</a>
</li>
<li><a href="#market">The low-key deal struck between GM and Sumitomo to allow global marketing of their magnets</a>
</li>
<li><a href="#permanent">Why there will never be a successful permanent magnet that does not use rare-earth elements</a></li>
</ul><p>
<strong>You were trying to make a cheaper magnet, as I understand it. You weren’t even necessarily trying to make a stronger one, although that turned out to be the case. What made you think you could make a cheaper magnet?</strong>
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A white-haired man in a red tie appears mildly bemused" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="79958f73a7f0091651c237e3866c766d" data-rm-shortcode-name="rebelmouse-image" id="70407" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-haired-man-in-a-red-tie-appears-mildly-bemused.jpg?id=30007525&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">John Croat</small>
</p><p>
<strong>John Croat:</strong> Well, the problem with samarium-cobalt…they were an excellent magnet. They had good temperature properties. You’ve probably heard the phrase that rare earths aren’t really that rare, but samarium is one of the more rare ones. It constitutes only about 0.8 percent of the composition of the ores that are typically exploited today for rare earths. So it was a fairly expensive rare earth. And, of course, cobalt was very expensive. During my early years at <a href="https://en.wikipedia.org/wiki/General_Motors_Research_Laboratories" target="_blank">General Motors Research Labs</a>, there was a war in Central African Zaire [now known as the Democratic Republic of the Congo], which is a big cobalt supplier. And the price of cobalt went up to something like $45 a kilogram. Remember, this was in the 1970s, so it basically stopped our research on samarium-cobalt magnets.
</p><p>
<strong>Masato, what do you remember? What do you recall of the state of the permanent-magnet market and technology in the 1970s in Japan?</strong>
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A white-haired man smiles at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="473beb7df9bbfad4c705d88ea6d17366" data-rm-shortcode-name="rebelmouse-image" id="d9132" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-haired-man-smiles-at-the-camera.jpg?id=30007549&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Masato Sagawa</small>
</p><p>
<strong>Masato Sagawa:</strong> I joined Fujitsu in 1972, so that’s in the same age as with John. And I was given from the company to improve the <a href="https://idealmagnetsolutions.com/knowledge-base/samarium-cobalt-vs-neodymium-magnets/" target="_blank">samarium-cobalt magnet</a>, to improve the mechanical strength. But I wondered why there is no iron compound. Iron is much cheaper and much more [available] than cobalt, and iron has higher magnetic moment than cobalt. So if I can produce rare-earth iron magnets, I thought I will have higher magnetic strengths and much lower cost. So I started to research the samarium-cobalt—or rare-earth iron compound. But it’s an official subject in Fujitsu. And I worked hard on the samarium-cobalt. And I succeeded in the development of samarium-cobalt magnet with high strength. And I asked the company to work on a rare-earth iron compound permanent magnet. But I was not allowed. But I had an idea. Rare-earth, iron and, I think, a small amount of additive elements like some carbon or boron, which are known to have a very small atomic diameter. I studied the rare-earth, iron, boron or rare-earth, iron, carbon. So underground, I did this research for several years. And I reached this neodymium-boron several years later. It was in 1982.
</p><p>
<a href="#top">Back to top</a>
</p><p>
<a name="metals"></a><strong>What was it that made you focus on neodymium, iron, and boron? Why those?</strong>
</p><p>
<strong>Croat:</strong> Well, of course, when samarium-cobalt magnets were developed, everyone in this field thought about developing a rare-earth-iron magnet because iron is virtually free compared to cobalt. Now, in terms of the rare earths, as I said, rare earths are not really that rare. The light rare earths, lanthanum, cerium, praseodymium, and neodymium, constitute about 90 percent of the composition of a typical rare-earth deposit…. So we knew at the start that if we wanted to make an economically viable magnet, both Dr. Sagawa and I realized that we had to make the permanent magnet from one of these four rare earths: lanthanum, cerium, neodymium, or praseodymium. The problem with lanthanum and cerium, as you know, the lanthanides are formed by filling the <a href="https://en.wikibooks.org/wiki/High_School_Chemistry/Lanthanides_and_Actinides" target="_blank">4F electrons</a> in the 4F series. However, lanthanum and cerium, the two most abundant rare-earths, had no 4F electrons. And we knew by this time, based on the work with samarium-cobalt magnets, that one of the things that you had to have was these 4F electrons to give you the coercivity for the material.
</p><p>
<a href="#top">Back to top</a>
</p><p>
<a name="magnet"></a><strong><a href="https://spectrum.ieee.org/r/entryeditor/2657538742#top" target="_self"></a>Can you give us a quick definition of coercivity?</strong>
</p><p>
<strong>Croat:</strong> <a href="https://idealmagnetsolutions.com/knowledge-base/understanding-coercivity/" target="_blank">Coercivity</a> is the resistance to demagnetization. In a permanent magnet, as you say, the moments are all aligned parallel. If you put a magnetic field in the reverse direction, the coercivity will resist the magnet flipping into the opposite direction.
</p><p>
	We knew that we wanted iron instead of cobalt…. And both of us set out with the intention of making a rare-earth iron permanent magnet from neodymium or praseodymium. The problem was that there was no intermetallic compounds available. Unlike in this rare-earth cobalt phase diagram—there was lots of interesting intermetallic compounds—the rare-earth-iron phase diagrams do not contain suitable usable intermetallic compounds.
</p><p>
<strong>In plain language, what is an intermetallic phase, and why is it important?</strong>
</p><p>
<strong>Croat:</strong> An intermetallic compound or an <a href="https://www.giessereilexikon.com/en/foundry-lexicon/Encyclopedia/show/intermetallic-phase-4565/?cHash=fc50d9a4ec7a9ad5757902d9d229d24f" target="_blank">intermetallic phase</a> is a phase with a fixed ratio of the components. Like, terbium-iron two has one terbium and two irons. And it sits on a crystal lattice in very specific sites on the lattice. You have to have that. That’s one of the quintessential requirements for any rare-earth transition-metal permanent magnet.
</p><p>
<strong>It provides the structure and stability you need or the reproducibility?</strong>
</p><p>
<strong>Croat:</strong> All of that. In other words, it’s the thing that holds the magnetic moment in place in the structure. You have to have this crystal structure.
</p><p>
<strong>So what was the solution?</strong>
</p><p>
<strong>Croat:</strong> The fact that there was no intermetallic compound was a baffling problem for some time. But then, in 1976, I and a couple of colleagues saw <a href="https://apps.dtic.mil/sti/pdfs/ADA035484.pdf" rel="noopener noreferrer" target="_blank">a paper by Art Clark</a>. He was working at the Naval Surface Weapons Laboratory. He had taken a sputtered sample of terbium iron two [TbFe<sub>2</sub>] and annealed it at increasingly higher temperatures. And at about 350 °C, the coercivity shot up to about 3.5 kilo oersted. And we surmised, and I think correctly at the time, that what had happened was that during the crystallization process, a metastable phase had formed. This was exciting because this is the first time anyone had ever developed a coercivity in a rare-earth iron material. It was also exciting because of the fact that TbFe<sub>2</sub> is a cubic material. And a cubic material should not develop coercivity. You have to have a crystal structure with a <a href="https://www.britannica.com/science/uniaxial-crystal" target="_blank">uniaxial crystal</a> lattice, like hexagonal, rhombohedral, or tetragonal.
</p><p>
	And so I started out with that thesis: to create magnetically hard metastable phases that are practical for permanent magnets. And by using rapid solidification, I started making melt-spun materials and crystallizing them. And it worked very well. I had developed very high coercivities right away. The problem with these materials were that they were all unstable. I started to heat them up at about 450 °C, and they would decompose into their equilibrium structure, and the coercivity would go away. So I began to add things to see if I could make them more stable. And one of the things I added was boron. And one day I found that when I heated my sample up containing boron, it did not decompose into its equilibrium structure. And so I knew that I had discovered a ternary neodymium-iron-boron intermetallic phase, a very interesting, technically important intermetallic phase. And it turns out that Masato discovered the same one [laughter].
</p><p>
<strong>Sagawa-san, you mentioned that you were interested in a sintering process, which was similar to the process that was then being used to manufacture samarium-cobalt magnets.... When you were working on a way to make neodymium-iron-boron magnets using sintering, did you encounter specific challenges that were difficult, that took a lot of effort to solve?</strong>
</p><p>
<strong>Sagawa:</strong> I was not able to give coercivity to the neodymium-iron-boron alloy. And I tried many processes. But the cost of sintering is good because to give coercivity to the alloy, you have to make a cellular structure in the alloy. So to produce cellular structure, the sintering is a very good way because first, you make single crystal or powder and you align the powder and then sintering. And during sintering, you form automatically cellular structure.
</p><p>
	So I tried to form cellular structure. I tested many, many kinds of elements starting from copper. Copper is used in the case of samarium-cobalt magnets. And starting from copper, I tested many, many additive elements almost throughout the periodic table. But I was not able to give coercivity by making additional elements. And at last, I found a good additive element. It’s not another element—it’s neodymium itself. Additional neodymium gives home to cellular structure forming a grain boundary area around the neodymium-iron-boron particles. So I succeeded in giving coercivity to the neodymium-iron-boron by sintering and a neodymium-rich composition. And I succeeded in developing a neodymium-boron sintered magnet with record-high BH maximum [a measure of the maximum magnetic energy that can be stored in a magnet] in the world. It was in 1982.
</p><p>
<strong>This work is mostly happening in the late 1970s, early 1980s. You’re both working on almost the same problem on different sides of the world. Sagawa-san, when did you first find out that General Motors was also working on the same challenge that you were working on?</strong>
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two men stand in a conference room behind a podium." class="rm-shortcode" data-rm-shortcode-id="c772ee8ce43d91ee877da47053232af0" data-rm-shortcode-name="rebelmouse-image" id="0d8c7" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-stand-in-a-conference-room-behind-a-podium.jpg?id=30007554&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Masato Sagawa  of Sumitomo [left] announced the invention of a revolutionary neodymium-iron-boron permanent magnet at a conference in Pittsburgh, in November 1983. At the same meeting, John Croat of General Motors announced the invention of a magnet using the exact same elements.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Masato Sagawa</small>
</p><p>
<strong>Sagawa:</strong> It was when I made the first presentation at the MMM Conference, Magnetism and Magnetic Material Conference, held in Pittsburgh in 1983.
</p><p>
<strong>Croat:</strong> November 1983.
</p><p>
<strong>Sagawa:</strong> November 1983. At the same conference, John Croat and his group presented a paper on the same neodymium-boron alloy magnets.
</p><p>
<strong>So for years, you both had been working on this problem, attacking the same problem. And you both found out about the other effort at the same conference in Pittsburgh in 1983?</strong>
</p><p>
<strong>Croat:</strong> Yes.
</p><p>
<strong>That’s astounding. Did you talk to each other at that conference? Did you get together and say anything to each other?</strong>
</p><p>
<strong>Croat:</strong> I think we introduced ourselves to each other, but I don’t remember much more than that.
</p><p>
<strong>What do you recall, Sagawa-san? Do you recall any conversation with John at that meeting?</strong>
</p><p>
<strong>Sagawa:</strong> I remember that I saw John, but I don’t remember if we talked together or not.
</p><p>
<strong>Croat:</strong> I think it would have been logical if we did, but I cannot remember it. We probably considered ourselves competitors [laughter].
</p><p>
<strong>You both came up with independent means of manufacturing. General Motors came up with a technique called melt-spinning, and Sumitomo’s was a sintering process. They had different characteristics. The sintered magnets seem to have more structural strength or resilience. The GM magnets can be produced more inexpensively. They both found large market applications, somewhat different but still large uses. John, why don’t you take a crack in just explaining what their market niches became and still are to this day?</strong>
</p><p>
<strong>Croat:</strong> Yes. The rapidly solidified materials are isotropic. And during the rapid solidification process, you form a magnetic powder. That powder is blended with an epoxy and made into a magnet. But it turned out that these magnets were ideal for making small ring magnets that go into micromotors like spindle motors for <a href="https://www.stanfordmagnets.com/what-are-the-uses-of-neodymium-magnets-in-computers.html" target="_blank">hard-disk drives</a> or CD-ROMs or for stepper motors. So that has—
</p><p>
<strong>For robots.</strong>
</p><p>
<strong>Croat:</strong> For robots and things of that nature, servo motors for robots, but also spindle and stepper motors for various applications. And that has been the primary market for these bonded magnets because making a thin-wall ring magnet by the sintering process is very difficult. They tend to crack and break apart. But in contrast, the sintered-magnet market, which is much bigger actually than the bonded-magnet market, has been used primarily for bigger motors, wind-turbine generators, MRIs. Most of the electric-vehicle motors are sintered magnets. So again, most of the market is motors. But the market is bigger for the sintered-magnet market than it is for the bonded-magnet market. But there are two distinctly different markets in general.
</p><p>
<strong>Sagawa:</strong> I think one of the most important applications of the neodymium-iron-boron magnet is the hard-disk drive. If the neodymium-boron was not found, it would have been difficult to miniaturize the hard-disk drive. Before the appearance of the neodymium-boron magnet, the hard-disk drive was very big. It was difficult to lift by one person, 10 kilo or 20 kilogram or so. Now it becomes very small. And this is because of the invention of neodymium-boron sintered magnet which is used in the actuator motor. And also, the bonded-magnet neodymium is used in the spindle motor to rotate the hard disk. This was a very important invention for the start of our IT society.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An open hard drive" class="rm-shortcode" data-rm-shortcode-id="af71c0dd5f6c106b07b8f32ef9d2f5b1" data-rm-shortcode-name="rebelmouse-image" id="7747e" loading="lazy" src="https://spectrum.ieee.org/media-library/an-open-hard-drive.jpg?id=30007556&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Hard-disk drives contain several neodymium permanent magnets. There’s one in the spindle motor that rotates the disk, and typically two others in the read-write arm, also known as the actuator arm (the triangular shaped structure in the photo) that detects and writes data on the disk.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Getty Images</small>
</p><p>
<a href="#top">Back to top</a>
</p><p>
<a name="market"></a><strong>You had little or no contact with each other until this meeting in Pittsburgh in 1983, by which time you’d already established all your intellectual property. And yet there was a long-running—well, not that long-running, but a patent case between General Motors and Sumitomo. John, can you start off and tell us a little bit about what happened there?</strong>
</p><p>
<strong>Croat:</strong> Yes. I guess we didn’t mention it, but both Sumitomo and General Motors filed patents shortly after the invention of this material, which turned out to be early 1982, apparently within weeks of each other. But it turns out, because of patent law, the way patent law is written, General Motors ended up with the patents in North America, and Sumitomo ended up with the patents for the composition neodymium-iron-boron in Japan and Europe. General Motors had the neodymium-iron-boron composition in North America. This meant that neither company could market worldwide, and they <em>had</em> to market worldwide to be economically viable. So they actually had a dispute, of course. I don’t know if they actually sued each other. But anyway, they had a negotiation. And I remember being part of these negotiations where we ended up with an agreement where we cross-licensed each other, which allowed both companies to <a href="https://macropolo.org/analysis/permanent-magnets-case-study-industry-chinese-production-supply/" target="_blank">market the material worldwide</a>—manufacture and market the material worldwide.
</p><p>
<strong>But you could only manufacture and market your type of material, which, in your case, was this melt-spun, rapid—</strong>
</p><p>
<strong>Croat:</strong> Solidification, melt-spinning.
</p><p>
<strong>Solidification. And Sumitomo had the sintering worldwide, North America, Asia, Europe, everywhere.</strong>
</p><p>
<strong>Croat:</strong> It turned out it was based on the particle size of the material. Sumitomo had the rights to manufacture magnets with a particle size greater than one micron, General Motors less than one micron.
</p><p>
<strong>Sagawa:</strong> Oh, you remember!
</p><p>
<strong>Croat:</strong> Yes. [Both laugh]
</p><p>
<a href="#top">Back to top</a>
</p><p>
<a name="permanent"></a><strong>Right now, of course, there’s a lot of controversy over the fact that an enormous amount of the world’s market for rare-earth elements is controlled by China, the mining, the production, and so on. So many countries, particularly in Europe and North America, are looking to broaden their base of suppliers for rare earths. But at the same time, there’s this existing market for these magnets. So is this having an effect of any kind on the future directions of R&D in permanent magnets?</strong>
</p><p>
<strong>Croat:</strong> I am no longer close enough to the R&D to know what’s going on, but I think there has been no change. People are still interested in making permanent magnets primarily containing a rare earth.
</p><p>
	I don’t see how they’re ever going to get the rare earth out of a rare-earth transition metal magnet and make a good high-performance magnet. So the rare-earth supply problem is going to continue and will maybe even grow in the future as the market for these magnets grows. And I think the only way that they can overcome that is that Japan and Korea and Western Europe and North America will have to have some kind of government help to establish a rare-earth market outside of [China]. There are a lot of countries that have rare earths. India, for example, has rare earths. Australia, Canada have rare earths. United States, of course, has several big deposits. But what happened was, of course, the Chinese reduced the price to the point back in the 1990s and drove everybody else out of business. So somehow, some political will has to be put forth to change the dynamics of the rare-earth market today.
</p><p>
<strong>Sagawa:</strong> I think it’s impossible to produce high-grade magnet without rare earths. It’s concluded recently. There are very active research on an iron-nickel compound; it was promising. It has high-saturation magnetization and a very high anisotropy field. But I think, in recent research in Japan, it was concluded [that] it’s impossible to produce high-performance permanent magnet from this iron-nickel compound. And this is the last research subject on the rare-earth-free compound consisting of only 3D [orbital] -electron elements, iron-cobalt-nickel.
</p>]]></description><pubDate>Tue, 21 Jun 2022 22:14:58 +0000</pubDate><guid>https://spectrum.ieee.org/the-men-who-made-the-magnet-that-made-the-modern-world</guid><category>Magnets</category><category>Magnetism</category><category>Permanent magnets</category><category>Neodymium permanent magnets</category><category>History of technology</category><dc:creator>Glenn Zorpette</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-cube-made-up-of-silver-spheres.jpg?id=30007520&amp;width=980"></media:content></item><item><title>Charles Babbage’s Difference Engine Turns 200</title><link>https://spectrum.ieee.org/charles-babbage-difference-engine</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-intricate-metal-clockwork-consisting-of-columns-of-toothed-gears-on-a-rectangular-wooden-base.jpg?id=29844145&width=1200&height=800&coordinates=0%2C150%2C0%2C150"/><br/><br/><p><strong>It was an idea </strong>born of frustration, or at least that’s how Charles Babbage would later recall the events of the summer of 1821. That fateful summer, Babbage and his friend and fellow mathematician John Herschel were in England editing astronomical tables. Both men were founding members of the Royal Astronomical Society, but editing astronomical tables is a tedious task, and they were frustrated by all of the errors they found. Exasperated, Babbage exclaimed, “I wish to God these calculations had been executed by steam.” To which Herschel replied, “It is quite possible.“</p><p>Babbage and Herschel were living in the midst of what we now call the Industrial Revolution, and steam-powered machinery was already upending all types of business. Why not astronomy too? </p><hr/><p>Babbage set to work on the concept for a Difference Engine, a machine that would use a clockwork mechanism to solve polynomial equations. He soon had a small working model (now known as Difference Engine 0), and on 14 June 1822, he presented a one-page “<a href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015077097874&view=1up&seq=89" rel="noopener noreferrer" target="_blank">Note respecting the Application of Machinery to the Calculation of Astronomical Tables</a>” to the Royal Astronomical Society. His note doesn’t go into much detail—it’s only one page, after all—but Babbage claimed to have “repeatedly constructed tables of squares and triangles of numbers” as well as of the very specific formula <em>x</em><sup>2</sup> + <em>x </em>+ 41. He ends his note with much optimism: “From the experiments I have already made, I feel great confidence in the complete success of the plans I have proposed.” That is, he wanted to build a full-scale Difference Engine.</p><p>Perhaps Babbage should have tempered his enthusiasm. His magnificent Difference Engine proved far more difficult to build than his note suggested.</p><p>It wasn’t for lack of trying, or lack of funds. For Babbage managed to do something else that was almost as unimaginable: He convinced the British government to fund his plan. The government saw the value in a machine that could calculate the many numerical tables used for navigation, construction, finance, and engineering, thereby reducing human labor (and error). With an initial investment of £1,700 in 1823 (about US $230,000 today), Babbage got to work.</p><h2>The Difference Engine was a calculator with 25,000 parts</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A daguerreotype portrait of British mathematician Charles Babbage shows a man in period dress of the mid-19th century." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9546dca6bd652145c268055d07a1240c" data-rm-shortcode-name="rebelmouse-image" id="cf030" loading="lazy" src="https://spectrum.ieee.org/media-library/a-daguerreotype-portrait-of-british-mathematician-charles-babbage-shows-a-man-in-period-dress-of-the-mid-19th-century.jpg?id=29844155&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The 19th-century mathematician Charles Babbage’s visionary contributions to computing were rediscovered in the 20th century.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">The Picture Art Collection/Alamy</small></p><p>Babbage based his machine on the mathematical method of finite differences, which allows you to solve polynomial equations in a series of iterative steps that compare the differences in the resulting values. This method had the advantage of requiring simple addition only, which was easier to implement using gear wheels than one based on multiplication and division would have been. (The Computer History Museum has an <a href="https://www.computerhistory.org/babbage/howitworks/" rel="noopener noreferrer" target="_blank">excellent description</a> of how the Difference Engine works.) Although Babbage had once dreamed of a machine powered by steam, his actual design called for a human to turn a crank to advance each iteration of calculations.</p><p>Difference Engine No. 1 was divided into two main parts: the calculator and the printing mechanism. Although Babbage considered using different numbering systems (binary, hexadecimal, and so on), he decided to stick with the familiarity of the base-10 decimal system. His design in 1830 had a capacity of 16 digits and six orders of difference. Each number value was represented by its own wheel/cam combination. The wheels represented only whole numbers; the machine was designed to jam if a result came out between whole numbers. </p><p>As the calculator cranked out the results, the printing mechanism did two things: It printed a table while simultaneously making a stereotype mold (imprinting the results in a soft material such as wax or plaster of paris). The mold could be used to make printing plates, and because it was made at the same time as the calculations, there would be no errors introduced by humans copying the results.</p><p>Difference Engine No. 1 contained more than 25,000 distinct parts, split roughly equally between the calculator and the printer. The concepts of interchangeable parts and standardization were still in their infancy. Babbage thus needed a skilled craftsman to manufacture the many pieces. <a href="https://collection.sciencemuseumgroup.org.uk/people/ap24334/brunel-marc-isambard" rel="noopener noreferrer" target="_blank">Marc Isambard Brunel</a>, part of the father-and-son team of engineers who had constructed the first tunnel under the Thames, recommended <a href="https://www.lindahall.org/joseph-clement/" rel="noopener noreferrer" target="_blank">Joseph Clement</a>. Clement was an award-winning machinist and draftsman whose work was valued for its precision.</p><p>Babbage and Clement were both brilliant at their respective professions, but they often locked horns. Clement knew his worth and demanded to be paid accordingly. Babbage grew concerned about costs and started checking on Clement’s work, which eroded trust. The two did produce a portion of the machine [shown at top] that was approximately one-seventh of the complete engine and featured about 2,000 moving parts. Babbage demonstrated the working model in the weekly soirees he held at his home in London. </p><p>The machine impressed many of the intellectual society set, including a teenage Ada Byron, who understood the mathematical implications of the machine. Byron was not allowed to attend university due to her sex, but her mother supported her academic interests. Babbage suggested several tutors in mathematics, and the two remained correspondents over their lifetimes. In 1835, Ada married William King. Three years later, when he became the first Earl of Lovelace, Ada became Countess of Lovelace. (More about Ada Lovelace shortly.)</p><p>Despite the successful chatter in society circles about Babbage’s Difference Engine, trouble was brewing—cost overruns, political opposition to the project, and Babbage and Clement’s personality differences, which were causing extreme delays. Eventually, the relationship between Babbage and Clement reached a breaking point. After yet another fight over finances, Clement abruptly quit in 1832.</p><h2>The Analytical Engine was a general-purpose computer</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A watercolor portrait of the British mathematician Ada Lovelace shows a young woman with dark curls in a white dress." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="8b954f13cfcc5310632e93a1e9e1102d" data-rm-shortcode-name="rebelmouse-image" id="6e7a4" loading="lazy" src="https://spectrum.ieee.org/media-library/a-watercolor-portrait-of-the-british-mathematician-ada-lovelace-shows-a-young-woman-with-dark-curls-in-a-white-dress.jpg?id=29844218&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Ada Lovelace championed Charles Babbage’s work by, among other things, writing the first computer algorithm for his unbuilt Analytical Engine.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Interim Archives/Getty Images</small></p><p>Despite these setbacks, Babbage had already started developing a more ambitious machine: the Analytical Engine. Whereas the Difference Engine was designed to solve polynomials, this new machine was intended to be a general-purpose computer. It was composed of several smaller devices: one to list the instruction set (on <a href="https://www.computerhistory.org/storageengine/punched-cards-control-jacquard-loom/" rel="noopener noreferrer" target="_blank">punch cards</a> popularized by the Jacquard loom); one (called the mill) to process the instructions; one (which Babbage called the store but we would consider the memory) to store the intermediary results; and one to print out the results.</p><p>In 1840 Babbage gave a series of lectures in Turin on his Analytical Engine, to much acclaim. Italian mathematician <a href="https://en.wikipedia.org/wiki/Luigi_Federico_Menabrea" rel="noopener noreferrer" target="_blank">Luigi Federico Menabrea</a> published a description of the engine in French in 1842, “<em>Notions sur la machine analytique</em>.” This is where Lady Lovelace returns to the story.</p><p>Lovelace translated Menabrea’s description into English, discreetly making a few corrections. The English scientist <a href="https://ethw.org/Charles_Wheatstone" rel="noopener noreferrer" target="_blank">Charles Wheatstone</a>, a friend of both Lovelace and Babbage, suggested that Lovelace augment the translation with explanations of the Analytical Engine to help advance Babbage’s cause. The resulting “Notes,” published in 1843 in Richard Taylor’s <em>Scientific Memoirs</em>, was three times the length of Menabrea’s original essay and contained what many historians consider the first algorithm or computer program. It is quite an accomplishment to write a program for an unbuilt computer whose design was still in flux. Filmmakers John Fuegi and Jo Francis captured Ada Lovelace’s contributions to computing in their 2003 documentary <em>Ada Byron Lovelace: To Dream Tomorrow. </em>They also wrote a companion article published in the <em>IEEE Annals of the History of Computing</em>, entitled “<a href="https://ieeexplore.ieee.org/document/1253887" rel="noopener noreferrer" target="_blank">Lovelace & Babbage and the Creation of the 1843 ‘Notes</a>’.” </p><p>Although Lovelace’s translation and “Notes” were hailed by leading scientists of the day, they did not win Babbage any additional funding. Prime Minister Robert Peel had never been a fan of Babbage’s; as a member of Parliament back in 1823, he had been a skeptic of Babbage’s early design. Now that Peel was in a position of power, he secretly solicited condemnations of the Difference Engine. In a stormy meeting on 11 November 1842, the two men argued past each other. In January 1843, Babbage was informed that Parliament was sending the finished portion of Difference Engine No. 1 to the King’s College Museum. Two months later, Parliament voted to withdraw support for the project. By then, the government had spent £17,500 (about US $3 million today) and waited 20 years and still didn’t have a working machine. You could see why Peel thought it was a waste.</p><p>But Babbage, perhaps reinvigorated by his work on the Analytical Engine, decided to return to the Difference Engine in 1846. Difference Engine No. 2 required only 8,000 parts and had a much more elegant and efficient design. He estimated it would weigh 5 tons and measure 11 feet long and 7 feet high. He worked for another two years on the machine and left 20 detailed drawings, which were donated to the Science Museum after he died in 1871.</p><h2>A modern team finally builds Babbage’s Difference Engine</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A tall mechanical apparatus with columns of metal gears and a hand crank on the side." class="rm-shortcode" data-rm-shortcode-id="308378e458051ba7f534a6cbbd349471" data-rm-shortcode-name="rebelmouse-image" id="043d1" loading="lazy" src="https://spectrum.ieee.org/media-library/a-tall-mechanical-apparatus-with-columns-of-metal-gears-and-a-hand-crank-on-the-side.jpg?id=29844231&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..."> In 1985, a team at the Science Museum in London set out to build the streamlined Difference Engine No. 2 based on Babbage’s drawings. The 8,000-part machine was finally completed in 2002.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Science Museum Group</small></p><p>Although Difference Engine No. 2, like all the other engines, was never completed during Babbage’s lifetime, a team at the Science Museum in London set out to build one. Beginning in 1985, under the leadership of Curator of Computing <a href="https://en.wikipedia.org/wiki/Doron_Swade" rel="noopener noreferrer" target="_blank">Doron Swade</a>, the team created new drawings adapted to modern manufacturing techniques. In the process, they sought to answer a lingering question: Was 19th-century precision a limiting factor in Babbage’s design? The answer is no. The team concluded that if Babbage had been able to secure enough funding and if he had had a better relationship with his machinist, the Difference Engine would have been a success.</p><p>That said, some of the same headaches that plagued Babbage also affected the modern team. Despite leaving behind fairly detailed designs, Babbage left no introductory notes or explanations of how the pieces worked together. Much of the groundbreaking work interpreting the designs was done by Australian computer scientist and historian <a href="https://en.wikipedia.org/wiki/Allan_G._Bromley" rel="noopener noreferrer" target="_blank">Allan G. Bromley</a>, beginning in 1979. Even so, the plans had dimension inconsistencies, errors, and entire parts omitted (such as the driving mechanism for the inking), as described by Swade in a 2005 article for the <a href="https://ieeexplore.ieee.org/document/1498720" rel="noopener noreferrer" target="_blank"><em>IEEE Annals of the History of Computing</em></a>.</p><p>The team had wanted to complete the Difference Engine by 1991, in time for the bicentenary of Babbage’s birth. They did finish the calculating section by then. But the printing and stereotyping section—the part that would have alleviated all of Babbage’s frustrations in editing those astronomical tables—took another nine years. The finished product is on display at the Science Museum.</p><p>A duplicate engine was built with funding from former Microsoft chief technology officer <a href="https://www.intellectualventures.com/who-we-are/leadership/nathan-myhrvold/" rel="noopener noreferrer" target="_blank">Nathan Myhrvold</a>. The Computer History Museum displayed that machine from 2008 to 2016, and it now <a href="https://www.intellectualventures.com/buzz/insights/ivs-favorite-inventions-the-babbage-machine" rel="noopener noreferrer" target="_blank">resides in the lobby</a> of Myhrvold’s Intellectual Ventures in Bellevue, Wash. </p><p>The title of the textbook for the very first computer science class I ever took was <em>The Analytical Engine. </em>It opened with a historical introduction about Babbage, his machines, and his legacy. Babbage never saw his machines built, and after his death, the ideas passed into obscurity for a time. Over the course of the 20th century, though, his genius became more clear. His work foreshadowed many features of modern computing, including programming, iteration, looping, and conditional branching. These days, the Analytical Engine is often considered an invention 100 years ahead of its time. It would be anachronistic and ahistorical to apply today’s computer terminology to Babbage’s machines, but he was clearly one of the founding visionaries of modern computing.</p><p><em>Part of a </em><a href="https://spectrum.ieee.org/tag/Past+Forward" target="_self"><em>continuing series</em></a><em> looking at photographs of historical artifacts that embrace the boundless potential of technology.</em></p><p><em>An abridged version of this article appears in the June 2022 print issue as “The Clockwork Computer."</em></p>]]></description><pubDate>Fri, 27 May 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/charles-babbage-difference-engine</guid><category>Type:departments</category><category>Charles babbage</category><category>Difference engine</category><category>Analytical engine</category><category>History of computing</category><category>Ada lovelace</category><category>Past forward</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-intricate-metal-clockwork-consisting-of-columns-of-toothed-gears-on-a-rectangular-wooden-base.jpg?id=29844145&amp;width=980"></media:content></item><item><title>Before Ships Used GPS, There Was the Fresnel Lens</title><link>https://spectrum.ieee.org/fresnel-lens-milestone</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-fresnel-lens-at-the-seguin-island-light-station-in-maine.jpg?id=29808783&width=1200&height=800&coordinates=0%2C104%2C0%2C104"/><br/><br/><p>Ships today use satellite-based radio navigation, GPS, and other tools to prevent accidents. But back at the beginning of the 19th century, lighthouses guided ships away from rocky shores using an oil lamp placed between a concave mirror and a glass lens to produce a beam of light.</p><p>The mirrors were not very effective, though, and the lenses were murky. The light was difficult to see from a distance on a clear night, <a href="https://www.aps.org/publications/apsnews/201607/physicshistory.cfm" rel="noopener noreferrer" target="_blank">let alone in heavy fog or a storm</a>.</p><hr/><p>In 1822 French civil engineer <a href="https://en.wikipedia.org/wiki/Augustin-Jean_Fresnel" rel="noopener noreferrer" target="_blank">Augustin-Jean Fresnel</a> (pronounced “Frey Nel”) invented a new type of lens that produced a much stronger beam of light. The Fresnel lens is still used today in active lighthouses around the world. It also can be found in movie projectors, magnifying glasses, spacecraft, and other applications.</p><p>Fresnel’s technical achievement is worthy of being named an <a href="http://ieeemilestones.ethw.org/Milestones_Status_Report" rel="noopener noreferrer" target="_blank">IEEE Milestone</a>, according to the <a href="https://www.ieee.org/about/history-center/index.html" rel="noopener noreferrer" target="_blank">IEEE History Center</a>, but no one has proposed it yet. Any IEEE member can <a href="http://ieeemilestones.ethw.org/Milestone_Guidelines_and_How_to_Propose_a_Milestone" rel="noopener noreferrer" target="_blank">submit a milestone proposal</a> to the IEEE History Center. The Milestone program honors significant accomplishments in the history of electrical and electronics engineering.</p><p><strong>PREVENTING SHIPWRECKS</strong></p><p>Because of increasing complaints from French fishermen and ship captains about the poor quality of the light emanating from lighthouses, in 1811 the French Commission on Lighthouses established a committee under the authority of the <a href="https://simple.wikipedia.org/wiki/Corps_of_Bridges_and_Roads_(France)" rel="noopener noreferrer" target="_blank">Corps of Bridges and Roads</a> to investigate how lighthouse illumination could be improved. </p><p>One member of that committee was Fresnel, who worked for the French civil service corps as an engineer. He had considerable expertise in optics and light waves. In fact, in 1817 he proved that his wave theory—which stated the wave motion of light is <a href="https://www.generationgenius.com/learn-different-types-of-waves-for-kids/" rel="noopener noreferrer" target="_blank">transverse rather than longitudinal</a>—was correct. In transverse waves, a wave oscillates perpendicular to the direction of its travel. Longitudinal waves, like sound, oscillate in the same direction that the wave travels.</p><p>Fresnel’s analysis of contemporary lighthouse technology found the lenses were so thick that <a href="https://www.ponceinlet.org/Understanding-how-Lighthouse-Lenses-Work-1-6297.html" rel="noopener noreferrer" target="_blank">only half the light produced shined through</a>.</p><p>He decided he could do better using his wave theory. His design consisted of 24 glass prisms of varying shapes and sizes arranged in concentric circles within a wire cage. The prisms, placed both in front of and behind four oil lamps, replaced both the mirror and the glass lens of the previous method. Prisms at the edge of the circle refract light slightly more than those closer to the center, so the light rays all emerge in parallel. The design could focus nearly 98 percent of the rays generated by the lamps, producing a beam that could be seen more than 32 kilometers away.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt=" Looking up into a Fresnel lens used in the lamp of a Norwegian lighthouse" class="rm-shortcode" data-rm-shortcode-id="e51308410a253da84a7cb8a4bdf09023" data-rm-shortcode-name="rebelmouse-image" id="7b3ad" loading="lazy" src="https://spectrum.ieee.org/media-library/looking-up-into-a-fresnel-lens-used-in-the-lamp-of-a-norwegian-lighthouse.jpg?id=29808786&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Inside the Lindesnes Lighthouse's Fresnel lens in southern Norway.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">DeAgostini/Getty Images</small></p><p>A clock mechanism, which had to be wound by hand every few hours, was used to revolve the metal frame around the lamps to produce unique light patterns for specific lighthouses. A lighthouse could send out a flash regularly every 5 seconds, for example, or it could have a 10-second period of darkness and a 3-second period of brightness. Captains counted the number of flashes sent out by a lighthouse to calculate their ships’ location. </p><p>The lenses came in several sizes, known as orders. The largest order, the Hyper-Radial, had a 1,330-millimeter diameter. The smallest, the eighth order, had a 75-mm diameter and could be found in lighthouses on bays and rivers. </p><p>In 1823 the French Commission on Lighthouses committee approved the use of the Fresnel lens in all lighthouses in France. That same year, the first one was installed in the <a href="https://www.cordouan.culture.fr/en" rel="noopener noreferrer" target="_blank">Cordouan Lighthouse</a>, in southwestern France. The lens eventually was adopted in other countries. By the 1860s, all the lighthouses in the United States had been fitted with a Fresnel lens, according to the<a href="https://www.si.edu/object/fresnel-lighthouse-lens%3Anmah_844143" rel="noopener noreferrer" target="_blank"> Smithsonian Institution</a>.</p><p>Fresnel continued to modify the lens for several years. His final design, which he completed in 1825, could spin 360 degrees and was the first so-called fixed/flashing lens. It produced a fixed light followed by a brilliant flash followed by another fixed light.</p><p>With the invention of modern navigational tools, the lighthouse has become largely obsolete for maritime safety. But the lens invented for it lives on in side mirrors used on trucks, solar panels, and photographic lighting equipment.</p><p>If you are interested in submitting a proposal, <a href="http://ieeemilestones.ethw.org/Milestone_Guidelines_and_How_to_Propose_a_Milestone" rel="noopener noreferrer" target="_blank">do so here</a>.The History Center is funded by donations to the <a href="https://www.ieeefoundation.org/" rel="noopener noreferrer" target="_blank">IEEE Foundation</a>. For more on the history of lighthouse technology, visit the U.S. <a href="https://www.nps.gov/articles/fresnel-lens.htm" rel="noopener noreferrer" target="_blank">National Park Service</a>, <a href="https://www.ponceinlet.org/Understanding-how-Lighthouse-Lenses-Work-1-6297.html" rel="noopener noreferrer" target="_blank">Ponce Inlet Lighthouse and Museum</a>, and <a href="https://www.aps.org/publications/apsnews/201607/physicshistory.cfm" rel="noopener noreferrer" target="_blank">American Physical Society</a> websites.</p>]]></description><pubDate>Mon, 16 May 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/fresnel-lens-milestone</guid><category>Ieee history</category><category>Ieee news</category><category>Ieee milestone</category><category>Fresnel lens</category><category>Optics</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-fresnel-lens-at-the-seguin-island-light-station-in-maine.jpg?id=29808783&amp;width=980"></media:content></item><item><title>Tony Fadell: The Nest Thermostat Disrupted My Life</title><link>https://spectrum.ieee.org/nest-thermostat</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-holds-a-circular-device-in-front-of-a-blue-wall-that-says-nest-on-it.jpg?id=29755091&width=1200&height=800&coordinates=31%2C0%2C0%2C0"/><br/><br/><p>
<strong>The thermostat chased</strong> me for 10 years.
</p><p>
	That is pretty extreme, by the way. If you’ve got an idea for a business or a new product, you usually don’t have to wait a decade to make sure it’s worth doing.
</p><p>
	For most of the 10 years that I idly thought about thermostats, I had no intention of building one. It was the early 2000s, and I was at <a href="http://www.Apple.com" target="_blank">Apple</a> making the first iPhone. I got married, had kids. I was busy.
</p><p>
	But then again, I was also really cold. Bone-chillingly cold.
</p><p>
	Every time my wife and I drove up to our Lake Tahoe ski cabin on Friday nights after work, we’d have to keep our snow jackets on until the next day. The house took all night to heat up.
</p><h3></h3><br/><img alt="Book cover for Build by Tony Fadell" class="rm-shortcode" data-rm-shortcode-id="4d74e5469cb364c7f4243417b35c0392" data-rm-shortcode-name="rebelmouse-image" id="21d34" loading="lazy" src="https://spectrum.ieee.org/media-library/book-cover-for-build-by-tony-fadell.jpg?id=29760293&width=980"/><p>Adapted from the book <em><a href="https://www.amazon.com/Build-Unorthodox-Guide-Making-Things/dp/0063046067/ref=tmm_hrd_swatch_0?_encoding=UTF8&qid=1647967135&sr=8-1" rel="noopener noreferrer" target="_blank">BUILD: An Unorthodox Guide to Making Things Worth Making</a></em> by Tony Fadell. Copyright 2022 by Tony Fadell. Reprinted by permission of Harper Business, an imprint of HarperCollins Publishers.</p><h3></h3><br/><p>Walking into that frigid house drove me nuts. It was mind-boggling that there wasn’t a way to warm it up before we got there. I spent dozens of hours and thousands of dollars trying to hack security and computer equipment tied to an analog phone so I could fire up the thermostat remotely. Half my vacations were spent elbow-deep in wiring, electronics littering the floor. But nothing worked. So the first night of every trip was always the same: We’d huddle on the ice block of a bed, under the freezing sheets, watching our breath turn into fog until the house finally warmed up by morning.</p><p>Then on Monday I’d go back to Apple and work on the first iPhone. Eventually I realized I was making a perfect remote control for a thermostat. If I could just connect the HVAC system to my iPhone, I could control it from anywhere. But the technology that I needed to make it happen—reliable low-cost communications, cheap screens and processors—didn’t exist yet.</p><h3></h3><br/><p>How did these ugly, piece-of-crap thermostats cost almost as much as Apple’s most cutting-edge technology?</p><h3></h3><br/><p>A year later we decided to build a new, superefficient house in Tahoe. During the day I’d work on the iPhone, then I’d come home and pore over specs for our house, choosing finishes and materials and solar panels and, eventually, tackling the HVAC system. And once again, the thermostat came to haunt me. All the top-of-the-line thermostats were hideous beige boxes with bizarrely confusing user interfaces. None of them saved energy. None could be controlled remotely. And they cost around US $400. The iPhone, meanwhile, was selling for $499.</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>How did these ugly, piece-of-crap thermostats cost almost as much as Apple’s most cutting-edge technology?<br/></p><p>
	The architects and engineers on the Tahoe project heard me complaining over and over about how insane it was. I told them, “One day, I’m going to fix this—mark my words!” They all rolled their eyes—there goes Tony complaining again!
</p><p>
	At first they were just idle words born of frustration. But then things started to change. The success of the iPhone drove down costs for the sophisticated components I couldn’t get my hands on earlier. Suddenly high-quality connectors and screens and processors were being manufactured by the millions, cheaply, and could be repurposed for other technology.
</p><p>
	My life was changing, too. I quit Apple and began traveling the world with my family. A startup was not the plan. The plan was a break. A long one.
</p><p>
	We traveled all over the globe and worked hard not to think about work. But no matter where we went, we could not escape one thing: the goddamn thermostat. The infuriating, inaccurate, energy-hogging, thoughtlessly stupid, impossible-to-program, always-too-hot-or-too-cold-in-some-part-of-the-house thermostat.
</p><p>
	Someone needed to fix it. And eventually I realized that someone was going to be me.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Hardware including a square with electronics and paper with CAD electronic diagrams." class="rm-shortcode" data-rm-shortcode-id="b2ac20a01cbbc0ff34d127f61c79da8b" data-rm-shortcode-name="rebelmouse-image" id="b8c1b" loading="lazy" src="https://spectrum.ieee.org/media-library/hardware-including-a-square-with-electronics-and-paper-with-cad-electronic-diagrams.jpg?id=29760285&width=980"/>
<small class="image-media media-caption" data-gramm="false" data-lt-tmp-id="lt-320298" placeholder="Add Photo Caption..." spellcheck="false">This 2010 prototype of the Nest thermostat wasn’t pretty. But making the thermometer beautiful would be the easy part. The circuit board diagrams point to the next step—making it round.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tom Crabtree</small></p><p>
	The big companies weren’t going to do it. Honeywell and the other white-box competitors hadn’t truly innovated in 30 years. It was a dead, unloved market with less than $1 billion in total annual sales in the United States.
</p><p>
	The only thing missing was the will to take the plunge. I wasn’t ready to carry another startup on my back. Not then. Not alone.
</p><p>
	Then, magically, Matt Rogers, who’d been one of the first interns on the iPod project, reached out to me. He was a real partner who could share the load. So I let the idea catch me. I came back to Silicon Valley and got to work. I researched the technology, then the opportunity, the business, the competition, the people, the financing, the history.
</p><p>Making it beautiful wasn’t going to be hard. Gorgeous hardware, an intuitive interface—that we could do. We’d honed those skills at Apple. But to make this product successful—and meaningful—we needed to solve two big problems:</p><p> It needed to save energy.</p><p>
	And we needed to sell it.
</p><p>
	In North America and Europe, thermostats control half a home’s energy bill—something like $2,500 a year. Every previous attempt to reduce that number—by thermostat manufacturers, by energy companies, by government bodies—had failed miserably for a host of different reasons. We had to do it for real, while keeping it dead simple for customers.
</p><p>
	Then we needed to sell it. Almost all thermostats at that point were sold and installed by professional HVAC technicians. We were never going to break into that old boys’ club. We had to find a way into people’s minds first, then their homes. And we had to make our thermostat so easy to install that literally anyone could do it themselves.
</p><p>
	It took around 9 to 12 months of making prototypes and interactive models, building bits of software, talking to users and experts, and testing it with friends before Matt and I decided to pitch investors.
</p><h3>“Real People” Test the Nest</h3><p>
	Once we had prototypes of the thermostat, we sent it out to real people to test.
</p><p>It was fatter than we wanted. The screen wasn’t quite what I imagined. Kind of like the first iPod, actually. But it worked. It connected to your phone. It learned what temperatures you liked. It turned itself down when nobody was home. It saved energy. We knew self-installation was potentially a huge stumbling block, so everyone waited with bated breath to see how it went. Did people shock themselves? Start a fire? Abandon the project halfway through because it was too complicated? Soon our testers reported in: Installation went fine. People loved it. But it took about an hour to install. Crap. An hour was way too long. This needed to be an easy DIY project, a quick upgrade.</p><p>
	So we dug into the reports—what was taking so long? What were we missing?
</p><p class="pull-quote">Our testers...spent the first 30 minutes looking for tools.</p><p>Turns out we weren’t missing anything—but our testers were. They spent the first 30 minutes looking for tools—the wire stripper, the flathead screwdriver; no, wait, we need a Phillips. Where did I put that?<br/></p><p>
	Once they gathered everything they needed, the rest of the installation flew by. Twenty, 30 minutes tops.
</p><p>
	I suspect most companies would have sighed with relief. The actual installation took 20 minutes, so that’s what they’d tell customers. Great. Problem solved.
</p><p>
	But this was going to be the first moment people interacted with our device. Their first experience of Nest. They were buying a $249 thermostat—they were expecting a different kind of experience. And we needed to exceed their expectations. Every minute from opening the box to reading the instructions to getting it on their wall to turning on the heat for the first time had to be incredibly smooth. A buttery, warm, joyful experience.
</p><p>
	And we knew Beth. Beth was one of two potential customers we defined. The other customer was into technology, loved his iPhone, was always looking for cool new gadgets. Beth was the decider—she dictated what made it into the house and what got returned. She loved beautiful things, too, but was skeptical of supernew, untested technology. Searching for a screwdriver in the kitchen drawer and then the toolbox in the garage would not make her feel warm and buttery. She would be rolling her eyes. She would be frustrated and annoyed.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A white handheld device with 4 screwdriver heads, one on the bottom, and three at the top." class="rm-shortcode" data-rm-shortcode-id="ee497142bc8ce92c5aebccdaad2f3a39" data-rm-shortcode-name="rebelmouse-image" id="267fc" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-handheld-device-with-4-screwdriver-heads-one-on-the-bottom-and-three-at-the-top.jpg?id=29769357&width=980"/>
<small class="image-media media-caption" data-gramm="false" data-lt-tmp-id="lt-863492" placeholder="Add Photo Caption..." spellcheck="false">Shipping the Nest thermostat with a screwdriver "turned a moment of frustration into a moment of delight"</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Dwight Eschliman</small></p><p>
	So we changed the prototype. Not the thermostat prototype—the installation prototype. We added one new element: a little screwdriver. It had four different head options, and it fit in the palm of your hand. It was sleek and cute. Most importantly, it was unbelievably handy.
</p><p>
	So now, instead of rummaging through toolboxes and cupboards, trying to find the right tool to pry their old thermostat off the wall, customers simply reached into the Nest box and took out exactly what they needed. It turned a moment of frustration into a moment of delight.
</p><h3>Honeywell Laughs</h3><p>
	Sony laughed at the iPod. Nokia laughed at the iPhone. Honeywell laughed at the Nest Learning Thermostat.
</p><p>
	At first.
</p><p>
	In the stages of grief, this is what we call Denial.
</p><p>But soon, as your disruptive product, process, or business model begins to gain steam with customers, your competitors will start to get worried. And when they realize you might steal their market share, they’ll get pissed. Really pissed. When people hit the Anger stage of grief, they lash out, they undercut your pricing, try to embarrass you with advertising, use negative press to undermine you, put in new agreements with sales channels to lock you out of the market.</p><p>
	And they might sue you.
</p><p>
	The good news is that a lawsuit means you’ve officially arrived. We had a party the day Honeywell sued Nest. We were thrilled. That ridiculous lawsuit meant we were a real threat and they knew it. So we brought out the champagne. That’s right, f---ers. We’re coming for your lunch.
</p><h3>Nest Gets Googled</h3><p>
	With every generation, the product became sleeker, slimmer, and less expensive to build. In 2014, Google bought Nest for $3.2 billion. In 2016 Google decided to sell Nest, so I left the company. Months after I left, Google changed its mind. Today, Google Nest is alive and well, and they’re still making new products, creating new experiences, delivering on their version of our vision. I deeply, genuinely, wish them well.
</p>]]></description><pubDate>Sat, 07 May 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/nest-thermostat</guid><category>Consumer electronics</category><category>Nest</category><category>Iot</category><category>Internet of things</category><category>Design case history</category><category>History of technology</category><dc:creator>Tony Fadell</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-holds-a-circular-device-in-front-of-a-blue-wall-that-says-nest-on-it.jpg?id=29755091&amp;width=980"></media:content></item><item><title>Did J.J. Thomson Discover the Electron?</title><link>https://spectrum.ieee.org/discovery-of-the-electron</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-elongated-glass-vacuum-tube-sits-lengthwise-on-a-wooden-stand.jpg?id=29721938&width=1200&height=800&coordinates=138%2C0%2C138%2C0"/><br/><br/><p>
<strong>“We shall call</strong> such particles corpuscles,” announced the physicist J.J. Thomson, during a<a href="https://books.google.com/books?id=vBZbAAAAYAAJ&pg=PA104#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank"> lecture at the Royal Institution</a> in London, on 30 April 1897. “The atoms of the ordinary elements are made up of corpuscles and holes, the holes being predominant,” he continued. Thomson described his experiments with cathode rays to verify the existence of these subatomic corpuscles. This model of the atom became known as the “plum pudding” model, so named for the popular English dessert. In Thomson’s analogy, negatively charged corpuscles were like raisins suspended in a positively charged cake, resulting in a neutral atom. The model also became known as the Thomson model, although its chief proponent was William Thomson (Lord Kelvin), not J.J. Thomson, who merely endorsed the idea.
</p><p>
	Corpuscles and pudding are not how we think about the structure of an atom today. Corpuscles are electrons, and the plum pudding model gave way to Ernest Rutherford’s nuclear model in 1911. Yet J.J. Thomson is often hailed as the discoverer of the electron based on that lecture 125 years ago.
</p><hr/><p>
	Of course, history is always more muddled than that. For this month’s column, I knew that I wanted to write about the 125th anniversary of the electron’s discovery, which for simplicity’s sake I pegged to Thomson’s lecture. The challenge then was to find a museum artifact that captured that discovery. 
	<a href="https://collection.sciencemuseumgroup.org.uk/objects/co31178/karl-ferdinand-brauns-braun-tube-1897-cathode-ray-tube" target="_blank">A Braun vacuum tube</a>, like the one pictured at top, seemed like a good choice, because its inventor, <a href="https://www.lindahall.org/karl-ferdinand-braun/" rel="noopener noreferrer" target="_blank">Karl Ferdinand Braun</a>, created it to study beams of electrons, and Thomson used a similar instrument for his experiments.
</p><p>
	But as I dug into the histories of Thomson and Braun, I learned that theirs were two parallel stories involving many of the same players and similar outcomes (both men won a Nobel Prize in Physics), but having little else in common. Many people, in fact, had a reasonable claim to aspects of the electron’s discovery. The more I researched, the more I contemplated what it meant to discover something.
</p><h2>Cathode rays, vacuum tubes, and the birth of atomic theory</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="In a black-and-white image of the British physicist J.J. Thomson, he has a dark mustache and wire-rimmed glasses." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="ad7f7ac7ce3c7c09a4bcba82a85acb93" data-rm-shortcode-name="rebelmouse-image" id="1bc04" loading="lazy" src="https://spectrum.ieee.org/media-library/in-a-black-and-white-image-of-the-british-physicist-j-j-thomson-he-has-a-dark-mustache-and-wire-rimmed-glasses.jpg?id=29721951&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Exactly 125 years ago, the British physicist J.J. Thomson gave a lecture detailing his and others’ experiments with the energetic beams inside cathode-ray tubes.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">
	Universal Images Group/Getty Images
	</small>
</p><p>
	 As the 19th century was coming to a close, many prominent thinkers believed that all of the great discoveries in science had already been made. Electricity was being tamed, and theories of thermodynamics were coalescing to explain the workings of steam engines. Little did scientists know that atomic theory was about to upend science and change our fundamental understanding of matter. J.J. Thomson was a key player in establishing this new direction in physics.
</p><p>
	 In 1876, Thomson had received a scholarship to study at the University of Cambridge’s Trinity College, and four years later he graduated with a degree in mathematics. In 1884, he was appointed the Cavendish Professor of Experimental Physics and began his lifelong study of electromagnetism. Much of Thomson’s research was devoted to understanding the nature of cathode rays. We now know these rays are streams of electrons emerging from the cathode (or negative electrode) of a vacuum tube. But that knowledge was hard won, after decades of investigations by many players.
</p><p>
	 The purely scientific question—What are cathode rays?—was also wrapped up in national identity. Many German physicists believed that visible cathode rays resulted from an interaction with the ether—a colorless, weightless substance that enveloped all of space. Ether is part of what historians call the Classical Worldview, a 19th-century synthesis of physics that has its roots in Aristotle and Newton. French and British scientists, meanwhile, were beginning to argue that cathode rays were electrified subatomic particles. This did not fit neatly within the Classical Worldview, which held that everything was composed of immutable and indivisible atoms.
</p><p class="pull-quote">
	 In the “plum pudding” model of the atom, negatively charged corpuscles were like raisins suspended in a positively charged cake, resulting in a neutral atom.
</p><p>
	For his experiments, Thomson relied on a specialized vacuum tube known as a Crookes tube (more about Crookes and his tubes in a bit), in which he observed and photographed various phenomena, including the effect of a magnetic force on the electrical discharge at high pressure. He also compared experiments on the charges carried by cathode rays both within and outside of the tube.
</p><p>
	 Thomson’s foray into cathode rays was preceded by more than 200 years of demonstration and experimentation with low-vacuum globes and tubes. Early experimenters such as 
	<a href="https://www.lindahall.org/francis-hauksbee/" rel="noopener noreferrer" target="_blank">Francis Hauksbee</a> were simply trying to figure out what was happening inside the tubes and were mesmerized by the different colored lights they could produce. In the 1850s, things got more serious, when Julius Plücker, a physicist and mathematician at the University of Bonn, and his glassblower colleague Heinrich Geissler observed that the green phosphorescence on the glass of a high-vacuum tube was magnetic. When they placed a magnet near the cathode, the light spread out in a pattern similar to iron filings around a magnet.
</p><p>
	 Plücker’s student Johann Wilhelm Hittorf showed that an object placed in front of the cathode cast a shadow on the opposite wall of the tube. Hittorf’s “glow rays” started a line of inquiry about directionality, which Eugen Goldstein picked up on in the 1870s, in experiments that showed cathode rays could be focused using a concave cathode.
</p><p>
	 It seemed as if each new player made slight modifications to the tubes. Then came 
	<a href="https://www.lindahall.org/william-crookes/" rel="noopener noreferrer" target="_blank">William Crookes</a>. He designed a wide array of vacuum tubes for studying cathode rays and gave beautiful demonstrations with his tubes, popularizing their use in laboratories and making the public aware of the research. When Thomson began the investigations that led to his 30 April lecture, he used a Crookes tube.
</p><h2>How Karl Ferdinand Braun worked out the basics of the oscilloscope</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="In a black-and-white image of the German physicist Karl Ferdinand Braun, he has a full beard and small wire-rimmed glasses." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="d6d3777179ccd662926e87252e83b326" data-rm-shortcode-name="rebelmouse-image" id="1ebb0" loading="lazy" src="https://spectrum.ieee.org/media-library/in-a-black-and-white-image-of-the-german-physicist-karl-ferdinand-braun-he-has-a-full-beard-and-small-wire-rimmed-glasses.jpg?id=29722374&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The German physicist Karl Ferdinand Braun invented a type of vacuum tube that became the basis of 20th-century CRTs, used in television sets and computer monitors.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Alamy</small>
</p><p>
	In the years leading up to Thomson’s lecture, Geissler tubes, Plücker tubes, Hittorf tubes, and Crookes tubes filled laboratories and lecture halls. Mostly they were used to show the colorful effects of the electric discharge of cathode rays on different phosphorescent surfaces, without any clear practical applications.
</p><p>
	Then 
	<a href="https://ethw.org/Wilhelm_Roentgen" rel="noopener noreferrer" target="_blank">Wilhelm Röntgen</a> noticed something unusual. Even though the Crookes tube he was using was encased in black cardboard, a phosphorescent screen across the room started glowing. This led to his 1895 announcement that he had discovered X-rays. The discovery set off a fresh new wave of cathode-ray experiments.
</p><p>
	 One scientist inspired by Röntgen’s work was 
	<a href="https://www.lindahall.org/karl-ferdinand-braun/" rel="noopener noreferrer" target="_blank">Karl Ferdinand Braun</a>. Braun had received his Ph.D. from the University of Berlin in 1872, studying the oscillations of strings and elastic rods, and he spent the next 20 years in various positions at the universities of Marburg, Karlsruhe, and Tübingen. At the time of Röntgen’s discovery, Braun was director of the Physical Institute at Strasbourg. According to historian George Shiers, in his 1974 article “<a href="https://www.jstor.org/stable/24950032?refreqid=excelsior%3Ab8cc6dcfb75ae4d114113907212814cb" rel="noopener noreferrer" target="_blank">Ferdinand Braun and the Cathode Ray Tube</a>,” for <em>Scientific American</em>, Braun differed from many Röntgen enthusiasts in that he was more interested in the source of the X-rays than in the applications of the radiation.</p><p>
	Braun sought a new type of instrument, one that could visually capture the oscillatory and transitory phenomena in electrical circuits. In short, he wanted to map alternating current, in what would turn out to be a precursor of the oscilloscope. He instructed his instrument maker, Franz Müller, to modify a Crookes tube by adding a restrictive diaphragm across the middle of it. The diaphragm focused the thin beam of cathode rays. A phosphor-coated piece of mica served as a viewing screen. A coil outside the tube deflected the cathode-ray beam at right angles to the magnetic field. Because the beam responded almost immediately to changes in voltage or current, it could be used to trace these patterns on the screen. Braun published a description of his tube, including a diagram, in 
	<em>Annalen der Physik </em>in February 1897, 10 weeks before Thomson’s lecture.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A page of scientific text in German with a simple line drawing of an early vacuum tube." class="rm-shortcode" data-rm-shortcode-id="03d8441fd87f2ee31b8747a7297c7622" data-rm-shortcode-name="rebelmouse-image" id="649f2" loading="lazy" src="https://spectrum.ieee.org/media-library/a-page-of-scientific-text-in-german-with-a-simple-line-drawing-of-an-early-vacuum-tube.jpg?id=29722385&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In Karl Ferdinand Braun’s cathode-ray tube indicator, voltage was applied to the anode (A) and cathode (K), causing a negatively charged beam to be emitted from the cathode. A diaphragm (C) focused the beam, which then struck a phosphor-coated screen (D) at the opposite end.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The Linda Hall Library of Science, Engineering & Technology</small>
</p><p>If Thomson knew about Braun’s work when he gave his 30 April lecture, he made no mention of it. Nor did Thomson choose to call his subatomic particles by a name that was already in use: the electron. In 1874, the Irish physicist <a href="https://en.wikipedia.org/wiki/George_Johnstone_Stoney" rel="noopener noreferrer" target="_blank">George Johnstone Stoney</a> proposed <em>electrine</em> for the unknown subatomic particle, later changing it to <em>electron</em> in 1891. Stoney also estimated the electron’s charge (which turned out to be very close to the modern value), and he was frustrated that <a href="https://www.biodiversitylibrary.org/item/122066#page/432/mode/2up" rel="noopener noreferrer" target="_blank">Hermann von Helmholtz kept getting credit</a> for that discovery.</p><p>Thomson otherwise did a good job of tracing the history of experiments on cathode rays and naming scientists and instrument makers who had provided the foundation for his work. (The <a href="https://books.google.com/books?id=vBZbAAAAYAAJ&pg=PA104#v=onepage&q&f=false" rel="noopener noreferrer" target="_blank">text of Thomson’s lecture</a> was published in the 21 May 1897 issue of <em>The Electrician</em>.) Only in the final part of his lecture did he posit his corpuscle hypothesis and describe the experiments in which he measured the ratio of the corpuscles’ mass to their charge. But he did not conclude with a definitive statement on the discovery of the electron. Rather, he ended by simply noting that the ratio is of the same order as the value that Dutch physicist <a href="https://www.lindahall.org/pieter-zeeman/" rel="noopener noreferrer" target="_blank">Pieter Zeeman</a> deduced the previous year, in his experiments on the magnetic field of sodium light.</p><p>Zeeman discovered that when an element is burned in the presence of a strong magnetic field, spectral lines are split into regular patterns. Today we understand the “Zeeman effect” to be the result of electron spin, but the electron had not yet been discovered. However, Zeeman's supervisor Hendrik Lorentz had already theorized that atoms might consist of charged particles. Zeeman and Lorentz won a Nobel Prize in 1902 for this work.</p><h2>Different theories of discovery</h2><p>
	Who then discovered the electron? Braun’s invention and Thomson’s discovery both built on decades of work by numerous scientists and instrument makers, so it seems unfair to give credit to one individual. It’s true, though, that Braun never patented his invention, nor did he do much to promote it. By the end of 1897, he’d abandoned his cathode-ray research and moved on to wireless telegraphy, for which he shared a Nobel Prize in Physics with Guglielmo Marconi, in 1909. Braun’s CRTs didn’t see much action during his lifetime, but modified versions dominated television sets and computer monitors during the second half of the 20th century, and Braun is hailed as their original inventor.
</p><p>
	Meanwhile, Thomson doubled down on his corpuscle theory, winning his Nobel Prize in 1906. And, even though the plum pudding model he espoused fizzled after a few years, Thomson is the one recognized as the discoverer of the electron 125 years later.
</p><p>The question of discovery and who should get credit is much debated amongst historians, philosophers, scientists, and textbook writers, who often have different ideas on the matter, especially when there is a long path to the end result with many different players. In her book 
	<a href="http://www.amazon.co.uk/exec/obidos/ASIN/0748407200/qid=1125600918/sr=8-1/ref=sr_8_xs_ap_i1_xgl/202-2540451-5546262" rel="noopener noreferrer" target="_blank"><em>J.J. Thomson and the Discovery of the Electron</em></a>, the historian of mathematics and science <a href="https://uk.linkedin.com/in/isobel-falconer-a394b7a" target="_blank">Isobel Falconer</a> argues (quite persuasively) against the traditional nationalistic debate over the nature of cathode rays. She points out that Thomson didn’t even become interested in them until 1896, and even then, his corpuscles had little in common with what was being called an electron at the time. In his article “<a href="https://www.sciencedirect.com/science/article/abs/pii/S1355219896000196" target="_blank">Rethinking the ‘Discovery’ of the Electron</a>,” historian <a href="http://scholar.uoa.gr/tarabatz/home" target="_blank">Theodore Arabatzis</a> proposes a taxonomy of what it means to discover an unobservable entity. <a href="https://www.informationphilosopher.com/solutions/philosophers/hacking/" target="_blank">Philosopher Ian Hacking</a> suggests in his 1983 book, <a href="https://www.cambridge.org/core/books/representing-and-intervening/F6506B708BB5A8B6A5D884BDCF28E7B7" target="_blank"><em>Representing and Intervening</em></a>, that something has been discovered once a scientist finds a way to manipulate the entity. But maybe discovery should be counted from when something is demonstrated or published or named. Or perhaps the recognition of discovery should occur only retrospectively, once the modern concept of the entity’s various features has been established—in the case of the electron, that would be its mass, charge, spin, and particle-wave nature.<br/></p><p>
	I realize that bringing up these philosophical questions kills the joy of a narrative based on a single aha moment. It’s easier on our brains when histories of discovery have neat edges, with a finite cast of fascinating characters, perhaps some hardship and setbacks to overcome, and a rousing, definitive success at the end. That’s the Hollywood biopic version of discovery. It’s not wrong, necessarily, but the truth is messier, richer, and more tentative.
</p><p>
<em><em>Part of a </em><em><a href="https://spectrum.ieee.org/tag/Past+Forward" rel="noopener noreferrer" target="_self">continuing series</a></em><em> looking at photographs of historical artifacts that embrace the boundless potential of technology.</em></em></p><p><em><em></em><em>An abridged version of this article appears in the May 2022 print issue as “Birth of the Electron."</em></em>
</p>]]></description><pubDate>Sat, 30 Apr 2022 19:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/discovery-of-the-electron</guid><category>Atomic theory</category><category>Cathode-ray tubes</category><category>Discovery of the electron</category><category>J.j. thomson</category><category>Karl ferdinand braun</category><category>Past forward</category><category>Type:departments</category><category>Vacuum tubes</category><category>Electron discovery</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-elongated-glass-vacuum-tube-sits-lengthwise-on-a-wooden-stand.jpg?id=29721938&amp;width=980"></media:content></item><item><title>The Computers Who Brought ENIAC to Life</title><link>https://spectrum.ieee.org/eniac-woman-programmers</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-and-white-image-of-three-women-each-in-front-of-a-large-wheeled-cabinet-faced-with-countless-rotary-switches.jpg?id=29705856&width=1200&height=800&coordinates=14%2C0%2C0%2C0"/><br/><br/><p>The Electronic Numerical Integrator and Computer—better known as <a data-linked-post="2650274151" href="https://spectrum.ieee.org/great-computers-never-die" target="_blank">ENIAC</a>—became the world’s first programmable general-purpose electronic computer when it was completed in 1945. ENIAC’s hardware was designed by John Mauchly and J. Presper Eckert, but the programs it ran were largely the creation of <a href="https://penntoday.upenn.edu/news/eniacs-anniversary-nod-its-female-computers" target="_blank">a team of six women</a>. For decades, <a href="https://spectrum.ieee.org/untold-history-of-ai-invisible-woman-programmed-americas-first-electronic-computer" target="_blank">these women were largely unknown</a>, except only as unidentified figures in photographs of ENIAC. But as an undergraduate, <a href="https://www.linkedin.com/in/kathy-kleiman-09832b1/" target="_blank">Kathy Kleiman</a>—who would later help found <a href="https://www.icann.org/" target="_blank">ICANN</a> (Internet Corporation for Assigned Names and Numbers)—started looking into who they were. This weekend at the <a href="https://vcfed.org/events/vintage-computer-festival-east/" target="_blank">Vintage Computer Festival East</a> in Wall, N.J., Kleiman will be screening her short documentary <a href="https://eniacprogrammers.org/documentary-info/" target="_blank"><em>The Computers</em></a>, about the programmers. In advance of her talk, <em>IEEE Spectrum</em> spoke to Kleiman about the ENIAC women and her fascination with them.</p><p><strong>How did these women come to be such a central part of the history of computing?</strong></p><p><strong>Kathy Kleiman:</strong> During World War II, the army needed people to hand-calculate ballistics trajectories or artillery firing tables. And male mathematicians were running short. The Army relocated the project from rural Maryland to Philadelphia and went looking for women math majors in Philadelphia, which has a very high density of schools with coed universities and colleges and all-women schools. </p><p>Later they would go across the country looking for women math majors to come to the Morse School of Electrical Engineering, which is where they located this project and where they hand-calculated ballistic trajectories using mechanical desktop calculators. But it took 30 to 40 hours to calculate a single trajectory for one set of weather conditions for one gun and one projectile, and the Army needed hundreds of trajectories per firing table. </p><p>So in the dark days of the war, early 1943, when there was no end to the war in sight, they agreed to fund the experiment of a visionary guy who also happened to be at the Moore School at that time. His name was Dr. John Mauchly. He partnered with Presper Eckert, who was 23 years old at the time, a young engineering grad. They were yin and yang, a great combination. With Army funding, they built this machine that wasn’t supposed to work—18,000 vacuum tubes were never supposed to be able to work in concert. But they did it, a machine 8 feet tall and 80 feet long. </p><p>But when they’re almost done, they’re like, “Wait a second.” Part of the Army contract was delivering a working ballistic trajectory calculated by the machine. So a mathematician and Army lieutenant at the proving ground called Herman Goldstein picks six out of the 80 to 100 women who’ve been calculating trajectories. They [Kathleen Antonelli, Jean Bartik, Betty Holberton, Marlyn Meltzer, Frances Spence, and Ruth Teitelbaum] are given the wiring diagrams and the block diagrams and told to figure it out so they can do the ballistics trajectory equation. </p><p>The women don’t have security access to even see the actual computer, but they figure it out, doing what is now called <a href="https://studylib.net/doc/8742623/programming-the-eniac-before-its-rewiring." target="_blank">direct programming</a>. There’s looping, there’s conditional logic, and the women collectively mastered all this and made it perform the ballistic trajectory calculation that wound up becoming the climactic moment of demonstration day on 6 February 1946, when ENIAC was unveiled. </p><p><strong>Why did you start looking into their story?</strong></p><p><strong>Kleiman: </strong>I was at Harvard. I was kind of a social theory major. I took computer science from the first classes that I took in college because I was already a programmer because of a Western Electric program when I was in high school. I also noticed that as the levels of the computer science classes went higher, the number of women dropped. And I knew about Ada Lovelace. I knew about Grace Hopper. Ada Lovelace was in the 19th century, then Grace Hopper in the 20th century. And one woman succeeding in computing per century didn’t make me feel warm and fuzzy, so I went looking for more. </p><p>I found the pictures of ENIAC taken before demonstration day and given to the press and published across the country.  These pictures are beautiful black-and-white pictures, and they have men and women in them; some of them just have women! But while some of the men, particularly Eckert and Mauchly, are named in the captions, none of the names of the women are in the captions. I wanted to know who they were. I was told by some computer historians at the time that they were models, and they didn’t look like models to me. I tracked them down, and they weren’t models; they were programmers.</p><p><strong>You’ll be showing your documentary at <a href="https://vcfed.org/events/vintage-computer-festival-east/" target="_blank">VCF East</a>, which features interviews with four of the programmers conducted before they passed away, but you’ll also be <a href="https://www.grandcentralpublishing.com/titles/kathy-kleiman/proving-ground/9781549185458/" target="_blank">releasing a book later this year</a>, called<em> Proving Ground</em>?</strong></p><p><strong>Kleiman:</strong> The documentary raised as many questions as it answered. So I was kind of persuaded to tell the rest of the story, and really sit down and talk about the incredible work, not just of the ENIAC programmers but of millions of women on the home front during World War II. It turned out that is not a story we know very well. I had always known women went to the factories, they went to the farms. I didn’t realize until I sat down in front of newspapers of the period and saw the ads that there was an enormous push for women with science and technology backgrounds. If you had the interest, the aptitude, and some training, these articles made it clear they’d teach you the rest, not just for the military but for industry. That was just a whole part of the story I’d never heard, that women also filled in these gaps in science, technology, and engineering.</p>]]></description><pubDate>Thu, 21 Apr 2022 16:51:16 +0000</pubDate><guid>https://spectrum.ieee.org/eniac-woman-programmers</guid><category>Eniac</category><category>Women who code</category><category>Documentaries</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/black-and-white-image-of-three-women-each-in-front-of-a-large-wheeled-cabinet-faced-with-countless-rotary-switches.jpg?id=29705856&amp;width=980"></media:content></item><item><title>How a Parachute Accident Helped Jump-start Augmented Reality</title><link>https://spectrum.ieee.org/history-of-augmented-reality</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/man-wearing-goggles-and-earphones-holding-wired-controllers-in-each-hand.png?id=29633903&width=1200&height=800&coordinates=9%2C0%2C9%2C0"/><br/><br/><p>
<strong>I climb into</strong> an upper-body exoskeleton that’s covered in sensors, motors, gears, and bearings, and then lean forward, tilting my head up to press my face against the eyepieces of a vision system hanging from the ceiling. In front of me, I see a large wooden board, painted black and punctuated by a grid of metal holes. The board is real. So is the peg in my hand that I’m trying to move from one hole to another, as fast as I can. When I begin to move the peg, a virtual cone appears over the target hole, along with a virtual surface easing toward it. I can feel the surface as I slide the peg along it toward the cone and into the hole.
</p><p>
	This was the Virtual Fixtures platform, which was developed in the early 1990s to test the potential of “perceptual overlays” to improve human performance in manual tasks that require dexterity. And it worked.
</p><p>
	These days, virtual-reality experts look back on the platform as the first interactive augmented-reality system that enabled users to engage simultaneously with real and virtual objects in a single immersive reality.
</p><p>
<strong>The project began</strong> in 1991, when I pitched the effort as part of my doctoral research at <a href="https://www.stanford.edu/" target="_blank">Stanford University</a>. By the time I finished—three years and multiple prototypes later—the system I had assembled filled half a room and used nearly a million dollars’ worth of hardware. And I had <a href="https://www.proquest.com/openview/03a4bb46f5c658124bb211834fdce3dd" target="_blank">collected enough data</a> from human testing to definitively show that augmenting a real workspace with virtual objects could significantly enhance user performance in precision tasks.
</p><p>
	Given the short time frame, it might sound like all went smoothly, but the project came close to getting derailed many times, thanks to a tight budget and substantial equipment needs. In fact, the effort might have crashed early on, had a parachute—a real one, not a virtual one—not failed to open in the clear blue skies over Dayton, Ohio, during the summer of 1992.
</p><p>
	Before I explain how a parachute accident helped drive the development of augmented reality, I’ll lay out a little of the historical context.
</p><p>
	Thirty years ago, the field of virtual reality was in its infancy, the phrase itself having only been coined in 1987 by 
	<a href="http://www.jaronlanier.com/" target="_blank">Jaron Lanier</a>, who was commercializing some of the first headsets and gloves. His work built on earlier research by <a href="https://history.computer.org/pioneers/sutherland.html" target="_blank">Ivan Sutherland</a>, who pioneered head-mounted display technology and head-tracking, two critical elements that sparked the VR field. Augmented reality (AR)—that is, combining the real world and the virtual world into a single immersive and interactive reality—did not yet exist in a meaningful way.
</p><p>
	Back then, I was a graduate student at Stanford University and a part-time researcher at 
	<a href="https://www.nasa.gov/ames" target="_blank">NASA’s Ames Research Center</a>, interested in the creation of virtual worlds. At Stanford, I worked in the <a href="https://centerfordesignresearch.stanford.edu/" target="_blank">Center for Design Research</a>, a group focused on the intersection of humans and technology that created some of the very early VR gloves, immersive vision systems, and 3D audio systems. At NASA, I worked in the Advanced Displays and Spatial Perception Laboratory of the Ames Research Center, where researchers were exploring the fundamental parameters required to enable realistic and immersive simulated worlds.
</p><p>
	Of course, knowing how to create a quality VR experience and being able to produce it are not the same thing. The best PCs on the market back then used Intel 486 processors running at 33 megahertz. Adjusted for inflation, they cost about US $8,000 and weren’t even a thousandth as fast as a cheap gaming computer today. The other option was to invest $60,000 in a 
	<a href="https://www.computerhistory.org/revolution/computer-graphics-music-and-art/15/219" target="_blank">Silicon Graphics</a> workstation—still less than a hundredth as fast as a mediocre PC today. So, though researchers working in VR during the late 80s and early 90s were doing groundbreaking work, the crude graphics, bulky headsets, and lag so bad it made people dizzy or nauseous plagued the resulting virtual experiences.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Nine diagrams, in a grid, each showing a simple pegboard with four holes, with a variety of transparent representations of planes and other guide surfaces in front of it. " class="rm-shortcode" data-rm-shortcode-id="8e1458c0a629ce35150d6b4823e58e8b" data-rm-shortcode-name="rebelmouse-image" id="463a5" loading="lazy" src="https://spectrum.ieee.org/media-library/nine-diagrams-in-a-grid-each-showing-a-simple-pegboard-with-four-holes-with-a-variety-of-transparent-representations-of-plane.png?id=29634202&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">These early drawings of a real pegboard combined with virtual overlays generated by a computer—an early version of augmented reality—were created by Louis Rosenberg as part of his Virtual Fixtures project.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Louis Rosenberg</small>
</p><p>
	I was conducting a research project at NASA to 
	<a href="https://apps.dtic.mil/sti/pdfs/ADA289764.pdf" target="_blank">optimize depth perception</a> in early 3D-vision systems, and I was one of those people getting dizzy from the lag. And I found that the images created back then were definitely <em>virtual</em> but far from <em>reality.</em>
</p><p>
	Still, I wasn’t discouraged by the dizziness or the low fidelity, because I was sure the hardware would steadily improve. Instead, I was concerned about how enclosed and isolated the VR experience made me feel. I wished I could expand the technology, taking the power of VR and unleashing it into the real world. I dreamed of creating a merged reality where virtual objects inhabited your physical surroundings in such an authentic manner that they seemed like genuine parts of the world around you, enabling you to reach out and interact as if they were actually there.
</p><p>
	I was aware of one very basic sort of merged reality—the head-up display— in use by military pilots, enabling flight data to appear in their lines of sight so they didn’t have to look down at cockpit gauges. I hadn’t experienced such a display myself, but became familiar with them thanks to a few blockbuster 1980s hit movies, including 
	<em>Top Gun </em>and <em>Terminator</em>. In <em>Top Gun</em> a glowing crosshair appeared on a glass panel in front of the pilot during dogfights; in <em>Terminator</em>, crosshairs joined text and numerical data as part of the fictional cyborg’s view of the world around it.
</p><p>
	Neither of these merged realities were the slightest bit immersive, presenting images on a flat plane rather than connected to the real world in 3D space. But they hinted at interesting possibilities. I thought I could move far beyond simple crosshairs and text on a flat plane to create virtual objects that could be spatially registered to real objects in an ordinary environment. And I hoped to instill those virtual objects with realistic physical properties.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A square board with eight numbered holes. Two mesh cones appear in front of holes 2 and 6" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="f0110f3bb52495336ee9b4d61e97a4d1" data-rm-shortcode-name="rebelmouse-image" id="84a4b" loading="lazy" src="https://spectrum.ieee.org/media-library/a-square-board-with-eight-numbered-holes-two-mesh-cones-appear-in-front-of-holes-2-and-6.png?id=29634270&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">The Fitts’s Law peg-insertion task involves having test subjects quickly move metal pegs between holes. The board shown here was real, the cones that helped guide the user to the correct holes virtual.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Louis Rosenberg</small>
</p><p>
	I needed substantial resources—beyond what I had access to at Stanford and NASA—to pursue this vision. So I pitched the concept to the Human Sensory Feedback Group of the U.S. Air Force’s Armstrong Laboratory, now part of the 
	<a href="https://www.afrl.af.mil/" target="_blank">Air Force Research Laboratory</a>.
</p><p>
	To explain the practical value of merging real and virtual worlds, I used the analogy of a simple metal ruler. If you want to draw a straight line in the real world, you can do it freehand, going slow and using significant mental effort, and it still won’t be particularly straight. Or you can grab a ruler and do it much quicker with far less mental effort. Now imagine that instead of a real ruler, you could grab a virtual ruler and make it instantly appear in the real world, perfectly registered to your real surroundings. And imagine that this virtual ruler feels physically authentic—so much so that you can use it to guide your real pencil. Because it’s virtual, it can be any shape and size, with interesting and useful properties that you could never achieve with a metal straightedge.
</p><p>
	Of course, the ruler was just an analogy. The applications I pitched to the Air Force ranged from augmented manufacturing to surgery. For example, consider a surgeon who needs to make a dangerous incision. She could use a bulky metal fixture to steady her hand and avoid vital organs. Or we could invent something new to augment the surgery—a virtual fixture to guide her real scalpel, not just visually but physically. Because it’s virtual, such a fixture would pass right through the patient’s body, sinking into tissue before a single cut had been made. That was the concept that got the military excited, and their interest wasn’t just for in-person tasks like surgery but for distant tasks performed using remotely controlled robots. For example, a technician on Earth could repair a satellite by controlling a robot remotely, assisted by virtual fixtures added to video images of the real worksite. The Air Force agreed to provide enough funding to cover my expenses at Stanford along with a small budget for equipment. Perhaps more significantly, I also got access to computers and other equipment at 
	<a href="https://www.wpafb.af.mil/" target="_blank">Wright-Patterson Air Force Base</a> near Dayton, Ohio.
</p><p>
	And what became known as the Virtual Fixtures Project came to life, working toward building a prototype that could be rigorously tested with human subjects. And I became a roving researcher, developing core ideas at Stanford, fleshing out some of the underlying technologies at NASA Ames, and assembling the full system at Wright-Patterson.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A sketch of a person wearing earphones and googles gazing at two green cones and a grey pegboard with four holes." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="11cc66b55148e78496178db9250fda4e" data-rm-shortcode-name="rebelmouse-image" id="9c2f9" loading="lazy" src="https://spectrum.ieee.org/media-library/a-sketch-of-a-person-wearing-earphones-and-googles-gazing-at-two-green-cones-and-a-grey-pegboard-with-four-holes.png?id=29634315&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">In this sketch of his augmented-reality system, Louis Rosenberg shows a user of the Virtual Fixtures platform wearing a partial exoskeleton and peering at a real pegboard augmented with cone-shaped virtual fixtures.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Louis Rosenberg</small>
</p><p>
<strong>Now about those </strong>parachutes.
</p><p>
	As a young researcher in my early twenties, I was eager to learn about the many projects going on around me at these various laboratories. One effort I followed closely at Wright-Patterson was a project designing new parachutes. As you might expect, when the research team came up with a new design, they didn’t just strap a person in and test it. Instead, they attached the parachutes to dummy rigs fitted with sensors and instrumentation. Two engineers would go up in an airplane with the hardware, dropping rigs and jumping alongside so they could observe how the chutes unfolded. Stick with my story and you’ll see how this became key to the development of that early AR system.
</p><p>
	Back at the Virtual Fixtures effort, I aimed to prove the basic concept—that a real workspace could be augmented with virtual objects that feel so real, they could assist users as they performed dexterous manual tasks. To test the idea, I wasn't going to have users perform surgery or repair satellites. Instead, I needed a simple repeatable task to quantify manual performance. The Air Force already had a standardized task it had used for years to test human dexterity under a variety of mental and physical stresses. It’s called the 
	<a href="https://us.humankinetics.com/blogs/excerpt/understanding-fitts-law" target="_blank">Fitts’s Law</a> peg-insertion task, and it involves having test subjects quickly move metal pegs between holes on a large pegboard.
</p><p>
	So I began assembling a system that would enable virtual fixtures to be merged with a real pegboard, creating a mixed-reality experience perfectly registered in 3D space. I aimed to make these virtual objects feel so real that bumping the real peg into a virtual fixture would feel as authentic as bumping into the actual board.
</p><p>
	I wrote software to simulate a wide range of virtual fixtures, from simple surfaces that prevented your hand from overshooting a target hole, to carefully shaped cones that could help a user guide the real peg into the real hole. I created virtual overlays that simulated textures and had corresponding sounds, even overlays that simulated pushing through a thick liquid as it it were virtual honey.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A drawing of a person lying flat on a white surface, with green and white panels crisscrossing the body." class="rm-shortcode" data-rm-shortcode-id="47d541dc872550df895d205831f76f7d" data-rm-shortcode-name="rebelmouse-image" id="375c9" loading="lazy" src="https://spectrum.ieee.org/media-library/a-drawing-of-a-person-lying-flat-on-a-white-surface-with-green-and-white-panels-crisscrossing-the-body.png?id=29634319&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">One imagined use for augmented reality at the time of its creation was in surgery. Today, augmented reality is used for surgical training, and surgeons are beginning to use it in the operating room.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Louis Rosenberg</small>
</p><p>
	For more realism, I modeled the physics of each virtual element, registering its location accurately in three dimensions so it lined up with the user’s perception of the real wooden board. Then, when the user moved a hand into an area corresponding to a virtual surface, motors in the exoskeleton would physically push back, an interface technology now commonly called “haptics.” It indeed felt so authentic that you could slide along the edge of a virtual surface the way you might move a pencil against a real ruler.
</p><p>
	To accurately align these virtual elements with the real pegboard, I needed high-quality video cameras. Video cameras at the time were far more expensive than they are today, and I had no money left in my budget to buy them. This was a frustrating barrier: The Air Force had given me access to a wide range of amazing hardware, but when it came to simple cameras, they couldn’t help. It seemed like every research project needed them, most of far higher priority than mine.
</p><p>
<strong>Which brings me</strong> back to the skydiving engineers testing experimental parachutes. These engineers came into the lab one day to chat; they mentioned that their chute had failed to open, their dummy rig plummeting to the ground and destroying all the sensors and cameras aboard.
</p><p>
	This seemed like it would be a setback for my project as well, because I knew if there were any extra cameras in the building, the engineers would get them.
</p><p>
	But then I asked if I could take a look at the wreckage from their failed test. It was a mangled mess of bent metal, dangling circuits, and smashed cameras. Still, though the cameras looked awful with cracked cases and damaged lenses, I wondered if I could get any of them to work well enough for my needs.
</p><p>
	 By some miracle, I was able to piece together two working units from the six that had plummeted to the ground. And so, the first human testing of an interactive augmented-reality system was made possible by cameras that had literally fallen out of the sky and smashed into the earth.
</p><p>
	To appreciate how important these cameras were to the system, think of a simple AR application today, like 
	<em>Pokémon Go</em>. If you didn’t have a camera on the back of your phone to capture and display the real world in real time, it wouldn’t be an augmented-reality experience; it would just be a standard video game.
</p><p>
	The same was true for the Virtual Fixtures system. But thanks to the cameras from that failed parachute rig, I was able to create a mixed reality with accurate spatial registration, providing an immersive experience in which you could reach out and interact with the real and virtual environments simultaneously.
</p><p>
	As for the experimental part of the project, I conducted a series of human studies in which users experienced a variety of virtual fixtures overlaid onto their perception of the real task board. The most useful fixtures turned out to be cones and surfaces that could guide the user’s hand as they aimed the peg toward a hole. The most effective involved physical experiences that couldn’t be easily manufactured in the real world but were readily achievable virtually. For example, I coded virtual surfaces that were “magnetically attractive” to the peg. For the users, it felt as if the peg had snapped to the surface. Then they could glide along it until they chose to yank free with another snap. Such fixtures increased speed and dexterity in the trials by more than 100 percent.
</p><p>
	 Of the various applications for Virtual Fixtures that we considered at the time, the most commercially viable back then involved manually controlling robots in remote or dangerous environments—for example, during hazardous waste clean-up. If the communications distance introduced a time delay in the telerobotic control, virtual fixtures 
	<a href="https://apps.dtic.mil/sti/citations/ADA296363" target="_blank">became even more valuable</a> for enhancing human dexterity.
</p><p>
	Today, researchers are still exploring the use of virtual fixtures for telerobotic applications with great success, including for use in 
	<a href="https://ieeexplore.ieee.org/document/8429059" target="_blank">satellite repair</a> and <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/rcs.421" target="_blank">robot-assisted surgery</a>.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="An employee badge with a photo of a man and logo of NASA. Text on badge reads Ames Research Center" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="91fe6c228a5eef778a39065f1b3f1b99" data-rm-shortcode-name="rebelmouse-image" id="041d0" loading="lazy" src="https://spectrum.ieee.org/media-library/an-employee-badge-with-a-photo-of-a-man-and-logo-of-nasa-text-on-badge-reads-ames-research-center.png?id=29634237&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;"> Louis Rosenberg spent some of his time working in the Advanced Displays and Spatial Perception Laboratory of the Ames Research Center as part of his research in augmented reality.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Louis Rosenberg</small>
</p><p>
<strong>I went in</strong> a different direction, pushing for more mainstream applications for augmented reality. That’s because the part of the Virtual Fixtures project that had the greatest impact on me personally wasn’t the improved performance in the peg-insertion task. Instead, it was the big smiles that lit up the faces of the human subjects when they climbed out of the system and effused about what a remarkable experience they had had. Many told me, without prompting, that this type of technology would one day be everywhere.
</p><p>
	And indeed, I agreed with them. I was convinced we’d see this type of immersive technology go mainstream by the end of the 1990s. In fact, I was so inspired by the enthusiastic reactions people had when they tried those early prototypes, I founded a company in 1993—<a href="https://www.immersion.com/" target="_blank">Immersion</a>—with the goal of pursuing mainstream consumer applications. Of course, it hasn’t happened nearly that fast.
</p><p>
	At the risk of being wrong again, I sincerely believe that virtual and augmented reality, now commonly referred to as the metaverse, will become an important part of most people’s lives by the end of the 2020s. In fact, based on the recent surge of investment by major corporations into improving the technology, I predict that by the early 2030s augmented reality will replace the mobile phone as our primary interface to digital content.
</p><p>
	And no, none of the test subjects who experienced that early glimpse of augmented reality 30 years ago knew they were using hardware that had fallen out of an airplane. But they did know that they were among the first to reach out and touch our augmented future.
</p>]]></description><pubDate>Thu, 07 Apr 2022 19:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/history-of-augmented-reality</guid><category>Augmented reality</category><category>Virtual reality</category><category>Tech history</category><dc:creator>Louis Rosenberg</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/man-wearing-goggles-and-earphones-holding-wired-controllers-in-each-hand.png?id=29633903&amp;width=980"></media:content></item><item><title>The First Digital Camera Was the Size of a Toaster</title><link>https://spectrum.ieee.org/first-digital-camera-history</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-smiling-man-with-a-large-camera-with-several-cameras-in-front-of-him.jpg?id=29644015&width=1200&height=800&coordinates=0%2C0%2C0%2C100"/><br/><br/><p>In the past two weeks, you’ve probably shared at least one embarrassing photo with family or friends. The process, from taking the photo to sharing it, was practically instantaneous. Yet only 20 years ago, you would have had to load and unload film in your camera, drop the film off for processing, and then wait days before you’d know if you even had any images worth sharing.</p>
<p>Digital cameras have been around a lot longer than you might think, however. Invented in 1975 at <a href="https://www.kodak.com/en" rel="noopener noreferrer" target="_blank">Eastman Kodak</a> in Rochester, N.Y., the first digital camera displayed photos on its screen.</p><hr/>
<p>The <a href="https://www.diyphotography.net/worlds-first-digital-camera-introduced-man-invented/" rel="noopener noreferrer" target="_blank">Kodak digital camera</a> has been commemorated as an <a href="http://ieeemilestones.ethw.org/Main_Page" rel="noopener noreferrer" target="_blank">IEEE Milestone</a>. The dedication ceremony is scheduled for 26 April at the <a href="https://www.kodakcenter.com/" rel="noopener noreferrer" target="_blank">Kodak Center</a> in Rochester. <a href="https://events.vtools.ieee.org/m/308439" rel="noopener noreferrer" target="_blank">Registration is open</a>; guests can attend in person or virtually.</p>
<p>“This was more than just a camera,” IEEE Member <a href="https://spectrum.ieee.org/ten-on-tech-spotlight-on-steven-j-sasson" target="_self">Steven J. Sasson</a>, inventor of the device, told <a href="https://lens.blogs.nytimes.com/2015/08/12/kodaks-first-digital-moment/" rel="noopener noreferrer" target="_blank"><em>The New York Times</em></a> in 2015. “It was a photographic system to demonstrate the idea of an all-electronic camera that didn’t use any consumables in the capturing and display of still photographic images.”</p>
<p>Sasson says he never anticipated how the imaging of everything would become so ubiquitous.</p>
<p>“Photos have become the universal form of casual conversation,” he said in a 2018 <a href="https://spectrum.ieee.org/ten-on-tech-spotlight-on-steven-j-sasson" target="_self">interview</a> with <em>The Institute</em>. “And cameras are present in almost every type of environment, including in our own homes.” </p><p><strong>COMPLETING A DIFFICULT TASK</strong></p><p>Eastman Kodak wanted to find a way to digitize images using a charged coupled device—specifically <a href="https://www.mouser.com/manufacturer/fairchild-semiconductor/" rel="noopener noreferrer" target="_blank">Fairchild Semiconductor</a>’s 100-by-100-pixel CCD. The company assigned the job to Sasson in 1974, when he joined Kodak as an electronics engineer working in the apparatus division’s research lab.</p><p>CCDs, which were invented in 1969 by <a href="https://en.wikipedia.org/wiki/Willard_Boyle" rel="noopener noreferrer" target="_blank">Willard Boyle</a> and <a href="https://en.wikipedia.org/wiki/George_E._Smith" rel="noopener noreferrer" target="_blank">George E. Smith</a> at <a href="https://en.wikipedia.org/wiki/Bell_Labs" rel="noopener noreferrer" target="_blank">Bell Labs</a>, consist of a sensor that converts an incoming two-dimensional light pattern into an electrical signal that in turn becomes an image. In the case of Fairchild’s CCD, the image would be a square: 100 by 100 pixels.</p>
<p>Although the CCDs could capture an image, they couldn’t store it. So Sasson built a CCD into a camera with RAM to capture image data, which was then transferred to a cassette tape. In a 2016 interview with <a href="https://www.diyphotography.net/worlds-first-digital-camera-introduced-man-invented/" rel="noopener noreferrer" target="_blank">DIY Photography</a>, he said cassettes were the only permanent form of “digital storage” available to him at the time. He designed it so each tape would store 30 images.</p>
<p>To build his digital camera, Sasson scavenged a lens and an exposure mechanism from a Kodak XL55 movie camera. They served as his camera’s optics and were enclosed in a blue, rectangular box. The box also had a switch on the side that turned the device on and off and served as the camera’s shutter-release button. The blue box sat atop a layer of a half dozen circuit boards and 16 AA batteries. Enclosed in an open steel frame, all of the parts could be seen. The frame also could unfold to make it easier to modify the camera. A portable Memodyne cassette recorder that was attached to the side of the frame held the tape. The camera weighed 3.6 kilograms and was about the size of a toaster.</p>
<p>The photographer flipped the switch once to turn on the camera and flipped it a second time to take a photo. The CCD would capture the image, which would then run through a <a href="https://www.motorola.com/us/about" rel="noopener noreferrer" target="_blank">Motorola</a> analog-to-digital converter and be stored temporarily in a DRAM array of a dozen 4,096-bit chips. The image was then transferred to the cassette.</p>
<p>Sasson and his colleagues invented a device to take the information stored on the tape and turn it into digital images. This playback unit converted the data to a standard NTSC signal so the images could be displayed on a TV screen.</p>
<p>After a year of working on the camera, Sasson took his first photo in December 1975. It was of a Kodak lab technician, Joy Marshall.</p>
<p>“It only took 50 milliseconds to capture the image, but it took 23 seconds to record it to the tape,” Sasson said in the 2015 <em>Times</em> article. “I’d pop the cassette tape out, hand it to my assistant, and he would put it in our playback unit. About 30 seconds later, up popped the 100-pixel-by-100-pixel black-and-white image.”</p>
<p>But when Sasson displayed the photo on the lab’s computer, the image’s flaws were apparent. The camera could render shades that were clearly dark or light, things in between appeared as static, according to a 2020 <a href="https://spectrum.ieee.org/how-the-digital-camera-transformed-our-concept-of-history" target="_self"><em>IEEE Spectrum</em> article</a> about digital cameras. Therefore, Marshall’s face was not visible in the photograph. </p>
<p>Sasson fixed those problems and was granted a <a href="https://patents.google.com/patent/US4131919A/en" rel="noopener noreferrer" target="_blank">U.S. patent</a> for the camera in 1978, but it was never put into production. Even after a few demonstrations of how the camera worked, Kodak executives said they didn’t see a market for it. Sasson wasn’t allowed to publicly talk about the camera or show his prototype to anyone outside of Kodak, according to an article on the camera on<a href="https://www.thevintagenews.com/2016/07/25/steve-sasson-invented-the-first-digital-camera-in-1975-but-was-forced-to-keep-it-hidden/?chrome=1&A1c=1" rel="noopener noreferrer" target="_blank"> <em>The Vintage News</em></a>.</p>
<p>That didn’t deter Sasson, who continued to build more cameras for Kodak. In 1994 he built one of the first commercially available digital cameras—the <a href="https://www.digitalkameramuseum.de/en/cameras/item/kodak-ap-nc2000" rel="noopener noreferrer" target="_blank">AP NC2000</a>—in collaboration with <a href="https://www.nikon.com/" rel="noopener noreferrer" target="_blank">Nikon</a>.</p><p>Today Sasson’s original digital camera is on display at the <a href="https://www.si.edu/" rel="noopener noreferrer" target="_blank">Smithsonian Institution</a>’s <a href="https://americanhistory.si.edu/" rel="noopener noreferrer" target="_blank">National Museum of American History</a>, in Washington, D.C.</p><p>Administered by the <a href="http://www.ieee.org/about/history_center/index.html" rel="noopener noreferrer" target="_blank">IEEE History Center</a> and <a href="https://www.ieeefoundation.org/donate_history" rel="noopener noreferrer" target="_blank">supported by donors</a>, the Milestone program recognizes outstanding technical developments around the world. The <a href="https://r1.ieee.org/rochester/" rel="noopener noreferrer" target="_blank">IEEE Rochester (N.Y.) Section</a> sponsored the nomination for the digital camera. Its Milestone plaque, which is to be displayed in the lobby of the Kodak Center, reads:</p>
<p><em>A self-contained portable digital camera was invented at an Eastman Kodak Company laboratory. It used movie camera optics, a charge-coupled device as an electronic light sensor, a temporary buffer of random-access memory, and image storage on a digital cassette. Subsequent commercial digital cameras using flash memory storage revolutionized how images are captured, processed, and shared, creating opportunities in commerce, education, and global communications.</em></p>]]></description><pubDate>Wed, 06 Apr 2022 18:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/first-digital-camera-history</guid><category>Ieee history</category><category>Ieee milestone</category><category>Digital camera</category><category>Kodak</category><category>Steven sasson</category><category>History of technology</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/photo-of-a-smiling-man-with-a-large-camera-with-several-cameras-in-front-of-him.jpg?id=29644015&amp;width=980"></media:content></item><item><title>The 11 Commandments of Hugging Robots</title><link>https://spectrum.ieee.org/robot-hugs</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-smiling-woman-hugs-a-robotic-torso-with-a-soft-grey-sweatshirt-the-robot-s-head-has-a-purple-computer-screen-with-a-smiling-f.jpg?id=29615031&width=1200&height=800&coordinates=0%2C302%2C0%2C329"/><br/><br/><p>If we’ve learned anything over the past few years, it’s how important it is not to take physical contact for granted. Unfortunately, physical contact with humans is something that robots find especially difficult and occasionally dangerous, so that we cannot (yet) safely use them as a proxy for nuanced physical contact with another person. It’s not just that robots are strong and humans are squishy (although both of those things are true), it’s that there are a lot of complex facets to human-on-human interaction that robots simply don’t understand.</p><p>In 2018, <a href="https://spectrum.ieee.org/the-importance-of-teaching-robots-to-hug" target="_blank">we wrote about research by Alexis E. Block and Katherine J. Kuchenbecker from the Haptic Intelligence Department at the Max Planck Institute for Intelligent Systems in Stuttgart, Germany, on teaching robots to give good hugs</a>. Over the last several years, they’ve continued this research (along with coauthors Sammy Christen, Hasti Seifi, Otmar Hilliges, and Roger Gassert), and have <a href="https://dl.acm.org/doi/10.1145/3526110" target="_blank">just published a paper</a> outlining the introduction of a new hugging robot along with 11 commandments that robots can follow to give hugs that humans will be able to appreciate and enjoy—without getting squished.<br/></p><p>To grasp why the apparently simple act of hugging demands so much research effort, next time you hug another person pay careful attention to what you’re doing and what they’re doing, and you’ll begin to understand. Hugs are interactive, emotional, and complex, and giving a good hug (especially to someone whom you don’t know well or have never hugged before) is challenging. It takes a lot of social experience and intuition, which is another way of saying that it’s a hard robotics problem, because social experience and intuition are things that robots tend not to be great at. Obviously, robotic embraces are never going to supplant human hugs, but the idea here is that sometimes getting physical human comfort is difficult or impossible, and in these cases, maybe robots could have something useful to offer.</p><p><a href="https://dl.acm.org/doi/10.1145/3526110" target="_blank">In this paper—just accepted in the <em>ACM Transactions on Human-Robot Interaction (THRI)</em></a>—Block used a data-driven approach to develop the commandments for hugging robots, building off of <a href="https://dl.acm.org/doi/10.1145/3434073.3444656" target="_blank">research also presented at the 2021 Human-Robot Interaction Conference</a>. Through a series of hardware iterations and user studies, the original <a href="https://robots.ieee.org/robots/pr2/" target="_blank">PR2</a>-based robotic hugging platform (HuggieBot) was been completely rebuilt and upgraded to HuggieBot 3.0, which is “the first fully autonomous human-sized hugging robot that recognizes and responds to the user’s intra-hug gestures.”</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="On left, a humanoid robot in a purple skirt and grey sweatshirt and grey mittens stands in an office. On right, the human robot hugging a woman." class="rm-shortcode" data-rm-shortcode-id="7be91d65a9895296c9b1f24b0a9faa98" data-rm-shortcode-name="rebelmouse-image" id="01e6c" loading="lazy" src="https://spectrum.ieee.org/media-library/on-left-a-humanoid-robot-in-a-purple-skirt-and-grey-sweatshirt-and-grey-mittens-stands-in-an-office-on-right-the-human-robot.jpg?id=29615018&width=980"/>
</p><p>HuggieBot 3.0 is built around two six-degree-of-freedom Kinova JACO arms mounted horizontally on a custom metal frame, on top of a v-shaped horizontal base that makes it easier for humans to get in nice and close. The arms are padded, and the end effectors have mittens on them. Placed over the frame are chest and back panels made of air-filled chambers that provide softness as well as pressure sensing, and there’s a heating pad on top of each air chamber to make sure that the robot is nice and warm.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A humanoid robot torso with exposed robotic arms, a monitor for a head with a smiling face on it, with inflated chambers and heating pads on the torso." class="rm-shortcode" data-rm-shortcode-id="7801131fa70071a2419c33ceeff8076d" data-rm-shortcode-name="rebelmouse-image" id="77637" loading="lazy" src="https://spectrum.ieee.org/media-library/a-humanoid-robot-torso-with-exposed-robotic-arms-a-monitor-for-a-head-with-a-smiling-face-on-it-with-inflated-chambers-and-hea.jpg?id=29615020&width=980"/>
</p><p>When HuggieBot detects a user in its personal space, it opens its arms and invites the user in for a hug. Based on the height and size of the user, the robot does its best to appropriately place its arms, even making sure that its wrist joints are oriented to keep the end effector contact as flat as possible. The robot, being a robot, will hug you until you’re all hugged out, but releasing your embrace or starting to back away will signal HuggieBot that you’re done and it’ll let you go, presumably with some reluctance. But if you want another hug, go for it, because no two hugs from the robot will ever be identical.</p><p>“Hugging HuggieBot 3.0 is (in my humble and unbiased opinion) really enjoyable,” author Alexis E. Block tells <em>IEEE Spectrum</em>. “We are not trying to fool anyone by saying that it feels like hugging a person, because it does not. You’re hugging a robot, but that doesn’t mean that it can’t be enjoyable.”</p><p>Part of making hugs enjoyable for humans involves the use of intrahug gestures, the development and test of which is one of the major contributions of the new paper. Intrahug gestures are the things you do with your arms and hands midhug, and while you may not always be consciously aware that you’re doing them, they could include things like gentle rubbing, pats, or squeezes.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A graphic showing hold (staying still), rub (moving vertically), pat (tapping on back), and squeeze (tightening hold) motions that are analogous to these motions performed by a human" class="rm-shortcode" data-rm-shortcode-id="3d213034bc1ccd1540fe1579c0da6e49" data-rm-shortcode-name="rebelmouse-image" id="8cad7" loading="lazy" src="https://spectrum.ieee.org/media-library/a-graphic-showing-hold-staying-still-rub-moving-vertically-pat-tapping-on-back-and-squeeze-tightening-hold-motions-th.jpg?id=29615021&width=980"/>
</p><p>The hug “background” gesture is a hold, but (and you should absolutely try this at home), just doing an extended static hold-type hug will definitely make a hug feel kind of robotic. Human hugs involve extra gestures, and HuggieBot is now equipped for this. It’s able to classify the gestures that the human makes and respond with gestures of its own, although (to avoid being too robotic) those gestures aren’t always directly reciprocal, and sometimes the robot will initiate them independently. While the current version of HuggieBot can only rub, pat, or squeeze, future versions may also be able to perform other intrahug gestures, like leaning, or even tickling, if you’re into that.</p><p>Here are all 11 the commandments that HuggieBot 3.0 follows:</p><ol><li>A hugging robot shall be soft.</li><li>A hugging robot shall be warm.</li><li>A hugging robot shall be sized similar to an adult human.</li><li>When a hugging robot is the one initiating the interaction, it shall autonomously invite the user for a hug when it detects someone in its personal space. A hugging robot should wait for the user to begin walking toward it before closing its arms to ensure a consensual and synchronous hugging experience. </li><li>A hugging robot shall autonomously adapt its embrace to the size and position of the user’s body, rather than hug in a constant manner</li><li>A hugging robot shall reliably detect and react to a user’s desire to be released from a hug regardless of his or her arm positions.</li><li>A good hugging robot shall perceive the user’s height and adapt its arm positions accordingly to comfortably fit around the user at appropriate body locations.</li><li>It is advantageous for a hugging robot to accurately detect and classify gestures applied to its torso in real time, regardless of the user’s hand placement.</li><li>Users like a robot that responds quickly to their intrahug gestures.</li><li>To avoid appearing too robotic and to help conceal inevitable errors in gesture perception, a hugging robot shall not attempt perfect reciprocation of intrahug gestures. Rather, the robot should adopt a gesture response paradigm that blends user preferences with slight variety and spontaneity.</li><li>To evoke user feelings that the robot is alive and caring, a hugging robot shall occasionally provide unprompted, proactive affective social touch to the user through intrahug gestures.</li></ol><p>The researchers tested out HuggieBot 3.0 with actual human volunteers who seemed perfectly okay being partially crushed by an experimental hugging robot. Some of them couldn’t seem to get enough, in fact:</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="d24664e39b2a2edfc8779dd131302c35" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/nhq2Ps2wsUs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>The study participants were generally able to successfully detect and classify the majority of HuggieBot’s intrahug gestures. People appreciated the gestures, too, commenting that they helped the robot feel more alive, more social, and more realistic. Squeezes were particularly popular, with participants commenting that it felt “closest to a real human hug” and gave them “a sense of security and comfort.” Interestingly, some people characterized the hugs that they received from the robot in very specific, anthropomorphic ways, often attributing emotions, mood swings, and attitudes to the robot depending on their perception of the hug. Hugs were described as “a comforting hug from a mother,” “a distant relative at a funeral,” “receiving a pity hug from someone who doesn’t want to,” and even “hugging a lover.” Overall, the majority of the study participants felt that the experience was pretty great, and 40 percent of them said that they came to think of HuggieBot as their friend. </p><p>It’s important to note that this is not an “in-the-wild” study of HuggieBot. It took place in a laboratory environment, and all of the participants specifically signed up to be part of a robot hugging study, for which they were compensated. “We look forward to conducting a thorough in-the-wild study to see how many everyday people would and would not be interested in hugging a robot,” say the researchers. Ideally, such a study would take place over weeks and months, to help determine how much of HuggieBot’s appeal is simply due to novelty—always a potential problem for new robots.</p><p> The other thing to keep in mind is that, as the researchers point out, HuggieBot does not in any way understand what hugs mean:</p><blockquote>We acknowledge that the current version of our robot does not deliver on the full aspirational goal of a hugging robot. Rather, HuggieBot simulates a hug in a reasonably compelling way, and our data suggest that users enjoy the hug and can engage with the robot and relate to it as an autonomous being. However, in its current state, HuggieBot does not have an internal emotional model similar to humans, and thus it is not capable of engaging in the embodied emotional experience of a hug.</blockquote><p>Fortunately, hugs are both an emotional thing <em>and</em> a physical thing, and even an emotionless robot can use physical contact to potentially have a tangible, measurable impact on the emotional states of humans—something that the researchers do in fact hope to measure in a quantitative way.</p><p class="pull-quote">We certainly are not trying to replace human hugs but to provide a supplement when it might be difficult or impossible to receive a hug from another person. —Alexis Block</p><p>We’ll talk about their future work in just a minute, but first, <em>IEEE Spectrum</em> spoke with first author <a href="https://is.mpg.de/~alexis" target="_blank">Alexis Block</a> (who <a href="https://is.mpg.de/en/news/otto-hahn-medal-awarded-to-alexis-e-block" target="_blank">just received the Max Planck Society’s Otto Hahn Medal for her work on HuggieBot</a>) for more details on this new generation robot hugs:</p><p><strong><a href="https://spectrum.ieee.org/the-importance-of-teaching-robots-to-hug" target="_blank">We asked you in 2018 why teaching robots to hug is important</a>. Three years on, what do you now think the importance of this research is?</strong></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A woman with long hair smiles while resting an arm on a sweatshirt-clad robot." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="0d5d371010d24464fe7a0086dd20ce00" data-rm-shortcode-name="rebelmouse-image" id="4c772" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-with-long-hair-smiles-while-resting-an-arm-on-a-sweatshirt-clad-robot.jpg?id=29615053&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Alexis Block with her hugging robot</small></p><p><strong>Alexis Block: </strong>I think the COVID-19 pandemic has made the importance of this research more salient than ever. While we could never have foreseen a situation like we are currently experiencing, in 2018 we were researching robot hugs so we could one day provide the emotional support and health benefits of human hugs to people wherever or whenever they needed it. At the time, my advisor Katherine Kuchenbecker and I were primarily thinking of friends and family separated by a physical distance, like how we were separated from our families (living in Germany and our families living in the United States). Before the pandemic, we hugged, high-fived, and otherwise socially contacted our friends and family without a second thought. Now, many of us have realized that social distancing and the resulting lack of physical contact can harm our overall well-being. Even after we return to “normal,” some members of our society may be medically more vulnerable and will not be able to join us. We believe HuggieBot could be used as a tool to supplement, not replace, human hugs for situations when it is difficult or uncomfortable to get the support you need or want from another person. </p><p><strong>Can you share some qualitative feedback from study participants? </strong></p><p><strong>Block: </strong>In our validation study with HuggieBot 3.0, the average user hug duration was about 25 seconds long. For comparison, the average hug between humans is 2 to 3 seconds. To receive the positive benefits of deep pressure touch, researchers have found <a href="https://pubmed.ncbi.nlm.nih.gov/15206831/" rel="noopener noreferrer" target="_blank">20 seconds of constant hugging between romantic partners is necessary</a>, and our users, on average, hugged our robot for even longer. We made it clear to our users that they were free to hug the robot for as long or as short a duration as they liked. Compare hugging a stranger or acquaintance (2 to 3 seconds) to hugging a partner, a friend, or a family member (20 seconds). </p><p>On average, based on the duration of how long our users felt comfortable hugging our robot, we think users treated HuggieBot 3.0 more like a partner, friend, or family member than a stranger or acquaintance. That was impressive because they had never met HuggieBot before! In their free-response questionnaires, several of our users mentioned that they thought the robot was their “friend” by the end of the experiment. We believe these results speak to the quality of the embrace users felt during the embrace; they truly felt like they were hugging a friend, which was especially meaningful because we conducted this study during the pandemic.</p><p><strong>Many participants seemed very happy while hugging the robot! But did you have anyone react negatively? </strong></p><p><strong>Block: </strong>While most users had positive things to say about the robot and their experience during the study, two users still mentioned that while they enjoyed the interaction, they didn’t understand the purpose of a hugging robot because they felt that human hugs are “irreplaceable.” We certainly are not trying to replace human hugs but to provide a supplement when it might be difficult or impossible to receive a hug from another person.</p><p><strong>Are there any ways in which robot hugs are potentially superior to human hugs?</strong></p><p><strong>Block: </strong>The main way robot hugs are potentially superior to human hugs is due to the lack of social pressure. When you’re hugging HuggieBot, you know you’re hugging a robot and not another person, and that’s part of its beauty. You don’t have to worry about being judged for needing to be held “too long” to “too tight.” Instead, the robot is there to support you and your needs. Many users commented that they feel more comfortable hugging the robot than other people because they don’t have to worry about the timing or judgment aspect involved with hugging another person. </p><p><strong>You are likely the world’s foremost expert on robot hugs—what have you learned over the past several years that was most surprising to you?</strong></p><p><strong>Block: </strong>One surprising result was that when investigating how the robot should respond to users’ intrahug gestures, we initially thought the robot should mimic the gestures it felt. But, interestingly, users expressed that they wanted a variety of gestures in response to theirs instead of one-to-one reciprocation. Furthermore, they explained that it felt superficial and mechanical when the robot parroted back their gestures. However, when the robot responded with a different gesture of a similar “emotional investment level,” they mentioned feeling like the robot “understands [them] and makes his own decision.” </p><p>We also were unsure how users would respond to proactive robotic intrahug gestures, which is when the robot squeezes, pats, or rubs a user who is holding still within the hug. We worried, particularly with the squeeze, that the users would be alarmed by the unprompted motion and think that the robot was malfunctioning. In this instance, we were pleasantly surprised to find that users really enjoyed proactive robotic intrahug gestures, mentioning that they felt the robot was comforting them rather than responding to their inputs. Furthermore, they attributed emotions and feelings to the robot, saying they felt the robot cared about them when it chose to perform its own gesture.</p><p class="pull-quote">Ultimately, if we can help even just a few people be a little happier by giving them a way to hug friends and family they thought they wouldn’t be able to, I think that would be an incredible outcome. —Alexis Block</p><p><strong>How do you hope that your research will be applied in useful ways in the future?</strong></p><p><strong>Block: </strong>Back in 2016, when Katherine and I started this work as my master’s thesis at the University of Pennsylvania, we were inspired because our families lived far away, and we missed them. Especially given the COVID-19 pandemic and the resulting isolation, I think many people now understand first-hand the significant effect social touch with friends and loved ones has on our mental health. I hope that in the future, this research can be used to help strengthen personal relationships separated by a physical distance. Ultimately, if we can help even just a few people be a little happier by giving them a way to hug friends and family they thought they wouldn’t be able to, I think that would be an incredible outcome.</p><div class="horizontal-rule"></div><p>Block is already testing an upgraded version of HuggieBot: HuggieBot 4.0 is the best hugging robot yet, featuring improved hug positioning and better prehug technique, among other upgrades. “With these improvements to HuggieBot, we finally felt we had a version of a hugging robot that was of high enough quality to compare to hugging another person,” says Block. This comparison will be physiological, measuring whether and to what extent hugging a robot may elicit physical responses that are similar to hugging a real human. The researchers plan to “induce stress upon voluntary participants” (!) and then provide either an active human hug, a passive human hug, an active robot hug, or a passive robot hug and use periodic saliva measurements to measure cortisol and oxytocin levels. Hopefully, the results will show that humans can derive real benefits from robot hugs, and that when human hugs are not an option, we can look for a soft, warm robotics embrace instead.</p>]]></description><pubDate>Thu, 31 Mar 2022 18:42:46 +0000</pubDate><guid>https://spectrum.ieee.org/robot-hugs</guid><category>Hri</category><category>Robotics</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-smiling-woman-hugs-a-robotic-torso-with-a-soft-grey-sweatshirt-the-robot-s-head-has-a-purple-computer-screen-with-a-smiling-f.jpg?id=29615031&amp;width=980"></media:content></item><item><title>What Texas Instruments’ Little Professor Can Teach Us</title><link>https://spectrum.ieee.org/texas-instruments-calculator</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-little-professor-calculator.jpg?id=29605180&width=1200&height=800&coordinates=0%2C125%2C0%2C125"/><br/><br/><p><strong>In the early</strong> 1990s, the literary critic Stephen Greenblatt wrote about the tension between <a href="http://stephengreenblatt.com/sites/default/files/Karp-Levine.pdf" target="_blank">resonance and wonder</a> in museums. The wondrous objects, he wrote, are unique and instantly amazing—<a href="https://airandspace.si.edu/stories/editorial/mars-rover-lands-virginia" target="_blank">a model of a Mars rover</a>, say, or <a href="https://www.thehenryford.org/collections-and-research/digital-collections/artifact/225212/" target="_blank">a glass test tube holding Edison’s last breath</a>. These are the objects that stop visitors in their tracks and proclaim: “I belong in a museum!” </p><p>Objects that evoke resonance, on the other hand, are more relatable and more familiar. They reach out from their exhibit case to spark conversations with visitors: “I remember Grandma having a <a href="https://americanhistory.si.edu/collections/object-groups/slide-rules" target="_blank">slide rule</a> like that.” </p><hr/><p>Which brings me to the object at top. I encountered this funny-looking calculator during a recent visit to the <a href="https://www.whipplemuseum.cam.ac.uk/" target="_blank">Whipple Museum of the History of Science</a>, at the University of Cambridge. Many years ago, my 7-year-old sister had a calculator just like this, and I desperately wanted one, too. There is no stronger resonance than that of your inner jealous 5-year-old seeing the calculator that your older sister wouldn’t let you play with. </p><p>This Texas Instruments calculator, called the Little Professor, debuted in 1976 and proved an instant hit. Over the next decade, parents bought millions of the jaunty yellow devices to help their children learn basic arithmetic. The backward-functioning calculator displays a simple equation—such as “9 – 3 =”—and the user then has to punch in the answer. Get it wrong and the LED display flashes “EEE.” After a set of 10 questions, the calculator displays the user’s score. A 9-volt battery powered the Texas Instrument <a href="http://www.vintagecalculators.com/html/little_professor.html" target="_blank">TMS0975NL</a> integrated circuit and the eight-digit LED display. In the 1980s, the LED display was replaced with an LCD. (Texas Instruments still makes a version of the Little Professor, only now it’s sustainably <a href="https://education.ti.com/en/products/calculators/elementary-calculators/little-professor-solar" target="_blank">powered with solar cells</a>.)  </p><p>The iconic calculator features the face of a mustachioed man wearing square glasses and reading a book. The top of the device suggests a mortarboard, a nod to the professor’s status, with the “tassel” being a wrist strap to hold the device. The calculator initially sold for US $17, which dropped to $13 by mid-1977. </p><p>This Texas Instruments ad from 1978 presents the calculator as a gateway device for young learners to explore “the wonder of numbers”:</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="9acbae9aeb080f202c7fe4677efa0356" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TlOIXK1b5kw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Texas Instruments - "Little Professor" (Commercial, 1978)</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=TlOIXK1b5kw" target="_blank">www.youtube.com</a>
</small>
</p><p>The Little Professor came with a <a href="http://www.datamath.net/Manuals/LittleProf76_US.pdf" target="_blank">workbook of games</a> like Jackpot and Loco Motion that coaxed users into solving math problems, either on their own or with friends. Math as a fun group activity? Why not. </p><h2>How the Little Professor Found a Home at the Whipple Museum </h2><p>The Little Professor came to the Whipple in 1987 as part of the Francis Hookham Collection of Handheld Calculators, which includes more than 400 electronic calculators from the 1970s and ’80s. The donation caught the attention of a columnist at <em>The Daily Telegraph </em>named Boris Johnson. (Yes, <a href="https://www.telegraph.co.uk/boris-johnson-archive/" target="_blank"><em>that</em> Boris Johnson</a>.) He wrote a disparaging article about the museum’s acquisition of a “peerless collection of obsolete gadgets.” </p><p>What the article really shows is that Johnson had no idea how museums work. At the very least, the Whipple’s Little Professor isn’t obsolete because it’s still in working condition. In the hands of a child, it could still teach arithmetic tables. </p><p>And the Little Professor may be a child’s calculator, but it’s one that a number of museums have deemed worthy of their collections. The Whipple currently displays its <a href="https://collections.whipplemuseum.cam.ac.uk/objects/10874/" target="_blank">Little Professor</a> along with other models from the Hookham Collection to show the evolution of electronic calculators. Back in 1980 the Science Museum in London exhibited its <a href="https://collection.sciencemuseumgroup.org.uk/objects/co8000285/little-professor-electronic-calculator-1980-electronic-calculator-educational-toy" target="_blank">Little Professor</a> as part of the <a href="http://tcm.computerhistory.org/CHMfiles/The%20Challenge%20of%20the%20Chip,%20London%20Science%20Museum%201980.pdf" target="_blank">Challenge of the Chip</a> exhibition, which encouraged visitors to consider how the tiny microchip was going to change their lives. The Smithsonian’s <a href="https://collections.si.edu/search/detail/edanmdm:nmah_334611" target="_blank">Little Professor</a> makes an appearance in <a href="https://www.press.jhu.edu/books/title/9570/tools-american-mathematics-teaching-1800-2000" target="_blank"><em>Tools of American Mathematics Teaching, 1800–2000</em></a>, the 2008 book by Peggy Aldrich Kidwell, Amy Ackerberg-Hastings, and David Lindsay. Personally, I would like to see the Little Professor in an exhibit about designing for children. </p><p>But Boris Johnson seemed to believe that museums should collect only the wondrous. He favored the <a href="https://www.museothyssen.org/en/collection/carmen-thyssen" target="_blank">Carmen Thyssen Collection</a> in Madrid and the <a href="https://burrellcollection.com/" target="_blank">Burrell Collection</a> in Glasgow, both replete with high art and old masters. There was no space in Johnson’s museum world for the merely resonant. </p><p>The fact is that museums need both wonder and resonance. Resonant objects make museums more approachable and more accessible to the average visitor. You realize that you don’t have to be a Newton or an Einstein or a billionaire to have owned an object that’s now in a museum’s collection.  </p><p>While working on this month’s column, I chatted with several friends and colleagues about the Little Professor. Every one of them remarked with glee, “I had one of those!” That’s excellent design—to create a connection that endures decades later. Seeing a museum object that speaks to your own life and experience helps you realize that everyone has an origin story—and it might just include a goofy-looking calculator toy. </p><p><em>Part of a</em><a href="https://spectrum.ieee.org/tag/Past+Forward" target="_self"><em> continuing series</em></a> <em>looking at photographs of historical artifacts that embrace the boundless potential of technology.</em> </p><p><em>An abridged version of this article appears in the April 2022 print issue as </em><em>“</em><em>Lessons From the Little Professor</em><em>.”</em></p>]]></description><pubDate>Thu, 31 Mar 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/texas-instruments-calculator</guid><category>Past forward</category><category>Texas instruments</category><category>Calculators</category><category>Consumer electronics</category><category>Museums</category><category>History</category><category>Type:departments</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-little-professor-calculator.jpg?id=29605180&amp;width=980"></media:content></item><item><title>The Vacuum Tube’s Forgotten Rival</title><link>https://spectrum.ieee.org/the-vacuum-tubes-forgotten-rival</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-an-older-woman-in-front-of-an-early-generation-computer.jpg?id=29534788&width=1200&height=800&coordinates=0%2C0%2C84%2C0"/><br/><br/><p>
<strong>During the Second </strong>World War, the German military developed what were at the time some very sophisticated technologies, including the 
	<a href="https://en.wikipedia.org/wiki/V-2_rocket" target="_blank">V-2 rockets</a> it used to rain destruction on London. Yet the V-2, along with much other German military hardware, depended on an obscure and seemingly antiquated component you’ve probably never heard of, something called the magnetic amplifier or mag amp.
</p><p>
	In the United States, mag amps had long been considered obsolete—“too slow, cumbersome, and inefficient to be taken seriously,” according to one source. So U.S. military-electronics experts of that era were baffled by the extensive German use of this device, which they first learned about from interrogating German prisoners of war. What did the Third Reich’s engineers know that had eluded the Americans?
</p><hr/><p>
	After the war, U.S. intelligence officers scoured Germany for useful scientific and technical information. Four hundred experts sifted through billions of pages of documents and shipped 3.5 million microfilmed pages back to the United States, along with almost 200 tonnes of German industrial equipment. Among this mass of information and equipment was the secret of Germany’s magnetic amplifiers: metal alloys that made these devices compact, efficient, and reliable.
</p><p>
	U.S. engineers were soon able to reproduce those alloys. As a result, the 1950s and ’60s saw a renaissance for magnetic amplifiers, during which they were used extensively in the military, aerospace, and other industries. They even appeared in some early solid-state digital computers before giving way entirely to transistors. Nowadays, that history is all but forgotten. So here I’ll offer the little-known story of the mag amp.
</p><p>
	An amplifier, by 
	<span style="background-color: initial;">definition, is a device that allows a small signal to control a larger one. An old-fashioned </span><a href="https://en.wikipedia.org/wiki/Triode" rel="noopener noreferrer" target="_blank">triode</a> vacuum tube does that using a voltage applied to its grid electrode. A modern <a href="https://en.wikipedia.org/wiki/Field-effect_transistor" rel="noopener noreferrer" target="_blank">field-effect transistor</a> does it using a voltage applied to its gate. The mag amp exercises control electromagnetically.
</p><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A photo of a rocket on a launcher with trees in the background. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1ed9632956d30a79a7b69ee12ff8b394" data-rm-shortcode-name="rebelmouse-image" id="aa0d5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-of-a-rocket-on-a-launcher-with-trees-in-the-background.jpg?id=29583518&width=980" style="max-width: 100%"/>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A photo of a man sitting at an early computer next to a typewriter." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="6bd9ace981c115f2a72aa816d082e1d8" data-rm-shortcode-name="rebelmouse-image" id="cd239" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-of-a-man-sitting-at-an-early-computer-next-to-a-typewriter.jpg?id=29583522&width=980" style="max-width: 100%"/>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A photo of two men sitting at a terminal in front of an early computer." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="41a4d01ff0ce2791a10a83f65a75009c" data-rm-shortcode-name="rebelmouse-image" id="81829" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-of-two-men-sitting-at-a-terminal-in-front-of-an-early-computer.png?id=29583533&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Magnetic amplifiers were used for a variety of applications, including in the infamous V-2 rockets [top] that the Germany military employed during the Second World War and in the Magstec computer [middle], completed in 1956. The British Elliot 803 computer of 1961 [bottom] used related core-transistor logic.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">From top: Fox photos/Getty Images; Remington Rand Univac; Smith Archive/Alamy</small>
</p>
</div><p>
	To understand how it works, first consider a simple inductor, say, a wire coiled around an iron rod. Such an inductor will tend to block the flow of alternating current through the wire. That’s because when current flows, the coil creates an alternating magnetic field, concentrated in the iron rod. And that varying magnetic field induces voltages in the wire that act to oppose the alternating current that created the field in the first place.
</p><p>
	If such an inductor carries a lot of current, the rod can reach a state called saturation, whereby the iron cannot become any more magnetized than it already is. When that happens, current passes through the coil virtually unimpeded. Saturation is usually undesirable, but the mag amp exploits this effect.
</p><p>
	Physically, a magnetic amplifier is built around a metallic core of material that can easily be saturated, typically a ring or square loop with a wire wrapped around it. A second wire also wrapped around the core forms a control winding. The control winding includes many turns of wire, so by passing a relatively small direct current through it, the iron core can be forced into or out of saturation.
</p><p>
	The mag amp thus behaves like a switch: When saturated, it lets the AC current in its main winding pass unimpeded; when unsaturated, it blocks that current. Amplification occurs because a relatively small DC control current can modify a much larger AC load current.
</p><p>
	The history of magnetic amplifiers starts in the United States with some patents filed in 1901. By 1916, large magnetic amplifiers were being used for transatlantic radio telephony, carried out with an invention called an 
	<a href="https://en.wikipedia.org/wiki/Alexanderson_alternator" target="_blank">Alexanderson alternator</a>, which produced a high-power, high-frequency alternating current for the radio transmitter. A magnetic amplifier <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1645957" target="_blank">modulated the output of the transmitter</a> according to the strength of the voice signal to be transmitted.
</p><p class="pull-quote">
	One Navy training manual of 1951 explained magnetic amplifiers in detail—although with a defensive attitude about their history.
</p><p>
	In the 1920s, improvements in vacuum tubes made this combination of Alexanderson alternator and magnetic amplifier obsolete. This left the magnetic amplifier to play only minor roles, such as for light dimmers in theaters.
</p><p>
	Germany’s later successes with magnetic amplifiers hinged largely on the development of advanced magnetic alloys. A magnetic amplifier built from these materials switched sharply between the on and off states, providing greater control and efficiency. These materials were, however, exquisitely sensitive to impurities, variations in crystal size and orientation, and even mechanical stress. So they required an exacting manufacturing process.
</p><p>
	The best-performing German material, developed in 1943, was called Permenorm 5000-Z. It was an extremely pure fifty/fifty nickel-iron alloy, melted under a partial vacuum. The metal was then cold-rolled as thin as paper and wound around a nonmagnetic form. The result resembled a roll of tape, with thin Permenorm metal making up the tape. After winding, the module was annealed in hydrogen at 1,100 °C for 2 hours and then rapidly cooled. This process oriented the metal crystals so that they behaved like one large crystal with uniform properties. Only after this was done were wires wrapped around the core.
</p><p>
	By 1948, scientists at the U.S. 
	<a href="https://en.wikipedia.org/wiki/Naval_Ordnance_Laboratory" target="_blank">Naval Ordnance Laboratory</a>, in Maryland, had figured out how to manufacture this alloy, which was soon marketed by <a href="https://dome.mit.edu/bitstream/handle/1721.3/38909/MC665_r04_E-422.pdf?sequence=1" target="_blank">an outfit called Arnold Engineering</a> Co. under the name Deltamax. The arrival of this magnetic material in the United States led to renewed enthusiasm for magnetic amplifiers, which tolerated extreme conditions and didn’t burn out like vacuum tubes. Mag amps thus found many applications in demanding environments, especially military, space, and industrial control.
</p><p>
	During the 1950s, the U.S. military was using magnetic amplifiers in automatic pilots, fire-control apparatus, servo systems, radar and sonar equipment, the 
	<a href="https://en.wikipedia.org/wiki/RIM-2_Terrier" target="_blank">RIM-2 Terrier surface-to-air missile</a>, and many other roles. One <a href="https://www.google.com/books/edition/Magnetic_Amplifiers/d74EGQAACAAJ?hl=en" target="_blank">Navy training manual</a> of 1951 explained magnetic amplifiers in detail—although with a defensive attitude about their history: “Many engineers are under the impression that the Germans invented the magnetic amplifier; actually it is an American invention. The Germans simply took our comparatively crude device, improved the efficiency and response time, reduced weight and bulk, broadened its field of application, and handed it back to us.”
</p><p>
	The U.S. space program also made extensive use of magnetic amplifiers because of their reliability. For example, the 
	<a href="https://www.thespacereview.com/article/1836/1" target="_blank">Redstone rocket</a>, which launched Alan Shepard into space in 1961, used magnetic amplifiers. In the Apollo missions to the moon during the 1960s and ’70s, magnetic amplifiers controlled power supplies and fan blowers. Satellites of that era used magnetic amplifiers for signal conditioning, for current sensing and limiting, and for telemetry. Even the space shuttle used magnetic amplifiers to dim its fluorescent lights.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="The image shows a Redstone rocket at the launch pad, with three space-suit-wearing astronauts in the foreground." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="3acc7ce4aa3c310b496b4708af8b346a" data-rm-shortcode-name="rebelmouse-image" id="30a85" loading="lazy" src="https://spectrum.ieee.org/media-library/the-image-shows-a-redstone-rocket-at-the-launch-pad-with-three-space-suit-wearing-astronauts-in-the-foreground.png?id=29535245&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Magnetic amplifiers were also used in Redstone rockets, like the one shown here behind astronauts John Glenn, Virgil Grissom, and Alan Shepard.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Universal Images Group/Getty Images</small>
</p><p>
	Magnetic amplifiers also found heavy use in industrial control and automation, with many products containing them being marketed under such brand names as General Electric’s 
	<a href="https://ieeexplore.ieee.org/document/6444917" target="_blank">Amplistat</a>, CGS Laboratories’ <a href="https://www.steampoweredradio.com/jim%20garrett/manufacturers%20data/trak%20electronics/Trak%20Electronics%20Increductor%20Notes%20Number%201.pdf" target="_blank">Increductor</a>, Westinghouse’s <a href="https://worldradiohistory.com/Archive-Company-Publications/Westinghouse-Engineer/Westinghouse-Engineer-1955-07.pdf" target="_blank">Cypak</a> (cybernetic package), and <a href="https://en.wikipedia.org/wiki/Librascope" target="_blank">Librascope</a>’s Unidec (universal decision element).
</p><p>
<strong>The magnetic materials </strong>developed in Germany during the Second World War had their largest postwar impact of all, though, on the computer industry. In the late 1940s, researchers immediately recognized the ability of the new magnetic materials to store data. A circular magnetic core could be magnetized counterclockwise or clockwise, storing a 0 or a 1. Having what’s known as <a href="https://www.sciencedirect.com/science/article/pii/S1474667017702741" target="_blank">a rectangular hysteresis</a> loop ensured that the material would stay solidly magnetized in one of these states after power was removed.
</p><p>
	Researchers soon constructed what was called core memory from dense grids of magnetic cores. And these technologists soon switched from using wound-metal cores to cores made from ferrite, a ceramic material containing iron oxide. By the mid-1960s, ferrite cores were stamped out by the billions as manufacturing costs dropped to a fraction of a cent per core.
</p><p>
	But core memory is not the only place where magnetic materials had an influence on early digital computers. The first generation of those machines, starting in the 1940s, computed using vacuum tubes. These were replaced in the late 1950s with a second generation based on transistors, followed by third-generation computers built from integrated circuits.
</p><p class="pull-quote">
	Transistors weren’t an obvious winner for early computers, and many other alternatives were developed, including magnetic amplifiers.
</p><p>
	But technological progress in computing wasn’t, in fact, this linear. Early transistors weren’t an obvious winner, and many other alternatives were developed. Magnetic amplifiers were one of several largely forgotten computing technologies that fell between the generations.
</p><p>
	That’s because researchers in the early 1950s realized that magnetic cores could not only hold data but also perform logic functions. By putting multiple windings around a core, inputs could be combined. A winding in the opposite direction could inhibit other inputs, for example. Complex logic circuits could be implemented by connecting such cores together in various arrangements.
</p><div class="ieee-sidebar-medium">
<h3>How Magnetic Amplifiers Amplify</h3>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img class="rm-shortcode" data-rm-shortcode-id="1857e348d612af7fb38b7d0ff9d762e1" data-rm-shortcode-name="rebelmouse-image" id="b8e51" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=29585172&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The magnetic amplifier exploits the fact that the presence of magnetizable material [tan] in the core of an induction coil increases its impedance. Reducing the influence of that magnetic material by physically withdrawing it from a coil would reduce its impedance, allowing more power to flow to an AC load. </small>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img class="rm-shortcode" data-rm-shortcode-id="5985de901f75129578459c8e0f55b863" data-rm-shortcode-name="rebelmouse-image" id="4300c" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=29583388&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The influence of a magnetizable material, here taking the form of a toroidal core [tan], can be changed by applying a DC bias using a second coil [left side of toroid]. Applying a DC bias current sufficient to force the material into a condition called saturation—a state in which it cannot become more magnetized—is functionally equivalent to removing the material from the coil, which allows more power to flow to the AC load.</small>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="This sidebar contains three diagrams of increasing complexity showing how a magnetic amplifier works. " class="rm-shortcode" data-rm-shortcode-id="2a07c3d4c6e405e4d8da473b3a390c23" data-rm-shortcode-name="rebelmouse-image" id="33aa7" loading="lazy" src="https://spectrum.ieee.org/media-library/this-sidebar-contains-three-diagrams-of-increasing-complexity-showing-how-a-magnetic-amplifier-works.jpg?id=29535276&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A more realistic circuit would include two counter-wound AC coils, to avoid inducing currents in the control winding. It would also include diodes, shown here in a bridge configuration, allowing the circuit to control a DC load. Feedback coils [not shown] can be used to increase amplification.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">David Schneider</small>
</p>
</div><p>
	In 1956, the 
	<a href="https://en.wikipedia.org/wiki/Sperry_Corporation" target="_blank">Sperry Rand Co.</a> developed a high-speed magnetic amplifier called the <a href="https://books.google.com/books?id=P10pDwAAQBAJ&pg=PA151&lpg=PA151&dq=Ferractor&source=bl&ots=LoGtBxTlzf&sig=ACfU3U3McMb0ZgGopfCMWQ6c1qvICjYoAQ&hl=en&sa=X&ved=2ahUKEwiwmp-SvKr1AhWwrHIEHWWrBBwQ6AF6BAgMEAM#v=onepage&q=Ferractor&f=false" target="_blank">Ferractor</a>, capable of operating at several megahertz. Each Ferractor was built by winding a dozen wraps of one-eighth-mil (about 3 micrometers) Permalloy tape around a 0.1-inch (2.5-mm) nonmagnetic stainless-steel bobbin.
</p><p>
	The Ferractor’s performance was due to the remarkable thinness of this tape in combination with the tiny dimensions of the bobbin. Sperry Rand used the Ferractor in a military computer called the Univac Magnetic Computer, also known as the Air Force Cambridge Research Center (AFCRC) computer. This machine contained 1,500 Ferractors and 9,000 germanium diodes, as well as a few transistors and vacuum tubes.
</p><p>
	Sperry Rand later created business computers based on the AFCRC computer: the 
	<a href="https://en.wikipedia.org/wiki/UNIVAC_Solid_State" target="_blank">Univac Solid State</a> (known in Europe as the Univac Calculating Tabulator) followed by the less expensive STEP (Simple Transition Electronic Processing) computer. Although the Univac Solid State didn't completely live up to its name—its processor used 20 vacuum tubes—it was moderately popular, with hundreds sold.
</p><p>
	Another division of Sperry Rand built a computer called 
	<a href="http://vipclubmn.org/Articles/SperryMilitaryComputersbyGG.pdf" target="_blank">Bogart</a> to help with codebreaking at the U.S. National Security Agency. Fans of <em>Casablanca</em> and <em>Key Largo</em> will be disappointed to learn that this computer was named after the well-known <em>New York Sun</em> editor John Bogart. This relatively small computer earned that name because it edited cryptographic data before it was processed by the NSA’s larger computers.
</p><p>
	Five Bogart computers were delivered to the NSA between 1957 and 1959. They employed a novel magnetic-amplifier circuit designed by 
	<a href="https://en.wikipedia.org/wiki/Seymour_Cray" target="_blank">Seymour Cray</a>, who later created the famous Cray supercomputers. Reportedly, out of his dozens of patents, Cray was <a href="https://doomedengineers.wordpress.com/2016/01/19/seymour-cray-1925-1996-supercomputers/" target="_blank">most proud of his magnetic-amplifier design</a>.
</p><p>
	Computers based on magnetic amplifiers didn’t always work out so well, though. For example, in the early 1950s, Swedish billionaire industrialist 
	<a href="https://en.wikipedia.org/wiki/Axel_Wenner-Gren" target="_blank">Axel Wenner-Gren</a> created a line of vacuum-tube computers, called the ALWAC (Axel L. Wenner-Gren Automatic Computer). In 1956, he told the U.S. Federal Reserve Board that he could deliver a magnetic-amplifier version, the ALWAC 800, in 15 months. After the Federal Reserve Board paid US $231,800, development of the computer ran into engineering difficulties, and the project ended in total failure.
</p><p>
	Advances in transistors during the 1950s led, of course, to the decline of computers using magnetic amplifiers. But for a time, it wasn’t clear which technology was superior. In the mid-1950s, for example, Sperry Rand was debating between magnetic amplifiers and transistors for the 
	<a href="https://en.wikipedia.org/wiki/ATHENA_computer" target="_blank">Athena</a>, a 24-bit computer to control the <a href="https://en.wikipedia.org/wiki/LGM-25C_Titan_II" target="_blank">Titan nuclear missile</a>. Cray built two equivalent computers to compare the technologies head-to-head: the Magstec (magnetic switch test computer) used magnetic amplifiers, while the Transtec (transistor test computer) used transistors. Although the Magstec performed slightly better, it was becoming clear that transistors were the wave of the future. So Sperry Rand built the Univac Athena computer from transistors, relegating mag amps to minor functions inside the computer’s power supply.
</p><p>
<strong>In Europe, too</strong>, <span style="background-color: initial;">the transistor was battling it out with the magnetic amplifier. For example, engineers at </span><a href="https://en.wikipedia.org/wiki/Ferranti" target="_blank">Ferranti</a><span style="background-color: initial;">, in the United Kingdom, developed magnetic</span><span style="background-color: initial;">-</span><span style="background-color: initial;">amplifier circuits for their computers. But they found that transistors provided more reliable amplification, so they replaced the magnetic amplifier with a transformer in conjunction with a transistor. They called this circuit the </span><a href="https://collection.sciencemuseumgroup.org.uk/objects/co8711551/neuron-compartment-from-ferranti-sirius-computer-mainframe-computer-component" target="_blank">Neuron</a> because it produced an output if the inputs exceeded a threshold, analogous to a biological neuron. The Neuron became the heart of Ferranti’s Sirius and Orion business computers.
</p><p>
	Another example is the Polish EMAL-2 computer of 1958, which used magnetic-core logic along with 100 vacuum tubes. This 34-bit computer was Poland’s first truly productive digital computer. It was compact but slow, performing only 150 or so operations per second.
</p><p>
<span style="background-color: initial;">And in the Soviet Union, the 15-bit </span><a href="https://dl.acm.org/doi/10.1145/368453.368456" target="_blank">LEM-1 computer</a> from 1954 used 3,000 ferrite logical elements (along with 16,000 selenium diodes). It could perform 1,200 additions per second.
</p><p>
	In France, magnetic amplifiers were used in the 
	<a href="https://ieeexplore.ieee.org/document/4637513" target="_blank">CAB 500</a> (Calculatrice Arithmétique Binaire 500), sold in 1960 for scientific and technical use by a company called Société d’Electronique et d’Automatisme (SEA). This 32-bit desk-size computer used a magnetic logic element called the Symmag, along with transistors and a vacuum-tube power supply. As well as being programmed in Fortran, Algol, or SEA’s own language, PAF (Programmation Automatique des Formules), the CAB 500 could be used as a desk calculator.
</p><p>
	Some computers of this era used multiaperture cores with complex shapes to implement logic functions. In 1959, engineers at Bell Laboratories developed a ladder-shaped magnetic element called 
	<a href="https://ieeexplore.ieee.org/document/6773051" target="_blank">the Laddic</a>, which implemented logic functions by sending signals around different “rungs.” This device was later used in some nuclear-reactor safety systems.
</p><p>
	Another approach along these lines was something called the 
	<a href="https://www.sciencedirect.com/science/article/abs/pii/0032063361903002" target="_blank">Biax logic element</a>—a ferrite cube with holes along two axes. Another was dubbed the <a href="https://patents.google.com/patent/US3376427A/en" target="_blank">transfluxor</a>, which had two circular openings. Around 1961, engineers at the Stanford Research Institute built the <a href="https://www.sri.com/hoi/all-magnetic-logic-computer/#:~:text=In%20the%201950s%2C%20SRI%20began,of%20magnetic%20ferrite%20memory%20cores." target="_blank">all-magnetic logic computer</a> for the U.S. Air Force using such multi-aperture magnetic devices. <a href="https://en.wikipedia.org/wiki/Douglas_Engelbart" target="_blank">Doug Engelbart</a>, who famously went on to invent the mouse and much of the modern computer user interface, was a key engineer on this computer.
</p><p>
	Some computers of the time used transistors in combination with magnetic cores. The idea was to minimize the number of then-expensive transistors. This approach, called core transistor logic (CTL), was used in the British 
	<a href="https://en.wikipedia.org/wiki/Elliott_803" target="_blank">Elliott 803</a> computer, a small system introduced in 1959 with an unusual 39-bit word length. The <a href="http://ed-thelen.org/comp-hist/BRL64-b.html" target="_blank">Burroughs D210</a> magnetic computer of 1960, a compact computer of just 35 pounds (about 16 kilograms) designed for aerospace applications, also used core-transistor logic.
</p><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img class="rm-shortcode" data-rm-shortcode-id="aafe761bd727283d36e37f7c1acfaed1" data-rm-shortcode-name="rebelmouse-image" id="e0abe" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=29535184&width=980"/>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="One image shows a portion of a computer board with many tiny rings through which wires pass. The other image is an enlargement, which makes it possible to see that the rings have different orientations." class="rm-shortcode" data-rm-shortcode-id="53ff9458169d3d1b8126d4c43cbeb9ac" data-rm-shortcode-name="rebelmouse-image" id="55f64" loading="lazy" src="https://spectrum.ieee.org/media-library/one-image-shows-a-portion-of-a-computer-board-with-many-tiny-rings-through-which-wires-pass-the-other-image-is-an-enlargement.png?id=29535165&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This board from a 1966 IBM System/360 [top] shows some of the machine’s magnetic-core memory, which made use of small ferrite rings through which wires were strung [bottom].</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Top: Maximilian Schönherr/picture-alliance/dpa/AP; Bottom: Sheila Terry/Rutherford Appleton Laboratory/Science Source</small>
</p>
</div><p>
	Core-transistor logic was particularly popular for space applications. A company called Di/An Controls produced a line of logic circuits and claimed that “most space vehicles are packed with them.” The company’s Pico-Bit was a competing core-transistor-logic product, advertised in 1964 as “Your best bit in space.” Early prototypes of NASA’s 
	<a href="https://en.wikipedia.org/wiki/Apollo_Guidance_Computer" target="_blank">Apollo Guidance Computer</a> were built with core transistor logic, but in 1962 the designers at MIT made a risky switch to integrated circuits.
</p><p>
	Even some “fully transistorized” computers made use of magnetic amplifiers here and there. The MIT 
	<a href="https://www.ll.mit.edu/sites/default/files/page/doc/2018-05/LookingBack_19_1.pdf" target="_blank">TX-2</a> of 1958 used them to control its tape-drive motors, while the <a href="https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP7090.html" target="_blank">IBM 7090</a>, introduced in 1959, and the popular IBM <a href="https://www.ibm.com/ibm/history/ibm100/us/en/icons/system360/" target="_blank">System/360</a> mainframes, introduced in 1964, used magnetic amplifiers to regulate their power supplies. Control Data Corp.’s <a href="https://en.wikipedia.org/wiki/CDC_160_series" target="_blank">160 minicomputer</a> of 1960 used a magnetic amplifier in its console typewriter. Magnetic amplifiers were too slow for the logic circuits in the <a href="https://en.wikipedia.org/wiki/UNIVAC_LARC" target="_blank">Univac LARC</a> supercomputer of 1960, but they were used to drive its core memory.
</p><p>
<strong>In the 1950s, </strong><span style="background-color: initial;">engineers in the U.S. Navy had called magnetic amplifiers “a rising star” and one of “the marvels of postwar electronics.” As late as 1957, more than 400 engineers attended a conference on magnetic amplifiers.</span> <span style="background-color: initial;">But interest in these devices steadily declined during the 1960s when transistors and other semiconductors took over.</span>
</p><p>
	Yet long after everyone figured that these devices were destined for the dust heap of history, mag amps found a new application. In the mid-1990s, the 
	<a href="https://en.wikipedia.org/wiki/ATX" target="_blank">ATX</a> standard for personal computers required a carefully regulated 3.3-volt power supply. It turned out that magnetic amplifiers were an inexpensive yet efficient way to control this voltage, making the mag amp a key part of most PC power supplies. As before, this revival of magnetic amplifiers didn’t last: DC-DC regulators have largely replaced magnetic amplifiers in modern power supplies.
</p><p>
	All in all, the history of magnetic amplifiers spans about a century, with them becoming popular and then dying out multiple times. You’d be hard pressed to find a mag amp in electronic hardware produced today, but maybe some new application—perhaps for quantum computing or wind turbines or electric vehicles—will breathe life into them yet again.
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Sun, 27 Mar 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/the-vacuum-tubes-forgotten-rival</guid><category>Amplifiers</category><category>Tubes</category><category>Military</category><dc:creator>Ken Shirriff</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-an-older-woman-in-front-of-an-early-generation-computer.jpg?id=29534788&amp;width=980"></media:content></item><item><title>When New York City Was a Wiretapper’s Dream</title><link>https://spectrum.ieee.org/illegal-wiretapping</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-wearing-headphones-aims-a-flashlight-at-some-telephone-wires-in-a-dark-basement-room.jpg?id=29594155&width=2366&height=1775&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>
<strong>On February 11,</strong> 1955, an anonymous tip led two New York Police Department detectives and two New York Telephone Company investigators to an apartment on the fourth floor of a residential building at 360 East 55th Street in midtown Manhattan. In the back bedroom of the unit, the group discovered a cache of stolen wiretapping equipment that turned out to have direct lines into six of New York City’s largest telephone exchanges: <a href="https://www.nytimes.com/1978/07/24/archives/new-jersey-pages-phone-exchanges-lose-their-letters-imagine-an.html" rel="noopener noreferrer" target="_blank">PLaza 1, 3, and 5; MUrray Hill 8; ELdorado 5; and TEmpleton 8</a>. The connections blanketed an area of Manhattan running from East 38th Street to East 96th Street, a swath of the city’s most expensive real <a href="https://www.nytimes.com/1978/07/24/archives/new-jersey-pages-phone-exchanges-lose-their-letters-imagine-an.html" rel="noopener noreferrer" target="_blank">estate</a>.
</p><p>
	“There wasn’t a single tap-free telephone on the east side of New York,” professional wiretapper 
	<a href="https://en.wikipedia.org/wiki/Bernard_Spindel" rel="noopener noreferrer" target="_blank">Bernard Spindel</a> remarked of the arrangement. (Spindel was in all likelihood the source of the anonymous tip.) News of the discovery made the <a href="https://www.nytimes.com/1955/02/18/archives/phone-tap-center-raided-in-54th-st-phone-tap-center-uncovered-in.html" rel="noopener noreferrer" target="_blank">front page of the <em>New York Times</em></a> a week later.
</p><hr/><p>
	The midtown Manhattan “wiretap nest,” as the 55th Street listening post came to be known, remains one of the largest and most elaborate private eavesdropping operations ever uncovered in the United States. Subscribers whose phones were tapped at the time of the raid included a range of New York commercial interests, with assets both large and small: a modeling agency and an insurance company; an art gallery and a lead mining company; and perhaps most sensationally, two publicly traded pharmaceutical corporations with competing patent interests. (The two firms, Bristol-Myers and 
	<a rel="noopener noreferrer" target="_blank"></a>E. R. Squibb, were at the time locked in a nasty legal battle over the commercial rights to the antibiotic tetracycline. Evidence later revealed that representatives from a third firm, Pfizer, had employed the wiretap nest to spy on both entities, paying more than $60,000 in cash for the service.)
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A woman in theatrical garb appears shocked by what she is hearing over a telephone." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="118d9878a6b0e3d645cb7f7bf6831800" data-rm-shortcode-name="rebelmouse-image" id="29f8e" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-in-theatrical-garb-appears-shocked-by-what-she-is-hearing-over-a-telephone.jpg?id=29594181&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Burlesque artist Ann Corio was among the celebrities targeted by an illegal wiretapping operation in New York City in 1955.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Bettmann/Getty Images</small>
</p><p>
	Contrary to the popular image of the phone tap as either a technology of state surveillance or a tool of corporate espionage, the vast majority of the lines ensnared in the 
	<a href="http://www.bugsweeps.com/info/how_wiretap.html" rel="noopener noreferrer" target="_blank">55th Street operation</a> turned out to be owned by private individuals. Some—like the burlesque artist <a href="https://en.wikipedia.org/wiki/Ann_Corio" rel="noopener noreferrer" target="_blank">Ann Corio</a>, whose phone conversations were recorded in a dragnet search for incriminating information on prominent midtown residents—were the targets of blackmail. Others—like the New York socialite <a href="https://en.wikipedia.org/wiki/John_Jacob_Astor_VI" rel="noopener noreferrer" target="_blank">John Jacob Astor VI</a>, who wanted someone to keep tabs on his wife—were involved in messy civil suits and divorce cases. By all accounts, the setup had the technical capacity to monitor as many as a hundred telephone lines at the same time. Between 50,000 and 100,000 individual subscribers were alleged to have been tapped over the course of fifteen months.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A man in a double-breasted suit, arms folded over his chest, and with a haughty expression on his face, leans against a wall in a telephone kiosk." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="2faf8031b3173541d3d8ebb4f38a2ca8" data-rm-shortcode-name="rebelmouse-image" id="264cd" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-a-double-breasted-suit-arms-folded-over-his-chest-and-with-a-haughty-expression-on-his-face-leans-against-a-wall-in.jpg?id=29594262&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">New York City private investigator and attorney John G. “Steve” Broady was convicted as the mastermind of the 55th Street “wiretap nest,” the largest illegal wiretapping operation ever to come to light in the United States. Broady paid a high price: Besides being disbarred, he served the entirety of his four-year sentence in jail.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Evelyn Straus/NY Daily News Archive/Getty Images</small>
</p><p>
	Four men were eventually indicted in conjunction with the raid on the 55th Street wiretap nest: 
	<a href="https://historyengine.richmond.edu/episodes/view/6796" rel="noopener noreferrer" target="_blank">John G. Broady</a>, an attorney and private investigator; Warren B. Shannon, a freelance electrical technician; and Walter Asmann and Carl R. Ruh, two rogue employees of the New York Telephone Company. In the course of the ensuing criminal trial, Shannon, Asmann, and Ruh were all granted immunity in exchange for testifying against Broady, who emerged as the brains behind the operation. <a href="http://content.time.com/time/subscriber/article/0,33009,861731,00.html" rel="noopener noreferrer" target="_blank">Broady</a> ended up receiving an unusually harsh prison sentence—four years, twice as long as the penalty suggested by New York’s penal code. At the close of the proceedings the presiding judge broke custom by publicly chastising the principals in the case: “In my many years as a judge, I have made it a rule not to excoriate defendants when imposing a prison sentence. However, the public interest requires some comment concerning this case. Illegal wiretapping is a slimy activity, which directly and adversely affects our social and economic life. It cannot be condemned too strongly.”
</p><p>
	The gravity of the response to Broady’s conviction only heightened the suspicion that there was more to the story than met the eye. A number of strange details from the early newspaper reports on the case remained unexplained at the end of the trial. The freelance electrician initially indicted for the crime, Warren Shannon, turned out to have been living in the apartment at East 55th Street for more than a year. Although he was at home with his wife when investigators arrived on February 11, no arrests were made, and no wiretapping devices were confiscated. When the NYPD returned to the scene a week later, much of the equipment used in the operation had disappeared.
</p><p>
	Considering the size and longevity of the 55th street operation (established, sources said, in December 1953), it seemed possible that NYPD officials were aware of its existence prior to the February 11 raid. Had dishonest cops agreed to look the other way in exchange for the ability to shake down local criminals via wiretap? Such an arrangement would certainly have been consistent with earlier grand jury inquiries into police corruption in NYPD gambling and vice investigations. The fact that the case involved New York Telephone employees only reinforced this conjecture. Bell system linemen were long rumored to have had a hand in the city’s illegal wiretap trade.
</p><p class="pull-quote">
	Tiny, cheap, and almost impossible to detect in action, induction coils were in wide use in wiretapping operations of all sorts by the late 1930s, and nowhere more so than in New York
</p><p>
	According to journalist Ray Graves, the attempted cover-up of the 55th Street scandal was the American public’s first glimpse of “the ‘Big A,’ or The Alliance.” Writing in the July 1955 issue of 
	<em>Confidential</em> Magazine, he identified it as “a group made up of corrupt cops, telephone men, and expert illegal wiretappers in the private eye racket…[that] deals in outright blackmail, selling information, and…does much of its work for big businessmen who want to get the jump on a competitor.” The midtown Manhattan tap nest was one of many private listening posts around the country (“Los Angeles, Chicago, Philadelphia, Detroit, Boston, Miami, and Washington all have wiretap centers comparable to the cozy set-up recently exposed in New York”), and the shadowy “Alliance” had a vested interest in keeping their workings under wraps.
</p><p>
	The rumors of conspiracy and corruption now seem far-fetched. But at the time, the story was plausible enough to occasion internal handwringing among Bell system providers. In a company bulletin dated March 9, 1955, New York Telephone assured nervous stakeholders that there was “no foundation” to national reports that there was a “corrupt alliance between telephone employees, the police, and illegal wire-tappers.”
</p><p>
<strong>Conspiracy or not,</strong> the 55th Street “wiretap nest” was itself an unsettling image. That four men could set up shop in a midtown apartment, commandeer an array of stolen electronic devices, and tap into thousands of lines servicing some of the most high-profile addresses in New York City—the story seemed to confirm creeping anxieties about the invasive reach of modern communications systems and their susceptibility to manipulation and control.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A man with dark, closely cropped hair and wearing a glen plaid suit jacket smiles bemusedly." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="66118e1fc0f6ff028d5b1c9d91e9f056" data-rm-shortcode-name="rebelmouse-image" id="d55b5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-with-dark-closely-cropped-hair-and-wearing-a-glen-plaid-suit-jacket-smiles-bemusedly.jpg?id=29594295&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Anthony P. Savarese, a member of the New York State Assembly, headed a commission set up after the discovery of the 55th Street wiretapping operation to investigate the prevalence of illegal eavesdropping in the state.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Queens Chamber of Commerce Collection/Queens Borough Public Library Archives</small>
</p><p>
	To quell further public uproar, the New York state legislature in Albany appointed 
	<a href="https://www.nytimes.com/2002/08/11/nyregion/anthony-p-savarese-jr-85-former-assemblyman-and-judge.html" rel="noopener noreferrer" target="_blank">Anthony P. Savarese</a>, an assemblyman with connections to the New York City Anti-Crime Committee, to convene an emergency joint commission on the illegal interception of electronic communications. Charged with cutting through the “miasma of hearsay” surrounding the tap-nest scandal and recommending corrective legislation, Savarese began his work in late February 1955. He filed a hotly anticipated preliminary report the following year. But the commission’s official findings only served to bolster the sense that wiretapping was more entrenched and pervasive than the national debates had made it seem.
</p><p>
	According to the Commission’s March 1956 report, the 55th Street scandal was the product of a host of developments that had made the New York telephone system “vulnerable to tapping:” technological advances that made phone taps both easier to plant and harder to detect; corruption among state police officers and low-level employees in the telecommunications industry; and the unfettered expansion of the private investigation field in the years following World War II. Yet the Commission’s most enduring conclusion—echoed in later studies like Samuel Dash’s influential 1959 report 
	<a href="https://www.repository.law.indiana.edu/cgi/viewcontent.cgi?article=3118&context=ilj" rel="noopener noreferrer" target="_blank"><em>The Eavesdroppers</em></a>—was that any honest effort to curb illegal wiretapping in America would have to start at the state and municipal levels.
</p><div class="ieee-sidebar-medium">
<p>
<em>Excerpted from </em><a href="https://www.amazon.com/Books-Brian-Hochman/s?rh=n%3A283155%2Cp_27%3ABrian+Hochman" rel="noopener noreferrer" target="_blank">The Listeners: A History of Wiretapping in the United States <em>by Brian Hochman</em></a><em><u>,</u> published by Harvard University Press (2022).</em>
</p>
</div><p>
	To be sure, the failings of New York state wiretap law were legion. A comprehensive court-order system had governed the phone tap protocols for New York law enforcement agencies since 1938. Although many policy experts considered the system a model for federal wiretap reform, the Savarese Commission discovered that judicial oversight was easy to circumvent, and existing criminal laws offered the state little room to prosecute police officers who chose to tap wires illegally. The foundations of New York’s laws against private wiretapping (i.e., wiretapping conducted by individuals acting outside of the state’s “sovereign authority”) were even shakier. The New York penal code prohibited any attempt to “cut, break…or make connection with any telegraph or telephone line, wire, cable, or instrument,” a clear sign that wiretapping without the written permission of a state judge was a criminal offense.
</p><p>
	The problem was that the statute was written in 1892. Six decades’ worth of technological advancements had all but rendered it obsolete—so much so, the Commission noted, that almost every attempt to prosecute illegal wiretapping in the state of New York since 1892 had failed on technical grounds.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A man places an electronic device near a wire while a woman operates recording gear." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="cc8dc01ba95b8023a0edadb4a48d9204" data-rm-shortcode-name="rebelmouse-image" id="5824f" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-places-an-electronic-device-near-a-wire-while-a-woman-operates-recording-gear.jpg?id=29594302&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Eavesdroppers started using induction coils to tap into phone calls in the 1930s. Here, technicians demonstrate the state of the art in 1940.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">FPG/Archive Photos/Getty Images</small>
</p><p>
	One major challenge to New York’s 1892 wiretap law, frightful for midcentury observers to behold, was the rise of what was known as induction wiretapping, a newfangled eavesdropping technique that didn’t require a physical connection to a telephone line. With the help of simple magnetic devices called “
	<a href="http://www.unterzuber.com/tap.html" rel="noopener noreferrer" target="_blank">induction coils</a>”—essentially spare radio parts, available at most any hardware store—the induction method amounted, somewhat paradoxically, to a wireless wiretap. In the words of one electronics manufacturer, “Simply slip [an induction coil] under the base of a desk phone or lay on top of a ringer box of wall phones” and achieve “optimum results.”.
</p><p>
	Tiny, cheap, and almost impossible to detect in action, induction coils were in wide use in wiretapping operations of all sorts 
	<a href="https://www.newyorker.com/magazine/1938/06/18/tapping-the-wires" rel="noopener noreferrer" target="_blank">by the late 1930s</a>, and nowhere more so than in New York. In part this was because the state’s penal code had explicitly defined illegal wiretapping as an unwarranted physical connection to a telephone line. As the Savarese Commission pointed out, it was impossible to bring criminal charges against wiretappers caught using induction coils when they never so much as touched the phone company’s equipment.
</p><p>
	The 55th Street operation had relied on wiretapping techniques that were more primitive than induction. But the Savarese Commission went to great lengths to show that even simple wiretap installations were impossible to prevent and prosecute according to the letter of the law. For most of the twentieth century, both private surveillance experts and law enforcement officials mostly relied on what was known as the direct wiretap method. As its name suggests, this technique involved connecting directly to the circuitry of the telephone system, either by scraping away the insulation along the route of a phone line and appending an extension wire, or by attaching an amplifier and headphones to a telephone junction box, where multiple residential lines met and joined the system’s 
	<a rel="noopener noreferrer" target="_blank"></a>main frame.
</p><p>
	Direct wiretapping was tedious work that became both more and less difficult to carry out in the postwar years. More difficult, because installing a direct wiretap required the ability to find the subscriber’s line and pinpoint the location where the tap wire needed to be connected. Identifying this location, known as an “ <a href="https://casetext.com/case/adams-v-lankford" rel="noopener noreferrer" target="_blank">appearance” point</a> or location, became increasingly difficult as the telephone system expanded its labyrinthine reach. By World War II, telecommunications providers had also wised up to security concerns, adding locks to the most obvious direct tap locations, such as basement junction boxes.
</p><p>
	But direct wiretapping proved less difficult to carry out in this period for almost the exact same set of reasons. The sprawl of the telephone system also meant that communications hardware and infrastructure—and, more importantly, the employees who managed them on a daily basis—were impossible to oversee in their entirety. For the right price, the Savarese Commission discovered, anyone who wanted to find a line to tap could bribe a phone company employee for the relevant cable appearances, or even for direct access to the main frame, just as John Broady had when setting up the tap nest.
</p><p class="pull-quote">
<span></span>The wiretapping statute was written in 1892. Six decades’ worth of technological advancements had all but rendered it obsolete
</p><p>
	“90 per cent of all tappers today are old telephone company men,” reported William J. Mellin, a retired government investigator who claimed to have tapped more than 15,000 lines during his forty years of work for the Internal Revenue Service. Mellin’s estimate would have the ring of hyperbole if the Savarese Commission hadn’t come to the same conclusion.
</p><p>
	What truly distinguished the Empire State in the 1950s—what made it America’s “
	<a href="https://www.jstor.org/stable/1119131" rel="noopener noreferrer" target="_blank">eavesdropping capital,</a>” in the words of the privacy law expert Alan Westin—was yet another loophole in state wiretap law, one that raised doubts as to whether the sort of wiretapping that the NYPD discovered at East 55th Street was even illegal at all.
</p><p>
	The loophole was the result of a curious court decision involving a Brooklyn businessman named Louis Appelbaum, who sued his wife for divorce in 1949. The evidence in the suit was partly based on telephone conversations that Appelbaum had permitted Robert La Borde, a notoriously prolific New York private investigator, to record on his home line. The presiding judge dismissed the divorce suit and went on to charge both Appelbaum and La Borde for violating the state’s wiretapping law. Both men were convicted. But an appellate court reversed the ruling in 1950, reasoning that telephone subscribers maintained a “paramount right” to tap their own lines.
</p><p>
	The language of the appellate court’s opinion in 
	<em>People v. Appelbaum</em> (1950) was unambiguous in its support for what would become known as “<a href="https://www.dmlp.org/legal-guide/new-york-recording-law" rel="noopener noreferrer" target="_blank">one-party consent</a>” eavesdropping: “When a subscriber consents to the use of his line by his employee or by a member of his household, or by his wife, there is a condition implied that the telephone will not be used to the detriment of the subscriber’s business, household, or marital status…. In such situations, the subscriber…may have his own line tapped or otherwise checked so that his business may not be damaged, his household relations impaired, or his marital status disrupted.” For a resident of New York in the early 1950s—a man, most likely, because the gendered language of the ruling perversely implied that men had more claim on subscriber’s rights than women—it was entirely legal, under <em>Appelbaum</em>, to record any conversation made on your home telephone. It was also entirely legal to hire someone else to do it for you.
</p><p>
	The Savarese Commission spent most of its investigative energy working to understand the effects of the 
	<em>Appelbaum</em> decision, eventually coming to the conclusion that it had encouraged a “lively, active, and lucrative” private eavesdropping industry throughout New York State. According to the Commission’s March 1956 report, the case had thrown into confusion what was left of New York’s 1892 wiretap law. It had also created a growing market for an urban professional whose doings had long preoccupied studies of electronic surveillance nationwide: the wiretapper-for-hire—or, more colloquially, “private ear.” These were men (again: almost all were men) with a uniquely modern expertise. They knew how to tap any telephone, and they knew how to locate any telephone that was tapped. The tools of their trade were cheap, easy to use, and virtually impossible to detect in action. <em>Appelbaum</em> gave them <a rel="noopener noreferrer" target="_blank">license</a> to bring their work, long maligned as dirty and disreputable, out into the open.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two men in suits stand near a table heaped with tape-recording and other electronic gear." class="rm-shortcode" data-rm-shortcode-id="ba87ffaba7cb054c4cda89ae12aaa505" data-rm-shortcode-name="rebelmouse-image" id="054f4" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-in-suits-stand-near-a-table-heaped-with-tape-recording-and-other-electronic-gear.jpg?id=29594305&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Among the committees set up to investigate illegal wiretapping after the sensational revelation of the 55th Street “wiretapper’s nest” in New York City was one in the U.S. House of Representatives led by Emanuel Celler [right], a New York Democrat.  On 3 May 1955, professional wiretapper Bernard Spindel startled Celler by playing back for the congressman recordings of his own recent telephone calls.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Bettmann/Getty Images
	</small>
</p><p>
	After 1950, in the words of the Savarese Commission, New York private ears were “immune practitioners in a nonhazardous occupation.” They went about their business as freely as plumbers, housepainters, and insurance salesmen.
</p><p>
	Reliable facts and figures about the private eavesdropping industry that prospered under 
	<em>Appelbaum</em> are difficult to find. The Savarese Commission conducted months of closed-session interviews to create a thumbnail sketch of the men who were offering freelance wiretapping services around the state of New York. Most were either proficient in electronics early on, tapping their first lines by the age of twelve or thirteen, or had received special technical training while serving in the military. Most had gone on to find paying jobs in telecommunications, law enforcement, or freelance private investigation, three professional fields that expanded dramatically after World War II. And in the course of their regular duties, most had the opportunity to discover that telephone lines were easy and lucrative to tap—easy and lucrative enough, in any event, to turn wiretapping into a dedicated career, despite the risks that occasionally came with it. In 1955, the year of the 55th Street scandal, private wiretapping contractors were reported to net as much as $250 per day in Brooklyn and Manhattan. The jobs with the most legal exposure commanded the highest rates.
</p><p>
	The biggest names in the profession—
	<a href="https://www.nytimes.com/1970/06/24/archives/robert-c-la-borde-devised-wiretaps.html" rel="noopener noreferrer" target="_blank">Robert La Borde</a>, John Broady, Bernard Spindel—tended to make their money monitoring telephone lines for New York businesses. Many more found work in the domestic sphere, helping to litigate civil and marital disputes.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A man in a suit stands next to a woman in a white sleeveless dress holding a fur coat." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="5a3bfbc6ae253987950c098976dce89e" data-rm-shortcode-name="rebelmouse-image" id="4334d" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-a-suit-stands-next-to-a-woman-in-a-white-sleeveless-dress-holding-a-fur-coat.jpg?id=29594325&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">John Jacob Astor VI was among the powerful people whose name was dredged up by the investigation into the 55th-Street wiretapping operation. Shortly after returning from his honeymoon, Astor had filed for divorce from his third wife, the former Dolores Margaret “Dolly” Fullman, and was thought to be seeking incriminating evidence against her</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Bettmann/Getty Images</small>
</p><p>
	The Savarese Commission discovered that divorce wiretapping was far and away the most common job for private eavesdropping specialists in the 1950s. Because New York divorce laws were “adversarial,” requiring one party to show fault in the other before the state could terminate a union, wiretap recordings that captured evidence of infidelity could have a dramatic effect on the outcome of individual cases. This was why John Jacob Astor VI had turned to John Broady—Astor believed that a wiretap would prove that his wife was having an affair with another man. The Savarese Commission found the arrangement to be surprisingly common. New York’s private ears tapped more lines to monitor cheating spouses than their counterparts in law enforcement did to gather criminal evidence.
</p><p>
	The Savarese Commission’s report would inaugurate a new day for wiretapping in the Empire State—or so it seemed on the surface. In July 1957, after more than two years of legislative wrangling, policymakers in Albany added an amendment to the New York penal code that expanded the state’s definition of illegal eavesdropping to include both direct and induction wiretapping and levied hefty fines on phone companies that failed to report violations of the new law. The amendment also closed the Appelbaum loophole, prohibiting one-party consent eavesdropping and barring the use of wiretap recordings or transcripts in civil court proceedings. But when the Savarese Commission recommended tightening oversight of law enforcement wiretapping, police officials pushed back, and lobbyists in Albany eventually pressured the legislature to keep the state’s court-order system intact. The resulting compromise seemed to place New York law enforcement beyond the reach of reform.
</p><p class="pull-quote">
	In 1955, the year of the 55th Street scandal, private wiretapping contractors were reported to net as much as $250 per day in Brooklyn and Manhattan.
</p><p>
	The legacy of the 55th Street scandal in New York was thus mixed. By the end of the decade, it seemed as though both everything and nothing had changed. When Congress held exploratory hearings on “
	<a href="https://play.google.com/store/books/details?id=wZ0TAAAAIAAJ&rdid=book-wZ0TAAAAIAAJ&rdot=1" rel="noopener noreferrer" target="_blank">Wiretapping, Eavesdropping, and the Bill of Rights</a>” in the winter of 1959, ranking members of the Senate Subcommittee on Constitutional Rights wrote to Wellington Powell, New York Telephone’s vice president of operations, to testify about the outcome of the wiretap nest case. In an official letter later introduced into the congressional record, Powell expressed optimism about the success of the Savarese Commission’s effort to curb illegal wiretapping in New York.
</p><p>
	“The new laws have strengthened privacy of communications by providing new sanctions and by eliminating loopholes and administrative difficulties under old laws,” he reported. To bolster the new legal regime, New York Telephone had also “added more specially trained personnel to [its] special agents’ forces” and intensified “indoctrination and supervision concerning security practices.” But between the lines, Powell’s letter offered an ominous set of statistics that underscored just how unworkable the twin ideals of privacy and security were in the field of telecommunications. In Manhattan alone, the New York Telephone Company managed 75,000 terminal boxes. Those 75,000 boxes connected to more than 4,000 miles of cable, and those 4,000 miles of cable contained more than 3 million miles of telephone wire. The entire New York Telephone System serviced an estimated 7,900,000 handsets. In a communications network so unmanageably vast, preventing an isolated illegal act was nothing less than a Sisyphean task.
</p><p>
<strong>Federal agencies wouldn’t</strong> begin to face political consequences for the abuse of wiretaps in national security <a href="https://www.nytimes.com/1978/03/23/archives/at-t-and-specter-of-wiretapping-is-company-vigilance-enough-to.html" rel="noopener noreferrer" target="_blank">investigations until the 1970s</a>. In the wake of the 55th Street controversy, state and municipal governments around the country likewise passed a flurry of wiretap reforms, many of which sought to prohibit the private use of electronic surveillance equipment. But at least in New York, the sense among those who knew best was that aggressive policy measures amounted to little more than sound and fury.
</p><p>
	“You can’t legislate…against illegal wiretapping,” warned New York District Attorney Edward Silver. “They did it before there were statutes and they will do it regardless of what you do.” On the other side of the law, private ears like Bernard Spindel offered equally worrisome predictions about the spread of the wiretap trade in the face of new policies: “Never before have so many people been willing to pay so much to find out what others are thinking and doing. Never before have we been so capable of accomplishing these desires. Whatever legislation may be enacted…is already many years too late.” Futility was the order of the day. “Most experts believe that no matter what legislation is enacted, the unhappy outlook as of now is that wiretapping is here to stay and will increase,” 
	<em>Newsweek</em> reported in an article on “The Busy Wiretappers” in the spring of 1955. The tumultuous decade that followed proved all of the predictions right.
</p><p>
<em>This article appears in the August 2022 print issue as “1950s New York Was a Wiretapper’s Dream.”</em>
</p>]]></description><pubDate>Fri, 25 Mar 2022 18:01:01 +0000</pubDate><guid>https://spectrum.ieee.org/illegal-wiretapping</guid><category>Eavesdropping</category><category>Wiretapping</category><category>Bugging</category><category>Listening devices</category><dc:creator>Brian Hochman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-wearing-headphones-aims-a-flashlight-at-some-telephone-wires-in-a-dark-basement-room.jpg?id=29594155&amp;width=980"></media:content></item><item><title>Engineering Lunar Network 2.0</title><link>https://spectrum.ieee.org/apollo-communications</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-astronaut-on-the-moon-with-an-antenna-in-the-foreground.png?id=29583209&width=1200&height=800&coordinates=0%2C42%2C0%2C42"/><br/><br/><p><strong>When Neil Armstrong</strong> uttered one of the most famous sentences in human history, he did so via a microphone in his helmet and a 3-kilogram VHF-band radio in his backpack. The radio linked to a rig in the lunar lander, which launched microwave signals on a 325,000-kilometer journey to Earth.</p><p>That radio was a tech marvel. In a package just 35 by 15 by 3.2 centimeters, its designers fit two AM receivers, two AM transmitters, either an FM receiver or an FM transmitter, and also a telemetry system that transmitted spacesuit status and physiological data about the astronaut. While those specs might not seem so dazzling today, kindly remember that this radio was designed (by RCA) in the mid-1960s. The whole thing was done without integrated circuits, which were available in the mid-1960s and used extensively elsewhere in the Apollo program but were still relatively uncommon and expensive.</p><hr/><p>The astronauts’ backpack radios were just a small piece of a sprawling communications infrastructure assembled by NASA in the 1960s. For the Apollo missions, during the moon walks, the heart of the communications system was the rig in the lunar lander, known as the <a href="https://ntrs.nasa.gov/citations/19720023255" target="_blank">Lunar Module Communications System</a>. It communicated not only with the astronauts’ radios but also had microwave links to the orbiting command module and to Earth, through a globe-spanning network of more than 30 dish antennas called the Manned Space Flight Network.</p><p>A memorable feature of the Apollo communication system was the <a href="http://workingonthemoon.com/WOTM-Erectable-S-Band.html" target="_blank">S-band erectable antenna</a>, which was connected to the lunar module’s radio system. Stowed as a cylinder 25 by 100 cm, it was unfurled on the moon into a 3-meter-diameter parabolic dish covered with a very fine, flexible, gold-plated mesh. The erectable antenna had a transmit gain of 34 decibels, about 12 dB better than the “steerable” antenna mounted on the lander. The higher gain was needed to accommodate color-TV signals, along with the voice and data channels. Unfortunately, the first time the antenna was used, there wasn’t much video to broadcast. As he was setting up the camera, astronaut Alan Bean accidentally aimed it at the sun and burned out its image tube.</p><p>“Here is the TV,” <a href="https://www.hq.nasa.gov/alsj/a12/a12.tvtrbls.html" rel="noopener noreferrer" target="_blank">he said</a>. “And it’s pointing toward the sun. That’s bad.” After the mishap, NASA equipped future Apollo TV cameras with a lens cap.</p><p>One of the <a href="http://www.arrl.org/eavesdropping-on-apollo-11" rel="noopener noreferrer" target="_blank">more poignant stories</a> of Apollo’s communications involved a radio amateur in Kentucky: Larry Baysinger, W4EJA. By day, Baysinger, who had an interest in radio astronomy, was the station engineer at WHAS AM in Louisville. On the evening of 20 July 1969, <a href="https://space.stackexchange.com/questions/33359/hijacked-space-data-notable-instances-of-recovering-images-or-other-goodies-fro/33361#33361" rel="noopener noreferrer" target="_blank">Basinger managed to pick up</a> not the powerful S-band signal from the lunar lander but rather the weak VHF signals from Neil Armstrong’s backpack radio on the moon. For 35 minutes he listened to the astronauts’ conversation, and even heard them being congratulated by President Richard Nixon. The feat is <a rel="noopener noreferrer" target="_blank">all the more</a> remarkable considering Baysinger’s rig—a rebuilt 20-year-old Army surplus tank radio and a steerable antenna he built out of aluminum tubing and chicken wire.</p><p class="pull-quote">With dozens of missions planned for the next decade, Jet Propulsion Laboratory has partnered with the Italian aerospace company Argotec to design a satellite-based lunar network</p><p>Now Earth-to-moon communications are poised for a new era. With dozens of missions planned for the next decade, the <a href="https://www.jpl.nasa.gov/" rel="noopener noreferrer" target="_blank">Jet Propulsion Laboratory</a> has partnered with the Italian aerospace company <a href="https://www.argotec.it/online/" rel="noopener noreferrer" target="_blank">Argotec</a> to design a satellite-based lunar network that would provide coverage to most of the moon at any given time. <a href="https://spectrum.ieee.org/lunar-communications" target="_blank">The plan</a> calls for 24 satellites to move in four highly elliptical orbits, relaying signals between the lunar surface and Earth. The network wouldn’t be very fast—it would deliver tens of megabits per second, which is less than a decent fiber-to-the-home hookup.</p><p>But the Argotec-JPL concept is just one of several budding initiatives to design future lunar communications infrastructure, including proposals to serve future lunar residents. These explorers would need enormously powerful data links to conduct experiments, control remote equipment, receive and issue warnings about dangerous space weather, rescue stranded surface travelers, and even combat homesickness. The largest project to return humans to the moon, <a href="https://www.nasa.gov/specials/artemis/" rel="noopener noreferrer" target="_blank">NASA’s Artemis</a>, has already spawned several proposals for such lunar networks. NASA itself has developed an architecture it calls <a href="https://www.nasa.gov/feature/goddard/2021/lunanet-empowering-artemis-with-communications-and-navigation-interoperability" rel="noopener noreferrer" target="_blank">LunaNet</a>, which it recently shared with industrial and government partners. And the Japan Aerospace Exploration Agency recently awarded separate contracts to <a href="https://aithority.com/internet-of-things/space-technology/jaxa-selects-a-consortium-to-study-navigation-and-communication-technology-development-for-lunar-surface-activities/" rel="noopener noreferrer" target="_blank">ArkEdge Space</a> and <a href="https://www.businesswire.com/news/home/20220125005509/en/WARPSPACE-Selected-for-JAXA-RD-Project-to-Consider-the-Design-of-Optical-Cislunar-Communication-Architecture-for-the-Lunar-Exploration" rel="noopener noreferrer" target="_blank">Warpspace Co.</a> to perform studies for robust and technologically advanced lunar networks.</p><p>It’s not too much of a stretch to envision these future moon dwellers <a href="https://www.cbc.ca/radio/quirks/apollo-landing-sites-now-protected-by-u-s-law-but-what-about-the-flags-1.5866031" rel="noopener noreferrer" target="_blank">gaping at the landing sites</a> of the Apollo missions. There, amid the lander descent stages, <a href="https://www.popsci.com/trash-items-left-on-moon-apollo-maps/" rel="noopener noreferrer" target="_blank">nail clippers, US $2 bills, and vomit bags</a>, they’ll see the S-band erectable antennas, television cameras, and lunar rovers. With their 21st-century smartphones, they might even take selfies alongside some of the most remarkable communications gear of the 20th.</p><p><em>This article appears in the April 2022 print issue as “One Small Step for Radio.”</em></p>]]></description><pubDate>Fri, 25 Mar 2022 16:52:02 +0000</pubDate><guid>https://spectrum.ieee.org/apollo-communications</guid><category>Moon</category><category>Apollo</category><category>Artemis</category><category>Communications</category><category>Radio</category><category>Lunar communications</category><dc:creator>Glenn Zorpette</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/photo-of-astronaut-on-the-moon-with-an-antenna-in-the-foreground.png?id=29583209&amp;width=980"></media:content></item><item><title>Success Storied: How to Advocate for Tech Policy</title><link>https://spectrum.ieee.org/tech-policy-successes</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-shows-a-blue-lock-with-circuitry-and-red-white-and-blue-lines-passing-through-it.jpg?id=29594614&width=1200&height=800&coordinates=0%2C175%2C0%2C176"/><br/><br/><p>Of tech policy groups in recent memory, few have been quite as successful as the Cyberspace Solarium Commission (CSC). Turning policies into reality might not be the flashiest process, but it’s hard to say that CSC hasn’t made a splash. Thanks to its laundry list of cybersecurity recommendations, the United States now has an <a href="https://www.whitehouse.gov/oncd/" rel="noopener noreferrer" target="_blank">Office of the National Cyber Director</a>, the U.S. government can subpoena ISPs over vulnerabilities, and the country has a raft of updated cybersecurity protocols.</p><p>The CSC has reached the end of its first life. But, already, well over half of its recommendations have been put into U.S. law—a rather impressive strike rate for a group of its kind. Looking at the CSC’s history and its background, there are a few factors that primed it for success—including more than a bit of fortune.</p><p>For one, the CSC owes its existence to the will of U.S. lawmakers: It was formed by and included members of the U.S. Congress in 2019. At that time, Donald Trump’s administration had dissolved the predecessor of the Office of the National Cyber Director, and members of Congress fretted over the hole that was left behind. So, they created the CSC to sort out how best to fill it.</p><p class="pull-quote">The CSC ultimately recommended 82 policies. Of those, somewhere between 50 and 65 were ultimately enacted.</p><p>That legislative backing set the commission apart from counterparts that came before it. “They might have had congressional members, but [the CSC] was different, because it was actually written into legislation and funded by the Congress,” says <a href="https://www.csis.org/people/james-andrew-lewis" rel="noopener noreferrer" target="_blank">James Lewis</a>, policy expert at the Center for Strategic & International Studies. “In that way, it was a first.”</p><p>For another, the CSC had official precedents. It gets its name from Project Solarium, a group founded in 1953 to study U.S. policy toward the Soviet Union. (The <a href="https://www.whitehousehistory.org/the-solarium" target="_blank">White House Solarium</a>, sitting atop the building, is where government officials met to brainstorm the group’s existence). Another, more recent precedent was the <a href="https://www.nscai.gov/about/" rel="noopener noreferrer" target="_blank">National Security Commission of Artificial Intelligence</a>, formed in 2018 to study how American government policy wonks should react to the rising prevalence of AI.</p><p>An even larger inspiration may have been the 9/11 Commission, which had been founded by U.S. lawmakers in 2002 to pore over the causes of the previous year’s terrorist attacks. Its eventual report did <a href="https://www.chicagotribune.com/news/sns-ap-sept-11-commission-story.html" rel="noopener noreferrer" target="_blank">receive criticism</a> for conflicts of interest and using questionable evidence, and the government’s success in carrying out its recommendations <a href="https://www.brookings.edu/opinions/911-commission-a-review-of-the-second-act/" rel="noopener noreferrer" target="_blank">was questionable</a>. But it proved to be immensely influential over its successors—the CSC included.</p><p>The CSC recommended a total of 82 policies. Of those, somewhere between 50 and 65 were ultimately enacted, in the estimate of <a href="https://www.solarium.gov/about/staff/mark-montgomery" rel="noopener noreferrer" target="_blank">Mark Montgomery</a>, the CSC’s executive director. But the process of actually implementing those recommendations was a delicate one that often needed to maneuver around the realities of U.S. federal lawmaking.</p><p>By far, the CSC’s highest-profile policy success was the creation of the National Cyber Director. But when the process began, it wasn’t clear that Donald Trump wouldn’t be reelected. In that course of events, the new director’s office would have had to deal with Trump’s rival wishes. Fortunately for the CSC, the incoming Joe Biden administration <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/" rel="noopener noreferrer" target="_blank">was far friendlier</a> to its ideas.</p><p>Many of the CSC’s recommendations were added into the pages of 2021’s version of the National Defense Authorization Act, the yearly law that funds the U.S. military and the U.S. Department of Defense. While that ensured those recommendations would pass, it could lead to administrative tangles over who could lead cybersecurity activities.</p><p>“I think that one of the problems for the U.S. in general is that, since Congress is a little dysfunctional at times, they tend to tack everything into the National Defense Authorization Act, which makes it a DOD activity, and cybersecurity isn’t a DOD activity,” says Lewis. (It’s generally under the umbrella of a different government department, the Department of Homeland Security.)</p><p>“If you try to do this through the prism of DOD, it will make it harder to be successful,” says Lewis.</p><p>Now, with the Congress-backed phase of its existence over, the CSC prepares to start a second life as a think tank, under the umbrella of the <a href="https://www.fdd.org/" rel="noopener noreferrer" target="_blank">Foundation for Defense of Democracies</a>. Again, there’s precedent here; previous commissions have often transformed into think tanks after the end of their tenure.</p><p>It’s likely that the CSC’s new form won’t have as easy of a time quickly pushing through recommendations en masse. But it may be able to keep watch over the fruits of its labors so far, and it can continue pushing for the rest of its laundry list, such as reinforcing cybersecurity in key sectors like utilities and health care and bringing more people into the cybersecurity workforce.</p>“If there’s a lot that we can tell needs to be studied and done, maybe they need a new commission,” Montgomery <a href="https://www.cyberscoop.com/congress-set-establish-white-house-national-cyber-director-enact-solarium-commission-recommendations/" rel="noopener noreferrer" target="_blank">told</a> CyberScoop. “But I think we’re going to spend two years to try to push through on implementation. In hindsight, you need longer to track implementation.”]]></description><pubDate>Fri, 25 Mar 2022 15:26:39 +0000</pubDate><guid>https://spectrum.ieee.org/tech-policy-successes</guid><category>Cybersecurity</category><category>Politics</category><category>Policy</category><category>Technology policy</category><dc:creator>Rahul Rao</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-shows-a-blue-lock-with-circuitry-and-red-white-and-blue-lines-passing-through-it.jpg?id=29594614&amp;width=980"></media:content></item><item><title>The Essential Vannevar Bush</title><link>https://spectrum.ieee.org/the-essential-vannevar-bush</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-suited-man-at-a-desk-holding-a-pen-to-paper.jpg?id=29430571&width=1200&height=800&coordinates=0%2C230%2C0%2C231"/><br/><br/><p class="">Vannevar Bush was the first trained electrical engineer to publicly proclaim in influential circles that EEs are one engine of innovation and the drive behind digital technology. As early as the 1930s, he viewed the entrepreneur and the engineer as twin forces for progress and technological advancement. </p><p>Born in 1890 in Massachusetts, he came to prominence as the nation’s top designer of computers while at MIT In the 1930s. During World War II, as personal science and engineering adviser to President Franklin D. Roosevelt, he led all research by civilians for the military and organized the Manhattan Project. After the war, he distinguished himself as a prime advocate for government funding of science and as a leading visionary the coming revolution in information technology. His “As We May Think” article, for the July 1945 issue of <em>The </em><em>Atlantic </em>magazine, envisioned a desktop computer and a “web” of “associative trails” that no less than the founders of Google cite as the inspiration for today’s information science and the Internet. </p><p>Bush was a fount of wisdom, particularly when it came to defining the engineer’s role in the industrial research and development enterprise. During the course of compiling Bush’s seminal works for my new book <em><a href="https://www.amazon.com/dp/0231116438/ref=cm_sw_em_r_mt_dp_07YP0641GT5723YDZ066" rel="noopener noreferrer" target="_blank">The Essential Writings of Vannevar Bush</a>,</em>  just published by Columbia University Press (2022), I pulled out some quotations of particular interest to <em>IEEE </em><em>Spectrum</em> readers:</p><h3>Engineers and Good Writing</h3><br/><p>Writing is essential to the success of an electrical engineer, Bush wrote in 1922, in his first textbook, <a href="https://www.goodreads.com/book/show/28613644-principles-of-electrical-engineering" target="_blank"><em>The Principles of Electrical Engineering</em></a> (coauthored with a colleague, William H. Timbie). The success of any engineer’s plan, Bush insisted, often depends on good writing:</p><p>“Once the plan has been decided upon, he must convince his superiors that the plan should be carried out. This convincing requires that the engineer write brief clear English, which adequately and concisely conveys the meaning in a convincing way. Good proposals have been turned down because the engineers who drafted them could not present them in convincing form.”</p><h3>Engineers Are Pioneers</h3><br/><p>The physical world is limited, but the world of ideas without boundaries; and there are seemingly no limits especially in technology and science. To highlight this point, in Bush’s <a href="https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm" target="_blank"><em>Science the Endless Frontier</em></a> report to President Harry S. Truman in July 1945, Bush wrote:</p><p>“The pioneer spirit is still vigorous within this nation. Science offers a largely unexplored hinterland for the pioneer who has the tools for his task.”</p><h3>As We May Think</h3><br/><img class="rm-shortcode" data-rm-shortcode-id="cc925ed0aab3d243afff7ff228f7bfab" data-rm-shortcode-name="rebelmouse-image" id="b23aa" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=29430592&width=980"/><p>Bush didn’t foresee the technical means by which vast amounts of information could be stored electronically, but he correctly predicted in 1945 that radical miniaturization of information was on the horizon. Writing in his most famous essay, “<a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/" target="_blank">As We May Think</a>,” published in <em>The Atlantic</em> magazine in 1945, he envisioned that</p><p>“A library of a million volumes could be compressed into one end of a desk.”</p><h3>Grappling With Good and Evil</h3><br/><p>Bush promoted a cold-eyed realism about the uncertain outcomes of discovery and invention. He tried to balance optimism and pessimism about human activities in turbulent times. To scientists and engineers, he advised:</p><p>“We are embarked upon a great adventure, and it is our privilege to further it. Even though at times the box that is opened be Pandora’s, even though there are both good and evil in what we learn, it is our duty and our calling to extend [humanity’s] grasp of the universe…. By this process, of beginning to understand, we have made such progress as we have. Though the path be thorny, this is still the way in which we should proceed if we would finally emerge from darkness into the light.”</p><h3>Envy the Duck</h3><br/><p>A master of metaphors, Bush searched for everyday analogies to explain technical products and processes. One of his favorites, from an essay, he penned in 1932 about the design and engineering of fabrics and clothing, was the duck:</p><p>“I have always envied the duck. He can dive under water and come up dry. Yet his coat is pervious to air as it should be for his good health, and it fits beautifully. The duck looks comfortable in his waterproof garments on a hot day, but the only raincoat I ever bought was hot, or it wasn’t waterproof, and it leaked at the neck when the rain drove horizontally. The Mongolians made a pervious felt that shed rain well, and Caesar thought well enough of it to adopt it. The duck can turn his head in any direction, and yet his covering on his neck lies marvelously smooth and sleek. Columbus found the Indians of Central America using feather garments; but even with their example, we still do not emulate the duck, nor do we look natty or comfortable in the rain.”</p><h3>Research Is the Key</h3><br/><img alt="Black and white photo of a man with a pipe holding large test tubes in a lab" class="rm-shortcode" data-rm-shortcode-id="8fa2b3011ec42665228147d9e6af89cc" data-rm-shortcode-name="rebelmouse-image" id="6cd1b" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-man-with-a-pipe-holding-large-test-tubes-in-a-lab.jpg?id=29430536&width=980"/><p>Before there was a “knowledge economy,” and before innovation became nearly synonymous with mastering new knowledge, Bush wrote, in 1932, about the perilous future facing individuals and institutions that failed to rely on research to enhance their survival:</p><p>“The key to accomplishment is research. It need not be complex, in order to be useful, but it certainly should be intelligent. In our modern tempo that industry is in danger which is in a static state. [Research] is a necessity [if only as] a defense against encroachment from without.”</p><h3>Versatility as Virtue</h3><br/><p>While he spent many years as a professor of electrical engineering at MIT, Bush sought to imbue fellow professors with a goal of being practical, versatile, and deep. Praising English-born engineer and inventor Elihu Thomson in 1933, Bush said:</p><p>“You have showed us that a man may be truly a professor and at the same time very practical. And one more thing I wish to emphasize. You have shown us that a scientist or an engineer may be, even in this complex world, versatile and yet not superficial.”</p><h3>Disruption Leads to New Jobs</h3><br/><img alt="Black and white photo of a group of six seated men smiling and laughing in front of a chalk board with scientific drawings" class="rm-shortcode" data-rm-shortcode-id="54b4b2af99ab2c4072696bb678083186" data-rm-shortcode-name="rebelmouse-image" id="aa73f" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-group-of-six-seated-men-smiling-and-laughing-in-front-of-a-chalk-board-with-scientific-drawings.jpg?id=29430619&width=980"/><p>To the question of how we best consider the costs of technological advance, of what he viewed as “progress,” Bush told a congressional committee in 1939:</p><p>“Progress, sir, always pays for itself by at least temporary disturbance. If we have a static world…that is lovely in one way, but if we are going to go ahead technically or in any other way then we must expect at least local disruption and temporary disruption, which means unemployment. There is no question, however, that the whole trend of invention, the whole trend of the introduction into industry of new devices and new ways of doing old things has been to greatly increase employment in the long run ….”</p><h3>Peace Through Strength</h3><br/><p>The decision to work on weapons technology, or not, was highly personal to Bush and a decision not easy to unpack. Bush opposed rigid opposition to working on military problems on the grounds that peace can sometimes best be preserved through strength, writing in 1935:</p><p>“Perhaps the worker on antiaircraft is more effectively a worker for peace than his brother who condemns him. I do not make the assertion, I pose the question.”</p><p>In 1940, as fascist Germany overran France and threatened England with invasion, and as the U.S. government began to consider military aid to Europe’s democracies, Bush answered his own question with the statement, “For war or peace, we must leave no stones unturned in research.”</p><h3>Revere the Inventor</h3><br/><p>Throughout his long career as a builder of large research organizations, Bush retained his belief in the lone inventor and the daring technological entrepreneur. Speaking to an audience of electrical engineers in early 1944, he said:</p><p>“There must remain in the U.S. the opportunity for an Edison, the opportunity for any youth with initiative, resourcefulness, practicality, and vision, to create his own name and, by his own efforts, new things that will tend to make this country vigorous and strong and safe.”</p><h3>On Push-button Warfare</h3><br/><img alt="Black and white photo of three suited men" class="rm-shortcode" data-rm-shortcode-id="6cc9faeedb61c8d1c9f79729ffe4daa6" data-rm-shortcode-name="rebelmouse-image" id="58b39" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-three-suited-men.jpg?id=29430590&width=980"/><p>As the organizer of the Manhattan Project and among the experts who advised President Truman on dropping atomic bombs on Japan, Bush became an important voice after the war in trying to calm an American public anxious about nuclear war. In doing so, he sometimes erred in his predictions, as when he dismissed, in a 1947 article in <em>The New York Times</em>, the widespread fears that nuclear weapons would someday be launched by pushing a button (which eventually became possible):</p><p>“Push-button warfare be damned. This talk has done a lot of harm.… Certainly there would be innovations…but it would not be push-button warfare in any sense. Some of this ‘Buck Rogers’ thinking is a lot of hooey.”</p><h3>Engineers Are Heroes</h3><br/><p>While scientists seemed to grab the glory in the popular press, Bush always sought to remind others that engineers are the true heroes. As he told a congressional committee on research in 1963:</p><p>“Much of what we call scientific research is really engineering research or even straight engineering. Putting a communications satellite into orbit, and working, is an engineering job, and incidentally a magnificent one.”</p><p class=""><em>Compiled by G. Pascal Zachary. The source of Bush’s quotations is </em><a href="https://www.amazon.com/dp/0231116438/ref=cm_sw_em_r_mt_dp_07YP0641GT5723YDZ066" target="_blank">The Essential Writings of Vannevar Bush</a><em>, edited by Zachary and published by Columbia University Press (2022). Quote No. 11 comes from </em>The New York Times<em>, 9 January 1947, “Push-Button Ideas of War Hit by Bush.”</em></p>]]></description><pubDate>Tue, 01 Mar 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/the-essential-vannevar-bush</guid><category>Vannevar bush</category><category>Engineering careers</category><category>Engineering heroes</category><category>Engineering history</category><dc:creator>G. Pascal Zachary</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-suited-man-at-a-desk-holding-a-pen-to-paper.jpg?id=29430571&amp;width=980"></media:content></item><item><title>How USB Came to Be</title><link>https://spectrum.ieee.org/how-usb-came-to-be</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/close-up-of-a-white-usb-cable-on-a-red-background-info-for-editor-if-needed-usb-cable-for-an-apple-g4-computer-2003-this-usb.jpg?id=29403591&width=1200&height=800&coordinates=0%2C83%2C0%2C84"/><br/><br/><p>Today even the least tech-savvy consumers can attach a camera, printer, scanner, or other accessory to their PC effortlessly. But in the early 1990s, attaching a peripheral to a computer was not so simple.</p><p>Before the development of USB (Universal Serial Bus), it was often tricky to connect external devices. Users sometimes needed to open up their computer and add hardware to give them the communications port they needed.</p><hr/><p>The Universal Serial Bus, which was released in 1996 by <a href="https://www.intel.com/content/www/us/en/homepage.html" rel="noopener noreferrer" target="_blank">Intel</a>, simplified things. USB ports now are standard on personal computers and are built into many other electronic devices such as smartphones, eBook readers, and game consoles.</p><p>The ubiquitous USB standard has been commemorated with an <a href="http://ieeemilestones.ethw.org/Main_Page" rel="noopener noreferrer" target="_blank">IEEE Milestone</a>. The <a href="http://www.ieee-oregon.org/" rel="noopener noreferrer" target="_blank">IEEE Oregon Section</a> sponsored the nomination. The dedication ceremony is still being planned.</p><p>Administered by the <a href="http://www.ieee.org/about/history_center/index.html" rel="noopener noreferrer" target="_blank">IEEE History Center</a> and <a href="https://www.ieeefoundation.org/donate_history" rel="noopener noreferrer" target="_blank">supported by donors</a>, the Milestone program recognizes outstanding technical developments around the world.</p><p>“I thought [USB] was a [one-off US] $40 million opportunity,” <a href="https://www.linkedin.com/in/ajay-bhatt-ab37b74" rel="noopener noreferrer" target="_blank">Ajay Bhatt</a>, one of the engineers at Intel who helped develop it, said in a 2013 <a href="https://www.cnn.com/2013/04/26/tech/innovation/usb-intel-billion-seller/index.html" rel="noopener noreferrer" target="_blank">interview with CNN</a> about the technology. “I couldn’t imagine where USB has gone or where it will continue to go. This has exceeded the wildest of my imaginations.”</p><p><strong>COLLABORATION IS KEY</strong></p><p>Many of the problems consumers encountered when they tried to attach peripherals to their computer in the 1990s arose because of the lack of standard practices among the industry’s many suppliers, as noted in the Milestone’s entry on the <a href="http://ieeemilestones.ethw.org/Milestone-Proposal:Universal_Serial_Bus_(USB)" rel="noopener noreferrer" target="_blank">Engineering and Technology History Wiki</a>. Another problem was that most PCs had a limited number of input ports, and adding more could be difficult.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A man in glasses smiles at the camera. In the foreground is a computer. Info for editor if needed: Ajay Bhatt, Chief Platform Architect at Mobile Platform Group, Intel Corporation poses for a profile shoot on January 20, 2010 in New Delhi, India." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="cf1031e25ae1c32893261f5fd6e0d302" data-rm-shortcode-name="rebelmouse-image" id="5cd8c" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-glasses-smiles-at-the-camera-in-the-foreground-is-a-computer-info-for-editor-if-needed-ajay-bhatt-chief-platform-ar.jpg?id=29403698&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Ajay Bhatt is one of the engineers at Intel who helped develop USB.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Pradeep Gaur/Mint/Getty Images</small></p><p>Bhatt says that even as a technologist, he struggled with upgrading his PC.</p><p>“I looked at the architecture, and I thought, ‘You know what? There are better ways of working with computers, and this is just too difficult,’” he said in a 2019 interview with <a href="https://www.fastcompany.com/3060705/an-oral-history-of-the-usb" rel="noopener noreferrer" target="_blank"><em>Fast Company</em></a>.</p><p>In the early 1990s, Bhatt told his boss about his idea of developing a universal “plug -and-play” communication system—something the user didn’t need to adjust. His manager wasn’t interested.</p><p class="pull-quote">“I couldn’t imagine where USB has gone or where it will continue to go. This has exceeded the wildest of my imaginations.”</p><p>Bhatt was passionate about his idea, though, so he decided to join a different research team at Intel. And there he was given the green light.</p><p>In 1992 Bhatt visited the <a href="https://foursquare.com/v/intel--jones-farm-conference-center/4b7429e6f964a520b3ca2de3" rel="noopener noreferrer" target="_blank">Jones Farm Conference Center</a>, in Hillsboro, Ore., where he met with engineers from different tech companies who also were looking into developing a plug-and-play scheme. It was there that engineers from <a href="https://www.compaq.com/" rel="noopener noreferrer" target="_blank">Compaq</a>, <a href="https://en.wikipedia.org/wiki/Digital_Equipment_Corporation" rel="noopener noreferrer" target="_blank">Digital Equipment Corp.</a> (DEC), <a href="https://history-computer.com/ibm-history/" rel="noopener noreferrer" target="_blank">IBM</a>, Intel, <a href="https://www.microsoft.com/" rel="noopener noreferrer" target="_blank">Microsoft</a>,<a href="https://en.wikipedia.org/wiki/Nortel" rel="noopener noreferrer" target="_blank"> </a><a href="https://www.necam.com/" rel="noopener noreferrer" target="_blank">NEC</a>, and<a href="https://www.necam.com/" rel="noopener noreferrer" target="_blank"> </a><a href="https://en.wikipedia.org/wiki/Nortel" rel="noopener noreferrer" target="_blank">Nortel</a> formed an alliance.</p><p>“The industry as a whole recognized that it had a big problem that needed to be solved,” <a href="http://www.pappas.me/Jim.htm" rel="noopener noreferrer" target="_blank">Jim Pappas</a> said in an <a href="https://www.intel.com/content/dam/www/public/us/en/documents/articles/usb-two-decades-of-plug-and-play-article.pdf" rel="noopener noreferrer" target="_blank">Intel article</a> on the USB interface. At the time of the meeting, Pappas was an engineering manager at DEC, but he eventually joined Intel as a program manager for its USB-development team.</p><p><strong>FROM DREAM TO REALITY</strong></p><p>Before the group began developing USB, it explored what was already available. It looked at Ethernet-like technologies, audio interfaces, Apple’s GeoPort, and <a href="https://standards.ieee.org/ieee/1394/4377/" rel="noopener noreferrer" target="_blank">IEEE 1394</a>—also known as the Firewire standard. But none had all the traits the team sought. In particular, the engineers wanted something that was inexpensive, user-friendly, able to power peripherals, and offered a lot of bandwidth.To keep manufacturing costs down, the engineers designed USB to work with a slender, four-conductor cable that could be as long as 5 meters. One end of the cable had an A connector, which plugged into the computer; the B connector on the other end plugged into the external device.</p><p>At the time, computers didn’t typically provide power for such external devices. Most peripherals had to be plugged into an outlet while connected to a PC. But USB allowed a computer to supply sufficient power for some peripherals.</p><p>Another advantage of USB was that, in principle, it allowed as many as 127 peripherals to be connected to one PC at a time. A single computer was unlikely to have 127 USB ports, but the number of ports available could be increased by adding USB hubs.</p><p>For a name, the development team looked for something users could relate to, and they also wanted it to describe the technology. The engineers chose the word <em>bus</em> because it was both a technical term (a bus is used to carry data in a computer) and one consumers would recognize, according to the <em>Fast Company</em> article. Buses, in most people’s minds, were vehicles that got passengers from Point A to Point B, <a href="https://www.linkedin.com/in/bala-cadambi-1503304" target="_blank">Bala Cadambi</a>, who worked on the USB development team, said in the 2019 interview. <em>Universal</em> marked the USB as a tool that could be used with any hardware.</p><p>The team announced its first design in 1995. At 12 megabits per second, USB 1.0 was “faster than anything else that normally would come at the back of the PC,” Pappas told <em>Fast Company</em>.</p><p>The team encountered a problem, though: 12 Mb/s was too fast for computer mouses, joysticks, keyboards, and other accessories with unshielded cables. The engineers solved the problem by arranging for USB 1.0 to support communications at 1.5 Mb/s as well.</p><p>That approach allowed USB to work at low speed for low-cost peripherals with unshielded cables and at high speed for devices with shielded cables, such as printers and floppy-disk drives.</p><p>USB 1.1, released in 1996, did not become popular until 1998, after it was showcased at the <a href="https://en.wikipedia.org/wiki/COMDEX" rel="noopener noreferrer" target="_blank">COMDEX</a> trade show in Las Vegas. At a news conference there, an Intel team attached <a href="https://www.intel.com/pressroom/archive/releases/1998/us111698.htm" rel="noopener noreferrer" target="_blank">127 peripherals to one PC</a>. The engineers hired <a href="https://billnye.com/" rel="noopener noreferrer" target="_blank">Bill Nye</a> to plug in the last of the devices. In Pappas’s 2019 <em>Fast Company</em> interview, he said that once Nye did so, the team sent a document to various destinations to print. “We had a whole stage full of different printers!” Pappas said.</p><p>The delay between the release of USB 1.1 in 1996 and when it caught on is understandable because Microsoft <a href="https://en.wikipedia.org/wiki/Windows_98" rel="noopener noreferrer" target="_blank">Windows 98</a>, which was released in June 1998, was the first operating system to support USB. Two months later, <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> released its iMac, which lacked a floppy-disk drive but did have a pair of USB ports. Although Apple was not among the companies that worked on the USB project, it helped make the technology mainstream.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A USB stick about to be connected to a computer" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="a5f1f2f38f6a1ded17d326b21d34cf32" data-rm-shortcode-name="rebelmouse-image" id="62539" loading="lazy" src="https://spectrum.ieee.org/media-library/a-usb-stick-about-to-be-connected-to-a-computer.jpg?id=29403716&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">USB ports are now standard in computers.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Mint/Getty Images</small></p><p>Since then, three more USB generations have emerged. The most recent, USB4, was released in 2019.<br/></p><p>“Who would have thought that a connector that we had defined in the early ’90s would still be usable today?” Cadambi said in the <em>Fast Company</em> interview. “That’s very rare. We had cost constraints, performance constraints. It was designed for a desktop, not a smartphone. Looking back at it, it was wonderful that we accomplished what we did, that it withstood the test of time.”</p><p>The Milestone plaque, which is to be displayed in the lobby of the Jones Farm Conference Center, reads:</p><p><em>An industry consortium published the first Universal Serial Bus (USB) specification in January 1996. Initially intended to simplify attaching electronic devices to a PC, USB became a very successful low-cost, high-speed interface for home and business use. Its ability to support new device classes and functionalities, including data storage, power delivery, and battery charging, has made USB’s cabling, connectors, and logo recognizable worldwide.</em></p>]]></description><pubDate>Tue, 22 Feb 2022 19:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/how-usb-came-to-be</guid><category>Usb</category><category>Ieee milestone</category><category>Intel</category><category>Microsoft</category><category>Apple</category><category>Ieee history center</category><category>Universal serial bus</category><category>Tech history</category><category>Ieee history</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/close-up-of-a-white-usb-cable-on-a-red-background-info-for-editor-if-needed-usb-cable-for-an-apple-g4-computer-2003-this-usb.jpg?id=29403591&amp;width=980"></media:content></item><item><title>Behind the Design of the Tron Videogame</title><link>https://spectrum.ieee.org/tron-game</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/screenshot-shows-a-blue-ring-with-6-pink-floating-disks-3-on-each-side-a-game-avatar-throws-a-disk-from-one-side-towards-an-av.jpg?id=28785019&width=1200&height=800&coordinates=0%2C196%2C0%2C196"/><br/><br/><p>In the <a href="https://movies.disney.com/tron" rel="noopener noreferrer" target="_blank">Walt Disney film <em>Tron</em></a>, an evil Master Control Program schemes to take over the Pentagon and the Kremlin, telling its human henchman that it can run them 900 to 1200 times more efficiently. In the <a href="https://en.wikipedia.org/wiki/Midway_Games" rel="noopener noreferrer" target="_blank">Midway Manufacturing Co.’s</a> Tron arcade game, a master control program, known as the executive, makes more efficient use of game programmers’ time by taking care of all of the routine functions of accepting quarters, recording game scores, switching between players, and displaying messages on the screen.</p><p>Before the standardized executive was developed, programmers would write the computer code for each game from scratch. The executive allows them to concentrate on writing the code for the game play, which is unique to each game.</p><h3></h3><br/><p>This article was first published as "Tron: the master control program takes over." It appeared in the December 1982 issue of <em>IEEE Spectrum </em>as part of a special report, “Video games: The electronic big bang.” A <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6366912" rel="noopener noreferrer" target="_blank">PDF version</a> is available on IEEE Xplore.</p><h3></h3><br/><p>While the executive must be altered somewhat for each game, it still saves a great deal of time. For example, when a new weakness is discovered in arcade games, it only needs to be dealt with once, according to John Pasierb, vice president for engineering at Midway.</p><p>“We were having a problem where if someone tampered with the game by turning the power switch on and off really quickly, our battery backup didn’t maintain the information,” noted Bill Adams, Midway’s manager of software development. “Normally, you can turn one of our games off, come back two weeks later, and it will still remember all the high scores.</p><p>The solution was to change the software, Mr. Pasierb said, adding, “We did this in the executive, and that solved the problem in all the games.”</p><p>The same executive was used by Midway in one game before Tron, and it is now being used for all arcade games the company has in production. Copies of the code have also been distributed to freelance programmers who develop games for Midway.</p>]]></description><pubDate>Sat, 19 Feb 2022 16:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/tron-game</guid><category>History of technology</category><category>Software</category><category>Design</category><category>Videogames</category><dc:creator>Paul Wallich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/screenshot-shows-a-blue-ring-with-6-pink-floating-disks-3-on-each-side-a-game-avatar-throws-a-disk-from-one-side-towards-an-av.jpg?id=28785019&amp;width=980"></media:content></item></channel></rss>