<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.DC updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Distributed, Parallel, and Cluster Computing (cs.DC) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2022-11-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Distributed, Parallel, and Cluster Computing</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.11831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.01718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.00529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.09775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17356" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2211.01592">
<title>Try to Avoid Attacks: A Federated Data Sanitization Defense for Healthcare IoMT Systems. (arXiv:2211.01592v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2211.01592</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare IoMT systems are becoming intelligent, miniaturized, and more
integrated into daily life. As for the distributed devices in the IoMT,
federated learning has become a topical area with cloud-based training
procedures when meeting data security. However, the distribution of IoMT has
the risk of protection from data poisoning attacks. Poisoned data can be
fabricated by falsifying medical data, which urges a security defense to IoMT
systems. Due to the lack of specific labels, the filtering of malicious data is
a unique unsupervised scenario. One of the main challenges is finding robust
data filtering methods for various poisoning attacks. This paper introduces a
Federated Data Sanitization Defense, a novel approach to protect the system
from data poisoning attacks. To solve this unsupervised problem, we first use
federated learning to project all the data to the subspace domain, allowing
unified feature mapping to be established since the data is stored locally.
Then we adopt the federated clustering to re-group their features to clarify
the poisoned data. The clustering is based on the consistent association of
data and its semantics. After we get the clustering of the private data, we do
the data sanitization with a simple yet efficient strategy. In the end, each
device of distributed ImOT is enabled to filter malicious data according to
federated data sanitization. Extensive experiments are conducted to evaluate
the efficacy of the proposed defense method against data poisoning attacks.
Further, we consider our approach in the different poisoning ratios and achieve
a high Accuracy and a low attack success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Ying Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Leyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siquan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01713">
<title>iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud. (arXiv:2211.01713v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01713</link>
<description rdf:parseType="Literal">&lt;p&gt;GPUs are essential to accelerating the latency-sensitive deep neural network
(DNN) inference workloads in cloud datacenters. To fully utilize GPU resources,
spatial sharing of GPUs among co-located DNN inference workloads becomes
increasingly compelling. However, GPU sharing inevitably brings severe
performance interference among co-located inference workloads, as motivated by
an empirical measurement study of DNN inference on EC2 GPU instances. While
existing works on guaranteeing inference performance service level objectives
(SLOs) focus on either temporal sharing of GPUs or reactive GPU resource
scaling and inference migration techniques, how to proactively mitigate such
severe performance interference has received comparatively little attention. In
this paper, we propose iGniter, an interference-aware GPU resource provisioning
framework for cost-efficiently achieving predictable DNN inference in the
cloud. iGniter is comprised of two key components: (1) a lightweight DNN
inference performance model, which leverages the system and workload metrics
that are practically accessible to capture the performance interference; (2) A
cost-efficient GPU resource provisioning strategy that jointly optimizes the
GPU resource allocation and adaptive batching based on our inference
performance model, with the aim of achieving predictable performance of DNN
inference workloads. We implement a prototype of iGniter based on the NVIDIA
Triton inference server hosted on EC2 GPU instances. Extensive prototype
experiments on four representative DNN models and datasets demonstrate that
iGniter can guarantee the performance SLOs of DNN inference workloads with
practically acceptable runtime overhead, while saving the monetary cost by up
to 25% in comparison to the state-of-the-art GPU resource provisioning
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiabin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_R/0/1/0/all/0/1&quot;&gt;Ruitao Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fangming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01725">
<title>Distributed Reconfiguration of Spanning Trees. (arXiv:2211.01725v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01725</link>
<description rdf:parseType="Literal">&lt;p&gt;In a reconfiguration problem, given a problem and two feasible solutions of
the problem, the task is to find a sequence of transformations to reach from
one solution to the other such that every intermediate state is also a feasible
solution to the problem. In this paper, we study the distributed spanning tree
reconfiguration problem and we define a new reconfiguration step, called
$k$-simultaneous add and delete, in which every node is allowed to add at most
$k$ edges and delete at most $k$ edges such that multiple nodes do not add or
delete the same edge.
&lt;/p&gt;
&lt;p&gt;We first observe that, if the two input spanning trees are rooted, then we
can do the reconfiguration using a single $1$-simultaneous add and delete step
in one round in the CONGEST model. Therefore, we focus our attention towards
unrooted spanning trees and show that transforming an unrooted spanning tree
into another using a single $1$-simultaneous add and delete step requires
$\Omega(n)$ rounds in the LOCAL model. We additionally show that transforming
an unrooted spanning tree into another using a single $2$-simultaneous add and
delete step can be done in $O(\log n)$ rounds in the CONGEST model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Siddharth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_S/0/1/0/all/0/1&quot;&gt;Shreyas Pai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01747">
<title>Comparison of Algorithms for Distributed Mutual Exclusion through Simulations. (arXiv:2211.01747v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01747</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show how different types of distributed mutual algorithms
can be compared in terms of performance through simulations. A simulation
approach is presented, together with an overview of the relevant evaluation
metrics and statistical processing of the results. The presented simulations
can be used to learn students of a course on distributed software the basics of
algorithms for distributed mutual exclusion, together with a detailed
comparison study. Finally, a related work section is provided with relevant
references which contain use cases where distributed mutual exclusion algorithm
can be beneficial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turck_F/0/1/0/all/0/1&quot;&gt;Filip De Turck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01805">
<title>FedMint: Intelligent Bilateral Client Selection in Federated Learning with Newcomer IoT Devices. (arXiv:2211.01805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01805</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a novel distributed privacy-preserving learning
paradigm, which enables the collaboration among several participants (e.g.,
Internet of Things devices) for the training of machine learning models.
However, selecting the participants that would contribute to this collaborative
training is highly challenging. Adopting a random selection strategy would
entail substantial problems due to the heterogeneity in terms of data quality,
and computational and communication resources across the participants. Although
several approaches have been proposed in the literature to overcome the problem
of random selection, most of these approaches follow a unilateral selection
strategy. In fact, they base their selection strategy on only the federated
server&apos;s side, while overlooking the interests of the client devices in the
process. To overcome this problem, we present in this paper FedMint, an
intelligent client selection approach for federated learning on IoT devices
using game theory and bootstrapping mechanism. Our solution involves the design
of: (1) preference functions for the client IoT devices and federated servers
to allow them to rank each other according to several factors such as accuracy
and price, (2) intelligent matching algorithms that take into account the
preferences of both parties in their design, and (3) bootstrapping technique
that capitalizes on the collaboration of multiple federated servers in order to
assign initial accuracy value for the newly connected IoT devices. Based on our
simulation findings, our strategy surpasses the VanillaFL selection approach in
terms of maximizing both the revenues of the client devices and accuracy of the
global federated learning model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wehbi_O/0/1/0/all/0/1&quot;&gt;Osama Wehbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arisdakessian_S/0/1/0/all/0/1&quot;&gt;Sarhad Arisdakessian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahab_O/0/1/0/all/0/1&quot;&gt;Omar Abdel Wahab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otrok_H/0/1/0/all/0/1&quot;&gt;Hadi Otrok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otoum_S/0/1/0/all/0/1&quot;&gt;Safa Otoum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mourad_A/0/1/0/all/0/1&quot;&gt;Azzam Mourad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1&quot;&gt;Mohsen Guizani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01840">
<title>LE3D: A Lightweight Ensemble Framework of Data Drift Detectors for Resource-Constrained Devices. (arXiv:2211.01840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2211.01840</link>
<description rdf:parseType="Literal">&lt;p&gt;Data integrity becomes paramount as the number of Internet of Things (IoT)
sensor deployments increases. Sensor data can be altered by benign causes or
malicious actions. Mechanisms that detect drifts and irregularities can prevent
disruptions and data bias in the state of an IoT application. This paper
presents LE3D, an ensemble framework of data drift estimators capable of
detecting abnormal sensor behaviours. Working collaboratively with surrounding
IoT devices, the type of drift (natural/abnormal) can also be identified and
reported to the end-user. The proposed framework is a lightweight and
unsupervised implementation able to run on resource-constrained IoT devices.
Our framework is also generalisable, adapting to new sensor streams and
environments with minimal online reconfiguration. We compare our method against
state-of-the-art ensemble data drift detection frameworks, evaluating both the
real-world detection accuracy as well as the resource utilisation of the
implementation. Experimenting with real-world data and emulated drifts, we show
the effectiveness of our method, which achieves up to 97% of detection accuracy
while requiring minimal resources to run.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavromatis_I/0/1/0/all/0/1&quot;&gt;Ioannis Mavromatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Mompo_A/0/1/0/all/0/1&quot;&gt;Adrian Sanchez-Mompo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raimondo_F/0/1/0/all/0/1&quot;&gt;Francesco Raimondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pope_J/0/1/0/all/0/1&quot;&gt;James Pope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bullo_M/0/1/0/all/0/1&quot;&gt;Marcello Bullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weeks_I/0/1/0/all/0/1&quot;&gt;Ingram Weeks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carnelli_P/0/1/0/all/0/1&quot;&gt;Pietro Carnelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oikonomou_G/0/1/0/all/0/1&quot;&gt;George Oikonomou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spyridopoulos_T/0/1/0/all/0/1&quot;&gt;Theodoros Spyridopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aftab Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01863">
<title>Edge, Fog, and Cloud Computing : An Overview on Challenges and Applications. (arXiv:2211.01863v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01863</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid growth of the Internet of Things (IoT) and a wide range of
mobile devices, the conventional cloud computing paradigm faces significant
challenges (high latency, bandwidth cost, etc.). Motivated by those constraints
and concerns for the future of the IoT, modern architectures are gearing toward
distributing the cloud computational resources to remote locations where most
end-devices are located. Edge and fog computing are considered as the key
enablers for applications where centralized cloud-based solutions are not
suitable. In this paper, we review the high-level definition of edge, fog,
cloud computing, and their configurations in various IoT scenarios. We further
discuss their interactions and collaborations in many applications such as
cloud offloading, smart cities, health care, and smart agriculture. Though
there are still challenges in the development of such distributed systems,
early research to tackle those limitations have also surfaced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1&quot;&gt;Thong Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_P/0/1/0/all/0/1&quot;&gt;Pranjal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajpai_G/0/1/0/all/0/1&quot;&gt;Gaurav Bajpai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashef_R/0/1/0/all/0/1&quot;&gt;Rasha Kashef&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01928">
<title>MPI-based Evaluation of Coordinator Election Algorithms. (arXiv:2211.01928v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2211.01928</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we detail how two types of distributed coordinator election
algorithms can be compared in terms of performance based on an evaluation on
the High Performance Computing (HPC) infrastructure. An experimental approach
based on an MPI (Message Passing Interface) implementation is presented, with
the goal to characterize the relevant evaluation metrics based on statistical
processing of the results. The presented approach can be used to learn master
students of a course on distributed software the basics of algorithms for
coordinator election, and how to conduct an experimental performance evaluation
study. Finally, use cases where distributed coordinator election algorithms are
useful are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turck_F/0/1/0/all/0/1&quot;&gt;Filip De Turck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01945">
<title>Distributed Maximal Matching and Maximal Independent Set on Hypergraphs. (arXiv:2211.01945v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2211.01945</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the distributed complexity of maximal matching and maximal
independent set (MIS) in hypergraphs in the LOCAL model. A maximal matching of
a hypergraph $H=(V_H,E_H)$ is a maximal disjoint set $M\subseteq E_H$ of
hyperedges and an MIS $S\subseteq V_H$ is a maximal set of nodes such that no
hyperedge is fully contained in $S$. Both problems can be solved by a simple
sequential greedy algorithm, which can be implemented naively in $O(\Delta r +
\log^* n)$ rounds, where $\Delta$ is the maximum degree, $r$ is the rank, and
$n$ is the number of nodes.
&lt;/p&gt;
&lt;p&gt;We show that for maximal matching, this naive algorithm is optimal in the
following sense. Any deterministic algorithm for solving the problem requires
$\Omega(\min\{\Delta r, \log_{\Delta r} n\})$ rounds, and any randomized one
requires $\Omega(\min\{\Delta r, \log_{\Delta r} \log n\})$ rounds. Hence, for
any algorithm with a complexity of the form $O(f(\Delta, r) + g(n))$, we have
$f(\Delta, r) \in \Omega(\Delta r)$ if $g(n)$ is not too large, and in
particular if $g(n) = \log^* n$ (which is the optimal asymptotic dependency on
$n$ due to Linial&apos;s lower bound [FOCS&apos;87]). Our lower bound proof is based on
the round elimination framework, and its structure is inspired by a new round
elimination fixed point that we give for the $\Delta$-vertex coloring problem
in hypergraphs.
&lt;/p&gt;
&lt;p&gt;For the MIS problem on hypergraphs, we show that for $\Delta\ll r$, there are
significant improvements over the naive $O(\Delta r + \log^* n)$-round
algorithm. We give two deterministic algorithms for the problem. We show that a
hypergraph MIS can be computed in $O(\Delta^2\cdot\log r + \Delta\cdot\log
r\cdot \log^* r + \log^* n)$ rounds. We further show that at the cost of a
worse dependency on $\Delta$, the dependency on $r$ can be removed almost
entirely, by giving an algorithm with complexity $\Delta^{O(\Delta)}\cdot\log^*
r + O(\log^* n)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balliu_A/0/1/0/all/0/1&quot;&gt;Alkida Balliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1&quot;&gt;Sebastian Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_F/0/1/0/all/0/1&quot;&gt;Fabian Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivetti_D/0/1/0/all/0/1&quot;&gt;Dennis Olivetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.11831">
<title>DBS: Dynamic Batch Size For Distributed Deep Neural Network Training. (arXiv:2007.11831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2007.11831</link>
<description rdf:parseType="Literal">&lt;p&gt;Synchronous strategies with data parallelism, such as the Synchronous
StochasticGradient Descent (S-SGD) and the model averaging methods, are widely
utilizedin distributed training of Deep Neural Networks (DNNs), largely owing
to itseasy implementation yet promising performance. Particularly, each worker
ofthe cluster hosts a copy of the DNN and an evenly divided share of the
datasetwith the fixed mini-batch size, to keep the training of DNNs
convergence. In thestrategies, the workers with different computational
capability, need to wait foreach other because of the synchronization and
delays in network transmission,which will inevitably result in the
high-performance workers wasting computation.Consequently, the utilization of
the cluster is relatively low. To alleviate thisissue, we propose the Dynamic
Batch Size (DBS) strategy for the distributedtraining of DNNs. Specifically,
the performance of each worker is evaluatedfirst based on the fact in the
previous epoch, and then the batch size and datasetpartition are dynamically
adjusted in consideration of the current performanceof the worker, thereby
improving the utilization of the cluster. To verify theeffectiveness of the
proposed strategy, extensive experiments have been conducted,and the
experimental results indicate that the proposed strategy can fully utilizethe
performance of the cluster, reduce the training time, and have good
robustnesswith disturbance by irrelevant tasks. Furthermore, rigorous
theoretical analysis hasalso been provided to prove the convergence of the
proposed strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Mingjia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.01718">
<title>Communication Efficient Generalized Tensor Factorization for Decentralized Healthcare Networks. (arXiv:2109.01718v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.01718</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor factorization has been proved as an efficient unsupervised learning
approach for health data analysis, especially for computational phenotyping,
where the high-dimensional Electronic Health Records (EHRs) with patients&apos;
history of medical procedures, medications, diagnosis, lab tests, etc., are
converted to meaningful and interpretable medical concepts. Federated tensor
factorization distributes the tensor computation to multiple workers under the
coordination of a central server, which enables jointly learning the phenotypes
across multiple hospitals while preserving the privacy of the patient
information. However, existing federated tensor factorization algorithms
encounter the single-point-failure issue with the involvement of the central
server, which is not only easily exposed to external attacks but also limits
the number of clients sharing information with the server under restricted
uplink bandwidth. In this paper, we propose CiderTF, a communication-efficient
decentralized generalized tensor factorization, which reduces the uplink
communication cost by leveraging a four-level communication reduction strategy
designed for a generalized tensor factorization, which has the flexibility of
modeling different tensor distribution with multiple kinds of loss functions.
Experiments on two real-world EHR datasets demonstrate that CiderTF achieves
comparable convergence with a communication reduction up to 99.99%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiuchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jian Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Li Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhavani_S/0/1/0/all/0/1&quot;&gt;Sivasubramanium Bhavani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1&quot;&gt;Joyce C. Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12433">
<title>FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12433</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for training models on graphs distributed across multiple clients
have recently grown in popularity, due to the size of these graphs as well as
regulations on keeping data where it is generated, like GDPR in the EU.
However, a single connected graph cannot be disjointly partitioned onto
multiple distributed clients due to the cross-client edges connecting graph
nodes. Thus, distributed methods for training a model on a single graph incur
either significant communication overhead between clients or a loss of
available information to the training. We introduce the Federated Graph
Convolutional Network (FedGCN) algorithm, which uses federated learning to
train GCN models for semi-supervised node classification on large graphs with
fast convergence and little communication. Compared to prior methods that
require communication among clients at each training round, FedGCN clients only
communicate with the central server in one pre-training step, greatly reducing
communication costs. We theoretically analyze the tradeoff between FedGCN&apos;s
convergence rate and communication cost under different data distributions and
introduce a general framework that can be used for analysis of all
edge-completion-based GCN training algorithms. Experimental results show that
our FedGCN algorithm achieves 51.7% faster convergence on average and at least
100X less communication cost compared to prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Weizhao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;Srivatsan Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1&quot;&gt;Carlee Joe-Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.00529">
<title>Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top. (arXiv:2206.00529v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.00529</link>
<description rdf:parseType="Literal">&lt;p&gt;Byzantine-robustness has been gaining a lot of attention due to the growth of
the interest in collaborative and federated learning. However, many fruitful
directions, such as the usage of variance reduction for achieving robustness
and communication compression for reducing communication costs, remain weakly
explored in the field. This work addresses this gap and proposes Byz-VR-MARINA
- a new Byzantine-tolerant method with variance reduction and compression. A
key message of our paper is that variance reduction is key to fighting
Byzantine workers more effectively. At the same time, communication compression
is a bonus that makes the process more communication efficient. We derive
theoretical convergence guarantees for Byz-VR-MARINA outperforming previous
state-of-the-art for general non-convex and Polyak-Lojasiewicz loss functions.
Unlike the concurrent Byzantine-robust methods with variance reduction and/or
compression, our complexity results are tight and do not rely on restrictive
assumptions such as boundedness of the gradients or limited compression.
Moreover, we provide the first analysis of a Byzantine-tolerant method
supporting non-uniform sampling of stochastic gradients. Numerical experiments
corroborate our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1&quot;&gt;Eduard Gorbunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1&quot;&gt;Samuel Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1&quot;&gt;Gauthier Gidel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.09775">
<title>FedToken: Tokenized Incentives for Data Contribution in Federated Learning. (arXiv:2209.09775v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.09775</link>
<description rdf:parseType="Literal">&lt;p&gt;Incentives that compensate for the involved costs in the decentralized
training of a Federated Learning (FL) model act as a key stimulus for clients&apos;
long-term participation. However, it is challenging to convince clients for
quality participation in FL due to the absence of: (i) full information on the
client&apos;s data quality and properties; (ii) the value of client&apos;s data
contributions; and (iii) the trusted mechanism for monetary incentive offers.
This often leads to poor efficiency in training and communication. While
several works focus on strategic incentive designs and client selection to
overcome this problem, there is a major knowledge gap in terms of an overall
design tailored to the foreseen digital economy, including Web 3.0, while
simultaneously meeting the learning objectives. To address this gap, we propose
a contribution-based tokenized incentive scheme, namely \texttt{FedToken},
backed by blockchain technology that ensures fair allocation of tokens amongst
the clients that corresponds to the valuation of their data during model
training. Leveraging the engineered Shapley-based scheme, we first approximate
the contribution of local models during model aggregation, then strategically
schedule clients lowering the communication rounds for convergence and anchor
ways to allocate \emph{affordable} tokens under a constrained monetary budget.
Extensive simulations demonstrate the efficacy of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1&quot;&gt;Shashi Raj Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Lam Duc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1&quot;&gt;Petar Popovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11703">
<title>SCL: A Secure Concurrency Layer For Paranoid Stateful Lambdas. (arXiv:2210.11703v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11703</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a federated Function-as-a-Service (FaaS) execution model that
provides secure and stateful execution in both Cloud and Edge environments. The
FaaS workers, called Paranoid Stateful Lambdas (PSLs), collaborate with one
another to perform large parallel computations. We exploit cryptographically
hardened and mobile bundles of data, called DataCapsules, to provide persistent
state for our PSLs, whose execution is protected using hardware-secured TEEs.
To make PSLs easy to program and performant, we build the familiar Key-Value
Store interface on top of DataCapsules in a way that allows amortization of
cryptographic operations. We demonstrate PSLs functioning in an edge
environment running on a group of Intel NUCs with SGXv2.
&lt;/p&gt;
&lt;p&gt;As described, our Secure Concurrency Layer (SCL), provides
eventually-consistent semantics over written values using untrusted and
unordered multicast. All SCL communication is encrypted, unforgeable, and
private. For durability, updates are recorded in replicated DataCapsules, which
are append-only cryptographically-hardened blockchain with confidentiality,
integrity, and provenance guarantees. Values for inactive keys are stored in a
log-structured merge-tree (LSM) in the same DataCapsule. SCL features a variety
of communication optimizations, such as an efficient message passing framework
that reduces the latency up to 44x from the Intel SGX SDK, and an actor-based
cryptographic processing architecture that batches cryptographic operations and
increases throughput by 81x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1&quot;&gt;Alexander Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hanming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullen_W/0/1/0/all/0/1&quot;&gt;William Mullen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1&quot;&gt;Jeffery Ichnowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arya_R/0/1/0/all/0/1&quot;&gt;Rahul Arya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnakumar_N/0/1/0/all/0/1&quot;&gt;Nivedha Krishnakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_R/0/1/0/all/0/1&quot;&gt;Ryan Teoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Willis Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_A/0/1/0/all/0/1&quot;&gt;Anthony Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubiatowicz_J/0/1/0/all/0/1&quot;&gt;John Kubiatowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17356">
<title>IoT System Case Study: Personal Office Energy Monitor (POEM). (arXiv:2210.17356v1 [cs.DC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2210.17356</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the design, implementation, and user evaluation of an
IoT project focused on monitoring and management of user comfort and energy
usage in office buildings. The objective is to depict an instructive use case
and to illustrate experiences with all major phases of designing and running a
fairly complex IoT system. The design part includes motivation and outline of
the problem statement, the resulting definition of data to be collected, system
implementation, and subsequent changes resulting from the additional insights
that it provided. The user experience part describes quantitative findings as
well as key results of the extensive human factors study with over 70 office
users participating in two major pilots in France and Japan. The original idea
for this project came out from a diverse group of companies exploring
challenges of designing and operating smart buildings with net-positive energy
balance. Members included companies involved in the design and construction of
smart buildings, building-management and automation systems, computer design,
energy systems, and office furniture and space design. One of the early
insights was that maximum energy efficiency in office buildings cannot be
achieved and sustained without the awareness and active participation of
building occupants. The resulting project explored and evaluated several ways
to engage and empower users in ways that benefit them and makes them the
willing and active participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milenkovic_M/0/1/0/all/0/1&quot;&gt;Milan Milenkovic&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>