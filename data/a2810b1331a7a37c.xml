<?xml version="1.0" encoding="UTF-8"?>
<!--Generated by Site-Server v6.0.0-d62144382d0ca4260808a807acee0539a6396e4b-1 (http://www.squarespace.com) on Fri, 04 Nov 2022 21:22:16 GMT
--><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://www.rssboard.org/media-rss" version="2.0"><channel><title>Blog - OWNML</title><link>https://www.ownml.co/blog/</link><lastBuildDate>Tue, 27 Apr 2021 20:26:46 +0000</lastBuildDate><language>en-US</language><generator>Site-Server v6.0.0-d62144382d0ca4260808a807acee0539a6396e4b-1 (http://www.squarespace.com)</generator><description><![CDATA[]]></description><item><title>Machine Learning Canvas v1.1: change log</title><dc:creator>Louis Dorard</dc:creator><pubDate>Wed, 28 Apr 2021 17:06:04 +0000</pubDate><link>https://www.ownml.co/blog/machine-learning-canvas-v11-change-log</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:60887386ac36b74a3cde84e3</guid><description><![CDATA[<p class="">Earlier this month (April 2021), I released version&nbsp;v1.1&nbsp;of the Machine Learning Canvas. It’s grown more stable in the last couple of years, so last November, instead of updating from&nbsp;v0.4&nbsp;to v0.5, I decided to go straight to v1.0. After a couple of months of experience with it, coaching a few companies in their usage of MLC v1.0, I made a couple of tweaks and ended up with v1.1.</p><p class="">Let's have a look at all those changes since v0.4. I’ve changed the ordering of some of the boxes, in a way that makes the structure of the MLC easier to understand (and memorize). I’ve also refined the boxes’ headers and prompts, to help fill the MLC in the right way.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          <a class="
                sqs-block-image-link
                
          
        
              " href="https://www.ownml.co/machine-learning-canvas"
              
          >
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1619556468841-1PPJRP1JTKXNR5JQ50BS/MLC_v0.4_v1.1.jpeg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt="MLC_v0.4_v1.1.jpeg" data-load="false" data-image-id="6088787138903865432d5dc8" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1619556468841-1PPJRP1JTKXNR5JQ50BS/MLC_v0.4_v1.1.jpeg?format=1000w" />
            
          
        
          </a>
        

        
      
        </figure>
      

    
  


  


<hr />

<p class=""><a href="https://www.ownml.co/machine-learning-canvas"><strong>Get the latest version of the Machine Learning Canvas</strong></a></p>


<hr />

<p class="">These changes have a slight impact on the scope of the MLC. Since v0.4, it's become increasingly important to adopt a user-centered and responsible approach when designing intelligent systems that are based on ML: .</p><ul data-rte-list="default"><li><p class="">User-centered design → there is now more emphasis on the end-user, their workflow, and the interface with the system, under <em>Value Proposition</em>.</p></li><li><p class="">Responsible ML/AI:</p><ul data-rte-list="default"><li><p class=""><em>Offline Evaluation</em> now mentions fairness constraint</p></li><li><p class=""><em>Building Models</em> now mentions their “analysis”, which should include global model explanations</p></li><li><p class=""><em>Making Predictions</em> now mentions the post-processing of predictions, which can include single prediction explanations.</p></li></ul></li></ul><h2><strong>Changes to the layout: reordering</strong></h2><ul data-rte-list="default"><li><p class=""><em>Decisions</em> and <em>Data Collection</em> relate to the integration of the ML system in the domain of application. They were brought closer to the center of the MLC, next to <em>Value Proposition</em>. This also makes sense because the left-most box is <em>ML Task</em>, then followed by <em>Decisions</em>, which makes the connection with how value is provided.</p></li><li><p class=""><em>Making Predictions</em> and <em>Building Models</em> relate to the predictive engine which is at the core of the ML system. Same story here, they were also brought closer to the center. Doing this also gives more space for a long list of <em>Features</em>, and for <em>Offline Evaluation</em>, which typically contains a lot of information; you can also end that block with information on the minimum performance value, for metrics that can be computed offline, but could also be monitored live (and <em>Live Monitoring</em> is right next to the bottom of that box!).</p></li></ul><h2><strong>Changes to the headers and prompts</strong></h2><p class="">Here is a review of the 10 boxes of the MLC, highlighting changes to the headers and prompts.</p><ul data-rte-list="default"><li><p class=""><strong>Value Proposition:</strong></p><ul data-rte-list="default"><li><p class="">Before (v0.4): <em>What are we trying to do for the end-user(s) of the predictive system? What objectives are we serving?</em></p></li><li><p class="">After (v1.0 onwards): <em>Who is the end-user? What are their objectives? </em><strong><em>How will they benefit from the ML system? Mention workflow/interfaces.</em></strong></p></li></ul></li><li><p class=""><strong>Live Monitoring → Monitoring:</strong></p><ul data-rte-list="default"><li><p class="">Before: <em>Methods and metrics to evaluate the system after deployment, and to quantify value creation.</em></p></li><li><p class="">After: <em>Metrics to quantify value creation and measure the ML system’s </em><strong><em>impact in production (on end-users and business)</em></strong></p></li></ul></li><li><p class=""><strong>ML Task </strong>→<strong> Prediction Task: </strong>Changed the terminology. "Input" was interpreted by some as values manipulated by the machine, whereas what was meant was the real-world object, or "entity", on which predictions are to be made. "Output" was replaced by "outcome", which also sounds less technical and closer to something that's observed in the real world. Chose to focus on prediction tasks, instead of covering all ML tasks (including unsupervised learning). ML-powered predictions is how most of the value is created with "AI" today. This focus allows to highlight the time dimension of things, and the need to wait: from the moment an entity is observed and a prediction is requested, to the moment an outcome of interest can be observed.</p><ul data-rte-list="default"><li><p class="">Before (v0.4): <em>Input, output to predict, type of problem.</em></p></li><li><p class="">v1.0: <em>Type of task? Input object? </em><strong><em>Output: definition, parameters (e.g. prediction horizon), possible values?</em></strong></p></li><li><p class="">After (v1.1): <em>Type of task? </em><strong><em>Entity on which predictions are made? Possible outcomes? Wait time before observation?</em></strong></p></li></ul></li><li><p class=""><strong>Features:</strong> Emphasized that they should be <strong>available at prediction time</strong>.</p></li><li><p class=""><strong>Data sources: </strong>Made it a bit more specific by making a distinction between input data (aka "entities" in Prediction Task) and output data (aka "observed outcomes"), and suggesting to mention databases and tables, or APIs and methods of interest.</p><ul data-rte-list="default"><li><p class="">Before (v0.4): <em>Which raw data sources can we use (internal and external)?</em></p></li><li><p class="">v1.0: <em>Which raw data sources can we use (internal, external)? Mention databases and tables, or APIs and methods of interest.</em></p></li><li><p class="">After (v1.1): <em>Where can we get </em><strong><em>(raw) information on entities and observed outcomes</em></strong><em>? Mention </em><strong><em>database tables, API methods, websites to scrape, etc.</em></strong></p></li></ul></li><li><p class=""><strong>Data collection (brand new prompt!): </strong>Added a focus on continuous data collection (not just initial train set). Highlighted the differences in input and output collection (aka occurence of new entities and observation of outcomes), the cost of the latter, and finally, holding production entities out of the decision process (to deal with feedback loops).</p><ul data-rte-list="default"><li><p class="">Before (v0.4): <em>How do we get new data to learn from (inputs and outputs)?</em></p></li><li><p class="">v1.0: <strong><em>Strategy for initial train set, and continuous update. Collection rate?</em></strong><em> Holdout on prod inputs? Output acquisition cost?</em></p></li><li><p class="">After (v1.1): <em>Strategy for initial train set and continuous update. Mention collection rate, </em><strong><em>holdout on production entities, cost/constraints to observe outcomes</em></strong><em>.</em></p></li></ul></li><li><p class=""><strong>Making Predictions: </strong>This was already mentioning the featurization that needs to happen before predictions, and now it also mentions the post-processing, which could include prediction explanations, or preparing predictions for usage in the decision-making process. I’ve also added a focus on the compute target (which leads to considering things such as memory constraints, in addition to latency constraints)</p><ul data-rte-list="default"><li><p class="">Before: <em>When do we make predictions on new inputs? How long do we have to featurize a new input and make a prediction?</em></p></li><li><p class="">After: <em>When do we make </em><strong><em>real-time / batch</em></strong><em> predictions? Time available for this + featurization + </em><strong><em>post-processing</em></strong><em>? </em><strong><em>Compute target</em></strong><em>?</em></p></li></ul></li><li><p class=""><strong>Building Models: </strong>Clarified so that you get to think about how many models are needed, e.g. 1 per end-user, 1 per locality, etc. Similarly to <em>Making Predictions</em>, there was a mention of the featurization that needs to happen before model building, and now there’s also a mention of the analysis of the model that needs to happen afterwards. This includes things such as computing global model explanations, and testing the model to see if it can be safely deployed. The compute target is not mentioned here, because it’s likely to be more flexible than for making predictions, and less of a bottleneck.</p><ul data-rte-list="default"><li><p class="">Before: <em>When do we create/update models with new training data? How long do we have to featurize training inputs and create a model?</em></p></li><li><p class="">After: <strong><em>How many production models are needed?</em></strong><em> When would we update? Time available for this (including featurization and </em><strong><em>analysis</em></strong><em>)?</em></p></li></ul></li><li><p class=""><strong>Offline Evaluation → Impact Simulation (+ brand new prompt!): </strong>This wasn’t specific enough, but the new version now highlights that this is about decide whether it’s ok to deploy in production or not, and simulating the impact of predictions + decisions. Note that we want to evaluate decisions, not just predictions. This simulation is described via the test data on which predictions will be made (how is it collected? over which period of time?) and via the cost and gain values associated to (in)correct decisions (it’s best to avoid “abstract” metrics and to focus instead on domain-specific ones).</p><ul data-rte-list="default"><li><p class="">Before (v0.4): <em>Methods and metrics to evaluate the system before deployment.</em></p></li><li><p class="">v1.0: <em>Simulation of the impact of decisions/predictions? Which test data? Cost/gain values? Deployment criteria (min performance value, fairness)?</em></p></li><li><p class="">After (v1.1): <em>Can models be deployed? Which test data to assess performance? Cost/gain values for (in)correct predictions? </em><a href="https://developers.google.com/machine-learning/glossary#fairness-constraint" target="_blank"><em>Fairness constraint</em></a><em>?</em></p></li></ul></li></ul><p class="">Looking forward to seeing you make the best use of this new version of the MLC!</p>


<hr />

<p class=""><a href="https://www.ownml.co/machine-learning-canvas"><strong>Get the latest version of the Machine Learning Canvas</strong></a></p>


&nbsp;]]></description></item><item><title>Introducing the Prediction Task Canvas</title><dc:creator>Louis Dorard</dc:creator><pubDate>Wed, 10 Mar 2021 16:05:11 +0000</pubDate><link>https://www.ownml.co/blog/introducing-the-prediction-task-canvas</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:6048eb63d164797fcae7fb91</guid><description><![CDATA[When applying Machine Learning in your company, the 1st challenge is to 
formalize a Prediction Task that connects to a Value Proposition. I’ve 
created a new tool to help do this.]]></description><content:encoded><![CDATA[<h3>Formalizing prediction tasks that lead to value creation</h3><p class="">When applying Machine Learning in your company, the 1st challenge is to formalize a Prediction Task that connects to a Value Proposition. I’ve created a new tool to help do this.</p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1615391796811-SIJNHYFXG9S3Y39684MR/Prediction+Task+Canvas.png" data-image-dimensions="1476x1014" data-image-focal-point="0.5,0.5" alt="Prediction Task Canvas.png" data-load="false" data-image-id="6048ec34458be87d2776e0ac" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1615391796811-SIJNHYFXG9S3Y39684MR/Prediction+Task+Canvas.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  




<p class="">In the same way that the Value Proposition Canvas helps formulate a Value Proposition, which is one aspect of the Business Model Canvas, the Prediction Task Canvas helps detail one aspect of the&nbsp;<a href="http://machinelearningcanvas.com/" target="_blank">Machine Learning Canvas</a>, which is the Prediction Task.</p><p class="">There are 4 parts to it:</p><ul data-rte-list="default"><li><p class="">ENTITY: Object on which a prediction is to be made.</p></li><li><p class="">WAIT: Duration (or event) until an outcome can be observed.</p></li><li><p class="">OUTCOMES: Entity aspect (or events) we can only observe after waiting.</p></li><li><p class="">HEURISTIC: What’s a simple way to predict the outcome, when we first observe the entity?</p></li></ul><p class="">There’s also a time axis that helps see that the outcome we want to predict at time T can only be observed at time T+W.</p><p class="">The heuristic is useful to provide a baseline that we need our predictive model to beat. It’s also a good way to start thinking about what aspects of the entity need to be taken into account when making a prediction.</p><p class=""></p><p class="">Here are some simple examples, just to give you a better idea:</p><ul data-rte-list="default"><li><p class=""><strong>Gmail's Priority Inbox</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: email</p></li><li><p class="">WAIT for end of next user session</p></li><li><p class="">Possible OUTCOMES to observe / predict: email discarded without opening, or email opened and archived / replied to / left in inbox</p></li><li><p class="">HEURISTIC: if sender is in address book, email will be opened</p></li></ul></li><li><p class=""><strong>Real-estate price prediction</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: property</p></li><li><p class="">WAIT for sale</p></li><li><p class="">OUTCOME: transaction price</p></li><li><p class="">HEURISTIC: linear function of surface</p></li></ul></li><li><p class=""><strong>Fake review detection</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: review</p></li><li><p class="">WAIT for hand-labelling, or visitor to report fake review</p></li><li><p class="">OUTCOMES: ‘fake’ or ‘real’</p></li></ul></li><li><p class=""><strong>Credit risk</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: credit application + bank customer</p></li><li><p class="">WAIT for 2 years</p></li><li><p class="">OUTCOMES: no repayment delay, or all delays &lt; 90 days, or 1 delay of 90+ days</p></li></ul></li><li><p class=""><strong>Email marketing</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: subscriber</p></li><li><p class="">WAIT for them to open email and consider purchase</p></li><li><p class="">OUTCOMES: sale, unsubscribe, or none</p></li></ul></li><li><p class=""><strong>Churn</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: customer</p></li><li><p class="">WAIT for subscription to end in 15 days</p></li><li><p class="">OUTCOMES: renewal or cancellation</p></li></ul></li><li><p class=""><strong>Fraud Detection</strong></p><ul data-rte-list="default"><li><p class="">ENTITY: transaction</p></li><li><p class="">WAIT for 45 days, or hand-labelling</p></li><li><p class="">OUTCOMES: chargeback or none</p></li></ul></li></ul><p class=""></p>


















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          <a class="
                sqs-block-image-link
                
          
        
              " href="http://machinelearningcanvas.com"
              
          >
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1615391852240-T1P3M5GIKVR1QNFEHI9Y/Machine+Learning+Canvas+v1.1.jpg" data-image-dimensions="1584x1224" data-image-focal-point="0.5,0.5" alt="Machine Learning Canvas v1.1.jpg" data-load="false" data-image-id="6048ec6b97b21936f28c3cc7" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1615391852240-T1P3M5GIKVR1QNFEHI9Y/Machine+Learning+Canvas+v1.1.jpg?format=1000w" />
            
          
        
          </a>
        

        
      
        </figure>
      

    
  


  




<p class="">The type of possible outcomes determines the nature of your prediction task, e.g. regression, binary classification, N-class classification... Looking back at the bigger picture, in the Machine Learning Canvas there’s a Decisions block in between Prediction Task and Value Proposition: it’s where you would describe the process for turning the output of the prediction task into value for the end-user.</p><p data-rte-preserve-empty="true" class=""></p><p class="">I hope this will new tool will be as useful to you as it's been for me — let me know in the comments if so!</p>


&nbsp;]]></content:encoded><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1615392297102-MA5S9SZ29BX6ZQ61GOBA/Prediction+Task+Canvas.png?format=1500w" medium="image" isDefault="true" width="1476" height="1014"><media:title type="plain">Introducing the Prediction Task Canvas</media:title></media:content></item><item><title>The investor's approach to ML projects</title><dc:creator>Louis Dorard</dc:creator><pubDate>Fri, 06 Nov 2020 16:11:12 +0000</pubDate><link>https://www.ownml.co/blog/investors-approach-to-machine-learning-projects</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:5f6cb338b8f7ec6492e30cbb</guid><description><![CDATA[There are many opportunities to create value with ML: increasing 
productivity, avoiding undesirable events, automating repetitive tasks... 
But there are also many sources of cost and uncertainty. ML projects can 
feel like games of poker: you need to pay to see if you've got a winning 
idea, and you should avoid going all-in without strong odds in your favor! 
How can you figure out your odds, maximize chances of success, and minimize 
costs? By thinking like an investor! Here are 9 steps…]]></description><content:encoded><![CDATA[<figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602692107760-K7GPOSWKXO3UDNCKC6QQ/keenan-constance-VTLcvV6UVaI-unsplash.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" alt="Photo by Keenan Constance on Unsplash" data-load="false" data-image-id="5f872409ba7a483b51f2165d" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602692107760-K7GPOSWKXO3UDNCKC6QQ/keenan-constance-VTLcvV6UVaI-unsplash.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Photo by <a href="https://unsplash.com/@keenangrams?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Keenan Constance</a> on <a href="https://unsplash.com/s/photos/poker?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>
          </figcaption>
        
      
        </figure>
      

    
  


  


<p>How do you set Machine Learning projects for success?</p>
<p>There are many opportunities to create value with ML: increasing productivity, avoiding undesirable events, automating repetitive tasks... But there are also many sources of cost and uncertainty. ML projects can feel like games of poker: you need to pay to see if you've got a winning idea, and you should avoid going all-in without strong odds in your favor!</p>
<p>How can you <strong>figure out your odds</strong>, <strong>maximize chances of success</strong>, and <strong>minimize costs</strong>? By thinking like an investor! Here are 9 steps:</p>
<ol>
<li>Write the plan</li>
<li>Conduct customer/user studies</li>
<li>Set up Proof of Value with simulations and AutoML</li>
<li>Invest in data to increase performance</li>
<li>Shadow-deploy a Minimum Viable Product</li>
<li>Update model with new data</li>
<li>Canary-test and monitor</li>
<li>Invest in larger-scale deployment</li>
<li>Increase value / generate new value</li>
</ol>

&nbsp;<h2 id="1-write-the-plan">1. Write the plan</h2>
















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
              
              
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1601637438929-0YMGCCEBP4KTQ8VAVKU4/Machine+Learning+Canvas+template+v0.4.jpg" data-image-dimensions="2500x1932" data-image-focal-point="0.5,0.5" alt="Machine Learning Canvas template v0.4.jpg" data-load="false" data-image-id="5f770c3d2dae5f1f6dca8a16" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1601637438929-0YMGCCEBP4KTQ8VAVKU4/Machine+Learning+Canvas+template+v0.4.jpg?format=1000w" />
            
          
        
            
          
        

        
      
        </figure>
      

    
  


  


<p>When starting a new business, you’d use the business model canvas. For ML projects, use the <a href="https://www.ownml.co/machine-learning-canvas">ML Canvas</a>. It helps connect a value proposition to a prediction problem, identify bottlenecks, anticipate costs, and define the desired performance level.</p>
<p>Here are some questions you'll need to answer:</p>
<ul>
<li>Which value are you proposing, for which end-user?</li>
<li>Which prediction problem are you targeting?</li>
<li>How are you turning predictions into the proposed value? (Tip: ask yourself "what would I do if I already had a perfect model?")</li>
<li>How are you changing the end-user workflow?</li>
<li>How will you monitor the impact, quantify, and prove that value is created?</li>
</ul>
<p>The MLC helps formalize your plan, which you can use to better communicate with others, and convince them to join your efforts. Starting with the MLC is <a href="https://aws.amazon.com/blogs/apn/building-the-business-case-for-machine-learning-in-the-real-world/">recommended practice at AWS</a>. There are a few other frameworks with similar names, but what's unique with this one is that it helps anticipate running costs of the ML system you set out to build. For instance, you'll have to think about the volume of models and predictions to create, and this will determine infrastructure costs.</p>

&nbsp;















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604599451310-5XAR4DV2KR4718W9XS24/wizardofoz.jpg" data-image-dimensions="640x360" data-image-focal-point="0.5,0.5" alt="wizardofoz.jpg" data-load="false" data-image-id="5fa43e9ba12889310b6a5ee8" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604599451310-5XAR4DV2KR4718W9XS24/wizardofoz.jpg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  


<h2 id="2-conduct-customer-user-studies">2. Conduct customer/user studies</h2>
<p>Wouldn't it be great if you could make sure that end-users of your ML system will be able to use it — and that they'll want to — before you start building? For this, you can use mockups and run "wizard-of-oz" experiments, where you fake the ML system. It's been <a href="https://medium.com/google-design/human-centered-machine-learning-a770d10562cd">recommended practice at Google for years</a>.</p>
<p>Early UX research can also result in a better plan that factors in feedback loops (those could hurt in the long term) and new data collection opportunities.</p>

&nbsp;<h2 id="3-set-up-proof-of-value-with-simulations-and-automl">3. Set up Proof of Value with simulations and AutoML</h2>
















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          <a class="
                sqs-block-image-link
                
          
        
              " href="https://builtin.com/artificial-intelligence/artificial-intelligence-automotive-industry"
              
          >
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604599130437-46NUJM5BU389WTHKPZ6D/image-asset.jpeg" data-image-dimensions="1200x630" data-image-focal-point="0.5,0.5" alt="If ML was a car, AutoML would be Tesla’s Autopilot! Actually, H2o.ai’s solution is even called Driverless AI…" data-load="false" data-image-id="5fa43d5afdc1165c0be072e5" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604599130437-46NUJM5BU389WTHKPZ6D/image-asset.jpeg?format=1000w" />
            
          
        
          </a>
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">If ML was a car, AutoML would be Tesla’s Autopilot! Actually, H2o.ai’s solution is even called <a href="https://www.h2o.ai/products/h2o-driverless-ai/">Driverless AI</a>…</p>
          </figcaption>
        
      
        </figure>
      

    
  


  


<p>Before investing too much time/money/efforts, you'll want to create a baseline model with minimal costs, and in the quickest way possible. This means small data (e.g. 50 examples of each class, for a classification problem), automated ML, and no data scientist!</p>
<p>You will then proceed to proving that this baseline model can create value, by running it through a simulation (aka “offline evaluation”). The idea is to see how much value it would create, by making correct predictions on a "test" dataset. For this, you'll be using cost/gain values for the different types of incorrect/correct predictions.</p>
<p>One way to get a test dataset is to split your existing data into training and test sets, but there's a risk of "data leakage". The most secure option is to collect some more data, and to use it as test.</p>
<p>If you've reached your desired performance value, great! Go to step 5.</p>

&nbsp;















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602522249592-8OG67A3QV5TCIWFNJWZ2/Learning%2Bcurve%2B%252B%2Bdrawing%2B%252B%2Bplayer.jpg" data-image-dimensions="1058x684" data-image-focal-point="0.5,0.5" alt="Learning curve extrapolation" data-load="false" data-image-id="5f848c89fc98834485f02380" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602522249592-8OG67A3QV5TCIWFNJWZ2/Learning%2Bcurve%2B%252B%2Bdrawing%2B%252B%2Bplayer.jpg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p class="">Learning curve extrapolation</p>
          </figcaption>
        
      
        </figure>
      

    
  


  


<h2 id="4-invest-in-data-to-increase-performance">4. Invest in data to increase performance</h2>
<p>Where should you invest, to increase performance? Common ideas are to: a) tune the ML algorithm, b) collect more data, c) improve data preparation.</p>
<p>One of the highlights of my survey results on <a href="https://www.linkedin.com/pulse/survey-results-what-managers-should-know-machine-learning-dorard/">what managers should know about ML</a> was Sébastien Arnaud's "<em>invest in data, and activities around it: cleaning, analysis, processing pipeline, annotation, labelling, training/test set, validation</em>." Indeed, the most impactful improvements to your ML system will be coming from data collection and preparation. This means you should forget about a).</p>
<p>To choose between b) and c) you need to plot a <em>learning curve</em>. This consists in running the previous simulation again but with subsets of the training data that go from 10% to 100% in size (see illustration). You would look at how performance increases with the amount of training data that's available. Also consider how much time/money it took to acquire this training set (getting unbiased data can come at a cost, e.g. randomly approving transactions or loan applications). What happens if you extrapolate to more data?</p>
<p>How do you generate ideas to improve data preparation? The key is to inspect individual predictions among top errors, top predictions, and most uncertain ones. Prioritize ideas by grouping errors in different types, and counting occurrences of each type. Inspecting predictions also helps determine if you can trust the model: look at prediction explanations and check that they make sense.</p>
<p>Go through the implementation/inspection loop a few times, until performance is higher than the desired value. If you can't get there quickly enough, consider changing parameters of your prediction problem (e.g. reducing the time horizon).</p>

&nbsp;















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1601637651959-NHYL1IQBA4PXJ3JJKJOM/ML+system+architecture.jpg" data-image-dimensions="1811x2039" data-image-focal-point="0.5,0.5" alt="ML system architecture.jpg" data-load="false" data-image-id="5f770d1215e48c4fbd5b9576" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1601637651959-NHYL1IQBA4PXJ3JJKJOM/ML+system+architecture.jpg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  


<h2 id="5-shadow-deploy-a-minimum-viable-product">5. Shadow-deploy a Minimum Viable Product</h2>
<p>"Shadow-deployment" is the same as real deployment in production, except you don’t actually use predictions but you just log them. This allows to check whether model performance on production inputs is similar to what it was on test inputs. You'll also want to monitor performance through time, to see how fast it decreases, and thus to inform model refresh rate (i.e. updating with fresher data).</p>
<p>In the context of ML, a Minimum Viable Product (MVP) isn't just a model, but it's a <em>system</em> that's made of several components — read more in <a href="https://www.linkedin.com/feed/update/urn:li:activity:6717822425423245313/">this post</a> and see how these components are interconnected in the diagram above. You should use an ML platform to get (and configure) a production-ready model builder and a server, and you should start with manual orchestration (this doesn't need to be a software component at first).</p>
<p>Finally, use shadow-deployment to check all running costs (ML platform / tools / infrastructure / compute), and see if there are any additional ones you hadn’t anticipated, when going from a lab environment to production.</p>

&nbsp;<h2 id="6-update-model-with-new-data">6. Update model with new data</h2>
<p>As hinted above, you can expect performance to go down with time. This is because the dataset used to build your model becomes less and less representative of the reality, as time goes by. You'll need to periodically retrain your model with fresher data.</p>
<p>In the previous section, I advised to start with manual orchestration, which means manual (re)training and deployment. This should be fine in most cases, but you should test your ability to do this in a timely manner and without errors. This will also be an opportunity to collect new inputs and outputs and thus to confirm the cost of data collection.</p>

&nbsp;<h2 id="7-canary-test-and-monitor">7. Canary-test and monitor</h2>
<p>I already mentioned monitoring in step 5, but this was about the model's performance. You'll also want to monitor the impact of your ML system, and make sure that you're system is creating value. But for this, you need to act on predictions and to integrate them into your end-users' app / workflow. You can start doing this for just a small subset of them (the "canary test"), to minimize the risk of "breaking" things with your new ML system.</p>

&nbsp;















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
              
              
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602582161706-I4G31Z2JRLLTKNBXFO2V/Value+vs+Cost.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt="Value vs Cost.jpg" data-load="false" data-image-id="5f857690ae879c7bde657085" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1602582161706-I4G31Z2JRLLTKNBXFO2V/Value+vs+Cost.jpg?format=1000w" />
            
          
        
            
          
        

        
      
        </figure>
      

    
  


  


<h2 id="8-invest-in-larger-scale-deployment">8. Invest in larger scale deployment</h2>
<p>Larger scale means increasing the subset of end-users who will be exposed to the new ML system / product. This means more predictions, higher running costs, changing the workflow of more people...</p>
<p>These costs have to be paid before getting gains, but you’ve proved that your MVP creates value, so eventually it will pay off!</p>

&nbsp;<h2 id="9-increase-value-or-generate-new-value-">9. Increase value or generate new value?</h2>
<p>If you're looking to invest more in ML, you have two options: 1) improve the current system and thus increase the value it creates, or 2) generate new value with a new ML use case.</p>
<h3 id="increase-value">Increase value</h3>
<p>Now's the right time to hire data scientists to <strong>tune modelling algorithms</strong>, or ML engineers to <strong>automate orchestration</strong>!</p>
<ul>
<li>Modelling improvements will result in higher performance, thus more value creation.</li>
<li>The idea with automation is to spend less time maintaining the system in production. You'll be setting up alerts (based on live performance, data drift detection...), triggers, and actions (train model, evaluate, deploy).</li>
</ul>
<h3 id="generate-new-value">Generate new value</h3>
<p>Targeting a new ML use case might provide a higher Return On Investment, if you <strong>make your ML assets reusable</strong>: reusing your previous data acquisition and preparation pipelines for other ML use cases involving the same input objects will greatly reduce the cost of deploying an MVP. This is something that Uber does with their "feature store", where you can reuse feature representations of drivers across use cases, for instance.</p>

&nbsp;















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          <a class="
                sqs-block-image-link
                
          
        
              " href="https://www.louisdorard.com/own-machine-learning"
              
          >
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604597072275-KYDXHSGUYIEDXGGW0CSL/cover.png" data-image-dimensions="1600x1000" data-image-focal-point="0.5,0.5" alt="cover.png" data-load="false" data-image-id="5fa43549a80c7c291da9b320" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604597072275-KYDXHSGUYIEDXGGW0CSL/cover.png?format=1000w" />
            
          
        
          </a>
        

        
      
        </figure>
      

    
  


  


<h2 id="10-bonus-invest-in-ml-education">10. (Bonus) Invest in ML education</h2>
<p>In the end, Machine Learning is software. But it's a very different paradigm than "regular" software. If you want to be smart about how you allocate ressources to execute ML projects successfully, you need to adopt the right mindset and you need the right amount of practical knowledge (but don't trouble yourself with books on ML algorithms and fancy Python libraries).</p>
<p>I created a hands-on course that covers the above 9 steps in more detail. It's targeted at leaders, innovators, managers, entrepreneurs, domain experts, and consultants. It teaches the skills to lead ML projects more effectively, and to build ML systems that create value: it's called <strong><a href="https://www.ownml.co/own-machine-learning">Own Machine Learning</a></strong>!</p>
<p>As Warren Buffet said, “there’s one investment that supersedes all others: invest in yourself.” Here, you'll need to invest 15 hours of your time to watch the course's videos, and about the same to do the exercices. This is the equivalent of 4 days of work (obviously, I invested way more time to acquire that same knowledge I'm passing on 😉). If you're a consultant, this is time that you won't be billing. But it will pay off by making you better at selling and leading ML projects, and faster at executing them successfully!</p>

















  

    
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604602997886-U404SP9WAP44L2B9QFHF/5stars.png" data-image-dimensions="173x44" data-image-focal-point="0.5,0.5" alt="5stars.png" data-load="false" data-image-id="5fa44c75c2877d06aad39eb9" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1604602997886-U404SP9WAP44L2B9QFHF/5stars.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  


  






<figure class="block-animation-none">
  <blockquote data-animation-role="quote"
  >
    <span>&#147;</span>I can build my own ML projects, and it feels great<span>&#148;</span>
  </blockquote>
  <figcaption class="source">&mdash; <a href="https://www.louisdorard.com/own-machine-learning#review-widget">Yonni S., UX lead</a></figcaption>
</figure>



  <a href="https://www.ownml.co/own-machine-learning" class="sqs-block-button-element--small sqs-button-element--tertiary sqs-block-button-element"
    
  >
    Get the course
  </a>

&nbsp;]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1601637015896-AXXRFCFSM8I0LKZVXE8U/keenan-constance-VTLcvV6UVaI-unsplash.jpg?format=1500w" medium="image" isDefault="true" width="1500" height="1000"><media:title type="plain">The investor's approach to ML projects</media:title></media:content></item><item><title>From Data to AI with the Machine Learning Canvas (Part I)</title><dc:creator>Louis Dorard</dc:creator><pubDate>Fri, 18 Nov 2016 11:15:00 +0000</pubDate><link>https://medium.com/louis-dorard/from-data-to-ai-with-the-machine-learning-canvas-part-i-d171b867b047#.q62mj04k0</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:58834289a5790abf8b6b5127</guid><description><![CDATA[A framework to connect the dots between data collection, machine learning, 
and value creation]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/from-data-to-ai-with-the-machine-learning-canvas-part-i">Permalink</a><p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1484997301531-DX00V0OD8023KJDYG22X/1-BNhz7GIrXnNAVKhgd9YYlA.jpeg?format=1500w" medium="image" isDefault="true" width="1500" height="844"><media:title type="plain">From Data to AI with the Machine Learning Canvas (Part I)</media:title></media:content></item><item><title>What developers actually need to know about Machine Learning</title><dc:creator>Louis Dorard</dc:creator><pubDate>Wed, 06 Apr 2016 10:09:00 +0000</pubDate><link>https://stories.papis.io/what-developers-actually-need-to-know-about-machine-learning-1a3ea2284631#.oovlxiah3</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:588340feebbd1a30e98107c2</guid><description><![CDATA[Something is wrong in the way ML is being taught to developers...]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/what-developers-need-to-know-about-machine-learning">Permalink</a><p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1484996983324-NKGQ4J0VKMQECN8PGYSD/1-2y9j-K0M94B-Amph516d8w.jpeg?format=1500w" medium="image" isDefault="true" width="1500" height="660"><media:title type="plain">What developers actually need to know about Machine Learning</media:title></media:content></item><item><title>Trusting AI with important decisions: capabilities and challenges</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 04 Jan 2016 11:12:00 +0000</pubDate><link>https://stories.papis.io/trusting-ai-with-important-decisions-capabilities-and-challenges-18952422914f#.3y57n0ww6</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:588341e959cc684854aabba0</guid><description><![CDATA[Artificial Intelligence has become increasingly present in our lives in the 
form of tools like smartphone apps. It can also be found in high-stakes 
autonomous systems where it makes decisions that involve the lives of human 
beings — such as Autonomous Vehicles (e.g. the “Google Car”) — or that 
involve important amounts of money — such as automated investment systems. 
AI can increase our productivity and creativity, or it can replace human 
intervention altogether by making better decisions, both in everyday life 
and in business. There is strong potential in AI-powered automation, but 
also important issues to address such as control, morality, and market 
uptake. Let’s dive in…]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/trusting-ai-with-important-decisions">Permalink</a><p>]]></content:encoded><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1484997162405-4UVPNI4YVI4JPSZ0BHIM/1-hTUW_1A8zqyu22RtT58s2g.png?format=1500w" medium="image" isDefault="true" width="1500" height="597"><media:title type="plain">Trusting AI with important decisions: capabilities and challenges</media:title></media:content></item><item><title>How to choose a machine learning API to build predictive apps</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 20 Jul 2015 13:49:15 +0000</pubDate><link>https://medium.com/@louisdorard/developer-considerations-for-choosing-a-machine-learning-api-20e2de15eb3a</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:55acfbcbe4b01d02078d6ec8</guid><description><![CDATA[Two years ago, Mike Gualtieri of Forrester Research coined the term 
“predictive applications” and pitched it as the “next big thing in app 
development”. Today, some people estimate that more than 50% of the apps on 
a typical smartphone have predictive features. Predictive apps were defined 
by Gualtieri as “apps that provide the right functionality and content at 
the right time, for the right person, by continuously learning about them 
and predicting what they’ll need.” For that, they use Machine Learning (ML) 
techniques and data. APIs such as the ones provided by Amazon Machine 
Learning, BigML, Google Prediction API and PredicSis all promise to make it 
easy for developers to apply ML to data and thus to add predictive features 
to their apps, but it’s not obvious how these APIs differ from one another 
and how to choose the right one based on your apps’ needs...]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/how-to-choose-a-machine-learning-api">Permalink</a><p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1437400134956-3MTE04N0J5CKSOE097UN/image-asset.jpeg?format=1500w" medium="image" isDefault="true" width="1500" height="1125"><media:title type="plain">How to choose a machine learning API to build predictive apps</media:title></media:content></item><item><title>Machine learning APIs: which performs best?</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 04 May 2015 13:09:30 +0000</pubDate><link>https://www.ownml.co/blog/machine-learning-apis-comparison</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:55476086e4b08ca26f80dbcd</guid><description><![CDATA[Amazon Machine Learning made a lot of noise when it came out last month. 
Shortly afterwards, someone posted a link to Google Prediction API on 
HackerNews and it quickly became one of the most popular’s posts. Google’s 
product is quite similar to Amazon’s but it’s actually much older since it 
was introduced in 2011. Anyway, this gave me the idea of comparing the 
performance of Amazon’s new ML API with that of Google. For that, I used 
the Kaggle “give me some credit” challenge. But I didn’t stop there: I also 
included startups who provide competing APIs in this comparison — namely, 
PredicSis and BigML. In this wave of new ML services, the giant tech 
companies are getting all the headlines, but bigger companies do not 
necessarily have better products. Here's how I compared them and which 
results I got… ]]></description><content:encoded><![CDATA[<p><a href="http://aws.amazon.com/machine-learning/">Amazon ML</a> (Machine Learning) made a lot of noise <a href="http://www.infoq.com/news/2015/04/aws-launches-machine-learning">when it came out</a> last month. Shortly afterwards, someone posted a link to <a href="https://cloud.google.com/prediction/">Google Prediction API</a> on HackerNews and it quickly became one of the most popular’s posts. Google’s product is quite similar to Amazon’s but it’s actually much older since it was introduced in 2011. Anyway, this gave me the idea of comparing the performance of Amazon’s new ML API with that of Google. For that, I used the <a href="https://www.kaggle.com/c/GiveMeSomeCredit">Kaggle “give me some credit” challenge</a>. But I didn’t stop there: I also included startups who provide competing APIs in this comparison — namely, <a href="http://www.predicsis.com">PredicSis</a> and <a href="http://www.bigml.com">BigML</a>. In this wave of new ML services, the giant tech companies are getting all the headlines, but bigger companies do not necessarily have better products. Here's how I compared them and which results I got… (if you just want the summary of my results, jump to the bottom of the article).</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/machinelearning?src=hash">#machinelearning</a> <a href="https://twitter.com/hashtag/APIs?src=hash">#APIs</a> perf.: <a href="https://twitter.com/amazon">@Amazon</a> most accurate, <a href="https://twitter.com/bigmlcom">@bigmlcom</a> fastest, <a href="https://twitter.com/PredicSis">@PredicSis</a> best trade-off, <a href="https://twitter.com/google">@Google</a> last <a href="http://t.co/m6ISYmIgAd">http://t.co/m6ISYmIgAd</a></p>— Louis Dorard (@louisdorard) <a href="https://twitter.com/louisdorard/status/595219158775771136">May 4, 2015</a></blockquote>
<h2 id="methodology">Methodology</h2>















 

  
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          <a class="
                sqs-block-image-link
                
          
        
              " href="https://www.kaggle.com/c/GiveMeSomeCredit"
              
          >
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1430745680157-D9GH3W9C1K5SKYH7ALXM/front_page.png" data-image-dimensions="213x100" data-image-focal-point="0.5,0.5" alt="Image courtesy of Kaggle.com" data-load="false" data-image-id="5547724fe4b03724d71659a5" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1430745680157-D9GH3W9C1K5SKYH7ALXM/front_page.png?format=1000w" />
            
          
        
          </a>
        

        
          
          <figcaption class="image-caption-wrapper">
            <p>Image courtesy of Kaggle.com</p>
          </figcaption>
        
      
        </figure>
      

    
  



<p>The ML problem in the Kaggle credit challenge is a binary classification one: you’re given a dataset of input-output pairs where each input corresponds to an individual who has applied for a credit and the output says whether he later defaulted or not. The idea is to use ML to predict whether a new individual applying for a credit will default.</p><p>ML has two phases: train and predict. The “train” phase consists in using a set of input-output examples to create a model that maps inputs to outputs. The “predict” phase consists in using the model on new inputs to get predictions of the associated outputs. Amazon ML, Google Prediction API, PredicSis and BigML all have similar API methods for each phase:</p><ul>
<li>One method that takes in a dataset (in csv format for instance), and that returns the id of a model trained on this dataset</li>
<li>One method that takes a model id and an input, and that returns a prediction.</li>
</ul>















 

  
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1430745522663-6CWS092FEWXX2DCSZAZD/image-asset.jpeg" data-image-dimensions="1786x797" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="554771abe4b0deabe55d5faa" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1430745522663-6CWS092FEWXX2DCSZAZD/image-asset.jpeg?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  



<p>All 4 services offer free accounts which I used for this comparison (note: PredicSis is still in private beta but you can <a href="http://launch.predicsis.com/">request an account here</a>). In this post, I will only compare the performance of these two methods and I won't consider other aspects such as pricing, features, DX, UX, etc.</p>

<p>In order to evaluate the models produced by the APIs, we need to separate our dataset downloaded from Kaggle in two: a training set which we use to create a model, and an evaluation set. We apply the model to the inputs of the evalution set and we get a prediction for each input. We can evaluate the accuracy of the model by comparing the predicted output with the true output (which was held out).</p>

<p>The dataset we start with contains 150,000 instances and weighs 7.2 MB. I randomly selected 90% of the dataset for training and in the remaining 10% I randomly selected 5,000 inputs for evaluation.</p>

<h2 id="results">Results</h2>

<p>For each API, there are three things to measure: the time taken by each method and the accuracy of predictions made by the model. For accuracy, I used the same performance measure as that of the Kaggle challenge, which is called <a href="http://www.kaggle.com/wiki/AreaUnderCurve">AUC</a>. I won’t explain what it is here, but what you have to know about AUC is that a) performance values are between 0 and 1, b) a random classifier would have an AUC of around 0.5, c) a perfect classifier would have an AUC of 1. As a consequence, the higher the AUC, the better the model. </p>

<center>
<table class="tg">
  <tr>
    <th class="tg-031e"></th>
    <th class="tg-e3zv">Amazon</th>
    <th class="tg-e3zv">Google</th>
    <th class="tg-e3zv">PredicSis</th>
    <th class="tg-e3zv">BigML</th>
  </tr>
  <tr>
    <td class="tg-031e">Accuracy (AUC)</td>
    <td class="tg-031e">0.862</td>
    <td class="tg-031e">0.743</td>
    <td class="tg-031e">0.858</td>
    <td class="tg-031e">0.853</td>
  </tr>
  <tr>
    <td class="tg-031e">Time for training (s)</td>
    <td class="tg-031e">135</td>
    <td class="tg-031e">76</td>
    <td class="tg-031e">17</td>
    <td class="tg-031e">5</td>
  </tr>
  <tr>
    <td class="tg-031e">Time for predictions (s)</td>
    <td class="tg-031e">188</td>
    <td class="tg-031e">369</td>
    <td class="tg-031e">5</td>
    <td class="tg-031e">1</td>
  </tr>
</table>
</center><p><strong><em>UPDATE - NEW BIGML RESULTS:</em></strong><em> As pointed out by Francisco Martin, if you just change the objective field (SeriousDlqin2yrs) to be numeric instead of categorical, BigML's accuracy for a <a href="https://bigml.com/shared/model/joejTuzK2F33nUZBI9WxF7o8vtZ">single model</a> goes to 0.8530 (whereas it was initially reported as 0.790 - the accuracy in the table above and the Kaggle rank below have been updated to reflect that).</em></p>

<p>Times for predictions correspond to 5,000 predictions. FYI, the top entry on the leaderboard had an AUC of 0.870. If you’d used these APIs in the Kaggle competition, here’s the approximate rank you could have had:</p>

<ul>
<li>#60 for Amazon</li>
<li>#570 for PredicSis</li>
<li>#636 for BigML</li>
<li>#810 for Google</li>
</ul>

<p>It’s important to note that, depending on your application, some of these 3 performance measures will be more critical than others. The leaderboard for this Kaggle challenge doesn’t take time into account, but for certain applications where you have to make predictions at high frequence (for instance when you want to predict if a user is going to click on an ad, for every user coming to a high-traffic website), prediction time will be super critical.</p>
<hr /><h2 id="summary">Summary</h2>

<p><strong><em>DISCLAIMER</em></strong><em>: this comparison was performed with a real-world dataset, but you may get different results with another dataset. You should try these APIs with your own data to figure out which is the best for you!</em></p>

<ul>
<li>PredicSis offered the best trade-off between accuracy and speed by being the second fastest and second most accurate</li>
<li>BigML was the fastest in both training and predictions, but less accurate</li>
<li>Amazon was the most accurate, but at the cost of being the slowest in training and also very slow in predictions</li>
<li>Google was last on accuracy and prediction time</li>
</ul>

<h2 id="towardsanactualbenchmark">Towards an actual benchmark</h2>

<p>This was a very simple comparison but it’s still a bit far from an actual benchmark. One of the first things I’d like to do to improve on this would be to make it easy for others to reproduce (and verify) these results. I used the web interfaces of these services to get the AUC values and it would be better to have code that computes AUC locally. For now, you can check out this <a href="https://github.com/louisdorard/papiseval">repo for evaluating ML/prediction APIs</a>. Pull requests are welcome! (e.g. new APIs, new evaluation metrics, etc.)</p>

<p>In a future benchmark, it would be interesting to also try regression problems, and to try various types of datasets: small, big, unbalanced, etc.</p>

<h2 id="learnmore">Learn more</h2>

<ul>
<li>If you’d like to learn more about PredicSis and BigML, they will both be at <a href="http://www.papis.io/connect">PAPIs Connect</a> on 21 May in Paris — come join us!</li>
<li>BigML will also be at <a href="http://mediterranea.apidays.io">APIdays Mediterranea</a> on 7 May in Barcelona with an exciting talk by their CTO on the future of ML APIs.</li>
<li>I’m giving away free tickets to both conferences! <a href="http://goo.gl/forms/9OFNvxunZH">Sign up here for PAPIs Connect</a> and <a href="http://goo.gl/forms/n0Tz33rVqb">here for APIdays Mediterranea</a>.</li>
<li>With these new ML/prediction APIs, I’m thinking of updating my book, <a href="https://www.ownml.co/machine-learning-book">Bootstrapping Machine Learning</a>, in which I already covered Google Prediction and BigML… But until then, you might be interested in checking out an excerpt of the current edition in my Machine Learning Starter Kit!</li>
</ul>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe>

<p>Enjoyed this article? <a href="http://www.twitter.com/louisdorard">Follow me!</a></p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1430741419102-1DLQSHUSOW3MTB9OQ8SC/image-asset.jpeg?format=1500w" medium="image" isDefault="true" width="960" height="692"><media:title type="plain">Machine learning APIs: which performs best?</media:title></media:content></item><item><title>9 ways Data Science can improve your business</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 12 Jan 2015 13:57:00 +0000</pubDate><link>https://www.ownml.co/blog/data-science-business</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:548b07e7e4b04308d753bc76</guid><description><![CDATA[There's a number of ways you could be using Data Science (DS) in your 
business. To manage your DS projects efficiently and have them deliver real 
value to your business, you should have a good overview of what DS can help 
you with and how. I've listed 9 things that I've grouped in 3 areas:

- I. Increasing the number of customers
- II. Serving customers better
- III. Serving customers more efficiently

Data Science can provide help in each area with the use of Machine Learning 
techniques. The idea is to map situations to outcomes by analyzing data, so 
we can then predict outcomes in new situations. Let’s see how this is used 
concretely... ]]></description><content:encoded><![CDATA[<p>There's a number of ways you could be using Data Science (DS) in your business. To manage your DS projects efficiently and have them deliver real value to your business, you should have a good overview of what DS can help you with and how. I've listed 9 things, but let's first go back to some business fundamentals to see how this list is structured...</p>

<p>Businesses serve customers. Better businesses serve more customers, they serve them better, and in a more efficient way. How well they’re doing that can be measured with revenue and costs, or simply with revenue-costs=profit. The different ways we can improve a business (increase profits) can thus be divided in 3 areas:</p>

<ul>
<li>I. Increasing the number of customers</li>
<li>II. Serving customers better</li>
<li>III. Serving customers more efficiently</li>
</ul>

<p>Data Science can help in these 3 areas through the use of "supervised" Machine Learning (ML) techniques. The idea is to map situations to outcomes, so we can predict outcomes in new situations. One example is predicting whether a customer who's presented a product (situation) will show interest in it or not (outcome). For that, Machine Learning techniques need examples to work with: situations (characterized as finely as possible, along with any contextual information) and outcomes observed in these situations. An analysis of the example data allows to find patterns, and thus relationships between situations and outcomes. Predictions on outcomes are made automatically by using these relationships. Let’s see how this is used in a business context… </p>

<h2 id="iincreasingthenumberofcustomers">I. Increasing the number of customers</h2>

<p>There are two things that affect the number of customers: new customers coming and existing customers leaving.</p>

<ul>
<li><strong>1. Reducing customer attrition (a.k.a. “churn”)</strong>: We use historical customer data to map snapshots of customers taken at a given point in time (who they are, what they bought, how they interacted with products/services sold to them) to the fact they later churned or not (see my <a href="https://blog.kissmetrics.com/improve-by-predicting-churn/">Kissmetrics article for more practical information</a>). Then, for each current customer we look at the likelihood of (predicted) churn, how valuable the customer is, and we take action to prevent the most valuable customers to churn.</li>
<li><strong>2. Acquiring new customers</strong>:
<ul><li><strong>Lead scoring</strong>: We map customers to revenue in order to predict the revenue we would get (if any) from a new prospective customer.</li>
<li><strong>Optimizing marketing campaigns</strong>: We map lead-campaign combinations to conversion indicators. Then, for a given lead we predict the conversion likelihood with each possible campaign, we multiply that by the predicted revenue for that lead, factor in the cost of the campaign, and thus we can predict Return On Investment.</li></ul></li>
</ul>

<h2 id="iiservingcustomersbetter">II. Serving customers better</h2>

<p>Serving customers well implies offering products they’re interested in and can afford, or providing services they engage with.</p>

<ul>
<li><strong>3. Cross-selling products</strong>: We map customer-product pairs to purchase indicator (as recorded in historical data), so we know who to target when launching a new product or when promoting an existing one (you can also optimize promotional campaigns in the same way as seen previously).</li>
<li><strong>4. Optimizing products and pricing</strong>: We map product characterizations (including price) to numbers of sales, so we can change the price and other characteristics to see how they impact revenue (price * number of sales) and thus to see what would work best.</li>
<li><strong>5. Increasing engagement</strong>: We observe customer behavior when presented "items", in order to map customer-item pairs to interest indicators. We can then predict needs and interests&nbsp;and take them into account in the service provided to the customer.</li>
</ul>

<h2 id="iiiservingcustomersmoreefficiently">III. Serving customers more efficiently</h2>

<p>Here, we want to improve operations in order to reduce costs. Besides, serving customers efficiently often translates into serving customers well — for instance when we’re able to anticipate issues or to better handle customer support requests.</p>

<ul>
<li><strong>6. Predicting demand</strong>: This is important to businesses that observe high variability in demand for their services/products, for instance businesses who sell perishable goods: they need to avoid having too much or not enough in stock. The idea is to measure demand and the context in which it occurred, so we can map context to demand. This can also be used in order to decide how much staff to hire in anticipation of how busy the business is going to be.</li>
<li><strong>7. Automating tasks</strong>: We can save time&nbsp;by having machines perform certain "intelligent" tasks automatically. Sometimes we already perform these tasks with hand-crafted rules, but we can also have the machine automatically learn (better?) rules from example data. One example is scoring credit applications or insurance claims, where the outcome is <em>approve</em> or <em>reject</em>. Performing risk analyses automatically from historical data can also be of interest to many, before investing time and money into new projects.</li>
<li><p><strong>8. Making enterprise apps predictive</strong>: Apps used by employees for customer relationship management, enterprise resource planning, human resources, etc., can help them do their job more efficiently by embedding predictive features that... </p>

<ul><li><strong>Prioritize things</strong>, to direct user focus to what's most important. This can be email (think Google Priority Inbox), customer support requests (to reply more quickly to the important ones),  mentions of your company you should reply to urgently, etc. The "outcome" for the corresponding learning tasks is <em>important</em> or <em>not important</em>.</li>
<li><strong>Use adaptive workflows</strong> based on predictions rather than on manual rules. This can be for instance to route customer support requests to whom will treat them best, in which case the outcome is a customer support team or person.</li>
<li><strong>Adapt the interface</strong> to show just what users need at the time they use the app, by mapping a context to an action that will need to be performed.</li>
<li><strong>Set configurations and preferences automatically</strong> by analyzing application usage data.</li></ul>

<p>As Katherine Barr, Partner at VC-firm MDV <a href="http://techcrunch.com/2014/12/31/7-venture-capitalists-predict-what-will-happen-in-2015/?ncid=txtlnkusaolp00000629">said in TechCrunch</a>: "Pairing human workers with machine learning and automation will transform knowledge work and unleash new levels of human productivity and creativity."</p></li>
</ul>

<h2 id="othermachinelearningtechniques">Other machine learning techniques</h2>

<p>Learning relationships between situations and outcomes is referred to as "supervised learning". The "supervision" is you telling the system that there is a particular outcome you're interested in. Machine learning techniques can also be used when there is no such outcome, in order to find structure in the data. In that case, we can just talk of "data points" instead of situations and outcomes, and we talk of "unsupervised learning" techniques. Here are two example applications of unsupervised learning:</p>

<ul>
<li><strong>9. Predictive maintenance</strong>: Each data point represents the state of a system at a given point in time, and we're interested in detecting states that are abnormal. The idea is that these states would later lead to issues/failures that would require considerable attention to be dealt with. Identifying anomalies in the state of a system allows to better focus the system's maintenance efforts by taking action before failure happens. It is another way of serving customers better and more efficiently. Anomaly detection is also used for fraud detection in credit card companies, where it is preferable to identify suspicious transactions and block them, rather than to reimburse the victim of the fraud after it's happened.</li>
<li><strong>Segmentation (e.g. customer)</strong>: Each data point is a customer, you specify a number of segments (a.k.a. clusters) and the machine analyzes customer data to assign a segment to each customer, based on similarities between them. This allows to better understand the different types of customers there are. Segmentation can be used in any of the 9 goals listed previously, for instance it can be used to guide the process of creating marketing campaigns by creating at least one per segment.</li>
</ul>

<p>In the case where there are identified outcomes that we're interested in predicting, it is not always necessary to look for situation-outcome mappings. When looking to recommend items for instance (for cross-selling), you could either look for a mapping from customer and product characterizations to purchase indicator, or you could use other techniques to analyze graph data on who bought what in order to recommend products to a customer based on what others with similar habits bought (without having to look at customer or product characterizations).</p>

<p>&nbsp;</p>

<p>Now that we've seen the different business goals that Data Science and Machine Learning can help you pursue, the next step for you is to figure out the one that would deliver the most value to your business right now — which will it be?</p>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe>

<p>Enjoyed this article? <a href="http://www.twitter.com/louisdorard">Follow me!</a></p>]]></content:encoded><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1419845536446-SYBC7DZX3UJZ2ZI5Z93N/image-asset.png?format=1500w" medium="image" isDefault="true" width="1301" height="472"><media:title type="plain">9 ways Data Science can improve your business</media:title></media:content></item><item><title>Key learnings from Strata Barcelona 2014</title><dc:creator>Louis Dorard</dc:creator><pubDate>Wed, 03 Dec 2014 15:04:57 +0000</pubDate><link>https://www.ownml.co/blog/strata-barcelona-2014</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:547ed463e4b0d77d6ee07f9f</guid><description><![CDATA[Immediately after PAPIs.io '14 I spent a couple of days at Strata in 
Barcelona. Strata has several tracks and I ended up going mostly to 
“business” sessions, but this synthesis of things I heard at the conference 
will be of interest to technical people as well. Actually there was one 
business session that had more code in it than another data science session 
I went to!

Here is my selection of key take-away messages, from sessions I attended 
(Strata is a huge conference so this is just a very partial view of it):

    * Make predictions that you can act on
    * When hitting a performance plateau, use new data — not fancier
      algorithms
    * Ensuring your work is reproducible has never been easier — now make
      it your habit
    * Getting a business edge with data requires automating decisions]]></description><content:encoded><![CDATA[<p>Immediately after <a href="http://www.papis.io/2014">PAPIs.io '14</a> — write-up coming soon! — I spent a couple of days at <a href="http://strataconf.com/strataeu2014/">Strata</a> in Barcelona.
Strata has several tracks and I ended up going mostly to “business” sessions, but this synthesis of things I heard at the conference will be of interest to technical people as well. Actually there was one business session that had more code in it than another data science session I went to!</p>

<p>Here is my selection of key take-away messages, from sessions I attended (Strata is a huge conference so this is just a <em>very</em> partial view of it). Click the links to go directly to the ones you're interested in:</p>

<ul>
<li><a href="#vince-darley">Make predictions that you can act on</a></li>
<li><a href="#foster-provost">When hitting a performance plateau, use new data — not fancier algorithms</a></li>
<li><a href="#jeroen-janssens">Ensuring your work is reproducible has never been easier — now make it your habit</a></li>
<li><a href="#uwe-weiss">Getting a business edge with data requires automating decisions</a></li>
</ul>

<p><a data-preserve-html-node="true" name="vince-darley"></a></p>

<h2 id="makepredictionsthatyoucanacton">Make predictions that you can act on</h2>

<h4 id="vincedarleybigdataatkinghttpstrataconfcomstrataeu2014publicscheduledetail39232"><em>Vince Darley: <a href="http://strataconf.com/strataeu2014/public/schedule/detail/39232">Big Data at King</a></em></h4>

<p>Making predictions about customers (e.g. engagement, churn) is useful, but keep in mind that an action has to follow from this prediction… </p>

<ul>
<li>You could use a descriptive model that tells you why this prediction was made, i.e. which combination of factors results in predicted churn, etc., which will help you choose the right action (e.g. "the customer might churn because of <em>this</em> so we should address this particular point").</li>
<li>You could directly predict an action to take, based on data you’ve collected about the success of different actions in different contexts.</li>
</ul>

<p><a data-preserve-html-node="true" name="foster-provost"></a></p>

<h2 id="whenhittingaperformanceplateauusenewdatanotfancieralgorithms">When hitting a performance plateau, use new data — not fancier algorithms</h2>

<h4 id="fosterprovostdiggingintopredictiveanalyticswithfinegrainedbehaviordatahttpstrataconfcomstrataeu2014publicscheduledetail38069"><em>Foster Provost: <a href="http://strataconf.com/strataeu2014/public/schedule/detail/38069">Digging into Predictive Analytics with Fine-grained Behavior Data</a></em></h4>

<p>When you’re hitting a performance plateau with your predictive model, it probably means that you’ve extracted all the information there was to extract from the (limited) data you had. There’s no need to try ever fancier algorithms, but what you should do is work on the data itself. Foster Provost talks of looking for “massive fine-grained data”. I didn't know this terminology. I would just say to make a new pass on feature engineering (i.e. finding all the things we could measure that could have an impact on the outcome we want to predict). Here are some examples he gave of data to take into account:</p>

<ul>
<li>locations frequented</li>
<li>things liked on facebook</li>
<li>people connected to</li>
<li>words, phrases in text</li>
<li>mobile analytics</li>
<li>inferred personal traits</li>
<li>document classes</li>
</ul>

<p><a data-preserve-html-node="true" name="jeroen-janssens"></a></p>

<h2 id="ensuringyourworkisreproduciblehasneverbeeneasiernowmakeityourhabit">Ensuring your work is reproducible has never been easier — Now make it your habit</h2>

<h4 id="jeroenjanssensjeroenhjanssenshttpstwittercomjeroenhjanssensdatasciencetoolboxandtheimportanceofreproducibleresearchhttpstrataconfcomstrataeu2014publicscheduledetail37320"><em>Jeroen Janssens (<a href="https://twitter.com/jeroenhjanssens">@jeroenhjanssens</a>): <a href="http://strataconf.com/strataeu2014/public/schedule/detail/37320">Data Science Toolbox and the Importance of Reproducible Research</a></em></h4>

<h3 id="virtualmachine">Virtual Machine</h3>

<p>The take-away message here is that you should always work off of a Virtual Machine which is created from a common base (e.g. Ubuntu 14.04) and a set of instructions that is contained in a configuration file (this can be through <a href="http://vagrantup.com">Vagrant</a> or <a href="https://www.docker.com">Docker</a> coupled with a provisioning tool such as <a href="http://www.ansible.com/home">Ansible</a>). This config file should be added to your version control system and should be “in sync” with your code: if you make changes to the VM config, it should recreate the VM from scratch and you should make sure your code still runs.</p>

<p>Jeroen created a base VM called the <a href="http://datasciencetoolbox.org">Data Science Toolbox</a>, which contains most of what you'll need for Data Science (<a href="http://datascienceatthecommandline.com">at the command line</a>). If you need extra things to install, it’s just a change in the config file. The provisioning instructions of the DST are <a href="http://github.com/DataScienceToolbox/data-science-toolbox">open source</a>.</p>

<h3 id="drakemakefordata">Drake: Make for data</h3>

<p>For Data Science, you also want to be careful when using spreadsheet programs for data transformation. While I personally recommend using Excel (up to a certain point — <a href="http://randyzwitch.com/excel-destroys-data/">open but don't save</a>), if you need to transform your data you should do it with scripts and keep a copy of the original data file. You’ll probably need to reproduce your data transformations when you get more data or fresher data. Have a look at <a href="https://github.com/Factual/drake">Drake</a> to make it easier to do so.</p>

<h3 id="notebooks">Notebooks</h3>

<p>Notebooks are a great way to document how to reproduce your analyses as you’re able to explain in a web page all the steps you’ve taken to go from pre-requirements to final result, with embedded, runnable blocks of code for each step. This is popular in the Python community with <a href="http://ipython.org/notebook.html">IPython notebooks</a> but the <a href="http://jupyter.org">Jupyter project</a> extends it to other languages.</p>

<p><a data-preserve-html-node="true" name="uwe-weiss"></a></p>

<h2 id="gettingabusinessedgewithdatarequiresautomatingdecisions">Getting a business edge with data requires automating decisions</h2>

<h4 id="uweweissweissuhttpstwittercomweissuwhydecisionautomationiskeyinbigdataanalysishttpstrataconfcomstrataeu2014publicscheduledetail37468"><em>Uwe Weiss (<a href="https://twitter.com/WeissU">@WeissU</a>): <a href="http://strataconf.com/strataeu2014/public/schedule/detail/37468">Why decision automation is key in big data analysis</a></em></h4>

<p>Decision making is very challenging for humans, for several reasons:</p>

<ul>
<li>situations are complex (many things to take into account)</li>
<li>decisions must be made frequently</li>
<li>we humans have natural biases which make us take irrational/bad decisions (see Daniel Kahneman's <a href="http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555">Thinking Fast and Slow</a>)</li>
</ul>

<p>Using data can help us make more informed decisions and remove some of the bias. But Uwe’s practical experience points out to communication issues:</p>

<ul>
<li>metrics dashboards are contradictory and confusing</li>
<li>monthly reports are ignored after two iterations</li>
<li>in-house analyst teams are overworked and powerless</li>
</ul>

<p>Another part of the solution to improve decision making process is to write business rules. But they’re still going to be followed by a human — with the limitations it implies. Business rules are like computer programs, written by non programmers. So how about having them written by a programmer? Better yet: how about having the rules figured out (and written) by a machine, after an analysis of all the data that’s available (i.e. with machine learning)? This data is always much more than a human can process, and often times a small change in one of the things we measure about a given situation can have an important impact on the outcome we observe.</p>

<p>Predictive modelling can be used to extract relationships between situations and outcomes, from historical data. In businesses that sell perishable goods, it is of interest to predict demand based on the current context. You can then decide how much to have in stock accordingly. If you don’t have enough then you’re losing business; if you have too much then you’ve wasted money. The ideal decision-making process is as follows (completely automated): the machine collects and stores data, it analyzes it to create a predictive model, it uses the model to make predictions, and it makes a decision based on predictions.</p>

<h2 id="others">Others</h2>

<h3 id="googledocscanworkasafrontendtoyourdatawarehouse">Google Docs can work as a front-end to your data warehouse</h3>

<p>Seeking Alpha have done it and they've <a href="http://strataconf.com/strataeu2014/public/schedule/detail/37121">showed us how</a>. Google Docs are easily accessible (web, tablet, smartphone), shareable, and they can be used by everyone up to C-level execs. They take care of a lot of the IT and front-end work in making it possible to show your company’s latest data internally to as many people as possible.</p>

<h3 id="abtestingdoesnotworkinsocial">A/B testing does not work in social</h3>

<p>In many cases where you need to test in the real world how good your predictive models are doing, A/B testing does the job, but not when you’re testing features with social components (e.g. for games): what if 2 friends are in different segments? I’d never come across that problem and I’m not sure what to do in that situation but it’s worth mentioning and thinking about… </p>

<h2 id="wouldirecommendstrata">Would I recommend Strata?</h2>

<p>This was my first Strata conference and it's certainly what I'd call a commercial conference (in my own "commercial" vs "industry community" vs "academic" conference classification). For $1000 it’s not exactly small business-friendly… (I was very lucky to get a free pass) It also feels like you get sold a lot of things and some talks are thinly-veiled sales pitches for the technology they're presenting. But you still get to hear very interesting things and stories (as shown in this post I hope!) and it's a place where <em>many</em> people in the data community come and meet (I was told the audience is around 1500 people).</p>

<p>If you're a big company, Strata will surely be something you'll want to attend. I wish there were a similar event that would be more suited to small businesses...</p>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe>

<p>Enjoyed this article? <a href="http://www.twitter.com/louisdorard">Follow me!</a></p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1417598978073-BSFVTBR4GUF3C4PN0DP0/image.jpg?format=1500w" medium="image" isDefault="true" width="1156" height="650"><media:title type="plain">Key learnings from Strata Barcelona 2014</media:title></media:content></item><item><title>Qucit launches world’s first bikeshare predictive API</title><dc:creator>Louis Dorard</dc:creator><pubDate>Sat, 01 Nov 2014 15:00:54 +0000</pubDate><link>https://www.rudebaguette.com/2014/10/22/qucit-launches-worlds-first-bikeshare-predictive-api/</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:5454f476e4b070eb36689c67</guid><description><![CDATA[Big data startup Qucit released this month the world’s first bikeshare 
predictive API, tightly integrated in the popular mobile app for Bordeaux 
bikes. This is an inspiring example of Machine Learning usage in the real 
world. One of the value propositions of Predictive is better resource 
management. Here, in the "smart city" context, it impacts our everyday 
lives.]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/qucit-launches-worlds-first-bikeshare-predictive-api">Permalink</a><p>]]></content:encoded></item><item><title>Import.io webinar: everyone can do data science</title><dc:creator>Louis Dorard</dc:creator><pubDate>Thu, 02 Oct 2014 14:01:17 +0000</pubDate><link>http://blog.import.io/post/become-a-data-scientist-in-an-hour</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:542d5a80e4b0ff63702b98e4</guid><description><![CDATA[Last week I visited the Import.io offices in London and did a webinar with 
them in which I showed:

    * how to use their tool to easily scrape real estate data from the web
    * how to clean that data with the Pandas library in Python
    * how to build a real-estate pricing model by sending the clean data to
      BigML.

Check out the video recording and the write-up they did!]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/importio-webinar">Permalink</a><p>]]></content:encoded></item><item><title>Calling All Predictive APIs &#x26; Apps Makers</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 22 Sep 2014 13:20:25 +0000</pubDate><link>https://www.ownml.co/blog/papis-calling</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:541966f4e4b0fa06a0186de3</guid><description><![CDATA[I will be chairing the PAPIs.io conference taking place on 17-18 November 
2014 in Barcelona, right before Strata. It will be the first ever 
international conference dedicated to Predictive APIs and Predictive Apps. 
If you're interested in presenting your work in this space, a Call for 
Proposals is open until 8 October 2014.]]></description><content:encoded><![CDATA[<figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1411341388805-KQ6X91M2NNNT8ZL5GY3P/image-asset.png" data-image-dimensions="518x365" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="541f5bfce4b07b5243b5b5d5" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1411341388805-KQ6X91M2NNNT8ZL5GY3P/image-asset.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  





<p><strong>TL;DR:</strong>&nbsp;<a href="http://www.papis.io">PAPIs.io</a>&nbsp;will take place on 17-18 November 2014 in Barcelona, right before <a href="http://www.strataconf.com/strataeu2014">Strata</a>. It will be the first ever international conference dedicated to Predictive APIs and Predictive Apps. If you're interested in presenting your work in this space, a <a href="http://www.papis.io/cfp">Call for Proposals is open until 8 October 2014</a>.</p><p><em><span>Photo: José Antonio Ortega presenting a predictive API © <a href="https://www.flickr.com/photos/14114646@N00/7621062122/in/photostream/">Joan Carles Ambrojo</a></span></em></p>


<hr /><p>Demand for predictive apps is growing. There’s increasing awareness of the value that can be obtained from data with Machine Learning. At the same time, there’s a growing number of APIs that make ML accessible to the masses and that remove the barriers to entry to predictive app development.</p>

<p>Today, the predictive APIs and apps community has grown to a level where it deserves a practical conference dedicated to them. API makers are working hard at making the lives of developers simpler, so the event should connect them with those who make predictive apps. Beyond demonstrating these APIs, we need to focus on applications. If we want predictive technology to have real-world impact, it's important that predictive app developers share their stories, their challenges and how they're getting value from predictive.</p>

<h2 id="holapapis">Hola PAPIs!</h2>

<p>Today, I’m very proud to announce that I will be chairing <a href="http://www.papis.io/">PAPIs.io ’14</a>, the 1st ever international conference dedicated to predictive APIs and predictive apps. Seeding a conference is no easy task and PAPIs would not be possible without the invaluable support of <a href="https://www.linkedin.com/in/cisko">Francisco Martin</a> (our Program Chair), <a href="https://www.linkedin.com/in/aaalee">Ali Syed</a>, and the members of the <a href="http://www.papis.io/#committee">Program Committee</a> who are sharing our vision. We are also happy to count with the early support of great companies like <a href="http://www.bigml.com">BigML</a>, <a href="http://www.clevertask.com">CleverTask</a>, <a href="http://www.dataiku.com/">Dataiku</a>, <a href="http://www.gcsgroup.com.au/">GCS Agile</a>, <a href="http://persontyle.com/">Persontyle</a>, <a href="http://strands.com/">Strands</a> and <a href="http://www.taiger.com/">Taiger</a>!</p>

<p><strong>PAPIs.io '14 will take place on 17-18 November 2014 in Barcelona, Spain</strong>, right before <a href="http://strataconf.com/strataeu2014">Strata</a>. There will be API tutorials on Monday 17 and conference sessions on Tuesday 18. We’ve got some great speakers lined up already, but there are still some slots to fill: click below if you want to propose something to present.</p>



  <a href="http://www.papis.io/cfp" class="sqs-block-button-element--medium sqs-button-element--primary sqs-block-button-element"
    
  >
    Present at PAPIs
  </a>

<h2 id="whatscomingup">What’s coming up</h2>

<p>This is not a one off conference but the first of many. After this year’s edition, we’ll be going to Sydney, Australia in 2015 and to Portland, Oregon in 2016. But now’s your chance to be part of the very first edition of PAPIs! We’ve just opened <a href="http://www.papis.io/#register">registration</a> and you can get Early Bird tickets at a discount price until 20 October 2014.</p>

<p>If you not only want to register but also to support us further, <a href="http://www.papis.io/contact/">please get in touch</a>!</p>

<p>If you’re not ready to register yet, sign up below to get updates on PAPIs. We will give more details in a few weeks on the final list of speakers.</p>






  <form method="POST" novalidate data-form-id="54199eede4b003eaab47b068" autocomplete="on" onsubmit="return (function (form) {
    Y.use('squarespace-form-submit', 'node', function usingFormSubmit(Y) {
      (new Y.Squarespace.FormSubmit(form)).submit({
        formId: '54199eede4b003eaab47b068',
        collectionId: '5206b959e4b0bdc26006be4a',
        objectName: 'item-541966f4e4b0fa06a0186de3'
      });
    });
    return false;
  })(this);" class="newsletter-form">
    <header class="newsletter-form-header">
      <h2 class="newsletter-form-header-title">Get updates on PAPIs.io</h2>
      
    </header>
    
      
        
          
            
          
            
              <fieldset id="name-yui_3_17_2_1_1410963616309_30324" class="newsletter-form-name-fieldset form-item fields name required">
                
                  <label class="newsletter-form-field-label title">First Name</label>
                  <input data-title="First" spellcheck="false" maxlength="30" name="fname" placeholder="First Name" type="text" class="newsletter-form-field-element field-element field-control" x-autocompletetype="given-name" />
                
                
                  <label class="newsletter-form-field-label title">Last Name</label>
                  <input data-title="Last" spellcheck="false" maxlength="30" name="lname" placeholder="Last Name" type="text" class="newsletter-form-field-element field-element field-control" x-autocompletetype="surname" />
                
              </fieldset>
            
          
        
        
          
            
              <label for="email-yui_3_17_2_1_1410963616309_30323-field" class="newsletter-form-field-label title">Email Address</label>
              <input autocomplete="email" spellcheck="false" name="email" id="email-yui_3_17_2_1_1410963616309_30323-field" placeholder="Email Address" type="email" class="newsletter-form-field-element field-element" x-autocompletetype="email" />
            
          
        
          
        
      
      
        <button
          class="
            newsletter-form-button
            sqs-system-button
            sqs-editable-button-layout
            sqs-editable-button-style
            sqs-editable-button-shape
            sqs-button-element--primary
          "
          type="submit"
          value="Sign Up"
        >
          <span class="newsletter-form-spinner sqs-spin light large"></span>
          <span class="newsletter-form-button-label">Sign Up</span>
          <span class="newsletter-form-button-icon"></span>
        </button>
      
      
        
      
    
    <p>We respect your privacy.</p>
    Thank you!
    
  </form>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1411340713538-GZZLHUEAHHXLWS4DE45P/image-asset.jpeg?format=1500w" medium="image" isDefault="true" width="1500" height="844"><media:title type="plain">Calling All Predictive APIs &#x26; Apps Makers</media:title></media:content></item><item><title>6 questions you should ask about prediction APIs</title><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 15 Sep 2014 15:42:19 +0000</pubDate><link>http://www.programmableweb.com/news/6-questions-you-should-ask-about-prediction-apis/analysis/2014/09/12</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:54170873e4b0fb5c3b4f4fae</guid><description><![CDATA[As machine learning and predictive analytics services become more widely 
embraced in the business world, predictive APIs are starting to open up. 
When evaluating this class of API, it is useful to have a common set of 
questions — the answers to which will help determine whether prediction 
APIs are a good fit for your needs and to steer you toward the best product 
for your organization.]]></description><content:encoded><![CDATA[<p><a href="https://www.ownml.co/blog/6-questions-you-should-ask-about-prediction-apis">Permalink</a><p>]]></content:encoded></item><item><title>How to predict abstention rates with open data</title><dc:creator>Louis Dorard</dc:creator><pubDate>Tue, 29 Jul 2014 08:46:00 +0000</pubDate><link>https://www.ownml.co/blog/predict-abstention-rates-open-data</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:53ac25e1e4b01cbc3292f0f7</guid><description><![CDATA[Open data is a way to increase transparency into what happens in our 
society. When coupled with predictive modelling, it becomes a way to 
interpret why things happened. Even though it sounds complex, these 
techniques have become accessible to the masses. Let's see how this works 
with elections data.

Co-authored with Alexandre Vallette]]></description><content:encoded><![CDATA[<p><em>This post was written in collaboration with <a href="http://twitter.com/vallettea">Alexandre Vallette</a> who's my co-author on an upcoming guide to hacking with open data (scroll down to find out more).</em></p>


<p>Open data is a way to increase transparency into what happens in our society. When coupled with predictive modelling, it becomes a way to interpret <em>why</em> things happened. Even though it sounds complex, these techniques have become accessible to the masses. Let's see how this works with elections data.</p>

<h2 id="itsallaboutthecontext">It's all about the context</h2>

<p>Let's say you're given abstention rates for every township in an election. You can create a heat map to visually identify regions of high/low rates.</p>















 

  
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1405878157278-4C5AT6FUCEGV2DFPGLHX/image-asset.jpeg" data-image-dimensions="1757x1767" data-image-focal-point="0.5,0.5" alt="Heat map of the abstention rate for the 2008 mayoral elections in France, courtesy of Snips / Alexandre Vallette" data-load="false" data-image-id="53cbff8ce4b02c6d14ac032e" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1405878157278-4C5AT6FUCEGV2DFPGLHX/image-asset.jpeg?format=1000w" />
            
          
        
          
        

        
          
          <figcaption class="image-caption-wrapper">
            <p>Heat map of the abstention rate for the 2008 mayoral elections in France, courtesy of <a href="http://snips.net/blog/posts/2014/02-18-predicting-abstention-rate-from-open-data.html">Snips / Alexandre Vallette</a></p>
          </figcaption>
        
      
        </figure>
      

    
  



<p>On election night, everyone has their own interpretation of why rates were higher or lower than average in certain places. But these are only speculations — they are not based on facts that have been measured. What if you could use machines to crunch all the open data that's available in order to find patterns across the townships where abstention rates are high/low?</p>

<p>As a first step to do that, <a href="https://twitter.com/vallettea">Alexandre Vallette</a> showed <a href="http://snips.net/blog/posts/2014/02-18-predicting-abstention-rate-from-open-data.html">how you could enrich elections data</a> by merging different sources of open data into a representation of the "context" of each township, with information such as population, hourly salary, number of people living in social housings, etc. Alexandre then applies machine learning to create a model of the relationship between <em>context</em> and <em>abstention rate</em>. This model can be used to make predictions for future elections, or to estimate how slight changes in a given context could impact the outcome (the abstention rate). He implemented this idea on French mayoral elections, but his method can be adapted to any other election anywhere else. The data he used for the 37586 French communes can be downloaded <a href="https://bml-data.s3.amazonaws.com/communes.csv.gz">here</a> (35.2 MB, gzipped CSV file).</p>

<h2 id="predictivemodelsmadeeasy">Predictive models made easy</h2>

<p>In this post, I want to show you two more things:</p>

<ol>
<li>If you use <em>descriptive</em> predictive models such as <em>decision trees</em>, you can see how predictions are made as sequences of rules. For instance: <strong>IF</strong> (hourly salary > 9.78) <strong>AND</strong>  (number of people in social housings > 1118) <strong>AND</strong> (region = ile-de-france) <strong>THEN</strong> (abstention = 65.61%).</li>
<li>Anyone can create such models with services like BigML that abstract away the technical complexities of predictive modelling. All it takes is to upload data to the service and to select the target to predict (here, the abstention rate).</li>
</ol>

<p>Below is a "SunBurst" visualization of the model I built with BigML, using the same data as Alexandre (click on ‘View on BigML’ to see a bigger version).</p>
<iframe allowfullscreen="allowfullscreen" src="https://bigml.com/embedded/model/oMTcOM4PWyomwBY8DiidBY6dflv" allowtransparency="true" width="100%" frameborder="0" height="400px"></iframe><p><strong>How to read the SunBurst visualization:</strong></p>

<ul>
<li>Each arc corresponds to a set of townships that satisfy certain conditions — hover your mouse over a few of these arcs to see examples.</li>
<li>The color of the arc reflects the average predicted abstention rate for the corresponding set of townships.</li>
<li>Arcs are split into sub-arcs, representing subsets, as you move away from the center.</li>
<li>The number of degrees spanned by an arc is proportional to the number of townships found in the data that belong to the corresponding set.</li>
</ul>

<p>Note that this model is just one way to interpret the data and to present the correlations that were found. Even though we are dealing with a rule-based model, bear in mind that the "features" used in the rules to characterize townships are not necessarily causes of abstention. <a href="http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">Correlation does not imply causation</a> — if you take the example of sunglasses and ice-cream, their sales are correlated but one does not cause the other.</p>

<h2 id="somemoredetails">Some more details</h2>

<p>Actually, I did something a tiny bit more sophisticated than just uploading the data to BigML. I uploaded the data to create a first model, and BigML showed me the 10 most important context features to predict abstention. To make the data smaller and the model simpler, I then removed the other features (there were 930 of them originally) and I built a new model based on this selection of 10 features ('hourly salary', 'population', 'latitude', 'people in social housings', 'region', 'registered', 'people age 0-14', 'households w. people in intermediate occupations', 'farmers', 'households with individual electrical heating'). The data shrunk from 176.5 MB to 2.5 MB (download it <a href="https://bml-data.s3.amazonaws.com/communes-selected-columns.csv">here</a>). As a comparison, <a href="https://github.com/jseabold/538model">when you replicate Nate Silver's 538 Election Forecasting Model</a> (US 2012 presidential elections), you get a dataset of only 188 KB.</p>

<h3 id="howaccurateisthemodel">How accurate is the model?</h3>

<p>To evaluate the accuracy of the model, I did an 80-20% split of the data. The first part was used to build a model, the second one was used to make predictions and to compare them to the true — withheld — abstention values. On average, there was a difference of 7.52 points between predicted and actual abstention rates. With an R-squared value of 0.5 (the maximum being 1 and the baseline 0), the model works reasonably well.</p>

<p>If you're interested in finding out more about how the data was assembled, check out Alex's <a href="https://github.com/vallettea/politics-open-data">source code on Github</a>.</p>






  <form method="POST" novalidate data-form-id="53cd349ee4b0a11d417f7e4a" autocomplete="on" onsubmit="return (function (form) {
    Y.use('squarespace-form-submit', 'node', function usingFormSubmit(Y) {
      (new Y.Squarespace.FormSubmit(form)).submit({
        formId: '53cd349ee4b0a11d417f7e4a',
        collectionId: '5206b959e4b0bdc26006be4a',
        objectName: 'item-53ac25e1e4b01cbc3292f0f7'
      });
    });
    return false;
  })(this);" class="newsletter-form">
    <header class="newsletter-form-header">
      <h2 class="newsletter-form-header-title">Coming soon: "Hacking with Open Data"</h2>
      <p>Alexandre Vallette and me are writing a sponsored guide to innovation with open data that will feature this case study. Sign up here to be notified!</p>
    </header>
    
      
        
        
          
            
              <label for="email-yui_3_17_2_1_1405951496514_38554-field" class="newsletter-form-field-label title">Email Address</label>
              <input autocomplete="email" spellcheck="false" name="email" id="email-yui_3_17_2_1_1405951496514_38554-field" placeholder="Email Address" type="email" class="newsletter-form-field-element field-element" x-autocompletetype="email" />
            
          
        
          
        
      
      
        <button
          class="
            newsletter-form-button
            sqs-system-button
            sqs-editable-button-layout
            sqs-editable-button-style
            sqs-editable-button-shape
            sqs-button-element--primary
          "
          type="submit"
          value="Sign Up"
        >
          <span class="newsletter-form-spinner sqs-spin light large"></span>
          <span class="newsletter-form-button-label">Sign Up</span>
          <span class="newsletter-form-button-icon"></span>
        </button>
      
      
        
      
    
    <p>We won't send spam. You can unsubscribe at any time.</p>
    Thank you!
    
  </form>

<p>Enjoyed this article? <a href="https://news.ycombinator.com/submit" class="hn-button" data-title="How to predict abstention rates with open data" data-url="http://www.louisdorard.com/blog/predict-abstention-rates-open-data">Vote on Hacker News</a> and <a href="http://www.twitter.com/louisdorard">follow me!</a></p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1405957814698-125JV67N4JBOXIM96VZL/image-asset.jpeg?format=1500w" medium="image" isDefault="true" width="1023" height="605"><media:title type="plain">How to predict abstention rates with open data</media:title></media:content></item><item><title>Training, interview and books to win with HumanCoders</title><dc:creator>Louis Dorard</dc:creator><pubDate>Sat, 05 Jul 2014 10:13:40 +0000</pubDate><link>https://www.ownml.co/blog/humancoders</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:53ad88d1e4b075a251473169</guid><description><![CDATA[I am proud to announce that I have teamed up with HumanCoders to set up a 
groundbreaking Machine Learning training program which is based on 
Prediction APIs and brings you up to speed in 3 days. At this occasion, 
they interviewed me and I gave them 3 copies of my book, Bootstrapping 
Machine Learning, to give away — here's your chance to snatch one of them!]]></description><content:encoded><![CDATA[<h2 id="3copiesofbootstrappingmachinelearningtowin">3 copies of Bootstrapping Machine Learning to win</h2>

<p><a href="https://www.ownml.co/machine-learning-book">Bootstrapping Machine Learning</a> is the first book that teaches Machine Learning through the use of Prediction APIs. This makes it much quicker to get started and it allows to focus more on practical aspects. You can read reviews on <a href="http://machinelearningmastery.com/bootstrapping-machine-learning-book-review/">Machine Learning Mastery</a> and on <a href="https://www.goodreads.com/book/show/22515367-bootstrapping-machine-learning">Goodreads</a>.</p>

<p>Are you intrigued by what the book has to offer, but haven't decided to get it yet? There are three copies to win in PDF / ePub / Mobi formats! <a href="http://contest.humancoders.com/contests/26-machine-learning-book">Sign up here before 7 July 2014</a> (the page is in French  but anyone can sign up).</p>

<h2 id="goingfurthertrainingsessions">Going further: training sessions</h2>

<p>You'll learn a lot with the book, but what should you do if you wanted to learn even more? I've been thinking about this for some time now, and I've been chatting with various people about setting up in-person training sessions. In particular, I've been talking with <a href="http://humancoders.com">HumanCoders</a> who are based in France and who provide a variety of top-quality training to IT, software engineering and web professionals.</p>

<p>Their feedback has been extremely useful when designing a training program. They have a lot of experience in this domain and they have perfected the art of creating engaging learning experiences. We have recently advertized dates for a first training session in Paris. It will span 3 days and half of the time will be dedicated to hands-on work. You can <a href="http://formations.humancoders.com/formations/machine-learning">check out the program here</a> (in French, again).</p>

<p>In the near future I'll be looking to propose the same training in other cities worldwide. Stay tuned!</p>

<h2 id="aninterview">An interview</h2>

<p>If you read French (or if you're happy with Google Translate), you may also be interested in having a look at <a href="http://blog.humancoders.com/interview-louis-dorard-formateur-machine-learning-1239/">this interview</a> I did for the HumanCoders blog in which I explain what brought me to the field of Machine Learning, then to write a book and finally to give training sessions.</p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1404554779145-5EI9F5ZQXYE1KEXADVYC/formation-wordpress.JPG?format=1500w" medium="image" isDefault="true" width="1500" height="1125"><media:title type="plain">Training, interview and books to win with HumanCoders</media:title></media:content></item><item><title>How to Improve Your Subscription-Based Business by Predicting Churn</title><dc:creator>Louis Dorard</dc:creator><pubDate>Fri, 27 Jun 2014 15:06:11 +0000</pubDate><link>http://blog.kissmetrics.com/improve-by-predicting-churn/</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:53ad7769e4b003339a863047</guid><description><![CDATA[Churn prediction is one of the most popular Big Data use cases in business. 
It consists of detecting customers who are likely to cancel a subscription 
to a service. Although originally a telco giant thing, this concerns 
businesses of all sizes, including startups. The problem of churn 
prediction can be tackled with machine learning techniques. Now, thanks to 
prediction services and APIs, this sort of predictive analytics is no 
longer exclusive to big players that can afford to hire teams of data 
scientists.]]></description><content:encoded><![CDATA[<p>A couple of weeks ago I wrote a guest post on churn prediction for Kissmetrics, and they <a href="http://blog.kissmetrics.com/improve-by-predicting-churn/#comment-307001">just published it</a>.</p>

<p>Churn prediction is one of the most popular Big Data use cases in business. It consists in detecting which customers are likely to cancel a subscription to a service based on how they use the service. Being able to predict churn based on customer data has proven extremely valuable to big telecom companies. <strong>Now, thanks to prediction services and APIs, it’s accessible to businesses of all sizes</strong> — not only those who can afford to hire teams of data scientists.</p>

<p>The process is as follows:</p>

<ul>
<li><strong>Step 1:</strong> Gather historical customer data that you save to a CSV file.</li>
<li><strong>Step 2:</strong> Upload that data to a prediction service that automatically creates a “predictive model.”</li>
<li><strong>Step 3:</strong> Use the model on each current customer to predict whether they are at risk of leaving.</li>
</ul>

<p>Check out my <a href="http://blog.kissmetrics.com/improve-by-predicting-churn/#comment-307001">full post on the Kissmetrics blog</a> for details on these steps and for an illustration of how to go through Steps 2 and 3 with <a href="http://www.bigml.com/">BigML</a>.</p>

<p>If you find this article useful, please <a href="https://news.ycombinator.com/submitlink?u=http%3A%2F%2Fblog.kissmetrics.com%2Fimprove-by-predicting-churn%2F&amp;t=How%20to%20improve%20your%20subscription-based%20business%20by%20predicting%20hurn">vote for it on HackerNews</a>!</p>

<p>If you want to dig deeper and see what to do next, check out my book, <a href="https://www.ownml.co/machine-learning-book">Bootstrapping Machine Learning</a>.</p>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe><p><a href="https://www.ownml.co/blog/predict-churn">Permalink</a><p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1403879125372-WT0BWZATJ6E4LW37WF2N/exit.jpg?format=1500w" medium="image" isDefault="true" width="1280" height="970"><media:title type="plain">How to Improve Your Subscription-Based Business by Predicting Churn</media:title></media:content></item><item><title>Automating the Data Scientist</title><category>Business</category><category>Machine Learning</category><dc:creator>Louis Dorard</dc:creator><pubDate>Thu, 15 May 2014 14:41:42 +0000</pubDate><link>https://www.ownml.co/blog/automating-the-data-scientist</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:5374d233e4b02004337721c0</guid><description><![CDATA[What is a Data Scientist? What is it that they do that is now being 
automated? What are the new solutions out there that are bringing Data 
Science directly to domain experts? What is it changing?]]></description><content:encoded><![CDATA[<p>Talking about automating one's job is a difficult exercise. People who need to have the job done but don't know how (or don't have enough resources) are bound to like the message. People who work on this automation can only approve. But many of those who do that job for a living will disagree. As I said in a <a href="http://www.louisdorard.com/blog/building-a-business-around-machine-learning-apis">previous post</a>, my GigaOM piece on <a href="http://gigaom.com/2014/05/07/the-goal-of-data-scientists-is-to-put-themselves-out-of-business/">Data Scientists putting themselves out of business</a> received mixed reactions. This is a complex topic, so let me add some more details to explain the perspective and context.</p>

<h2 id="whatisadatascientist">What is a Data Scientist?</h2>

<p>The role of a Data Scientist hasn't been standardized, so everyone has their own definition. Also, the duties vary across companies of different sizes operating in different industries and structured in different ways. When I read about what people who have a "Data Scientist" title do, I often have to revisit my own conception of what a DS is.</p>

<p>So instead of giving a definition of the role, let me try to characterize the abilities of the people behind the title. To me, a Data Scientist is someone who has business acumen, technical skills, and who knows Machine Learning. The last point is key. Those who do not have Machine Learning expertise are Data Engineers, Data Analysts, <a href="http://www.fastcompany.com/3020211/dialed/why-data-artisans-are-the-new-data-scientists">Data Artisans</a>, Data <em>Somethingelse</em>. Therefore, this discussion is focused on the Machine Learning component of the Data Scientist's role.</p>

<p>Apparently, people were shocked to hear me say that "most of a Data Scientist’s time is spent creating predictive models." Based on what I just said, applying Machine Learning technology to data should be an important part of the job. The fact that you're combining this with technical skills and business acumen implies that you're working on that data and that you're creating value from the model. "Creating predictive models" encompasses collecting, cleaning, wrangling data that is used to build the model, and putting the model to good use. </p>

<p>I <a href="http://www.john-foreman.com/1/post/2014/05/the-forgotten-job-of-a-data-scientist-editing.html">totally agree with John Foreman when he says that predictive modeling is a means, not an end</a>. This is something that I emphasize in <a href="http://www.louisdorard.com/machine-learning-book">my book</a> (<em>Chapter 5. Applying ML to your domain > Specifying the right problem > Making predictions... so what?</em>): don't build a model before you have an idea of how it will be of value and before you know how to measure its impact. Another very good point that Foreman makes is that "complexity must be justified". Actually, I like simplicity so much that I'm talking about it in my <a href="http://www.louisdorard.com/about">bio</a>. As a side note, one of the things that's cool with tools like <a href="http://www.bigml.com/">BigML</a> is that they allow you to automate the creation of predictive models but also to choose the degree of complexity of your model.</p>

<h2 id="wherewearetoday">Where we are today</h2>

<p>There are three points I want to make about automating the Data Scientist:</p>

<ul>
<li>When you're in the business of making computer programs, for data-related things or anything else, it's your job to try to automate yourself. Succeeding at this is a different thing. When I did my ML thesis for instance, I wasn't able to fully automate the selection of hyperparameters of my <a href="http://discovery.ucl.ac.uk/1348319/">Gaussian Process bandit algorithm</a>, but I still tried and experimented.</li>
<li>People in the fields of Data Science and Machine Learning are actively working on automating parts of their jobs and they've come up with new predictive modeling solutions that are being used directly by domain experts: <a href="http://www.ayasdi.com/product/">Ayasdi</a>, <a href="https://www.youtube.com/watch?v=R2EQ3Gg8r6s">BeyondCore</a>, <a href="http://www.baynote.com/products-solutions/">Baynote</a>, <a href="http://apigee.com/about/products/big-data">Apigee</a>, <a href="http://www.emeraldlogic.com/">Emerald Logic</a>, <a href="http://emcien.com/solutions/uses/predictive-analytics/">Emcien</a>, <a href="http://www.bigml.com">BigML</a>, <a href="http://www.predictivedb.com/">PredictiveDB</a>, <a href="https://developers.google.com/prediction/">Google</a>, <a href="http://www.wise.io">Wise.io</a>, <a href="http://www.ersatzlabs.com/">Ersatz Labs</a>, <a href="http://www.xyggy.com/">Xyggy</a>, <a href="http://predicsis.com/">PredicSis</a> and many others. Besides, a couple of days ago I came across a research project at Cambridge University led by Zoubin Ghahramani and entitled <a href="http://mlg.eng.cam.ac.uk/?p=1578">"The Automatic Statistician", which has won a $750,000 Google Focused Research Award</a>.</li>
<li>Some of these solutions are actually being used in production. There aren't many public benchmarks to assess their performance, but for now, you can check out <a href="http://blog.bigml.com/2012/08/27/machine-learning-throwdown-part-4-predictions/">how BigML compares to popular algorithms in Weka</a>, <a href="http://www.wise.io/blog/benchmarking-random-forest-part-1">how Wise.io compares to various random forest implementations</a>, and <a href="http://www.prnewswire.com/news-releases/emerald-logic-announces-collaboration-with-kings-college-london-on-quantitative-biomarker-discovery-189970381.html">how Emerald Logic's FACET advances medical research</a>.</li>
</ul>

<p>A post on automating the Data Scientist wouldn't have made much sense three years ago because the solutions I mention didn't exist or weren't used then, and there wasn't a trend yet. If you are sceptic about what they do, I urge you to set up a free account with <a href="http://www.bigml.com/">BigML</a> (which today is the most accessible of these solutions) and to give it a try — if only for intellectual curiosity. Then, take a minute to consider the benefits that a service like this can bring.</p>

<h2 id="plentyofcompaniesproblemsanddata">Plenty of companies, problems and data</h2>

<p>As for any technology-related activity, people's outlook on Data Science can vary quite much depending on the type of company they work for. In large companies that have been doing Data Science for a while and where people use state-of-the-art solutions that they spent countless hours to set up, you don't see things changing anytime soon. </p>

<p>Then there are businesses who need to stay at the forefront or even to advance the state of the art if they want to stay ahead of the competition. Netflix or Spotify are not going to use automatic solutions like <a href="http://www.directededge.com/">DirectedEdge</a>'s to power their movie or music recommendations. They are, and will likely always be, building their own recommender systems. Right now, they are tackling the next big challenge in this area: looking at and understanding the actual content of items before making recommendations to users.</p>

<p>But state-of-the-art doesn't concern everyone. There are businesses that can shine with the simplest application of Machine Learning / predictive analytics, because they use it in a domain where no one else knows about it, or in a way that no one has thought of.</p>

<blockquote>
  <p>“You don’t need to predict accurately to get great value. Organizations, by making predictions per person that are a good bit better than guessing, actually play the numbers game better.” — Eric Siegel</p>
</blockquote>

<p>In a startup or in a small business, you can now do the same thing a Data Scientist would, without knowing ML algorithms. All you need is business acumen, technical skills, and Prediction APIs to remove Machine Learning's barrier to entry. <a href="http://www.fastcompany.com/3020211/dialed/why-data-artisans-are-the-new-data-scientists">As Dan Putler of Alteryx puts it</a>: "Data artisans allow businesses to do deeper, more predictive analytics than they would otherwise be able to do." He is also one to predict that Data Artisans will partially take over the functions of Data Scientists in the near future.</p>

<p>Is that a threat to Data Scientists? Not necessarily. According to Josh Wills, Senior Director of Data Science at Cloudera, there's no reason to worry about their future: "There is plenty of data and plenty of problems for everybody to work on."</p>

<h2 id="stillhardwork">Still hard work</h2>

<p>Remember that, even though some of the learning techniques are automated, you still have to prepare the data to learn from (feature engineering, data extraction, cleaning, etc.) and to deliver value with whatever predictive model you come up with. If it was that easy, I wouldn't have written a <a href="http://www.louisdorard.com/machine-learning-book">book</a> on the topic!</p>

<p>Here is a workflow for applying Machine Learning in your business (or in your application):</p>















 

  
  
    

      

      
        <figure class="
              sqs-block-image-figure
              intrinsic
            "
        >
          
        
        

        
          
            
          
            
              <img class="thumb-image" data-image="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1400347749122-5M0VUMOCBW5PH3YSER69/image-asset.png" data-image-dimensions="317x473" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="53779c65e4b0f1679c09f391" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1400347749122-5M0VUMOCBW5PH3YSER69/image-asset.png?format=1000w" />
            
          
        
          
        

        
      
        </figure>
      

    
  



<p>Only three of these things are being automated (those in black). This means that it still takes lots of efforts to transform data into value, but that Data Scientists can now focus on more meaningful activities. If revenue is on the line, they can later work at making fancier models that improve accuracy, but only after all the rest has been done.</p>

<p>For those who work in smaller companies, you can now "bootstrap" Machine Learning to improve your business. Even though there's hard work ahead of you, it's manageable and it's considerably less than if you'd also had to learn how to use Machine Learning algorithms to create predictive models. Most often, these algorithms are the barrier to entry to Data Science, but they are not what represents most of the work: it's rather in what happens before and after the algorithms are used, and you're likely to already have the right competencies in your team to work on that.</p>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe><p>Enjoyed this article? <a href="https://news.ycombinator.com/submit" class="hn-button" data-title="Automating the data scientist" data-url="http://www.louisdorard.com/blog/automating-the-data-scientist">Vote on Hacker News</a> and <a href="http://www.twitter.com/louisdorard">follow me!</a></p>]]></content:encoded><media:content type="image/jpeg" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1400166236848-C3GYOC7BOATIZUDPUBKF/4782904694_3c6720fa68_o.jpg?format=1500w" medium="image" isDefault="true" width="1500" height="1125"><media:title type="plain">Automating the Data Scientist</media:title></media:content></item><item><title>Building a business around Machine Learning APIs</title><category>Machine Learning</category><category>Business</category><dc:creator>Louis Dorard</dc:creator><pubDate>Mon, 12 May 2014 14:28:43 +0000</pubDate><link>https://www.ownml.co/blog/building-a-business-around-machine-learning-apis</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:5370bd74e4b01877b9cd4eed</guid><description><![CDATA[Is it possible to build a business around Prediction APIs such as Google's 
or BigML's? And if so, how can you differentiate yourself from others who 
could use just the same APIs?]]></description><content:encoded><![CDATA[<p>I got a variety of reactions on Twitter following my GigaOM piece on <a href="http://gigaom.com/2014/05/07/the-goal-of-data-scientists-is-to-put-themselves-out-of-business/">how Data Scientists work at automating themselves</a>. One of them I want to discuss today is about building businesses on top / around Prediction APIs such as <a href="https://developers.google.com/prediction/">Google</a>'s or <a href="http://www.bigml.com/">BigML</a>'s (a.k.a. machine learning APIs). <a href="https://twitter.com/AnthonyNystrom">Anthony Nyström</a>, who is <a href="http://mashablehq.com/post/72475474632/we-sat-down-with-anthony-nystrom-mashables">Principal for Artificial Intelligence at Mashable</a>, raises a very interesting point: is this even possible, and if you do build your business around these APIs, how are you going to differentiate yourself from others who could use the same APIs?</p>

<h2 id="heresonesuchbusiness">Here's one such business</h2>

<p>The first thing that comes to my mind is <a href="http://gigaom.com/2013/07/31/this-is-interesting-a-fraud-detection-company-built-on-googles-prediction-api/">Pondera Solutions</a>, which is a company that was founded in November 2011 and that provides Fraud Detection as a Service. Their product is built on top of Google Prediction API. To me, the value of Pondera's business resides in their domain knowledge and in the connections they have that enable them to bring solutions of that sort to government agencies. Even though I know about Google Prediction API, there’s no way I would be able to replicate what they’ve done, get the data they have, and set up a business that competes with them.</p>

<h2 id="waystodifferentiateyourself">Ways to differentiate yourself</h2>

<p>Now, let's imagine that your business has a direct competitor, looking to tackle the same problem as you with the same Prediction API. I see at least two ways you can differentiate yourself:</p>

<ol>
<li><p><strong>In the way you prepare data to send to the API.</strong> Here, there are two things to consider:</p>

<ul><li><strong>The amount of data points</strong> (i.e. the number of rows). Generally, <a href="http://anand.typepad.com/datawocky/2008/03/more-data-usual.html">more data beats better algorithms</a>. If each data point is tied to an action of a customer, one could get more data by having more customers or by finding tricks to collect more data from each of them. An example of such a trick is Facebook asking me to rate movies in the side bar on the right and making the experience fun and engaging.</li>
<li><strong>The features used</strong> (i.e. the number of columns in the data). Imagine that the aim is to predict the value of a house, as <a href="http://www.zillow.com/zestimate/">Zillow's "zestimates"</a> do. For this, you collect example data consisting of houses characteristics and the price at which they sold. Let's say that the houses represented in your data and in your competitor's are the same. If your competitor represents houses with their number of rooms, surface, type and year of construction, while you are smart enough to also extract and take into account information such as proximity to public transports, schools and amenities, then you will most likely come up with better estimates.</li></ul></li>
<li><p><strong>In the way you use predictions.</strong> One of my favorite examples to illustrate the importance of this is churn detection. It consists in detecting customers who are likely to cancel a subscription to a service. Assume that you could detect such customers with perfect accuracy. What would you do then to make them stay? Give them a special offer, or contact each one of them? Which offer would that be, or what would you tell them? How much would it cost to do that and what proportion of customers would actually end up staying? Depending on the answers to these questions, your return on investment can be very high or as low as zero, even though your churn detection system has perfect accuracy.</p></li>
</ol>

<h2 id="valueoftechnology">Value of technology</h2>

<p>I would have liked to give more examples of businesses like Pondera, that have been built around Prediction APIs. Trust me, there are more. But for now it turns out that they are quite shy to say that publicly. As I understand it, they’re afraid that this would lower the perceived value to investors. I think that the value of those businesses is not in the machine learning technology they use but in the way they apply it to their domain. This is stuff you can patent. In most countries, it's not the technology itself that is patentable but applications of the technology to solve one specific problem in a specific domain.</p>

<h2 id="comingsoon">Coming soon</h2>

<p>One thing I want to bring to your attention is the novelty that is yet to come. Prediction services and APIs are going to enable applications of ML that we weren't able to think of. These tools are used by domain experts and they are the ones who will develop new ideas. What happens when people understand the possibilities of Machine Learning and combine that with their domain knowledge? I’m eager to find out.</p>

<p>Nyström said that you should "hire [Data Scientists] and [Machine Learning] experts if you want to move and shake at the dance". But there are also new dances to create. Initially, your competition is limited, but then, once your "dance" gets validation and traction, you'll be able to hire those experts if you ever want to. Prediction APIs will have done their job: removing machine learning's barrier to entry.</p>
<iframe scrolling="yes" src="https://dl.dropboxusercontent.com/u/685335/optin.html" allowtransparency="true" width="100%" frameborder="0" id="mailchimp" height="400px"></iframe>

<p class="text-align-center"><span>Enjoyed this article?&nbsp;</span><a href="http://twitter.com/louisdorard">Follow me on Twitter!</a></p>]]></content:encoded><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1399903978445-TWO53D9RFJ39X3HRMU19/build-business.png?format=1500w" medium="image" isDefault="true" width="1345" height="618"><media:title type="plain">Building a business around Machine Learning APIs</media:title></media:content></item><item><title>Bootstrapping Machine Learning is launching today</title><dc:creator>Louis Dorard</dc:creator><pubDate>Wed, 07 May 2014 13:22:15 +0000</pubDate><link>https://www.ownml.co/blog/bootstrapping-machine-learning-launching-today</link><guid isPermaLink="false">5206b718e4b0bdc26006bae2:5206b959e4b0bdc26006be4a:536a31b0e4b0e77a57296e48</guid><description><![CDATA[Today, I am releasing the first edition of Bootstrapping Machine 
Learning. Prediction APIs are making Machine Learning accessible to 
everyone and this book is the first that teaches how to use them.]]></description><content:encoded><![CDATA[<p>Today, I am releasing the <a href="http://www.louisdorard.com/machine-learning-book">first edition of Bootstrapping Machine Learning</a>.</p>

<p>Prediction APIs are making Machine Learning accessible to everyone and this book is the first that teaches how to use them. You will learn the possibilities offered by these APIs, how to formulate your own Machine Learning problem, and what are the key concepts to grasp — not how algorithms work, so it doesn't take a university degree to understand. This all makes for a very different angle from the other books on the market.</p>

<p>Although only 196 pages long, Bootstrapping Machine Learning has been quite long in the making. The book covers the principles that make ML work, its limitations, several business and apps examples, it shows you how to apply ML to your own domain and which Prediction APIs you can use. Finally, there's a case study in which I explain step by step how I would go about reimplementing Gmail's Priority Inbox.</p>

<p>The original table of contents was bigger but I made a few cuts to keep the book concise. I also spent a good chunk of time on making tutorials, screencasts, IPython notebooks, code, a Virtual Machine, and some additional resources. All in all it would take a couple of hours for a first read and if you get one of the packages with the extra resources you could get hacking with Prediction APIs within half a day.</p>

<p>The book is available as of today and with a 20% discount for 24 hours only! <a href="http://www.louisdorard.com/machine-learning-book">Check it out now and download the first chapters for free</a>.</p>

<p>Also, please help me spread the word by sharing some social network love!</p>

<p>Louis</p>

<p><a href="http://twitter.com/louisdorard">@louisdorard</a></p>]]></content:encoded><media:content type="image/png" url="https://images.squarespace-cdn.com/content/v1/5206b718e4b0bdc26006bae2/1399469307325-ZR1UCUW73HPN0BX2OT65/launching.png?format=1500w" medium="image" isDefault="true" width="1500" height="1417"><media:title type="plain">Bootstrapping Machine Learning is launching today</media:title></media:content></item></channel></rss>