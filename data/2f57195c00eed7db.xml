<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alexis Perrier - Data Science</title>
    <description>Data science, machine learning, predictive analytics, artifical intelligence.
</description>
    <link>https://alexisperrier.com/</link>
    <atom:link href="https://alexisperrier.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 06 Mar 2021 07:44:50 +0000</pubDate>
    <lastBuildDate>Sat, 06 Mar 2021 07:44:50 +0000</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Intro to NLP, new  course on Openclassrooms</title>
        <description>&lt;h1 id=&quot;a-new-intro-to-nlp-course&quot;&gt;A new Intro to NLP course&lt;/h1&gt;

&lt;p&gt;I am super excited for the launch of my new &lt;a href=&quot;https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing&quot;&gt;Intro to NLP&lt;/a&gt; course on Openclassrooms.&lt;/p&gt;

&lt;p&gt;This could not have happened without OC’s amazing team, with a special shoutout to Alexandra. :)&lt;/p&gt;

&lt;p&gt;The course covers basic BOW to static embeddings, glove style, with NLTK, Spacy and Gensim.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Part #1 - Preprocess Text Data
    &lt;ol&gt;
      &lt;li&gt;Build Your First Word Cloud&lt;/li&gt;
      &lt;li&gt;Remove Stop Words From a Block of Text&lt;/li&gt;
      &lt;li&gt;Apply Tokenization Techniques&lt;/li&gt;
      &lt;li&gt;Create a Unique Word Form With SpaCy&lt;/li&gt;
      &lt;li&gt;Extract Information With Regular Expression&lt;/li&gt;
      &lt;li&gt;Quiz: Preprocess Text Data&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Part #2 - Vectorize Text for Classification Using Bag-of-Words
    &lt;ol&gt;
      &lt;li&gt;Apply a Simple Bag-of-Words Approach&lt;/li&gt;
      &lt;li&gt;Apply the TF-IDF Vectorization Approach&lt;/li&gt;
      &lt;li&gt;Apply Classifier Models for Sentiment Analysis&lt;/li&gt;
      &lt;li&gt;Quiz: Vectorize Text Using Bag-of-Words Techniques&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Part #3 - Vectorize Text for Exploration Using Word Embeddings
    &lt;ol&gt;
      &lt;li&gt;Discover The Power of Word Embeddings&lt;/li&gt;
      &lt;li&gt;Compare Embedding Models&lt;/li&gt;
      &lt;li&gt;Train Your First Embedding Models&lt;/li&gt;
      &lt;li&gt;Bonus! Doing More with SpaCy&lt;/li&gt;
      &lt;li&gt;Quiz: Check Your Understanding of Embedding Techniques&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I tried to make the course more interesting and engaging by working on classic texts and funky song lyrics.
Among other things we study the white rabbit in Alice in Wonderland, aliens in War of the Worlds and love and swords in Shakespeare.&lt;/p&gt;

&lt;p&gt;Enjoy&lt;/p&gt;

&lt;p&gt;Alexis&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Feb 2021 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/nlp/2021/02/02/intro_nlp_course_openclassrooms.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/nlp/2021/02/02/intro_nlp_course_openclassrooms.html</guid>
        
        
        <category>nlp</category>
        
      </item>
    
      <item>
        <title>Best practices when sharing your data analysis - Jupyter Notebooks</title>
        <description>&lt;h1 id=&quot;tips-to-make-you-data-analysis-easier-to-share&quot;&gt;Tips to make you data analysis easier to share&lt;/h1&gt;

&lt;h3 id=&quot;context&quot;&gt;Context:&lt;/h3&gt;

&lt;p&gt;You work on a large dataset let’s say over 1Gb. You do an analysis. And you want to share&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the data&lt;/li&gt;
  &lt;li&gt;the script / jupyter notebook&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;so that other people can work / reproduce / tweak your results.&lt;/p&gt;

&lt;p&gt;Here are a few personal tips to make things easier for the poor schmuck / schmuckette who has to read your code.&lt;/p&gt;

&lt;h3 id=&quot;compressed-data-in-a-bucket&quot;&gt;Compressed data in a bucket&lt;/h3&gt;

&lt;p&gt;Host your data on S3, google storage, Azure, dropbox etc …. whatever fits your mood as long as it can provide a unique URI.&lt;/p&gt;

&lt;p&gt;Sharing datasets in an email, or in google drive is flaky and confusing.
Drive is not the right place to host datasets. Space is limited, and access control can be hazy.&lt;/p&gt;

&lt;p&gt;By hosting the dataset on the cloud:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;your data has a unique URI. It is centralized, and you can easily enforce versions of the data.&lt;/li&gt;
  &lt;li&gt;You control who has access.&lt;/li&gt;
  &lt;li&gt;You are not limited in space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you share your notebook, the data is downloaded using this unique URI instead of&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;this is my local path, don't forget to change it to your own local path&amp;gt;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, managing access permissions on specific items in the cloud can be a &lt;a href=&quot;https://www.trek10.com/blog/aws-s3-permissions/&quot;&gt;real pain&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;By the way &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pd.read_csv&lt;/code&gt; natively reads gzipped files :). Just add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compression='gzip'&lt;/code&gt; parameter:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df = pd.read_csv('S3_bucket/sample.tar.gz', compression='gzip')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;host-your-notebook-on-google-colab-or-on-a-similar-executable-platform&quot;&gt;host your notebook on google colab or on a similar executable platform&lt;/h3&gt;

&lt;p&gt;Your script may be efficient, bug free, superbly commented out etc … but still end up only working only on your platform.
I’ve had the case recently of a friend, not particularly python savvy, trying to open a 1.9 Gb text file on a windows machine and being faced with abstruse unicode errors. He was stuck. However, the same script worked like a charm on my mac.&lt;/p&gt;

&lt;p&gt;So hosting the notebook on Google Colab will go a long way to make it reproducible without undue efforts.&lt;/p&gt;

&lt;h3 id=&quot;demo-and-prod-mode&quot;&gt;Demo and Prod mode&lt;/h3&gt;

&lt;p&gt;If the data is large and running the whole notebook takes forever, it’s always a good idea to implement 2 modes: a sanbox and a production one with a simple test.
Something as simple as:
    MODE = “demo”
    MODE = “prod”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if MODE == 'demo':
    # subsample the large initial dataset
else:
    # basically do nothing

# then the rest of the code and results etc ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This way the recipient of your analysis can run the whole script quickly and start playing with the parameters and results right away instead of having to wait for loops or apply lambdas to finish.&lt;/p&gt;

&lt;p&gt;You can choose whichever mode as the default one depending on your audience.&lt;/p&gt;

&lt;h3 id=&quot;one-operation-per-cell&quot;&gt;One operation per cell&lt;/h3&gt;

&lt;p&gt;Following the single responsibility principle, is an excellent practice when working with jupyter noteboooks.&lt;/p&gt;

&lt;p&gt;The single responsibility principle states that &lt;em&gt;every module, class, function should have responsibility over a single part of the functionality provided by the script.&lt;/em&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_responsibility_principle&quot;&gt;wikipedia single responsibility principle&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This allow the user to insert other cells to explore the resulting objects and data. Very useful.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://thoughtbot.com/blog/back-to-basics-solid&quot;&gt;ruby&lt;/a&gt; community is very strong on that single responsibility principle with excellent results in terms of bug reduction, readability and maintainability of the code.&lt;/p&gt;

&lt;h3 id=&quot;structure-your-code&quot;&gt;Structure your code&lt;/h3&gt;
&lt;p&gt;The more structure the better. By default&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I always import all the libraries,&lt;/li&gt;
  &lt;li&gt;then define all the functions,&lt;/li&gt;
  &lt;li&gt;load the data (from AWS S3). (subset the datsaet in DEMO mode.)&lt;/li&gt;
  &lt;li&gt;make sure it’s as expected,&lt;/li&gt;
  &lt;li&gt;explore it (df.head(), df.describe())&lt;/li&gt;
  &lt;li&gt;and finally dive into the problem at hand&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;from-start-to-finish-is-your-notebook-really-running&quot;&gt;From start to finish, is your notebook really running?&lt;/h3&gt;

&lt;p&gt;The main default of Jupyter notebooks is the lost state problem where a cell depends on previous runs of other cells which may have already been modified. So just making sure everything works as intended from importing the libraries to the end results is adamant before sharing.&lt;/p&gt;

&lt;h3 id=&quot;add-a-requirements-file&quot;&gt;Add a requirements file&lt;/h3&gt;

&lt;p&gt;I find this optional but that’s just my ingrained laziness. See this post for more on the subject by &lt;a href=&quot;https://twitter.com/jakevdp?lang=en&quot;&gt;JakevanderPlas&lt;/a&gt; on &lt;a href=&quot;https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/&quot;&gt;Installing Python Packages from a Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;super-meaningful-variables-names&quot;&gt;super meaningful variables names.&lt;/h3&gt;
&lt;p&gt;Can’t emphasize this one enough. I often spend significant amounts of time looking for synonyms that will convey precisely the true nature of an important variable to a reader, myself included.
The time gained by abbreviating any variable will be lost a thousand fold later on when trying to figure out what the variable stands for.&lt;/p&gt;

&lt;h3 id=&quot;and-do-follow-coding-best-practice-such-as&quot;&gt;And do follow coding best practice such as:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Alternate code with comments specific markdown cells and data exploration cells&lt;/li&gt;
  &lt;li&gt;Keep the code &lt;a href=&quot;https://en.wikipedia.org/wiki/Don't_repeat_yourself&quot;&gt;DRY&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;comment and exploration cells&lt;/li&gt;
  &lt;li&gt;avoid loops and prefer array operations&lt;/li&gt;
  &lt;li&gt;comment and exploration cells&lt;/li&gt;
  &lt;li&gt;insert test cells dedicated to &lt;a href=&quot;https://dbader.org/blog/python-assert-tutorial&quot;&gt;asserting&lt;/a&gt; that what you have is what you expect&lt;/li&gt;
  &lt;li&gt;comment and exploration cells&lt;/li&gt;
  &lt;li&gt;etc …&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comments should focus on explaining the choices made in terms of methods and parameters. Not simply rephrasing the code.&lt;/p&gt;

&lt;h3 id=&quot;elsewhere&quot;&gt;Elsewhere:&lt;/h3&gt;

&lt;p&gt;Google has a longer, more precise list of excellent &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/best-practices-that-can-improve-the-life-of-any-developer-using-jupyter-notebooks&quot;&gt;best practices when working on Google Colab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A good paper on &lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007&quot;&gt;Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Please drop me a line on &lt;a href=&quot;https://twitter.com/alexip&quot;&gt;twitter @alexip&lt;/a&gt; if you’d like to add something or comment an item.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
</description>
        <pubDate>Sat, 15 Feb 2020 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/datascience/2020/02/15/jupyter_notebooks_sharing_best_practices.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/datascience/2020/02/15/jupyter_notebooks_sharing_best_practices.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>New statistical learning course on openclassroooms</title>
        <description>&lt;p&gt;I am very excited to announce that my new course on statistical learning is now available on openclassrooms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data&quot;&gt;Design Effective Statistical Models to Understand Your Data&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;clear: both; display: table; width: 90%; margin-bottom: 20px; margin-left: 10%; float: center;&quot;&gt;
&lt;img src=&quot;/assets/openclassrooms_course.png&quot; alt=&quot;Design Effective Statistical Models&quot; width=&quot;80%&quot; style=&quot; float: center&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In this course I explore linear, logistic and polynomial regression with hands on exercises, real-world use-cases and non trivial datasets.&lt;/p&gt;

&lt;p&gt;Regression is the mother of all statistical models. Simple, flexible and highly interpretable.&lt;/p&gt;

&lt;p&gt;To shine a new light on such a venerable topic, I decided to bridge the gap between the classic statistical approach and the machine learning one.&lt;/p&gt;

&lt;p&gt;Regression in the statistical sense aims at modeling the inner dynamics of a dataset. The method uses multiple statistical tests to validate the relevance and reliability of the observations and results.&lt;/p&gt;

&lt;p&gt;On the other hand, the machine learning approach strives to build models that perform well on previously unseen data. We no longer care about p-values, null hypothesis or statistical tests but focus instead on the performance of the trained model on new data.&lt;/p&gt;

&lt;p&gt;The great thing is that we can use the same simple modeling techniques, linear regression to illustrate both approaches. Bridging the gap between statistical modeling and machine learning.&lt;/p&gt;

&lt;p&gt;Here’s the  outline of  the course:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I - Understand the Fundamentals of Statistical Modeling&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/5873603-review-the-fundamental-vocabulary&quot;&gt;Review the Fundamental Vocabulary&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6191896-assess-linearity&quot;&gt;Assess Linearity&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6199591-calculate-correlation&quot;&gt;Calculate Correlation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6208146-test-a-hypothesis&quot;&gt;Test a Hypothesis&lt;/a&gt;&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/exercises/3456&quot;&gt;Quiz: Test Your Knowledge on Linearity, Correlation, and Hypothesis Testing&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;II - Build Linear Regression Models&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6229141-build-and-interpret-a-univariate-linear-regression-model&quot;&gt;Build and Interpret a Univariate Linear Regression Model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6229526-build-and-interpret-a-multivariate-linear-regression-model&quot;&gt;Build and Interpret a Multivariate Linear Regression Model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6232921-check-assumptions-of-regression&quot;&gt;Check Assumptions of Regression&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233001-appreciate-ordinary-least-square-and-maximum-likelihood-estimation&quot;&gt;Appreciate Ordinary Least Square and Maximum Likelihood Estimation&lt;/a&gt;&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/exercises/3512&quot;&gt;Quiz: Test Your Knowledge on Building Linear Regression Models!&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;III - Build Generalized Linear Models&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233016-build-and-interpret-a-logistic-regression-model&quot;&gt;Build and Interpret a Logistic Regression Model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233031-handle-categorical-predictors&quot;&gt;Handle Categorical Predictors&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233046-build-and-interpret-a-polynomial-regression-model&quot;&gt;Build and Interpret a Polynomial Regression Model&lt;/a&gt;&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/exercises/3519&quot;&gt;Quiz: Test Your Knowledge on Building Generalized Linear Models!&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;IV - Build Resilient Predictive Models&lt;/strong&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233071-select-the-best-predictive-model&quot;&gt;Select the Best Predictive Model&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/6233091-evaluate-classification-models&quot;&gt;Evaluate Classification Models&lt;/a&gt;&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://openclassrooms.com/en/courses/5873596-design-effective-statistical-models-to-understand-your-data/exercises/3572&quot;&gt;Quiz: Test Your Knowledge About Predictive Models!&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The course notebooks are available on github: &lt;a href=&quot;https://github.com/OpenClassrooms-Student-Center/Design-Statistical-Models&quot;&gt;Notebooks for the course Design Statistical Models on OpenClassrooms&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Oct 2019 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/datascience/2019/10/21/regression-course-openclassrooms.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/datascience/2019/10/21/regression-course-openclassrooms.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>Teaching Data Science at UM6P</title>
        <description>&lt;p&gt;Teaching Data Science is demanding, often intense, sometimes exhausting but always an enriching and extremely rewarding experience. There are magical moments of teacher-student resonance when you can feel the knowledge flowing across the room.&lt;/p&gt;

&lt;p&gt;I had the chance of teaching a 2-week session of predictive analytics in Morocco in the fall 2018 with students of the &lt;a href=&quot;https://www.emines-ingenieur.org/en/&quot;&gt;Emines school of industrial management&lt;/a&gt; in their fourth and final year.&lt;/p&gt;

&lt;div style=&quot;clear: both; display: table; width: 50%; margin-left: 0%; float: right&quot;&gt;
&lt;img src=&quot;/assets/um6p_campus.png&quot; alt=&quot;Mohammed VI Polytechnic University Campus&quot; width=&quot;90%&quot; style=&quot; float: right&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;In 2018, the renowned &lt;a href=&quot;https://www.polytechnique.edu/en&quot;&gt;École Polytechnique&lt;/a&gt;, &lt;a href=&quot;https://www.um6p.ma/&quot;&gt;Mohammed VI Polytechnic University&lt;/a&gt; and the Foundation of École Polytechnique &lt;a href=&quot;https://www.polytechnique.edu/en/content/lx-launched-chair-data-science-and-industrial-processes-morocco&quot;&gt;launched a Chair&lt;/a&gt; in “Data Science and Industrial Processes” in Morocco which I inaugurated on the amazing &lt;a href=&quot;https://www.um6p.ma/&quot;&gt;UM6P&lt;/a&gt; campus of the &lt;a href=&quot;https://www.emines-ingenieur.org/en/&quot;&gt;Emines school of industrial management&lt;/a&gt; located in Benguerir Morocco. This campus was designed by the famous architect Ricardo Bofill in 2011. It is quite a sight to &lt;a href=&quot;http://www.ricardobofill.com/projects/universite-mohammed-vi-polytechnique/&quot;&gt;see&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=zk0TU_fIEhc&quot;&gt;watch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The desired outcome of this 2-week course was to give the students a deep and practical understanding of data science and machine-learning in terms of scope and tools with a focus on building a hands-on data-driven experience.&lt;/p&gt;

&lt;p&gt;For the students to be able to work on relevant data science problems within just two weeks of teaching is a real challenge. Data science is a multi-faceted activity and several domains have to be taught in parallel: &lt;strong&gt;software engineering&lt;/strong&gt; in order to build robust and reproducible scripts, &lt;strong&gt;data science&lt;/strong&gt; techniques and workflows to generate reliable results, &lt;strong&gt;probabilistic theory&lt;/strong&gt; underlying machine-learning algorithms to go beyond the simple copy-pasting usage of &lt;a href=&quot;https://scikit-learn.org/&quot;&gt;popular libraries&lt;/a&gt;, and most of all, an engineer &lt;strong&gt;can-do attitude&lt;/strong&gt; required to tackle real world problems. Data science demands pragmatism, resilience, creativity from students and experienced data scientists alike.&lt;/p&gt;

&lt;p&gt;The course was organized to leave a large place to interactions and Q&amp;amp;A between the students and the teacher and to limit the amount of time dedicated to slides-based lectures. Every time a new machine-learning concept or modelisation algorithm was introduced, it was followed by lab work using applied to datasets on increasing complexity and scope. The goal being to expose the students to real-world issues that data scientists often face in predictive analytics projects. The course slides, datasets and scripts are available on &lt;a href=&quot;https://github.com/alexisperrier/XEmines-datascience&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&quot;clear: both; display: table; width: 50%; margin-left: 0%; float: left&quot;&gt;
&lt;img src=&quot;/assets/um6p_students.png&quot; alt=&quot;Mohammed VI Polytechnic University Students&quot; width=&quot;90%&quot; style=&quot; float: left&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;In parallel, the students participated in the &lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot;&gt;Iowa Housing Kaggle competition&lt;/a&gt; to practice their newly acquired skills in a challenging project. Their enthusiasm was impressive as exclamations of joy broke silent phases of deep focus whenever someone reached a new high score.&lt;/p&gt;

&lt;p&gt;To have had the privilege of launching this first session of the DSIP chair with such accomplished and convivial students and engineers was a fantastic experience and a great honor. Thanks to the excellence and enthusiasm of UM6P Emines students, the learning outcomes have been fully reached.&lt;/p&gt;

&lt;p&gt;I will have the pleasure to close this year’s data science course in March 2019 by teaching a week-long course this time focused on probabilistic programming and PyMC3.&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Feb 2019 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/datascience/2019/02/15/datascience-emines.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/datascience/2019/02/15/datascience-emines.html</guid>
        
        
        <category>datascience</category>
        
      </item>
    
      <item>
        <title>Reduce GPU costs with startup scripts on the Google Cloud Engine</title>
        <description>&lt;h1 id=&quot;reduce-gpu-costs-with-on-demand-instances-and-startup-scripts&quot;&gt;Reduce GPU costs with on demand instances and startup scripts&lt;/h1&gt;

&lt;div style=&quot;clear: both; display: table; width: 100%; margin-left: 3%&quot;&gt;
&lt;img src=&quot;/assets/gcp/compute_engine.png&quot; alt=&quot;Google Compute Engine&quot; width=&quot;30%&quot; style=&quot; float: left&quot; /&gt;
&lt;img src=&quot;/assets/gcp/cloud_billing_API.png&quot; alt=&quot;Billing&quot; width=&quot;30%&quot; style=&quot; float: left;&quot; /&gt;
&lt;img src=&quot;/assets/gcp/GPU.png&quot; alt=&quot;GPU&quot; width=&quot;30%&quot; style=&quot; float: left;&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;This post is about leveraging on demand capabilities of costly virtual instances on the Google Cloud Engine using startup scripts.&lt;/p&gt;

&lt;h1 id=&quot;deep-learning-is-expensive&quot;&gt;Deep Learning is expensive&lt;/h1&gt;

&lt;p&gt;Here’s the situation: You’re working on some large dataset, and you feel the irresistible urge to release the Deep Learning beast on your models with VMs armed to the teeth with GPUs.&lt;/p&gt;

&lt;p&gt;Since your local Macbook steps into the twilight zone every time you launch Keras, you decide to spin up a dragster style, GPU-powered VM on the Google Platform, AWS or Azure. Once the VM is ready, which in truth may take several days if it’s your first encounter with the CUDA Toolkit, you ssh into the VM and start working on your data and your models.&lt;/p&gt;

&lt;p&gt;After a few hours of work, you’re still working on your scripts, cleaning up the data, training models, evaluating, and on and on. Time passes on while the earth pursues its never ending spin in the interplanetary void. When you realize your brain has as much jitsu left as a greek yogurt, you decide to call it a day and give the whole thing a rest. And of course, you sometimes forget to stop the instance. Your cash reserves leak out cent after cent, dollar/euro/pound after dollar/euro/pound throughout the night.&lt;/p&gt;

&lt;p&gt;All these hours do add up. And at the end of the months you realize GPUs are way more expensive than you ever imagined. But hey Deep learning is really fun. Can’t stop now. Just need a few more hours. After all nobody really understands how these neural network work, do they? And you really need to practice to be able to call yourself a Deep Learning expert. Please just a few more hours of GPUs. Just 30 minutes, … I swear, …. come on!&lt;/p&gt;

&lt;p&gt;So what’s a data scientist to do?&lt;/p&gt;

&lt;p&gt;One solution is to go back to random forests and SVMs and give up on the whole deep learning thing. After all, as &lt;a href=&quot;https://www.youtube.com/watch?v=5mvfpSdWsOo&quot;&gt;Vladimir Vapnik&lt;/a&gt; says, Deep Learning is just &lt;a href=&quot;https://medium.com/intuitionmachine/the-brute-force-method-of-deep-learning-innovation-58b497323ae5&quot;&gt;brute force&lt;/a&gt; training with a whole lot of data.&lt;/p&gt;

&lt;p&gt;The other solution is to make the most of the &lt;strong&gt;on demand&lt;/strong&gt; promise in cloud computing.&lt;/p&gt;

&lt;h2 id=&quot;on-demand-and-serverless&quot;&gt;On demand and serverless&lt;/h2&gt;

&lt;p&gt;The whole promise of cloud computing is that you can spin up and release resources as needed.
Way back in the early 2000s that mostly meant being able to add servers on the fly to support your traffic exploding when BoingBoing or Gizmodo suddenly put your startup on their front page.
But for machine learning the same on demand concept is relevant when extra high computing power is needed.
When working with Deep Learning, most of the mundane work of data cleaning and shaping can probably be carried out on your local machine or a low level VM. The only time GPU enabled VMs are truly needed is to train the Deep Learning models.&lt;/p&gt;

&lt;p&gt;Which means that a resource conservative workflow should look like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) Local: ETL, extract and explore dataset, clean and format data for consumption by Keras / Tensorflow / ….&lt;/li&gt;
  &lt;li&gt;2) Storage: Upload the properly formatted data to storage (GCS, S3, SQL, BQ) to make accessible by the VM&lt;/li&gt;
  &lt;li&gt;3) Local:&lt;strong&gt;Create a GPU enabled VM&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;4) EC2, GCE: run your Keras / TF / Pytorch script to train your model&lt;/li&gt;
  &lt;li&gt;5) EC2, GCE: Store intermediate and final results to storage, so you can access it from local&lt;/li&gt;
  &lt;li&gt;6) &lt;strong&gt;Shutdown or delete the VM when the script has finished running&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;7) Local: retrieve the models, make some predictions, evaluations, selections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, &lt;em&gt;Local&lt;/em&gt; can be replaced by a smaller, less powerful VM running on CPUs and not GPUs.&lt;/p&gt;

&lt;p&gt;With this workflow, you only spend money on expensive cloud resources on steps 3) and 4), potentially saving you significant amounts of cash at the end of the day. If you can manage to shutdown or even delete the VM once the script has finished running then you won’t even run the risk of leaving it running all through the night! Brilliant!&lt;/p&gt;

&lt;p&gt;So in order to limit our resources, we need to be able to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) Create a VM in the fly&lt;/li&gt;
  &lt;li&gt;2) Run a script on the VM from your local&lt;/li&gt;
  &lt;li&gt;3) Terminate the VM when the script is done&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And we should do that (create, run, terminate) every time we want to  test a new version of the dataset, a new DL architecture or new parameters.
We could even potentially run several trials in parallel.&lt;/p&gt;

&lt;p&gt;Let’s go!&lt;/p&gt;

&lt;h2 id=&quot;create-a-vm-on-the-fly-from-an-existing-disk&quot;&gt;Create a VM on the fly from an existing disk&lt;/h2&gt;

&lt;p&gt;I assume here that you already have created a VM and installed everything needed to run your scripts. Things like the conda distribution, scikit-learn, keras, GPUs and so forth. See &lt;a href=&quot;https://www.datacamp.com/community/tutorials/google-cloud-data-science&quot;&gt;How to setup a VM for data science on GCP &lt;/a&gt; and &lt;a href=&quot;https://hackernoon.com/launch-a-gpu-backed-google-compute-engine-instance-and-setup-tensorflow-keras-and-jupyter-902369ed5272&quot;&gt;Launch a GPU-backed Google Compute Engine instance&lt;/a&gt; for more details (also &lt;a href=&quot;https://vxlabs.com/2017/03/17/miniconda3-tensorflow-keras-on-google-compute-engine-gpu-instance-the-step-by-step-guide/&quot;&gt;this one&lt;/a&gt;) to install the &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit&quot;&gt;CUDA Toolkit&lt;/a&gt; and the &lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;cuDNN&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;For those in a hurry, here’s an example of the command line for creating a &lt;a href=&quot;https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance&quot;&gt;preemptible&lt;/a&gt; VM on Google Cloud Engine (Ubuntu 17.10 with 50gb disk space). Preemptible VMs are temporary but way way cheaper than non preemptible ones. Great for research work not so much for production APIs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud beta compute --project &quot;&amp;lt;project_name&amp;gt;&quot; instances create &quot;&amp;lt;instance_name&amp;gt;&quot; --zone &quot;us-east1-c&quot; \
--machine-type &quot;n1-standard-1&quot; --subnet &quot;default&quot; --no-restart-on-failure --maintenance-policy &quot;TERMINATE&quot; \
--preemptible --service-account &quot;&amp;lt;your_service_account&amp;gt;&quot;  --image &quot;ubuntu-1710-artful-v20180126&quot; \
--image-project &quot;ubuntu-os-cloud&quot; --boot-disk-size &quot;50&quot; --no-boot-disk-auto-delete \
--boot-disk-type &quot;pd-standard&quot; --boot-disk-device-name &quot;&amp;lt;disk_name&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above and quite longish command, the non obvious but important flags are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--no-boot-disk-auto-delete&lt;/code&gt;: The disk will not be deleted when the instance is deleted&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--preemptible&lt;/code&gt;: makes the VM temporary and saves money&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--service-account &quot;&amp;lt;your_service_account&amp;gt;&quot;&lt;/code&gt;: A &lt;a href=&quot;https://cloud.google.com/compute/docs/access/service-accounts&quot;&gt;service account&lt;/a&gt; is used by your VM to interact with other Google Cloud Platform APIs. The default service account is identifiable with that email &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[PROJECT_NUMBER]-compute@developer.gserviceaccount.com&lt;/code&gt; where the [PROJECT_NUMBER] can be found on your &lt;a href=&quot;https://console.cloud.google.com/home/dashboard&quot;&gt;project dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So let’s assume that your have created your initial disk and deleted the associated VM. The disk is now free to be used to create a new VM.&lt;/p&gt;

&lt;p&gt;The following command line creates a preemptible VM from that disk&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud compute instances create &amp;lt;instance name&amp;gt; --disk name=&amp;lt;disk name&amp;gt;,boot=yes --preemptible
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ok so we can create a VM on the fly based on that disk. Now we want to run a script on that VM. Let’s say a python script.&lt;/p&gt;

&lt;p&gt;The simplest way is to use the SSH command with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--command&lt;/code&gt; flag&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud compute ssh &amp;lt;instance name&amp;gt; \
--command '{Absolute path to }/python {absolute path to}&amp;lt;the script&amp;gt;.py'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and low and behold, that command will display the output of the remote python script on your local terminal. Try for instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcloud compute ssh &amp;lt;instance name&amp;gt; --command 'ls -al'&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A more sophisticated way is to automatically run a script (shell, python, R, whatever…) when the VM is created by using startup scripts&lt;/p&gt;

&lt;p&gt;For instance, we could want to run the following shell script on creation&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#! /bin/bash
sudo apt-get update

printf '%s %s Some log message \n' $(date +%Y-%m-%d) $(date +%H:%M:%S) &amp;gt;&amp;gt; '{absolute_path}/startup_script.log'

# add and activate the github keys
eval &quot;$(ssh-agent -s)&quot;
ssh-add  {path to github key}

# log script start
cd {path to application folder}

# git update application
git pull origin master
# run script
{path to }/python {absolute path to}/&amp;lt;the script&amp;gt;.py

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That script activates the github keys, updates the application from github, and runs the script. Pretty neat.
Creating the VM and making sure the VM runs that script is just an extension of the above VM creation command line
But first you need to make the script available to the VM by uploading to Google Storage with gsutil&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gsutil cp {local}/&amp;lt;shell script&amp;gt; gs://{bucket name}/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;for more gsutil example check my post on &lt;a href=&quot;Top gsutil examples for Google Cloud Storage&quot;&gt;https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And now we can create the VM and have the script run on start.  The following command line will attach the script to the VM as a startup script.
Every time the VM is started the script will be executed.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud compute instances create &amp;lt;instance name&amp;gt; --disk name=&amp;lt;disk name&amp;gt;,boot=yes --preemptible \
--scopes storage-ro \
--metadata startup-script-url=gs://{bucket name}/&amp;lt;shell script&amp;gt;  --preemptible
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;now-shutdown-that-vm&quot;&gt;Now shutdown that VM!&lt;/h2&gt;

&lt;p&gt;So we are now able to&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;start a VM on demand from a disk&lt;/li&gt;
  &lt;li&gt;run a script on that VM from our local&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We just need to find a way to terminate that VM once the script has finished running.
This is where things get tricky mainly because it can be difficult to know with certainty when the script has finished running.&lt;/p&gt;

&lt;p&gt;The simplest solution is to add the following shutdown line at the end of the startup script&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shutdown -h now
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This forces the VM to stop once the script is done. The VM still exists and is not deleted.
In terms of pricing, this might just be enough as the VM is not priced when it’s idle. From the &lt;a href=&quot;https://cloud.google.com/compute/docs/instances/stopping-or-deleting-an-instance#billing_for_stopped_instances&quot;&gt;google doc&lt;/a&gt;: &lt;em&gt;Instances that are in a TERMINATED state are not charged …&lt;/em&gt;. The associated disks, IPs, and other resources are still billed but  not the VM.&lt;/p&gt;

&lt;p&gt;It’s also possible to not only shutdown the VM but also delete it from within the VM. In other words having the VM commit seppuku.
I mean specially if the script fails to run, the VM should take the blame for having failed its master (You) and rightly self suicide, makes sense. In a way maybe. The following code is from &lt;a href=&quot;https://groups.google.com/forum/#!topic/gce-discussion/fgZIZFaDhp8&quot;&gt;this thread&lt;/a&gt;. It can run from within the VM. It requires a bit of tuning on the &lt;a href=&quot;https://cloud.google.com/compute/docs/authentication#using&quot;&gt;service account scopes&lt;/a&gt; for it to have the proper permissions and actually work.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcloud compute instances delete $myName --zone=$myZone --quiet
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the name of the VM comes from  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;myName=$(hostname)&lt;/code&gt;
and the zone from&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Get the zone
zoneMetadata=$(curl &quot;https://metadata.google.internal/computeMetadata/v1/instance/zone&quot; -H &quot;Metadata-Flavor:Google&quot;)
# Split on / and get the 4th element to get the actual zone name
IFS=$'/'
zoneMetadataSplit=($zoneMetadata)
myZone=&quot;${zoneMetadataSplit[3]}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The only problem with that approach (stopping or deleting from the startup script) is that every time you start the VM, well, it will run the script and shutdown, and at the same time preventing you from ssh’ing into it to check the logs, make some modifications or inspect the results. Bummer!&lt;/p&gt;

&lt;p&gt;The other solution would be to have your main model training script write a status update to a file or an external storage bucket or database, and from your local, regularly check the status of the script before deciding to the the shutdown command.&lt;/p&gt;

&lt;p&gt;In the end, although a bit hacky, I think this is the best solution. Your whole workflow becomes now&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create the VM and set it up with all the right libraries and GPUs. Make sure the disk is persistent.&lt;/li&gt;
  &lt;li&gt;Delete the VM&lt;/li&gt;
  &lt;li&gt;Write the startup script that runs your model training python code. Make sure the python code exports its status (running, failed, done) to some external resources or some internal file as its being executed. Upload the startup script to a bucket on Google Storage&lt;/li&gt;
  &lt;li&gt;Create (1st time) / start (subsequent times) the VM and associate the startup script to it&lt;/li&gt;
  &lt;li&gt;Have a local &lt;em&gt;conductor shutdown&lt;/em&gt; script regularly check the status of the training python execution on the VM and shutdown the VM when the status is &lt;em&gt;failed&lt;/em&gt; or &lt;em&gt;done&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: Instead of using a startup script, you could also simply run the python model code using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcloud compute ssh -- command &amp;lt;python script&amp;gt;&lt;/code&gt; command in conjunction with the local &lt;em&gt;conductor shutdown&lt;/em&gt; script. But I feel the use of start up scripts basically dedicates the VM to that usage and that usage only. Similarly to writing good quality code, where a method or function should do only one thing at a time, my feeling is that a VM should be used only for one goal and one goal only. After all you can have as many VMs as you like as long as they are idle. Disks prices are low and usually not a problem.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The whole point behind using cloud resources is to leverage its at-will / on-demand capabilities to reduce costs.
Doing so requires using startup scripts and some external monitoring to shutdown the VM once the task has been completed.&lt;/p&gt;

&lt;p&gt;This is of course just one way of doing things.&lt;/p&gt;

&lt;p&gt;Please let me know in the comments, how &lt;strong&gt;YOU&lt;/strong&gt; manage costly on demand instances. And thanks for reading this post until the end.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you liked this post, please &lt;a href=&quot;https://twitter.com/intent/tweet?url=https://alexisperrier.com/gcp/2018/02/21/gpu-costs-compute-engine.html&amp;amp;text=Reduce GPU costs with startup scripts on the Google Cloud Engine&amp;amp;via=alexip&quot; target=&quot;_blank&quot;&gt; share it on twitter&lt;/a&gt;
And leave me your feedback, questions, comments, suggestions below.
Much appreciated :)
&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Wed, 21 Feb 2018 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/gcp/2018/02/21/gpu-costs-compute-engine.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/gcp/2018/02/21/gpu-costs-compute-engine.html</guid>
        
        
        <category>gcp</category>
        
      </item>
    
      <item>
        <title>iPhone addiction? Get a grip!</title>
        <description>&lt;h1 id=&quot;is-it-a-dna-thing&quot;&gt;Is it a DNA thing?&lt;/h1&gt;

&lt;p&gt;My wife has a super power! She is totally immune to the constant nagging of her iPhone. She has an amazing ability to resist checking her emails every 5 minutes, texting back on the spot and playing the whack-a-notification game all day long. Maybe it’s in her DNA. After all, some genes do &lt;a href=&quot;https://animalcare.umich.edu/genetic-markers-influence-addiction-discovered-rat-study&quot;&gt;influence predisposition to addiction&lt;/a&gt;.
I used to be annoyed by her nonchalance. Now I’m just envious.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/iphone_addiction.png&quot; alt=&quot;iphone addiction&quot; width=&quot;300&quot; style=&quot;float:left; padding:10px;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![iphone addiction](https://alexisperrier.com/assets/iphone_addiction.png) --&gt;

&lt;p&gt;Like millions of people across the globe, I’m an smartPhone / iPhone addict.
Until very recently,  I used to frenetically check my emails and twitter feed all the time. Probably hundreds of times a day and even at night. Watching TV, talking to my kids, eating, commuting, waking up or before going to bed, ….&lt;/p&gt;

&lt;p&gt;This has not been a positive factor on my state of mind and overall mood. As numerous studies show, &lt;a href=&quot;https://news.illinois.edu/view/6367/334240&quot;&gt;Mobile device addiction&lt;/a&gt; can generate or reinforce depression by trapping us in an endless cycle of meaningless social interactions. &lt;strong&gt;A &lt;em&gt;like&lt;/em&gt; does not replace a hug&lt;/strong&gt; or an eye contact. We have become hamsters in cages who can no longer refrain to hit the switch to check if there’s food coming. Maybe you’re not there yet, I know I was.&lt;/p&gt;

&lt;p&gt;This addiction phenomenon is not a side effect of using mobile phones. The never stopping pull from our phones is a feature not a bug. On this subject read the works of Robert Lustig of the Pediatric Endocrine Society: &lt;a href=&quot;https://www.amazon.com/Hacking-American-Mind-Corporate-Takeover/dp/1101982586&quot;&gt;The Hacking of the American Mind: The Science Behind the Corporate Takeover of Our Bodies and Brains&lt;/a&gt;.  To sum up, the problem with compulsive mobile device interactions, is that we overstimulate a short-term reward mechanism in our brain. Each new interaction has the potential (but not always) to relieve a dopamine hit that excites our pleasure brain center. Quoting this article from the FT on how &lt;a href=&quot;https://www.ft.com/content/19de6f72-60aa-11e7-91a7-502f7ee26895&quot;&gt;Our digital addiction makes us miserable&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Over time our brains become conditioned to hoping that each click leads to a bigger and better hit than the last, or that the next social media response will be more flattering to our ego than the one before.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So I decided to fight back. Here are the 5 simple tricks I’ve implemented. The good news is that contrary to stronger addiction mechanism (think cigarettes), smartphone addiction simply disappears after a few hours and the benefits are immediate.&lt;/p&gt;

&lt;h3 id=&quot;silence-the-notifications&quot;&gt;Silence the notifications&lt;/h3&gt;

&lt;p&gt;This one is obvious.&lt;/p&gt;

&lt;p&gt;The first thing to do is to turn off all the notifications on the phone. Applications are constantly competing for your undivided attention. You can silence them with a bit of time and clicks. Once you’ve turned all the notifications off, you will appreciate no longer being interrupted by a message or a number in red shouting “Look at me, Look at me, there’s something new, come on, do it, …”. This app muting is a wonderful thing for your peace of mind. My phone has stopped screaming at me for attention. Feels great!&lt;/p&gt;

&lt;h3 id=&quot;remove-social-network-apps&quot;&gt;Remove social network apps&lt;/h3&gt;

&lt;p&gt;I am no longer on Facebook. I lost interest a while back. So Facebook was the first app to be removed from my phone.  I am however, a heavy user of twitter. Removing twitter from the phone took more efforts. But doing so instantaneously lowered my phone usage.&lt;/p&gt;

&lt;p&gt;Once twitter had been exfoliated from my phone, removing Feedly, Reddit, and other information apps was easy. By decluttering my phone I no longer had that nagging feeling that maybe there was something exciting that I had not yet read about.&lt;/p&gt;

&lt;h3 id=&quot;a-phone-free-bedroom&quot;&gt;A phone free bedroom&lt;/h3&gt;
&lt;p&gt;The problem with smart phones is that they are loaded with features. Flashlight, camera, alarm clocks, weather, etc … Each one bringing us back to our dopamine delivery device. But &lt;strong&gt;for every app there’s an object that can replace it!&lt;/strong&gt;.
My phone was my alarm clock and flashlight. Buying a real alarm clock and removing the phone from my bedroom had instant impact on my sleep patterns. Better sleep, more reading, taking the time to remember my dreams before attacking the day…., the list of benefits goes on. The phone now sleeps in my kitchen.&lt;/p&gt;

&lt;h3 id=&quot;oldies-but-goodies&quot;&gt;Oldies but goodies&lt;/h3&gt;

&lt;p&gt;Another simple thing to do is to keep your old phone as long as possible. Don’t renew it. I have an iPhone 5. It is slow as hell. It’s buggy. crashes, nearly as bad as a windows 95 experience! But the worse it becomes, the less I feel inclined to use it. By ensuring the interaction becomes more frustrating, I distance myself from this enslaving tamagotchi.&lt;/p&gt;

&lt;h3 id=&quot;the-final-surgery-emails&quot;&gt;The final surgery, emails&lt;/h3&gt;
&lt;p&gt;That was maybe the less obvious step but it was the most effective one in the end. I deleted the email app!. No more emails on my phone. After all, I spend many hours working on my laptop everyday with plenty of time for email checking. An email can always wait a few hours. That was the whole point of asynchronous communications after all. Removing emails from the phone was the last nail in the device coffin. I had finally silenced the siren’s song.&lt;/p&gt;

&lt;h1 id=&quot;whats-next&quot;&gt;What’s next&lt;/h1&gt;

&lt;p&gt;I think I’ve found a good method to get rid of my mobile device addiction. I sleep better, I’m more present with people around me, I read stuffs on paper, and I can concentrate for longer periods of time. And it was after all very easy to do. But time will tell.&lt;/p&gt;

&lt;p&gt;I still check my email and twitter feed way too many times when I’m working on my laptop. So in order to regain control of my time and focus, my plan to build a Bot that will handle all my social network interactions. A sort of social network secretary. Personal Machine Learning to counteract Corporate Machine Learning.  That will take a bit more time and efforts than simple removing apps. Stay tuned!&lt;/p&gt;

&lt;p&gt;In the 1st episode of the TV show, &lt;a href=&quot;https://www.imdb.com/title/tt6257970/&quot;&gt;The end of the f…. world&lt;/a&gt;, you see Alyssa, the teenage girl smash her phone in an act of pure teenage rebellion. Is that the new cool? Freedom from mobile phones, tracking, nagging, liking? Smashing your phone is the new buzz!&lt;/p&gt;

&lt;p&gt;I would love to know how you deal with your phone addiction or lack thereof.&lt;/p&gt;

&lt;p&gt;Please leave a comment below or on twitter &lt;a href=&quot;https://twitter.com/alexip&quot;&gt;@alexip&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 06 Feb 2018 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/life/2018/02/06/control_iphone_addiction.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/life/2018/02/06/control_iphone_addiction.html</guid>
        
        
        <category>life</category>
        
      </item>
    
      <item>
        <title>Top gsutil command lines to get started on Google Cloud Storage </title>
        <description>&lt;p&gt;Google storage is a file storage service available from Google Cloud. Quite similar to Amazon S3 it offers interesting functionalities such as signed-urls, bucket synchronization, collaboration bucket settings, parallel uploads and is S3 compatible. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gsutil&lt;/code&gt;, the associated command line tool is part of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; command line interface.&lt;/p&gt;

&lt;p&gt;After a brief presentation of the &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;Google Cloud Storage&lt;/a&gt; service, I will list the most important and useful &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; command lines and address a few of the service particularities.&lt;/p&gt;

&lt;h1 id=&quot;google-storage&quot;&gt;Google storage&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gcp/google_cloud_storage.png&quot; alt=&quot;Google Cloud Storage Icon&quot; /&gt;
The google storage platform is Google’s Entreprise storage solution. Google Storage offers a classic bucket based file structure similarly to AWS S3 and Azure Storage. Google Storage was introduced in may 2010 as &lt;a href=&quot;https://googlecode.blogspot.com/2010/05/google-storage-for-developers-preview.html&quot;&gt;Google Storage for Developers&lt;/a&gt;, a RESTful cloud service limited at the time to a few hundreds developers. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; the command line tool associated with Google Storage was released at the same time.&lt;/p&gt;

&lt;p&gt;Fast forward to 2018, Google Storage now offers &lt;a href=&quot;https://cloud.google.com/storage-options/&quot;&gt;3 levels of storage&lt;/a&gt; with different accessibility and pricing.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;standard storage&lt;/strong&gt; is for fast access to large amounts of data. It offers high speed of response to requests.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DRA&lt;/strong&gt; is for long-term data storage and infrequent access and is priced lower than standard storage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nearline&lt;/strong&gt; storage is for even less frequent access and offers longer response times. It is the cheapest option.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Storage price structure depends on location and storage class and evolves frequently. At time of writing prices are $0.026 per Gb-month for Standard , $0.01 for Nearline and as low as $0.007 for Coldline storage with a Multi-regional location. See the &lt;a href=&quot;https://cloud.google.com/storage/pricing&quot;&gt;pricing page&lt;/a&gt; for uptodate prices. See also &lt;a href=&quot;https://medium.com/@duhroach/google-cloud-storage-on-a-shoestring-budget-55f054fad436?__s=veqfs39xxza6piky7ktj&quot;&gt;Google Cloud Storage on a shoestring budget&lt;/a&gt;
for an interesting cost breakdown.&lt;/p&gt;

&lt;p&gt;A distinct trait of Google Storage structure is that folders and subfolders within a bucket are not associated with a “&lt;em&gt;physical&lt;/em&gt;” structure as they would be on your local machine. On Google Storage, buckets have &lt;em&gt;virtual&lt;/em&gt; folders. &lt;strong&gt;The full path to a file is interpreted as being the entire filename&lt;/strong&gt;.
Consider for instance, the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hello_world.txt&lt;/code&gt; located in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mybucket/myfolder/&lt;/code&gt;. The file’s URL is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://mybucket/myfolder/hello_world.txt&lt;/code&gt;. Google Storage interprets that file has having the filename &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;myfolder/hello_world.txt&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The slash &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&lt;/code&gt; character is part of the object filename instead of being an indication of an existing folder. As Google calls it, this object naming scheme creates &lt;em&gt;” the illusion of a hierarchical file tree atop the “flat” name space”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Although this is transparent most of the time, virtual paths may results in misplaced files when uploading a folder with multiple subfolders. If the upload fails and needs to be restarted, the copy command will have unexpected results since the folder did not exist in the first upload but does with the second try.&lt;/p&gt;

&lt;p&gt;In order to avoid these weird cases, the best practice, is to make sure to start by creating the expected folder structure and only then upload the files to their target folders.&lt;/p&gt;

&lt;h2 id=&quot;gsutil&quot;&gt;gsutil&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gsutil&lt;/code&gt; is the command line tool used to manage buckets and objects on Google Storage. It is part of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; shell scripts. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gsutil&lt;/code&gt; is fully &lt;a href=&quot;https://github.com/GoogleCloudPlatform/gsutil&quot;&gt;open sourced on github&lt;/a&gt;, and under active development.&lt;/p&gt;

&lt;p&gt;Gsutil goes well beyond simple file transfers with an impressive lists of advanced gsutil features, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ACLs&lt;/strong&gt;: setting access control via Access Control Lists&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;rsync&lt;/strong&gt;: synchronizing folders and buckets&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;lifeline&lt;/strong&gt;: defining lifecycle rules&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;signed urls&lt;/strong&gt;: setting time limited online access ()&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;perfdiag&lt;/strong&gt; for troubleshooting&lt;/li&gt;
  &lt;li&gt;logging, notifications and versioning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before diving in these powerful functionalities, let’s walk through a simple case of file transfer.
If you don’t have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; installed on your local machine or cloud instance, follow the &lt;a href=&quot;https://cloud.google.com/sdk/&quot;&gt;Google Cloud SDK install instructions&lt;/a&gt; for your OS in order to get started. You may need to sign up for a &lt;a href=&quot;https://cloud.google.com/free/&quot;&gt;free trial account&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;getting-around-with-gsutil&quot;&gt;Getting around with gsutil&lt;/h2&gt;

&lt;p&gt;In the following examples, I create a bucket, upload some files, get information on these files, move them around and change the bucket storage class.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First things first. In order to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;help&lt;/code&gt; on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; or any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; sub commands:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;help&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &amp;lt;&lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;help&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Now create a bucket named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bucketname&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All buckets names share a single global Google namespace and must not be already taken.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil mb gs://&amp;lt;bucketname&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that there are certain &lt;a href=&quot;https://cloud.google.com/storage/docs/naming&quot;&gt;restrictions&lt;/a&gt; on bucket naming and creation beyond the uniqueness condition. For instance you cannot change the name of an existing bucket, and a bucket name cannot include the word &lt;em&gt;google&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Upload and download a file with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &amp;lt;local_file&amp;gt; gs://&amp;lt;bucketname&amp;gt;/
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp  &lt;/span&gt;gs://&amp;lt;bucketname&amp;gt;/&amp;lt;remote_file&amp;gt; ./&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;And transfer a file between buckets:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp  &lt;/span&gt;gs://&amp;lt;bucket_A&amp;gt;/&amp;lt;remote_file&amp;gt; gs://&amp;lt;bucket_B&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Create a folder in a bucket with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &amp;lt;new_folder&amp;gt; gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Upload a file to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;new_folder&amp;gt;&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &amp;lt;local_file&amp;gt; gs://&amp;lt;bucketname&amp;gt;/&amp;lt;new_folder&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will create the folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;new_folder&amp;gt;&lt;/code&gt; and at the same time upload the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;local_file&amp;gt;&lt;/code&gt; to that folder. Note the trailing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&lt;/code&gt; that tells &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; to actually interpret &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;new_folder&amp;gt;&lt;/code&gt; as a new folder and not as the target filename. If you omit the trailing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/&lt;/code&gt; gsutil will rename the file with the filename &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;new_folder&amp;gt;&lt;/code&gt; once uploaded and the new folder will not be created.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;List the folder with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;ls &lt;/span&gt;gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Check storage space with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;du&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;du&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt; gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;where the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-h&lt;/code&gt; flag makes it human readable&lt;/p&gt;

&lt;h2 id=&quot;the--r-and--m-flags&quot;&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m&lt;/code&gt; flags&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Copy a local folder and its content to a bucket with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp -r&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; ./&amp;lt;local_folder&amp;gt; gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Consider for instance a local &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./img&lt;/code&gt; directory that contain several image files. We can copy that entire local directory and create the remote folder at the same time with the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; ./img gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The bucket now has the virtual folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/img&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Improve performance with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m&lt;/code&gt; flag&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When moving large number of files, adding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m&lt;/code&gt; flag to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt; will run the transfers in parallel and significantly improve performance provided you are using a reasonably fast network connection.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Wildcards&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;gsutil supports &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt;  wildcards only for files. To include folders in the wildcard target you need to double the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt; sign. For instance, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil ls gs://&amp;lt;bucketname&amp;gt;/**.txt&lt;/code&gt;  will list all the text files in all subdirectories. The &lt;a href=&quot;https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames&quot;&gt;wildcard page&lt;/a&gt; offers more details.&lt;/p&gt;

&lt;h2 id=&quot;gsutil-full-configuration&quot;&gt;Gsutil full configuration&lt;/h2&gt;
&lt;p&gt;Gsutil full configuration is available in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.boto&lt;/code&gt; file. You can edit that file directly or via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil config&lt;/code&gt; command. Some interesting parameters are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parallel_composite_upload_threshold&lt;/code&gt;: to specify the maximum size of a file to be uploaded in a single stream. Files larger than this threshold will be uploaded in parallel. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parallel_composite_upload_threshold&lt;/code&gt; parameter is disabled by default.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;check_hashes&lt;/code&gt;: to enforce integrity checks when downloading data, always, never or conditionally.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefer_api&lt;/code&gt;: to specify the API to use when interacting with cloud storage providers (S3, GCS, …)&lt;/li&gt;
  &lt;li&gt;and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_access_key_id&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws_secret_access_key&lt;/code&gt; for interoperability with S3.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cloud storage compatibility is powerful. Not only can you migrate easily from AWS S3 to GCP or vice versa but you can also sync S3 buckets and GCP buckets with the rsync command.&lt;/p&gt;

&lt;h2 id=&quot;acl&quot;&gt;ACL&lt;/h2&gt;

&lt;p&gt;As stated in the documentation, &lt;em&gt;Access Control Lists (ACLs) allow you to control who can read and write your data, and who can read and write the ACLs themselves&lt;/em&gt;.
ACL are assigned to objects (files) or buckets. By default all files in a bucket have the same ACL as the bucket they’re in.&lt;/p&gt;

&lt;p&gt;ACL has 3 commands&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;GET&lt;/strong&gt;: lists the permissions on a given object. For instance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil acl get gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt; outputs the access settings for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bucketname&amp;gt;&lt;/code&gt; bucket.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SET&lt;/strong&gt;: sets the permissions on a given object. The best way to set the permissions and avoid mistakes is by first exporting them to a file with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil acl get gs://&amp;lt;bucketname&amp;gt;/&amp;lt;filename&amp;gt; act.txt&lt;/code&gt;, modify the acl.txt file and then set the new permissions with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil acl set acl.txt gs://bucket/&amp;lt;filename&amp;gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CH&lt;/strong&gt;: for change, modifies the current permissions on a given object. For instance to grant WRITE access to a user &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil acl ch -u someone@gmail.com:WRITE gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The default settings for buckets are defined with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;defacl&lt;/code&gt; command which also responds to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ch&lt;/code&gt; subcommands. The command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil defacl get gs://&amp;lt;bucketname&amp;gt;/&lt;/code&gt; will return the default settings for the bucket &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bucketname&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Several pre defined setings are available:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;project-private&lt;/strong&gt;: is the default setting for new objects. It gives permission to the project team based on their roles. All team members have READ permission while editors and owners have OWNER permission.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;private&lt;/strong&gt;: Gives the requester OWNER permission for a bucket or object&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;public-read&lt;/strong&gt;: Opens the objects to the whole internet as it gives all users read permission.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;public-read-write&lt;/strong&gt;: The dangerous setting that allows anyone on the internet to upload files to your bucket.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further ACL details are available in the &lt;a href=&quot;https://cloud.google.com/storage/docs/gsutil/commands/acl&quot;&gt;ACL page&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;rsync&quot;&gt;rsync&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil rsync&lt;/code&gt; makes the content of a target folder identical to the content of a source folder by copying, updating or deleting any file in the target folder that has changed in the source folder. This synchronization works across local and GCP folders as well as other gsutil cloud compatible storage solutions such as AWS S3. With the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil rsync&lt;/code&gt; command you have everything you need to create an automatic backup of your data in the cloud. The rsync command follows:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil rsync &amp;lt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;folder&amp;gt; &amp;lt;target folder&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Consider a local folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./myfolder&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bucketname&amp;gt;&lt;/code&gt; bucket, the following command synchronizes the content of the local folder with the storage bucket:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; rsync &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; ./myfolder gs://&amp;lt;bucketname&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The content of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://&amp;lt;bucketname&amp;gt;&lt;/code&gt; will match the content of your local &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./myfolder&lt;/code&gt; directory, effectively backing up the local documents.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Note the presence of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r&lt;/code&gt; flag which ensures that all subfolders are matched.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt; flag is to be used with caution as it will delete the content in the target when deleted from the source. If you inadvertently make a mistake in your command, for instance inverting the source and target folders, you may end up deleting your content. A good way to ensure that does not happen is to enable bucket versioning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you don’t want to have to run the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil&lt;/code&gt; command every time you make a change in the source folder, you can set up a cron job on your local with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;crontab -e&lt;/code&gt; or the equivalent for windows machines. For instance the following cron job will backup your local folder to Google Cloud every 15mn.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;se&quot;&gt;\*&lt;/span&gt;/15  &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; gsutil &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; rsync &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &amp;lt; full path to myfolder&amp;gt; gs://mybucket &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &amp;lt;full path to log file&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;bucket-versioning&quot;&gt;Bucket versioning&lt;/h2&gt;

&lt;p&gt;Bucket versioning is a powerful feature that prevents any file deletion by mistake. Enabling and disabling versioning is done at the bucket level with the command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil versioning &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;on gs://&amp;lt;bucketname&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil versioning &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;off gs://&amp;lt;bucketname&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When versioning is enabled on a bucket, objects become accessible by specifying their version number. Listing the content of a bucket will show the version numbers of its objects as such:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;gs://&amp;lt;bucketname&amp;gt;/&amp;lt;filename&amp;gt;#&amp;lt;version_number_1&amp;gt;
gs://&amp;lt;bucketname&amp;gt;/&amp;lt;filename&amp;gt;#&amp;lt;version_number_2&amp;gt;
gs://&amp;lt;bucketname&amp;gt;/&amp;lt;filename&amp;gt;#&amp;lt;version_number_3&amp;gt;
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To retrieve the correct version, simply append the version number to the object name in the cp command.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/storage/docs/object-versioning&quot;&gt;object versioning&lt;/a&gt; page offers more details on the subject.&lt;/p&gt;

&lt;h2 id=&quot;signed-urls&quot;&gt;Signed URLs&lt;/h2&gt;

&lt;p&gt;Signed URLs is a mechanism for query string authentication for buckets and objects. In other words, Signed urls provide a way to give time-limited read or write access to anyone in possession of the URL, regardless of whether they have a Google account.
To create a signed url you first need to generate a generate a private key following these &lt;a href=&quot;https://console.cloud.google.com/apis/credentials&quot;&gt;instructions&lt;/a&gt;. Click on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Create a service account key&lt;/code&gt;, select  your project, and download the JSON file that contains your private key.&lt;/p&gt;

&lt;p&gt;You can now create a signed urls for one of your file with&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil signurl &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; 10m &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; GET &amp;lt;path/private_key.json&amp;gt;  gs://&amp;lt;bucketname&amp;gt;/&amp;lt;filename&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that signed urls do not work on directories. If you want to give access to multiple files you can use wildcards. For instance the following command will give access for 10 minutes on all the png files in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gs://&amp;lt;bucketname&amp;gt;/img/&lt;/code&gt; folder.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil signurl &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; 10m &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; GET &amp;lt;path/private_key.json&amp;gt; gs://&amp;lt;bucketname&amp;gt;/img/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.png&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check the &lt;a href=&quot;https://cloud.google.com/storage/docs/access-control/signed-urls&quot;&gt;signed urls&lt;/a&gt; page for more info&lt;/p&gt;

&lt;h3 id=&quot;service-accounts&quot;&gt;Service accounts&lt;/h3&gt;

&lt;p&gt;Service accounts are special accounts that represent software rather than people. They are the most common way applications authenticate with Google Cloud Storage. Every project has service accounts associated with it, which may be used for different authentication scenarios, as well as to enable advanced features such as Signed URLs and browser uploads using POST.&lt;/p&gt;

&lt;p&gt;When you use a service account to authenticate your application, you do not need a user to authenticate to get an access token. Instead, you obtain a private key from the Google Cloud Platform Console, which you then use to send a signed request for an access token. You can then use the access token like you normally would. For more information see the &lt;a href=&quot;https://cloud.google.com/storage/docs/authentication&quot;&gt;Google Cloud Platform Auth Guide&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lifecycle&quot;&gt;Lifecycle&lt;/h3&gt;

&lt;p&gt;Lifecycle configurations allows you to automatically delete or change the storage class of objects when some criterion is met.&lt;/p&gt;

&lt;p&gt;To enable lifecycle for a bucket with settings defined in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;config_file.json&lt;/code&gt; file, run:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gsutil lifecycle &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &amp;lt;config_file.json&amp;gt; gs://&amp;lt;bucket_name&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For instance, in order to delete the content of the bucket after 30 days, the config file would be:
Example: delete after 10 days&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;lifecycle&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;rule&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s2&quot;&gt;&quot;action&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;Delete&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
                &lt;span class=&quot;s2&quot;&gt;&quot;condition&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;s2&quot;&gt;&quot;age&quot;&lt;/span&gt;: 30,
                    &lt;span class=&quot;s2&quot;&gt;&quot;isLive&quot;&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
                &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;While changing  storage class of a bucket to Nearline after a year would be:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;action&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;SetStorageClass&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;storageClass&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;NEARLINE&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
    &lt;span class=&quot;s2&quot;&gt;&quot;condition&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;age&quot;&lt;/span&gt;: 365,
      &lt;span class=&quot;s2&quot;&gt;&quot;matchesStorageClass&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;MULTI_REGIONAL&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;STANDARD&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;DURABLE_REDUCED_AVAILABILITY&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check the &lt;a href=&quot;https://cloud.google.com/storage/docs/lifecycle&quot;&gt;lifecycle configurations page&lt;/a&gt; for more info.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Google Cloud Storage is a fully featured enterprise level service which offers a viable alternative to AWS S3. Prices, scalability, and reliability are key features of the service. I’ve been using Google Storage for awhile across different projects and find it very user friendly. Definitely worth testing if you need to store significant amount of data.&lt;/p&gt;

&lt;h1 id=&quot;further-readings&quot;&gt;Further readings&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/solutions/transferring-big-data-sets-to-gcp&quot;&gt;Transferring Big Data Sets to Cloud Platform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/storage/docs/streaming&quot;&gt;Streaming data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@duhroach/google-cloud-storage-performance-4cfcec8bad72?__s=veqfs39xxza6piky7ktj&quot;&gt;Google Cloud Storage Performance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/google-cloud/use-cases-and-a-few-different-ways-to-get-files-into-google-cloud-storage-c8dce8f4f25a&quot;&gt;Use Cases and Different Ways to get Files Into Google Cloud Storage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you liked this post, please &lt;a href=&quot;https://twitter.com/intent/tweet?url=https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html&amp;amp;text=Top gsutil command lines to get started on Google Cloud Storage &amp;amp;via=alexip&quot; target=&quot;_blank&quot;&gt; share it on twitter&lt;/a&gt;
And leave me your feedback, questions, comments, suggestions below.
Much appreciated :)
&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Mon, 01 Jan 2018 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html</guid>
        
        
        <category>gcp</category>
        
      </item>
    
      <item>
        <title>AutoML on AWS</title>
        <description>&lt;h1 id=&quot;build-a-predictive-analytics-pipeline-in-a-flash&quot;&gt;Build a predictive analytics pipeline in a flash&lt;/h1&gt;

&lt;p&gt;When Bayesian optimization meets the Stochastic Gradient Descent algorithm on the AWS marketplace, rich features bloom, models are trained, Time-To-Market shrinks and stakeholders are satisfied.&lt;/p&gt;

&lt;p&gt;In this article, we present an AWS based framework which allows non technical people to build predictive pipelines in a matter of hours while achieving results that rival solutions handcrafted by data scientists. This data flow revolves around two online services: Amazon’s own Machine Learning service for model training and selection and Predicsis.ai, a service available on the AWS marketplace, for feature engineering.&lt;/p&gt;

&lt;h1 id=&quot;expectations---frustrations&quot;&gt;Expectations - Frustrations&lt;/h1&gt;

&lt;p&gt;As the saying goes, to hire data scientists call it machine learning, to raise money call it Artificial intelligence and data science in all other cases. Whatever name you give to predictive analytics, it is the new gold rush for companies across all industries. This revolution impacts the way we communicate, trade and interact with the world. It’s a new golden age of cybernetics that we have just begun to explore. But predictive analytics takes time and can be frustrating. Expectations are high and with a bullish job market, expertise is scarce. Predictive analytics workflows require a mix of domain knowledge, data science expertise, and software proficiency that not only can be difficult to assemble but which are also challenging to bring to production. Data scientist may be the sexiest job of the early 21st century, but it will be a few years before the surging new education programs from bootcamps to college degrees, bring enough data scientists to the market. Companies don’t have the luxury to wait as competition gears up.&lt;/p&gt;

&lt;p&gt;Within that context, &lt;strong&gt;AutoML&lt;/strong&gt; offers a way to shorten project delivery time and reduce the need for junior data scientists. Before presenting these 2 services, let’s see where they fit in within the different phases of a data science project.&lt;/p&gt;

&lt;h1 id=&quot;data-science-project-management&quot;&gt;Data Science project management&lt;/h1&gt;

&lt;p&gt;A typical data science workflow can be framed as following these 7 steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Business&lt;/strong&gt;: start with a business question, define the goal and measure of success&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: find, access and explore the data&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: extract, assess and evaluate, select and sort&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: find the right model for the problem at hand, compare, optimize, and fine tune&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Interpret and communicate the results&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Production&lt;/strong&gt;: transform the code into production ready code, integrate into current ecosystem and deploy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Maintain&lt;/strong&gt;: adapt the models and features to the evolution of the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_data_science_workflow.png&quot; alt=&quot;Data Science Workflow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This workflow is not linear. Many iterations are necessary before first results can be interpreted and evaluated by the business. Business assessment of the results trigger new expectations and sends the team back to work on data, features and models. Finally after several such iterations, the research code is ready to be refactored for production.&lt;/p&gt;

&lt;p&gt;Feature engineering and model selection are the most resource intensive phases and the hardest to plan for. These two phases require data science expertise in order to tackle inherent data problems such as outliers, missing values, skewed distributions and make sure the models don’t overfit through cross validation and hyper-parameter tuning.&lt;/p&gt;

&lt;p&gt;Two services available in the AWS ecosystem can help businesses reduce their needs for data science skills and speed up the discovery phase of the data science workflow: &lt;a href=&quot;https://predicsis.ai/free-trial/?utm_source=apdec2017&quot;&gt;Predicsis.ai&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/aml/&quot;&gt;AWS Machine Learning&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;aws-machine-learning-to-the-rescue&quot;&gt;AWS Machine Learning to the rescue&lt;/h1&gt;

&lt;p&gt;Launched in April 2015, &lt;a href=&quot;https://aws.amazon.com/aml/&quot;&gt;Amazon’s Machine Learning&lt;/a&gt; service, aims at “&lt;em&gt;putting data science in reach of any developer&lt;/em&gt;”. AWS ML greatly simplifies the model training, optimization and selection phases by limiting the choice of available models to a unique one, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;stochastic gradient descent&lt;/a&gt; (SGD). By offering parameter tuning via a simple web interface, AWS ML further simplifies the modeling step while maintaining a very high predictive performance. A simple UI, and a simple yet powerful model is the key to a very efficient modeling process. The SGD is a veteran algorithm whose conception started in the 1950’s with a seminal paper by Robbins and Monro titled &lt;a href=&quot;https://projecteuclid.org/euclid.aoms/1177729586&quot;&gt;A Stochastic approximation Method&lt;/a&gt;. The SGD has been studied and optimized extensively since then. It is powerful and robust.&lt;/p&gt;

&lt;p&gt;AWS ML goes beyond the modeling phase of the project by offering production ready endpoints with a simple click. Thus removing the need for resource intensive code development.&lt;/p&gt;

&lt;p&gt;However, before we can train a predictive model, we need to extract the variables from the raw data having the biggest predictive potential. This feature extraction phase is the most unpredictable and often requires expert domain knowledge. Schemes quickly become over complicated and it’s easy to end up with thousands of variables in the hope of increasing prediction performances. While trying to keep the total number of variables down, an unavoidable constraint in order to prevent overfitting.&lt;/p&gt;

&lt;h1 id=&quot;predicsisai&quot;&gt;Predicsis.ai&lt;/h1&gt;

&lt;p&gt;This is where &lt;a href=&quot;https://aws.amazon.com/marketplace/seller-profile?id=a3c7ecaa-4a8b-4856-b1ba-aab7dd5d91de&quot;&gt;Predicsis.ai&lt;/a&gt; steps in by boiling down the complexity of the feature engineering phase to a few clicks through smart bayesian optimization of the feature space. Being able to automatically discover the variables that are the most predictive of the outcome, again via a simple web interface, is a game changer in a predictive analytics project. Predicsis.ai is available on the &lt;a href=&quot;https://aws.amazon.com/marketplace/seller-profile?id=a3c7ecaa-4a8b-4856-b1ba-aab7dd5d91de&quot;&gt;AWS marketplace&lt;/a&gt;. It is the de facto companion to the AWS Machine Learning service and has been selected as a key &lt;a href=&quot;https://aws.amazon.com/mp/ai/&quot;&gt;machine learning provider&lt;/a&gt; at the recent AWS:reInvent 2017.&lt;/p&gt;

&lt;p&gt;With these two services in mind, the entire predictive analytics pipeline now boils down to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build the raw dataset&lt;/li&gt;
  &lt;li&gt;Transform it into a powerful training dataset with Predicsis.ai&lt;/li&gt;
  &lt;li&gt;Use that dataset to train a model on AWS ML&lt;/li&gt;
  &lt;li&gt;Create an endpoint for production with AWS ML&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With AWS ML and Predicsis.ai, building a predictive pipeline from raw data to production endpoints can now be done in a few hours by team members wit no data science skills. People from the sales or marketing department can explore, assess and create efficient pipelines for their prediction targets.&lt;/p&gt;

&lt;p&gt;The AWS ML service is well &lt;a href=&quot;https://aws.amazon.com/documentation/machine-learning/&quot;&gt;documented&lt;/a&gt;. We will focus here on Predicsis.ai and its use of bayesian optimization to build feature rich datasets. AutoML is what drives Predicis.ai.&lt;/p&gt;

&lt;h1 id=&quot;automl&quot;&gt;AutoML&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ml4aad.org/automl/&quot;&gt;AutoML&lt;/a&gt; means several things for different people but overall, AutoML is considered to be about algorithm selection, hyperparameter tuning of the model, iterative modeling, and model assessment. Without claims of exhaustivity, some of the currently available AutoML platforms and actors are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html&quot;&gt;H2O’s AutoML&lt;/a&gt; which &lt;em&gt;automates the process of training a large selection of candidate prediction models&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/automl/auto-sklearn&quot;&gt;Auto-sklearn&lt;/a&gt; which recently won the &lt;a href=&quot;https://competitions.codalab.org/competitions/2321&quot;&gt;ChaLearn Automatic Machine Learning Challenge&lt;/a&gt;. &lt;a href=&quot;https://www.kdnuggets.com/2016/08/winning-automl-challenge-auto-sklearn.html&quot;&gt;Auto-sklearn&lt;/a&gt; is built on top of scikit-learn and includes &lt;em&gt;15 ML algorithms, 14 preprocessing methods, and all their respective hyperparameters, for a total of 110 hyperparameters and optimizes hyperparameter selection&lt;/em&gt; through bayesian optimization.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html&quot;&gt;TPOT&lt;/a&gt; &lt;em&gt;automatically optimizes a series of feature preprocessors and models that maximize the cross-validation accuracy on the data set&lt;/em&gt;. It is also scikit-learn compatible.&lt;/li&gt;
  &lt;li&gt;A precursor of AutoML is &lt;a href=&quot;https://www.cs.ubc.ca/labs/beta/Projects/autoweka/&quot;&gt;Auto-WEKA&lt;/a&gt; which considers &lt;em&gt;the problem of simultaneously selecting a learning algorithm and setting its hyperparameters&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;And of course &lt;a href=&quot;https://www.datarobot.com/&quot;&gt;DataRobot&lt;/a&gt;, a leader in automated machine learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To recapitulate, these AutoML libraries focus on optimizing&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the &lt;strong&gt;model selection&lt;/strong&gt;: deciding whether to choose an SVM over a random forest of a gradient boosted machine.&lt;/li&gt;
  &lt;li&gt;the &lt;strong&gt;hyperparameters tuning&lt;/strong&gt;: which kernel should be used for the SVM, how to set the learning rate for the Stochastic Gradient Descent or the number of trees for the random forest?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and in some cases&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the &lt;strong&gt;transformations&lt;/strong&gt; to be applied on the training data: should the data be one hot encoded or normalized through a box-cox transformation?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While these approaches are very powerful to create powerful models, we are still left with the task of engineering the most predictive variables straight out of the raw data. The challenge is even more complex when dealing with hierarchical data.&lt;/p&gt;

&lt;h1 id=&quot;hierarchical-data&quot;&gt;Hierarchical data&lt;/h1&gt;

&lt;p&gt;Consider for instance the case of predicting a potential buyer behavior based on three different sources: the customer’s current demographic profile, a trace of that person’s online behavior and a history of communication through emails and texts. We could probably also add that person’s social network activities, or emails or geolocation to the dataset. These different datasets come from different sources that can be aggregated to form a global customer centric hierarchical dataset. For a single person we end up with a complex mix of many variables that come in all forms and shapes: categories, flags, tags, continuous values as well as text and time series. The potential feature space expands very quickly.&lt;/p&gt;

&lt;p&gt;Having such a large feature space triggers several problems such as multi-collinearity, overfitting and what is known as the curse of dimensionality where the feature space becomes too sparse for the algorithm to be able to effectively infer the right signal for predictions. On top of that, there’s always the potential for a yet undiscovered mix of variables to hold even better predictive potential than the ones already engineered.&lt;/p&gt;

&lt;p&gt;If we want to reduce the time it takes to engineer and score these different variables mix, we need to automate the construction of variables. The good news is that we can adapt the model centric AutoML approach to the feature space. In other words, the bayesian approach to hyper parameter tuning can also be applied to the feature space with the goal of having a concise and powerful training dataset.&lt;/p&gt;

&lt;h1 id=&quot;bayesian-optimization-as-feature-fertilizer&quot;&gt;Bayesian optimization as feature fertilizer&lt;/h1&gt;

&lt;p&gt;Bayesian based feature optimization is presented in the paper&lt;a href=&quot;https://www.marc-boulle.fr/publications/BoulleECML14.pdf&quot;&gt;&lt;em&gt;Towards Automatic Feature Construction for Supervised Classification&lt;/em&gt;&lt;/a&gt; by &lt;a href=&quot;https://www.marc-boulle.fr/&quot;&gt;Marc Boullé&lt;/a&gt; from Orange Labs. The process can be decomposed as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Define an initial feature space from the original raw hierarchical data.&lt;/li&gt;
  &lt;li&gt;Extend the initial feature space by applying a pre-defined series of transformations on the data.&lt;/li&gt;
  &lt;li&gt;Score the variables using &lt;em&gt;an evaluation criterion of the constructed variables using a Bayesian approach&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Filter out variables that score below 0 (worse than random).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With that approach it becomes possible to use all the available data as input, set the dimension of the overall feature space considered for evaluation and dictate the final number of features in the expected training dataset. This is exactly what Predicsis.ai allows. That optimized training dataset is then fed to the AWS ML service for model training and selection.&lt;/p&gt;

&lt;h1 id=&quot;real-life-project&quot;&gt;Real life project&lt;/h1&gt;

&lt;p&gt;We will now show how to use &lt;a href=&quot;https://predicsis.ai/free-trial/?utm_source=apdec2017&quot;&gt;Predicsis.ai&lt;/a&gt; in combination with AWS ML to implement a buyer prediction model based on multi-relational data. The raw data is extracted from 4 tables:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Master table holds the customer’s profile information with demographic data as well as purchase history&lt;/li&gt;
  &lt;li&gt;The Orders table has order information (status, amount, web site, …) as well as the user’s web signature (OS, browser, mobile, …)&lt;/li&gt;
  &lt;li&gt;The Emails table holds email campaign related information (campaign name, recipient action, …)&lt;/li&gt;
  &lt;li&gt;The Visited Pages table where the customer’s online behavior is recorded.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From that data, we want to predict whether the customer will buy again from our store. The outcome corresponds to the LABEL column in the Master file. The whole dataset can be &lt;a href=&quot;https://github.com/PredicSis/predicsis-ai-tutorials/raw/master/Outbound%20Mail%20Campaign%20mono.zip&quot;&gt;downloaded here&lt;/a&gt;. We will first build the optimized dataset via Predicsis.ai and then use that dataset to train a model on AWS ML.&lt;/p&gt;

&lt;h1 id=&quot;predicsisai-in-action&quot;&gt;Predicsis.ai in action&lt;/h1&gt;

&lt;p&gt;As mentioned before, Predicsis.ai is available on the AWS Marketplace but you can also try out Predicsis.ai feature optimization by signing up for a &lt;a href=&quot;https://predicsis.ai/free-trial/?utm_source=apdec2017&quot;&gt;free trial&lt;/a&gt; on their website. To try out AWS marketplace version of Predicsis.ai:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Login to your AWS account and go to the AWS marketplace &lt;a href=&quot;https://aws.amazon.com/marketplace/seller-profile?id=a3c7ecaa-4a8b-4856-b1ba-aab7dd5d91de&quot;&gt;Predicsi.ai page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Start the EC2 instance.&lt;/li&gt;
  &lt;li&gt;Copy the instance’s url into your favorite browser&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You are now ready to start working with the Predicsis.ai platform. First create a new project and upload all &lt;a href=&quot;https://github.com/PredicSis/predicsis-ai-tutorials/raw/master/Outbound%20Mail%20Campaign%20mono.zip&quot;&gt;your data&lt;/a&gt; (Master, Orders, Emails and VisitedPages). The workflow is split between feature importance exploration followed by feature selection and data export. Start by exploring the Master file since it contains the outcome variable (LABEL). The following screenshot shows the relative importance of the different variables from that file. For instance, we see that the age variable contributes for 15% of the model prediction and is split in 3 bins each with a different coverage and frequency. This visualization gives us a good way to understand what really drives our predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_master_exploration_01.png&quot; alt=&quot;Master exploration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the next screenshot, our first model achieves a Gini score of 0.69 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gini = 2 * AUC -1&lt;/code&gt; where AUC is the classic Area Under Curve.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_model_01.png&quot; alt=&quot;First model preformance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we can improve on that model by adding the data files to the feature mix.&lt;/p&gt;

&lt;p&gt;In the next screenshot, we add the orders file and tell the model to generate 100 new feature aggregates and only keep a total of 30 features for our model. We explore the feature space and at the same time limit its dimension. Doing so will bring to the surface new features and create a new model with improved performances.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_feature_surfacing_02.png&quot; alt=&quot;Feature surfacing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we see below, our model’s performance has been sgnificantly improved from 0.69 to 0.87 while keeping the total number of features to 30:  6 of which come from the Master file, and 24 are new aggregates obtained from the order file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_model_02.png&quot; alt=&quot;Second model performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can iterate that feature generation process and add the other files (VisitedPages, Emails) to the feature mix, have the platform create and assess new aggregates and only keep the most important and powerful variables.&lt;/p&gt;

&lt;p&gt;This is just a glimpse of Predicsis.ai capabilities. Predicsis.ai also offers rich visualizations to assess and understands the lift and cumulative gains related to the population slice.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/prediscis_assess_dataviz.png&quot; alt=&quot;Assess model performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that all these steps can also be carried out using the Predicsis.ai &lt;a href=&quot;https://predicsis-python-sdk.readthedocs.io/en/stable/&quot;&gt;python SDK&lt;/a&gt;. The aggregation functions are quite simple (simple selects, binning or simple stats: mean, std, median). These functions can for the most part be implemented in SQL at the data extraction level directly from the database and therefore straightforward to apply on unseen production data. It’s also possible to use PredicSis.ai in batch mode to materialize those features on any new data with the same relational structure.&lt;/p&gt;

&lt;p&gt;We can now export our optimized dataset to AWS ML and build a prediction model. To do so we follow the standard AWS ML workflow:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Upload the training data to S3&lt;/li&gt;
  &lt;li&gt;Define a data source. Let AWS ML infer the schema, double check and validate it.&lt;/li&gt;
  &lt;li&gt;Let AWS create the training and testing subsets.&lt;/li&gt;
  &lt;li&gt;AWS ML will suggest default transformations aka recipes  which consists mostly in binning continuous variables. These binning transformations are important to achieve good predictions. You can either accept the suggested bins or the ones inferred by PredicSis.ai.&lt;/li&gt;
  &lt;li&gt;Train the model. With parameter tuning reduced to the bare minimum, you will only have to choose the type and strength of regularization&lt;/li&gt;
  &lt;li&gt;Wait a few minutes for the evaluation results.&lt;/li&gt;
  &lt;li&gt;if satisfied by the prediction score, create an end point with a click.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You now have a fully trained predictive model, based on a highly optimized features set, all in a matter of minutes without involving any scripting.&lt;/p&gt;

&lt;p&gt;For that particular dataset, we obtain an AUC of 0.87 with the Predicsis.ai boosted dataset, which is very decent for such an imbalanced dataset so little efforts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://alexisperrier.com/assets/predicsis/predicsis_aws_ml_auc.png&quot; alt=&quot;AWS ML Evaluation Score&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;As predictive analytics becomes a must have for companies big and small, the need for fast and reliable data pipelines that can be easily understood and interpreted grows stronger. Using existing AWS based services available through simple UI we are able to build efficient end to end predictive models. From multi-relational raw data to optimized training datasets, powerful models and stable production end points. The reliability of this approach is grounded on the robust mathematical methods that these two services are built upon: Bayesian optimization for feature extraction and stochastic optimization for model training. Methods that have been studied for decades and that can be safely and simply exploited by business users for real life data projects.&lt;/p&gt;

&lt;h4 id=&quot;further-resources&quot;&gt;Further resources&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;How to launch a Predicsis ready EC2 instance: &lt;a href=&quot;https://www.youtube.com/watch?v=c1gZfjBG204&quot;&gt;video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Predicsis.ai &lt;a href=&quot;https://resources.predicsis.ai/hc/en-us/categories/115000398525-KNOWLEDGE-BASE&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Predicsis.ai &lt;a href=&quot;https://predicsis-python-sdk.readthedocs.io/en/stable/&quot;&gt;Python SDK&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;AWS Machine Learning &lt;a href=&quot;https://opendatascience.com/blog/amazon-machine-learning-nice-and-easy-or-overly-simple/&quot;&gt;tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;AWS Machine Learning, &lt;a href=&quot;https://www.amazon.com/Effective-Amazon-Machine-Learning-Perrier-ebook/dp/B01NCJ4NXP&quot;&gt;the book&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 04 Dec 2017 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/aws/2017/12/04/automl_aws_data_science.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/aws/2017/12/04/automl_aws_data_science.html</guid>
        
        
        <category>aws</category>
        
      </item>
    
      <item>
        <title>Gsutil cheatsheet</title>
        <description>&lt;p&gt;&lt;strong&gt;gsutil&lt;/strong&gt; is Google Storage CLI tool. Equivalent to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3&lt;/code&gt; but for the Google Cloud Platform, it allows you to access Google Cloud Storage from the command line. Beyond moving files and managing buckets, gsutil is a powerful file management (rsync) and file publication tool (signed urls).&lt;/p&gt;

&lt;p&gt;Please find below a shortlist of the most important and frequent commands and their relative syntax.&lt;/p&gt;

&lt;p&gt;This is a work in progress that will be updated within the next few days.&lt;/p&gt;

&lt;h1 id=&quot;gsutil-cheatsheet&quot;&gt;Gsutil cheatsheet&lt;/h1&gt;

&lt;p&gt;gsutil is Google Cloud storage CLI.
More info on&lt;/p&gt;

&lt;h5 id=&quot;definitions&quot;&gt;Definitions&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GCP&lt;/code&gt;: Google Cloud Platform&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;install&quot;&gt;Install&lt;/h5&gt;

&lt;p&gt;To get started with gsutil you need python (at least 2.7) and to install the Google Cloud SDK. See https://cloud.google.com/sdk/docs/ to download the right package for your environment.&lt;/p&gt;

&lt;h5 id=&quot;general-commands&quot;&gt;General Commands&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil ls&lt;/code&gt;: &lt;strong&gt;lists&lt;/strong&gt; all your buckets&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil help &amp;lt;topic&amp;gt;&lt;/code&gt;: help on the &lt;strong&gt;topic&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;buckets&quot;&gt;Buckets&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil mb gs://&amp;lt;bucket_name&amp;gt;&lt;/code&gt;: creates the &lt;strong&gt;gs://bucket_name&lt;/strong&gt;&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil rb gs://&amp;lt;bucket_name&amp;gt;&lt;/code&gt;: deletes the &lt;strong&gt;bucket&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;files&quot;&gt;Files&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil cp &amp;lt;filename&amp;gt; gs://&amp;lt;bucket_name&amp;gt;/&lt;/code&gt;: copies the local filename into the bucket **&lt;bucket_name&gt;**.&lt;/bucket_name&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil cp &amp;lt;filename&amp;gt; gs://&amp;lt;bucket_name&amp;gt;/directory/&lt;/code&gt;: copies the local filename into the directory **&lt;directory&gt;**&lt;sup&gt;2&lt;/sup&gt;.&lt;/directory&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil mv &amp;lt;src_filename&amp;gt; gs://&amp;lt;bucket_name&amp;gt;/directory/&amp;lt;tgt_filename&amp;gt;&lt;/code&gt;: moves the local &lt;strong&gt;src_filename&lt;/strong&gt; to the &lt;strong&gt;directory&lt;/strong&gt; and renames it as &lt;strong&gt;tgt_filename&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil rm gs://&amp;lt;bucket_name&amp;gt;/file_or_dir&lt;/code&gt;: deletes the &lt;strong&gt;file_or_dir&lt;/strong&gt; object.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;folder&quot;&gt;Folder&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gsutil cp &amp;lt;filename&amp;gt; gs://&amp;lt;bucket_name&amp;gt;/&lt;/code&gt;: copies the local filename into the bucket **&lt;bucket_name&gt;**.&lt;/bucket_name&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;1: The bucket name has to be unique across GCP.&lt;/li&gt;
  &lt;li&gt;2: Note the trailing ‘/’ slash after &lt;strong&gt;&amp;lt; directory &amp;gt;&lt;/strong&gt; to tell gsutil that the target is a directory and not a file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you liked this post, please &lt;a href=&quot;https://twitter.com/intent/tweet?url=https://alexisperrier.com/gcp/2017/11/02/gsutil-cheatsheet.html&amp;amp;text=Gsutil cheatsheet&amp;amp;via=alexip&quot; target=&quot;_blank&quot;&gt; share it on twitter&lt;/a&gt;
And leave me your feedback, questions, comments, suggestions below.
Much appreciated :)
&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Thu, 02 Nov 2017 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/gcp/2017/11/02/gsutil-cheatsheet.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/gcp/2017/11/02/gsutil-cheatsheet.html</guid>
        
        
        <category>gcp</category>
        
      </item>
    
      <item>
        <title>AWS Machine Learning Big Data NYC</title>
        <description>&lt;p&gt;My slides on AWS Machine Learning platform at the Global Big Data conference - NYC 2017 Oct 24.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: The AWS Machine Learning service is a simple but very efficient predictive analytics service for supervised classification and regression.
The AWS ML service greatly simplifies the model selection and model optimization phases of the data science workflow. However it is somewhat limited when it comes to feature engineering. This is where an auto-ml platform such as &lt;a href=&quot;https://predicsis.ai/&quot;&gt;Predicsis.ai&lt;/a&gt; comes in. Predicsis.ai is a service available on the AWS marketplace that generates predictive features in hierarchical datasets (multiple sources for user profile, user online behavior and user buying events for instance). When used in combination Predicsis.ai and the AWS ML service have the power to greatly shorten the data science cycle from the expression of the business problem to actionable insights to stakeholders.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/cuIFIEuGAJXJu&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/alexip/aws-machine-learning-big-data-nyc&quot; title=&quot;AWS Machine Learning Big Data NYC &quot; target=&quot;_blank&quot;&gt;AWS Machine Learning Big Data NYC &lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;https://www.slideshare.net/alexip&quot; target=&quot;_blank&quot;&gt;Alexis Perrier&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you liked this post, please &lt;a href=&quot;https://twitter.com/intent/tweet?url=https://alexisperrier.com/aws/2017/10/24/aws-machine-learning-bigdata-nyc.html&amp;amp;text=AWS Machine Learning Big Data NYC&amp;amp;via=alexip&quot; target=&quot;_blank&quot;&gt; share it on twitter&lt;/a&gt;
And leave me your feedback, questions, comments, suggestions below.
Much appreciated :)
&lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Tue, 24 Oct 2017 14:00:00 +0000</pubDate>
        <link>https://alexisperrier.com/aws/2017/10/24/aws-machine-learning-bigdata-nyc.html</link>
        <guid isPermaLink="true">https://alexisperrier.com/aws/2017/10/24/aws-machine-learning-bigdata-nyc.html</guid>
        
        
        <category>aws</category>
        
      </item>
    
  </channel>
</rss>
