<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Tim Dettmers</title>
	<atom:link href="https://timdettmers.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://timdettmers.com/</link>
	<description>Making deep learning accessible.</description>
	<lastBuildDate>Tue, 06 Sep 2022 14:33:10 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.3</generator>

<image>
	<url>https://i0.wp.com/timdettmers.com/wp-content/uploads/2021/11/cropped-profile_2021_512x512.png?fit=32%2C32&#038;ssl=1</url>
	<title>Tim Dettmers</title>
	<link>https://timdettmers.com/</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">106749684</site>	<item>
		<title>LLM.int8() and Emergent Features</title>
		<link>https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/</link>
					<comments>https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Wed, 17 Aug 2022 12:51:19 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[emergent features]]></category>
		<category><![CDATA[LLM.int8()]]></category>
		<guid isPermaLink="false">https://timdettmers.com/?p=1093</guid>

					<description><![CDATA[<p>When I attended NAACL, I wanted to do a little test. I had two pitches for my LLM.int8() paper. One pitch is about how I use advanced quantization methods to achieve no performance degradation transformer inference at scale that makes large models more accessible. The other pitch talks about emergent outliers in transformers and how [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8() and Emergent Features</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="eplus-3CSL7i">When I attended NAACL, I wanted to do a little test. I had two pitches for my <a href="https://arxiv.org/abs/2208.07339">LLM.int8() paper.</a> One pitch is about how I use advanced quantization methods to achieve no performance degradation transformer inference at scale that makes large models more accessible. The other pitch talks about emergent outliers in transformers and how they radically change what transformers learn and how they function.</p>



<p class="eplus-W9Xcci">From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job. </p>



<span id="more-1093"></span>



<p class="eplus-DFPRLL">How that job is done for you through the bitsandbytes library with Hugging Face integration so that you can easily run OPT-175B and BLOOM-176B on a single machine is described in another <a href="https://huggingface.co/blog/hf-bitsandbytes-integration">blog post</a> by my colleague and collaborator <a href="https://younesbelkada.github.io/">Younes Belkada</a>. </p>



<p class="eplus-UuxR1O">This blog post will spill some mandatory details about quantization, but I want to mostly make it about these emergent features that I found in transformers at scale. I know the claims in the paper are highly robust. This blog post is a more speculative version of the paper that teases out the super curious details about the fascinating properties surrounding the emergent outlier features I found. I cannot spill all the details because my next project will delve deep into understanding these outlier features, but the space is so rich that I am happy to give you many curious details.</p>





<h2 class="eplus-Ng4f4T">Mandatory quantization details</h2>



<p class="eplus-dmc53I">In a previous version of this blog post, I jokingly had a section with the big title “All You Ever Wanted to Know about Quantization” The section read: “If you quantize from 16-bit to 8-bit, you lose precision which might degrade model prediction quality.”</p>



<p class="eplus-VdCm0n">That is it.</p>



<p class="eplus-QmPB8B">Most people do not want to learn more about quantization — and honestly, the small sentence above is already enough information. The details are very gritty and complicated, but it is all in the code. The math and concepts are very simple and straightforward — if you have worked on quantization before. If you have not encountered quantization, it is likely a hot devilish nightmare that will eat your liver.&nbsp;</p>



<p class="eplus-9l7CrQ">For those that say, “Pfff! Why do I need a liver anyways?”. Well, here you go. For others, just move ahead and read about the mysteries of emergent features.</p>



<h3 class="eplus-6ppO01">What is quantization?</h3>



<p class="eplus-pcdGpk">Let us say you have a data type I5 with values [0, 1, 2, 3, 4, 5] and a data type, I3, with values [0, 2, 4], how do you quantize from data type I5 to I3? You follow a two-step procedure:</p>



<ol class="eplus-GsdfGv"><li>Normalize the range of I5 into I3.</li><li>Round to the nearest value of I3.</li></ol>



<p class="eplus-tefJ7X">Let&#8217;s do an example. Let&#8217;s say we have the vector [3, 1, 2, 3] in I5, and we want to quantize to I3.</p>



<p class="eplus-aorZsE">Here the step-by-step recipe for quantization:</p>



<ol class="eplus-R2sRDM"><li>We find the absolute maximum value of the vector: [3, 1, 2, 3] -&gt; 3</li><li>Then we divide by that value: [3, 1, 2, 3] -&gt; [1, 0.33, 0.66, 1.0]</li><li>And now we multiple by the range of the target data type I3, which is 4: [1, 0.33, 0.66, 1.0] -&gt; [4.0, 1.33, 2.66, 4.0]</li><li>Now we round to the nearest value: [4.0, 1.33, 2.66, 4.0] -&gt; [4, 0, 2, 4]</li></ol>



<p class="eplus-KZcrJX">We now converted [3, 1, 2, 4] in I5 to [4, 0, 2, 4] in I3. To dequantize, we reverse this process.</p>



<ol class="eplus-gS9Jqh"><li>Divide by 4: [4, 0, 2, 4] -&gt; [1.0, 0.0, 0.5, 1.0]</li><li>Multiply by the absolute maximum: [1.0, 0.0, 0.5, 1.0] -&gt; [3.0, 0.0, 1.5, 3.0]</li><li>Now we round again: [3.0, 0.0, 1.5, 3.0] -&gt; [3, 0, 2, 3]</li></ol>



<p class="eplus-3diZxN">We see that our dequantization and quantization led to one error:<br>[3, 1, 2, 3] to [3, 0, 2, 3]<br>The second element changed from 1 to 0. This is a quantization error that leads to the loss of information in terms of how precise the information is encoded. If we have such errors and propagate them through many layers of a neural network, they accumulate, and they may change the result of a prediction and degrade the prediction quality.</p>



<h3 class="eplus-ugaKKG">How to make quantization methods more precise</h3>



<p class="eplus-gLCi7x">Quantization can be enhanced in two ways. Use a better data type, or use more normalization constants (absolute maximum). </p>



<p class="eplus-Cjg0qn">Regarding data types, Int8 is a terrible data type for deep learning. That is why I developed <a href="https://arxiv.org/abs/2110.02861">new data types</a> in my research. However, currently, GPUs do not support other than Int8 data types on the hardware level, and as such, we are out of luck and need to use Int8.</p>



<p class="eplus-aOZcxy">The only way to improve quantization is through more normalization constants. A normalization constant squishes the input distribution, for example, I5, into the target distribution, for example, I3. We can increase precision, by squishing each vector only as much as is needed. For example, if you have the two vectors:</p>



<p class="eplus-fL9v3i">[3, 1, 2, 3]&nbsp;<br>[0, 2, 2, 0]</p>



<p class="eplus-4iFHKa">Then you can squish the first by 4 and the second by 2. This will give you twice the precision to quantize the second vector because the inputs are now spread over a broader range of the I3 data type. In fact, the second vector can be quantized without errors if you use an additional absolute maximum value. If you use only a single constant over both vectors (tensor-wise constants), then you will have two errors.</p>



<h3 class="eplus-mEvg3p">Vector-wise quantization</h3>



<p class="eplus-FvNMmE">So now that we know how to make quantization more precise, how do we achieve maximum precision for matrix multiplication?</p>



<p class="eplus-bi1bOM">The key is this: If we use different normalization constants for dependent vectors, we then need to recover this information in the dequantization step. For example, if we subtract a constant to center one distribution over another: (A-minA)(B-minB) then to dequantize the output in A*B=C we need to do:</p>



<p class="eplus-8c1xuZ">A*B = C</p>



<p class="eplus-0T0W7B">(A-minA)(B-minB) = A*B &#8211; A*minB &#8211; B*minA + minA*minB = C&nbsp; &#8211; A*minB &#8211; B*minA + minA*minB</p>



<p class="eplus-fOM75R">As such, dependent quantization produces additional computation, in this case, a couple of matrix-vector multiplications and additions which can be expensive (if we assume, A and B are matrices).</p>



<p class="eplus-XGpnrc">As such, we look for the most normalization constants we can get that are still independent. What does this look like?</p>



<p class="eplus-moVvwz">We can see a matrix multiplication as a sequence of independent inner products between row vectors of A and column vectors of B. We can have a separate constant for each of these vectors. Denormalization happens by multiplying these two constants together for a particular element. No other computation is needed. This is vector-wise quantization. More details in the <a href="https://arxiv.org/abs/2208.07339">paper</a>.</p>



<h3 class="eplus-HoJ53w">Mixed precision decomposition</h3>



<p class="eplus-MoRO01">Before we come to the emergent magnitude features, let me explain the last part of our method that is absolutely critical to achieving zero-degradation quantization at scales of up to 175B parameters.</p>



<p class="eplus-aXtLwz">So it turns out, that transformers have these emergent features that have very large values. They occur in particular hidden dimensions and are active in up to 75% of all sequence dimensions. They occur in all layers (well most layers, but we come to that). So if you have a transformer hidden state X of dimensionality [batch, sequence, hidden], then X[:, :, i] for some i have values that look like this:</p>



<p class="eplus-E1g6wT">[-60.. -45, -51, -35, -20, -67]</p>



<p class="eplus-nuJruQ">Whereas 99.9% of dimensions look like this (normally distributed with one outlier)</p>



<p class="eplus-Q3dQ1x">[-0.10, -0.23,&nbsp; 0.08, -0.38, -0.28, -0.29, -2.11,&nbsp; 0.34, -0.53, -67.0]</p>



<p class="eplus-nbZzlJ">If we quantize and dequantize a row without an outlier, we get this:</p>



<p class="eplus-JJx9Bc">[-0.10, -0.23, 0.08, -0.38, -0.28, -0.28, -2.11, 0.33, -0.53]</p>



<p class="eplus-BR9Pcp">only a single error, -0.28 instead of -0.29, at the 0.01 precision level. However, if we quantize the same vector with the outlier, we get this:</p>



<p class="eplus-n2kDRO">[ -0.00,&nbsp; -0.00, &nbsp; 0.00,&nbsp; -0.53,&nbsp; -0.53,&nbsp; -0.53,&nbsp; -2.11, &nbsp; 0.53,&nbsp; -0.53, -67.00]</p>



<p class="eplus-NIWxqf">In other words, even if we use vector-wise quantization, we squish a lot of information to zero and have large errors. On average, vectors without outliers have a mean error of 0.015. This vector has an error of 0.12. Do this for a couple of layers, and we remove all information and end up with pure noise.</p>



<p class="eplus-1fgfP3">The problem is that at a scale of 6.7B parameters and above, 75% of hidden state sequences are affected. So this absolutely wrecks quantization.</p>



<p class="eplus-mAjJ8A">The good news is that these outliers are highly systematic. While you have 150,000 outliers per sequence in a 6.7B transformer, they only occur in 6 feature dimensions (6 different indices “i” as in X[:, :, i]).</p>



<p class="eplus-9OcMD8">As such, we can separate these emergent features into a separate, high precision matrix multiplication, quantize the other 99.9% of values to Int8, can combine the output of both matrix multiplications. This avoids the information squishing to zero effect, and we can recover full transformer performance.</p>



<p class="eplus-3LyUrS"></p>



<figure class="wp-block-image size-large eplus-X45Ytq"><img data-attachment-id="1096" data-permalink="https://timdettmers.com/llm_int8/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/LLM_int8.gif?fit=600%2C300&amp;ssl=1" data-orig-size="600,300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LLM_int8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/LLM_int8.gif?fit=300%2C150&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/LLM_int8.gif?fit=600%2C300&amp;ssl=1" width="600" height="300" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/LLM_int8.gif?resize=600%2C300&#038;ssl=1" alt="" class="wp-image-1096" data-recalc-dims="1"/></figure>



<h2 class="eplus-ZfQ3WS">Results</h2>



<p class="eplus-blVVmm">The results show that this method works well. We can recover full performance by using the LLM.int8() quantization procedure. You can clearly see that there is a big dip in performance for the 8-bit baseline, which is vector-wise quantization. We need both vector-wise quantization and mixed precision decomposition, that is, the full LLM.int8() method to recover full performance. Either of these methods alone is not sufficient.<img src="https://lh5.googleusercontent.com/TDrTlopijg4gvi2tTjPsMNLO23wcTJvhnYfc3WHszjtk5nsUgPQUxWOY5vyJysAomfhvbhgjhXP94sKT9v898vP53WW9ptb_itIpQ92xmkdfL7VHdY7cS1ldLpxh3parcz-lIdNgKL3NoxVXikqLfB0" width="624" height="504"></p>



<h2 class="eplus-nYJtTh">Emergent Features</h2>



<p class="eplus-rejxPU">There are a lot of exciting findings in the paper:&nbsp;</p>



<ol class="eplus-QUM2k0"><li>Emergence is not sudden but gradual and grows according to an exponential function related to perplexity and not model size.</li><li>Outlier features grow very quickly once their phase shift occurs.</li><li>The number of outliers features is strictly proportional to perplexity.&nbsp;</li></ol>



<p class="eplus-KJZ7Yd">Many other findings did not make it into the paper because these were too difficult to verify robustly, but I wanted to share them here anyway. Since these results are less robust, take them with a grain of salt.</p>



<p class="eplus-ZGP3s4">But I am jumping ahead: What is the emergence, and what makes an emergent feature? If I put it in my own words, I would say:</p>



<p class="eplus-89vQsn"><em>Emergence is a gradual change in a property that suddenly undergoes a phase shift and then changes the quality of its substrate.</em></p>



<p class="eplus-XpgRap">Let’s think step-by-step.</p>



<p class="eplus-r9F3TD"><strong>Substrate</strong>: Transformer<br><strong>Property</strong>: Very large features in particular hidden dimensions across the transformer<br><strong>Gradual change</strong>: Decreasing perplexity, more and larger outlier features<br><strong>Phase shift</strong>: Outlier features suddenly become available in all transformer layers and coordinate through a few hidden dimensions.<br><strong>Change of quality:</strong> Highly sparse, almost discrete attention; very dense FFN layers; “dual attention”; long-range attention (?); stable training through increased numerical stability.</p>



<p class="eplus-lh0MYv">Some additional terms. What is a feature?</p>



<p class="eplus-tabQid">If you have hidden state X that is passed along a transformer with dimensionality [batch, sequence, hidden], then a feature is a particular dimension X[:, :, i], which offers some weak explanation for the label.</p>



<h3 class="eplus-RWqToH">Emergent Features in a Nutshell</h3>



<p class="eplus-noiMEj">To get a little sense of what this is all about, here is a short explanation encapsulating everything important about emergent features.&nbsp;</p>



<p class="eplus-LreO17">The most intuitive explanation of feature outliers is that transformers have two processing streams. One stream learns features that explain the inputs, and the other stream learns features that remove other features. Removing noisy, context-irrelevant features is the key to making accurate predictions. The more noisy, context-irrelevant features you remove in early layers, the less conflicting high-level features you have in later layers.&nbsp;</p>



<p class="eplus-A1bu0Z">For example, if you classify dogs vs. cats, it makes sense to “sharpen” the key features that make these animals different (e.g. cat eyes, cat ears) and remove the similar features (fur color and potentially texture). This is particularly relevant if you have many noisy “weak” features as in natural language processing.&nbsp;</p>



<p class="eplus-2y7oYu">If you take this mechanism to an extreme, you can get discretization, which goes hand-in-hand with context-dependent memory and “reasoning” over elements. Discretization means, you have, say, 100 features, but you decide to remove 99% of them by setting them to zero, and you amplify the rest. The result is a single feature that is now a discrete entity. Once discretized, this entity can be stored and reused later.</p>



<p class="eplus-PRBtGb">To coordinate these streams throughout the transformer, it is useful to dedicate certain hidden dimensions to the functionality of removing other features. That way, if the transformer needs to remove features, it knows beforehand which feature dimension to access to perform that functionality.</p>



<p class="eplus-En0aGT">How do you remove features? You have a single dimension with very large positive or negative values, and you multiply that dimension with a positive/negative number.</p>



<p class="eplus-wXtjS9">Take the following matrix, which is similar to how emergent features are represented in hidden states.&nbsp;</p>



<p class="eplus-QkHSlm">[0, 1, -60, 4]<br>[3, 0, -50, -2]<br>[-1, 0, -55, 1]<br>[3, 2, -60, 1]</p>



<p class="eplus-LIJ94J">If we want to remove, say, features (columns) 0 and 3 in a matrix multiplication followed by a non-linear function, all we have to do is to multiply everything by a negative number and multiply the outlier feature for columns 0 and 3 by a positive number. If we do this with negative and positive 1s, it looks like this:</p>



<p class="eplus-W0OQTf">[-1, -1, -1, -1]<br>[-1, -1, -1, -1]<br>[<strong> 1</strong>, -1,&nbsp; -1,&nbsp; <strong>1</strong>]<br>[-1, -1, -1, -1]</p>



<p class="eplus-hewdEE">We receive the following after a softmax:</p>



<p class="eplus-JrPnkX">[0, 0.5, 0.5, 0]<br>[0, 0.5, 0.5, 0]<br>[0, 0.5, 0.5, 0]<br>[0, 0.5, 0.5, 0]</p>



<p class="eplus-93BJW7">The neat thing about this system is, that if you always maintain the outlier feature in dimension 3, you know beforehand where to insert a positive number to remove a feature (row 3 of the other matrix).</p>



<p class="eplus-gXg955">Transformers seem to coordinate these dimensions throughout all layers except the attention function and the second feedforward network where these outliers are “consumed” to remove features.</p>



<p class="eplus-IHtJRm">This means that transformers always use a certain dimension for these outliers, and each layer &#8220;knows&#8221; beforehand how to remove a feature because these feature dimensions always have very large values with a particular sign (some are negative, some are positive).</p>



<p class="eplus-jrpePQ">However, this full &#8220;coordination&#8221; through a single dimension only happens after the phase shift. Before the phase shift, in transformers with less than 6.7B parameters some layers disagree which dimension to use for these large features.</p>



<h3 class="eplus-2V1R69">How Emergent Features Emerge</h3>



<p class="eplus-10ZNUc">Emergent outlier features are present in even very small transformers (125M parameters), and they do start out in the attention projection layers (key/query/value). Feature outliers are “consumed” in the attention function (softmax) and the second fully connected sublayer (contraction layer). The outlier features are likely consumed in these layers since the second feedforward network (FFN) sub-layer, and the softmax have non-linear functions that can easily squash features to zero.</p>



<p class="eplus-AG4262">Once you scale transformers a bit more (350M to 1.3B), outliers also occur in the FFN and attention output layers. At this scale, some successive attention layers and FFN layers use the same dimension to coordinate what features to remove. This has synergy. The attention layer is good at context-dependent selection and pattern matching, while the FFN layers are good at globally, context-independent pattern matching.</p>



<p class="eplus-QLO9ja">At this scale, however, outliers are still probabilistic. This means they occur mostly in some dimensions, but these dimensions can change slightly from mini-batch to mini-batch and between layer and layer. At this scale, layers have not yet learned to coordinate outlier features through the same dimension. This makes it more difficult to remove unwanted features.</p>



<p class="eplus-dG3aQO">At the 2.7B to 6B scale, things become much more coordinated. Now 60% of layers agree on which outlier dimension to use.</p>



<p class="eplus-YKGjaY">The phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers. At this point, a couple of things happen rapidly:</p>



<ol class="eplus-lio37T"><li>Outliers become very large quickly. They grow from about 15 for a 6B model to about 60 for a 13B model. OPT-66B has outliers of size around 95, which indicates this growth phase is temporary.</li><li>Attention layers become very sparse. The attention is very concentrated so that just a few sequence dimensions determine the top probability and the overall probability mass. Almost all sequence dimensions have zero probability.&nbsp;However, this is still context-dependent, and the transformer seems to be &#8220;unsure&#8221; what to attend to for some sequences.</li><li>FFN layers become more “dense”. While in computer vision, you can prune about 95% of weights without severe performance degradation, that number is 30% for transformers trained on NLP data. After emergence, this number shrinks to well below 5%. It seems that canceling out features can remove noise that is generated from the many weak features that are activated. Because these are silenced now, each set of neurons can learn much more features that are almost independent of each other due to the masking of context-dependent features.</li><li>Transformers become more stable. If you treat the outlier features separately, I believe you can probably run and even train transformers in less than 8-bit precision without degradation in performance.</li></ol>



<h2 class="eplus-oZEQDR">The Most Important Take-aways for Your Research</h2>



<p class="eplus-4zc88t">You may say, &#8220;This is all good and well, Tim, but what does this mean for me and my research?&#8221; Good question! I think it changes quite a bit.</p>



<h4 class="eplus-tTFqmG">There are two types of transformers and you should not generalize from one to the other.</h4>



<p class="eplus-uOoJXZ">From these findings it is clear that transformer after the phase shift at 6.7B parameters behave very different to transformers before the phase shift. As such, one should not try to generalize from &lt;6.7B transformers to beyond 6.7B parameters. </p>



<p class="eplus-VaNyCJ">But training and using 6.7B transformers can be pretty painful. At Facebook AI research, I had a 1.3B parameter baseline and I would usually run 2-3 of those models on 128 GPUs each for a total of 384 GPUs. Despite these massive resources it would still feel &#8220;slow&#8221; in that my research progress was mostly hindered by compute. I imagine to train 6.7B models on 8 GPUs or even 32 GPUs must be super painful. Is there a way that we can avoid this?</p>



<p class="eplus-5sKPkJ">I think another key finding from the paper can help. We found that emergence of features occurs smoothly according to an exponential distribution of decreasing perplexity. As such, one could do the following.</p>



<p class="eplus-091Qkp">We train multiple smaller models, say, 125M, 350M and 1.3B parameters, and then we measure the emergent property in those models and relate it to the property that we are interested in analyzing, for example, a new architecture or a new from of interpreting models. Once we gathered this data, we can measure how the change in the emergent property changes the results of our new method. With that, we might be able to determine if our new method generalizes to models beyond 6.7B parameters.</p>



<p class="eplus-xHTtw8">While, by definition, the phase shift leads to a stark change in behavior, this method of extrapolating emergent behavior might yield  more robust predictions for your research. It would be effortful and complicated to do this, but this is better than &#8220;wishful thinking&#8221; research that does not generalize.</p>



<h4 class="eplus-N03ebN">We might be able to find new emergent properties by studying &#8220;scaling laws of emergence&#8221;.</h4>



<p class="eplus-mg1iST">The finding that emergence can be measured in small models means that new emergent properties that require models larger than 175B parameters might be already measurable in the open-source OPT models.</p>



<p class="eplus-vPJp5e">If we can correlate statistics of a property with increasing capabilities and if this property follows a function that will eventually, &#8220;threshold&#8221;, we might have discovered a new emergent property that leads to new capabilities.</p>



<h2 class="eplus-Zh5AMx">Conclusion</h2>



<p class="eplus-HhWilC">In this blog post, I introduced LLM.int8() and gave an introduction into the emergent features that we discovered in language model at scale. I discussed the implication of these emergent features in particular how it relates to generalization.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">LLM.int8() and Emergent Features</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/feed/</wfw:commentRss>
			<slash:comments>13</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">1093</post-id>	</item>
		<item>
		<title>How to Choose Your Grad School</title>
		<link>https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/</link>
					<comments>https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Sun, 13 Mar 2022 22:43:57 +0000</pubDate>
				<category><![CDATA[Academia]]></category>
		<category><![CDATA[PhD Life]]></category>
		<category><![CDATA[Advisors]]></category>
		<category><![CDATA[Grad school]]></category>
		<category><![CDATA[PhD]]></category>
		<guid isPermaLink="false">https://timdettmers.com/?p=805</guid>

					<description><![CDATA[<p>If you are reading this, then you probably finished the long and arduous journey to grad school. You emerged victoriously, and this success is well-deserved. But which school should you choose? How to make a right choice if all schools look great in their own way? This blog post is centered around these questions. It [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/">How to Choose Your Grad School</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>If you are reading this, then you probably finished the long and arduous journey to grad school. You emerged victoriously, and this success is well-deserved. But which school should you choose? How to make a right choice if all schools look great in their own way? This blog post is centered around these questions. It is most useful if you are a computer science student aiming to study machine learning and, in particular, natural language processing in the US, but most of the information here is equally valid for any field of research and any country.</p>
<p>The choice of grad school that is right for you can be tricky and confusing. We live in a time of hyper-competitiveness, where even undergrads need to optimize for metrics like paper count to make it to the next level — grad school. This heavily career-centered perspective was probably advantageous to get you into grad school, and it remains crucial to get you to the level after that: a great job in industry or academia. So choosing the school which is best for your career can feel like an obvious choice. However, a PhD is a very long journey, and choosing your grad school based on this perspective alone might make you more vulnerable to burn-out, disillusionment, and general dissatisfaction.</p>
<p>In this blog post, I will discuss this career-centered perspective in detail, but I also provide you with three other views that hopefully help you make a balanced choice that not only leads to academic success but long-term satisfaction and a full and rich life. Balancing your decision based on all four perspectives probably leads to a better choice than looking at one angle alone. Before I go into the details, let me briefly introduce these four perspectives: The Career Perspective, the Identity Perspective, the Stability Perspective, and the Variability Perspective.</p>
<p><span id="more-805"></span></p>
<p><span data-preserver-spaces="true">A quite intuitive perspective is the </span><strong><span data-preserver-spaces="true">Career Perspective</span></strong><span data-preserver-spaces="true">, which is about determining and weighing the factors that help you to be successful in your PhD and have a successful career.</span></p>
<p><span data-preserver-spaces="true">A different perspective is the </span><strong><span data-preserver-spaces="true">Identity Perspective</span></strong><span data-preserver-spaces="true">: not looking at your career but at who you want to be and how your choice enables and facilitates that identity. The social environment that you are in has a strong causal effect on your development: We are strongly influenced by the people and culture around us, and the friends of friends that you do not even know will make you honest/deceitful, selfish/selfless, caring/exploiting, and so forth. If you choose a school where the unwritten motto is &#8220;The worth of a person is measured in papers and citations&#8221; you will slowly but surely grow to be a person that would live by such a motto. Would you like to be such a person? So by choosing a school you in some way also define and constrain the person that you can become.</span></p>
<p><span data-preserver-spaces="true">The </span><strong><span data-preserver-spaces="true">Stability Perspective</span></strong><span data-preserver-spaces="true"> says that choosing the &#8220;right&#8221; school is an illusion but that there are other choices that matter much more because they give you the stability that you need to succeed in the arduous PhD journey. It is well known that the effect of most moderately painful or enjoyable events that significantly affect your life will wear off within about two years and that you will </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://en.wikipedia.org/wiki/Hedonic_treadmill#Happiness_set_point" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">return to your baseline happiness</span></a><span data-preserver-spaces="true"> and stay there. However, some things are more stable. A great and friendly social environment where you always feel supported and not alone will provide you with the most human needs and will make a 5-year-or-so journey a breeze. On the other hand, a tiny research group with a distant advisor will make for an uncertain, lonely, and stressful 5 years.</span></p>
<p><span data-preserver-spaces="true">Another valid way to select a school is by the variability of experience it will offer — the </span><strong><span data-preserver-spaces="true">Variability Perspective</span></strong><span data-preserver-spaces="true">. You probably sacrificed in some way to get into grad school. You neglected your passions outside work, neglected friends or your partner or your family, neglected self-development, neglected to work on your mental, physical or spiritual health, or you neglected other things that are important to you. By choosing the school that is best for your career, you might very well continue on this path of neglect. When does it stop? Once you have completed an excellent PhD, you might labor on by choosing that super competitive assistant professor job, then tenure, then being a leading figure in your field, and so on. There is nothing wrong with such a path through life, but continuous exploitation will lead to local minima. The two most common regrets of the dying are &#8220;I wish I&#8217;d had the courage to live a life true to myself, not the life others expected of me&#8221; and &#8220;I wish I hadn&#8217;t worked so hard.&#8221; The dying probably would have avoided their situation if they would have known better. Making sure you have the time and opportunity for further exploration is very helpful in gathering the information necessary to make better choices in the future that do not lead to regret.</span></p>
<p></p>
<h1><span data-preserver-spaces="true">The Career Perspective: Choosing Based on Expected Success</span></h1>
<p><span data-preserver-spaces="true">The career perspective looks at the most critical factors for your academic success and success beyond that and chooses the school that is best according to these factors. Let me go through each factor. I list the factors in order of importance, starting from the most important.</span></p>
<h2><span data-preserver-spaces="true">Advisor</span></h2>
<p><span data-preserver-spaces="true">Finding suitable advisors is probably the most crucial task when choosing between grad schools. One could even go further and argue that one should not choose a school, but one should choose an advisor. A lousy advisor can make you miserable, unproductive, stressed, and might be the main reason why you would want to drop out of the program. The right advisor will help you to be productive, stay healthy, and help you to enjoy doing your research. It is important to emphasize personal fit here: Some advisors are great for you and bad for others and vice versa. The following criteria will help you identify advisors that might be better for you than others. However, there is a great deal of gut feeling to this decision. It is a bit similar to dating, even if everything is right on paper doesn&#8217;t mean this is the right person for you.</span></p>
<p><span data-preserver-spaces="true">Another important note is that you should be looking for advisors and not a single advisor. This complicates an already complicated process, but it is risky to choose a school based on a single person. Relationships are complicated, and things might not work out as expected with your advisor. If possible, you should have an alternative advisor to whom you can switch if it does not work out with the other advisor. This strategy also offers the possibility of being co-advised — two advisors that complement each other may provide a great fit even though a single advisor might not.</span></p>
<p><span data-preserver-spaces="true">The following advisor-related factors do not have a particular order.</span></p>
<h3>Research Style</h3>
<p>Research style is probably the most elusive quality but also by far the most important quality that you acquire during your PhD. While many would say that the goal of a PhD is to become an independent researcher, the truth is that with the steep requirements for ML/NLP PhD positions, many students are already somewhat close to being independent. They can generate ideas with ease and execute them confidently in research projects. However, the actual quality that new students lack is research style.</p>
<p>Harriet Zuckerman is probably the person that studied scientific expertise to the greatest qualitative depth. In her work <a href="https://www.amazon.com/Scientific-Elite-Laureates-Foundations-Education/dp/1560008555">Scientific Elite</a>, she interviewed almost every US Nobel prize laureate of the 20th century. She found that these individuals often rose through the ranks through accumulated advantage. One advantage helped them secure the next position/grant/collaboration, which increased their advantage and helped them secure the next one, and so forth. Zuckerman found that the main advantage gained through this ladder-climbing was not necessarily more resources (money, equipment), but having the opportunity to culturally adopt the research style of other successful scientists. Consistent with this, most future Nobel laureates have been advised by Nobel laureates or would be Nobel laureates. So good questions to ask are: Can my research advisor&#8217;s research style help me in my career? Do I want to be a researcher that follows the style of my advisor?</p>
<p>While your advisor will be the focal point of research culture, research culture is also created through interactions between your advisor&#8217;s lab students. It is usually subconsciously adopted over time. Most students might not be aware of how they were shaped by their advisor and research group. It happens automatically and does not necessarily require explicit thinking or effort.</p>
<p>To give you some examples of what facets of a research style might look like:</p>
<ul>
<li aria-level="1">Ideas are cheap and belong to the research group. Execution of those ideas as research projects is real research.</li>
<li aria-level="1">Novel ideas are everything. If someone publishes something even remotely similar to what you have, you should give up the project and work on something nobody is working on.</li>
<li aria-level="1">Good science is good math. A paper should be mathematically solid so that it will stand for years, holding valuable insights and generalizations that go beyond the current theoretical application.</li>
<li aria-level="1">Good science is robust science. A paper should have careful claims with robust evidence. This will help make the field progress more quickly by providing reliable information to build on.</li>
<li aria-level="1">Good science is a good research vision. A paper should be about what is possible in the future and where a line of research could lead to. Evidence augments vision, but a paper without vision is blind, incremental, and will be forgotten.</li>
<li aria-level="1">Good science is good insight. Some insights can be extrapolated and be applied to many other scientific problems, many of which have not been formulated yet. Finding and expressing these insights is vital for scientific progress.</li>
<li aria-level="1">It is all about productivity. Research is inherently noisy and messy, and it&#8217;s tough to predict the outcome of an idea or set of experiments in the development stage. Navigating this uncertainty is best done through fast iterations and balancing multiple projects to maximize the chances of a big success.</li>
<li aria-level="1">Good science is collaborative. Different people can bring unique perspectives to a project and increase the chance of serendipitous insights. Collaborations bring the best out of people and can result in a sum that is larger than its parts.</li>
<li aria-level="1">Good science is solitary. To gain the deepest insights into a problem, one has to understand a problem in its fullness without outside help. While collaborators can join later, the all-encompassing understanding of a problem through solitary pursuit is critical to tackling the most important scientific problems and for growing as a researcher.</li>
</ul>
<p>These are just some examples, and usually, a research style is made up of a multitude of facets like these. Research style is complex but can best be encapsulated by the questions &#8220;What does good/bad research look like?&#8221; However, if you ask these questions during the visit days, you often find that people answer what they think good research is supposed to look like, rather than what it looks like for them. Therefore, better questions for visit days are:</p>
<ul>
<li>&#8220;What are examples of research papers you like?&#8221;</li>
<li>&#8220;What research papers (in your area) do you think are the most important ones in the last years?&#8221;</li>
</ul>
<p>These questions often reveal what people think are important problems and the &#8220;correct&#8221; manner of approaching these problems. Both qualities are strongly related to research style.</p>
<p>Since the acquisition of research style is largly automatic and subconscious, it is crucial to understand which research style you will be adopting by joining a particular school and lab/advisor. So, what can such adoption look like?</p>
<p>Take a friend as an example who, at the start of the PhD, can be culturally described as a minimalistic hacker-researcher—someone who tinkers around with minimal changes to a system to improve it in a simple manner. He teamed up with an insight-driven neat professor for his PhD. After a couple of years in his PhD, he learned to be an insight-driven hacker. He builds hacks, understands the deep relationships of how his hack affects the system, and then extracts this insight in the most minimalistic and well-formulated way possible along with his practical hack. The combination is a pretty potent mix: the minimalistic insight-driven hacker-researcher. This person finds small hacks that yield robust results and insights into how other research and the hack relate to other concepts.</p>
<p>One friend described me as a product-driven experimental hacker, meaning someone that rapidly prototypes changes to a system and tests them experimentally for reliable effects. If reliable effects are found, the hack is extracted into a product that other researchers can easily use. I was pretty surprised by that view at first, but I now think it pretty much hits the nail on the head.</p>
<p>Some friends I would describe as:</p>
<ul>
<li>concept-centered experimental visionary</li>
<li>gregarious cool-stuff-can-be-good-science collaborator</li>
<li>mind-the-gap collaborator</li>
<li>principled neat-and-tidy collaborator</li>
<li>I-like-cool-stuff researcher</li>
</ul>
<p>It is important to note that there is neither right or wrong nor good or bad research style. For example, in Zuckerman&#8217;s work, two Nobel Laureates in the same field would sometimes have radically opposite research styles, yet each different research style made both Nobel Laureates and their students successful. Similarly, while an I-like-cool-stuff style sounds unimpressive, the Feynman-like playfulness of an I-like-cool-stuff researcher might lead to significant discoveries that others overlook because others do not deem these problems serious enough to consider working on them.</p>
<p>Looking at my friends, they often came in with a particular mindset, and looking at them now, they very clearly adopted the central cultural tenet of their research environment.</p>
<p>It can be very empowering to enter grad school with this view. A friend of mine entered grad school and, upon hearing this interpretation, actively thought about how he could augment his research style with a particular advisor&#8217;s style. He switched advisors until he found the right ones. Then he leaned in and tried to adopt the advisor&#8217;s central cultural research facet as quickly as possible. My friend&#8217;s primary research advisor told him four years into the PhD that there is nothing left that he can teach him and that he should graduate and move on to learn more. I was not surprised and think it directly related to my friend&#8217;s viewpoint that adopting a research style and developing research taste is the most crucial element of a PhD.</p>
<p>So while elusive and hard to define, the research style of a particular advisor or department can be an essential consideration to choosing the grad school that is right for you.</p>
<p>While the following sections will dive into other angles on choosing potential advisors, they can also be interpreted as sub-component of research style, particularly the advisor values section.</p>
<h3><span data-preserver-spaces="true">Advisor Research Fit</span></h3>
<p><span data-preserver-spaces="true">Students often do not know what to look for in an advisor and often cling to the idea that they need to find an advisor that is interested in the same research that they want to do. There is some truth to this idea, but this idea is more dangerous than it is helpful. From my friends in the 2nd year, about 66% changed their research direction completely — many of them in the first year. That number is higher if I look at later years. Most of them still work in their subfield (robotics/NLP/vision), but they switched to different research areas in those fields. Some examples:</span></p>
<ul>
<li><span data-preserver-spaces="true">Multilingual parsing -&gt; multilingual models -&gt; machine translation</span></li>
<li><span data-preserver-spaces="true">question answering -&gt; dialog -&gt; reinforcement learning -&gt; semantic parsing</span></li>
<li><span data-preserver-spaces="true">NLP architectures -&gt; machine translation -&gt; model efficiency</span></li>
<li><span data-preserver-spaces="true">human pose recognition -&gt; sim2real</span></li>
<li><span data-preserver-spaces="true">question answering -&gt; model efficiency -&gt; interpretability -&gt; model efficiency</span></li>
</ul>
<p><span data-preserver-spaces="true">What you see from these transitions is that an exact fit is not needed with advisors since your research interests will change. The same is true for your potential advisors: they might no longer be interested in research that they are well known for, or they might be interested in a direction which they have not yet published in. Compared to students, advisors have much more breadth and might be equally interested in many different research directions at once. Furthermore, while new professors are often compelled to stick closer to a specific research direction until they get tenure, tenured professors can be very flexible in research directions, and their interest might also be influenced significantly by the interests of their students. More senior professors are often happy to take on a completely new research direction that is interesting to you and compelling to them — this can be the advantage of hands-off advisors, which I will talk about in the next section.</span></p>
<p><span data-preserver-spaces="true">Despite the overall fluidity of research interests of both you and your advisor, it is a good idea to have at least some overlap. It might be worth asking about the advisor&#8217;s long-term research vision, but be aware that such plans are often not well fleshed out and can change quickly based on changes in the field (e.g., BERT). It might also be worth looking at the values of an advisor because they are rather stable over time, and they can hint which kind of research they like — more on values later.</span></p>
<h3><span data-preserver-spaces="true">Advising style: Hands-on vs Hands-off</span></h3>
<p><span data-preserver-spaces="true">Advising styles can be mainly separated into hands-on and hands-off styles. What does this mean?</span></p>
<p><span data-preserver-spaces="true">In general, what you can expect from a hands-off advisor is that you do all the work, and your advisor gives you feedback on what you have produced. For a hands-on style, the advisor also helps with the producing in some way.</span></p>
<p><span data-preserver-spaces="true">More concretely, a hands-on advisor might be helping you with many details of your research: Brainstorming research ideas, discussing research ideas and problems in detail, help define research problems and ideas, thinking about a narrative for your paper, formulating claims, structuring research project into certain pieces with milestones, checking in frequently to discuss partial results, discussing programming problems and bugs, providing rapid feedback, steering the project to prevent failure, providing detailed feedback for the write-up, providing detailed feedback for presentation slides – all of these are signs for a hands-on advising style.</span></p>
<p><span data-preserver-spaces="true">Hands off advisor will be helping you with high-level details of your research: Discussing viability and impact of a research idea, discussing research narrative/pitches/claims, discussing research results, providing (high-level) feedback on final paper draft and slides. Working with a very hands-off advisor has many benefits, but in terms of direct help and interaction with your hands-off advisor you often cannot expect much more than I list here.</span></p>
<p><span data-preserver-spaces="true">The hands-on / hands-off dichotomy is a continuum — usually, an advisor exhibits a mix of these traits. For example, some advisors might be very hands-off, but are very involved in idea generation, while yet others really like to give detailed feedback on writing. Usually, advisors also adjust slightly to the needs of each student and can be more hands-on in research areas where he or she is well-established. It is useful to talk to students to get the exact details in which areas the advisor is hands-on or hands-off. Areas here can refer to activity areas (help with writing, brainstorming ideas, thinking about a research story, etc.), technical areas (helping with bugs/code, finding the right software framework), and research areas (machine translation, question answering, etc.). So you should not ask students, &#8220;Is your advisor hands-on or hands-off?&#8221;, but instead you should ask, &#8220;Is your advisor hands-on with giving feedback on writing?&#8221; and so forth. Ask about the areas that are most important to you (your weak areas).</span></p>
<p><span data-preserver-spaces="true">A hands-on advisor is great if you are less experienced in research, need more structure and deadlines, are unsure about potential research topics, and are externally motivated. A hands-off advisor is great if you want more freedom and independence, and also, if you want to learn more through failure and adversity — being on your own for a good portion of the PhD can be difficult, but it also makes you a better independent researcher. If an advisor is not overly helpful, that is great for you in the long-term, but it can be difficult for you in the short-term, especially from in the first year or if you need to navigate important milestones such as conference deadlines.</span></p>
<p><span data-preserver-spaces="true">Usually, hands-off advisors are more senior and can also provide more connections for internships and collaborations and are able to link ideas to some good-old research ideas that most people forgot about. They usually also have a more extensive lab with postdocs and a range of senior PhD students, which can provide valuable hands-on advice.</span></p>
<p><span data-preserver-spaces="true">A hands-on advisor can usually develop you in more detail and, in the ideal case, will provide a gradual increase of independence towards the end of your PhD. Through this process, you will become similar to your advisor since a hands-on advisor develops you in his or her image. This can be a good or bad thing, depending on what you want to do with your career. If your hands-on advisor&#8217;s research vision is highly sought after in industry or academia, it is an advantage; if the market is saturated with the same research vision, you are just another fish in the sea.</span></p>
<h3><span data-preserver-spaces="true">Advisor Values, Strengths, and Weaknesses</span></h3>
<p><span data-preserver-spaces="true">What does the advisor care about? This is often overlooked, but the values of your advisor can make or break a good fit. It also defines the environment within the research group. Why do values matter?</span></p>
<p><span data-preserver-spaces="true">As noted above, interests change all the time, but values are much more stable and vital for a healthy relationship. While differences in interests are often fine (machine translation vs question answering), differences in values can create conflicts (overclaiming vs underclaiming). In general, you want the same as in any relationship: Share as many values as possible and have differences in strengths and weaknesses which complement each other. So what do values in an academic relationship look like?</span></p>
<h4><span data-preserver-spaces="true">Neats vs Hackers</span></h4>
<p><span data-preserver-spaces="true">One fundamental difference in academic values is if your advisor is a Neat or a Hacker: A Neat is someone that values systematic investigation, sound assumptions, proofs, precise claims, and theoretical progress whereas a hacker believes that adherence to rigid schemes slows down progress. A neat is careful in their methodology, cautious in making claims, and lets results speak for themselves: &#8220;Another solid result for the literature.&#8221; A Hacker first and foremost values results and their impact and practicality: anything &#8220;that works&#8221; is acceptable. A hacker values unconstrained exploration, integration of things &#8220;that work&#8221; and progress that makes a difference in the real world. Hackers are usually less careful with making precise claims because they believe it is more important to think about the (yet unproven) potential and possibilities of an idea. Hackers like to show off their work: &#8220;Look at this cool method — the results are unbelievable!&#8221;</span></p>
<p><span data-preserver-spaces="true">None of these roles is inherently more valuable than another — both are needed to make progress in science. The best results in science often come from critical discussion and work across these camps.</span></p>
<p><span data-preserver-spaces="true">This is also a continuum. I am a Hacker at heart, but I get offended if someone misuses (or does not use) statistics or if someone makes theoretical claims built on weak theoretical foundations.</span></p>
<h4><span data-preserver-spaces="true">Discretion and In-group cohesion</span></h4>
<p><span data-preserver-spaces="true">Does the advisor value discretion, privacy, and is open at the same time? This encourages honesty and directness between you and your advisor, but you might know less about what other students work on and how they make progress on their project. A lack of such information might feel isolating. On the contrary, an advisor that tells you about his or her other student&#8217;s projects and progress makes it easy for you to get involved with other students, which facilitates in-group collaboration and cohesion — you stick together and support each other, and it feels a bit like a family. The problem with that is that if you say something, everybody will know soon enough — so you need to be careful what you say which can be stressful and can lead to a culture of closedness or faking: &#8220;Everything is okay with my project — I do not need help!&#8221;</span></p>
<h4><span data-preserver-spaces="true">Well-being and Research Progress</span></h4>
<p><span data-preserver-spaces="true">Does the advisor value your well-being over research progress or vice versa? An advisor who values your well-being will make sure that there is freedom for work-life balance and that your 1-on-1 meetings are not only about research. While your mental health and stress levels are first and foremost your responsibility, an empathetic advisor will be able to see if you are overdoing it and can offer guidance to avoid overwork and burn-out. On the other hand, such an advisor makes it easier for you to slack off and have research projects slide into oblivion, which can stop progress and make you feel depressed or make you feel like a failure.</span></p>
<p><span data-preserver-spaces="true">Advisors that push you to your limit to do research might be a good fit if you need some pushing to be productive. However, too much pushing, or if you do not like to be pushed, might cause burn-out, high stress, or might make you anxious to meet the high expectations of your advisor.</span></p>
<h4><span data-preserver-spaces="true">Communication</span></h4>
<p><span data-preserver-spaces="true">Does the advisor value sharp, direct criticism or indirect, gentle hinting that something is off? If your advisor values head-on criticism, he or she will call out bullshit and tell you how much your project idea sucks. This is difficult to take as a student that labored hard on a research idea. On the other hand, you do not need to waste more time on this idea and can move on. If you are able to remain calm and digest such feedback, then you might be able to quickly adjust an idea and make it work. With such an advisor, you know a research idea is air-tight if he or she gives you good feedback, and it makes you proud that this idea &#8220;passed your advisor.&#8221; From there, it is easy to move on and work on the idea.</span></p>
<p><span data-preserver-spaces="true">An advisor with an indirect communication style will hint that something is off, but you might not know what or why. That can make progress slow, or it can create considerable uncertainty if your project is any good even months into the project. However, your feelings are not hurt with such a communication style. Furthermore, this indirectness might also demonstrate the intellectual humbleness of your advisor: if an experienced advisor believes he or she can be wrong, it might open up the possibility for a candid dialog of exploring what is true and what is not. This is an admirable quality that many intellectuals value highly, and it might rub off onto you. In the long-term, indirect communication has the advantage that you need to think about problems more by yourself, which makes you more independent and a better researcher.</span></p>
<h4><span data-preserver-spaces="true">Strengths and Weaknesses</span></h4>
<p><span data-preserver-spaces="true">As noted before, beyond values, it is also essential to think about how you and your advisor&#8217;s strengths and weaknesses complement each other. This is generally important for collaborations. For example, you might be great at executing research projects quickly so that you get the evidence to decide along which path you push the project, but you might be bad at generating good research ideas. An advisor that matches your core values and complements your weaknesses — idea generation in this case — will make a great tag team partner and will make it easy to wrestle those challenging research projects into submission. On the other hand, sharing weaknesses can make you and your advisor blind to problems in your research. Good advisors will recognize your weaknesses and strengths and will try to complement your style of research.</span></p>
<h4><span data-preserver-spaces="true">Self-reflection Key to Good Decision</span></h4>
<p><span data-preserver-spaces="true">To understand the relationship between values, strengths, and weaknesses between you and your potential advisor, it might be well worth it to find some time for a session or two of careful self-reflection to understand who you are and how you align with potential advisors and schools. Beyond alignment, it might also help you to identify schools and advisors, which possibly facilitate growth toward specific values and strengths that you cherish but not yet possess.</span></p>
<p><span data-preserver-spaces="true">Some questions that could get you started: Can you deal with direct, sharp criticism? How much do you value your privacy? How much honest and open conversation? Are you more like a Hacker or more like a Neat? Are you a &#8220;family person&#8221; and favor very close cohesion within the research group? How self-motivated are you? Do you need deadlines and milestones to keep you motivated and on track? Do you work well if someone pushes you? How much work-life balance do you need?</span></p>
<h3><span data-preserver-spaces="true">Advisor Availability and Absent-mindedness</span></h3>
<h4><span data-preserver-spaces="true">Availability</span></h4>
<p><span data-preserver-spaces="true">Availability does make a difference. It is better to have more frequent meetings, even if these meetings are with more hands-off advisors, and you have no results. If an advisor also works at a startup/company or has many students, it might be that meetings are infrequent, canceled, postponed, and additional meetings in the time of need are not possible. However, availability is not only about your advisor&#8217;s busy schedule but also their attitude. For some advisors, student meetings are &#8220;holy&#8221; and are rarely canceled or rescheduled. Some advisors are also open for frequent meetings in times of need while others are not.</span></p>
<h4><span data-preserver-spaces="true">Absent-mindedness</span></h4>
<p><span data-preserver-spaces="true">Another closely related factor is absent-mindedness. There are advisors who forget about projects, and you have to explain them over and over again what you are working on. Even if an advisor is available, a certain degree of absent-mindedness can make interactions frustrating and unproductive. On the other hand, similarly to a hands-off advisor, this forces you to think carefully about your project and formulate exact problems before a meeting, which will make you a better researcher in the long-term. Being able to formulate your project as a concise elevator pitch with a precise definition is a highly valuable skill that will impress anyone whom you meet at a conference. The other extreme are advisors that reserve blocks of time to think about your project outside of meetings just on their own — which has obvious benefits: better feedback, guidance, and new angles to the project, which might improve it significantly. On the other hand, this can make you dependent on your advisors&#8217; thoughts, which can prevent you from becoming an excellent independent researcher.</span></p>
<h2><span data-preserver-spaces="true">Peers, Postdocs, and Research Group</span></h2>
<h4><span data-preserver-spaces="true">Peers</span></h4>
<p><span data-preserver-spaces="true">The peers and the research group are the second most important factor to go to a school, and this factor is not far behind the advisor in importance. Regarding research interest, it is a bit similar to advisors: Your peer&#8217;s research interests change over time but will usually stay in a related area. As such, it might be possible to have long and fruitful collaborations with particular students, but probably it is more realistic to see people in your research group as peers with whom to discuss research ideas and get feedback from.</span></p>
<p><span data-preserver-spaces="true">But there are other things which are robust over time, such as general interests and values. During your visit days, you get to know some of your possible peers — both other admits and people in research groups — and sometimes it is evident when you &#8220;click&#8221;. Although it is difficult to get to know people in detail in this short time, a group of people that you click with might be a good reason to go to that school. If you have a friend who supports you through the difficult times and who challenges you to grow will be very helpful in your journey through a PhD and beyond.</span></p>
<h4><span data-preserver-spaces="true">Research Group</span></h4>
<p><span data-preserver-spaces="true">Beyond individual peers, you should also consider the research group of your potential advisor in your choice of grad school. The dynamics of the research group are quite revealing about the norms and values of the research group. The values of the advisor (see above) shapes the dynamics in the research group strongly. You can use a similar framework as presented above to assess the values and expectations of your peers within a research group.</span></p>
<p><span data-preserver-spaces="true">Another critical view on research groups are the power dynamics and diversity, which are strong predictors for the success of the overall research group. Research says groups work best if a powerful individual brings together people with very diverse backgrounds, views, and experiences, and once they are among themselves gives up his or her power and lets these people collaborate on an even playing field.</span></p>
<p><span data-preserver-spaces="true">Diversity is particularly important for creative endeavors because diversity helps to prevent echo-chambers. Let&#8217;s say you have a group of hackers that reads about a new research method A:</span></p>
<p><span data-preserver-spaces="true">Hacker 1: &#8220;Wow method A is so exciting. The results on Task C are so great! It would be so cool to mash it together with method B and try it on task D!&#8221;</span><br />
<span data-preserver-spaces="true">Hacker 2: &#8220;You are right, that would be so interesting!&#8221;</span><br />
<span data-preserver-spaces="true">Hacker 1: &#8220;Let&#8217;s do it! Let&#8217;s hack it together!&#8221;</span></p>
<p><span data-preserver-spaces="true">It is a very different dynamics if you add some neats into the mix:</span></p>
<p><span data-preserver-spaces="true">Neat 1: &#8220;From Author, et al. 2020, I know that the standard deviation on task C is quite high and I think confidence intervals from method A would overlap with method X — the results from method A do not seem any better than results from the simpler method X. So, by Occam&#8217;s razor, I do not think there is any reason to extend method A.&#8221;</span><br />
<span data-preserver-spaces="true">Neat 2: &#8220;I think their performance is mostly explained by their unusual initialization rather than method A. With that initialization you expect lower relative differences between the eigenvalues of the Hessian and thus faster training —so I think the number of epochs is a confounding factor and their comparison is invalid. I do not believe method A is actually better. They should have used the same initialization or at least done a grid search over learning rates and epochs for a proper comparison.&#8221;</span></p>
<p><span data-preserver-spaces="true">This is an example of a neat vs hacker debate, but the same goes for many other traits and values. For example, if you have only people who discuss ideas with direct, blunt criticism, the interactions can feel pretty overwhelming and intense, and good ideas might be lost within the group because it is too tiresome to talk about it. Instead, a mix of playful and serious people might be able to balance the free idea generation with rigor and carefulness.</span></p>
<p><span data-preserver-spaces="true">Other extreme dichotomies might include theory vs applications thinking: &#8220;Life is temporary, only proofs are eternal.&#8221; vs &#8220;If you make the &#8220;greatest&#8221; invention ever and it does not affect a single life, then what is the point of that?&#8221; Quantitative vs qualitative thinking: &#8220;If you cannot measure it, it does not exist!&#8221; vs &#8220;Try to measure how much you love your spouse and then tell me which number it is — it does not work!&#8221; There are probably many more of these extremes.</span></p>
<p><span data-preserver-spaces="true">Of course, virtually nobody believes in these statements, but some people identify more with one than the other, and having a healthy mix of each of these perspectives within a research group prevents groupthink, bias, and unreasonable extremes.</span></p>
<h4><span data-preserver-spaces="true">Postdocs and Senior PhD Students</span></h4>
<p><span data-preserver-spaces="true">Briefly mentioned above, postdocs and senior PhDs can also have a tremendous impact on the advising situation and should be considered carefully in your choice. If your advisor has postdocs and senior PhDs which frequently collaborate with new PhD students, it can be a big win for both parties: You get additional hands-on experience, and a research perspective which is different from your advisor (especially with postdocs) and they might be able to get another publication before they move on to the next job. Having senior PhDs and postdocs is, in particular, valuable if your potential advisor is hands-off — in this case, you can get the best of both worlds in terms of advising.</span></p>
<h4><span data-preserver-spaces="true">Others</span></h4>
<p><span data-preserver-spaces="true">Other important factors for a good research group are how much ideas are shared and discussed (what happens in a regular research group meeting) and how much students collaborate (easy to check by looking at their publications). The degree of collaboration is also a good proxy of group cohesion. I will talk a bit more about the importance of socializing in research groups further below in the &#8220;Stability Perspective&#8221; section, and I will not repeat myself here.</span></p>
<h2><span data-preserver-spaces="true">School Name and Resources</span></h2>
<h3><span data-preserver-spaces="true">Accumulated Advantage</span></h3>
<p><span data-preserver-spaces="true">To make rational choices about the prestige of a school, it is essential to understand why it actually matters.</span></p>
<p><span data-preserver-spaces="true">The scientific reason why school names matter is that they represent a proxy of accumulated advantage, which is a good predictor of current performance. </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://www.jstor.org/stable/2095162" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Cumulative advantage</span></a><span data-preserver-spaces="true"> is the idea that the more privilege you had in life, the more likely you had the resources (money, educated parents, mentoring, good peers, free time, extracurricular activities, extensive social network) to do well (rapid development, good grades) and this gives you more resources (better schools, better jobs, better connections) to do even better (promotions, tenure, grants) which yields even more resources (even more extensive social network, collaborations, grants, funding) to do even better (Nobel prize, Fields Medal, unicorn startups).</span></p>
<p><span data-preserver-spaces="true">The distribution of advantage at any of these stages is highly unequal with the top few percents being the most productive and gaining the most resources: ⅓ of the US population get a Bachelor&#8217;s degree, 2% a PhD, 0.2% a top 20 undergrad degree, 0.06% a tenure track position, </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://www.sciencemag.org/news/2014/07/1-scientific-publishing" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">0.0006% of people publish 41% of papers in research journals</span></a><span data-preserver-spaces="true">. But at the same time, at a top school, </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://timdettmers.com/2018/11/26/phd-applications/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">73% of PhD positions are given to people with undergrads from the top 20 schools</span></a><span data-preserver-spaces="true">, and the top 18 schools produce </span><a class="_e75a791d-denali-editor-page-rtfLink broken_link" href="https://advances.sciencemag.org/content/1/1/e1400005" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">50% of professors</span></a><span data-preserver-spaces="true">. We can do some back-of-the-envelope calculation with these data by making some simplifying assumptions to calculate the probabilities of becoming a professor if you do a bachelor or a PhD at a top 18 school. If we assume that the 50% of professor from the top 18 schools are equally distributed then 1/36 of all professors come from each top 18 school. </span></p>
<p>Thus if you do a PhD at a top 20 school, your prior probability of becoming a professor jumps from 0.06% to about 2.8% — about 50 times more probable, but still only as likely as rolling two sixes with a pair of dice. This means you can increase your chances dramatically by choosing a prestigious school, but the odds are still heavily stacked against you. Similar statistics hold true for making other choices based on prestige or school ranking. Making a choice based on school ranking alone will probably not lead to success. Other factors, like a great advisor, great peers, a productive research group, school culture, and social opportunities, are probably more critical for success.</p>
<h4><span data-preserver-spaces="true">Some Failure and Adversity is Critical for Success</span></h4>
<p><span data-preserver-spaces="true">A different perspective that might seem unintuitive at first is that a long streak of privilege can have harmful long-term consequences for you. Failure and adversity are great tools for personal growth and </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://pubmed.ncbi.nlm.nih.gov/20939649/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">growth as a researcher</span></a><span data-preserver-spaces="true">. This is a </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://psycnet.apa.org/buy/2010-21218-001" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">well-established finding in psychology</span></a><span data-preserver-spaces="true">: To succeed in life, you need to fail sometimes but not too often. The intuition is that the extremes of privilege or adversity lead to poor mental models of perfectionism and </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://en.wikipedia.org/wiki/Learned_helplessness" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">learned helplessness</span></a><span data-preserver-spaces="true">, respectively, while occasional failures lead to a mindset of </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://en.wikipedia.org/wiki/Learned_industriousness" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">learned industriousness</span></a><span data-preserver-spaces="true">. This means, too much privilege will make you afraid to take risks and fail because you never failed before. Occasional failure will make you resilient because you know adversity is normal and temporary — a mindset that enables the pursuit of creative but risky ideas.</span></p>
<p><span data-preserver-spaces="true">For example, if you are at a top 20 school, it might be expected of you that you behave like a top 20 school researcher: Publish many world-class papers in a short period of time. Such a competitive environment might encourage &#8220;safe&#8221; research that is easily publishable over creative research that is prone to fail. Such a school, while providing a boost in privilege and resources, might prevent you from becoming a successful and creative researcher in the long-term. Challenging yourself in a non-perfectionist way is important — make sure there is enough opportunity for lessons learned through failure at the school that you choose.</span></p>
<p><span data-preserver-spaces="true">Similar strategies are also used in industry: Failing a startup is often seen as a requirement for founding a successful startup. Joining a scrappy startup might make you a skilled engineer while joining a big tech company might lead to stagnation in skill — you are just another cog in the machine.</span></p>
<h3>Computational Resources</h3>
<p>After the release of BERT, some of my peers felt energized by the exciting results of large pretrained models, but equally many of them seemed defeated. It must be excruciating to see your research sub-field and the research that you worked so hard on being crushed by the simple idea of throwing more GPUs and data at the problem. But this is the reality that we live in. With the advent of GPT-4, we reach another of such critical points, and one should take care that one is in a position where one can do meaningful research after GPT-4. What does this mean?</p>
<p>In most sub-fields, there are general ideas that are unaffected by scale or even incorporate scaling into their outlook. As such, hope is not lost even if your research will be affected by GPT-4. However, this may mean that the research you are doing will be very different before and after GPT-4—just like it was with BERT. While it is very unpredictable how research will shift with models like GPT-4, it can be worthwhile to think of possible ways your research could adapt in specific scenarios.</p>
<p>While it might be completely counter to your current research and values, it might be helpful to think about hypothetical worst-case scaling scenarios. Consider this example that may border on outrageousness depending on your values. While most bias and fairness research in *CL conferences mostly frames large pretrained models as a problem, there is already a hint that scaling models might solve such issues if used correctly. As such, a worst-case hypothetical to ask yourself could be: &#8220;If scaling laws for bias and fairness show that scaling resolves bias and fairness eventually with scale, what would you do?&#8221; An answer to this particular case might be among others:</p>
<ol>
<li aria-level="1">Shift your research to the broader perspective of human preferences, which already has a foothold into scaling.</li>
<li aria-level="1">Incorporate scaling laws into your research and analyze the properties of models/data/methods that lead to improved bias scaling, particularly at a smaller scale or with fewer resources.</li>
<li aria-level="1">Try to scale differently from computing and data, for example, by using reinforcement learning on bias and fairness user feedback data.</li>
<li aria-level="1">Think about alternative research sub-fields you might want to switch to if this happens.</li>
<li aria-level="1">Relate bias and fairness to some effect known to be a factor at scale. For example, how does memorization relate to bias and fairness?</li>
</ol>
<p>The next question to ask yourself is this: how many computational resources will I need to proceed in this manner? And with that, it follows: will I have these computational resources if I join a particular school over another?</p>
<h2><span data-preserver-spaces="true">School location (campus &amp; city)</span></h2>
<p><span data-preserver-spaces="true">I will not elaborate here since I will address important considerations for these factors, mostly in the Stability and Variability Perspective. To foreshadow a bit how to think about these: The campus and city with its possibilities will offer opportunities that help you to do the things that you know will ground you and make you stable so that you can sustain the difficult journey that a PHD is (stability). Each city and campus also offers a different range of new activities and experiences (variability), which help you to explore who you are and what you like and make you a fuller, more vibrant human being.</span></p>
<p><span data-preserver-spaces="true">The right way to think about this factor is very personal and can either be an insignificant factor or a factor which is more important than the other factors listed before — it is worth it to stop, think and reflect quite a bit on this. But more on that in the Stability and Variability Perspective sections.</span></p>
<h2><span data-preserver-spaces="true">Other Factors</span></h2>
<p><span data-preserver-spaces="true">There are other factors that I could write about, but they are not that important. Housing, living costs, stipend / salary are not that important. There are differences, and one school might pay more than another or has lower living costs, but the outcome will be the same — you will not be rich, you will not be poor, and it does not matter what your living arrangements are it will feel like home eventually. At some universities, you can work part-time in an industry research lab, and you can make much more money, but it also adds extra complications.</span></p>
<p><span data-preserver-spaces="true">It also makes sense to consider the university culture and the research group culture, but these are quite closely tied to the values that your potential advisor, peers, and research group holds. Culture is also closely related to the Identity Perspective, which I will introduce next.</span></p>
<h1><span data-preserver-spaces="true">The Identity Perspective: Who do you want to be?</span></h1>
<p><span data-preserver-spaces="true">If you choose a particular school, you will be actively shaped by the environment you live in and the people you interact with on a day-to-day basis. The identity perspective is then the perspective where you try to optimize for the person that you might become. While the career perspective looks at the question “How much success am I expected to have?” the identity perspective looks at the person that you might become and asks, “Do I want to be that person?”</span></p>
<p><span data-preserver-spaces="true">Choosing a school based on who you think you will be is a very personal and subjective choice. I do not believe some specific examples will help you understand how to think about this choice. Instead, I want to give you my personal experience and how that experience shaped my belief about the person that I might become for each school. I gathered most of this experience during visit days where people tend to show their best selves, but I also experienced interactions at conferences and internships, which might have been a bit more authentic. I believe the aggregate identities can give you an accurate picture of the person that you might become.</span></p>
<h4><span data-preserver-spaces="true">My Experience at Visit Days</span></h4>
<p><span data-preserver-spaces="true">While I have mixed experiences from students of most schools, there is one school from which I never met someone that was nice or treated me with decency. They often ask where I study, and if the answer is not the right university, they will move on to people that actually study or studied at these “right” universities. Sometimes, they would look at my badge if it showed my university (Università della Svizzera italiana); they would proceed to ignore me in a conversation with other people. During the visit days of one school, someone looked at my badge that displayed my undergrad university, The Open University, and said: “Wow! It is nice that they give people like you a chance!” He left before I could respond. I am sure there are friendly people at these places, but if I meet 15 people from the university and they are all very superficial and disrespectful, that is quite telling. So do I want to be a vain, shallow, and rude person? No, thanks.</span></p>
<p><span data-preserver-spaces="true">At another school, I had the most alienating and isolating experience I have had during my visit days. People from elite universities formed cliques and did not let other people in. My accommodation that I had to make because of flight scheduling was not paid for. I felt as if I was being made fun of for my food preferences. My meeting with a potential advisor was botched, and I needed to share a time-slot with another student. This happened twice. One potential advisor was not there and did not try to contact me before/after the visit days. Another potential advisor belittled me in the meeting I had with him. For many people that I met at that school, it seemed that they felt the need to put on a happy face when they were actually very sad or stressed. Do I want to be a person that supports an environment where anything goes, where deception is the norm? Do I want to be a person that feels the need to show off a “happy face” even when miserable? No, thanks.</span></p>
<h4><span data-preserver-spaces="true">My Experience at the University of Washington</span></h4>
<p><span data-preserver-spaces="true">In stark contrast, the environment at the University of Washington (UW) was designed, so nothing can go wrong. I felt that everyone had a good time at the UW visit days. That at least shows that people are conscious and aware of social dynamics. What was most striking to me was the student panel: A visitor brought up the question of mental health and stress during the PhD. The panel went all out, talked about their mental health issues, and how they coped with it and what mental health resources are at your disposal as a UW student. Similarly, many students were honest, open, and emphasized that the students are a team and that they look after each other. They also made clear that time outside of work is very important to them. One other very endearing thing to me was how friendly people in Seattle are in general. When I arrived in New York, I could immediately feel the tension and impatience. The opposite is true for Seattle. There is a certain gentleness about things. Drivers are more patient and responsible. In Seattle, people exit through the backdoor of articulated buses and yell to the bus driver: “Thank you!” And in their faces, you can see that they do not just say that for show or to conform to social norms — they mean it. So, do I want to be an honest and open student that admits to his struggles, supports his peers, is collaborative, has a life outside work, and who kindly thanks the bus driver? Yes, please.</span></p>
<p><span data-preserver-spaces="true">You might think it is silly that you decide on a grad school based on if people in that city thank the bus driver. But really it is not! Since I started at UW, I have embraced being a bus-driver-thanking person. Maybe this made me more kind. Maybe that makes me more appreciative of the hard work that all the people around me do. Maybe it made me write this blog post with which I hope to help you in your difficult choice. Maybe, if I would have chosen another school, this blog post would be about a cool startup idea; or perhaps I would just have spent that extra time on more research. Identities matter. With the choice of studying at UW, I, in part, chose who I want to be.</span></p>
<p><span data-preserver-spaces="true">If I take off my UW hat, I can see that one could also have a very different interpretation. Maybe the mean people that disrespected me at the elite school were just protecting their limited resources by giving time to people that probably mattered more. Perhaps the lovely people from UW are socially naive — trying to make everybody happy, which is clearly not possible. Maybe it is wiser to concentrate resources where they matter.</span></p>
<p><span data-preserver-spaces="true">It can also be different for different people. I have a good friend who is very blunt and direct — totally normal for the country he is coming from where this bluntness is a signal of trust and honesty. However, calling out bullshit in a blunt and direct way does not fit in nicely with the UW identity and culture, and that can lead to misunderstandings and problems.</span></p>
<h1><span data-preserver-spaces="true">The Stability Perspective: Schools do not matter, but what does?</span></h1>
<p><span data-preserver-spaces="true">Grad school is incredibly tough: By definition, the final goal of the PhD is to gain the skill to independently explore and confront the unknown to produce new knowledge. This requires a lot of self-motivation, enduring failure and rejection, and lots of hard work. I often heard how difficult a PhD is, but I did not believe it. Now I understand what that means. And it has not only been tough for me, but for most of my peers. As such, you want to have something to cling on to that stabilizes you and helps you to cope and enjoy the experience.</span></p>
<p><span data-preserver-spaces="true">The stability perspective acknowledges that many factors, such as happiness, gradually revert to a personal set-point while other elements are stable over time and provide you with energy, resilience, and courage in the long-term. As such, the stability perspective is about prioritizing factors that you know will help you to have a successful grad school experience in the long-term. Research shows that relationships are the most important and stable source of well-being. So it is crucial to look at the social environment when you choose a grad school.</span></p>
<p><span data-preserver-spaces="true">Usually, within a grad school, the social environments are the office, research group meetings, other group meetings, lunchtime, and social activities organized by the department or by grad organizations. Fewer research groups have social outings as a group, but that definitely helps to make grad school more enjoyable and manageable.</span></p>
<p><span data-preserver-spaces="true">One reason why I really wanted to go to University College London (UCL) was that I already knew the people there, and they were super friendly and helpful. </span><a class="_e75a791d-denali-editor-page-rtfLink" href="http://www.riedelcastro.org/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Sebastian Riedel</span></a><span data-preserver-spaces="true"> is an absolute great advisor and a very wise person, and it was a joy working with him. But another important reason why I wanted to join UCL was actually the daily lunches.</span></p>
<p><span data-preserver-spaces="true">Someone would announce “lunch” in the office. Some would go downstairs to buy some food. Some people brought some food. Then we sit all around the table. We would chat about our everyday life, everyday problems. Sometimes our passions or the one or other curiosity. Some politics and news. Sometimes some research ideas. It felt like a family where people cared for each other. It was great! It gave me the energy and focus to do great work even after lunchtime when I am usually tired and less focused. If I would know, I could get this experience at a school or research group, this would definitely be part of the reason why I choose that school.</span></p>
<p><span data-preserver-spaces="true">A great source of stability for me at the University of Washington (UW) has been my office. As desks freed up, we moved more and more NLP people into our office. Now we have an NLP office where we chat about research, support each other for deadlines, and in general, take care of each other, and it feels great. If I had known that I can have all these great, friendly peers around me, this would have been another reason for UW.</span></p>
<p><span data-preserver-spaces="true">The office environment, group meetings, and having more social lunches is something that will keep you stable and mentally tough throughout the PhD and can be a valid and important reason to choose one school over another.</span></p>
<p><span data-preserver-spaces="true">Beyond the social environment, there can also be fundamental personal reasons to prefer one school over another. These are usually not discussed much because they are too distinct — I will give you some examples anyway, which might be a guide on how to think about your personal reasons.</span></p>
<p><span data-preserver-spaces="true">For me personally, Stanford was one of the top choices. Stanford is impressive academically, and I had a great fit with potential advisors there. However, one other thing that stood out for me was the bicycle track around Stanford. I am an avid inline-skater — or rollerblader, as it is called in the US — and inline-skating gives me a lot of stability. It is vital for my mental health. The joy and freedom I feel while skating helps me to get through the dark periods in my life. The bicycle track around Stanford is an absolute dream if you are a skater: Very smooth, flat, dry weather. I imagined myself getting up at 5:00 am and skating every morning through a deserted campus — what a pleasant thought!</span></p>
<p><span data-preserver-spaces="true">Another popular topic are relationships, family, and friends. Most people do a PhD because it is their passion. If you can combine your passion with a great partner, it could give you all you need to flourish as a person. If you can go to a school together with your partner, this can be a good reason to choose one school over another. If you make this choice, you should, however, also beware that doing a PhD is a significant stressor for a relationship, and you should think about if you would like to stay at a particular school if your relationship would end. From my friends who started the PhD with me, for most of them, including for myself, their relationship ended partly due to doing a PhD. A PhD is not easy for a relationship: Moving across country or continent, adjusting to a new culture, working long-hours, little pay, night shifts, and high-stress before deadlines. The pressure and stress from a PhD can make you depressed, anxious, absent-minded, and unresponsive — not the ideal state to be in for a relationship. You get used to this and learn how to handle these stressors, but in particular, when you do your first year of as PhD it can be an enormous strain on your relationship. On the other hand, being able to bring your partner along or to reunite with your family will give you great strength and motivation. You will be able to push harder and further with your research. You will be able to cope better and recuperate faster. A PhD is challenging, and having the most important person close behind you makes it much more manageable.</span></p>
<h2></h2>
<h1><span data-preserver-spaces="true">The Variability Perspective: The possibility of a better you</span></h1>
<p><span data-preserver-spaces="true">The stability perspective was about choosing a school based on factors that you know will stabilize you so that you can do your best work. The variability perspective is about choosing a school based on possibilities that will enable you to become your full self — a flourishing human being. Possibilities mean you do not know for sure that these factors are important to you, but you have a hunch, a feeling, a common thread through your life that makes it look like that you just need to try certain things. Schools that enable you to explore certain unexplored parts of yourself and your interests have the potential to make you mature and a fuller human being with the right breadth and depth of experiences. Schools with low or the wrong kind of variability do bear the risk that at one point in life, you will stop, and feel that you lived a life that was not your own.</span></p>
<p><span data-preserver-spaces="true">But it is not only about experiences per se but also about memories. Even the greatest moments pass, and your happiness will regress back to its mean — but memories will stick with you. Memories that you create will be your own for your entire life. However, if you look back at your most precious memories, it probably is not the time when you hit the library and studied really, really hard for that test. More likely, it is about a unique experience and moments which are emotionally meaningful, and it involves people that you care about. How likely are you going to have these exceptional moments at a school where it is common culture to work really hard on weekends to get in that extra paper for the next conference deadline? How likely are you going to have these exceptional moments at a school where the school is deserted on the weekend, and your advisor tells you &#8220;It is time to submit the final paper draft&#8221; 10:00pm on the deadline night, even though the deadline is 4:00am the next morning?</span></p>
<p><span data-preserver-spaces="true">Academic excellence is great and important, but it is not all that matters in life.</span></p>
<p><span data-preserver-spaces="true">You might have experienced that first-hand in this crazy competitive environment where it is all about coming out at the top to make it to the next stage: Be it PhD admissions, an excellent postdoc position, the superb research scientist job in industry, an assistant professor job, tenure, being recognized as a &#8220;great&#8221; professor and so forth. If you want to turn the hamster&#8217;s wheel — you can turn it all day long just fine. But as you turn the wheel over and over, you might realize, to your dismay, that you never made all the experiences that other people call common life experience.</span></p>
<p><span data-preserver-spaces="true">Maybe you wanted to learn to play the guitar or try that cool sport at the gym, but then you realize the research deadline is in 3 months, and it will be tight — so you better put in those couple of extra hours! Maybe you had the feeling that you might really enjoy doing improv theater with the local group, but somehow you could not squeeze it in between classes and research. Maybe you always wanted to write some blog posts about that one topic that you are passionate about, but how can you justify spending your weekend on a blog post when on Monday you have a meeting with your advisor, and you do not have new results yet? Maybe you wanted to improve your social skills, and you want to ask your coworker to go out and have fun, but then you realize that all your coworkers have no time because they are stressing out about the next research deadline: &#8220;Let&#8217;s do it after the deadline!&#8221; If you find yourself in a trap like this, it might be time that you make choices that offer you a different range of experiences and opportunities.</span></p>
<p><span data-preserver-spaces="true">The critical bit is that a school should not only have opportunities that interest you, but the culture should also be one that encourages the exploration of those opportunities. If you live in the best city in the world and have the best people around you, it does still not really work if your advisor and coworkers expect you to work on weekends and long hours and give you a hard time when you do not. Both opportunity and a culture that supports exploration are needed for a choice to have a good variability of experiences. The variability in experiences and memories that you will get is more like the minimum of those two factors — so also try to figure out how much freedom you have in your research group.</span></p>
<p><span data-preserver-spaces="true">What might a concrete case look like where variability makes sense? Let me tell you a bit about my situation when I was about to start my master&#8217;s degree. During my Bachelor studies, I discovered machine learning and a bit later deep learning. I was hooked and realized that this is something I wanted to do for the rest of my life. However, I also knew if I wanted to get into a good PhD program, I would need to have research experience. The problem was that at the online university I was studying, I could not do research, and since I did not have any credentials at that time, nobody I contacted wanted to work with me on research. So I decided to quit my job, study full time, and do my own research during my online Bachelor studies. The work was relentless and fraught with dead ends and failure, but I did not want to give up. I was highly motivated to succeed, and so I decided to isolate myself and work tirelessly with an intense focus on a research problem that involved parallelization on multiple GPUs. It was a surreal time where sometimes months fly by without any human contact. I eventually succeeded, wrote up a paper, and </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://arxiv.org/abs/1511.04561" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">published it at ICLR2016</span></a><span data-preserver-spaces="true">. This was a big success, but it took its toll. While my peers gained life experience, social experience, got to know who they are, what they like, and enjoy; I just learned how to do research and degenerated into a weird, confused, isolated hermit. On top of that, all my PhD applications were rejected, and my only choices were Master&#8217;s.</span></p>
<p><span data-preserver-spaces="true">I did not want to do a master&#8217;s since I knew I would not learn much. I knew math, I knew computer science, I knew machine learning — it seems a master&#8217;s degree would just be a piece of paper, and I would not benefit from all the experience.</span></p>
<p><span data-preserver-spaces="true">Enter the variability perspective.</span></p>
<p><span data-preserver-spaces="true">When looked from the variability perspective, doing a master&#8217;s is an excellent opportunity to figure out parts of life that are unclear and to catch up on social and life experience. Since I was already pretty good at the things that I need to study for the degree, I could slack off in class and just focus on something outside of class. That is precisely what I did, and the experience was absolutely marvelous, and it made me into the person that I am today.</span></p>
<p><span data-preserver-spaces="true">I chose the University of Lugano for my master&#8217;s degree. It had small, intimate classes perfect for getting to know everyone. The master&#8217;s degree was highly international, and usually, we did not have two people of the same nationality in a class. I overcame my social anxiety, just hung out with people, and developed my social skills. I made my first romantic experiences, which are still very special to me. I also learned that I do not like to hang out with people in bars, who would tell me how drunk they were the last time and what they did in their drunken state, or how nice their vacations were. But then I organized a weekly philosophical evening with two friends where we would talk about philosophy, neuroscience, psychology, research in deep learning, rationality, game theory, altered states of consciousness and how all these things relate and it was always lots of fun and very meaningful to us — from there I knew where I belonged.</span></p>
<p><span data-preserver-spaces="true">In my spare time, I experimented with blog posts. For example, thinking </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">about the future of computing and how it is related to the brain and deep learning</span></a><span data-preserver-spaces="true">. I experimented with writing </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://developer.nvidia.com/blog/author/tdettmers/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">guest blog posts for NVIDIA</span></a><span data-preserver-spaces="true">. I experimented with spontaneous blog posts about what is on my mind. I finished such a blog post in one morning, and I would consider it to be one of </span><a class="_e75a791d-denali-editor-page-rtfLink" href="https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">my best blog posts</span></a><span data-preserver-spaces="true">, despite the little effort I invested in it.</span></p>
<div>
<dl id="attachment_811">
<dt><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?ssl=1"><img src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=1024%2C768&#038;ssl=1" alt="" width="1024" height="768" data-recalc-dims="1" /></a></dt>
<dd>Inline skating through the public park and along the lake in Lugano was a unique experience.</dd>
</dl>
</div>
<p>I got back into inline skating. Inline skating along the park and lake in Lugano was a unique experience. I will remember forever the early mornings with a deserted town, with mist on the mountains while skating with high speed along the still water and beautiful flowers.</p>
<div>
<dl id="attachment_807">
<dt><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20160602_203843861-1.jpg?ssl=1"><img src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20160602_203843861-1.jpg?resize=1024%2C576&#038;ssl=1" alt="" width="1024" height="576" data-recalc-dims="1" /></a></dt>
<dd>View from my friend’s place. I remember sitting at his balcony having BBQ and talking about how to view important problems in life from a perspective of psychology and computer science.</dd>
</dl>
</div>
<p>Also I used my extra time to gather more research experience with an internship at Microsoft Research in the US and a research internship at UCL in London. While already very valuable for my self-development, these experiences have been instrumental to my success in my PhD application.</p>
<div>
<dl id="attachment_808">
<dt><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180704_204856273_HDR-1.jpg?ssl=1"><img src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180704_204856273_HDR-1.jpg?resize=1024%2C768&#038;ssl=1" alt="" width="1024" height="768" data-recalc-dims="1" /></a></dt>
<dd>My father and I were hiking along a trail near Lugano with this view — that experience will always be with me as a vivid memory.</dd>
</dl>
</div>
<p>At the end of it, I can say that I met people from dozens of different countries with very different cultures. I lived in 4 different countries across 2 continents. I learned how to be a good friend and learned which people I belonged to. I learned what my place is in this world. I revived the joy in some activities that I enjoyed in the past and found new activities that I enjoyed. I made unique memories that will always be with me. All of these experiences and memories are at the very heart of the variability perspective. I could not have gotten all of this if I had chosen the program that offered more of the same or if I had submitted to the attitude of &#8220;my master&#8217;s degree is just a piece of paper.&#8221; So with your choice of grad school, you also have the power to choose the range of experiences that will shape you into the beautiful person that you will become.</p>
<h3>Acknowledgements</h3>
<p>This blog post features contributions from Gabriel Ilharco. I would like to thank Hattie Zhou, Nelson Liu, Noah Smith, Gabriel Ilharco, Mitchell Wortsman, Luke Zettlemoyer, Aditya Kusupati, Jungo Kasai, and Ofir Press for their valuable feedback on drafts of this blog post.</p>
<h3>Update History</h3>
<p>2022-03: Added sections on Research Style and Computational Resources.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/">How to Choose Your Grad School</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/feed/</wfw:commentRss>
			<slash:comments>18</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">805</post-id>	</item>
		<item>
		<title>Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</title>
		<link>https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/</link>
					<comments>https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Mon, 07 Sep 2020 16:11:04 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[AMD]]></category>
		<category><![CDATA[CPU]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Matrix Multiplication]]></category>
		<category><![CDATA[Parallel Computing]]></category>
		<category><![CDATA[PCIe Lanes]]></category>
		<category><![CDATA[Sparse Training]]></category>
		<guid isPermaLink="false">http://timdettmers.wordpress.com/?p=4</guid>

					<description><![CDATA[<p>Making the right choice when it comes to buying a GPU is critical. So how do you select the GPU which is right for you? This blog post will delve into that question and will lend you advice which will help you to make choice that is right for you.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/">Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<div class="wp-container-1 wp-block-group eplus-nHlelL"><div class="wp-block-group__inner-container">
<p class="eplus-huy78r">Deep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you.</p>



<span id="more-6"></span>



<p class="eplus-sxlaEI">This blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast, and what is unique about the new NVIDIA RTX 30 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. These form the core of the blog post and the most valuable content.</p>



<p class="eplus-1g9jAa">(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.</p>



<p class="eplus-ErgVnq">(3) If you want to get an in-depth understanding of how GPUs and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics.</p>



<p class="eplus-RhAGvU">I will head each major section with a small summary, which might help you to decide if you want to read the section or not.</p>





<h2 class="eplus-dYbLao"><strong>Overview</strong></h2>



<p class="eplus-xVYlmp">This blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. Then I will make theoretical estimates for GPU performance and align them with some <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">marketing benchmarks</a> from NVIDIA to get reliable, unbiased performance data. I discuss the unique features of the new NVIDIA RTX 30 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for 1-2, 4, 8 GPU setups, and GPU clusters. After that follows a Q&amp;A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others.&nbsp;</p>



<h2 class="eplus-TzWEWa">How do GPUs work?</h2>



<p class="eplus-YDMSkp">If you use GPUs frequently, it is useful to understand how they work. This knowledge will come in handy in understanding why GPUs might be slow in some cases and fast in others. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer:</p>



<span class='quora-content-embed' data-name='Why-are-GPUs-well-suited-to-deep-learning/answer/Tim-Dettmers-1'>Read <a class='quora-content-link' data-width='560' data-height='260' href='https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning/answer/Tim-Dettmers-1' data-type='answer' data-id='21379913' data-key='bbb3732f88834d75dfa98d816eb9eccd' load-full-answer='False' data-embed='jqubkoa'><a href='https://www.quora.com/Tim-Dettmers-1'>Tim Dettmers</a>&#039; <a href="/Why-are-GPUs-well-suited-to-deep-learning?top_ans=21379913" class="broken_link">answer</a> to <a href="/Why-are-GPUs-well-suited-to-deep-learning" ref="canonical" class="broken_link"><span class="rendered_qtext">Why are GPUs well-suited to deep learning?</span></a></a> on <a href='https://www.quora.com'>Quora</a><script type="text/javascript" src="https://www.quora.com/widgets/content"></script></span>



<p class="eplus-bfPm9F">This is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another.</p>



<h2 class="eplus-9AkYN1">The Most Important GPU Specs for Deep Learning Processing Speed</h2>



<p class="eplus-XflNML">This section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself.</p>



<h3 class="eplus-YyGRdI">Tensor Cores</h3>



<p class="eplus-Aoe0Cj">Summary:</p>



<ul class="eplus-nkeP06"><li>Tensor Cores reduce the used cycles needed for calculating multiply and addition operations, 16-fold — in my example, for a 32&#215;32 matrix, from 128 cycles to 8 cycles.</li><li>Tensor Cores reduce the reliance on repetitive shared memory access, thus saving additional cycles for memory access.</li><li>Tensor Cores are so fast that computation is no longer a bottleneck. The only bottleneck is getting data to the Tensor Cores.</li></ul>



<p class="eplus-JKvKVO">There are now enough cheap GPUs that almost everyone can afford a GPU with Tensor Cores. That is why I only recommend GPUs with Tensor Cores. It is useful to understand how they work to appreciate the importance of these computational units specialized for matrix multiplication. Here I will show you a simple example of A*B=C matrix multiplication, where all matrices have a size of 32&#215;32, what a computational pattern looks like with and without Tensor Cores. This is a simplified example, and not the exact way how a high performing matrix multiplication kernel would be written, but it has all the basics. A CUDA programmer would take this as a first “draft” and then optimize it step-by-step with concepts like double buffering, register optimization, occupancy optimization, instruction-level parallelism, and many others, which I will not discuss at this point. </p>



<p class="eplus-JQnNxT">To understand this example fully, you have to understand the concepts of cycles. If a processor runs at 1GHz, it can do 10^9 cycles per second. Each cycle represents an opportunity for computation. However, most of the time, operations take longer than one cycle. Thus it creates a pipeline where for one operation to start, it needs to wait for the number of cycles of time it takes for the previous operation to finish. This is also called the latency of the operation.</p>



<p class="eplus-Nh32jj">Here are some important cycle timings or latencies for operations:</p>



<ul class="eplus-yOHNRh"><li>Global memory access (up to 48GB): ~200 cycles</li><li>Shared memory access (up to 164 kb per Streaming Multiprocessor): ~20 cycles</li><li>Fused multiplication and addition (FFMA): 4 cycles</li><li>Tensor Core matrix multiply: 1 cycle</li></ul>



<p class="eplus-xmhx78">Furthermore, you should know that the smallest units of threads on a GPU is a pack of 32 threads — this is called a warp. Warps usually operate in a synchronous pattern — threads within a warp have to wait for each other. All memory operations on the GPU are optimized for warps. For example, loading from global memory happens at a granularity of 32*4 bytes, exactly 32 floats, exactly one float for each thread in a warp. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers/shared memory/Tensor Core resources per warp.</p>



<p class="eplus-5GehcT">For both of the following examples, we assume we have the same computational resources. For this small example of a 32&#215;32 matrix multiply, we use 8 SMs (about 10% of an RTX 3090) and 8 warps per SM.</p>



<h4 class="eplus-Lj4Hjc">Matrix multiplication without Tensor Cores</h4>



<p class="eplus-eBAn6w">If we want to do an A*B=C matrix multiply, where each matrix is of size 32&#215;32, then we want to load memory that we repeatedly access into shared memory because its latency is about ten times lower (200 cycles vs 20 cycles). A memory block in shared memory is often referred to as a memory tile or just a tile. Loading two 32&#215;32 floats into a shared memory tile can happen in parallel by using 2*32 warps. We have 8 SMs with 8 warps each, so due to parallelization, we only need to do a single sequential load from global to shared memory, which takes 200 cycles.</p>



<p class="eplus-bSRgVT">To do the matrix multiplication, we now need to load a vector of 32 numbers from shared memory A and shared memory B and perform a fused multiply-and-accumulate (FFMA). Then store the outputs in registers C. We divide the work so that each SM does 8x dot products (32&#215;32) to compute 8 outputs of C. Why this is exactly 8 (4 in older algorithms) is very technical. I recommend Scott Gray’s blog post on <a href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">matrix multiplication</a> to understand this. This means we have 8x shared memory access at the cost of 20 cycles each and 8 FFMA operations (32 in parallel), which cost 4 cycles each. In total, we thus have a cost of:</p>



<p class="eplus-UJKQje">200 cycles (global memory) + 8*20 cycles (shared memory) + 8*4 cycles (FFMA) = 392 cycles</p>



<p class="eplus-G6zbco">Let&#8217;s look at the cycle cost of using Tensor Cores.</p>



<h4 class="eplus-dS0Vu0">Matrix multiplication with Tensor Cores</h4>



<p class="eplus-8bWcQj">With Tensor Cores, we can perform a 4&#215;4 matrix multiplication in one cycle. To do that, we first need to get memory into the Tensor Core. Similarly to the above, we need to read from global memory (200 cycles) and store in shared memory. To do a 32&#215;32 matrix multiply, we need to do 8&#215;8=64 Tensor Cores operations. A single SM has 8 Tensor Cores. So with 8 SMs, we have 64 Tensor Cores — just the number that we need! We can transfer the data from shared memory to the Tensor Cores with 1 memory transfers (20 cycles) and then do those 64 parallel Tensor Core operations (1 cycle). This means the total cost for Tensor Cores matrix multiplication, in this case, is:</p>



<p class="eplus-ZzxYmo">200 cycles (global memory) + 20 cycles (shared memory) + 1 cycle (Tensor Core) = 221 cycles.</p>



<p class="eplus-KVQYN0">Thus we reduce the matrix multiplication cost significantly from 392 cycles to 221 cycles via Tensor Cores. In this simplified case, the Tensor Cores reduced the cost of both shared memory access and FFMA operations.&nbsp;</p>



<p class="eplus-G9pv5H">While this example roughly follows the sequence of computational steps for both with and without Tensor Cores, please note that this is a very simplified example. Real cases of matrix multiplication involve much larger shared memory tiles and slightly different computational patterns.</p>



<p class="eplus-2785xF">However, I believe from this example, it is also clear why the next attribute, memory bandwidth, is so crucial for Tensor-Core-equipped GPUs. Since global memory is the most considerable portion of cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).</p>


<div class="crp_related   crp_related_block  mobile-only crp-rounded-thumbs"><ul><li><a href="https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/"     class="crp_link post-805"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="How to Choose Your Grad School" title="How to Choose Your Grad School" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">How to Choose Your Grad School</span></a></li><li><a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"     class="crp_link post-1093"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="LLM.int8() and Emergent Features" title="LLM.int8() and Emergent Features" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">LLM.int8() and Emergent Features</span></a></li></ul><div class="crp_clear"></div></div>


<h3 class="eplus-7uVl7z">Memory Bandwidth</h3>



<p class="eplus-Sc9M0X">From the previous section, we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during BERT Large training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 30%, meaning that 70% of the time, Tensor Cores are idle.</p>



<p class="eplus-RsGhaC">This means that when comparing two GPUs with Tensor Cores, one of the single best indicators for each GPU’s performance is their memory bandwidth. For example, The A100 GPU has 1,555 GB/s memory bandwidth vs the 900 GB/s of the V100. As such, a basic estimate of speedup of an A100 vs V100 is 1555/900 = 1.73x.</p>



<h3 class="eplus-Nd29of">Shared Memory / L1 Cache Size / Registers</h3>



<p class="eplus-Qs2qZL">Since memory transfers to the Tensor Cores are the limiting factor in performance, we are looking for other GPU attributes that enable faster memory transfer to Tensor Cores. Shared memory, L1 Cache, and amount of registers used are all related. To understand how a memory hierarchy enables faster memory transfers, it helps to understand how matrix multiplication is performed on a GPU.</p>



<p class="eplus-ecxtvX">To perform matrix multiplication, we exploit the memory hierarchy of a GPU that goes from slow global memory to fast local shared memory, to lightning-fast registers. However, the faster the memory, the smaller it is. As such, we need to separate the matrix into smaller matrices. We perform matrix multiplication across these smaller tiles in local shared memory that is fast and close to the streaming multiprocessor (SM) — the equivalent of a CPU core. With Tensor Cores, we go a step further: We take each tile and load a part of these tiles into Tensor Cores. A matrix memory tile in shared memory is ~10-50x faster than the global GPU memory, whereas the Tensor Cores’ registers are ~200x faster than the global GPU memory.&nbsp;</p>



<p class="eplus-ifo0Kt">Having larger tiles means we can reuse more memory. I wrote about this in detail in my <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPU vs GPU</a> blog post. In fact, you can see TPUs as having very, very, large tiles for each Tensor Core. As such, TPUs can reuse much more memory with each transfer from global memory, which makes them a little bit more efficient at matrix multiplications than GPUs.</p>



<p class="eplus-h6CECa">Each tile size is determined by how much memory we have per streaming multiprocessor (SM) — the equivalent to a “CPU core” on a GPU. We have the following shared memory sizes on the following architectures:</p>



<ul class="eplus-CmF7pW"><li>Volta: 96kb shared memory / 32 kb L1</li><li>Turing: 64kb shared memory / 32 kb L1</li><li>Ampere: 164 kb shared memory / 32 kb L1</li></ul>



<p class="eplus-mGR5cO">We see that Ampere has a much larger shared memory allowing for larger tile sizes, which reduces global memory access. Thus, Ampere can make better use of the overall memory bandwidth on the GPU memory. This improves performance by roughly 2-5%. The performance boost is particularly pronounced for huge matrices.</p>



<p class="eplus-AYgqUV">The Ampere Tensor Cores have another advantage in that they share more data between threads. This reduces the register usage. Registers are limited to 64k per streaming multiprocessor (SM) or 255 per thread. Comparing the Volta vs Ampere Tensor Core, the Ampere Tensor Core uses 3x fewer registers, allowing for more tensor cores to be active for each shared memory tile. In other words, we can feed 3x as many Tensor Cores with the same amount of registers. However, since bandwidth is still the bottleneck, you will only see tiny increases in actual vs theoretical TFLOPS. The new Tensor Cores improve performance by roughly 1-3%.</p>



<p class="eplus-McZkib">Overall, you can see that the Ampere architecture is optimized to make the available memory bandwidth more effective by using an improved memory hierarchy: from global memory to shared memory tiles, to register tiles for Tensor Cores.</p>



<h2 class="eplus-Uq5SHY">Estimating Ampere Deep Learning Performance</h2>



<p class="eplus-faj04W">Summary:</p>



<ul class="eplus-bbu6Q3"><li>Theoretical estimates based on memory bandwidth and the improved memory hierarchy of Ampere GPUs predict a speedup of 1.78x to 1.87x.</li><li>NVIDIA provides accuracy benchmark data of Tesla A100 and V100 GPUs. These data are biased for marketing purposes, but it is possible to build a debiased model of these data.</li><li>Debiased benchmark data suggests that the Tesla A100 compared to the V100 is 1.70x faster for NLP and 1.45x faster for computer vision.</li></ul>



<p class="eplus-gsZ188">This section is for those who want to understand the more technical details of how I derive the performance estimates for Ampere GPUs. If you do not care about these technical aspects, it is safe to skip this section.</p>



<h3 class="eplus-IWxMN8">Theoretical Ampere Speed Estimates</h3>



<p class="eplus-ikPNgj">Putting together the reasoning above, we would expect the difference between two Tensor-Core-equipped GPU architectures to be mostly about memory bandwidth. Additional benefits come from more shared memory / L1 cache and better register usage in Tensor Cores.</p>



<p class="eplus-VFFzZC">If we take the Tesla A100 GPU bandwidth vs Tesla V100 bandwidth, we get a speedup of 1555/900 = 1.73x. Additionally, I would expect a 2-5% speedup from the larger shared memory and 1-3% from the improved Tensor Cores. This puts the speedup range between 1.78x and 1.87x. With similar reasoning, you would be able to estimate the speedup of other Ampere series GPUs compared to a Tesla V100.&nbsp;</p>



<h3 class="eplus-t3NVr4">Practical Ampere Speed Estimates</h3>



<p class="eplus-H6ehT0">Suppose we have an estimate for one GPU of a GPU-architecture like Ampere, Turing, or Volta. It is easy to extrapolate these results to other GPUs from the same architecture/series. Luckily, NVIDIA already <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">benchmarked the A100 vs V100</a> across a wide range of computer vision and natural language understanding tasks. Unfortunately, NVIDIA made sure that these numbers are not directly comparable by using different batch sizes and the number of GPUs whenever possible to favor results for the A100. So in a sense, the benchmark numbers are partially honest, partially marketing numbers. In general, you could argue that using larger batch sizes is fair, as the A100 has more memory. Still, to compare GPU architectures, we should evaluate unbiased memory performance with the same batch size.</p>



<p class="eplus-3xKGXX">To get an unbiased estimate, we can scale the V100 and A100 results in two ways: (1) account for the differences in batch size, (2) account for the differences in using 1 vs 8 GPUs. We are lucky that we can find such an estimate for both biases in the data that NVIDIA provides.&nbsp;</p>



<p class="eplus-LO0cd2">Doubling the batch size increases throughput in terms of images/s (CNNs) by 13.6%. I benchmarked the same problem for transformers on my RTX Titan and found, surprisingly, the very same result: 13.5% — it appears that this is a robust estimate.</p>



<p class="eplus-obwQUJ">As we parallelize networks across more and more GPUs, we lose performance due to some networking overhead. The A100 8x GPU system has better networking (NVLink 3.0) than the V100 8x GPU system (NVLink 2.0) — this is another confounding factor. Looking directly at the data from NVIDIA, we can find that for CNNs, a system with 8x A100 has a 5% lower overhead than a system of 8x V100. This means if going from 1x A100 to 8x A100 gives you a speedup of, say, 7.00x, then going from 1x V100 to 8x V100 only gives you a speedup of 6.67x.&nbsp; For transformers, the figure is 7%.&nbsp;</p>



<p class="eplus-M8lGMx">Using these figures, we can estimate the speedup for a few specific deep learning architectures from the direct data that NVIDIA provides. The Tesla A100 offers the following speedup over the Tesla V100:</p>



<ul class="eplus-2N544K"><li>SE-ResNeXt101: 1.43x</li><li>Masked-R-CNN: 1.47x</li><li>Transformer (12 layer, Machine Translation, WMT14 en-de): 1.70x</li></ul>



<p class="eplus-hLEvfM">Thus, the figures are a bit lower than the theoretical estimate for computer vision. This might be due to smaller tensor dimensions, overhead from operations that are needed to prepare the matrix multiplication like img2col or Fast Fourier Transform (FFT), or operations that cannot saturate the GPU (final layers are often relatively small). It could also be artifacts of the specific architectures (grouped convolution).</p>



<p class="eplus-K95gae">The practical transformer estimate is very close to the theoretical estimate. This is probably because algorithms for huge matrices are very straightforward. I will use these practical estimates to calculate the cost efficiency of GPUs.</p>



<h3 class="eplus-CUQjZa">Possible Biases in Estimates</h3>



<p class="eplus-UQWjwg">The estimates above are for A100 vs V100. In the past, NVIDIA sneaked unannounced performance degradations into the “gaming” RTX GPUs: (1) Decreased Tensor Core utilization, (2) gaming fans for cooling, (3) disabled peer-to-peer GPU transfers. It might be possible that there are unannounced performance degradations in the RTX 30 series compared to the full Ampere A100. </p>



<p class="eplus-IQX3ZD">As of now, one of these degradations was found: Tensor Core performance was decreased so that RTX 30 series GPUs are not as good as Quadro cards for deep learning purposes. This was also done for the RTX 20 series, so it is nothing new, but this time it was also done for the Titan equivalent card, the RTX 3090. The RTX Titan did not have performance degradation enabled.</p>



<p class="eplus-l9YowP">I will update this blog post as information about further unannounced performance degradation becomes available.</p>



<h2 class="eplus-hICahu"><strong>Additional Considerations for Ampere / RTX 30 Series</strong></h2>



<p class="eplus-UblFcg">Summary:</p>



<ul class="eplus-iFdlRK"><li>Ampere allows for sparse network training, which accelerates training by a factor of up to 2x.</li><li>Sparse network training is still rarely used but will make Ampere future-proof.</li><li>Ampere has new low-precision data types, which makes using low-precision much easy, but not necessarily faster than for previous GPUs.</li><li>The new fan design is excellent if you have space between GPUs, but it is unclear if multiple GPUs with no space in-between them will be efficiently cooled.</li><li>3-Slot design of the RTX 3090 makes 4x GPU builds problematic. Possible solutions are 2-slot variants or the use of PCIe extenders.</li><li>4x RTX 3090 will need more power than any standard power supply unit on the market can provide right now.&nbsp;</li></ul>



<p class="eplus-0zpF2c">The new NVIDIA Ampere RTX 30 series has additional benefits over the NVIDIA Turing RTX 20 series, such as sparse network training and inference. Other features, such as the new data types, should be seen more as an ease-of-use-feature as they provide the same performance boost as Turing does but without any extra programming required. </p>



<h3 class="eplus-2ZdClk">Sparse Network Training</h3>



<p class="eplus-vRUUJh">Ampere allows for fine-grained structure automatic sparse matrix multiplication at dense speeds. How does this work? Take a weight matrix and slice it into pieces of 4 elements. Now imagine 2 elements of these 4 to be zero. Figure 1 shows how this could look like. </p>



<div class="wp-block-image eplus-IiM2y7"><figure class="aligncenter size-large is-resized"><img data-attachment-id="935" data-permalink="https://timdettmers.com/sparse_matrix_ampere/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?fit=321%2C387&amp;ssl=1" data-orig-size="321,387" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Ampere Sparse Matrix Multiplication Tensor Cores Matrix" data-image-description="&lt;p&gt;Figure X: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. Figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on  &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Accelerating Sparsity in the NVIDIA Ampere Architecture&lt;/a&gt; by the courtesy of NVIDIA.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. Figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on  &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Accelerating Sparsity in the NVIDIA Ampere Architecture&lt;/a&gt; by the courtesy of NVIDIA.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?fit=249%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?fit=321%2C387&amp;ssl=1" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?resize=258%2C311&#038;ssl=1" alt="Figure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool's GTC 2020 presentation on  Accelerating Sparsity in the NVIDIA Ampere Architecture by the courtesy of NVIDIA." class="wp-image-935" width="258" height="311" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?w=321&amp;ssl=1 321w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matrix_ampere.png?resize=249%2C300&amp;ssl=1 249w" sizes="(max-width: 258px) 100vw, 258px" data-recalc-dims="1" /><figcaption>Figure 1: Structure supported by the sparse matrix multiplication feature in Ampere GPUs. The figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf" rel="noreferrer noopener" target="_blank">Accelerating Sparsity in the NVIDIA Ampere Architecture</a> by the courtesy of NVIDIA.</figcaption></figure></div>



<p class="eplus-JGsMdm">When you multiply this sparse weight matrix with some dense inputs, the sparse matrix tensor core feature in Ampere automatically compresses the sparse matrix to a dense representation that is half the size as can be seen in Figure 2. After this compression, the densely compressed matrix tile is fed into the tensor core which computes a matrix multiplication of twice the usual size. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.</p>



<figure class="wp-block-image size-large eplus-O3WVqX"><img data-attachment-id="934" data-permalink="https://timdettmers.com/sparse_matmul/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=1055%2C638&amp;ssl=1" data-orig-size="1055,638" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sparse Matrix Multiplication in Ampere" data-image-description="&lt;p&gt;Figure X: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. Figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on  &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Accelerating Sparsity in the NVIDIA Ampere Architecture&lt;/a&gt; by the courtesy of NVIDIA.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. Figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on  &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Accelerating Sparsity in the NVIDIA Ampere Architecture&lt;/a&gt; by the courtesy of NVIDIA.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=300%2C181&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=1024%2C619&amp;ssl=1" width="1024" height="619" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&#038;ssl=1" alt="Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. " class="wp-image-934" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=300%2C181&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=768%2C464&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?w=1055&amp;ssl=1 1055w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /><figcaption>Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. The figure is taken from Jeff Pool&#8217;s GTC 2020 presentation on  <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf" rel="noopener noreferrer" target="_blank">Accelerating Sparsity in the NVIDIA Ampere Architecture</a> by the courtesy of NVIDIA.</figcaption></figure>



<p class="eplus-dCX1Ah">I was working on <a href="https://arxiv.org/abs/1907.04840">sparse network training</a> in my research and I also wrote a <a href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">blog post about sparse training</a>. One criticism of my work was that “You reduce the FLOPS required for the network, but it does not yield speedups because GPUs cannot do fast sparse matrix multiplication.” Well, with the addition of the sparse matrix multiplication feature for Tensor Cores, my algorithm, or <a href="https://arxiv.org/abs/2002.03231" rel="nofollow">other</a> <a href="https://arxiv.org/abs/2002.07376" rel="nofollow">sparse</a> <a href="https://arxiv.org/abs/1911.11134" rel="nofollow">training</a> <a href="https://arxiv.org/abs/1902.05967" rel="nofollow">algorithms</a>, now actually provide speedups of up to 2x during training.</p>



<figure class="wp-block-image size-large eplus-uspqxA"><a href="https://arxiv.org/abs/1907.04840"><img data-attachment-id="779" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/sparse_momentum/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1096%2C528&amp;ssl=1" data-orig-size="1096,528" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sparse Momentum Dettmers &#038; Zettlemoyer 2019" data-image-description="&lt;p&gt;Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=300%2C145&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1024%2C493&amp;ssl=1" width="1024" height="493" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&#038;ssl=1" alt="Figure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post." class="wp-image-779" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=768%2C370&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?w=1096&amp;ssl=1 1096w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption>Figure 3: The <a href="https://arxiv.org/abs/1907.04840">sparse training algorithm</a> that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my <a href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">sparse training blog post</a>.</figcaption></figure>



<p class="eplus-9YQAyM">While this feature is still experimental and training sparse networks are not commonplace yet, having this feature on your GPU means you are ready for the future of sparse training.</p>



<h3 class="eplus-uvNoM4">Low-precision Computation</h3>



<p class="eplus-AgG4J1">In my work, I’ve previously shown that new data types can improve stability during <a href="https://arxiv.org/abs/1511.04561">low-precision backpropagation</a>. </p>



<figure class="wp-block-image size-large eplus-4aBkKy"><img data-attachment-id="941" data-permalink="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/8-bit_data_types/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=869%2C268&amp;ssl=1" data-orig-size="869,268" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="8-bit_data_types" data-image-description="&lt;p&gt;Figure X: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent large numbers and small numbers with high precision.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent large numbers and small numbers with high precision.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=300%2C93&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=869%2C268&amp;ssl=1" width="869" height="268" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=869%2C268&#038;ssl=1" alt="Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision." class="wp-image-941" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?w=869&amp;ssl=1 869w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=300%2C93&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=768%2C237&amp;ssl=1 768w" sizes="(max-width: 869px) 100vw, 869px" data-recalc-dims="1" /><figcaption>Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.</figcaption></figure>



<p class="eplus-3DDVFs">Currently, if you want to have stable backpropagation with 16-bit floating-point numbers (FP16), the big problem is that ordinary FP16 data types only support numbers in the range [-65,504, 65,504]. If your gradient slips past this range, your gradients explode into NaN values. To prevent this during FP16 training, we usually perform loss scaling where you multiply the loss by a small number before backpropagating to prevent this gradient explosion.&nbsp;</p>



<p class="eplus-WQx9qD">The Brain Float 16 format (BF16) uses more bits for the exponent such that the range of possible numbers is the same as for FP32: [-3*10^38, 3*10^38]. BF16 has less precision, that is significant digits, but gradient precision is not that important for learning. So what BF16 does is that you no longer need to do any loss scaling or worry about the gradient blowing up quickly. As such, we should see an increase in training stability by using the BF16 format as a slight loss of precision.</p>



<p class="eplus-NLJqlG">What this means for you: With BF16 precision, training might be more stable than with FP16 precision while providing the same speedups. With TF32 precision, you get near FP32 stability while giving the speedups close to FP16. The good thing is, to use these data types, you can just replace FP32 with TF32 and FP16 with BF16 — no code changes required!</p>



<p class="eplus-S1SD30">Overall, though, these new data types can be seen as lazy data types in the sense that you could have gotten all the benefits with the old data types with some additional programming efforts (proper loss scaling, initialization, normalization, using Apex). As such, these data types do not provide speedups but rather improve ease of use of low precision for training.</p>


<div class="crp_related   crp_related_block  mobile-only crp-rounded-thumbs"><ul><li><a href="https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/"     class="crp_link post-805"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="How to Choose Your Grad School" title="How to Choose Your Grad School" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">How to Choose Your Grad School</span></a></li><li><a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"     class="crp_link post-1093"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="LLM.int8() and Emergent Features" title="LLM.int8() and Emergent Features" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">LLM.int8() and Emergent Features</span></a></li></ul><div class="crp_clear"></div></div>


<h3 class="eplus-UXUbWi">New Fan Design / Thermal Issues</h3>



<p class="eplus-yw9sMm">The new fan design for the RTX 30 series features both a blower fan and a push/pull fan. The design is ingenious and will be very effective if you have space between GPUs. So if you have 2 GPUs and one slot space between them (+3 PCIe slots), you will be fine, and there will be no cooling issues. However, it is unclear how the GPUs will perform if you have them stacked next to each other in a setup with more than 2 GPUs. The blower fan will be able to exhaust through the bracket away from the other GPUs, but it is impossible to tell how well that works since the blower fan is of a different design than before. 

So my recommendation: If you want to buy 1 GPU or 2 GPUs in a 4 PCIe slot setup, then there should be no issues. However, if you&#8217;re going to use 3-4 RTX 30 GPUs next to each other, I would wait for thermal performance reports to know if you need different GPU coolers, PCIe extenders, or other solutions. I will update the blog post with this information as it becomes available.</p>



<p class="eplus-H9FLpS">To overcome thermal issues, water cooling will provide a solution in any case. Many vendors offer water cooling blocks for RTX 3080/RTX 3090 cards, which will keep them cool even in a 4x GPU setup. Beware of all-in-one water cooling solution for GPUs if you want to run a 4x GPU setup, though it is difficult to spread out the radiators in most desktop cases.</p>



<p class="eplus-CTvlva">Another solution to the cooling problem is to buy PCIe extenders and spread the GPUs within the case. This is very effective, and other fellow PhD students at the University of Washington and I use this setup with great success. It does not look pretty, but it keeps your GPUs cool! It can also help if you do not have enough space to spread the GPUs. For example, if you can find the space within a desktop computer case, it might be possible to buy standard 3-slot-width RTX 3090 and spread them with PCIe extenders within the case. With this, you might solve both the space issue and cooling issue for a 4x RTX 3090 setup with a single simple solution.</p>



<div class="wp-block-image eplus-dDY0Kb"><figure class="aligncenter size-large"><img data-attachment-id="861" data-permalink="https://timdettmers.com/4x_rtx2080ti_desktop_extenders/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=1920%2C2560&amp;ssl=1" data-orig-size="1920,2560" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.9&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Redmi Note 5&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1557156443&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.94&quot;,&quot;iso&quot;:&quot;1250&quot;,&quot;shutter_speed&quot;:&quot;0.05&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="4x_RTX2080Ti_desktop_extenders" data-image-description="&lt;p&gt;4x GPUs with PCIe extenders&lt;/p&gt;
" data-image-caption="&lt;p&gt;4x GPUs with PCIe extenders&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=225%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=768%2C1024&amp;ssl=1" width="768" height="1024" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders.jpg?resize=768%2C1024&#038;ssl=1" alt="Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs." class="wp-image-861" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1152%2C1536&amp;ssl=1 1152w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1536%2C2048&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?w=1920&amp;ssl=1 1920w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1" /><figcaption>Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.</figcaption></figure></div>



<h3 class="eplus-JbBDhp">3-slot Design and Power Issues</h3>



<p class="eplus-DovhM1">The RTX 3090 is a 3-slot GPU, so one will not be able to use it in a 4x setup with the default fan design from NVIDIA. This is kind of justified because it runs at 350W TDP, and it will be difficult to cool in a multi-GPU 2-slot setting. The RTX 3080 is only slightly better at 320W TDP, and cooling a 4x RTX 3080 setup will also be very difficult.</p>



<p class="eplus-pfi9QE">It is also difficult to power a 4x 350W = 1400W system in the 4x RTX 3090 case. Power supply units (PSUs) of 1600W are readily available, but having only 200W to power the <a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">CPU and motherboard</a> can be too tight. The components’ maximum power is only used if the components are fully utilized, and in deep learning, the CPU is usually only under weak load. With that, a 1600W PSU might work quite well with a 4x RTX 3080 build, but for a 4x RTX 3090 build, it is better to look for high wattage PSUs (+1700W). Some of my followers have had great success with cryptomining PSUs — have a look in the comment section for more info about that. Otherwise, it is important to note that not all outlets support PSUs above 1600W, especially in the US. This is the reason why in the US, there is currently not a standard desktop PSU above 1600W on the market. If you get a server or cryptomining PSUs, beware of the form factor — make sure it fits into your computer case.</p>



<h3 class="eplus-wvweIn">Power Limiting: An Elegant Solution to Solve the Power Problem?</h3>



<p class="eplus-eq3oCc">It is possible to set a power limit on your GPUs. So you would be able to programmatically set the power limit of an RTX 3090 to 300W instead of their standard 350W. In a 4x GPU system, that is a saving of 200W, which might just be enough to build a 4x RTX 3090 system with a 1600W PSU feasible. It also helps to keep the GPUs cool. So setting a power limit can solve the two major problems of a 4x RTX 3080 or 4x RTX 3090 setups, cooling, and power, at the same time. For a 4x setup, you still need effective blower GPUs (and the standard design may prove adequate for this), but this resolves the PSU problem.</p>



<figure class="wp-block-image size-large eplus-uDERbv"><img data-attachment-id="933" data-permalink="https://timdettmers.com/power_limit_nvidia_smi/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=1187%2C1195&amp;ssl=1" data-orig-size="1187,1195" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Power Limit Cooling Effect NVIDIA SMI" data-image-description="&lt;p&gt;Figure X: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=298%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=1017%2C1024&amp;ssl=1" width="1017" height="1024" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&#038;ssl=1" alt="Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent." class="wp-image-933" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&amp;ssl=1 1017w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=298%2C300&amp;ssl=1 298w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=768%2C773&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?w=1187&amp;ssl=1 1187w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /><figcaption>Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.</figcaption></figure>



<p class="eplus-dIRoAn">You might ask, “Doesn’t this slow down the GPU?” Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7. </p>



<figure class="wp-block-image size-large eplus-WIGOOF"><img data-attachment-id="939" data-permalink="https://timdettmers.com/rtx-2080-ti-slowdown-vs-power-limit/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" data-orig-size="853,703" data-comments-opened="1" data-image-meta="[]" data-image-title="RTX 2080 Ti Slowdown vs Power Limit" data-image-description="&lt;p&gt;Figure 6: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure 6: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).&lt;/p&gt;
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" width="853" height="703" src="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" alt="Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer)." class="wp-image-939"/><figcaption>Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).</figcaption></figure>



<p class="eplus-FATkvQ">As we can see, setting the power limit does not seriously affect performance. Limiting the power by 50W — more than enough to handle 4x RTX 3090 — decreases performance by only 7%.</p>



<h2 class="eplus-A4G25G">GPU Deep Learning Performance</h2>



<p class="eplus-xyHWc1">The following benchmark includes not only the Tesla A100 vs Tesla V100 benchmarks but I build a model that fits those data and four different benchmarks based on the Titan V,  Titan RTX, RTX 2080 Ti, and RTX 2080.[<a href="https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/">1</a>,<a href="https://bizon-tech.com/blog/gtx1080ti-titan-rtx-2080-ti-deep-learning-benchmarks">2</a>,<a href="https://bizon-tech.com/blog/best-gpu-for-deep-learning-rtx-2080-ti-vs-titan-rtx-vs-rtx-8000-vs-rtx-6000">3</a>,<a href="https://www.pugetsystems.com/labs/hpc/NVIDIA-RTX-2080-Ti-vs-2080-vs-1080-Ti-vs-Titan-V-TensorFlow-Performance-with-CUDA-10-0-1247/#:~:text=Yes%2C%20they%20are%20great!,their%20astounding%20Ray%2DTracing%20performance.">4</a>] In an update, I also factored in the recently discovered performance degradation in RTX 30 series GPUs. And since I wrote this blog post, we now also have the first <a href="https://www.evolution.ai/post/benchmarking-deep-learning-workloads-with-tensorflow-on-the-nvidia-geforce-rtx-3090">solid benchmark</a> for computer vision which confirms my numbers.</p>



<p class="eplus-e2Q3z6">Beyond this, I scaled intermediate cards like the RTX 2070, RTX 2060, or the Quadro RTX 6000 &amp; 8000 cards via interpolating between those data points of benchmark data. Usually, within an architecture GPUs scale quite linearly with respect to streaming multiprocessors and bandwidth, and my within-architecture model is based on that.</p>



<p class="eplus-QGap1m">I collected only benchmark data for mixed-precision FP16 training since I believe there is no good reason why one should use FP32 training.</p>



<p class="eplus-E0zXIr"></p>



<figure class="wp-block-image size-large eplus-BfX2so"><img data-attachment-id="1025" data-permalink="https://timdettmers.com/normalized-gpu-performance-ampere-2/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-GPU-Performance-Ampere-1.svg" data-orig-size="1730,1685" data-comments-opened="1" data-image-meta="[]" data-image-title="Normalized GPU Performance Ampere" data-image-description="&lt;p&gt;Figure 8: Normalized GPU deep learning performance relative to an RTX 2080 Ti.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure 8: Normalized GPU deep learning performance relative to an RTX 2080 Ti.&lt;/p&gt;
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-GPU-Performance-Ampere-1.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-GPU-Performance-Ampere-1.svg" width="1730" height="1685" src="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-GPU-Performance-Ampere-1.svg" alt="Normalized GPU Performance Ampere" class="wp-image-1025"/><figcaption>Figure 8: Normalized GPU deep learning performance relative to an RTX 2080 Ti.</figcaption></figure>



<p class="eplus-gzQ71E">Compared to an RTX 2080 Ti, the RTX 3090 yields a speedup of 1.41x for convolutional networks and 1.35x for transformers while having a 15% higher release price. Thus the Ampere RTX 30 yields a substantial improvement over the Turing RTX 20 series in raw performance and is also cost-effective (if you do not have to upgrade your power supply and so forth).</p>



<h2 class="eplus-1SzBFE">GPU Deep Learning Performance per Dollar</h2>



<p class="eplus-FUgaC5">What is the GPU that gives you the best bang for your buck? It depends on the cost of the overall system. If you have an expensive system, it makes sense to invest in more expensive GPUs.&nbsp;</p>



<p class="eplus-rzYDNr">Here I have <a href="https://pcpartpicker.com/user/tim_dettmers/saved/#view=fwqbK8" class="broken_link">three PCIe 3.0 builds</a>, which I use as base costs for 2/4 GPU systems. I take these base costs and add the GPU costs on top of it. The GPU costs are the mean of the GPU’s Amazon and eBay costs. For the new Ampere GPUs, I use just the release price. Together with the performance values from above, this yields performance per dollar values for these systems of GPUs. For the 8-GPU system, I use a Supermicro barebone — the industry standard for RTX servers — as baseline cost. Note that these bar charts do not account for memory requirements. You should think about your memory requirements first and then look for the best option in the chart. Here some rough guidelines for memory:</p>



<ul class="eplus-NSksPW"><li>Using pretrained transformers; training small transformer from scratch&gt;= 11GB</li><li>Training large transformer or convolutional nets in research / production: &gt;= 24 GB</li><li>Prototyping neural networks (either transformer or convolutional nets) &gt;= 10 GB</li><li>Kaggle competitions &gt;= 8 GB</li><li>Applying computer vision &gt;= 10GB</li><li>Neural networks for video: 24 GB</li><li>Reinforcement learning =10GB + a strong <a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">deep learning desktop</a> the largest Threadripper or EPYC CPU you can afford.</li></ul>



<figure class="wp-block-image size-large eplus-pa6F5m"><img data-attachment-id="1024" data-permalink="https://timdettmers.com/normalized-1-and-2-gpu-performance-per-dollar-ampere-2/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-1-and-2-GPU-Performance-per-Dollar-Ampere-1.svg" data-orig-size="1703,1679" data-comments-opened="1" data-image-meta="[]" data-image-title="Normalized 1 and 2-GPU Performance per Dollar Ampere" data-image-description="&lt;p&gt;Figure 9: Normalized deep learning performance-per-dollar relative to RTX 3080.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure 9: Normalized deep learning performance-per-dollar relative to RTX 3080.&lt;/p&gt;
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-1-and-2-GPU-Performance-per-Dollar-Ampere-1.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-1-and-2-GPU-Performance-per-Dollar-Ampere-1.svg" width="1703" height="1679" src="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-1-and-2-GPU-Performance-per-Dollar-Ampere-1.svg" alt="Normalized 1 and 2-GPU Performance per Dollar Ampere" class="wp-image-1024"/><figcaption>Figure 9: Normalized deep learning performance-per-dollar relative to RTX 3080.</figcaption></figure>



<figure class="wp-block-image size-large eplus-bqLboO"><img data-attachment-id="1022" data-permalink="https://timdettmers.com/normalized-4-gpu-performance-per-dollar-ampere-2/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-4-GPU-Performance-per-Dollar-Ampere-1.svg" data-orig-size="1717,1691" data-comments-opened="1" data-image-meta="[]" data-image-title="Normalized 4-GPU Performance per Dollar Ampere" data-image-description="&lt;p&gt;Figure 10: Normalized 4-GPU deep learning performance-per-dollar relative to RTX 3080.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure 10: Normalized 4-GPU deep learning performance-per-dollar relative to RTX 3080.&lt;/p&gt;
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-4-GPU-Performance-per-Dollar-Ampere-1.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-4-GPU-Performance-per-Dollar-Ampere-1.svg" width="1717" height="1691" src="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-4-GPU-Performance-per-Dollar-Ampere-1.svg" alt="Normalized 4-GPU Performance per Dollar Ampere" class="wp-image-1022"/><figcaption>Figure 10: Normalized 4-GPU deep learning performance-per-dollar relative to RTX 3080.</figcaption></figure>



<figure class="wp-block-image size-large eplus-503Hlx"><img data-attachment-id="1023" data-permalink="https://timdettmers.com/normalized-8-gpu-performance-per-dollar-ampere-2/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-8-GPU-Performance-per-Dollar-Ampere-1.svg" data-orig-size="1703,1685" data-comments-opened="1" data-image-meta="[]" data-image-title="Normalized 8-GPU Performance per Dollar Ampere" data-image-description="&lt;p&gt;Figure 11: Normalized 8-GPU deep learning performance-per-dollar relative to RTX 3080&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure 11: Normalized 8-GPU deep learning performance-per-dollar relative to RTX 3080&lt;/p&gt;
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-8-GPU-Performance-per-Dollar-Ampere-1.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-8-GPU-Performance-per-Dollar-Ampere-1.svg" width="1703" height="1685" src="https://timdettmers.com/wp-content/uploads/2020/09/Normalized-8-GPU-Performance-per-Dollar-Ampere-1.svg" alt="Normalized 8-GPU Performance per Dollar Ampere" class="wp-image-1023"/><figcaption>Figure 11: Normalized 8-GPU deep learning performance-per-dollar relative to RTX 3080</figcaption></figure>


<div class="crp_related   crp_related_block  mobile-only crp-rounded-thumbs"><ul><li><a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"     class="crp_link post-1093"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="LLM.int8() and Emergent Features" title="LLM.int8() and Emergent Features" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2022/08/zeroshot3.png?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">LLM.int8() and Emergent Features</span></a></li><li><a href="https://timdettmers.com/2022/03/13/how-to-choose-your-grad-school/"     class="crp_link post-805"><figure><img loading="lazy"  width="150" height="150"  src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1" class="crp_thumb crp_featured" alt="How to Choose Your Grad School" title="How to Choose Your Grad School" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=2&amp;resize=150%2C150&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/03/IMG_20180420_195552216_HDR.jpg?zoom=3&amp;resize=150%2C150&amp;ssl=1 450w" sizes="(max-width: 150px) 100vw, 150px" /></figure><span class="crp_title">How to Choose Your Grad School</span></a></li></ul><div class="crp_clear"></div></div>


<h2 class="eplus-RwDsYV">GPU Recommendations</h2>



<p class="eplus-CQxjtd">The first thing that need to emphasize again: If you choose a GPU, you need to make sure that it has enough memory for what you want to do. The steps in selecting the best deep learning GPU for you should be:</p>



<ol class="eplus-w4MZ5u"><li>What do I want to do with the GPU(s): Kaggle competitions, machine learning, learning deep learning, hacking on small projects (GAN-fun or big language models?), doing research in computer vision / natural language processing / other domains, or something else?</li><li>How much memory do I need for what I want to do?</li><li>Use the Cost/Performance charts from above to figure out which GPU is best for you that fulfills the memory criteria.</li><li>Are there additional caveats for the GPU that I chose? For example, if it is an RTX 3090, can I fit it into my computer? Does my power supply unit (PSU) have enough wattage to support my GPU(s)? Will heat dissipation be a problem, or can I somehow cool the GPU effectively?</li></ol>



<p class="eplus-9yb9Eg">Some of these details require you to self-reflect about what you want and maybe research a bit about how much memory the GPUs have that other people use for your area of interest. I can give you some guidance, but I cannot cover all areas here.</p>



<h3 class="eplus-3RSqDc">When do I need &gt;= 11 GB of Memory?</h3>



<p class="eplus-zMASNR">I mentioned before that you should have at least 11 GB of memory if you work with transformers, and better yet, &gt;= 24 GB of memory if you do research on transformers. This is so because most previous models that are pretrained have pretty steep memory requirements, and these models were trained with at least RTX 2080 Ti GPUs that have 11 GB of memory. Thus having less than 11 GB can create scenarios where it is difficult to run certain models.</p>



<p class="eplus-UsKhB6">Other areas that require large amounts of memory are anything medical imaging, some state-of-the-art computer vision models, anything with very large images (GAN, style transfer).</p>



<p class="eplus-EvalZF">In general, if you seek to build models that give you the edge in competition, be it research, industry, or Kaggle competition, extra memory will provide you with a possible edge.</p>



<h3 class="eplus-WGUdpQ">When is &lt;11 GB of Memory Okay?</h3>



<p class="eplus-cBPqoe">The RTX 3070 and RTX 3080 are mighty cards, but they lack a bit of memory. For many tasks, however, you do not need that amount of memory.</p>



<p class="eplus-FjF39v">The RTX 3070 is perfect if you want to learn deep learning. This is so because the basic skills of training most architectures can be learned by just scaling them down a bit or using a bit smaller input images. If I would learn deep learning again, I would probably roll with one RTX 3070, or even multiple if I have the money to spare. </p>



<p class="eplus-tXqTR7">The RTX 3080 is currently by far the most cost-efficient card and thus ideal for prototyping. For prototyping, you want the largest memory, which is still cheap. With prototyping, I mean here prototyping in any area: Research, competitive Kaggle, hacking ideas/models for a startup, experimenting with research code. For all these applications, the RTX 3080 is the best GPU.</p>



<p class="eplus-WR2yDR">Suppose I would lead a research lab/startup. I would put 66-80% of my budget in RTX 3080 machines and 20-33% for “rollout” RTX 3090 machines with a robust water cooling setup. The idea is, RTX 3080 is much more cost-effective and can be shared via a slurm cluster setup as prototyping machines. Since prototyping should be done in an agile way, it should be done with smaller models and smaller datasets. RTX 3080 is perfect for this. Once students/colleagues have a great prototype model, they can rollout the prototype on the RTX 3090 machines and scale to larger models.&nbsp;</p>



<h3 class="eplus-XIdGkn">How can I fit +24GB models into 10GB memory?</h3>



<p class="eplus-8KX6FF">It is a bit contradictory that I just said if you want to train big models, you need lots of memory, but we have been struggling with big models a lot since the onslaught of BERT and solutions exists to train 24 GB models in 10 GB memory. If you do not have the money or what to avoid cooling/power issues of the RTX 3090, you can get RTX 3080 and just accept that you need do some extra programming by adding memory-saving techniques. There are enough techniques to make it work, and they are becoming more and more commonplace.</p>



<p class="eplus-TtN7bc">Here just a list of common techniques: </p>



<ul class="eplus-kaToMv"><li>FP16/BF16 training (<a href="https://medium.com/the-artificial-impostor/use-nvidia-apex-for-easy-mixed-precision-training-in-pytorch-46841c6eed8c">apex</a>)</li><li><a href="https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb">Gradient checkpointing</a> (only store some of the activations and recompute them in the backward pass)</li><li>GPU-to-CPU <a href="https://arxiv.org/abs/2002.05645v5">Memory Swapping</a> (swap layers not needed to the CPU; swap them back in just-in-time for backprop)</li><li><a href="https://timdettmers.com/2014/11/09/model-parallelism-deep-learning/">Model Parallelism</a> (each GPU holds a part of each layer; supported by fairseq)</li><li>Pipeline parallelism (each GPU hols a couple of layers of the network) </li><li><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO</a> parallelism (each GPU holds partial layers)</li><li><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">3D parallelism</a> (Model + pipeline + ZeRO)</li><li>CPU Optimizer state (store and update Adam/Momentum on the CPU while the next GPU forward pass is happening) </li></ul>



<p class="eplus-DK7SPD">If you are not afraid to tinker a bit and implement some of these techniques — which usually means integrating packages that support them with your code — you will be able to fit that 24GB large network on a smaller GPU. With that hacking spirit, the RTX 3080, or any GPU with less than 11 GB memory, might be a great GPU for you.</p>



<h3 class="eplus-pb5gym">Is upgrading from RTX 20 to RTX 30 GPU worth it? Or Should I wait for the next GPU?</h3>



<p class="eplus-CD5XWA">If I were you, I would think twice about upgrading from an RTX 20 GPU to an RTX 30 GPU. You might be eager to get that 30% faster training or so, but it can be a big headache to deal with all the other RTX 30 GPU problems. The power supply, the cooling, you need to sell your old GPUs. Is it worth it all?</p>



<p class="eplus-mzyOel">I could imagine if you need that extra memory, for example, to go from RTX 2080 Ti to RTX 3090, or if you want a huge boost in performance, say from RTX 2060 to RTX 3080, then it can be pretty worth it. But if you stay &#8220;in your league,&#8221; that is, going from Titan RTX to RTX 3090, or, RTX 2080 Ti to RTX 3080, it is hardly worth it. You gain a bit of performance, but you will have headaches about the power supply and cooling, and you are a good chunk of money lighter. I do not think it is worth it. I would wait until a better alternative to GDDR6X memory is released. This will make GPUs use less power and might even make them faster. Maybe wait a year and see how the landscape has changed since then. </p>



<p class="eplus-7kB7bE">It is worth mentioning that technology is slowing anyways. So waiting for a year might net you a GPU, which will stay current for more than 5 years. There will be a time when cheap HBM memory can be manufactured. If that time comes, and you buy that GPU and you will likely stay on that GPU for more than 7 years. Such GPUs might be available in 3-4 years. As such, playing the waiting game can be a pretty smart choice.</p>



<h3 class="eplus-REZnig">General Recommendations</h3>



<p class="eplus-1jH1ES">In general, the RTX 30 series is very powerful, and I recommend these GPUs. Be aware of memory, as discussed in the previous section, but also power requirements and cooling. If you have one PCIe slot between GPUs, cooling will be no problem at all. Otherwise, with RTX 30 cards, make sure you get water cooling, PCIe extenders, or effective blower cards (data in the next weeks will show the NVIDIA fan design is adequate).</p>



<p class="eplus-AcU031">In general, I would recommend the RTX 3090 for anyone that can afford it. It will equip you not only for now but will be a very effective card for the next 3-7 years. As such, it is a good investment that will stay strong. It is unlikely that HBM memory will become cheap within three years, so the next GPU would only be about 25% better than the RTX 3090. We will probably see cheap HBM memory in 3-5 years, so after that, you definitely want to upgrade.</p>



<p class="eplus-0g0Yv8">For PhD students, those who <a href="https://timdettmers.com/2018/11/26/phd-applications/">want to become PhD students</a>, or those who <a href="https://timdettmers.com/2020/03/10/how-to-pick-your-grad-school/">get started with a PhD</a>, I recommend RTX 3080 GPUs for prototyping and RTX 3090 GPUs for doing rollouts. If your department has a GPU cluster, I would highly recommend a <a href="https://slurm.schedmd.com/documentation.html">Slurm</a> GPU cluster with 8 GPU machines. However, since the cooling of RTX 3080 GPUs in an 8x GPU server setup is questionable it is unlikely that you will be able to run these. If the cooling works, I would recommend 66-80% RTX 3080 GPUs and the rest of the GPUs being either RTX 3090 or Tesla A100. If cooling does not work I would recommend 66-80% RTX 2080 and the rest being Tesla A100s. Again, it is crucial, though, that you make sure that heating issues in your GPU servers are taken care of before you commit to specific GPUs for your servers. More on GPU clusters below. </p>



<p class="eplus-DWBQlo">If you have multiple RTX 3090’s, make sure you choose solutions that guarantee sufficient cooling and power. I will update the blog post about this as more and more data is rolling in what is a proper setup.</p>



<p class="eplus-lu9hPv">For anyone without strictly competitive requirements (research, competitive Kaggle, competitive startups), I would recommend in order: Used RTX 2080 Ti, used RTX 2070, new RTX 3080, new RTX 3070. If you do not like used cards, but the RTX 3080. If you cannot afford the RTX 3080, go with the RTX 3070. All of these cards are very cost-effective solutions and will ensure fast training of most networks. If you use the right memory tricks and are fine with some extra programming, there are now enough tricks to make a 24 GB neural network fit into a 10 GB GPU. As such, if you accept a bit of uncertainty and some extra programming, the RTX 3080 might also be a better choice compared to the RTX 3090 since performance is quite similar between these cards.</p>



<p class="eplus-t8wbEB">If your budget is limited and an RTX 3070 is too expensive, a used RTX 2070 is about $260 on eBay. It is not clear yet if there will be an RTX 3060, but if you are on a limited budget, it might also be worth waiting a bit more. If priced similarly to the RTX 2060 and GTX 1060, you can expect a price of $250 to $300 and a pretty strong performance.</p>



<p class="eplus-mtG0kc">If your budget is limited, but you still need large amounts of memory, then old, used Tesla or Quadro cards from eBay might be best for you. The Quadro M6000 has 24 GB of memory and goes for $400 on eBay. The Tesla K80 has a 2-in-1 GPU with 2x 12 GB of memory for about $200. These cards are slow compared to more modern cards, but the extra memory can come in handy for specific projects where memory is paramount.</p>



<h3 class="eplus-D11TrS">Recommendations for GPU Clusters</h3>



<p class="eplus-lyFOSe">GPU cluster design depends highly on use. For a +1,024 GPU system, networking is paramount, but if users only use at most 32 GPUs at a time on such a system investing in powerful networking infrastructure is a waste. Here, I would go with similar prototyping-rollout reasoning, as mentioned in the RTX 3080 vs RTX 3090 case.&nbsp;</p>



<p class="eplus-IpYlAh">In general, RTX cards are banned from data centers via the CUDA license agreement. However, often universities can get an exemption from this rule. It is worth getting in touch with someone from NVIDIA about this to ask for an exemption. If you are allowed to use RTX cards, I would recommend standard Supermicro 8 GPU systems with RTX 3080 or RTX 3090 GPUs (if sufficient cooling can be assured). A small set of 8x A100 nodes ensures effective “rollout” after prototyping, especially if there is no guarantee that the 8x RTX 3090 servers can be cooled sufficiently. In this case, I would recommend A100 over RTX 6000 / RTX 8000 because the A100 is pretty cost-effective and future proof.</p>



<p class="eplus-X7ewYg">In the case you want to train vast networks on a GPU cluster (+256 GPUs), I would recommend the NVIDIA DGX SuperPOD system with A100 GPUs. At a +256 GPU scale, networking is becoming paramount. If you want to scale to more than 256 GPUs, you need a highly optimized system, and putting together standard solutions is no longer cutting it.&nbsp;</p>



<p class="eplus-1U1GON">Especially at a scale of +1024 GPUs, the only competitive solutions on the market are the Google TPU Pod and NVIDIA DGX SuperPod. At that scale, I would prefer the Google TPU Pod since their custom made networking infrastructure seems to be superior to the NVIDIA DGX SuperPod system — although both systems come quite close to each other. The GPU system offers a bit more flexibility of deep learning models and applications over the TPU system, while the TPU system supports larger models and provides better scaling. So both systems have their advantages and disadvantages.</p>



<h3 class="eplus-0edda7">Do Not Buy These GPUs</h3>



<p class="eplus-zATKzd">I do not recommend buying multiple RTX Founders Editions (any) or RTX Titans unless you have PCIe extenders to solve their cooling problems. They will simply run too hot, and their performance will be way below what I report in the charts above. 4x RTX 2080 Ti Founders Editions GPUs will readily dash beyond 90C, will throttle down their core clock, and will run slower than properly cooled RTX 2070 GPUs.</p>



<p class="eplus-CCqPf7">I do not recommend buying Tesla V100 or A100 unless you are forced to buy them (banned RTX data center policy for companies) or unless you want to train very large networks on a huge GPU cluster — these GPUs are just not very cost-effective.</p>



<p class="eplus-mD8QMG">If you can afford better cards, do not buy GTX 16 series cards. These cards do not have tensor cores and, as such, provide relatively poor deep learning performance. I would choose a used RTX 2070 / RTX 2060 / RTX 2060 Super over a GTX 16 series card. If you are short on money, however, the GTX 16 series cards can be a good option.</p>



<h3 class="eplus-jAqelF">When Is it Best Not to Buy New GPUs?</h3>



<p class="eplus-ixU5wM">If you already have RTX 2080 Tis or better GPUs, an upgrade to RTX 3090 may not make sense. Your GPUs are already pretty good, and the performance gains are negligible compared to worrying about the PSU and cooling problems for the new power-hungry RTX 30 cards — just not worth it.</p>



<p class="eplus-F9yeNw">The only reason I would want to upgrade from 4x RTX 2080 Ti to 4x RTX 3090 would be if I do research on huge transformers or other highly compute dependent network training. However, if memory is a problem, you may first consider some memory tricks to fit large models on your 4x RTX 2080 Tis before upgrading to RTX 3090s.</p>



<p class="eplus-jgLXoi">If you have one or multiple RTX 2070 GPUs, I would think twice about an upgrade. These are pretty good GPUs. Reselling those GPUs on eBay and getting RTX 3090s could make sense, though, if you find yourself often limited by the 8 GB memory. This reasoning is valid for many other GPUs: If memory is tight, an upgrade is right.</p>



<h2 class="eplus-rZBwp1">Question &amp; Answers &amp; Misconceptions</h2>



<p class="eplus-RN5QsL">Summary:</p>



<ul class="eplus-wflokB"><li>PCIe 4.0 and PCIe lanes do not matter in 2x GPU setups. For 4x GPU setups, they still do not matter much.</li><li>RTX 3090 and RTX 3080 cooling will be problematic. Use water-cooled cards or PCIe extenders.</li><li>NVLink is not useful. Only useful for GPU clusters.</li><li>You can use different types of GPUs in one computer (e.g., GTX 1080 + RTX 2080 + RTX 3090), but you will not be able to parallelize across them efficiently.</li><li>You will need Infiniband +50Gbit/s networking to parallelize training across more than two machines.</li><li>AMD CPUs are cheaper than Intel CPUs; Intel CPUs have almost no advantage.</li><li>Despite heroic software engineering efforts, AMD GPUs + ROCm will probably not be able to compete with NVIDIA due to lacking community and Tensor Core equivalent for at least 1-2 years.</li><li>Cloud GPUs are useful if you use them for less than 1 year. After that, a desktop is the cheaper solution.</li></ul>



<h3 class="eplus-BKkk3i">Do I need PCIe 4.0?</h3>



<p class="eplus-9ByauH">Generally, no. PCIe 4.0 is great if you have a GPU cluster. It is okay if you have an 8x GPU machine, but otherwise, it does not yield many benefits. It allows better parallelization and a bit faster data transfer. Data transfers are not a bottleneck in any application. In computer vision, in the data transfer pipeline, the data storage can be a bottleneck, but not the PCIe transfer from CPU to GPU. So there is no real reason to get a PCIe 4.0 setup for most people. The benefits will be maybe 1-7% better parallelization in a 4 GPU setup.</p>



<h3 class="eplus-iAnpcZ">Do I need 8x/16x PCIe lanes?&nbsp;</h3>



<p class="eplus-4e3Zwd">Same as with PCIe 4.0 — generally, no. <a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">PCIe lanes</a> are needed for parallelization and fast data transfers, which are seldom a bottleneck. Operating GPUs on 4x lanes is fine, especially if you only have 2 GPUs. For a 4 GPU setup, I would prefer 8x lanes per GPU, but running them at 4x lanes will probably only decrease performance by around 5-10% if you parallelize across all 4 GPUs.</p>



<h3 class="eplus-5dLLUU">How do I fit 4x RTX 3090 if they take up 3 PCIe slots each?</h3>



<p class="eplus-ZiOIiH">You need to get one of the two-slot variants, or you can try to spread them out with PCIe extenders. Besides space, you should also immediately think about cooling and a suitable PSU. It seems the most manageable solution will be to get 4x <a href="https://www.evga.com/articles/01434/evga-geforce-rtx-30-series/" rel="nofollow" class="broken_link">RTX 3090 EVGA Hydro Copper</a> with a custom water cooling loop. This will keep the cards very cool. EVGA produced hydro copper versions of GPUs for years, and I believe you can trust in their water-cooled GPUs’ quality. There might also be other variants which are cheaper though.</p>



<p class="eplus-hKh4h6">PCIe extenders might also solve both space and cooling issues, but you need to make sure that you have enough space in your case to spread out the GPUs. Make sure your PCIe extenders are long enough!</p>



<h3 class="eplus-2ceRV0">How do I cool 4x RTX 3090 or 4x RTX 3080?</h3>



<p class="eplus-Fchmll">See the previous section.</p>



<h3 class="eplus-hlYdEC">Can I use multiple GPUs of different GPU types?</h3>



<p class="eplus-cOwKhd">Yes, you can! But you cannot parallelize efficiently across GPUs of different types. I could imagine a 3x RTX 3070 + 1 RTX 3090 could make sense for a prototyping-rollout split. On the other hand, parallelizing across 4x RTX 3070 GPUs would be very fast if you can make the model fit onto those GPUs. The only other reason why you want to do this that I can think of is if you&#8217;re going to use your old GPUs. This works just fine, but parallelization across those GPUs will be inefficient since the fastest GPU will wait for the slowest GPU to catch up to a synchronization point (usually gradient update).</p>



<h3 class="eplus-YbZb25">What is NVLink, and is it useful?</h3>



<p class="eplus-kZZzbt">Generally, NVLink is not useful. NVLink is a high speed interconnect between GPUs. It is useful if you have a GPU cluster with +128 GPUs. Otherwise, it yields almost no benefits over standard PCIe transfers.</p>



<h3 class="eplus-pqooRC">I do not have enough money, even for the cheapest GPUs you recommend. What can I do?</h3>



<p class="eplus-ClLhtz">Definitely buy used GPUs. Used RTX 2070 ($400) and RTX 2060 ($300) are great. If you cannot afford that, the next best option is to try to get a used GTX 1070 ($220) or GTX 1070 Ti ($230). If that is too expensive, a used GTX 980 Ti (6GB $150) or a used GTX 1650 Super ($190). If that is too expensive, it is best to roll with free GPU cloud services. These usually provided a GPU for a limited amount of time/credits, after which you need to pay. Rotate between services and accounts until you can afford your own GPU.</p>



<h3 class="eplus-OqapSw">What is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?</h3>



<p class="eplus-zai7Ps">I built a <a href="https://github.com/TimDettmers/carbonneutral">carbon calculator</a> for calculating your carbon footprint for academics (carbon from flights to conferences + GPU time). The calculator can also be used to calculate a pure GPU carbon footprint. You will find that GPUs produce much, much more carbon than international flights. As such, you should make sure you have a green source of energy if you do not want to have an astronomical carbon footprint. If no electricity provider in our area provides green energy, the best way is to buy carbon offsets. Many people are skeptical about carbon offsets. Do they work? Are they scams?</p>



<p class="eplus-EWsGx0">I believe skepticism just hurts in this case, because not doing anything would be more harmful than risking the probability of getting scammed. If you worry about scams, just invest in a portfolio of offsets to minimize risk.</p>



<p class="eplus-69USNG">I worked on a project that produced carbon offsets about ten years ago. The carbon offsets were generated by burning leaking methane from mines in China. UN officials tracked the process, and they required clean digital data and physical inspections of the project site. In that case, the carbon offsets that were produced were highly reliable. I believe many other projects have similar quality standards.</p>



<h3 class="eplus-3vCLFD">What do I need to parallelize across two machines?</h3>



<p class="eplus-fTOuun">If you want to be on the safe side, you should get at least +50Gbits/s network cards to gain speedups if you want to parallelize across machines. I recommend having at least an EDR Infiniband setup, meaning a network card with at least 50 GBit/s bandwidth. Two EDR cards with cable are about $500 on eBay.</p>



<p class="eplus-9X2IIc">In some cases, you might be able to get away with 10 Gbit/s Ethernet, but this is usually only the case for special networks (certain convolutional networks) or if you use certain algorithms (Microsoft DeepSpeed).</p>



<h3 class="eplus-PgjlVt">Is the sparse matrix multiplication features suitable for sparse matrices in general?</h3>



<p class="eplus-R0OltN">It does not seem so. Since the granularity of the sparse matrix needs to have 2 zero-valued elements, every 4 elements, the sparse matrices need to be quite structured. It might be possible to adjust the algorithm slightly, which involves that you pool 4 values into a compressed representation of 2 values, but this also means that precise arbitrary sparse matrix multiplication is not possible with Ampere GPUs.</p>



<h3 class="eplus-MYBgIz">Do I need an Intel CPU to power a multi-GPU setup?</h3>



<p class="eplus-ILUHTD">I do not recommend Intel CPUs unless you heavily use CPUs in Kaggle competitions (heavy linear algebra on the CPU). Even for Kaggle competitions AMD CPUs are still great, though. AMD CPUs are cheaper and better than Intel CPUs in general for deep learning. For a 4x GPU built, my go-to CPU would be a Threadripper. We built dozens of systems at our university with Threadrippers, and they all work great — no complaints yet. For 8x GPU systems, I would usually go with CPUs that your vendor has experience with. CPU and PCIe/system reliability is more important in 8x systems than straight performance or straight cost-effectiveness.</p>



<h3 class="eplus-V6H8gS">Does computer case design matter for cooling?</h3>



<p class="eplus-rSzCnL">No. GPUs are usually perfectly cooled if there is at least a small gap between GPUs. Case design will give you 1-3 C better temperatures, space between GPUs will provide you with 10-30 C improvements. The bottom line, if you have space between GPUs, cooling does not matter. If you have no space between GPUs, you need the right cooler design (blower fan) or another solution (water cooling, PCIe extenders), but in either case, case design and case fans do not matter.</p>



<h3 class="eplus-Umve55">Will AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?</h3>



<p class="eplus-sOjnl6">Not in the next 1-2 years. It is a three-way problem: Tensor Cores, software, and community.&nbsp;</p>



<p class="eplus-NfDW5K">AMD GPUs are great in terms of pure silicon: Great FP16 performance, great memory bandwidth. However, their lack of Tensor Cores or the equivalent makes their deep learning performance poor compared to NVIDIA GPUs. Packed low-precision math does not cut it. Without this hardware feature, AMD GPUs will never be competitive. Rumors show that <a href="https://wccftech.com/amd-cdna-architecture-radeon-instinct-arcturus-gpu-120-cu-7680-cores/">some data center card</a> with Tensor Core equivalent is planned for 2020, but no new data emerged since then. Just having data center cards with a Tensor Core equivalent would also mean that few would be able to afford such AMD GPUs, which would give NVIDIA a competitive advantage.</p>



<p class="eplus-O3Dvhx">Let’s say AMD introduces a Tensor-Core-like-hardware feature in the future. Then many people would say, “But there is no software that works for AMD GPUs! How am I supposed to use them?” This is mostly a misconception. The AMD software via ROCm has come to a long way, and support via PyTorch is excellent. While I have not seen many experience reports for AMD GPUs + PyTorch, all the software features are integrated. It seems, if you pick any network, you will be just fine running it on AMD GPUs. So here AMD has come a long way, and this issue is more or less solved.</p>



<p class="eplus-OTgr4r">However, if you solve software and the lack of Tensor Cores, AMD still has a problem: the lack of community. If you have a problem with NVIDIA GPUs, you can Google the problem and find a solution. That builds a lot of trust in NVIDIA GPUs. You have the infrastructure that makes using NVIDIA GPUs easy (any deep learning framework works, any scientific problem is well supported). You have the hacks and tricks that make usage of NVIDIA GPUs a breeze (e.g., apex). You can find experts on NVIDIA GPUs and programming around every other corner while I knew much less AMD GPU experts.</p>



<p class="eplus-9AWxYr">In the community aspect, AMD is a bit like Julia vs Python. Julia has a lot of potential, and many would say, and rightly so, that it is the superior programming language for scientific computing. Yet, Julia is barely used compared to Python. This is because the Python community is very strong. Numpy, SciPy, Pandas are powerful software packages that a large number of people congregate around. This is very similar to the NVIDIA vs AMD issue.</p>



<p class="eplus-o6eRO8">Thus, it is likely that AMD will not catch up until Tensor Core equivalent is introduced (1/2 to 1 year?) and a strong community is built around ROCm (2 years?). AMD will always snatch a part of the market share in specific subgroups (e.g., cryptocurrency mining, data centers). Still, in deep learning, NVIDIA will likely keep its monopoly for at least a couple more years.</p>



<h3 class="eplus-OkbBy3">When is it better to use the cloud vs a dedicated GPU desktop/server?</h3>



<p class="eplus-eFDtSu">Rule-of-thumb: If you expect to do deep learning for longer than a year, it is cheaper to get a desktop GPU. Otherwise, cloud instances are preferable unless you have extensive cloud computing skills and want the benefits of scaling the number of GPUs up and down at will.</p>



<p class="eplus-i5Jvrp">For the exact point in time when a cloud GPU is more expensive than a desktop depends highly on the service that you are using, and it is best to do a little math on this yourself. Below I do an example calculation for an AWS V100 spot instance with 1x V100 and compare it to the price of a desktop with a single RTX 3090 (similar performance). The desktop with RTX 3090 costs $2,200 (<a ref="https://pcpartpicker.com/user/tim_dettmers/saved/#view=mZ2rD3">2-GPU barebone</a> + RTX 3090). Additionally, assuming you are in the US, there is an additional $0.12 per kWh for electricity. This compares to $2.14 per hour for the AWS on-demand instance.</p>



<p class="eplus-sF5f8X">At 15% utilization per year, the desktop uses:&nbsp;</p>



<p class="eplus-jfhFj2">(350 W (GPU) + 100 W (CPU))*0.15 (utilization) * 24 hours * 365 days = 591 kWh per year</p>



<p class="eplus-uZPGyz">So 591 kWh of electricity per year, that is an additional $71.</p>



<p class="eplus-EqZNc1">The break-even point for a desktop vs a cloud instance at 15% utilization (you use the cloud instance 15% of time during the day), would be about 300 days ($2,311 vs $2,270):</p>



<p class="eplus-WO1NL4">$2.14/h * 0.15 (utilization) * 24 hours * 300 days = $2,311</p>



<p class="eplus-3brSoQ">So if you expect to run deep learning models after 300 days, it is better to buy a desktop instead of using AWS on-demand instances.</p>



<p class="eplus-aYvyd0">AWS spot instances are a bit cheaper at about 0.9$ per hour. However, many users on Twitter were telling me that on-demand instances are a nightmare, but that spot instances are <em>hell</em>. AWS itself lists the average frequency of interruptions of V100 GPU spot instances to be above 20%. This means you need a pretty good spot instance management infrastructure to make it worth it to use spot instances. But if you have it, AWS spot instances and similar services are pretty competitive. You need to own and run a desktop for 20 months to run even compared to AWS spot instances. This means if you expect to run deep learning workloads in the next 20 months, a desktop machine will be cheaper (and easier to use).</p>



<p class="eplus-CmY2qr">You can do similar calculations for any cloud service to make the decision if you go for a cloud service or a desktop.</p>



<p class="eplus-6m3gGS">Common utilization rates are the following:</p>



<ul class="eplus-sNwMkX"><li>PhD student personal desktop: &lt; 15%</li><li>PhD student slurm GPU cluster: &gt; 35%</li><li>Company-wide slurm research cluster: &gt; 60%</li></ul>



<p class="eplus-deH52E">In general, utilization rates are lower for professions where thinking about cutting edge ideas is more important than developing practical products. Some areas have low utilization rates (interpretability research), while other areas have much higher rates (machine translation, language modeling). In general, the utilization of personal machines is almost always overestimated. Commonly, most personal systems have a utilization rate between 5-10%. This is why I would highly recommend slurm GPU clusters for research groups and companies instead of individual desktop GPU machines.</p>



<h2 class="eplus-MNN7ZT"><strong>TL;DR advice</strong></h2>



<p class="eplus-3oTerp"><strong>Best GPU overall</strong>: RTX 3080 and RTX 3090.</p>



<p class="eplus-syLana"><strong>GPUs to avoid (as an individual)</strong>: Any Tesla card; any Quadro card; any Founders Edition card; Titan RTX, Titan V, Titan XP.</p>



<p class="eplus-kwa5yO"><strong>Cost-efficient but expensive</strong>: RTX 3080.</p>



<p class="eplus-XeLVZu"><strong>Cost-efficient and cheaper</strong>:&nbsp; RTX 3070, RTX 2060 Super</p>



<p class="eplus-vkySjI"><strong>I have little money</strong>: Buy used cards. Hierarchy: RTX 2070 ($400), RTX 2060 ($300), GTX 1070 ($220), GTX 1070 Ti ($230), GTX 1650 Super ($190), GTX 980 Ti (6GB $150).</p>



<p class="eplus-iqSmcd"><strong>I have almost no money</strong>: There are a lot of startups that promo their clouds: Use free cloud credits and switch companies accounts until you can afford a GPU.</p>



<p class="eplus-jBi8iG"><strong>I do Kaggle: </strong>RTX 3070.</p>



<p class="eplus-HAtKS0"><strong>I am a competitive computer vision, pretraining, or machine translation researcher</strong>: 4x RTX 3090. Wait until working builds with good cooling, and enough power are confirmed (I will update this blog post).</p>



<p class="eplus-Kxjcu5"><strong>I am an NLP researcher</strong>: If you do not work on machine translation, language modeling, or pretraining of any kind, an RTX 3080 will be sufficient and cost-effective.</p>



<p class="eplus-afPlcq"><strong>I started deep learning, and I am serious about it</strong>: Start with an RTX 3070. If you are still serious after 6-9 months, sell your RTX 3070 and buy 4x RTX 3080. Depending on what area you choose next (startup, Kaggle, research, applied deep learning), sell your GPUs, and buy something more appropriate after about three years (next-gen RTX 40s GPUs).</p>



<p class="eplus-2YddxW"><strong>I want to try deep learning, but I am not serious about it</strong>: The RTX 2060 Super is excellent but may require a new power supply to be used. If your motherboard has a PCIe x16 slot and you have a power supply with around 300 W, a GTX 1050 Ti is a great option since it will not require any other computer components to work with your desktop computer.</p>



<p class="eplus-hPS4gf"><strong>GPU Cluster used for parallel models across less than 128 GPUs: </strong>If you are allowed to buy RTX GPUs for your cluster: 66% 8x RTX 3080 and 33% 8x RTX 3090 (only if sufficient cooling is guaranteed/confirmed). If cooling of RTX 3090s is not sufficient buy 33% RTX 6000 GPUs or 8x Tesla A100 instead. If you are not allowed to buy RTX GPUs, I would probably go with 8x A100 Supermicro nodes or 8x RTX 6000 nodes.</p>



<p class="eplus-uN1scV"><strong>GPU Cluster used for parallel models across 128 GPUs:</strong> Think about 8x Tesla A100 setups. If you use more than 512 GPUs, you should think about getting a DGX A100 SuperPOD system that fits your scale.</p>



<h2 class="eplus-bXPSts">Version History</h2>



<ul class="eplus-xnXOsk"><li>2020-09-20: Added discussion of using power limiting to run 4x RTX 3090 systems. Added older GPUs to the performance and cost/performance charts. Added figures for sparse matrix multiplication.</li><li>2020-09-07: Added NVIDIA Ampere series GPUs. Included lots of good-to-know GPU details.</li><li>2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.</li><li>2018-11-26: Added discussion of overheating issues of RTX cards.</li><li>2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.</li><li>2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis</li><li>2017-04-09: Added cost-efficiency analysis; updated recommendation with NVIDIA Titan Xp</li><li>2017-03-19: Cleaned up blog post; added GTX 1080 Ti</li><li>2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations</li><li>2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series</li><li>2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation</li><li>2015-04-22: GTX 580 no longer recommended; added performance relationships between cards</li><li>2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580</li><li>2015-02-23: Updated GPU recommendations and memory calculations</li><li>2014-09-28: Added emphasis for memory requirement of CNNs</li></ul>



<h2 class="eplus-qkgeNA">Acknowledgments</h2>



<p class="eplus-FAO2nS">I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the current version of this blog post.</p>



<p class="eplus-kKOsc0">For past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes.</p>
</div></div>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/">Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/feed/</wfw:commentRss>
			<slash:comments>1659</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">6</post-id>	</item>
		<item>
		<title>On Creativity in Academia</title>
		<link>https://timdettmers.com/2019/09/03/creativity-in-academia/</link>
					<comments>https://timdettmers.com/2019/09/03/creativity-in-academia/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Tue, 03 Sep 2019 12:05:19 +0000</pubDate>
				<category><![CDATA[Academia]]></category>
		<category><![CDATA[PhD Life]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[PhD]]></category>
		<guid isPermaLink="false">https://timdettmers.com/?p=796</guid>

					<description><![CDATA[<p>I recently had a discussion about creativity with a colleague. We were discussing music and how creative many bands and groups are. At the end of our conversation, my friend told me, half-sarcastic-half-serious, how much more creative the people in the music industry are than him and that he just cannot find good ideas in [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2019/09/03/creativity-in-academia/">On Creativity in Academia</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>I recently had a discussion about creativity with a colleague. We were discussing music and how creative many bands and groups are. At the end of our conversation, my friend told me, half-sarcastic-half-serious, how much more creative the people in the music industry are than him and that he just cannot find good ideas in his area of research even though he tried so hard for such a long time. I was a bit surprised because I thought of him as someone very creative. However, it is not uncommon to hear scientists lament about their lack of creativity compared to academic superstars. I think about creativity in academia is a bit distorted and a straight view can help to feel less bad about one&#8217;s own creativity.</p>
<p><span id="more-796"></span></p>
<p>This blog post is part of a series of blog posts about scientific thinking in deep learning, natural language processing and science in general. I am currently on vacation in China, and I wanted to relax a bit by writing down some reflective blog posts which capture the thoughts that were lingering in my mind for weeks or months.</p>
<h2>Are Theoretical Physicists Creative?</h2>
<p>I think the paradox that very creative people think they are not creative is best demonstrated by looking at theoretical physicists and compare them to children. In psychological research, it is well known that children score much better on many tasks of divergent thinking than adults do: They do not think about the limitations of an object so that a brick which is used for building buildings is suddenly a tool for weight training, or a door stopper, or a paperweight, and so forth. If you ask people to build towers of spaghetti and marshmallows children do better than adults because they are not limited by what they think a structure of a tower should look like. But all of this is mere idea generation. Is this really creativity?</p>
<p>There is another famous case of similar creativity among physicists which might shine a light on what the boundary between idea generation and creativity in research is: The undergrad theory of everything. It is a common problem for academics in physics to be tortured by undergrads who just invented &#8220;a new theory of physics which can unify gravity and quantum mechanics&#8221;. The problem here is that the undergrads do not yet have the proper knowledge to understand the intricate relationships among equations to understand what is permissible and what is not. They see the brick as a door stopper, when in fact a brick is used for building buildings and paving walkways. An important part of creativity is to understand what are bad ideas — some physics undergrads think it is just about idea generation. Do not get me wrong, idea generation is important, but it is not the most important part of creativity in academia.</p>
<p>This can go to an extreme if you work in theoretical physics and other fields where ideas are severely constrained by proper thought. There are so many bad ideas and so few good ideas that nobody really is coming up with anything good anymore. However, it would be ludicrous to say that people like Edward Witten are not creative because he did not come up with any good ideas since string theory. Similarly, Albert Einstein labored for decades trying to unify gravity and quantum mechanics only to come up with nothing. Bertrand Russel would often take a sheet of paper in the morning and work on a logical problem and write down whenever he found a useful thought. Most often the paper was still blank in the evening. So if you see creativity as idea generation, Albeit Einstein and others should be seen as failures compared to the children that churn out ideas. This demonstrates that the view of creativity as idea generation is problematic.</p>
<p>One thing that has to be understood when thinking about creativity is that some fields of thought are highly constrained in terms of which ideas are valid. To come by a good idea is a very lengthy and labor-intensive process. Other fields, like music, are very free in their expression and you can take any two ideas which do not seem to be related at all, mash them together, and with a little bit of work you can make it sound nice. I am exaggerating, but you get the idea.</p>
<p>Some fields, like machine translations, are now more and more constrained and good ideas need a team of people equipped with large computational resources that collaborate effectively for a long time come up with, and verify an idea which will yield a tiny improvement. One can expect the constrains on ideas increase exponentially with time in any given sub-field — just like it did in experimental physics. However, while finding valid ideas is becoming exponentially more difficult these fields also spawn new sub-fields as offspring. In these new fields, it will be very easy to come up with new ideas since — similarly to the music industry — anything is valid. As the field progresses the idea space becomes more and more constraint and finding valid ideas is much more important than generating just any idea. If you work in an area which is very constrained, you should have more compassion with yourself. Creativity is not just about generating some imaginative ideas — it is more about finding strange ideas which are still valid.</p>
<h2>&#8220;Not Coming Up with Good Ideas&#8221; is Essential for Creativity</h2>
<p>Expertise is important and a requirement for creativity. You need to be able to understand what are valid ideas and which are not. The next step is to loosen up the boundaries between ideas that may seem unconnected at first glance. Psychological research says, that once one has one of these strange ideas it is important to hammer on it over and over to exhaustion. The idea will reshape itself from one form to the next and eventually, you will probably fail to come up with something reasonable that works. Science says, that this is normal and the further insights are made unconsciously. After you give up an idea, your unconscious mind is still in the process of piecing together the puzzle and you might arrive at something useful over time. With the next puzzle piece put into place by your unconscious mind, you might be able to make some progress on an idea which might lead to a working valid idea.</p>
<p>Many researchers fail in the creative process because they do not understand it well. They feel like failures if their ideas fail. But the process of hammering on ideas and not making any progress is the first part of creativity. Only if you know all the ways that do not work can you come up with the solutions that nobody else is seeing. The second step is often abandoning the idea for some time. Some researchers feel that if an idea did not work out and you abandon the idea you also failed and it is a sign of not having creativity. But this step can be a critical element of creativity. It is important to have phases in which you do not think about an idea so your unconscious mind can make the connections that your conscious mind cannot see. The next step is to pick up a failed idea and try again. The unconscious insights are revealed in this way and you might quickly have a way to get an idea to work.</p>
<p>Another problem with the creative process is that researchers often work on a single idea. Instead, it is much more effective to work on many ideas. One idea for you to work on actively, while the other ideas are in the back of your mind and provide enough material for your unconscious mind to churn on. These ideas do not need to be totally different from each other, just different enough to not bother your conscious mind while you work on another idea.</p>
<p>I think to have a sane creative process, it is essential to acknowledge and even embrace this long-winded exhausting struggle with multiple rounds of failure as an essential part of creativity.</p>
<h2>Conclusion</h2>
<p>Researchers are often very harsh critics of themselves in terms of creativity. They do not come up with good ideas or with too few ideas or their ideas do not work out. But this does not mean that you are not creative. Some fields of research are very constraint in what ideas are valid and it is expected that the raw quantity of ideas in these fields is low. Furthermore, making no progress and abandoning an idea to work on something else are essential parts of creativity and should be celebrated and embraced. The next time you fail to make progress and think about abandoning an idea you should give yourself a pat on the back — you just reached the first milestone to come up with a great idea!</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2019/09/03/creativity-in-academia/">On Creativity in Academia</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2019/09/03/creativity-in-academia/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">796</post-id>	</item>
		<item>
		<title>Sparse Networks from Scratch: Faster Training without Losing Performance</title>
		<link>https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/</link>
					<comments>https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Thu, 11 Jul 2019 13:07:26 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Sparse Training]]></category>
		<guid isPermaLink="false">https://timdettmers.com/?p=774</guid>

					<description><![CDATA[<p>This blog post is about my work with Luke Zettlemoyer on fast training of neural networks which we keep sparse throughout training. We show that by developing an algorithm, sparse momentum, we can initialize a neural network with sparse random weights and train it to dense performance levels — all while doing just a single training run. Furthermore, If we use optimized sparse convolution algorithms, we can speed up training between 3.5x for VGG to 12x for Wide Residual Networks. This stands in stark contrast to computationally expensive methods which require repetitive prune-and-retrain cycles as used by the Lottery Ticket Hypothesis (Frankle and Carbin, 2019) and other work. Thus we show that training sparse networks to dense performance levels does not require "winning the initialization lottery" but can be done reliably from random weights if combined with a method that moves weights around the network in a smart way. We call the paradigm that maintains sparsity throughout training while maintaining dense performance levels sparse learning. While this work shows that sparse learning is possible, future work holds the promise to train larger and deep networks on more data while requiring the same or less computational resources as current dense networks.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">Sparse Networks from Scratch: Faster Training without Losing Performance</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>This blog post is about my work, <a href="https://arxiv.org/abs/1907.04840">Sparse Networks from Scratch: Faster Training without Losing Performance</a>, with <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a>&nbsp;on fast training of neural networks which we keep sparse throughout training. We show that by developing an algorithm, sparse momentum, we can initialize a neural network with sparse random weights and train it to dense performance levels — all while doing just a single training run. Furthermore, If we use optimized sparse convolution algorithms, we can speed up training between 3.5x for VGG to 12x for Wide Residual Networks. This stands in stark contrast to computationally expensive methods which require repetitive prune-and-retrain cycles as used by the Lottery Ticket Hypothesis (Frankle and Carbin, 2019) and other work. Thus we show that training sparse networks to dense performance levels does not require &#8220;winning the initialization lottery&#8221; but can be done reliably from random weights if combined with a method that moves weights around the network in a smart way. We call the paradigm that maintains sparsity throughout training while maintaining dense performance levels <em>sparse learning</em>. While this work shows that sparse learning is possible, future work holds the promise to train larger and deep networks on more data while requiring the same or less computational resources as current dense networks.</p>
<p><span id="more-774"></span></p>
<h2>Why Sparse Learning?</h2>
<p>A significant driver of progress in deep learning has been advances in computational resources. From 2010 to 2018 we saw an increase of 9700% in computational GPU performance. However, we can expect increases of just little more than 80% GPU performance in the next 5-8 years due to reaching the physical limits of semiconductor technology. What does a research world look like where we cannot make further improvements in computational power?</p>
<p>A glimpse of this comes from the natural language processing (NLP) community where pretrained language models like ELMO, GPT, BERT, GPT-2, Grover, and XL-Net dominate the entire field by outperforming other methods on most NLP tasks. These models are often rather simple: You train them on lots of documents, and the task is mainly to predict a word given a sequence of other words&nbsp;— a bit like doing a fill-in-the-blank puzzle. The catch? These models are so big that they take well in excess of 100 GPUs hours to train. This is particularly frustrating for academic researchers that want to understand these models but are unable to do so because they lack the computational resources that big companies have. To truly understand these massive pretrained language models, a primary goal should be to democratize the training of these models by developing more resourceful training procedures.</p>
<p>One way to achieve this is to look at the human brain for inspiration. The human brain consumes 1/10th of the energy of a GPU but is 10^9 times more powerful. What makes the brain so computational efficient? <a href="https://timdettmers.com/2015/07/27/brain-vs-deep-learning-singularity/">There are many reasons</a>, but one reason is<em> sparsity</em>.</p>
<p>It has been found that the more neurons a primate brain has the fewer connections does the average neuron make with all other neurons (Herculano-Houzel et al., 2010). This is very much contrary to how we design deep neural networks, which is to connect every new neuron in a layer with all neurons in the previous layer. We already understand how to compress a fully trained dense network to a sparse network (Han et al., 2015), but there has been little work on how to do this successfully if one starts from a sparse network which we keep sparse during training. How do we do this?</p>
<h2>Sparse Momentum: An Efficient Way to Train Sparse Networks</h2>
<p>This section explains the sparse momentum algorithm from intutiton up to the full algorithm.</p>
<p><figure id="attachment_779" aria-describedby="caption-attachment-779" style="width: 1096px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?ssl=1"><img data-attachment-id="779" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/sparse_momentum/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1096%2C528&amp;ssl=1" data-orig-size="1096,528" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sparse Momentum Dettmers &#038; Zettlemoyer 2019" data-image-description="&lt;p&gt;Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.&lt;/p&gt;
" data-image-caption="&lt;p&gt;Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=300%2C145&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1024%2C493&amp;ssl=1" class="wp-image-779 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1096%2C528&#038;ssl=1" alt="Sparse Momentum determines where to grow new weights in a sparse network by looking at the weighted average of recent gradients (momentum) to find weights and layers which reduce the error consistently. (1) We determine the importance of each layer according to the mean momentum magnitude. (2) For each layer, we remove the 50\% of the smallest weights. (3) We then redistribute the weights across layers according to layer importance. Within a layer we grow weights where the momentum magnitude is large." width="1096" height="528" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?w=1096&amp;ssl=1 1096w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=768%2C370&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;ssl=1 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-779" class="wp-caption-text">Figure 1: Sparse Momentum determines where to grow new weights in a sparse network by looking at the weighted average of recent gradients (momentum) to find weights and layers which reduce the error consistently. (1) We determine the importance of each layer according to the mean momentum magnitude. (2) For each layer, we remove the 50% of the smallest weights. (3) We then redistribute the weights across layers according to layer importance. Within a layer we grow weights where the momentum magnitude is large.</figcaption></figure></p>
<h3>What is the Main Quality of Good Sparse Learning Algorithms?</h3>
<p>In sparse learning, the most important thing is to use every single weight in a neural network as effectively as possible. If you define &#8220;effectiveness&#8221; as &#8220;reducing the error,&#8221; then we have an obvious perspective on how we can proceed. We need to find a measure which describes how effective a weight is at reducing the error and remove all weights which do not. Once we removed weights, we want to regrow new weights in locations which we think are promising at reducing the error in the future.</p>
<p>If we look at the gradient of the error with respect to the weight, we actually have precisely such a measure. However, if we look at successive gradients, we find that gradients can wildly oscillate. For example if you have a neural network which classifies handwritten digits 0 to 9 then a weight might be good at detecting a straight line at the top and it might help to reduce the error for the numbers 5, 7 but then it might not help or even be detrimental for numbers 0, 1, 2, 3, 6, 8, 9. Instead, a weight which detects a curvy pattern in the top right might help for 0, 2, 3, 8, 9 and as such we would expect that this weight reduces the error more consistently over time than the &#8220;straight line at the top&#8221; weight. How can we detect such promising weights in a neural network automatically?</p>
<h3>Momentum: Finding Weights that Reduce the Error Consistently</h3>
<p>If you take the north pole to be a local minimum and a compass needle the gradient towards the local minimum, then you can simulate stochastic gradient descent updates by shaking the compass wildly to spin the compass needle. With every time the needle passes the north pole it will slow down and line-up more and more with the north pole, however, due to the spin it will still &#8220;overshoot&#8221; that direction. So it might be unclear where the north pole is from two or three measurements while the needle is still moving back and forth. However, if you take the average directions&nbsp;— one time the needle is a bit to the left of the north pole, another time it is more to the right&nbsp;— then these deviations cancel out, and you will immediately get a direction which is very close to the real north pole.</p>
<p>This is the main idea behind the momentum optimization technique: We average successive gradients to get a better estimate of the direction of the local minimum. Similarly to the compass needle, which gets more and more accurate over time as it slows down, we want to weight more recent gradient directions in stochastic gradient descent more highly. One way to do this is to assign a weighted average where we assign a much larger weight to the current gradient and a small weight to the previous gradients&nbsp;— this is called exponential smoothing. Through exponential smoothing the gradients of the weight we receive a weighted gradient matrix — this matrix is the momentum matrix which gives momentum optimization its name. With this measure, we can identify which are the weights which reduce the error consistently.</p>
<h3>Redistributing Weights: The Mean Momentum Magnitude of a Layer</h3>
<p>From here we make the first important observation for our sparse momentum algorithm: If the momentum of a weight indicates how much it reduces the error consistently, then the mean momentum magnitude of all the weights in a layer should indicate how much each layer is reducing the error on average. We take the magnitude because two different weights might consistently go into a negative direction and a positive direction. By taking the mean momentum magnitude of layers, we can easily compare how effective the average weight in each layer is. This enables to say, for example, that a weight in a convolutional layer A is on average 1/3 as effective at reducing the error as the average weight in fully connected layer B, or vice versa. This method enables us to redistribute weights effectively: if we find &#8220;useless&#8221; weights, we now know precisely in which layer to put it&nbsp;— but where to put them exactly within a layer?</p>
<h3>Which Weights Should be Removed? Where to Regrow them?</h3>
<p>The next two problems are more straightforward: Which are the most useless weights? Where do we grow weights within a layer? The first problem is a common problem in neural network compression research, where one often prunes the weights with the smallest magnitude. Why does this make sense? If we assume all weights receive on average inputs of similar magnitude&nbsp;— a reasonable assumption if one uses batch normalization&nbsp;— then weights with small magnitudes make the smallest difference in the activation of a neuron. As such, removing them should change the predictive performance of our networks by the smallest amount.</p>
<p>Once we removed weights and redistributed them to weight-effective layers as measured by the mean momentum magnitude of a layer, we need to decide where exactly to grow them within a layer. One possible solution becomes apparent if we ask: &#8220;Which two unconnected neurons would reduce the error consistently if we connect them?&#8221; The answer to this question would again point to the momentum magnitude. This time, however, we want to look at the momentum magnitude of &#8220;missing&#8221; or zero-valued weights, that is, we want to look at those weights which have been excluded from training before. Thus we grow weights in locations where missing weights have the largest momentum magnitude. This completes the sparse momentum algorithm, which depicted in Figure 1.</p>
<h2>Results</h2>
<p>The results are quite impressive! We compared against compression algorithms on MNIST, where sparse momentum outperforms most other methods. This is a pretty good result given that compression methods start from a dense network and usually retrain repetitively while we train a sparse network from scratch! Another impressive result is that we can match or even exceed the performance of dense networks by using 20% of weights (80% sparsity). On CIFAR-10, we compare against Single-shot Network Pruning which is designed for simplicity and not performance&nbsp;— so it is not surprising that sparse momentum does better. However, what is interesting is that we can train both VGG16-D (a version of VGG16 with two fully connected layers) and Wide Residual Network (WRN) 16-10 (16 layers deep and very wide WRN) to dense performance levels with just 5% of weights. For other networks, sparse momentum comes close to dense performance levels. Furthermore, as I will show later, with an optimized sparse convolution algorithm, we would be able to train a variety of networks to yield the same performance levels while training between 3.0-5.6x faster!</p>
<p>&nbsp;</p>
<p><figure id="attachment_800" aria-describedby="caption-attachment-800" style="width: 1830px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?ssl=1"><img data-attachment-id="800" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/sparse_momentum_vs_compression/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?fit=1830%2C697&amp;ssl=1" data-orig-size="1830,697" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="sparse_momentum_vs_compression" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?fit=300%2C114&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?fit=1024%2C390&amp;ssl=1" class="wp-image-800 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?resize=1830%2C697&#038;ssl=1" alt="Sparse Momentum results compared to neural network compression methods on MNIST for LeNet-300-100 and LeNet-5 Caffe." width="1830" height="697" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?w=1830&amp;ssl=1 1830w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?resize=300%2C114&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?resize=768%2C293&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum_vs_compression.png?resize=1024%2C390&amp;ssl=1 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-800" class="wp-caption-text">Sparse Momentum results compared to neural network compression methods on MNIST for LeNet-300-100 and LeNet-5 Caffe.</figcaption></figure></p>
<p>&nbsp;</p>
<p><figure id="attachment_801" aria-describedby="caption-attachment-801" style="width: 1024px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?ssl=1"><img data-attachment-id="801" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/imagenet_results/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?fit=1062%2C388&amp;ssl=1" data-orig-size="1062,388" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="imagenet_results" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?fit=300%2C110&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?fit=1024%2C374&amp;ssl=1" class="wp-image-801 size-large" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?resize=1024%2C374&#038;ssl=1" alt="ImageNet results for Sparse momentum and related methods. For the models that are not fully sparse, the first convolution and all downsample residual connections are dense from the start of training. In the fully sparse setting, all layers are sparse. Sparse momentum works better than other methods and works almost equally well if all the weights are sparse. This indicates that sparse momentum is efficient at finding important layers which require a high density." width="1024" height="374" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?resize=1024%2C374&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?resize=300%2C110&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?resize=768%2C281&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/imagenet_results.png?w=1062&amp;ssl=1 1062w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-801" class="wp-caption-text">ImageNet results for Sparse momentum and related methods. For the models that are not fully sparse, the first convolution and all downsample residual connections are dense from the start of training. In the fully sparse setting, all layers are sparse. Sparse momentum works better than other methods and works almost equally well if all the weights are sparse. This indicates that sparse momentum is efficient at finding important layers which require a high density.</figcaption></figure></p>
<p>On ImageNet, we are not able to reach dense performance levels, which indicates that there is room to improve sparse momentum. However, we can demonstrate that sparse momentum has a clear lead compared to other methods that maintain sparse weights throughout training.</p>
<h3>Speedups</h3>
<p>The main promise of sparse learning was to accelerate training&nbsp;— were we successful? Yes&nbsp;— and no. Sparse momentum accelerates training efficiently if we measure possible speedups for sparse convolution, but since sparse networks were only very recently used for training, no optimized sparse convolution algorithms exist for the GPU&nbsp;— at least not for the fine-grained sparse patterns of weights as exhibited by sparse momentum.</p>
<p>As such, we divide the speedups into two groups: Possible speedups which could be achieved if sparse convolution algorithms would exist, and speedups which we can achieve today with standard dense convolutional algorithms. How can dense convolutions help for sparse networks?</p>
<p>If we look at the sparsity pattern of our network, we have the case where a convolutional channel is entirely empty&nbsp;— a convolutional filter full of zeros! If this happens, we can remove the channel from the computation without changing the results of the convolution and thus gain speedups.</p>
<p>&nbsp;</p>
<p><figure id="attachment_799" aria-describedby="caption-attachment-799" style="width: 1024px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?ssl=1"><img data-attachment-id="799" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/dense_equivalents/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?fit=1096%2C468&amp;ssl=1" data-orig-size="1096,468" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dense_equivalents" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?fit=300%2C128&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?fit=1024%2C437&amp;ssl=1" class="wp-image-799 size-large" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?resize=1024%2C437&#038;ssl=1" alt="" width="1024" height="437" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?resize=1024%2C437&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?resize=300%2C128&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?resize=768%2C328&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/dense_equivalents.png?w=1096&amp;ssl=1 1096w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-799" class="wp-caption-text">Sparse momentum can replicate dense performance levels for a range of networks with a fraction of the weights thus leading to speedups.</figcaption></figure></p>
<p>However, if we look at the speedups, we see there is a marked difference between sparse convolution and dense convolution speedups. This clearly shows the need for optimized sparse convolution algorithms for the GPU.</p>
<h2>Why does Sparse Learning Work?</h2>
<p>Some of our sparse networks trained with sparse momentum matched the performance levels of dense networks with just 5% of weights. What makes these 5% of weights so efficient that they can match a neural network with 20 times as many weights?</p>
<p>To look into this question, we looked at how the features of sparse networks compare to dense networks. Low-level features might include things like edge detectors. Mid-level features might be things like wheels, noses, eyes, paws. High-level features might be the &#8220;face&#8221; of a car, a cat face, a fridge door, and so forth.</p>
<p>To reduce features to numbers we look at convolutional channels&nbsp;— the equivalent to a &#8220;neuron&#8221; in a convolutional network&nbsp;— and how useful the channel is to classes in the dataset. Edge detectors should be useful to almost all classes in the dataset&nbsp;— in other words, they should have a low level of class-specialization. Mid-level features like eyes should be useful to—some classes such as cats, dogs, and humans. High-level features should be useful to a few selected classes&nbsp;— they are highly class-specialized.</p>
<p><figure id="attachment_778" aria-describedby="caption-attachment-778" style="width: 1016px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?ssl=1"><img data-attachment-id="778" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/sparse_features/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?fit=1016%2C1098&amp;ssl=1" data-orig-size="1016,1098" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="sparse_features" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?fit=278%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?fit=948%2C1024&amp;ssl=1" class="wp-image-778 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?resize=1016%2C1098&#038;ssl=1" alt="Figure 6: Class-specialization histograms for sparse and dense networks for AlexNet, VGG16 and WRN 28-2." width="1016" height="1098" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?w=1016&amp;ssl=1 1016w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?resize=278%2C300&amp;ssl=1 278w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?resize=768%2C830&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_features.png?resize=948%2C1024&amp;ssl=1 948w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-778" class="wp-caption-text">Figure 6: Class-specialization histograms for sparse and dense networks for AlexNet, VGG16 and WRN 28-2.</figcaption></figure></p>
<p>What we find is that on average, sparse networks learn features which are useful to a broader range of classes&nbsp;— they learn more general features. This might be a possible explanation of why sparse networks can match the performance of dense networks with as few as 5% weights.</p>
<h2>The Future of Sparse Learning</h2>
<p>I believe sparse learning has a very bright future because (1) GPUs will stagnate in performance over the next years, (2) specialized processors for sparse workloads, Graphcore processors, are around the corner. Graphcore processors store an entire network in its 300 MB cache and accelerate it by a factor of roughly 100x. This means, if we can compress a network to 300 MB during training, then we will have 100x faster training overall. Training a ResNet-50 on ImageNet would then take only roughly 15 minutes using one Graphcore processor. With sparse learning, the 300 MB limit will be in reach without a problem.</p>
<p>My prediction is that the first research team that can train a sparse neural network on a Graphcore processor successfully will unlock an entirely new level of artificial intelligence.</p>
<p>Besides this, another challenge is to apply sparse learning algorithms to natural language processing (NLP). Unsurprisingly, my experimentation on transformers for natural language processing tasks show that sparse learning is much more difficult in NLP compared to computer vision&nbsp;— lots of work to do!</p>
<h2>Try Sparse Momentum with Your Own Model in 10 Lines of Code!</h2>
<p><figure id="attachment_781" aria-describedby="caption-attachment-781" style="width: 766px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?ssl=1"><img data-attachment-id="781" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/code/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?fit=766%2C533&amp;ssl=1" data-orig-size="766,533" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="code" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?fit=300%2C209&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?fit=766%2C533&amp;ssl=1" class="wp-image-781 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?resize=766%2C533&#038;ssl=1" alt="Figure 7: Example of a generic sparse learning script which you can use for your own model. With my sparselearning library it is easy to use sparse momentum: (1) Import the library, (2) add the parser options, (3) wrap your model with the Masking class, (4) apply mask instead of optimizer, (5) apply sparse momentum at the end of epoch. The library is also easily extendable with your own sparse learning algorithms for growth, pruning, or redistribution -- all it takes is a few lines of code!" width="766" height="533" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?w=766&amp;ssl=1 766w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/code.png?resize=300%2C209&amp;ssl=1 300w" sizes="(max-width: 766px) 100vw, 766px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-781" class="wp-caption-text">Figure 7: Example of a generic sparse learning script which you can use for your own model. With my sparselearning library it is easy to use sparse momentum: (1) Import the library, (2) add the parser options, (3) wrap your model with the Masking class, (4) apply mask instead of optimizer, (5) apply sparse momentum at the end of epoch. The library is also easily extendable with your own sparse learning algorithms for growth, pruning, or redistribution&nbsp;— all it takes is a few lines of code!</figcaption></figure></p>
<p>To make sparse learning accessible to everyone I developed a sparse learning library which allows the easy application of existing algorithms like sparse momentum to your own models&nbsp;— it can be done in less than 10 lines of code. The library is also designed to make it very easy to add your own sparse learning methods. You find my <a href="https://github.com/TimDettmers/sparse_learning">sparse learning library</a> on GitHub.</p>
<h3>Questions?</h3>
<p>For questions, I prefer if you post them below if they are simple and straightforward. If you have a more formal question regarding our work that requires careful answers, you can post an the question as <a href="https://github.com/TimDettmers/sparse_learning/issues">a GitHub issue</a>&nbsp;— I will try to answer as timely as possible.</p>
<h4>Acknowledgements</h4>
<p>I thank Luke Zettlemoyer for feedback on an early draft of this blog post.</p>
<h3>References</h3>
<p>Frankle, J. and Carbin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural networks. In <em>ICLR 2019</em>.</p>
<p>Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning both weights and connections for efficient neural network. In <em>Advances in neural information processing systems</em>, pages<br />
1135—1143.</p>
<p>Herculano-Houzel, S., Mota, B., Wong, P., and Kaas, J.H. (2010). Connectivity-driven white matter scaling and folding in primate cerebral cortex. In&nbsp;<em>Proceedings of the National Academy of Sciences of the United States of America</em>, 107 44:19008—13.</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">Sparse Networks from Scratch: Faster Training without Losing Performance</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/feed/</wfw:commentRss>
			<slash:comments>38</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">774</post-id>	</item>
		<item>
		<title>A Full Hardware Guide to Deep Learning</title>
		<link>https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/</link>
					<comments>https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Sun, 16 Dec 2018 18:25:41 +0000</pubDate>
				<category><![CDATA[Hardware]]></category>
		<category><![CDATA[AMD]]></category>
		<category><![CDATA[CPU]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Intel]]></category>
		<category><![CDATA[PCIe Lanes]]></category>
		<guid isPermaLink="false">https://timdettmers.wordpress.com/?p=121</guid>

					<description><![CDATA[<p>Here I will guide you step by step through the hardware you will need for a cheap high performance system for deep learning.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">A Full Hardware Guide to Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="eplus-Jtk1uQ">Deep Learning is very computationally intensive, so you will need a fast CPU with many cores, right? Or is it maybe wasteful to buy a fast CPU? One of the worst things you can do when building a deep learning system is to waste money on hardware that is unnecessary. Here I will guide you step by step through the hardware you will need for a cheap high-performance system.</p>



<span id="more-121"></span>



<p class="eplus-UTx6Bt">Over the years, I build a total of 7 different deep learning workstations and despite careful research and reasoning, I made my fair share of mistake in selecting hardware parts. In this guide, I want to share my experience that I gained over the years so that you do not make the same mistakes that I did before.</p>



<p class="eplus-LmAMuN">The blog post is ordered by mistake severity. This means the mistakes where people usually waste the most money come first.</p>




<h2><strong>GPU</strong></h2>
<p>This blog post assumes that you will use a GPU for deep learning. If you are building or upgrading your system for deep learning, it is not sensible to leave out the GPU. The GPU is just the heart of deep learning applications – the improvement in processing speed is just too huge to ignore.</p>
<p>I talked at length about GPU choice in <a href="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/">my GPU recommendations blog post</a>, and the choice of your GPU is probably the most critical choice for your deep learning system. There are three main mistakes that you can make when choosing a GPU: (1) bad cost/performance, (2) not enough memory, (3) poor cooling.</p>
<p>For good cost/performance, I generally recommend an RTX 2070 or an RTX 2080 Ti. If you use these cards you should use 16-bit models. Otherwise, GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are fair choices and you can use these GPUs with 32-bit (but not 16-bit).</p>
<p>Be careful about the memory requirements when you pick your GPU. RTX cards, which can run in 16-bits, can train models which are twice as big with the same memory compared to GTX cards. As such RTX cards have a memory advantage and picking RTX cards and learn how to use 16-bit models effectively will carry you a long way. In general, the requirements for memory are roughly the following:</p>
<ul>
<li>Research that is hunting state-of-the-art scores: &gt;=11 GB</li>
<li>Research that is hunting for interesting architectures: &gt;=8 GB</li>
<li>Any other research: 8 GB</li>
<li>Kaggle: 4 &#8211; 8 GB</li>
<li>Startups: 8 GB (but check the specific application area for model sizes)</li>
<li>Companies: 8 GB for prototyping, &gt;=11 GB for training</li>
</ul>
<p>Another problem to watch out for, especially if you buy multiple RTX cards is cooling. If you want to stick GPUs into PCIe slots which are next to each other you should make sure that you get GPUs with a blower-style fan. Otherwise you might run into temperature issues and your GPUs will be slower (about 30%) and die faster.</p>
<figure id="attachment_124" aria-describedby="caption-attachment-124" style="width: 700px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg"><img data-attachment-id="124" data-permalink="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/suspectlineup/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=3264%2C1836&amp;ssl=1" data-orig-size="3264,1836" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;U9200&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1403265762&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.13&quot;,&quot;iso&quot;:&quot;122&quot;,&quot;shutter_speed&quot;:&quot;0.016666&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="suspectlineup" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=1024%2C576&amp;ssl=1" class="wp-image-124 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?resize=700%2C394" alt="" width="700" height="394" border="None" data-recalc-dims="1" /></a><figcaption id="caption-attachment-124" class="wp-caption-text"><strong>Suspect line-up</strong><br />Can you identify the hardware part which is at fault for bad performance? One of these GPUs? Or maybe it is the fault of the CPU after all?</figcaption></figure>
<h2>RAM</h2>
<p>The main mistakes with RAM is to buy RAM with a too high clock rate. The second mistake is to buy not enough RAM to have a smooth prototyping experience.</p>
<h3>Needed RAM Clock Rate</h3>
<p>RAM clock rates are marketing stints where RAM companies lure you into buying &#8220;faster&#8221; RAM which actually yields little to no performance gains. This is best explained by &#8220;<a href="https://www.youtube.com/watch?v=D_Yt4vSZKVk">Does RAM speed REALLY matter?</a>&#8221; video on RAM von Linus Tech Tips.</p>
<p>Furthermore, it is important to know that RAM speed is pretty much irrelevant for fast CPU RAM-&gt;GPU RAM transfers. This is so because (1) if you used <a href="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers">pinned memory</a>, your mini-batches will be transferred to the GPU without involvement from the CPU, and (2) if you do not use pinned memory the performance gains of fast vs slow RAMs is about 0-3% &#8212; spend your money elsewhere!</p>
<h3>RAM Size</h3>
<p>RAM size does not affect deep learning performance. However, it might hinder you from executing your GPU code comfortably (without swapping to disk). You should have enough RAM to comfortable work with your GPU. This means you should have at least the amount of RAM that matches your biggest GPU. For example, if you have a Titan RTX with 24 GB of memory you should have at least 24 GB of RAM. However, if you have more GPUs you do not necessarily need more RAM.</p>
<p>The problem with this &#8220;match largest GPU memory in RAM&#8221; strategy is that you might still fall short of RAM if you are processing large datasets. The best strategy here is to match your GPU and if you feel that you do not have enough RAM just buy some more.</p>
<p>A different strategy is influenced by psychology: Psychology tells us that concentration is a resource that is depleted over time. RAM is one of the few hardware pieces that allows you to conserve your concentration resource for more difficult programming problems. Rather than spending lots of time on circumnavigating RAM bottlenecks, you can invest your concentration on more pressing matters if you have more RAM.  With a lot of RAM you can avoid those bottlenecks, save time and increase productivity on more pressing problems. Especially in Kaggle competitions, I found additional RAM very useful for feature engineering. So if you have the money and do a lot of pre-processing then additional RAM might be a good choice. So with this strategy, you want to have more, cheap RAM now rather than later.</p>
<h2>CPU</h2>
<p>The main mistake that people make is that people pay too much attention to PCIe lanes of a CPU. You should not care much about PCIe lanes. Instead, just look up if your CPU and motherboard combination supports the number of GPUs that you want to run. The second most common mistake is to get a CPU which is too powerful.</p>
<h3>CPU and PCI-Express</h3>
<p>People go crazy about PCIe lanes! However, the thing is that it has almost no effect on deep learning performance. If you have a single GPU, PCIe lanes are only needed to transfer data from your CPU RAM to your GPU RAM quickly. However, an ImageNet batch of 32 images (32x225x225x3) and 32-bit needs 1.1 milliseconds with 16 lanes, 2.3 milliseconds with 8 lanes, and 4.5 milliseconds with 4 lanes. These are theoretic numbers, and in practice you often see PCIe be twice as slow — but this is still lightning fast! PCIe lanes often have a latency in the nanosecond range and thus latency can be ignored.</p>
<p>Putting this together we have for an ImageNet mini-batch of 32 images and a ResNet-152 the following timing:</p>
<ul>
<li>Forward and backward pass: 216 milliseconds (ms)</li>
<li>16 PCIe lanes CPU-&gt;GPU transfer: About 2 ms (1.1 ms theoretical)</li>
<li>8 PCIe lanes CPU-&gt;GPU transfer: About 5 ms (2.3 ms)</li>
<li>4 PCIe lanes CPU-&gt;GPU transfer: About 9 ms (4.5 ms)</li>
</ul>
<p>Thus going from 4 to 16 PCIe lanes will give you a performance increase of roughly 3.2%. However, if you use <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">PyTorch&#8217;s data loader</a> with pinned memory you gain exactly 0% performance. So do not waste your money on PCIe lanes if you are using a single GPU!</p>
<p>When you select CPU PCIe lanes and motherboard PCIe lanes make sure that you select a combination which supports the desired number of GPUs. If you buy a motherboard that supports 2 GPUs, and you want to have 2 GPUs eventually, make sure that you buy a CPU that supports 2 GPUs, but do not necessarily look at PCIe lanes.</p>
<h3>PCIe Lanes and Multi-GPU Parallelism</h3>
<p>Are PCIe lanes important if you train networks on multiple GPUs with data parallelism? I have <a href="https://arxiv.org/abs/1511.04561">published a paper on this at ICLR2016</a>, and I can tell you if you have 96 GPUs then PCIe lanes are really important. However, if you have 4 or fewer GPUs this does not matter much. If you parallelize across 2-3 GPUs, I would not care at all about PCIe lanes. With 4 GPUs, I would make sure that I can get a support of 8 PCIe lanes per GPU (32 PCIe lanes in total). Since almost nobody runs a system with more than 4 GPUs as a rule of thumb: Do not spend extra money to get more PCIe lanes per GPU — it does not matter!</p>
<h3>Needed CPU Cores</h3>
<p>To be able to make a wise choice for the CPU we first need to understand the CPU and how it relates to deep learning. What does the CPU do for deep learning? The CPU does little computation when you run your deep nets on a GPU. Mostly it (1) initiates GPU function calls, (2) executes CPU functions.</p>
<p>By far the most useful application for your CPU is data preprocessing. There are two different common data processing strategies which have different CPU needs.</p>
<p>The first strategy is preprocessing while you train:</p>
<p>Loop:</p>
<ol>
<li>Load mini-batch</li>
<li>Preprocess mini-batch</li>
<li>Train on mini-batch</li>
</ol>
<p>The second strategy is preprocessing before any training:</p>
<ol>
<li>Preprocess data</li>
<li>Loop:
<ol>
<li>Load preprocessed mini-batch</li>
<li>Train on mini-batch</li>
</ol>
</li>
</ol>
<p>For the first strategy, a good CPU with many cores can boost performance significantly. For the second strategy, you do not need a very good CPU. For the first strategy, I recommend a minimum of 4 threads per GPU &#8212; that is usually two cores per GPU. I have not done hard tests for this, but you should gain about 0-5% additional performance per additional core/GPU.</p>
<p>For the second strategy, I recommend a minimum of 2 threads per GPU &#8212; that is usually one core per GPU. You will not see significant gains in performance when you have more cores if you are using the second strategy.</p>
<h3>Needed CPU Clock Rate (Frequency)</h3>
<p>When people think about fast CPUs they usually first think about the clock rate.  4GHz is better than 3.5GHz, or is it? This is generally true for comparing processors with the same architecture, e.g. “Ivy Bridge”, but it does not compare well between processors. Also, it is not always the best measure of performance.</p>
<p>In the case of deep learning there is very little computation to be done by the CPU: Increase a few variables here, evaluate some Boolean expression there, make some function calls on the GPU or within the program – all these depend on the CPU core clock rate.</p>
<p>While this reasoning seems sensible, there is the fact that the CPU has 100% usage when I run deep learning programs, so what is the issue here? I did some CPU core rate underclocking experiments to find out.</p>
<figure id="attachment_161" aria-describedby="caption-attachment-161" style="width: 804px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png"><img data-attachment-id="161" data-permalink="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/cpu_underclocking/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=603%2C406&amp;ssl=1" data-orig-size="603,406" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cpu_underclocking" data-image-description="" data-image-caption="&lt;p&gt;CPU underclocking on MNIST and ImageNet: Performance is measured as time taken on 100 epochs MNIST or half an epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a base line for each CPU. For comparison: Upgrading from a GTX 580 to a GTX Titan is about +20% performance; from GTX Titan to GTX 980 another +30% performance; GPU overclocking yields about +5% performance for any GPU&lt;/p&gt;
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=300%2C202&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=603%2C406&amp;ssl=1" class="wp-image-161" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?resize=804%2C541" alt="CPU underclocking on MNIST and ImageNet: Performance is measured as time taken on 100 epochs MNIST or half an epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a base line for each CPU. For comparison: Upgrading from a GTX 580 to a GTX Titan is about +20% performance; from GTX Titan to GTX 980 another +30% performance; GPU overclocking yields about +5% performance for any GPU" width="804" height="541" data-recalc-dims="1" /></a><figcaption id="caption-attachment-161" class="wp-caption-text"><strong>CPU underclocking on MNIST and ImageNet</strong>: Performance is measured as time taken on 200 epochs MNIST or a quarter epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a baseline for each CPU. For comparison: Upgrading from a GTX 680 to a GTX Titan is about +15% performance; from GTX Titan to GTX 980 another +20% performance; GPU overclocking yields about +5% performance for any GPU</figcaption></figure>
<p style="text-align: justify;">Note that these experiments are on a hardware that is dated, however, these results should still be the same for modern CPUs/GPUs.</p>
<h2 style="text-align: justify;"><strong>Hard drive/SSD</strong></h2>
<p>The hard drive is not usually a bottleneck for deep learning. However, if you do stupid things it will hurt you: If you read your data from disk when they are needed (blocking wait) then a 100 MB/s hard drive will cost you about 185 milliseconds for an ImageNet mini-batch of size 32 — ouch! However, if you asynchronously fetch the data before it is used (for example torch vision loaders), then you will have loaded the mini-batch in 185 milliseconds while the compute time for most deep neural networks on ImageNet is about 200 milliseconds. Thus you will not face any performance penalty since you load the next mini-batch while the current is still computing.</p>
<p>However, I recommend an SSD for comfort and productivity: Programs start and respond more quickly, and pre-processing with large files is quite a bit faster. If you buy an NVMe SSD you will have an even smoother experience when compared to a regular SSD.</p>
<p>Thus the ideal setup is to have a large and slow hard drive for datasets and an SSD for productivity and comfort.</p>
<h2 style="text-align: justify;"><strong>Power supply unit (PSU)</strong></h2>
<p>Generally, you want a PSU that is sufficient to accommodate all your future GPUs. GPUs typically get more energy efficient over time; so while other components will need to be replaced, a PSU should last a long while so a good PSU is a good investment.</p>
<p>You can calculate the required watts by adding up the watt of your CPU and GPUs with an additional 10% of watts for other components and as a buffer for power spikes. For example, if you have 4 GPUs with each 250 watts TDP and a CPU with 150 watts TDP, then you will need a PSU with a minimum of 4&#215;250 + 150 + 100 = 1250 watts. I would usually add another 10% just to be sure everything works out, which in this case would result in a total of 1375 Watts. I would round up in this case an get a 1400 watts PSU.</p>
<p>One important part to be aware of is that even if a PSU has the required wattage, it might not have enough PCIe 8-pin or 6-pin connectors. Make sure you have enough connectors on the PSU to support all your GPUs!</p>
<p>Another important thing is to buy a PSU with high power efficiency rating – especially if you run many GPUs and will run them for a longer time.</p>
<p>Running a 4 GPU system on full power (1000-1500 watts) to train a convolutional net for two weeks will amount to 300-500 kWh, which in Germany – with rather high power costs of 20 cents per kWh – will amount to 60-100€ ($66-111). If this price is for a 100% efficiency, then training such a net with an 80% power supply would increase the costs by an additional 18-26€ – ouch! This is much less for a single GPU, but the point still holds – spending a bit more money on an efficient power supply makes good sense.</p>
<p>Using a couple of GPUs around the clock will significantly increase your carbon footprint and it will overshadow transportation (mainly airplane) and other factors that contribute to your footprint. If you want to be responsible, please consider going <a href="https://wp.nyu.edu/ml2/carbon-neutral-lab/">carbon neutral like the NYU Machine Learning for Language Group (ML2)</a> — it is easy to do, cheap, and should be standard for deep learning researchers.</p>
<h2>CPU and GPU Cooling</h2>
<p>Cooling is important and it can be a significant bottleneck which reduces performance more than poor hardware choices do. You should be fine with a standard heat sink or all-in-one (AIO) water cooling solution for your CPU, but what for your GPU you will need to make special considerations.</p>
<h3>Air Cooling GPUs</h3>
<p>Air cooling is safe and solid for a single GPU or if you have multiple GPUs with space between them (2 GPUs in a 3-4 GPU case). However, one of the biggest mistakes can be made when you try to cool 3-4 GPUs and you need to think carefully about your options in this case.</p>
<p>Modern GPUs will increase their speed – and thus power consumption – up to their maximum when they run an algorithm, but as soon as the GPU hits a temperature barrier – often 80 °C – the GPU will decrease the speed so that the temperature threshold is not breached. This enables the best performance while keeping your GPU safe from overheating.</p>
<p>However, typical pre-programmed schedules for fan speeds are badly designed for deep learning programs, so that this temperature threshold is reached within seconds after starting a deep learning program. The result is a decreased performance (0-10%) which can be significant for multiple GPUs (10-25%) where the GPU heat up each other.</p>
<p>Since NVIDIA GPUs are first and foremost gaming GPUs, they are optimized for Windows. You can change the fan schedule with a few clicks in Windows, but not so in Linux, and as most deep learning libraries are written for Linux this is a problem.</p>
<p>The only option under Linux is to use to set a configuration for your Xorg server (Ubuntu) where you set the option &#8220;coolbits&#8221;. This works very well for a single GPU, but if you have multiple GPUs where some of them are headless, i.e. they have no monitor attached to them, you have to emulate a monitor which is hard and hacky. I tried it for a long time and had frustrating hours with a live boot CD to recover my graphics settings – I could never get it running properly on headless GPUs.</p>
<p>The most important point of consideration if you run 3-4 GPUs on air cooling is to pay attention to the fan design. The &#8220;blower&#8221; fan design pushes the air out to the back of the case so that fresh, cooler air is pushed into the GPU. Non-blower fans suck in air in the vincity of the GPU and cool the GPU. However, if you have multiple GPUs next to each other then there is no cool air around and GPUs with non-blower fans will heat up more and more until they throttle themselves down to reach cooler temperatures. Avoid non-blower fans in 3-4 GPU setups at all costs.</p>
<h3>Water Cooling GPUs For Multiple GPUs</h3>
<p>Another, more costly, and craftier option is to use water cooling. I do not recommend water cooling if you have a single GPU or if you have space between your two GPUs (2 GPUs in 3-4 GPU board). However, water cooling makes sure that even the beefiest GPU stay cool in a 4 GPU setup which is not possible when you cool with air. Another advantage of water cooling is that it operates much more silently, which is a big plus if you run multiple GPUs in an area where other people work. Water cooling will cost you about $100 for each GPU and some additional upfront costs (something like $50). Water cooling will also require some additional effort to assemble your computer, but there are many detailed guides on that and it should only require a few more hours of time in total. Maintenance should not be that complicated or effortful.</p>
<h3>A Big Case for Cooling?</h3>
<p>I bought large towers for my deep learning cluster, because they have additional fans for the GPU area, but I found this to be largely irrelevant: About 2-5 °C decrease, not worth the investment and the bulkiness of the cases. The most important part is really the cooling solution directly on your GPU — do not select an expensive case for its GPU cooling capability. Go cheap here. The case should fit your GPUs but thats it!</p>
<h3>Conclusion Cooling</h3>
<p>So in the end it is simple: For 1 GPU air cooling is best. For multiple GPUs, you should get blower-style air cooling and accept a tiny performance penalty (10-15%), or you pay extra for water cooling which is also more difficult to setup correctly and you have no performance penalty. Air and water cooling are all reasonable choices in certain situations. I would however recommend air cooling for simplicity in general — get a blower-style GPU if you run multiple GPUs. If you want to user water cooling try to find all-in-one (AIO) water cooling solutions for GPUs.</p>
<h2>Motherboard</h2>
<p>Your motherboard should have enough PCIe ports to support the number of GPUs you want to run (usually limited to four GPUs, even if you have more PCIe slots); remember that most GPUs have a width of two PCIe slots, so buy a motherboard that has enough space between PCIe slots if you intend to use multiple GPUs. Make sure your motherboard not only has the PCIe slots, but actually supports the GPU setup that you want to run. You can usually find information in this if you search your motherboard of choice on newegg and look at PCIe section on the specification page.</p>
<h2>Computer Case</h2>
<p>When you select a case, you should make sure that it supports full length GPUs that sit on top of your motherboard. Most cases support full length GPUs, but you should be suspicious if you buy a small case. Check its dimensions and specifications; you can also try a google image search of that model and see if you find pictures with GPUs in them.</p>
<p>If you use custom water cooling, make sure your case has enough space for the radiators. This is especially true if you use water cooling for your GPUs. The radiator of each GPU will need some space — make sure your setup actually fits into the GPU.</p>
<h3 style="text-align: justify;"><strong>Monitors</strong></h3>
<p style="text-align: justify;">I first thought it would be silly to write about monitors also, but they make such a huge difference and are so important that I just have to write about them.</p>
<p style="text-align: justify;">The money I spent on my 3 27 inch monitors is probably the best money I have ever spent. Productivity goes up by a lot when using multiple monitors. I feel desperately crippled if I have to work with a single monitor.  Do not short-change yourself on this matter. What good is a fast deep learning system if you are not able to operate it in an efficient manner?</p>
<figure id="attachment_123" aria-describedby="caption-attachment-123" style="width: 700px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg"><img data-attachment-id="123" data-permalink="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/2015-03-04-13-58-10/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=3264%2C1836&amp;ssl=1" data-orig-size="3264,1836" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;2.4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;U9200&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1425477490&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.13&quot;,&quot;iso&quot;:&quot;104&quot;,&quot;shutter_speed&quot;:&quot;0.016666&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="2015-03-04 13.58.10" data-image-description="&lt;p&gt;Typical layout when I do deep learning: Left: Papers, google searcheres, gmail, stackoverflow threads; middle: Code; right: Output windows, R, folders, systems monitors, GPU monitors, to-do list, and other small applications.&lt;/p&gt;
" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=1024%2C576&amp;ssl=1" class="wp-image-123 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?resize=700%2C394" alt="2015-03-04 13.58.10" width="700" height="394" data-recalc-dims="1" /></a><figcaption id="caption-attachment-123" class="wp-caption-text"><strong>Typical monitor layout when I do deep learning:</strong> Left: Papers, Google searches, gmail, stackoverflow; middle: Code; right: Output windows, R, folders, systems monitors, GPU monitors, to-do list, and other small applications.</figcaption></figure>
<h4 style="text-align: justify;"><strong>Some words on building a PC</strong></h4>
<p style="text-align: justify;">Many people are scared to build computers. The hardware components are expensive and you do not want to do something wrong. But it is really simple as components that do not belong together do not fit together. The motherboard manual is often very specific how to assemble everything and there are tons of guides and step by step videos which guide you through the process if you have no experience.</p>
<p style="text-align: justify;">The great thing about building a computer is, that you know everything that there is to know about building a computer when you did it once, because all computer are built in the very same way – so building a computer will become a life skill that you will be able to apply again and again. So no reason to hold back!</p>
<h2><strong>Conclusion / TL;DR</strong></h2>
<p><strong>GPU</strong>: RTX 2070 or RTX 2080 Ti. GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are good too!<br /><strong>CPU</strong>: 1-2 cores per GPU depending how you preprocess data. &gt; 2GHz; CPU should support the number of GPUs that you want to run. PCIe lanes do not matter.</p>
<p><strong>RAM</strong>:<br />&#8211; Clock rates do not matter — buy the cheapest RAM.<br />&#8211; Buy at least as much CPU RAM to match the RAM of your largest GPU.<br />&#8211; Buy more RAM only when needed.<br />&#8211; More RAM can be useful if you frequently work with large datasets.</p>
<p><strong>Hard drive/SSD</strong>:<br />&#8211; Hard drive for data (&gt;= 3TB)<br />&#8211; Use SSD for comfort and preprocessing small datasets.</p>
<p><strong>PSU</strong>:<br />&#8211; Add up watts of GPUs + CPU. Then multiply the total by 110% for required Wattage.<br />&#8211; Get a high efficiency rating if you use a multiple GPUs.<br />&#8211; Make sure the PSU has enough PCIe connectors (6+8pins)</p>
<p><strong>Cooling</strong>:<br />&#8211; CPU: get standard CPU cooler or all-in-one (AIO) water cooling solution<br />&#8211; GPU:<br />&#8211; Use air cooling<br />&#8211; Get GPUs with &#8220;blower-style&#8221; fans if you buy multiple GPUs<br />&#8211; Set coolbits flag in your Xorg config to control fan speeds</p>
<p><strong>Motherboard</strong>:<br />&#8211; Get as many PCIe slots as you need for your (future) GPUs (one GPU takes two slots; max 4 GPUs per system)</p>
<p><strong>Monitors</strong>:<br />&#8211; An additional monitor might make you more productive than an additional GPU.</p>
<p>Update 2018-12-14: Reworked entire blog post with up-to-date recommendations.<br />Update 2015-04-22: Removed recommendation for GTX 580</p><p>The post <a rel="nofollow" href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">A Full Hardware Guide to Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/feed/</wfw:commentRss>
			<slash:comments>945</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">121</post-id>	</item>
		<item>
		<title>Machine Learning PhD Applications — Everything You Need to Know</title>
		<link>https://timdettmers.com/2018/11/26/phd-applications/</link>
					<comments>https://timdettmers.com/2018/11/26/phd-applications/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Mon, 26 Nov 2018 19:13:57 +0000</pubDate>
				<category><![CDATA[Academia]]></category>
		<category><![CDATA[PhD Life]]></category>
		<category><![CDATA[Advisors]]></category>
		<category><![CDATA[Grad school]]></category>
		<category><![CDATA[PhD]]></category>
		<guid isPermaLink="false">http://timdettmers.com/?p=710</guid>

					<description><![CDATA[<p>I studied in depth how to be successful in my PhD applications and it paid off: I got admitted to Stanford, University of Washington, UCL, CMU, and NYU. This blog post is a mish-mash of how to proceed in your PhD applications from A to Z. It discusses what is important and what is not. [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2018/11/26/phd-applications/">Machine Learning PhD Applications — Everything You Need to Know</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>I studied in depth how to be successful in my PhD applications and it paid off: I got admitted to Stanford, University of Washington, UCL, CMU, and NYU. This blog post is a mish-mash of how to proceed in your PhD applications from A to Z. It discusses what is important and what is not. It discusses application materials like the statement of purpose (SoP) and how to make sense of these application materials.</p>
<p><span id="more-710"></span></p>
<p>There are some excellent sources out there on this topic and it is worth stopping for a second and understand what this blog post will give you and what other sources can give you. This blog post is mainly focused on PhD applications for deep learning and related fields like natural language processing, computer vision, reinforcement learning, and other sub-fields of deep learning. This blog post assumes that you already have a relatively strong profile, meaning you probably have already one or multiple publications under your belt and you worked with more than one person on research. This blog post is designed to help you optimize your chance for success for top programs.</p>
<p>If you seek more general information for PhD admissions, I recommend reading all the most highly voted questions and answers from <a href="https://academia.stackexchange.com/questions?sort=votes">Academia StackExchange</a>. Other important sources are&nbsp;<a href="http://www.cs.cmu.edu/~harchol/gradschooltalk.pdf">Applying to Ph.D. Programs in Computer Science</a>&nbsp;which is a detailed write-up of the full admission process as viewed by CMU faculty. A similar but more concise source — in particular, relevant for good but not strong candidates — is the blog post <a href="https://da-data.blogspot.com/2015/03/reflecting-on-cs-graduate-admissions.html">Reflecting on CS Graduate Admissions</a>&nbsp;which is again by CMU faculty. Less useful, but a quick read is the negative view of <a href="http://www.cs.cmu.edu/~pavlo/blog/2015/10/how-to-write-a-bad-statement-for-a-computer-science-phd-admissions-application.html">How to Write a Bad Statement for a Computer Science Ph.D. Admissions Application</a>.</p>
<p>This blog post will first define what is important in PhD applications. Then we will dive into the application materials and how to think about these. Then I will talk a bit about the application process. The final section of the main part of this blog post will be on selecting schools — which schools are too good or too bad for me? After that, I will close with a Q&amp;A section which was drawn from <a href="https://twitter.com/Tim_Dettmers/status/1064258559918002176">questions on Twitter</a>. I will update this Q&amp;A section periodically. If you have some questions regarding the application process, please leave a comment and I try to get back to you.</p>
<p><figure style="width: 600px" class="wp-caption aligncenter"><a href="http://phdcomics.com/comics/archive.php?comicid=368"><img src="https://i0.wp.com/phdcomics.com/comics/archive/phd091703s.gif?resize=600%2C260" alt="" width="600" height="260"  data-recalc-dims="1"></a><figcaption class="wp-caption-text">Source: <a href="http://phdcomics.com/comics/archive.php?comicid=368"> PhD Comics</a></figcaption></figure></p>
<h1>Understanding What Makes a Strong PhD Application</h1>
<p>The most important factor that determines admission at any research university is research potential: How likely are you to become a great researcher? The main direct indicators for this are in order of importance:</p>
<ol>
<li>Recommendations: Respected professors speak highly of you. Personal connections are important.</li>
<li>Research experience: You did successful research before. Measured in publications, first-authorship, and prestige of conference where you published.</li>
</ol>
<p>Other indirect factors can help sometimes if they are exceptional, but usually, only the first two factors, recommendations, and research experience count. In order of importance:</p>
<ol>
<li>Undergraduate university name: Some universities select aggressively for this, some others do not care so much.</li>
<li>Employer name: It is common that students are admitted that were previously employed in finance or at companies such as Google, Facebook, etcetera.</li>
<li>Smarts: Perfect GPA, perfect GRE is somewhat correlated with intelligence (or at least with how fast you can learn and understand).</li>
<li>Grit / Conscientiousness: You do well under continuous rejection, disappointment, and failure. If you faced and have overcome difficulties you might want to include your story in the statement of purpose.</li>
<li>Accomplishment: You won Math or CS competitions.</li>
<li>Recognition: You won prestigious scholarships/fellowships.</li>
<li>Good at math or engineering: You developed or contributed to open source projects. You worked with research code.</li>
<li>Heritage: Parents are professors.</li>
</ol>
<h1>Understanding Application Materials</h1>
<h2>Understanding Recommendation Letters</h2>
<p>For recommendation letters, one could devise four categories: Strong, Good, Weak, and Bad. Note that the main thing that admission committees look for in recommendation letters are indicators of research potential. This section has the main purpose of making you aware of what constitutes a good or strong letter and based on this information it might be easier for you to select letter writers.</p>
<h3>Signs of a Bad Recommendation Letter</h3>
<ul>
<li>Your letter writer knows you and writes bad things about you. Especially in the US anything even slightly critical is very bad.</li>
<li>Your letter writer does not know you (you had a class with her but you left no impression).</li>
<li>Your letter is short and only states that you did well in class.</li>
</ul>
<h3>Signs of a Weak Recommendation Letter</h3>
<ul>
<li>Your letter writer knows you from class only.</li>
<li>Your letter writer is favorable, but can only write about achievements in class: Great project work in class; part of lively, interesting discussions in class.</li>
<li>The letter writer does not comment on your research.</li>
<li>The letter writer is not known by the admission committee nor by potential advisors.</li>
</ul>
<h3>Signs of a Good Recommendation Letter</h3>
<ul>
<li>The name of the letter writer is known by parts of the admission committee.</li>
<li>The letter writer&#8217;s name and work are known by at least one potential advisor mentioned in the statement of purpose.</li>
<li>The letter writer worked with you on research.</li>
<li>The letter writer mentions your excellent research abilities in anecdotes&nbsp;that demonstrate your creativity, commitment, persistence and research skills in general.</li>
<li>The letter writer writes about how you published your research.</li>
<li>The letter writer comments about research done outside of her lab.</li>
</ul>
<h3>Signs of a Strong Recommendation Letter</h3>
<ul>
<li>US-style recommendation letter: The achievements are oozing through the paper. Everything is very much overdone, that is simple things become grand achievements.</li>
<li>The letter writer has an excellent command of English.</li>
<li>The letter writer is personally known by at least one potential advisor mentioned in the statement of purpose.</li>
<li>The letter writer is known for making excellent recommendations (previously recommended students do very well).</li>
<li>The letter writer mentions your excellent research abilities in anecdotes&nbsp;that demonstrate your creativity, commitment, persistence and research skills in general.</li>
<li>The letter writer mentions your abilities which help indirectly with research (engineering skills, presentation skills, interpersonal skills) and wraps these skills into anecdotes.</li>
<li>The letter writer comments about research done outside of her lab.</li>
</ul>
<p>Note a few things:</p>
<ul>
<li>Anecdotes are important because the show that the letter writer really knows you. They also read much better. Stories are more interesting than checklists.</li>
<li>The letter does not need to contain everything listed here to be considered &#8220;bad&#8221; or &#8220;strong&#8221; and so forth. Recommendation letters are complicated.</li>
<li>If you select recommendation letters it can make sense to have some diversity among letters that highlight different strengths. One strong letter on research skills, a good letter on engineering skills (internship), and a good letter on performance in class/project work is a great combination. This combination is better than a strong letter on research, a good letter on research, and a weak letter on research.</li>
<li>Please see more details about the process of asking about recommendation letters below.</li>
</ul>
<h2>Understanding Publications</h2>
<h3>Author Position</h3>
<p>Publications are direct evidence for research experience and research skill. If you published as a first author, people know that you did most of the work. If you published as a second author, people know that you did a good portion of the work (25%-50%). If your name is the third or later, your contribution is discounted, but you probably went through the entire research process towards publication and gained a good amount of research experience. If you published a couple of first author papers a third author paper looks very good: It shows that you can work in a team.</p>
<h3>Prestige of Venue</h3>
<p>If you published your work at a respectable&nbsp;conference, people know that: (1) Your work is high quality; (2) your work can be trusted; (3) that your current research skills is sufficient to publish at great conferences, (4) that you are competitive and/or you can stay productive under the pressure of publishing at a top conference.</p>
<p>It helps to view this in the eyes of a potential advisor: If you have two students, one published already at NeurIPS (Tier A) and one you published at a Tier B conference. You would know that the first student is probably ready to work on a research project which is aiming for NeurIPS next year. The second student would need further preparation, for example, publish in a workshop or at a less competitive Tier A conference before making the step towards NeurIPS. With the second student, there is some risk that this student might take more than a year to acquire the research skills to needed to publish at Tier A conferences. Pushing a student towards NeurIPS can be stressful for an advisor and it is easier to work with someone who already has the necessary research skills. If there is less stress between advisor and student then its easier to develop a strong professional relationship which makes it easier and fun to work with each other. So a potential advisor would have good reasons to select according to the prestige of the conference where you published at.</p>
<h3>Creativity, Citations, etcetera</h3>
<p>Other indicators have little effect on the application. Your work might be unusually creative, but you have no track record that you are a creative researcher. Maybe you got lucky.</p>
<p>The importance of publications often only emerges with the years.&nbsp;Often you published shortly before the PhD applications which means that the citations that you have on your work is a poor indicator of impact. If you get an usually high number of citations in a short time this can help, but maybe you just got lucky or good at marketing. Usually,&nbsp;the number of citations over the past 1-3 years&nbsp;is no reliable indicator of research potential and as such is disregarded. If you have a citation history over the past 5 years this might be a different story, but this does not apply to most applicants.</p>
<h2>Understanding the Statement of Purpose</h2>
<p>For most institutions, the statement of purpose is mainly a filter for people who took no time to polish the SoP. Your writing can show how you think, how you can sell, how you explain things, but it can also show that you are lazy and do not pay attention to details. It can show that you are not able to Google simple recipes of how to write (and how not to write) a simple formal document. For some institutions, the SoP can be important (CMU) but the content does not really differ for these institutions.</p>
<p>Beyond formalities, the SoP is also the only document where you can justify why you did underperform in certain circumstances. For example, you can explain any extraordinary difficulties that you had along the way to graduate school, or it can explain why you did not do so well in certain semesters/quarters at uni. The structure of a SoP should be the following:</p>
<ol>
<li>Intro to research interests with a catchy hook that makes the reader want to read more (one paragraph). This is the most important bit: If you do not interest your readers in this paragraph it is unlikely that they will focus on the rest of the letter.</li>
<li>The research experiences that you gathered along your way to grad school (about one page).</li>
<li>Identifying what research you want to do in the future.</li>
<li>Identify people with whom you want to work with and why.</li>
<li>(Optional) Explaining extenuating circumstances where appropriate.</li>
</ol>
<p>In some circumstances, the SoP can be very important. This is so if you showed good — but not strong or weak — academic potential and you had to overcome significant hardship to be able to do research. If your application is strong and write about hardships it might alienate your readers (privileged prick); if your application is weak it might also alienate your reader (whining looser). If your application is good it is exactly right (a&nbsp;smart person that pushed through difficulties). For example, I had a rare situation where I was barred from university access and my SoP was very important to explain the difficulties that I faced under these circumstances.</p>
<p>However, disclosing hardships and weaknesses — like learning disabilities and mental illnesses — can also be double-edged sword: You might either alienate the readers or you might draw their sympathizes and admiration for persisting in a difficult situation. If you disclose such facts, it needs to be done right and the SoP needs to be extremely polished. Do not attempt this if you do not have the feedback from expert writers. For some stories which are more socially acceptable you do not need expert feedback to do it right: It is easy to write a compelling story where you worked yourself from extreme poverty into college and that you now want to realize your potential by doing a PhD; it is difficult to write a compelling story about the hardships that you faced while suffering from schizophrenia or bipolar disorder.</p>
<p>However, if you did not face any hardship do not make up stories that make no sense: &#8220;As a white, male, upper-class US citizen, I was haunted by the responsibility of my&nbsp;privilege from an early age and my academic performance suffered in the process.&#8221;, instead, concentrate on your research experience.</p>
<h2>Understanding GRE, TOEFL, GPA</h2>
<p>The GRE &amp; TOEFL tests and GPA are usually used as filter criteria. A very high GPA can be a good indicator of &#8220;some intelligence&#8221; which can help with the recommendation letters and publications are borderline. But a GPA of 4.0 will not help if you have no publications and bad recommendation letters — it might even hurt you because it shows that you concentrate on useless classes rather than research. GRE and TOEFL scores are pure filters: If you have an okay score you are not filtered out. If you have a&nbsp;perfect GRE score, it can help a little bit but much less so than a perfect GPA. Great GRE scores do not matter: I got into three out of the top five US computer science programs with verbal 159 (81%), quantitative 163 (86%), writing 5.0 (93%) and a TOEFL 120/120 and a GPA of 8.1/10. Any GPA higher than 3.5 is good. Anything above 3.5 does not matter. A GPA of 4.0 might help a little bit.</p>
<h2>Understanding the CV</h2>
<p>The CV lists what you have done. There are no surprises here. The content is important but the content is also determined by what you have done before and cannot be changed. Do not &#8220;tune&#8221; your CV by phrasing things in a nice way or by making your CV look &#8220;nice&#8221; or &#8220;creative&#8221; — this is a waste of time. Just list what you have done.</p>
<h2>The Application Process</h2>
<h3>How to ask your professor for a recommendation letter</h3>
<p>You write two emails: (1) Just ask if the person can write you a good or strong recommendation letter. Knowledgeable recommendation letter writers will reject your request if they think they cannot write you a good letter. In this case, look for someone else. (2) If your recommender agrees she will ask you to include some information for the letter. Give a list of what you have done with the person. Write it in a style that can be easily wrapped into anecdotes:</p>
<ul>
<li>DO: &#8220;You told me in a meeting that with some extra work we could make it for the NeurIPS deadline. In the next two weeks, I develop an improved deep network architecture started writing up the findings. The next week, Jane extended my code for an additional task. We then had enough results to submit our work to NeurIPS&#8221;</li>
<li>DO not write: &#8220;Jane and I published our research at NeurIPS.&#8221;</li>
</ul>
<p>Anecdotes can also come from&nbsp;interactions with PhD students and post-docs:</p>
<ul>
<li>&#8220;I worked with Tom on developing the research library that served as the main framework for our research that we published at NeurIPS. I worked one week on the library and Tom told me that the library was well designed and well performing.&#8221;</li>
</ul>
<p>Your advisor will then ask the respective PhD student or post-doc for more information to write something like this:</p>
<ul>
<li>&#8220;My PhD student Tom — whom I regard as one of my most engineering-savvy students — worked with Jane on a research project where we needed to develop a code-base for language modeling before we could start the research. Tom gave this task to Jane and estimated it to take 3 weeks. Jane completed it within one week. Tom told me that after he inspected Jane&#8217;s code in a code-review, he found that Jane&#8217;s engineering abilities are on-par or even exceed his own — the code was very high quality and lightning fast. Jane&#8217;s engineering skills helped with the rapid development of research ideas. The research project became a walk in the park because of this. Jane published her work at NeurIPS2020&#8230;&#8221;</li>
</ul>
<p>(2) If you have three letters which are on or above the &#8220;Good&#8221; level, you should think about making your letters more diverse. I for example used one academic letter, one industry lab letter, and one letter from a lecturer who is aware of my research.</p>
<h3>Statement of Purpose</h3>
<p>Start early and ask experienced people for feedback. You should be safe if you follow the formula above. If you want to disclose difficulties that you had along the way to graduate school you will need a lot of time in your SoP and you can expect that the SoP will take by far the most time in all your application materials.</p>
<p>Try to reuse letters between universities. It takes too much time to &#8220;personalize&#8221; the SoP for universities. The only section that I changed in my SoP from university to university was the section that mentions the potential advisors I would like to work with.</p>
<h3>Online Application</h3>
<p>Start early filling out the online applications early. Some forms are terrible and take some time to fill out and it is great if you can get this out of the way as early as possible to focus on recommendation letters, university selection and your statement of purpose. You should have a good reserve of money to do these applications. The entire process might cost up to $1000. If you do not have the money, ask some relatives for some help early on.</p>
<h2>How to Select Schools for PhD Applications?</h2>
<h3>Can I get admitted to a top school?</h3>
<p>Many people reading this probably have the dream to get into a top school like Stanford, MIT, Berkeley, or CMU. But admission is really tough. Some programs are highly selective. Here admission statistics for one top school I was admitted to and the prior probability of getting admitted to the program. Note that I have hard statistics on the schools and publications, but I do not have hard statistics on the letters and personal connections but I make assumptions based on what I have heard and seen from admitted students that I talked to:</p>
<ul>
<li>Top 2 undergraduate school AND 1 to 3 publications AND &gt;=1 strong letter AND personal connections: 38%</li>
<li>Top 4 undergraduate school AND 1 to 3 publications AND &gt;=1 strong letter AND personal connections: 14%</li>
<li>Top 20 undergraduate school AND 2 to 4 publications AND &gt;=1 strong letter AND personal connections: 21%</li>
<li>Below top 20 undergraduate school AND best school in a country (Tokyo, Australian National) AND 2 to 4 publications AND 1&gt;= strong letter AND personal connections: 11%</li>
<li>Master in top 3 school AND 1 to 4 publications AND &gt;=1 strong letter AND personal connections: 5%</li>
<li>Below top 20 undergraduate school AND not the best school in a country AND &gt;4 publications and &gt;=2 strong letters AND personal connections: 5%</li>
<li>Below top 20 undergraduate school AND not the best school in a country AND &gt;3 publications and &gt;=2 strong letters AND award for Best Teacher/Young Scientist AND personal connections: 5%</li>
</ul>
<p>This program, like most top programs, selects aggressively for undergrad degree. Note that usually, some form of personal connection (a letter writer knows a possible advisor at the school) is a requirement especially for edge cases. Other top programs select differently. For example, while CMU also selects aggressively for undergrad degree, they also like candidates with an unusual background which reflects strong performance under difficult circumstances. Some schools really like awards in math/CS competitions. Many schools like it if you got some form of best teacher award. Some schools like it if you have a portfolio of hacks (MIT). However, in general, in order of importance to get admitted to top schools:</p>
<ol>
<li>Personal connections</li>
<li>Top undergrad school AND publications</li>
<li>Strong letters AND publications</li>
<li>Publications</li>
<li>Anything else</li>
</ol>
<p>This means if you doing an undergrad at a top 2 school and you have no publications you will still have a hard time. Top 2 school and a publication increase your chances of admittance dramatically. If you have no personal connections it is difficult to get admitted even with a strong profile. However, if your profile is overly strong under respected advisors then personal connections do not matter.</p>
<p>There are some other factors for special cases. For example, if you study at a top school and have only 1 publication then GPA will be an important factor. However, in general, top schools do not care about GPA numbers from schools below top 20 if it is at least a GPA of 3.5 or equivalent. So if you have a GPA of 3.5 at a below top 20 school and you have 4 publications you have a good chance of getting admitted.&nbsp; A low GPA (which is still &gt; 3.5) can be a factor in favor if your research profile is very strong as it demonstrates that you do not care about classes but that you are passionate about research — exactly what advisors want to see.</p>
<p>Another thing to note here is that we have publication inflation. This means the value of a single publication becomes less and less because more and more students fulfill this requirement. The more students are interested in ML PhDs the more stringent the publication requirements. It might have been fine to have no publications to get into an ML PhD, but this is often no longer the case.</p>
<h3>How to get admitted to top schools?</h3>
<p>These statistics above do not mean that you cannot get accepted by these schools, but it means that if your profile is too weak you should take another year to bolster it. I, for example, extended my master by a year to squeeze in a year of research internships. Without this, I would never have made it into these schools. If your dream is to get into one of these top schools this is by far the best option. Even if you do not necessarily want to get into top schools, a research internship is highly recommendable.</p>
<p>A research internship will give you:</p>
<ul>
<li>Improved research skills so you can get an easier start into a PhD.</li>
<li>A test whether a PhD or a certain research direction (NLP vs computer vision vs systems) is right for you.</li>
<li>A good or even strong recommendation letter (the longer the internship the better).</li>
<li>A possible publication.</li>
</ul>
<p>But even finding a research internship is easier said than done! How can you approach this? My next blog post will deal in detail with the topic of how you can improve your application file for the application cycle in the next year.</p>
<h3>Realistic School Selection</h3>
<p>You should apply for about 10-15 universities. If you apply for more, you run in the danger that you will not have enough time to really polish your applications. If you apply for less you run into the danger of not being accepted anywhere.</p>
<p>You should have one or two backup universities where it is likely that you are accepted (&gt; 75%). Often the university where you already studied at is a good candidate for this since your recommendation letter writers will be known to the university faculty.&nbsp;Apply for all top universities where you have some hope of getting admitted (&gt;10% chance).&nbsp;Fill out the rest of the university slots with universities where you expect to have a good admission rate (25-33%) — you should have a minimum of 3 universities of this kind. These universities are usually the ones where a recommendation letter writer has a personal connection to a faculty with whom you would like to work.</p>
<p>Note that the best advisors are not necessarily at the top schools. You can get excellent PhD training at many schools outside of the top 20. However, if you thinking about an academic career then the school rank will be really important and you should try to find an advisor at a top school.</p>
<p>Pick universities mainly according to possible advisors. Make sure each university has more than one advisor you would like to work with. Do not apply to a university where there is a single good advisor. If your list is too small, broaden your area of interest. For example, if you would like to do deep learning and NLP and you cannot find enough fitting advisors consider also some advisors in computer vision or other fields.</p>
<h1>General Q&amp;A</h1>
<h3>4 year UK PhD vs 6 year US PhD</h3>
<p>In the first 1-2 years of a US PhD you will do quite a few classes since the US PhD is designed for bachelor students. On the contrary, the UK PhD is designed for students that have already a (1 year) master degree and will have few classes. Thus you can get started immediately with research in a UK PhD which can be a nice advantage.\</p>
<p>US PhD:</p>
<ul>
<li>Designed for bachelor students</li>
<li>Classes for 1-2 years. Classes distract from research.</li>
<li>Funding guaranteed with admission, that is, you have guaranteed positions as a research assistant or a teaching assistant.</li>
</ul>
<p>UK PhD</p>
<ul>
<li>Designed for master students</li>
<li>Classes for 0.25 &#8211; 0.5 years. You can focus on your research from start to finish.</li>
<li>Funding can be problematic and is often dependent on your advisor. This is why it is important to get in touch with your potential advisor before you apply.</li>
<li>Less prestigious (in most cases) and thus it will be more difficult to get academic positions after your PhD. It will be more difficult to get oral presentations, best paper awards etc due to visibility bias.</li>
</ul>
<p>Also be aware of local effects. If you study in the US you will also be in a US research bubble. Same is true if you study in Europe or Asia. For example, researchers in Europe know the &#8220;famous&#8221; researchers worldwide, but beyond that, they know more European universities than US universities in general (e.g. Stony Brooks vs University of Sheffield). Same is true for other locations. If you want to join academia in Europe, and you cannot get admitted to top US schools, it might make sense to apply for mostly EU universities.</p>
<h3>Is a master&nbsp;required for a PhD?</h3>
<p>In continental Europe, bachelor degrees are usually 3 years long and you require a master degree to start a PhD. In the US and UK, bachelors are often 4 years long and you can start a PhD right after your bachelor.</p>
<h3>Does work experience matter?</h3>
<p>It can help especially if you work at a prestigious institution (Google, Facebook, McKinsey, Goldman Sachs etc.). Other work experience can help if it is software engineering related, but any research experience (research internship) will be seen as far superior. Just a good job and no research experience will not help you.</p>
<h3>How to pick advisors?</h3>
<ul>
<li>Look at recent publications to get a sense of overlapping interest. Avoid working with academics that did not publish papers recently. There does not need to be an overlap in current research, but you should be interested in the research that the advisor is doing.</li>
<li>Look at the list of students that graduated and where they are now. If you cannot find a list of students that graduated this is a red flag (or a new faculty). This is a good indicator of the quality of advice and training that you will get.</li>
<li>Does the advisor has a startup? How many students does the advisor have? The combination of these factors is a good indicator of how much time you can the advisor to have. Dependent on how experienced you are in research you will need an advisor that has more or less time.</li>
<li>Is there a fallback option in the same department? Sometimes relationships do not work out. Protect yourself by having a second advisor option as a fallback.</li>
</ul>
<h3>Should one even do a PhD?</h3>
<p>If you want to work in academia you will need a PhD.</p>
<p>In industry, everything is regulated by supply and demand. The supply of AI researchers will rise sharply in the next years. If the AI hype collapses the demand will recede. The situation might be very similar to the situation that data scientists face in 2018: Companies only take over-qualified applicants because there is much more supply than demand. In this situation, a PhD will make a big difference if you want to switch jobs or want to be promoted. You might get hired without a PhD now, but without a PhD but you might have problems if you want to switch to another research lab (because the supply of skilled PhDs might be high, while demand is low).</p>
<p>If the AI hype does not collapse (unlikely) then you can find and switch jobs easily without a PhD. However, note promotion might still be more difficult and you might need to do more &#8220;research engineering work&#8221; compared to research. If you are happy with a research engineer position a PhD might be useless for you.</p>
<p>Do not do a PhD for the reasons above alone. If you do not want to do research do not do a PhD.</p>
<h3>Contact advisor before application?</h3>
<p>This can make sense if one recommendation letter writer can introduce you to a potential advisor. However, this is not required in the US. It can also backfire since it removes a shroud of mystery around you and sometimes it is more impressive to see your publications and recommendation letters first rather than to talk to you in person and seeing the recommendation letters afterward. In the EU it is sometimes required to contact a potential advisor before an application. If you need to do so, also try to get introduced via someone that knows your advisor personally, for example, your bachelor or master thesis advisor. If you do not have a personal connection to your personal advisor you might want to write an email with:</p>
<ul>
<li>Your current advisor</li>
<li>A sentence about your past work (optionally: where did you publish your work?)</li>
<li>4 bullet points about potential work that you could do with the advisor in the form of &#8220;idea: One sentence that explains the idea&#8221;</li>
</ul>
<p>It is very unlikely that your potential advisor will read and even reply you if you do not have a personal contact. If you do not have a personal contact and you apply to EU (UK) universities, then you might want to apply somewhere else.</p>
<h3>How to pick a topic for your research proposal?</h3>
<p>The topic for the research proposal does not matter. Nobody will ask you to do the work that you described in your research proposal. You can pick your research proposal topic based on how easy it would be to reuse it across different applications. If you do not need to rewrite it for different applications you save a lot of time. One thing to consider: The more familiar you are with a topic the easier it is to write a good proposal.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2018/11/26/phd-applications/">Machine Learning PhD Applications — Everything You Need to Know</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2018/11/26/phd-applications/feed/</wfw:commentRss>
			<slash:comments>154</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">710</post-id>	</item>
		<item>
		<title>TPUs vs GPUs for Transformers (BERT)</title>
		<link>https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/</link>
					<comments>https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Wed, 17 Oct 2018 18:13:03 +0000</pubDate>
				<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Accelerators]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Matrix Multiplication]]></category>
		<guid isPermaLink="false">http://timdettmers.com/?p=686</guid>

					<description><![CDATA[<p>On the computational side, there have been confusions about how TPUs and GPUs relate to BERT. BERT base was trained with 4 TPU pods (16 TPU chips) in 4 days and BERT large with 16 TPUs (64 TPU chips) in 4 days. Does this mean only Google can train a BERT model? Does this mean [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPUs vs GPUs for Transformers (BERT)</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>On the computational side, there have been confusions about how TPUs and GPUs relate to <a href="https://arxiv.org/abs/1810.04805">BERT</a>. BERT base was trained with 4 TPU pods (16 TPU chips) in 4 days and BERT large with 16 TPUs (64 TPU chips) in 4 days. Does this mean only Google can train a BERT model? Does this mean that GPUs are dead? There are two fundamental things to understand here: (1) A TPU is a matrix multiplication engine — it does matrix multiplication and matrix operations, but not much else. It is fast at computing matrix multiplication, but one has to understand that (2) the slowest thing in matrix multiplication is to get the elements from the main memory and load it into the processing unit. In other words, the most expensive part in matrix multiplication is memory loads. Note the computational load for BERT should be about 90% for matrix multiplication. From these facts, we can do a small technical analysis on this topic.</p>
<p><span id="more-686"></span></p>
<h2>Bandwidth Model for TPUs and GPUs</h2>
<h3>Transformers for TPUs</h3>
<p>A common operation in BERT is matrix multiplication A*B=C where A is 256&#215;1024 and B is 1024&#215;1024 in dimension. A TPU computes such a matrix multiplication by splitting the matrix into many smaller 128&#215;128 matrix multiplications. This means we need to load 16 128&#215;128 matrix tiles from matrix A — and due to the nature of matrix multiplication — we need to load 64 tiles from B for every tile in A. This is a total of 16*64=1024 128&#215;128 loads. At 16-bit that is a total of 32 MB of data.</p>
<p><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg"><img data-attachment-id="698" data-permalink="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/cloud-tpu-feature/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?fit=1123%2C620&amp;ssl=1" data-orig-size="1123,620" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Cloud-TPU-Feature" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?fit=300%2C166&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?fit=1024%2C565&amp;ssl=1" class="aligncenter wp-image-698" title="TPU vs GPU" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature-1024x565.jpg?resize=745%2C411" alt="TPU vs GPU" width="745" height="411" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?resize=1024%2C565&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?resize=300%2C166&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?resize=768%2C424&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/Cloud-TPU-Feature.jpg?w=1123&amp;ssl=1 1123w" sizes="(max-width: 745px) 100vw, 745px" data-recalc-dims="1" /></a></p>
<p>Now we make a simplification: We assume that there is no latency if we do two memory loads after each other, which is actually not too unreasonable since often you can hide memory access latency under thread parallelism. In simple words, this means: While we wait for one 128&#215;128 matrix copy to complete, we already do the next one. In doing it this way, we only wait for the first memory copy and we do not wait for other copies. This is a <a href="https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning">core reason why GPUs are fast</a> and why we use many threads in GPUs thus 0 latency for overlapping memory transfers is not too far off from the real world. Using this simplification, we can now plainly use the memory bandwidth to compute the time needed to load the memory for the matrix multiplication. If we look at the bandwidth of the TPU we find that we have 600 GB/s, so we need 5.2e-05 seconds to transfer the 32 MB of data.</p>
<h3>Transformers on GPUs</h3>
<p>For a GPU we have the same process, but we use smaller tiles with more processors. Similarly to the TPU, we use two loads in parallel to hide memory latency. For GPUs, however, we would have a tile size of 96&#215;96 for 16-bit data. If we take a V100 Tesla GPU, then we can run 160 of these in parallel at full bandwidth with low memory latency. What this means compared to a TPU: Instead of 2 matrix units which can hold 128&#215;128 matrices, the GPU has 160 units (80 SMs, 160 thread blocks, each thread block has two 96&#215;96 matrices) which hold two 96&#215;96 matrices. Again this ensures that we can hide the memory latency through parallelism.</p>
<p><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg"><img data-attachment-id="697" data-permalink="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/attachment/010/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?fit=1024%2C576&amp;ssl=1" data-orig-size="1024,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="010" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?fit=1024%2C576&amp;ssl=1" class="aligncenter wp-image-697" title="TPU vs GPU" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010-1024x576.jpg?resize=818%2C460" alt="TPU vs GPU" width="818" height="460" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2018/10/010.jpg?resize=768%2C432&amp;ssl=1 768w" sizes="(max-width: 818px) 100vw, 818px" data-recalc-dims="1" /></a></p>
<p>If we repeat the calculation from the top we receive the following: For matrix A with 256&#215;1024 we have 33 96&#215;96 tiles; for B with 1024&#215;1024 we have 121 96&#215;96 tiles. In total, we need to do 33*121=3993 loads of size 96&#215;96 for a total of 70 MB. A V100 runs at 900 GB/s and so the memory loads will take 7.6e-05 seconds. Thus our model predicts that a GPU is 32% slower than a TPU for this specific scenario. Note that matrix tiles stay the same for an RTX 2080 Ti GPU, but the memory bandwidth decreases to 616 GB/s. Which means an RTX 2080 Ti is 54% slower than a TPU.</p>
<p>Note that both TPU and GPUs with Tensor Cores compute the respective matrix multiplication tile in one cycle. Thus the computation is about equally fast — the difference is only in how the memory is loaded.</p>
<h3>BERT Training Time Estimate for GPUs</h3>
<p>Using this data, a GPU cluster of V100s/RTX 2080 Tis with good networking (Infiniband +56GBits/s) and good parallelization algorithms (for example using Microsoft&#8217;s CNTK) we can expect to train BERT large on 64 GPUs (the equivalent to 16 TPUs) or BERT base on 16 GPUs in 5 1/3 days or 8 1/2 days. On an 8 GPU machine for V100/RTX 2080 Tis with any software and any parallelization algorithm (PyTorch, TensorFlow) one can expect to train BERT large in 21 days or 34 days and BERT base in 10 2/3 or 17 days. For a standard 4 GPU desktop with RTX 2080 Ti (much cheaper than other options), one can expect to replicate BERT large in 68 days and BERT base in 34 days.</p>
<h2>Limitations of the Bandwidth Model</h2>
<p>Note that all models are wrong, but some are useful. I would expect that this bandwidth model is in about 30% of the correct runtime values for TPU vs GPU.</p>
<p>The biggest limitation is that these calculations&nbsp;are for specific matrices sizes. Computational differences can be amplified&nbsp;for certain sizes. For example, if your batch-size is 128, there is a slight speedup for GPUs compared to TPUs. If you go below a batch size of 128 you can expect GPUs to be significantly faster; increasing the matrix B further makes TPUs better and better compared to GPUs. Decreasing the size of matrix B will make the performance of GPUs better. Note that the BERT paper optimized matrix A and B sizes for the TPU — one would not choose these dimensions if you train with a GPU. So this comparison might favor TPUs slightly.</p>
<p>Further direct limitations include fused operations. The TPU can calculate additional element-wise operations such as a non-linear activation function or a bias on the fly within a matrix multiplication. This means that the TPU does not need to load from slow global memory as often as a GPU. The GPU also supports these operations but NVIDIA has not implemented them and thus GPU users will not be able to benefit from this. Thus one can expect a slowdown of about 1.6% (loading and storing a 256&#215;1024 matrix) for each element-wise operation for a GPU. For example, if you apply a non-linear function and a bias, then the TPU would be about 3.2% faster compared to GPUs in this scenario.</p>
<h2>The Importance of 32-bit vs 16-bit vs 8-bit</h2>
<p>If we repeat the same calculations from above for 32-bit values (64x64x tiles) we find that TPUs would be 5.3x faster. So the datatype size has a much larger effect than switching from TPU to GPU and vice versa.</p>
<p>TPUs do not support 8-bit training, but Turing GPUs do. So we can also have a look at how 8-bit matrix multiplication would impact performance.&nbsp;<a href="https://arxiv.org/abs/1511.04561">I published research on 8-bit models</a> and it is not too difficult to train them with 8-bit alone. In fact, the <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Courbariaux%2C+M">literature</a> on <a href="https://arxiv.org/abs/1602.02830">low-bit</a> computing is <a href="https://dawn.cs.stanford.edu/2018/03/09/low-precision/">quite rich</a>. With 32-bit accumulation as supported by Turing GPUs 8-bit training should be even easier.&nbsp; If we can make 8-bit computing work for general models this would entail huge speedups for transformers. If we repeat the above calculations for 8-bit for GPUs (128&#215;128 tile) we find that GPUs are 3.0x faster than TPUs. 8-bit computation on an affordable standard machine with 4 RTX 2080 Ti would take about 11 days for BERT base and 22 days for BERT large. All of this makes 16-bit computational ability for a GPU and important criterion <a href="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/">if you are looking for a GPU</a> to work with transformers.</p>
<h2>Conclusion</h2>
<p>TPUs are about 32% to 54% faster for training BERT-like models. One can expect to replicate BERT base on an 8 GPU machine within about 10 to 17 days. On a standard, affordable GPU machine with 4 GPUs one can expect to train BERT base for about 34 days using 16-bit or about 11 days using 8-bit.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPUs vs GPUs for Transformers (BERT)</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/feed/</wfw:commentRss>
			<slash:comments>26</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">686</post-id>	</item>
		<item>
		<title>Deep Learning Hardware Limbo</title>
		<link>https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/</link>
					<comments>https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Thu, 21 Dec 2017 11:10:23 +0000</pubDate>
				<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Accelerators]]></category>
		<category><![CDATA[AMD]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Intel]]></category>
		<guid isPermaLink="false">http://timdettmers.com/?p=627</guid>

					<description><![CDATA[<p>With the release of the Titan V, we now entered deep learning hardware limbo. It is unclear if NVIDIA will be able to keep its spot as the main deep learning hardware vendor in 2018 and both AMD and Intel Nervana will have a shot at overtaking NVIDIA. So for consumers, I cannot recommend buying any hardware right now. The most prudent choice is to wait until the hardware limbo passes. This might take as little as 3 months or as long as 9 months. So why did we enter deep learning hardware limbo just now?</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/">Deep Learning Hardware Limbo</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>With the release of the Titan V, we now entered deep learning hardware limbo. It is unclear if NVIDIA will be able to keep its spot as the main deep learning hardware vendor in 2018 and both AMD and Intel Nervana will have a shot at overtaking NVIDIA. So for consumers, I cannot recommend buying any hardware right now. The most prudent choice is to wait until the hardware limbo passes. This might take as little as 3 months or as long as 9 months. So why did we enter deep learning hardware limbo just now?</p>
<p><span id="more-627"></span></p>
<p>NVIDIA has decided that it needs to cash-in on its monopoly position before the competition emerges. It needs the cash in order to defend itself in the next 1-2 years. This is reflected by the choice to price the Titan V at $3000. With TensorCores the Titan V has a new shiny deep learning feature, but at the same time, its cost/performance ratio is abysmal. This makes the Titan V very unattractive. But because there is no alternative, people will need to eat what there are served – at least for now.</p>
<p>The competition is strong. We have AMD whose hardware is now already better than NVIDIA’s and plans to get itself together to produce some deep learning software which is actually usable. With this step, the cost/performance ratio will easily outmatch NVIDIA cards and AMD will become the new standard. NVIDIA’s cash advantage will help fight AMD off so that we might see very cheap NVIDIA cards in the future. Note that this will only happen if AMD is able to push forward with good software — if AMD falters, NVIDIA cards will remain expensive and AMD will have lost its opportunity to grab the throne.</p>
<p>There is also a new contender in town: The Neural Network Processor (NNP) form Intel Nervana. With several unique features, it packs quite a punch. These new features make me drool — they are exactly what I want as a CUDA developer. The NNP solves most problems I face when I want to write CUDA kernels which are optimized for deep learning. This chip is the first true deep learning chip.</p>
<p>In general, for a 1-chip vs 1-chip ranking, we will see Nervana &gt; AMD &gt; NVIDIA, just because NVIDIA has to service gaming/deep learning/high-performance computing at once, while AMD only needs to service gaming/deep learning, whereas Nervana can just concentrate on deep learning – a huge advantage. The more concentrated a designed architecture, the less junk is on the chip for deep learning.</p>
<p>However, the winner is not determined by pure performance, and not even by pure cost/performance. It is determined by cost/performance + community + deep learning frameworks.</p>
<p>Let&#8217;s have a closer look at the individual positions of Nervana, AMD, and NVIDIA to see where they stand.</p>
<h2>Nervana’s Neural Network Processors</h2>
<blockquote class="twitter-tweet" data-lang="en">
<p dir="ltr" lang="en">Why <a href="https://twitter.com/NaveenGRao?ref_src=twsrc%5Etfw">@NaveenGRao</a> is excited about <a href="https://twitter.com/hashtag/Intel?src=hash&amp;ref_src=twsrc%5Etfw">#Intel</a> Nervana NNP: <a href="https://t.co/DAqgOYFtoR">https://t.co/DAqgOYFtoR</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://t.co/dbLMdnp63t">pic.twitter.com/dbLMdnp63t</a></p>
<p>— Intel AI (@IntelAI) <a href="https://twitter.com/IntelAI/status/925476243788697600?ref_src=twsrc%5Etfw">October 31, 2017</a></p></blockquote>
<p><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>Nervana’s design is very special mainly due to its large programmable caches (similar to CUDA shared memory) which are 10 times bigger per chip compared to GPUs and 50 times bigger per compute unit compared to GPUs. With this one will be able to design in-cache algorithms and models. This will speed up inference by at least an order of magnitude and one will be able to easily train on terabytes of data with small in-cache deep learning models, say, a multi-layer LSTM with 200 units. This will make this chip very attractive for startups and larger companies. Due to a special datatype, Flexpoint, one is able to store more data in caches/RAM and compute faster yielding even more benefits. All of this could mean speedup of about 10x compared to current NVIDIA GPUs for everybody. But this is only so if the main obstacles can be overcome: Community and software.</p>
<p>For the normal users and researchers, it will all depend on the community. Without community, we will not see in-cache algorithms. Without community, we will not see good software frameworks and it will be difficult to work with the chip. Everybody wants to use solid deep learning frameworks and it is questionable if Neon, Nervana’s deep learning framework, is up for the task. Software comes before hardware. If Nervana only ships pretty chips and does not push the software and community aspect effectively it will lose out to AMD and NVIDIA.</p>
<p>The community and software question is tightly bound to the price. If the price is too high, and students are not able to afford the NNP then no community can manifest itself around it. You do not get robust communities by just catering for industry. Although industry yields the main income for hardware companies, students are the main driver for the community. So if the price is right and students can afford it, then the community and the software will follow. Anything above $3000 will not work out. Anything above $2000 is critical and one would require special discounts for students to create a robust community. An NNP priced at $2000 will be manageable and find some adoption. Anything below $1500 will make Nervana the market leader for at least 2-3 years. An NNP at $1000 would make it extremely tough for NVIDIA and AMD to compete — software would not even be a question here, it follows automatically.</p>
<p>I personally will switch to NNPs if they are priced below $2500. They are just so much superior to GPUs for deep learning and I will be able to do things which are just impossible with NVIDIA hardware. If they are over $2500 then it also reaches my pain point for good hardware. I save up a lot of money to buy hardware — good hardware is just important to me — but I have to live from something.</p>
<p>For usual consumers not only the price will be important, but also how the community is handled. If we do not see Intel immediately pumping resources into the community to start up a solid software machinery then the NNP is likely to stagnate and die off. Unfortunately, Intel has a good history of mismanaging communities — it would be a shame if this happens because I really would like to see Nervana succeed.</p>
<p>In summary, we will see Nervana’s NNP will emerge as a clear winner if it will be priced below $2000 and if we see strong community and software development within the first few months after its release. With a higher price and less community support, the NNP will be strong, but might not be able to surpass other solutions in terms of cost/performance and convenience. If the software and community efforts fail or if the NNP is priced at $4000 it will likely fail. A price above $2000 will require significant discounts for students for the NNP to be viable.</p>
<h2>AMD: Cheap and Powerful – If You Can Use It</h2>
<p>&nbsp;</p>
<p><figure id="attachment_630" aria-describedby="caption-attachment-630" style="width: 500px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg"><img data-attachment-id="630" data-permalink="https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/amd_fiji_gpu_package_with_gpu_hbm_memory_and_interposer/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?fit=3872%2C2592&amp;ssl=1" data-orig-size="3872,2592" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;5&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;NIKON 1 V1&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1434546595&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;22.7&quot;,&quot;iso&quot;:&quot;400&quot;,&quot;shutter_speed&quot;:&quot;0.066666666666667&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="AMD_Fiji_GPU_package_with_GPU,_HBM_memory_and_interposer" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?fit=300%2C201&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?fit=1024%2C685&amp;ssl=1" class="wp-image-630" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer-300x201.jpg?resize=500%2C335" alt="" width="500" height="335" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?resize=300%2C201&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?resize=768%2C514&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?resize=1024%2C685&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?w=2000&amp;ssl=1 2000w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/AMD_Fiji_GPU_package_with_GPU_HBM_memory_and_interposer.jpg?w=3000&amp;ssl=1 3000w" sizes="(max-width: 500px) 100vw, 500px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-630" class="wp-caption-text">Source: <a href="https://commons.wikimedia.org/wiki/File:AMD_Fiji_GPU_package_with_GPU,_HBM_memory_and_interposer.jpg">Wikipedia</a></figcaption></figure></p>
<p>AMDs cards are incredible. The Vega Frontier Edition series clearly outmatches NVIDIA counterparts, and, from <a href="https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/">unbiased benchmarks</a>&nbsp;of Volta vs Pascal, it seems that the Vega Frontier will be on-a-par or better compared to a Titan V if it is liquid cooled. Note that the Vega is based on an old architecture while the Titan V is brand new. The new AMD architecture, which will be released in 2018Q3 will increase performance further still.</p>
<p>AMD hopes to advance deep learning hardware by just switching from 32-bit floats to 16-bit floats. This is a very simple and powerful strategy. The chips will not be useful for high-performance computing, but they will be solid for gamers and the deep learning community while development costs will be low because 16-bit float computation is straightforward.</p>
<p>They will not be able to compete in terms of performance with Nervana’s NNP, but the cost/performance might outmatch everything on the market. You can get a liquid cooled Vega Frontier for $700 which might be just a little worse than a $3000 Titan V.</p>
<p>The problem is software. Even if you have this powerful AMD GPU, you will hardly be able to use it – no major framework supports AMD GPUs well enough.</p>
<p>AMD is in limbo itself – in software limbo. It seems they want to abandon OpenCL for <a href="https://github.com/ROCm-Developer-Tools/HIP">HIP</a> but currently they officially still push and support the OpenCL path. If they push through with HIP, and if they put some good deep learning software on the market (not only libraries for convolution and matrix multiplication but full deep learning frameworks, say, HIP support for PyTorch) in the next 9 months then their release of their new GPU in 2018Q3 has the potential to demolish all competitors.</p>
<p>So in summary, if AMD gets its shit together in terms of software, it might become the dominating deep learning hardware solution.</p>
<h2>NVIDIA: The Titan</h2>
<p><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg"><img data-attachment-id="632" data-permalink="https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/titan_v/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?fit=1920%2C1080&amp;ssl=1" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Titan_V" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?fit=1024%2C576&amp;ssl=1" class="aligncenter wp-image-632" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V-1024x576.jpg?resize=500%2C281" alt="" width="500" height="281" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/12/Titan_V.jpg?w=1920&amp;ssl=1 1920w" sizes="(max-width: 500px) 100vw, 500px" data-recalc-dims="1" /></a></p>
<p>NVIDIA’s position is solid. They have the best software, the best tools, their hardware is good and the community is large, strong and well integrated.</p>
<p>NVIDIA’s main issue is that they have to serve multiple communities: High-performance computing people, deep learning people, and gamers, and this is a huge strain on their hardware. It is expensive to design chips which are custom made for these communities and NVIDIA’s strategy was currently to design a one-size-fits-all architecture. This worked until it didn’t. The Titan V is just mediocre all-around.</p>
<p>With the emerging competitors, NVIDIA has two choices. (1) Push the price on their cards down until they starve the competition to death, or (2) they can develop specialized deep learning hardware on their own. NVIDIA has the resources to pursue the first strategy, and it also has the expertise for the second strategy. A new design, however, will take some time and NVIDIA might lose the throne to another company in the meantime. So we might see both strategies played out a once: Starving competitors so that NVIDIA can compete until their own deep learning chip hits the market.</p>
<p>In summary, NVIDIAs throne is threatened, but it has the resources and the expertise to fight against emerging players. We will probably see cheaper NVIDIA cards in the future and chips which are more specialized for deep learning. If NVIDIA does not lower its prices it might (temporarily) pass the throne to another player.</p>
<h2>Conclusion</h2>
<p>Deep learning hardware limbo means that it makes no sense to invest in deep learning hardware right now, but it also means we will have cheaper NVIDIA cards, usable AMD cards, and ultra-fast Nervana cards quite soon. It is an exciting time and we consumers will profit from this immensely. But for now, we have to be patient. We have to wait. I will keep you updated as the situation changes.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/">Deep Learning Hardware Limbo</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2017/12/21/deep-learning-hardware-limbo/feed/</wfw:commentRss>
			<slash:comments>91</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">627</post-id>	</item>
		<item>
		<title>Credit Assignment in Deep Learning</title>
		<link>https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/</link>
					<comments>https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/#comments</comments>
		
		<dc:creator><![CDATA[Tim Dettmers]]></dc:creator>
		<pubDate>Sat, 16 Sep 2017 17:17:00 +0000</pubDate>
				<category><![CDATA[Academia]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[PhD]]></category>
		<guid isPermaLink="false">http://timdettmers.com/?p=596</guid>

					<description><![CDATA[<p>This morning I got an email about my blog post discussing the history of deep learning which rattled me back into a time of my academic career which I rather not think about. It was a low point which nearly ended my Master studies at the University of Lugano, and it made me feel so [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/">Credit Assignment in Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>This morning I got an email about my blog post discussing the <a href="https://developer.nvidia.com/blog/deep-learning-nutshell-history-training/">history of deep learning </a>which rattled me back into a time of my academic career which I rather not think about. It was a low point which nearly ended my Master studies at the University of Lugano, and it made me feel so bad about blogging that I took two long years to recover. So what has happened?</p>
<p><span id="more-596"></span></p>
<p>When I started my masters, I worked on blog posts for NVIDIA which featured introductions into deep learning. Part of this blog post series also discusses the history of deep learning. I hence discussed what I thought to be the historical milestones with the largest impact but in doing so, I inadvertently assigned credit to researchers that I thought had a good impact on the field. I worked on this blog post and circulated it in my deep learning class&#8217;s forums to the dismay of my then advisor who holds the opposite view of mine.</p>
<p>To evaluate the credit that a research idea deserves, I believe that it is not only important who has the first idea, but I also believe that it is equally important to actually make it work (the implementation). My ex-advisor believed that it only really matters who was the first who published the idea.</p>
<p>My advisor scolded me in class for my views since he felt very strongly that the first idea counts and that my view is plain wrong. To redeem myself and to salvage the relationship with him, I felt coerced to change my blog post to his wishes.</p>
<p>This quasi-censorship of my blog post eviscerated me, and in consequence, I lost all desire to blog for two years. Despite my efforts, the relationship with my then advisor deteriorated further, and I had to look for a new advisor.</p>
<p>Looking back at the blog post that I produced, I feel ashamed. It does not express my personal views. I value integrity, and my behavior did not reflect who I want to be.</p>
<p>I write this blog post to discuss my true beliefs about credit assignment and why I believe that the idea, its communication and its implementation are all equally important.</p>
<h1>Who Deserves Credit for Deep Learning Ideas?</h1>
<p>There has been a lot of discussion about how to assign credit to researchers, or in other words, how to determine whose work had a large impact. Note that I do not discuss here who deserves credit for discovering an idea, I look at who deserves credit for the impact that an idea has. Looking at this, there are two main camps: The first believes that ideas and implementation count equally, and, the second believes that it counts who had the ideas first.</p>
<p>The problem with this discussion is that it is not a scientific topic, but a philosophical one. How do we determine what has how much value? We use the scientific method. What is the scientific method in philosophy? Use reductions to arrive at simple statements, then use logic to derive other factual statements, failing that — like in this case — we make thought experiments where we isolate variables which we then take to extremes. Let&#8217;s do this now to get insight into the issue.</p>
<h2>All Ideas, No Communication, No Implementation</h2>
<p>Let&#8217;s imagine there exists a person that has come up with all ideas in deep learning of the past and all ideas in deep learning of the future. However, this person cannot communicate&nbsp;with either words or writing. This person also cannot write code. How much credit deserves such a person?</p>
<p>I would argue that such a person deserves zero credit. In fact, I think it is epistemologically correct that this person deserves no credit because nobody can know that he or she deserves credit.</p>
<h2>All Ideas, 1 Communication + No Ideas, Full Communication</h2>
<p>We have a Person 1 that invented everything in deep learning. Now this person can communicate, but he or she is so unclear that only a single Person 2 can understand these ideas.</p>
<p>Now, Person 2 has no creativity but is a perfect communicator. Person 2 basically just translates what Person 1 said and the entire world understands. Who deserves credit here?</p>
<p>It is tempting to think that Person 2 deserves all the credit because Person 1 is useless without Person 2. But similarly, Person 2 is useless without Person 1.</p>
<p>Both people thus deserve equal credit — no one can achieve anything without the other.</p>
<h2>All Ideas, Full Communication, 1 Implementation</h2>
<p>Let&#8217;s increase the complexity of the problem. Let us say the duo of Person 1 and Person 2 spread the ideas so that the entire world understands deep learning, but let us assume that all people are implementation agnostic. Nobody can make deep learning work. The world knows about all deep learning ideas but cannot solve any problem with it. In such a world, the ideas of deep learning are quickly abandoned by the large majority due to their uselessness (just like the majority of the population does not care much about pure mathematics, e.g., few care if&nbsp;<span class="texhtml">a<sup>n</sup> + b<sup>n</sup> = c<sup>n</sup></span>&nbsp;is true for all integer n &gt;2).</p>
<p>Enter Person 3. Person 3 has no creativity, cannot communicate, but he or she can implement all the deep learning ideas in a practical manner. The world looks at this person&#8217;s code and suddenly is able to solve all problems which are solvable with deep learning.</p>
<p>Who deserves the most credit: Person 1, Person 2, or Person 3?</p>
<p>As discussed before, Person 1 and Person 2 deserve equal credit, and also here, I would argue, that Person 3 deserves equal credit.</p>
<p>This becomes apparent when we think about the value of ideas. Ideas are useful when they have an affect. If they have no or only a small effect they just deserve no recognition or little recognition. If deep learning ideas have no practical value then they would not deserve more recognition than, say, the idea that there might be something beyond the observable universe — it is a nice idea, but it will never produce anything of much value.</p>
<h1>Comparative Individual Value For Collective Contributions</h1>
<p>The evaluation changes if we distribute the contributions of ideas, communication, and implementation among many individuals. If we can take the three scenarios above, expand Person 1-3 into groups of people and subject them to comparative evaluation, that is, how much value the contributions of each individual has compared to all the other people have we arrive at the following thought experiment.</p>
<h2>1 Ideas, 1000 Communication, 1000 Implementation</h2>
<p>We have 1 person who has all the ideas, 1000 people who can understand these ideas and communicate them to the world, and 1000 people who can implement them to yield practical value, then how do we assign credit?</p>
<p>As discussed it is reasonable that each of the areas, (1) ideas, (2) communication, (3) implementation deserve equal credit. If now the groups of 1000 people made contributions (communications and implementations) of equal value, it would be fair to say that:</p>
<ul>
<li>1 Ideas: 1/3 credit</li>
<li>1000 Communication: 1/3000 credit each</li>
<li>1000 Implementation: 1/3000 credit each.</li>
</ul>
<p>We see in this case the one person with the idea should receive the largest amount of credit.</p>
<p>Similarly, if we weight the numbers differently, and if we assume contributions of individuals in groups are equal, then this credit assignment holds for all other combinations like (1000, 1, 1000), or (10000, 1000, 1).</p>
<h1>Timing and Relational Effects</h1>
<p>In the real world, we have timing effects and relational effects. Not all 1000 Ideas, Communication, or Implementation people will publish their work at the same time, but they will have a specific sequence. In this sequence, they will influence and build on each other — they stand on the shoulders of giants. Who are the giants? Who deserves what amount of credit?</p>
<p>If we think about it, it is not much different than our first analysis. Lets take Person 1 that only has ideas and can communicate his or her ideas to only one other Person 2; Person 2, standing on Person 1&#8217;s shoulders, is only able to communicate the ideas to another person Person 3; Person 3, standing on Person 2&#8217;s shoulders, in turn, can communicate the ideas clearly to the entire world.</p>
<p>If we express the ability of people as numbers which represent the fraction of all value ideas, communication, and implementation we could weight Person 1, Person 2, and Person 3 in this way:</p>
<ul>
<li>Person1: [1, 1/10^10, 0]</li>
<li>Person2: [0, 1/10^10, 0]</li>
<li>Person3: [0, 1, 0]</li>
</ul>
<p>Which means that Person 1, has all the ideas (1), could communicate these ideas to 1 person (we assume a total population of 10 billion people to make the math easier). Person 2 has no ideas, could understand Person 1&#8217;s idea but could only communicate this idea to one other person, Person 3. Person 3 has no ideas, understands the idea of Person 2 and can communicate it so that everybody understands. Note that this example is simplified so that all people are implementation agnostic.</p>
<p>From these fractions, we see that Person 2 has almost no fraction of contributions since Person 2 is not creative and also not a good communicator. However, if we look at the relational effects we know Person 3 would have no value without Person 2, and Person 1 would also have no value without Person 2. So how do we solve this credit assignment problem?</p>
<p>We can try to solve this problem by expressing it as a weighted graph which expressed relationships over time and the relationships of the fractions with respect to the world.</p>
<p><figure id="attachment_602" aria-describedby="caption-attachment-602" style="width: 534px" class="wp-caption aligncenter"><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png"><img data-attachment-id="602" data-permalink="https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/relational_example-4/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?fit=534%2C449&amp;ssl=1" data-orig-size="534,449" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="relational_example" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?fit=300%2C252&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?fit=534%2C449&amp;ssl=1" class="wp-image-602 size-full" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?resize=534%2C449" alt="" width="534" height="449" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?w=534&amp;ssl=1 534w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2017/09/relational_example-3.png?resize=300%2C252&amp;ssl=1 300w" sizes="(max-width: 534px) 100vw, 534px" data-recalc-dims="1" /></a><figcaption id="caption-attachment-602" class="wp-caption-text">Graphical representation of this particular credit assignment problem: The world has 10^10 people (self-weight: 1). Person 1 (P1) has all the ideas that exist in the world (1) and can communicate to one other person in the world (1/10^10), that is P2 (1); P2 can communicate the ideas to one person in the world (1/10^10), which is P3 (1); P3 can communicate the idea to the entire world in an understandable way (1). Connections between P1-P2, and P2-P3 are bidirectional, meaning that it is important (a) to understand and (b) to communicate ideas.</figcaption></figure></p>
<p>How we weight the contribution of each&nbsp;person in this case? There are many answers to this, but here PageRank would be a good fit. PageRank works exactly as we discussed above, the credit is assigned&nbsp;comparatively, that is if we have a (1, 1000, 1000) distribution, the largest chunk of PageRank will be distributed by the single person. Thus it reflects our evaluation system. PageRank also takes into account the relationships between nodes and their recursive weight (standing on the shoulders of giants).</p>
<p>Using the scenario above, we find the contributions as follows:</p>
<p>
<table id="tablepress-2" class="tablepress tablepress-id-2">
<thead>
<tr class="row-1 odd">
	<th class="column-1">Name</th><th class="column-2">PageRank</th><th class="column-3">Relative Contribution</th>
</tr>
</thead>
<tbody class="row-hover">
<tr class="row-2 even">
	<td class="column-1">P2</td><td class="column-2">0.3450</td><td class="column-3">0.4319</td>
</tr>
<tr class="row-3 odd">
	<td class="column-1">P1</td><td class="column-2">0.2697</td><td class="column-3">0.3376</td>
</tr>
<tr class="row-4 even">
	<td class="column-1">P3</td><td class="column-2">0.1841</td><td class="column-3">0.2305</td>
</tr>
</tbody>
</table>
<!-- #tablepress-2 from cache --></p>
<p>We see that P2 has the largest contribution despite being only the bridge between P1 and P3 who have the largest fractions (all the ideas and full communication abilities). However, P1&#8217;s success depends on P2, and P3&#8217;s success depends on P2 and as such P2 is the most critical link in the entire system.</p>
<p>This is quite insightful. If you understand some obscure research and communicate this to just a few&nbsp;researchers who, in turn, influence many other researchers then you will have made a substantial contribution to the deep learning community.</p>
<p>It would not feel this way because you will probably not experience any fame or recognition here. The recognition will come for P1 (having ideas) and P3 (communicating ideas). But still, the numbers do not lie here.</p>
<p>This experiment was quite interesting, and if you want to experiment&nbsp;a bit by yourself, you can&nbsp;<a href="https://github.com/TimDettmers/CreditAssignment">download the code</a> to see what happens if you add more people and more relationships among these people. This exercise can give quite some insight into what is valuable for research.</p>
<h1>Response to Criticism on Reddit</h1>
<p>There has been some <a href="https://www.reddit.com/r/MachineLearning/comments/70j88n/d_credit_assignment_in_deep_learning_tim_dettmers/">sharp criticism on Reddit</a> concerning ideas expressed in this blog post. The user metacurse makes the point that in science we credit usually those researchers who had the idea first and that communication and implementation are not valued. For example we value Albert Einstein more highly for the discovery of general relativity and the photoelectric effect and not its communication by Neil deGrasse Tyson; similarly, Cocks is credited for RSA even though he never implemented it in any way that was widely used (and he could not produce public implementations due to the classified status of RSA). However, this entire argument is rather weak and unfair:</p>
<ul>
<li>I do not discuss who should be credited for an idea or the usage of the idea, I discuss who should be credited for the overall <em>impact</em> of an idea. These are very different questions.</li>
<li>He uses examples to try to prove his own hypothesis when we know that <a href="https://en.wikipedia.org/wiki/Problem_of_induction">examples cannot prove anything</a>&nbsp;(he uses classical philosophic techniques, which has some value, but it does not generate any reliable knowledge like analytical philosophy does). He mocks me for not using examples myself.</li>
<li>He appeals to the emotion of the readers, by saying that my views endorse unethical ideas like &#8220;stealing olds ideas and rebranding them as your own&#8221; when it has nothing to do with my argument (reductio ad Hitlerum). He does this quite successfully swaying many emotional readers. I do not think this is helpful.</li>
</ul>
<p>To make a sharper contrast why metacurse&#8217;s argument is not relevant to mine take this thought experiment.</p>
<p>We have a super genius who knows about all possible ideas and writes them down so that everybody can understand it easily. Then she locks these notes away in a locker and dies the next second. Over the next billions of years humanity rediscovers all ideas and uses them to build a flourishing society where all living things live in harmony and every being is fulfilled and so forth. One second before the last human dies in heat death, that human discovers the notebook.</p>
<p>Metacurse&#8217;s argument would look for the answer to the question: Should our super genius be credited for inventing everything? Metacurse would argue, yes, and I would totally agree.</p>
<p>What I discuss in this blog post: How much impact did our super genius have on the overall impact of all ideas? Very little, she never had any direct or even indirect effect with any of the ideas; the only impact she had was that one other person understood that she had the ideas before others had them. That is the total impact of her ideas. Her impact is almost zero.</p>
<h1>Conclusion</h1>
<p>Here I discussed how it is best to think about contributions in deep learning. From thought experiments, we could see that ideas, their communication, and their implementation are equally important contributions.</p>
<p>We also discussed how timing effects and dependencies could be modeled in a relational graph. We found that people that link ideas to communicators can make substantial contributions to the research community even if they themselves are not creative or good communicators. Creating the links between influential ideas and influential communicators (or people that implement) are important here.</p>
<p>The post <a rel="nofollow" href="https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/">Credit Assignment in Deep Learning</a> appeared first on <a rel="nofollow" href="https://timdettmers.com">Tim Dettmers</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://timdettmers.com/2017/09/16/credit-assignment-deep-learning/feed/</wfw:commentRss>
			<slash:comments>15</slash:comments>
		
		
		<post-id xmlns="com-wordpress:feed-additions:1">596</post-id>	</item>
	</channel>
</rss>
