<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/topic/computing.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Fri, 04 Nov 2022 19:04:21 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.aimbeagNFKGtififsLPFvztNYGr1_NMvLOOT1mPOjEU/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Introduction to the 5 Pillars of Data Acquisition</title><link>https://go.teledynelecroy.com/ieee_five_pillars_webinar</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Join Teledyne SP Devices for a live webinar about the foundations of data acquisition. </p><p>Explore digitizer fundamentals and learn more about analog front-end (AFE), triggering, clocking, signal processing, and the use of general-purpose input/output (GPIO).</p><p><a href="https://go.teledynelecroy.com/ieee_five_pillars_webinar" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a></p>]]></description><pubDate>Fri, 04 Nov 2022 19:04:21 +0000</pubDate><guid>https://go.teledynelecroy.com/ieee_five_pillars_webinar</guid><category>Analog</category><category>Data acquisition</category><category>Digitizer</category><category>Signal processing</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Nanowire Synapses 30,000x Faster Than Nature’s</title><link>https://spectrum.ieee.org/neuromorphic-computing-superconducting-synapse</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/small-and-large-squares-on-a-grey-background.jpg?id=32013586&width=1245&height=700&coordinates=0%2C133%2C0%2C133"/><br/><br/><p>Artificial-intelligence systems are increasingly limited by the hardware used to implement them. Now comes a new superconducting photonic circuit that mimics the links between brain cells—burning just 0.3 percent of the energy of its human counterparts while operating some 30,000 times as fast.</p><p>In <u><a href="https://spectrum.ieee.org/deep-neural-network" target="_self">artificial neural networks</a></u>, components called neurons are fed data and cooperate to solve a problem, such as <u><a href="https://spectrum.ieee.org/deep-learning-facial-recognition-technology" target="_self">recognizing faces</a></u>. The neural net repeatedly adjusts the <u><a href="https://spectrum.ieee.org/artificial-synapses" target="_self">synapses</a></u>—the links between its neurons—and determines whether the resulting patterns of behavior are better at finding a solution. Over time, the network discovers which patterns are best at computing results. It then adopts these patterns as defaults, mimicking the process of learning in the human brain.</p><p>Although <u><a href="https://spectrum.ieee.org/ai-failures" target="_self">AI systems</a></u> are increasingly finding <u><a href="https://spectrum.ieee.org/trustworthy-ai" target="_self">real-world applications</a></u>, they face a number of major challenges, given the hardware used to run them. One solution that researchers have investigated to solve this problem is to develop brain-inspired “<u><a href="https://spectrum.ieee.org/neuromorphic-computing-more-than-ai" target="_self">neuromorphic</a>”</u> computer hardware.</p><p class="pull-quote">“When I look around at all the concepts that have been unearthed, I really feel like we’re onto something.”<br/>—Jeffrey Shainline, NIST</p><p>For instance, neuromorphic microchip components may “<u><a href="https://spectrum.ieee.org/neuromorphic-computing-with-lohi2" target="_self">spike</a></u>,” or generate an output signal, only after they receive a certain amount of input signals over a given time, a strategy that more closely mimics how real biological neurons behave. By only rarely firing spikes, these devices shuffle around much less data than typical artificial neural networks and, in principle, require much less power and communication bandwidth.</p><p>However, neuromorphic hardware typically uses conventional electronics, which ends up limiting their complexity and speed. For example, biological neurons can each possess tens of thousands of synapses, but neuromorphic devices struggle to connect their artificial neurons to a few others. One solution is <u><a href="https://spectrum.ieee.org/neural-network-multiplex" target="_self">multiplexing</a></u>, in which a single data channel may carry many signals at the same time. However, as chips become larger and more intricate, computations may slow down.</p><p>In a new study, researchers explored using optical transmitters and receivers to connect neurons instead. Optical links, or waveguides, can in principle connect each neuron with thousands of others at light-speed communication rates.</p><p>The scientists used superconducting nanowire devices capable of detecting single photons. These optical signals are the smallest possible, constituting the physical limit of energy efficiency.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="c0ebb87f220db2429fc16ec5641dd6d3" data-rm-shortcode-name="rebelmouse-image" id="d90a0" loading="lazy" src="https://spectrum.ieee.org/media-library/this-artist-u2019s-u00a0rendering-shows-how-superconducting-circuits-that-mimic-synapses-u2014points-of-interface-between-neuron.jpg?id=32013601&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This artist’s rendering shows how superconducting circuits that mimic synapses—points of interface between neurons in the brain—might be used to create artificial optoelectronic neurons of the future.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">J. Chiles and J. Shainline/NIST</small></p><p>Performing photonic neural computations is often tricky because optical cavities that can entrap light for long spans of time are usually needed. Creating such cavities, and linking them with many waveguides, is very challenging to accomplish on an integrated microchip.</p><p>Instead, the researchers developed hybrid circuitry, in which the output signals from each detector were converted to ultrafast electrical pulses roughly 2 picoseconds long. These pulses each consisted of a single magnetic fluctuation, or <u><a href="https://en.wikipedia.org/wiki/Fluxon" rel="noopener noreferrer" target="_blank">fluxon</a></u>, within a network of <a href="https://en.wikipedia.org/wiki/SQUID" target="_blank">superconducting quantum-interference devices</a>, or SQUIDs. </p><p>“We’ve been doing theoretical work for years to try to identify the principles that will enable technology that can achieve the physical limits of neuromorphic computing,” says <a href="https://www.nist.gov/people/jeff-shainline" target="_blank">Jeffrey Shainline</a>, a researcher at NIST. “Pursuit of that objective has delivered us to this concept—combine optical communication at the single-photon level with neural computation performed by Josephson junctions.”</p><p>SQUIDs consist of one or more <a href="https://spectrum.ieee.org/a-new-hypersensitive-magnetometer-based-on-kinetic-inductance" target="_self"><u>Josephson j</u><u>unctions</u></a>—a sandwich of superconducting materials separated by a thin insulating film. If the current through the Josephson junction exceeds a certain threshold value, the SQUID begins to produce fluxons.<br/></p><p>Upon sensing a photon, the single-photon detector yields fluxons, which collect as current in the SQUID’s superconducting loop. This stored current serves as a form of memory, providing a record of how many times a neuron produced a spike.</p><p>“It was surprising that it was pretty easy to get the circuits to work,” Shainline says. “The fabrication and experiments took quite a bit of time in the design phase, but the circuits actually worked the first time we fabricated them. That bodes well for the future scalability of such systems.”</p><p>The scientists integrated the single-photon detector with the Josephson junction, creating a superconducting synapse. They calculated that the synapses are capable of spike rates exceeding 10 million hertz while consuming roughly 33 attojoules of power per synaptic event (an attojoule is 10<sup>-18</sup> of a joule). In contrast, human neurons have a maximum average spike rate of about <u><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5067378/" target="_blank">340 hertz</a></u> and consume roughly <u><a href="https://www.sciencedirect.com/science/article/pii/S2542435121000052" rel="noopener noreferrer" target="_blank">10 femtojoules</a></u> per synaptic event (a femtojoule is 10<sup>-15</sup> of a joule).</p><p>“When I look around at all the concepts that have been unearthed, I really feel like we’re onto something,” Shainline says. “It could be profoundly transformative.”</p><p>In addition, the scientists can vary the output from their devices from hundreds of nanoseconds to milliseconds. This means the hardware can interface with a range of systems, from high-speed electronics to more leisurely interactions with humans.</p><p>In the future, the researchers will combine their new synapses with on-chip sources of light to create fully integrated superconducting neurons.</p><p>“There are big remaining challenges there, but if we can get that last part integrated, there are many reasons to think the result could be a computational platform of immense power for artificial intelligence,” Shainline says.</p><p>The scientists recently detailed <u><a href="https://www.nature.com/articles/s41928-022-00840-9" rel="noopener noreferrer" target="_blank">their findings</a></u> online in the journal <em>Nature Electronics</em>.</p>]]></description><pubDate>Fri, 28 Oct 2022 17:24:53 +0000</pubDate><guid>https://spectrum.ieee.org/neuromorphic-computing-superconducting-synapse</guid><category>Superconducting</category><category>Artificial intelligence</category><category>Synapse</category><category>Josephson junctions</category><category>Single-photon detectors</category><category>Neural networks</category><category>Spiking neural networks</category><category>Optoelectronics</category><category>Neuromorphic computing</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/small-and-large-squares-on-a-grey-background.jpg?id=32013586&amp;width=980"></media:content></item><item><title>Micro 4D Printing Builds on Programmable Matter</title><link>https://spectrum.ieee.org/4d-printing-microscale</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/gif-of-flower-and-octopus-side-by-side-growing-larger.gif?id=32013280&width=1245&height=700&coordinates=25%2C0%2C26%2C0"/><br/><br/><p>Objects that can transform themselves after they’ve been built could have a host of useful applications in everything from robotics to biomedicine. A new technique that combines <u><a href="https://spectrum.ieee.org/tag/3d-printing" target="_self">3D printing</a></u> and an ink with dynamic chemical bonds can create microscale structures whose size and mechanical properties can be dynamically tuned after fabrication.<br/></p><p> The idea of “<u><a href="https://spectrum.ieee.org/make-your-own-world-with-programmable-matter" target="_self">programmable matter</a></u>” that can adapt its physical properties in response to stimuli has been around for decades, but recent advances in additive manufacturing have brought it within reach. By 3D printing with materials that react to things like high temperatures or electric currents, researchers have been able to create objects whose form or function can morph and adapt.</p><p> The approach is known as <u><a href="https://spectrum.ieee.org/4d-printing-turns-carbon-fiber-wood-into-shapeshifting-programmable-materials" target="_self">4D printing</a></u>, with time being the extra dimension, and has been used to create everything from <u><a href="https://www.nature.com/articles/srep31110#acknowledgements" target="_blank">robotic grippers</a></u> to adaptable <u><a href="https://spectrum.ieee.org/4d-bioprinting-smart-constructs-for-the-heart#toggle-gdpr" target="_self">scaffolds </a>for growing biological tissue</u>. Now researchers have unveiled a new technique that makes it possible to print intricate microscale structures that can later be expanded to up to eight times their size or made harder or softer at will.</p><p class="pull-quote">“This is a very fundamental thing on top of which you can build more ideas afterwards.”<br/>—Eva Blasco, University of Heidelberg</p><p> The key is a novel ink formulation that includes dynamic chemical bonds that can be opened and closed, making it possible to adjust the mechanical properties of the printed object or allow the incorporation of new material. Integrating these kinds of dynamic bonds and characteristics into 3D printed objects could allow users to carefully tweak a wide range of physical characteristics after fabrication, says <u><a href="https://www.cam.uni-heidelberg.de/user/details?id=196" rel="noopener noreferrer" target="_blank">Eva Blasco</a></u>, a professor of polymer chemistry at the University of Heidelberg, who led the research.</p><p> “It’s the first time that we have this ‘livingness’ concept, that we have active bonds in the printed microstructure that allow us to grow further or change the mechanical properties,” she says. “This is a very fundamental thing on top of which you can build more ideas afterwards.”</p><p> To create their detailed microstructures, Blasco’s team relied on an approach known as two-photon laser printing (2PLP), which works by shining a laser beam at inks that chemically react when exposed to light. Typically this involves a reaction known as <a href="https://en.wikipedia.org/wiki/Polymerization" target="_blank">polymerization</a>, in which smaller molecules known as monomers react together to create long chains known as polymers.</p><p> Wherever the laser is focused it creates networks of polymers. And so by carefully scanning over the photosensitive material, it is possible to build up complex 3D objects with precise features. The approach, Blasco says, is able to operate at smaller scales than other 3D printing methods.</p><p> The researchers’ main innovation, reported in a <a href="https://onlinelibrary.wiley.com/doi/10.1002/adfm.202207826" rel="noopener noreferrer" target="_blank"><u>paper in</u><u><em> Advanced Functional Materials</em></u></a><em>, </em>was to create a new ink formulation that results in a polymer featuring a special kind of chemical bond, known as a <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201813525#:~:text=Dynamic%20covalent%20bonds%20(DCBs)%20have,these%20DCBs%20in%20polymeric%20materials." target="_blank">dynamic covalent bond</a>. When certain chemicals are introduced to the polymer, these bonds can open and close, which makes it possible to tune the microstructure of the printed object after fabrication, says Blasco.</p><p> To demonstrate the potential of their approach, her team 3D-printed microscale models of a flower, an octopus, and a gecko. These objects were just a few micrometers across, fabricated via the researchers’ specially devised ink, which creates a rubbery substance when polymerized. The newly printed objects were then immersed in a vial containing styrene—a <a href="https://en.wikipedia.org/wiki/Monomer" target="_blank">monomer</a> that creates a glassy polymer called polystyrene—before being heated, which caused the dynamic covalent bonds to open.</p><p> Once the bonds opened up, strands of polystyrene formed in between the two ends, causing the volume of the microscale models to grow by as much as eight times in around four hours. The addition of polystyrene also resulted in the models becoming significantly harder, but despite the expansion they retained their shape and the fine detail of their structures.</p><p> In a separate experiment, the researchers showed they could alter the mechanical properties of printed structures without changing their size by heating them in the presence of a molecule that helps open up the dynamic bonds. This made the objects softer, because opening up the bonds reduces the amount of cross-linking in the polymer networks.</p><p> Tunable materials that swell or expand have been investigated for some time, says <u><a href="https://www.brunel.ac.uk/people/eujin-pei" rel="noopener noreferrer" target="_blank">Eujin Pei</a>,</u> a reader in additive manufacturing at Brunel University London. He says the new research is more an extension of current knowledge and capabilities in 4D printing than a significant advancement. He points out that most 4D-printing research focuses on achieving actuation, or a sequence of movements, rather than simply expanding in size.</p><p> Blasco qualifies this assessment, though, with the stipulation that her group’s approach is a proof of concept and still admittedly some distance from practical applications. Nonetheless, she says, the incorporation of dynamic covalent chemistry into 3D printing has considerable potential and will enable some unique capabilities. It could, she adds, potentially allow new monomers with novel functionality, such as elasticity or fluorescence, to be introduced after fabrication, all of which open up a range of novel possibilities.</p><p> Most 4D-printing approaches, according to Blasco, rely on materials that essentially flick between one of two states. In contrast, Blasco’s group’s method makes it possible to tune the material’s properties across a range of values. “From the application point of view, you have much more versatility to adapt the material properties by controlling the proportion of open and closed bonds,” says Blasco.</p><p> The team’s next steps will be to adapt their ink’s chemistry toward more specific applications, says Blasco, including finding ways to make it biocompatible.</p>]]></description><pubDate>Fri, 28 Oct 2022 14:57:30 +0000</pubDate><guid>https://spectrum.ieee.org/4d-printing-microscale</guid><category>4d printing</category><category>3d printing</category><category>Programmable matter</category><category>Polymers</category><category>Chemistry</category><category>Additive manufacturing</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/gif-of-flower-and-octopus-side-by-side-growing-larger.gif?id=32013280&amp;width=980"></media:content></item><item><title>Why Functional Programming Should Be the Future of Software Development</title><link>https://spectrum.ieee.org/functional-programming</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/strings-of-code-of-varying-colors-jumbling-and-tumbling-in-space-on-a-gradient-purple-backdrop.gif?id=31947274&width=1245&height=700&coordinates=0%2C380%2C0%2C381"/><br/><br/><p>
<strong>You’d expect</strong> <span style="background-color: initial;">the longest and most costly phase in the life</span> <span style="background-color: initial;">cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.</span>
</p><p>
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
</p><hr/><p>
	The struggle that most companies have maintaining code causes a second problem: fragility. Every new feature that gets added to the code increases its complexity, which then increases the chance that something will break. It’s common for software to grow so complex that the developers avoid changing it more than is absolutely necessary for fear of breaking something. In many companies, whole teams of developers are employed not to develop anything new but just to keep existing systems going. You might say that they run a software version of the 
	<a href="https://en.wikipedia.org/wiki/Red_Queen%27s_race" target="_blank">Red Queen’s race</a>, running as fast as they can just to stay in the same place.
</p><p>
	It’s a sorry situation. Yet the current trajectory of the software industry is toward increasing complexity, longer product-development times, and greater fragility of production systems. To address such issues, companies usually just throw more people at the problem: more developers, more testers, and more technicians who intervene when systems fail.
</p><p>
	Surely there must be a better way. I’m part of a growing group of developers who think the answer could be functional programming. Here I describe what functional programming is, why using it helps, and why I’m so enthusiastic about it.
</p><h2>With functional programming, less is more</h2><p>
	A good way to understand 
	<span style="background-color: initial;">the rationale for functional programming is by considering something that happened more than a half century ago. In the late 1960s, a programming paradigm emerged that aimed to improve the quality of code while reducing the development time needed. It was called </span><a href="https://en.wikipedia.org/wiki/Structured_programming" rel="noopener noreferrer" target="_blank">structured programming</a><span style="background-color: initial;">.</span>
</p><p>
	Various languages emerged to foster structured programming, and some existing languages were modified to better support it. One of the most notable features of these structured-programming languages was not a feature at all: It was the absence of something that had been around a long time—
	<a href="https://en.wikipedia.org/wiki/Goto" rel="noopener noreferrer" target="_blank">the GOTO statement</a>.
</p><p>
	The GOTO statement is used to redirect program execution. Instead of carrying out the next statement in sequence, the flow of the program is redirected to some other statement, the one specified in the GOTO line, typically when some condition is met.
</p><p>
<span></span>The elimination of the GOTO was based on what programmers had learned from using it—that it made the program very hard to understand. Programs with GOTOs were often referred to as spaghetti code because the sequence of instructions that got executed could be as hard to follow as a single strand in a bowl of spaghetti.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt='A plate of spaghetti made from code with a single strand of "spaghetti code" being pulled from the top of the frame in a neverending loop on a blue gradient background.' class="rm-shortcode" data-rm-shortcode-id="43605a2429891dad9116625a44962284" data-rm-shortcode-name="rebelmouse-image" id="12a26" loading="lazy" src="https://spectrum.ieee.org/media-library/a-plate-of-spaghetti-made-from-code-with-a-single-strand-of-spaghetti-code-being-pulled-from-the-top-of-the-frame-in-a-neveren.gif?id=31954498&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Shira Inbar</small>
</p><p>
	The inability of these developers to understand how their code worked, or why it sometimes didn’t work, was a complexity problem. Software experts of that era believed that those GOTO statements 
	<a href="https://dl.acm.org/doi/10.1145/362929.362947" target="_blank">were creating unnecessary complexity</a> and that the GOTO had to, well, go.
</p><p>
	Back then, this was a radical idea, and many programmers resisted the loss of a statement that they had grown to rely on. The debate went on for more than a decade, but in the end, the GOTO went extinct, and no one today would argue for its return. That’s because its elimination from higher-level programming languages greatly reduced complexity and boosted the reliability of the software being produced. It did this by limiting what programmers could do, which ended up making it easier for them to reason about the code they were writing.
</p><p>
	Although the software industry has eliminated GOTO from modern higher-level languages, software nevertheless continues to grow in complexity and fragility. Looking for how else such programming languages could be modified to avoid some common pitfalls, software designers can find inspiration, curiously enough, from their counterparts on the hardware side.
</p><h2>Nullifying problems with null references</h2><p>
	In designing hardware 
	<span style="background-color: initial;">for a computer, you can’t</span> have a resistor shared by<span style="background-color: initial;">, say,</span> both the keyboard and the monitor’s circuitry<span style="background-color: initial;">. But programmers do this kind of sharing all the time in their software. It’s called shared global state: Variables are owned by no one process but can be changed by any number of processes, even simultaneously.</span><br/>
</p><p>
	Now, imagine that every time you ran your microwave, your dishwasher’s settings changed from Normal Cycle to Pots and Pans. That, of course, doesn’t happen in the real world, but in software, this kind of thing goes on all the time. Programmers write code that calls a function, expecting it to perform a single task. But many functions have side effects that change the shared global state, 
	<a href="https://softwareengineering.stackexchange.com/questions/148108/why-is-global-state-so-evil" target="_blank">giving rise to unexpected consequences</a>.
</p><p>
	In hardware, that doesn’t happen because the laws of physics curtail what’s possible. Of course, hardware engineers can mess up, but not like you can with software, where just too many things are possible, for better or worse.
</p><p>
	Another complexity monster lurking in the software quagmire is called a 
	<a href="https://en.wikipedia.org/wiki/Null_pointer" target="_blank">null reference</a>, meaning that a reference to a place in memory points to nothing at all. If you try to use this reference, an error ensues. So programmers have to remember to check whether something is null before trying to read or change what it references.
</p><p>
	Nearly every popular language today has this flaw. The pioneering computer scientist 
	<a href="http://www.cs.ox.ac.uk/people/tony.hoare/" target="_blank">Tony Hoare</a> introduced null references in the <a href="https://en.wikipedia.org/wiki/ALGOL" target="_blank">ALGOL</a> language back in 1965, and it was later incorporated into numerous other languages. Hoare explained that he did this “simply because it was so easy to implement,” but today he considers it to be a “billion-dollar mistake.” That’s because it has caused countless bugs when a reference that the programmer expects to be valid is really a null reference.
</p><p>
	Software developers need to be extremely disciplined to avoid such pitfalls, and sometimes they don’t take adequate precautions. The architects of structured programming knew this to be true for GOTO statements and left developers no escape hatch. To guarantee the improvements in clarity that GOTO-free code promised, they knew that they’d have to eliminate it entirely from their structured-programming languages.
</p><p>
	History is proof that removing a dangerous feature can greatly improve the quality of code. Today, we have a slew of dangerous practices that compromise the robustness and maintainability of software. Nearly all modern programming languages have some form of null references, shared global state, and functions with side effects—things that are far worse than the GOTO ever was.
</p><p>
	How can those flaws be eliminated? It turns out that the answer 
	<a href="https://www.math-cs.gordon.edu/courses/cps323/LISP/lisp.html" target="_blank">has been around for decades</a>: purely functional programming languages.
</p><h3></h3><br/><div class="flourish-embed flourish-chart" data-src="visualisation/11061373?820658"><script src="https://public.flourish.studio/resources/embed.js"></script></div><p class="caption">Of the top dozen functional-programming languages, Haskell is by far the most popular, judging by the number of GitHub repositories that use these languages.</p><p>
	The first purely functional language to become popular, called 
	<a href="https://www.haskell.org/" target="_blank">Haskell</a>, was created in 1990. So by the mid-1990s, the world of software development really had the solution to the vexing problems it still faces. Sadly, the hardware of the time often wasn’t powerful enough to make use of the solution. But today’s processors can easily manage the demands of Haskell and other purely functional languages.
</p><p>
	Indeed, software based on pure functions is particularly well suited to modern 
	<a href="https://spectrum.ieee.org/the-trouble-with-multicore" target="_self">multicore CPUs</a>. That’s because pure functions operate only on their input parameters, making it impossible to have any interactions between different functions. This allows the compiler to be optimized to produce code that runs on multiple cores efficiently and easily.
</p><p>
	As the name suggests, with purely functional programming, the developer can write only pure functions, which, by definition, cannot have side effects. With this one restriction, you increase stability, open the door to compiler optimizations, and end up with code that’s far easier to reason about.
</p><p>
	But what if a function needs to know or needs to manipulate the state of the system? In that case, the state is passed through a long chain of what are called composed functions—functions that pass their outputs to the inputs of the next function in the chain. By passing the state from function to function, each function has access to it and there’s no chance of another concurrent programming thread modifying that state—another common and costly fragility found in far too many programs.
</p><div class="ieee-sidebar-large">
<strong><strong>
<h3>Avoiding Null-Reference Surprises </h3>
<p style="color: #666666">
<strong>A comparison of Javascript and Purescript shows how the latter can help programmers avoid bugs.</strong>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="8b2fdb5e9a5c50eb2754245cf72e3a76" data-rm-shortcode-name="rebelmouse-image" id="aaaf5" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=31996907&width=980"/>
</p>
</strong></strong>
</div><p>
	Functional programming also has a solution to Hoare’s “billion-dollar mistake,” null references. It addresses that problem by disallowing nulls. Instead, there is a construct usually called 
	<em>Maybe</em> (or <em>Option</em> in some languages). A <em>Maybe</em> can be <em>Nothing</em> or <em>Just</em> some value. Working with <em>Maybe</em><em>s</em> forces developers to always consider both cases. They have no choice in the matter. They must handle the <em>Nothing</em> case every single time they encounter a <em>Maybe</em>. Doing so eliminates the many bugs that null references can spawn.
</p><p>
	Functional programming also requires that data be immutable, meaning that once you set a variable to some value, it is forever that value. Variables are more like variables in math. For example, to compute a formula, 
	<em>y</em> = <em>x</em><sup>2</sup> + 2<em>x</em> – 11, you pick a value for <em>x</em> and at no time during the computation of <em>y</em> does <em>x</em> take on a different value. So, the same value for <em>x</em> is used when computing <em>x</em><sup>2</sup> as is used when computing 2<em>x</em>. In most programming languages, there is no such restriction. You can compute <em>x</em><sup>2</sup> with one value, then change the value of <em>x</em> before computing 2<em>x</em>. By disallowing developers from changing (mutating) values, they can use the same reasoning they did in middle-school algebra class.
</p><p>
	Unlike most languages, functional programming languages are deeply rooted in mathematics. It’s this lineage in the highly disciplined field of mathematics that gives functional languages their biggest advantages.
</p><p>
	Why is that? It’s because people have been working on mathematics for thousands of years. It’s pretty solid. Most programming paradigms, such as object-oriented programming, have at most half a dozen decades of work behind them. They are crude and immature by comparison.
</p><p class="pull-quote" style="">
	Imagine if every time you ran your microwave, your dishwasher’s settings changed from Normal Cycle to Pots and Pans. In software, this kind of thing goes on all the time.
</p><p>
	Let me share an example of how programming is sloppy compared with mathematics. We typically teach new programmers to forget what they learned in math class when they first encounter the statement 
	<em>x = x + 1</em>. In math, this equation has zero solutions. But in most of today’s programming languages, <em>x = x + 1 </em>is not an equation. It is a <em>statement</em> that commands the computer to take the value of <em>x</em>, add one to it, and put it back into a variable called <em>x</em>.
</p><p>
	In functional programming, there are no statements, only 
	<em>expressions</em>. Mathematical thinking that we learned in middle school can now be employed when writing code in a functional language.
</p><p style="">
	Thanks to functional purity, you can reason about code using algebraic substitution to help reduce code complexity in the same way you reduced the complexity of equations back in algebra class. In non-functional languages (imperative languages), there is no equivalent mechanism for reasoning about how the code works.
</p><h2>Functional programming has a steep learning curve</h2><p>
	Pure functional programming solves many of our industry’s biggest problems by removing dangerous features from the language, making it harder for developers to shoot themselves in the foot. At first, these limitations may seem drastic, as I’m sure the 1960s developers felt regarding the removal of GOTO. But the fact of the matter is that it’s both liberating and empowering to work in these languages—so much so that nearly all of today’s most popular languages have incorporated functional features, although they remain fundamentally imperative languages.
</p><p>
	The biggest problem with this hybrid approach is that it still allows developers to ignore the functional aspects of the language. Had we left GOTO as an option 50 years ago, we might still be struggling with spaghetti code today.
</p><p>
	To reap the full benefits of pure functional programming languages, you can’t compromise. You need to use languages that were designed with these principles from the start. Only by adopting them will you get the many benefits that I’ve outlined here.
</p><p>
	But functional programming isn’t a bed of roses. It comes at a cost. Learning to program according to this functional paradigm is almost like learning to program again from the beginning. In many cases, developers must familiarize themselves with math that they didn’t learn in school. The required math isn’t difficult—it’s just new and, to the math phobic, scary.
</p><p>
	More important, developers need to learn a new way of thinking. At first this will be a burden, because they are not used to it. But with time, this new way of thinking becomes second nature and ends up reducing cognitive overhead compared with the old ways of thinking. The result is a massive gain in efficiency.
</p><p>
	But making the transition to functional programming can be difficult. My own journey doing so a few years back is illustrative.
</p><p>
	I decided to learn Haskell—and needed to do that on a business timeline. This was the most difficult learning experience of my 40-year career, in large part because there was no definitive source for helping developers make the transition to functional programming. Indeed, no one had written anything very comprehensive about functional programming in the prior three decades.
</p><p class="pull-quote" style="">
	To reap the full benefits of pure functional programming languages, you can’t compromise. You need to use languages that were designed with these principles from the start.
</p><p>
	I was left to pick up bits and pieces from here, there, and everywhere. And I can attest to the gross inefficiencies of that process. It took me three months of days, nights, and weekends living and breathing Haskell. But finally, I got to the point that I could write better code with it than with anything else.
</p><p>
	When I decided that our company should switch to using functional languages, I didn’t want to put my developers through the same nightmare. So, I started building a curriculum for them to use, which became the basis for a book intended to help developers transition into functional programmers. In 
	<a href="https://leanpub.com/fp-made-easier" target="_blank">my book</a>, I provide guidance for obtaining proficiency in a functional language called <a href="https://www.purescript.org/" target="_blank">PureScript</a>, which stole all the great aspects of Haskell and improved on many of its shortcomings. In addition, it’s able to operate in both the browser and in a back-end server, making it a great solution for many of today’s software demands.
</p><p>
	While such learning resources can only help, for this transition to take place broadly, software-based businesses must invest more in their biggest asset: their developers. At my company, 
	<a href="http://www.panosoft.com/" target="_blank">Panoramic Software</a>, where I’m the chief technical officer, we’ve made this investment, and all new work is being done in either PureScript or Haskell.
</p><p>
	We started down the road of adopting functional languages three years ago, beginning with another pure functional language called 
	<a href="https://elm-lang.org/" target="_blank">Elm</a> because it is a simpler language. (Little did we know we would eventually outgrow it.) It took us about a year to start reaping the benefits. But since we got over the hump, it’s been wonderful. We have had no production runtime bugs, which were so common in what we were formerly using, <a href="https://en.wikipedia.org/wiki/JavaScript" target="_blank">JavaScript</a> on the front end and Java on the back. This improvement allowed the team to spend far more time adding new features to the system. Now, we spend almost no time debugging production issues.
</p><p>
	But there are still challenges when working with a language that relatively few others use—in particular, the lack of online help, documentation, and example code. And it’s hard to hire developers with experience in these languages. Because of that, my company uses recruiters who specialize in finding functional programmers. And when we hire someone with no background in functional programming, we put them through a training process for the first few months to bring them up to speed.
</p><h2>Functional programming’s future</h2><p>
	My company is small. It delivers software to governmental agencies to enable them to help veterans receive benefits from the 
	<a href="https://www.va.gov/" target="_blank">U.S. Department of Veteran’s Affairs</a>. It’s extremely rewarding work, but it’s not a lucrative field. With razor-slim margins, we must use every tool available to us to do more with fewer developers. And for that, functional programming is just the ticket.
</p><p>
	It’s very common for unglamorous businesses like ours to have difficulty attracting developers. But we are now able to hire top-tier people because they want to work on a functional codebase. Being ahead of the curve on this trend, we can get talent that most companies our size could only dream of.
</p><p>
	I anticipate that the adoption of pure functional languages will improve the quality and robustness of the whole software industry while greatly reducing time wasted on bugs that are simply impossible to generate with functional programming. It’s not magic, but sometimes it feels like that, and I’m reminded of how good I have it every time I’m forced to work with a non-functional codebase.
</p><p>
	One sign that the software industry is preparing for a paradigm shift is that functional features are showing up in more and more mainstream languages. It will take much more work for the industry to make the transition fully, but the benefits of doing so are clear, and that is no doubt where things are headed.
</p>]]></description><pubDate>Sun, 23 Oct 2022 15:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/functional-programming</guid><category>Computer programming</category><category>Software development</category><dc:creator>Charles Scalfani</dc:creator><media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/strings-of-code-of-varying-colors-jumbling-and-tumbling-in-space-on-a-gradient-purple-backdrop.gif?id=31947274&amp;width=980"></media:content></item><item><title>Pong-in-a-Dish</title><link>https://spectrum.ieee.org/pong-in-a-dish</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-grid-with-colorful-blocks-that-look-like-lights-and-columns-a-box-shows-control-panels-and-another-shows-a-visualizer-of-the.jpg?id=31994756&width=1245&height=700&coordinates=0%2C61%2C0%2C62"/><br/><br/><p>Ever hear of the <a href="https://spectrum.ieee.org/untold-history-of-ai-charles-babbage-and-the-turk" target="_self">Turk</a>—the 19th<sup></sup>-century mechanism topped by a turbaned head that played chess against all comers? In fact, hidden inside was a diminutive chessmaster, one you might imagine deadpanning, “<a href="https://www.youtube.com/watch?v=pyxJ7GKGFG0" rel="noopener noreferrer" target="_blank">Eh, It’s a living.</a>”</p><p>Then there’s its namesake, the <a href="https://spectrum.ieee.org/untold-history-of-ai-mechanical-turk-revisited-tktkt" target="_self">Mechanical Turk</a>—a 21st<sup></sup>-century service offered by Amazon to mark up images on the Web with the help of crowdsourced freelancers. They, too, might intone, glassy-eyed, “It’s a living.”</p><p>Now we have a kind of Biological Turk. A mass of neurons act as a computer that mimics a human being playing the classic computer game Pong. The neurons, some taken from mouse embryos, others grown from human precursor cells, spread out into a one-layer, 800,000-cell mesh called a biological neural network, which lives in a giant petri dish called the DishBrain. There it interfaces with arrays of electrodes that form an interface to silicon hardware. Software mounted on that hardware provides stimulation and feedback, and the minibrain learns how to control a paddle on a simulated ping-pong table.</p><p>The work was described recently in the journal <a href="https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6#%20" rel="noopener noreferrer" target="_blank"><em>Neuron</em></a> by Brett Kagan, the chief scientific officer of <a href="https://corticallabs.com/" rel="noopener noreferrer" target="_blank">Cortical Labs</a>, a startup in Melbourne, Australia, and nine colleagues at that company.</p><p>The authors talk hopefully about the emergence of sentience, a notion that other brain-in-a-dish researchers have also <a href="https://www.nature.com/articles/d41586-020-02986-y" rel="noopener noreferrer" target="_blank">recently floated</a>. But they seem to stand on solid ground when they say their method will help to advance brain science, on the one hand, and computer science, on the other. A bio-neuro-network might model the effects of drugs on the brain in ways that single-cell neurons can’t. Also, neurons may show themselves to be more than just protoplasmic logic switches but more like entire computers.</p><p>The question before us, though, is how does the thing play Pong?</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="0d57b1d55d16d51cf831540a01c2f883" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/GJaXiR_uvVI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Pong-in-a-Dish</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://youtu.be/GJaXiR_uvVI" target="_blank">youtu.be</a>
</small>
</p><p>First, the electronic scaffolding hits the minibrain with electrical signals that represent the position and movement of the virtual ball. It’s rather like the action potential that a firing neuron would use to convey, say, a sensory signal from the eye to the brain. Because the electrodes are placed at different points in the cell network, the system physically represents the different possible locations. Further information comes from the frequency of the signals, which varies with the distance of the ball to the virtual paddle.<br/></p><p>The network responds to these stimuli like a motor neuron, sending out a signal that moves the virtual paddle. If the resulting movement causes the ball to bounce, the neural network gets a “reward.” Failure results in a signal that has the opposite effect.<span></span></p><p>“Reward” is put in sneer quotes because these cells don’t have feelings. They can’t experience the joy of victory, the agony of defeat. There’s no dopamine, no salted popcorn. Instead, the researchers say, the network is working to minimize unpredictability. In this view, the so-called reward is a predictable signal, the anti-reward is an unpredictable one.</p><p>Kagan tells <em>IEEE Spectrum</em> that the system as a whole then reorganizes to become better at playing the game. The most marked improvement came in the first five minutes of play.</p><p>It seems amazing that a mere 800,000 neurons can model the world, even a simplified world. But, Kagan says, such feats are seen in nature. "Flies have even fewer neurons but must be able to do some modeling—although perhaps not in a way a human may—to navigate a complex and changing 3D world," he says.</p><p>As he and his colleagues point out in their report the ability of neurons to adapt to external stimuli is well established <em>in vivo</em>; it forms the basis for all animal learning. But theirs, they say, is the first <em>in vitro </em>demonstration involving a goal-directed behavior. </p><p>The current version of Pong is forgiving. The paddle is broad, the volley slow, the ball unspinning. Even a neophyte would crush DishBrain. Then again, the same was true of all of AI’s early assays in game playing.</p><p>The early chess machines would sometimes senselessly give up first a pawn, then a piece, then the queen—all because they were attempting to put off a disagreeable action to a point beyond the built-in <a href="https://spectrum.ieee.org/cars-may-think-but-will-they-achieve-artificial-stupidity" target="_self">planning horizon</a>. Poker-playing programs <a href="https://spectrum.ieee.org/poker-pros-battle-artificial-intelligence-to-statistical-draw" target="_self">got good</a> pretty fast, but the early ones sometimes played too well—that is, too cautiously—against weak human opponents, which reduced their winnings. Car navigation programs would send you into a vacant lot.</p><p>You might think that just getting a machine to play a decent game is the hard part, and that further improving it to perfection ought to be a snap. Edgar Allan Poe made that judgement when he called the Turk a fraud because it occasionally erred. His conclusion was correct but his reasoning was faulty.</p><p>It’s not easy turning a barely there machine into a world champion at chess or <a href="https://spectrum.ieee.org/alphago-wins-match-against-top-go-player" target="_self">Go</a>. And yet it has been done.</p>]]></description><pubDate>Sat, 22 Oct 2022 14:09:13 +0000</pubDate><guid>https://spectrum.ieee.org/pong-in-a-dish</guid><category>Neuroscience</category><category>Brain computer interface</category><category>Video games</category><category>Pong</category><category>Neurons</category><category>Biological neural networks</category><dc:creator>Philip E. Ross</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-grid-with-colorful-blocks-that-look-like-lights-and-columns-a-box-shows-control-panels-and-another-shows-a-visualizer-of-the.jpg?id=31994756&amp;width=980"></media:content></item><item><title>Accelerate the Future of Innovation</title><link>https://connectlp.keysight.com/Emerging_Technologies_White_Papers?elqCampaignId=22934&amp;cmpid=ASC-2106531&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=303</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=27151193&width=980"/><br/><br/><p>Looking for help with technical challenges related to emerging technologies like 5G, 6G, and quantum computing?<u></u><u></u></p><p><a href="https://connectlp.keysight.com/Emerging_Technologies_White_Papers?elqCampaignId=22934&cmpid=ASC-2106531&utm_source=ADSC&utm_medium=ASC&utm_campaign=303" rel="noopener noreferrer" target="_blank">Download these three whitepapers</a> to help inspire and accelerate your future innovations:<u></u><u></u></p><ul><li><strong>“5G Private Networks Enable Business Everywhere” <em>by MIT Technology Review Insights</em></strong> — Learn how 5G private networks will drive innovations in many industrial and enterprise sectors<u></u><u></u></li><li><strong>“How 6G Will Change the Way We Interact with Technology” </strong><em><strong>by MIT Technology Review Insights</strong></em> — Learn how Keysight and Nokia investigate the sub-terahertz spectrum to power the bandwidth needed to make 6G technology real<u></u><u></u></li><li><strong>“Quantum Computing: Infrastructure and Scaling Challenges”</strong> — Learn how researchers overcome challenges in this emerging computing ecosystem</li></ul>]]></description><pubDate>Tue, 18 Oct 2022 12:00:01 +0000</pubDate><guid>https://connectlp.keysight.com/Emerging_Technologies_White_Papers?elqCampaignId=22934&amp;cmpid=ASC-2106531&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=303</guid><category>5g</category><category>6g</category><category>Innovation</category><category>Keysight</category><category>Quantum computing</category><category>Type:whitepaper</category><dc:creator>Keysight</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/27151193/origin.png"></media:content></item><item><title>AI Language Models Are Struggling to “Get” Math</title><link>https://spectrum.ieee.org/large-language-models-math</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/the-words-alice-has-five-more-balls-than-bob-who-has-two-balls-after-he-gives-four-to-charlie-how-many-balls-does-alice-have.jpg?id=31891762&width=1245&height=700&coordinates=0%2C100%2C0%2C101"/><br/><br/><p>If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math.<br/></p><p>Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to <a href="https://spectrum.ieee.org/tag/large-language-models" target="_blank">natural language generation models</a>—algorithms such as <a href="https://arxiv.org/abs/2103.03874" target="_blank">GPT-2/3</a> and <a href="https://arxiv.org/abs/2109.02102" target="_blank">GPT-Neo</a>. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?”</p><p>“When we say computers are very good at math, they’re very good at things that are quite specific,” says <a href="https://research.google/people/GuyGurAri/" target="_blank">Guy Gur-Ari</a>, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle.</p><p class="pull-quote">“I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”<br/>—Ethan Dyer, Google</p><p>Solving word problems, or “<a href="https://pll.harvard.edu/course/quantitative-reasoning-practical-math?delta=0" target="_blank">quantitative reasoning</a>,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10.</p><p>“We work on math because we find it independently very interesting,” says <a href="https://scholar.google.com/citations?user=stCljMYAAAAJ&hl=en" target="_blank">Karl Cobbe</a>, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.”</p><p>As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach.</p><p>Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, <a href="https://paperswithcode.com/paper/measuring-mathematical-problem-solving-with" target="_blank">MATH</a> and <a href="https://paperswithcode.com/dataset/gsm8k" rel="noopener noreferrer" target="_blank">GSM8K</a>, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the <a href="https://safe.ai/" target="_blank">Center for AI Safety</a> who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.)</p><p>Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, <em>Clockwork Orange</em>–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says.</p><p>At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced <a href="https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html" target="_blank">78 percent accuracy</a> with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says.</p><p>Minerva uses Google’s own language model, <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" target="_blank">Pathways Language Model</a> (PaLM), which is fine-tuned on scientific papers from the <a href="http://arxiv.org" target="_blank">arXiv</a> online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer.</p><p>The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization.</p><p>What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a <a href="https://en.wikipedia.org/wiki/TI-84_Plus_series" target="_blank">TI-84</a>.</p><p>“Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.”</p><p>“I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says <a href="https://scholar.google.com/citations?user=LWeVRdUAAAAJ&hl=en" target="_blank">Ethan Dyer</a>, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred.</p><p><br/></p><p><strong></strong><em><strong>Update 14 Oct. 2022: </strong>A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of </em><em>large language generation models being taught to handle math word problems. </em><em>Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut. </em></p>]]></description><pubDate>Wed, 12 Oct 2022 13:55:00 +0000</pubDate><guid>https://spectrum.ieee.org/large-language-models-math</guid><category>Large language models</category><category>Machine learning</category><category>Artificial intelligence</category><category>Mathematics</category><dc:creator>Dan Garisto</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/the-words-alice-has-five-more-balls-than-bob-who-has-two-balls-after-he-gives-four-to-charlie-how-many-balls-does-alice-have.jpg?id=31891762&amp;width=980"></media:content></item><item><title>Ultrafast Racetrack Memory Enters the Third Dimension</title><link>https://spectrum.ieee.org/racetrack-memory</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-artist-s-depiction-of-a-3d-racetrack-memory-device.jpg?id=31879936&width=1245&height=700&coordinates=0%2C187%2C0%2C188"/><br/><br/><p>Racetrack memory could hold vast amounts of data that can be accessed extraordinarily quickly. Now, in a new study, scientists reveal 3D racetrack memory devices that may greatly increase the potential of this technology.</p><p><a href="https://spectrum.ieee.org/making-memory-with-light" target="_self">Racetrack memory</a> encodes bits of data in the form of <a href="https://spectrum.ieee.org/domain-wall" target="_blank">magnetic domain walls</a>. These walls divide a material into <u><a href="https://spectrum.ieee.org/antiferromagnets-ram" target="_self">domains</a>,</u> inside which magnetic poles all point in the same direction. Electric pulses can push these domain walls back and forth within nanowires, making them run like race cars down a track. Magnetic sensors and other electronics can then read and write data on any point on this racetrack.</p><p><a href="https://spectrum.ieee.org/swirly-skyrmions-could-be-the-future-of-data-storage" target="_self">Racetrack memory</a> potentially “has unrivaled density compared to other memory technologies,” says <a href="https://en.wikipedia.org/wiki/Stuart_Parkin" target="_blank">Stuart Parkin</a>, director of the <a href="https://www.mpi-halle.mpg.de/home" target="_blank">Max Planck Institute of Microstructure Physics</a> in Halle, Germany, and <a href="https://www.ibm.com/ibm/history/ibm100/us/en/icons/racetrack/" rel="noopener noreferrer" target="_blank">who developed the concept while at IBM</a>. Racetrack memory is solid-state like flash memory, lacking unwieldy moving parts, and is nonvolatile, storing data even after the power is removed.</p><p>In addition, racetrack memory’s electric pulses can move domain walls at speeds of up to <a href="https://www.nature.com/articles/s41467-021-25292-1" rel="noopener noreferrer" target="_blank">kilometers per second</a>. This suggests devices using <a href="https://spectrum.ieee.org/ferroelectric-domain-wall-memory-shows-its-promise" target="_blank">this kind of memory</a> can perform exceptionally quickly.</p><p>“Memories that operate at speeds of less than 1 nanosecond per write operation are possible,” Parkin says. “This is as fast or faster than the fastest write speeds of any commercially available memory today.”</p><p>Most research on <a href="https://spectrum.ieee.org/singing-to-data-gets-it-to-move" target="_self">racetrack memory</a> to date has focused on 2D devices. However, Parkin had long suggested it could be a 3D technology, with arrays of vertical racetracks much like a city filled with skyscrapers. An extra dimension added to racetrack memory would greatly increase the amount of data it could hold on a given footprint. But assembling such complex 3D structures, given the standard fabrication techniques used by the microelectronics industry, has proven difficult.</p><p>Now Parkin and his colleagues have developed a 3D racetrack memory device using a new method that lets them drape a 2D racetrack onto a 3D surface. They detailed <u><a href="https://www.nature.com/articles/s41565-022-01213-1" rel="noopener noreferrer" target="_blank">their findings</a></u> online 22 September in the journal <em>Nature Nanotechnology</em>.</p><p>The scientists fabricated a complex ultrathin film made of layers of tantalum nitride, cobalt, nickel, platinum, ruthenium, and magnesium oxide onto a water-soluble layer of strontium aluminate. They then immersed the film in water, releasing a freestanding magnetic film.</p><p>The researchers next transferred this magnetic film onto a sapphire surface engineered with speed-bump-like 3D ridges, a bit like one might drape a drop cloth over furniture while painting a room. Standard photolithography techniques then let them create racetracks made of wires—each 3 micrometers wide—with arching segments rising up to 900 nanometers high, following the shape of the sapphire surface.</p><p>“The 3D racetrack structure prepared and handled in this way survived intact, and behaves almost identically to the originally deposited structure before it was detached,” Parkin says.</p><p>In experiments, domain walls in the 3D racetracks traveled at speeds of up to 600 meters per second. “We believe that the speed can be increased by more than 10 times in related structures using novel materials,” Parkin says.</p><p>The scientists are exploring “many different ways of building 3D racetrack memories,” Parkin adds. “There are many interesting challenges that are still to be explored, which makes research into racetrack memory very exciting.”</p>]]></description><pubDate>Sun, 09 Oct 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/racetrack-memory</guid><category>Nanowires</category><category>Spintronics</category><category>Domain wall</category><category>Memory</category><category>Antiferromagnetism</category><category>Racetrack memory</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-artist-s-depiction-of-a-3d-racetrack-memory-device.jpg?id=31879936&amp;width=980"></media:content></item><item><title>Machine Learning Shaking Up Hard Sciences, Too</title><link>https://spectrum.ieee.org/machine-learning-in-physics</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-view-of-the-underground-alice-detector-used-to-study-heavy-ion-physics-at-the-large-hadron-collider-lhc.jpg?id=31878708&width=1245&height=700&coordinates=0%2C234%2C0%2C235"/><br/><br/><p>Particle physicists have long been early adopters—if not <a href="https://www.symmetrymagazine.org/article/the-coevolution-of-particle-physics-and-computing" target="_blank">inventors</a>—of tech from email to the Internet. It’s not surprising, then, that <a href="https://www.sciencedirect.com/science/article/abs/pii/S037026939701112X" target="_blank">as early as 1997</a>, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight.<br/></p><p>“I felt very threatened by machine learning,” says <a href="https://physics.mit.edu/faculty/jesse-thaler/" target="_blank">Jesse Thaler</a>, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says. </p><p>Over the past decade, in tandem with the broader<a target="_blank"> </a><a href="https://spectrum.ieee.org/what-is-deep-learning" target="_self">deep-learning revolution</a>, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges. </p><p class="pull-quote">Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data.<br/></p><p>For starters, particle-physics data is very different from the typical data used in machine learning. Though <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" target="_blank">convolutional neural networks</a> (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to <a href="https://jduarte.physics.ucsd.edu/" target="_blank">Javier Duarte</a>, a particle physicist at the University of California, San Diego, is that collision data such as that from the <a href="https://spectrum.ieee.org/tag/large-hadron-collider" target="_blank">Large Hadron Collider</a>, isn’t naturally an image.</p><p>Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs).</p><p>Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says <a href="https://www.physics.uci.edu/people/daniel-o-whiteson" target="_blank">Daniel Whiteson</a>, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded.</p><p>But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8299073/" rel="noopener noreferrer" target="_blank">techniques</a> like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (<a href="https://info.cobaltiron.com/blog/petabyte-how-much-information-could-it-actually-hold" target="_blank">equivalent to</a> about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating <a href="https://lup.lub.lu.se/student-papers/search/publication/9004751" rel="noopener noreferrer" target="_blank">strategies to compress</a> the data.</p><p class="pull-quote">“We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”<br/>—Jesse Thaler, MIT</p><p>Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction. </p><p>More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted.</p><p>These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte.</p><p>As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? <a href="https://www.wired.com/2015/08/coding-physics-course/" target="_blank">Stigma against coding</a>—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding.</p><p>“Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.103.036020" rel="noopener noreferrer" target="_blank">algorithms provide feedback</a> in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate.</p><p>“On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.”</p>]]></description><pubDate>Fri, 07 Oct 2022 18:42:45 +0000</pubDate><guid>https://spectrum.ieee.org/machine-learning-in-physics</guid><category>Machine learning</category><category>Graph neural networks</category><category>Particle physics</category><category>Artificial intelligence</category><dc:creator>Dan Garisto</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-view-of-the-underground-alice-detector-used-to-study-heavy-ion-physics-at-the-large-hadron-collider-lhc.jpg?id=31878708&amp;width=980"></media:content></item><item><title>AI’s Grandmaster Status Overshadows Chess Scandal</title><link>https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-men-playing-chess.jpg?id=31869740&width=1245&height=700&coordinates=0%2C0%2C0%2C491"/><br/><br/><p>Last week Magnus Carlsen, the world chess champion, <a href="https://www.nytimes.com/2022/09/28/crosswords/hans-niemann-magnus-carlsen-cheating-update.html" rel="noopener noreferrer" target="_blank">directly accused</a> Hans Niemann, a U.S. grandmaster, of cheating during their game at the <a href="https://en.wikipedia.org/wiki/Sinquefield_Cup" target="_blank">Sinquefield Cup</a>, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks.</p><p>Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides.</p><p>Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the <a href="https://en.chessbase.com/post/statistical-analysis-of-the-games-of-hans-niemann" target="_blank">large number of games</a> he has played in recent years that <a href="https://deadspin.com/ai-all-but-confirms-that-hans-niemann-has-been-cheating-1849593392" target="_blank">get a score of nearly perfect</a> from computer analysis. And behind it all are statements from leading players that they are <a href="https://zwischenzug.substack.com/p/six-thoughts-on-chesss-cheating-scandal" target="_blank">convinced that cheating happens all the time</a> nowadays, though hardly anybody ever gets caught.</p><p class="pull-quote">Computers loom so large because they now play chess like gods. </p><p>What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for <em>Sports Illustrated</em> titled “<a href="https://vindication-of-bobby-fischer.co/history-rodriguez/si/index.html" rel="noopener noreferrer" target="_blank">The Russians Have Fixed World Chess.</a>” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament.</p><p>Nor is the scandal notable for flagrancy. In 1967 <a href="https://en.wikipedia.org/wiki/Milan_Matulovi%C4%87" target="_blank">Milan Matulović</a>, a Yugoslavian grandmaster, shockingly <a href="https://en.wikipedia.org/wiki/Milan_Matulovi%C4%87" rel="noopener noreferrer" target="_blank">took back a move</a> he had just played and only then said “<a href="https://www.chess.com/terms/jadoube-chess" target="_blank">J’adoube</a>,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “<a href="https://www.dorsetchess.co.uk/daily-quiz-answers-4/" target="_blank">J’adoubavić</a>.”</p><p>No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the <a href="https://en.chessbase.com/post/the-most-beautiful-chess-club-in-the-world" target="_blank">Moscow Central Chess Club</a>. No wonder <a href="https://evidently.substack.com/p/are-chess-players-getting-younger" target="_blank">prodigies of the game have gotten younger and younger</a>.</p><p><br/></p><div class="rm-embed embed-media"><div class="flourish-embed flourish-chart" data-src="visualisation/11394050?607871"><script src="https://public.flourish.studio/resources/embed.js"></script></div></div><p><br/></p><p>And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.”</p><p>Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication.</p><p>Several things are at stake. There is the prize money, which runs in the <a href="https://www.chess.com/article/view/how-much-is-the-world-chess-championship-worth" target="_blank">hundreds of thousands of dollars</a> for <a href="https://grandchesstour.org/2022-grand-chess-tour/2022-sinquefield-cup" target="_blank">the circuit of which the Sinquefeld Cup tournament is a part</a>. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play <a href="https://www.chess.com/forum/view/general/will-magnus-carlsen-reach-2900" target="_blank">in the hope of raising his rating to an unprecedented 2900</a>.</p><p>The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of <a href="https://www.chess.com/" target="_blank">Chess.com</a>, the online playing forum in question. Recently, however, the <em>Wall Street Journal </em><a href="https://www.wsj.com/articles/chess-cheating-hans-niemann-report-magnus-carlsen-11664911524" target="_blank">reported</a> that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play. </p><p class="pull-quote">A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it.</p><p>Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent.</p><p>A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a <a href="https://www.independent.co.uk/news/world/americas/magnus-carlson-chess-hans-niemann-b2167248.html" target="_blank">buzzing sex toy</a>. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly <a href="https://www.vice.com/en/article/v7vzvm/cam-site-offers-hans-niemann-dollar1-million-to-play-chess-nude-to-prove-he-didnt-cheat" rel="noopener noreferrer" target="_blank">offered him [US] $1 million</a> to do so.</p><p>Computers loom so large because they now play chess like gods. The best free program, <a href="https://stockfishchess.org/blog/2021/stockfish-14/" rel="noopener noreferrer" target="_blank">Stockfish 14</a>, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a <a href="http://wismuth.com/elo/calculator.html" rel="noopener noreferrer" target="_blank">winning expectancy</a> of 99 percent.</p><p>In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way.</p><p>In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the <a href="https://www.chess.com/openings/Sicilian-Defense" target="_blank">Sicilian Defense</a>, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="58276d7790942bfb08458df24124ac4b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wljgxS7tZVE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Strongest Computer Chess Engines Over Time</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=wljgxS7tZVE" target="_blank">www.youtube.com</a>
</small>
</p><p>“It can’t play endgames,” Slate said. I kicked myself again.</p><p>Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride.</p><p>It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self <a href="http://wismuth.com/elo/calculator.html" rel="noopener noreferrer" target="_blank">99.9 percent</a> of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion.</p><p>Deep Blue filled a room. Today, a smartphone can crush any human player.</p>]]></description><pubDate>Wed, 05 Oct 2022 22:53:41 +0000</pubDate><guid>https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai</guid><category>Computer chess</category><category>Artificial intelligence</category><category>Machine learning</category><dc:creator>Philip E. Ross</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-men-playing-chess.jpg?id=31869740&amp;width=980"></media:content></item><item><title>Modeling Thermal Management Systems for Electronics</title><link>https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=26851526&width=980"/><br/><br/><p>
	The ability to dissipate heat is one of the most important features of modern electronic devices and is usually a limiting factor in the miniaturization of these devices.
</p>
<p>
<span></span>COMSOL Multiphysics includes functionality for heat transfer through conduction, convection, and radiation. Its ability to treat conjugate heat transfer, including laminar and turbulent flow as well as surface-to-surface radiation, has proven to be of great importance for the design and optimization of thermal management systems in electronics. Its multiphysics modeling capabilities also enable the study of thermoelectric effects as well as thermal–structural effects, such as thermal expansion.<u></u><u></u>
</p>
<p>
	In this webinar, we will demonstrate how to create models and apps for conjugate heat transfer in electronic devices. We will also give a general overview of the software’s capabilities for multiphysics modeling, including heat transfer as one of the modeled phenomena.
</p>
<p>
<a href="https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C" target="_blank">Register now for this free webinar!</a>
</p>
<hr/>
<p>
	Speaker<br/>
</p>
<p>
<span></span><span style="background-color: initial;"><strong>Mranal Jain<br/>
</strong></span>Senior Applications Engineer, COMSOL<br/>
	Mranal Jain has been with COMSOL since 2013 and currently leads the transport team in the Burlington, MA office. He studied microfluidics and electrokinetic transport, while pursuing his PhD in chemical engineering at the University of Alberta, Edmonton.
</p>]]></description><pubDate>Mon, 03 Oct 2022 12:00:00 +0000</pubDate><guid>https://event.on24.com/wcc/r/3959064/F4D5B0E14D03698DE8F5FB556899D62C</guid><category>Comsol</category><category>Multiphysics</category><category>Simulation</category><category>Thermal management</category><category>Type:webinar</category><dc:creator>COMSOL</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851526/origin.png"></media:content></item><item><title>Entangled Photons Can Come Out in Webs Now</title><link>https://spectrum.ieee.org/metasurface-entangled-photons</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/purple-toned-close-up-of-an-object-which-has-many-squares-on-it-covered-in-green-light.jpg?id=31842638&width=1245&height=700&coordinates=0%2C253%2C0%2C253"/><br/><br/><p>The equipment that generates <a href="https://spectrum.ieee.org/what-is-quantum-entanglement" target="_blank">quantum entanglement</a> is often bulky and produces entangled photons only one pair at a time. Now scientists have created a device roughly one-third as thick as a penny that can yield complex webs of entangled photons—not just in pairs but several pairs all linked together. The invention may not only greatly simplify the setup needed for quantum technology but also help support more complex quantum applications.</p><p>A common way to generate entangled photons is by shining a beam of light at a special “<a href="https://spectrum.ieee.org/quantum-holography-using-undetected-light" target="_self">nonlinear crysta</a>l.” These crystals each can split a photon into two lower-energy, longer-wavelength entangled photons.</p><p class="pull-quote">“The opportunities are vast, and we have just scratched the surface.”<br/>—Igal Brener, Sandia National Laboratories</p><p>The conventional techniques for producing entangled photons are not flexible—they generate pairs of photons only within specific ranges of wavelengths that are typically very narrow, says study cosenior author <a href="https://mpl.mpg.de/research-at-mpl/independent-research-groups/chekhova-research-group/" target="_blank">Maria Chekhova</a>, a physicist at the Max Planck Institute for the Science of Light in Erlangen, Germany. This narrow bandwidth can limit communication rates.</p><p>In addition, the standard methods for producing entangled photons end up dictating many of the properties of the entangled photons, such as their wavelength and polarization, Chekhova adds. More equipment is needed if one wants to further manipulate these features, she explains.</p><p>Moreover, nonlinear crystals are often bulky. This can prove cumbersome for applications that require many entangled photons. “A quantum computation source would require tens or hundreds of bulky crystals,” says study cosenior author <a href="https://www.lanl.gov/search-capabilities/profiles/igal-brener.shtml" target="_blank">Igal Brener</a>, a physicist at Sandia National Laboratories’ Center for Integrated Nanotechnologies in Albuquerque.</p><p>Instead of a lab full of crystals, lenses, mirrors, filters, and other equipment to generate entangled photons, scientists now find that devices only roughly a half-millimeter thick may suffice. The devices are <a href="https://spectrum.ieee.org/metalens" target="_self">metasurfaces</a>, which are surfaces covered with forests of microscopic pillars.</p><p>“We just need to focus one or more lasers onto a flat sample, and the rest is done by the metasurface,” Brener says.</p><p>The metasurfaces each consist of a glass surface 500 micromillimeters thick covered with gallium arsenide structures that each resemble cubes roughly 300 nanometers wide with notches cut into them. The way in which the composition, structure, and placement of each metasurface’s nanostructures are tailored could help the scientists control many features of light falling onto the devices.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A silver square with 110 pink rectangles in a grid. Blue waves come in the back and orange and red waves come out the front and are linked by white light." class="rm-shortcode" data-rm-shortcode-id="bc4935a69e19bdaad3cd85e538c3e18a" data-rm-shortcode-name="rebelmouse-image" id="b9db1" loading="lazy" src="https://spectrum.ieee.org/media-library/a-silver-square-with-110-pink-rectangles-in-a-grid-blue-waves-come-in-the-back-and-orange-and-red-waves-come-out-the-front-and.jpg?id=31842653&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In this artist rendering of a metasurface, light passes through tiny rectangular structures—the building blocks of the metasurface—and creates pairs of entangled photons at different wavelengths.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Sandia National Laboratories</small></p><p>Shining a laser beam onto these metasurfaces can result in entangled photons emerging. “One single metasurface can, in principle, create several types of entangled photon pairs,” Brener says. “Creating more complex quantum states using multiple photon pairs can lead to new or more efficient ways to do quantum computation, <a href="https://spectrum.ieee.org/superradiance-sensor" target="_self">sensing</a>, encryption, and so on.”</p><p>In addition, metasurfaces could manipulate a range of the entangled photons’ features, “but we haven’t explored that degree of freedom yet,” Brener says. “The opportunities are vast, and we have just scratched the surface.”</p><p>Currently the efficiency of these metasurfaces is low. “We had a rate of less than one pair per second, and standard crystals give hundreds of thousands pairs per second,” Chekhova says. However, she notes, further modifications of the devices may improve efficiency by at least a thousandfold.</p><p>The group’s research was detailed in a study <a href="https://www.science.org/doi/10.1126/science.abq8684" rel="noopener noreferrer" target="_blank">online</a> 25 August in <em>Science. </em></p>]]></description><pubDate>Fri, 30 Sep 2022 15:02:09 +0000</pubDate><guid>https://spectrum.ieee.org/metasurface-entangled-photons</guid><category>Gallium arsenide</category><category>Quantum computing</category><category>Quantum cryptography</category><category>Quantum sensor</category><category>Entanglement</category><category>Nonlinear</category><category>Metasurfaces</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/purple-toned-close-up-of-an-object-which-has-many-squares-on-it-covered-in-green-light.jpg?id=31842638&amp;width=980"></media:content></item><item><title>Machine Learning Will Tackle Quantum Problems, Too</title><link>https://spectrum.ieee.org/machine-learning-quantum</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/vector-art-of-a-head-with-circuits-examining-a-quantum-symbol.jpg?id=31830945&width=1245&height=700&coordinates=0%2C337%2C0%2C338"/><br/><br/><p>Quantum computers may prove far more powerful than any conventional supercomputer when it comes to performing the kinds of complex physics and chemistry simulations that could lead to <a href="https://spectrum.ieee.org/lithium-air-battery-quantum-computing" target="_self">next-generation batteries</a> or <a href="https://spectrum.ieee.org/quantum-drug" target="_self">new drugs</a>. However, it may take many years before practical and widespread quantum computing becomes reality.</p><p>Now a new study finds that <a href="https://spectrum.ieee.org/trustworthy-ai" target="_self">machine learning</a>, which now powers computer vision, speech recognition, and more, can also prove significantly better than regular computers at the kinds of tasks at which quantum computers excel. These findings suggest that machine learning may help tackle key quantum problems in the era before quantum computers finally arrive.</p><p>Quantum computers can theoretically achieve a “<a href="https://spectrum.ieee.org/quantum-computing" target="_self">quantum advantage</a>” where they can find the answers to problems no classical computers could ever solve. The more components known as qubits that a quantum computer has, the greater its computational power can grow, in an exponential fashion.</p><p class="pull-quote">“If quantum computers were mature right now, it would definitely be better to use quantum computers.”<br/>—Robert Hsin-Yuan Huang, Caltech</p><p>A major application for quantum computers may be modeling complex molecules and other systems where strange quantum effects play key roles. These odd phenomena include <a href="https://spectrum.ieee.org/longlasting-qubits" target="_self">superposition</a>, where an object may exist in two or more places or states at the same time, and <a href="https://spectrum.ieee.org/what-is-quantum-entanglement" target="_self">entanglement</a>, wherein multiple bodies can influence each other instantaneously regardless of how far apart they are.</p><p>Classical computers often struggle to model quantum systems, especially ones involving many bodies. In contrast, quantum computers are themselves quantum systems, and so can theoretically solve these kinds of quantum many-body problems far more quickly.</p><p>However, quantum computers are currently <a href="https://spectrum.ieee.org/quantum-computing" target="_self">noisy intermediate-scale quantum</a> (NISQ) platforms, meaning their qubits number up to a few hundred at most. To prove useful for practical applications, future quantum computers will likely need thousands of qubits to help compensate for <a href="https://spectrum.ieee.org/fault-tolerant-quantum-computing" target="_self">errors</a>, a goal that may take many years.</p><p>In the new study, researchers investigated machine-learning algorithms, ones that improve automatically through experience, running on classical computers. They found these classical machine-learning algorithms may solve challenging quantum problems better than any other algorithm on classical computers. They detailed <u><a href="http://dx.doi.org/10.1126/science.abk3333" rel="noopener noreferrer" target="_blank">their findings</a></u> online 22 September in the journal <em>Science</em>.</p><p>One set of applications the scientists analyzed involved finding the ground state of a molecule, the one in which it has the least amount of energy. Superposition and entanglement can make predicting a molecule’s ground state very difficult, especially when it possesses many atoms, says study lead author <a href="https://spectrum.ieee.org/quantum-computing" target="_self">Robert Hsin-Yuan Huan</a><a href="https://spectrum.ieee.org/quantum-computing" target="_self">g</a>, a quantum-information theorist at the California Institute of Technology, in Pasadena, Calif.</p><p>The researchers investigated what happened when classical machine-learning algorithms were given data on the ground states of molecules—for example, information supplied by experiments that collected quantum data from molecules. They found that such classical machine-learning algorithms could efficiently and accurately go on to predict the ground states of other molecules significantly better than other kinds of classical algorithms.</p><p>This advantage comes from how “nature operates quantum mechanically,” so data gathered from quantum experiments “contains fragments of the quantum computational power in nature,” Huang says. This means classical machine-learning algorithms that learn from this data “can predict more accurately and more efficiently than any non-machine-learning algorithm,” he adds.</p><p>All in all, when it comes to predicting ground states, a classical machine-learning algorithm “can predict more accurately than classical non-machine-learning algorithms with the same amount of computational time,” Huang says. “If we instead aim at achieving the same prediction accuracy, then classical machine learning can run <a href="https://en.wikipedia.org/wiki/Time_complexity#Superpolynomial_time" rel="noopener noreferrer" target="_blank">super-polynomially</a> faster than classical non-machine-learning algorithms.”</p><p>Another set of applications the researchers explored was classifying a wide range of quantum phases of matter. Familiar phases of matter include the many crystal structures that ice may adopt, whereas more exotic quantum phases of matter include the kinds seen in <a href="https://spectrum.ieee.org/photonic-topological-insulators-2658324693" target="_self">topological insulators</a>, where electricity or light can flow without scattering or losses.</p><p>The scientists found that when classical machine-learning algorithms were trained on classical data on quantum phases, they could efficiently learn how to accurately classify quantum phases they did not encounter during training.</p><p>“It is exciting to have formal proof that classical machine-learning algorithms trained with data from physical experiments could outperform any classical non-machine-learning algorithms in an important problem in quantum physics,” Huang says. “It really shows the power of classical machine learning in addressing challenging problems in physics, chemistry, and material sciences.”</p><p>Future research can explore what other important quantum problems for which classical machine learning could do well, Huang says. Further work can also explore how to optimizing the way in which classical machine-learning algorithms can solve quantum problems, in terms of how much training data and computational time they require, he notes.</p><p>Ultimately, one day <a href="https://spectrum.ieee.org/quantum-computing" target="_self">quantum computers will outperform even classical machine learning when it comes to simulating chemistry and physics experiments</a>. “If quantum computers were mature right now, it would definitely be better to use quantum computers,” Huang says.</p><p>Still, until quantum computers arrive, “classical machine-learning models trained on experimental data can solve practical problems in chemistry and materials science that would be too hard to solve using classical processing alone,” Huang says.</p>]]></description><pubDate>Wed, 28 Sep 2022 13:57:08 +0000</pubDate><guid>https://spectrum.ieee.org/machine-learning-quantum</guid><category>Quantum computing</category><category>Chemistry</category><category>Physics</category><category>Quantum advantage</category><category>Superposition</category><category>Entanglement</category><category>Nisq</category><category>Topological insulators</category><category>Machine learning</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/vector-art-of-a-head-with-circuits-examining-a-quantum-symbol.jpg?id=31830945&amp;width=980"></media:content></item><item><title>Meet the Open Source PC That Fits in Your Pocket</title><link>https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-purple-laptop-on-a-desk.jpg?id=31812808&width=1245&height=700&coordinates=0%2C210%2C0%2C210"/><br/><br/><p>Open source computing is coming to your pocket.</p><p>MNT Research, <a href="https://mntre.com/" rel="noopener noreferrer" target="_blank">creator of the Reform open source laptop</a> and <a href="https://shop.mntmn.com/products/zz9000-for-amiga-preorder" rel="noopener noreferrer" target="_blank">ZZ9000 add-in board for Amiga computers</a>, is going small for its next project. The MNT Pocket Reform has a 7-inch screen with a clamshell design that, when closed, will be less than 5 centimeters thick. If its perky purple facade looks a bit retro, that’s no surprise; the Pocket’s inspirations read like a ‘greatest hits’ list of pocketable computers. </p><p>“We had a mood board with several classic handheld computers: Nokia N900, Atari Portfolio, Cambridge Z88, Blackberry, Game Boy Advance SP, Alan Kay’s Dynabook,” says <a href="https://twitter.com/minut_e?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" target="_blank">Lukas F. Hartmann, CEO and founder of MNT Research</a>. “I have a Psion 5mx, which was kind of a benchmark for the keyboard.”</p><hr/><h2>Barely pocketable, very modular </h2><p>MNT initially sketched out a design with an even smaller, 5-inch display. This better delivered on the Pocket’s titular pocketability, but the resulting device lacked space for a comfortable keyboard. Ultimately, the designers decided on a larger display with more internal space. It may require cargo jeans to carry but should prove more usable day-to-day. </p><p>There are other ways MNT could have shaved down the device. The tiny Nokia N900, and many Blackberry devices, use a slide-hinge design. Some modern computers, like Lenovo’s Yoga laptop line, have a 360-degree hinge. “We like to keep it simple,” says Hartmann. “We are a very small team--you could say a boutique--and we have to pick our battles. Complicated mechanical and multimodal designs can snowball into all kinds of problems, while most people don’t actually need them or ask for them.”</p><p>The Pocket also differs in its approach to modularity. It’s not built around a strict base specification and instead offers support for a range of modules from entirely different manufacturers. The base module is an NXP i.MX 8M Plus with four ARM Cortex A53 cores, but the list of alternatives includes the <a href="https://spectrum.ieee.org/eben-upton-the-raspberry-pi-pioneer" target="_self">Raspberry Pi</a> Compute Module 4 and Pine SOQuartz RK3566. The Pocket will also support reusing  modules compatible with the larger MNT Reform laptop.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Rendering of the blue MNT Reform Pocket mainboard." class="rm-shortcode" data-rm-shortcode-id="f292350c8a3495e49e5825b2ef82b523" data-rm-shortcode-name="rebelmouse-image" id="98e0d" loading="lazy" src="https://spectrum.ieee.org/media-library/rendering-of-the-blue-mnt-reform-pocket-mainboard.jpg?id=31812831&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">MNT’s open source approach extends to hardware documentation.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MNT Research</small></p><p>An owner’s choice will affect performance: The default NXP module balances portability with speed, while the Raspberry Pi CM4 (which uses quicker Cortex A72 cores) will offer better overall performance and will support Vulkan GPU drivers. There’s also a selection of internal expansion options including NVMe and optional 4G/5G/LTE mobile data. </p><p>“It’s really about your individual needs—which features do you need most and which compromises you’re willing to make. You can mix and match and find something that works best for you,” says Hartmann.</p><h2>MNT is serious about open source hardware</h2><p>“Open source” has morphed into an elastic buzzword, which makes it easy to miss what MNT offers. MNT’s open source promise is not limited to an open source operating system or select internal components The Pocket Reform, like MNT’s full-size Reform laptop, will provide mainboard schematics, 3D models for physical components, and open source drivers, among other things. <a href="https://source.mnt.re/reform/reform" rel="noopener noreferrer" target="_blank">MNT’s extensive GitLab repository for the full-size Reform laptop</a> serves a taste of what’s coming.</p><p>This approach is not entirely unique, but MNT’s extreme commitment to open source stands out. Competitors like Framework and System76 offer laptops with open source hardware and software, but still rely on some mainstream, proprietary components (such as Intel <em>x</em>86 processors or Nvidia graphics). Portions of the design remain opaque.</p><p class="pull-quote">“We are offering an alternative to the treadmill of vendor-controlled and locked-in devices.”</p><p>In this sense, the Pocket is perhaps better understood as a pocketable open source hardware platform than a purpose-built computer. The Pocket will ship in a configuration that can be used without modification, but there’s obviously better devices for those who want to use an off-the-shelf computer. </p><p>“Consumer-ready open hardware products that are fully documented and allow you to modify, repair, or even clone them are incredibly rare,” says Hartmann. “We are offering an alternative to the treadmill of vendor-controlled and locked-in devices.” Offering the Pocket with an <em>x</em>86 processor from Intel or AMD, as does Framework and System76, would surely expand mainstream appeal­—but also runs against the Pocket’s open source mission. Modifying, tweaking, and hacking the Pocket is more than half the point.</p><p>The MNT Pocket Reform is still in development, with <a href="https://www.crowdsupply.com/mnt/pocket-reform" rel="noopener noreferrer" target="_blank">an initial launch planned on crowdfunding website Crowd Supply</a>. MNT has yet to commit to a date, but a representative says the company will have more to share “later this year.” Until then, you can keep up to date with the MNT Pocket Reform’s development on the company’s official blog or, if you want to see the nitty-gritty details, <a href="https://source.mnt.re/reform/pocket-reform" rel="noopener noreferrer" target="_blank">the Pocket Reform’s GitLab page</a>.</p>]]></description><pubDate>Tue, 27 Sep 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket</guid><category>Mnt</category><category>Mnt reform</category><category>Pocketpc</category><category>Open source</category><dc:creator>Matthew S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-purple-laptop-on-a-desk.jpg?id=31812808&amp;width=980"></media:content></item><item><title>Posits, a New Kind of Number, Improves the Math of AI</title><link>https://spectrum.ieee.org/floating-point-numbers-posits-processor</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/squares-with-0s-and-1s-form-a-colorful-brain-shape-and-blue-background.jpg?id=31797564&width=1245&height=700&coordinates=0%2C378%2C0%2C379"/><br/><br/><p>
	Training the large neural networks behind many modern AI tools requires real computational might: For example, 
	<a href="https://spectrum.ieee.org/large-language-models-meta-openai" target="_self">OpenAI’s most advanced language model, GPT-3</a>, required an astounding million billion billions of operations to <a href="https://spectrum.ieee.org/mlperf-rankings-2022" target="_self">train</a>, and cost about US $5 million in compute time. Engineers think they have figured out a way to ease the burden by using a different way of representing numbers.
</p><p>
	Back in 2017, 
	<a href="http://www.johngustafson.net/" rel="noopener noreferrer" target="_blank">John Gustafson</a>, then jointly appointed at <a href="https://www.a-star.edu.sg/acrc" rel="noopener noreferrer" target="_blank">A*STAR Computational Resources Centre</a> and the National University of Singapore, and <a href="https://scholar.google.com/citations?user=BE2yVIYAAAAJ&hl=en" rel="noopener noreferrer" target="_blank">Isaac Yonemoto</a>, then at Interplanetary Robot and Electric Brain Co., developed a <a href="http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf" rel="noopener noreferrer" target="_blank">new way of representing numbers</a>. These numbers, called posits, were proposed as an improvement over the standard floating-point arithmetic processors used today.
</p><p>
	Now, a team of researchers at the 
	<a href="https://www.ucm.es/english" target="_blank">Complutense University of Madrid</a> have <a href="https://ieeexplore.ieee.org/document/9817027/references#references" rel="noopener noreferrer" target="_blank">developed the first processor core</a> implementing the posit standard in hardware and showed that, bit-for-bit, the accuracy of a basic computational task increased by up to four orders of magnitude, compared to computing using standard floating-point numbers. They presented their results at last week’s <a href="https://arith2022.arithsymposium.org/program.html" rel="noopener noreferrer" target="_blank">IEEE Symposium on Computer Arithmetic</a>.
</p><p>
	“Nowadays it seems that Moore’s law is starting to fade,” says David Mallasén Quintana, a graduate researcher in the 
	<a href="https://artecs.dacya.ucm.es/" rel="noopener noreferrer" target="_blank">ArTeCS group</a> at Complutense. “So, we need to find some other ways of getting more performance out of the same machines. One of the ways of doing that is changing how we encode the real numbers, how we represent them.”
</p><p>
	The Complutense team isn’t alone in pushing the envelope with number representation. Just last week, Nvidia, Arm, and Intel agreed on a 
	<a href="https://arxiv.org/abs/2209.05433" rel="noopener noreferrer" target="_blank">specification</a> for using 8-bit floating-point numbers instead of the usual 32-bit or 16-bit for machine-learning applications. Using the smaller, less-precise format improves efficiency and memory usage, at the cost of computational accuracy.
</p><p>
	Real numbers can’t be perfectly represented in hardware simply because there are infinitely many of them. To fit into a designated number of bits, many real numbers have to be rounded. The advantage of posits comes from the way the numbers they represent exactly are distributed along the number line. In the middle of the number line, around 1 and -1, there are more posit representations than floating point. And at the wings, going out to large negative and positive numbers, posit accuracy falls off more gracefully than floating point.
</p><p>
	“It’s a better match for the natural distribution of numbers in a calculation,” says Gustafson. “It’s the right dynamic range, and it’s the right accuracy where you need more accuracy. There’s an awful lot of bit patterns in floating-point arithmetic no one ever uses. And that’s waste.”
</p><p>
	Posits accomplish this improved accuracy around 1 and -1 thanks to an extra component in their representation. Floats are made up of three parts: a sign bit (0 for positive, 1 for negative), several “mantissa” (fraction) bits denoting what comes after the binary version of a decimal point, and the remaining bits defining the exponent (2
	<sup>exp</sup>).
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Floating point number representation: sign, exponent, and fraction bits. Posit number representation: sign, regime, exponent, and fraction bits. Accuracy plot for floating point and posit: The accuracy of floats increases sharply and then remains flat over their dynamic range. Posit accuracy grows gradually and peaks above floating point when exponent is close to zero, then decays gradually for positives. " class="rm-shortcode" data-rm-shortcode-id="ab090a73ebcb2f30c2185015704068eb" data-rm-shortcode-name="rebelmouse-image" id="94999" loading="lazy" src="https://spectrum.ieee.org/media-library/floating-point-number-representation-sign-exponent-and-fraction-bits-posit-number-representation-sign-regime-exponent-an.jpg?id=31797566&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This graph shows components of floating-point-number representation [top] and posit representation [middle]. The accuracy comparison shows posits’ advantage when the exponent is close to 0. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Complutense University of Madrid/IEEE </small>
</p><p>
	Posits keep all the components of a float but add an extra “regime” section, an exponent of an exponent. The beauty of the regime is that it can vary in bit length. For small numbers, it can take as few as two bits, leaving more precision for the mantissa. This allows for the higher accuracy of posits in their sweet spot around 1 and -1.
	<br/>
</p><p>
	Deep neural networks usually work with normalized parameters called weights, making them the perfect candidate to benefit from posits’ strengths. Much of neural-net computation is comprised of multiply-accumulate operations. Every time such a computation is performed, each sum has to be truncated anew, leading to accuracy loss. With posits, a special register called a quire can efficiently do the accumulation step to reduce the accuracy loss. But today’s hardware implements floats, and so far, computational gains from using posits in software have been largely overshadowed by losses from converting between the formats.
</p><p>
	With their new hardware implementation, which was synthesized in a field-programmable gate array (FPGA), the Complutense team was able to compare computations done using 32-bit floats and 32-bit posits side by side. They assessed their accuracy by comparing them to results using the much more accurate but computationally costly 64-bit floating-point format. Posits showed an astounding four-order-of-magnitude improvement in the accuracy of matrix multiplication, a series of multiply-accumulates inherent in neural network training. They also found that the improved accuracy didn’t come at the cost of computation time, only a somewhat increased chip area and power consumption.
</p><p>
	Although the numerical accuracy gains are undeniable, how exactly this would impact the training of large AIs like GPT-3 remains to be seen.
</p><p>
	“It’s possible that posits will speed up training because you’re not losing as much information on the way,” says Mallasén “But these are things that that we don’t know. Some people have tried it out in software, but we also want to try that in hardware now that we have it.”
</p><p>
	Other teams are working on their own hardware implementations to advance posit usage. “It’s doing exactly what I hoped it would do; it’s getting adopted like crazy,” Gustafson says. “The posit number format caught fire, and there are dozens of groups, both companies and universities, that are using it.”
</p>]]></description><pubDate>Sun, 25 Sep 2022 13:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/floating-point-numbers-posits-processor</guid><category>Artificial intelligence</category><category>Gpt-3</category><category>Computer architecture</category><category>Mathematics</category><category>Floating point</category><category>Posit</category><category>Machine learning</category><dc:creator>Dina Genkina</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/squares-with-0s-and-1s-form-a-colorful-brain-shape-and-blue-background.jpg?id=31797564&amp;width=980"></media:content></item><item><title>Full-Wave EM Simulations: Electrically Large Antenna Placement and RCS Scenarios</title><link>https://wipl-d.com/electrically-large-scenarios/</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=26851496&width=980"/><br/><br/><p>Handling various complex simulation scenarios with a single simulation method is a rather challenging task for any software suite. We will show you how our software, based on Method-of-Moments, can analyze several scenarios including complicated and electrically large models (for instance, antenna placement and RCS) using desktop workstations.<u></u><u></u></p><p><a href="https://wipl-d.com/electrically-large-scenarios/" rel="noopener noreferrer" target="_blank">Download this free whitepaper now!</a></p>]]></description><pubDate>Fri, 23 Sep 2022 15:52:02 +0000</pubDate><guid>https://wipl-d.com/electrically-large-scenarios/</guid><category>Antennas</category><category>Electromagnetic waves</category><category>Simulation</category><category>Type:whitepaper</category><category>Wipl-d</category><dc:creator>WIPL-D</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851496/origin.png"></media:content></item><item><title>Disentangling the Facts From the Hype of Quantum Computing</title><link>https://spectrum.ieee.org/ieee-quantum-week</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-men-hold-a-large-gold-cylinder-that-is-suspended-by-several-narrow-gold-pipes.jpg?id=31720293&width=1245&height=700&coordinates=0%2C0%2C6%2C0"/><br/><br/><div></div><p><em>This is a guest post in recognition of <a href="https://qce.quantum.ieee.org/2022/" target="_blank">IEEE Quantum Week 2022</a>. The views expressed here are solely those of the author and do not represent positions of </em>IEEE Spectrum<em> or the IEEE.</em></p><p>Few fields invite as much unbridled hype as quantum computing. Most people’s understanding of quantum physics extends to the fact that it is unpredictable, powerful, and almost existentially strange. A few years ago, I provided <em>IEEE Spectrum </em>an update on the <a href="https://spectrum.ieee.org/an-optimists-view-of-the-4-challenges-to-quantum-computing" target="_self">state of quantum computing</a> and looked at both the positive and negative claims across the industry. And just as back in 2019, I remain enthusiastically optimistic today. Even though the hype is real and has outpaced the actual results, much has been accomplished over the past few years.<br/></p><p>First, let’s address the hype. </p><p>Over the past five years, there has been undeniable hype around quantum computing—hype around approaches, timelines, applications, and more. As far back as 2017, vendors were claiming the commercialization of the technology was just a couple of years away. There was even what I’d call <a href="https://spectrum.ieee.org/the-case-against-quantum-computing" target="_self">antihype</a>, with some questioning if quantum computers would materialize at all. I hope they end up being wrong.</p><p>More recently, companies have shifted their timelines from a few years to a decade, but they continue to release road maps showing <a href="https://www.theverge.com/2021/5/19/22443453/google-quantum-computer-2029-decade-commercial-useful-qubits-quantum-transistor" rel="noopener noreferrer" target="_blank">commercially viable systems as early as 2029</a>. And these hype-fueled expectations are becoming institutionalized: The <a href="https://www.dhs.gov/" target="_blank">Department of Homeland Security</a> even <a href="https://www.nextgov.com/emerging-tech/2021/10/dhs-issues-roadmap-help-organizations-prepare-quantum-computing-threat/185844/" target="_blank">released a road map</a> to protect against the threats of quantum computing, in an effort to help institutions transition to new security systems. This creates an “adopt or you’ll fall behind” mentality for both quantum-computing applications and postquantum cryptography security.</p><p>Market research firm <a href="https://www.gartner.com/en" target="_blank">Gartner</a> (of the <a href="https://www.gartner.com/en/research/methodologies/gartner-hype-cycle" target="_blank">“Hype Cycle”</a> fame) believes quantum computing may have already reached peak hype, or phase two of its five-phase growth model. This means the industry is about to enter a phase called “the trough of disillusionment." According to <a href="https://www.mckinsey.com/" target="_blank">McKinsey & Company</a>, “fault tolerant quantum computing <a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/quantum-computing-use-cases-are-getting-real-what-you-need-to-know" target="_blank">is expected between 2025 and 2030</a> based on announced hardware roadmaps for gate-based quantum computing players.” I believe this is not entirely realistic, as we still have a long journey to achieve quantum practicality—the point at which quantum computers can do something unique to change our lives. </p><p>In my opinion, quantum practicality is likely still 10 to 15 years away. However, progress toward that goal is not just steady; it’s accelerating. That’s the same thing we saw with Moore’s Law and semiconductor evolution: The more we discover, the faster we go. Semiconductor technology has taken decades to progress to its current state, accelerating at each turn. We expect similar advancement with quantum computing.</p><p>In fact, we are discovering that what we have learned while engineering transistors at <a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank">Intel</a> is also helping to speed our quantum-computing development work today. For example, when developing <a href="https://spectrum.ieee.org/intels-quantum-computing-plans-hot-qubits-cold-control-chips-and-rapid-testing" target="_self">silicon spin qubits</a>, we’re able to leverage existing transistor-manufacturing infrastructure to ensure quality and to speed up fabrication. We’ve started the mass production of qubits on a 300-millimeter silicon wafer in a high-volume fab facility, which allows us to fit an array of more than 10,000 quantum dots on a single wafer. We’re also leveraging our experience with semiconductors to create a cryogenic quantum control chip, called <a href="https://www.intel.com/content/www/us/en/research/quantum-computing.html" rel="noopener noreferrer" target="_blank">Horse Ridge</a>, which is helping to solve the interconnect challenges associated with quantum computing by eliminating much of the cabling that today crowds the dilution refrigerator. And our experience with testing semiconductors has led to the development of the <a href="https://www.intel.com/content/www/us/en/newsroom/news/intels-cryoprober-quantum-research.html" rel="noopener noreferrer" target="_blank">cryoprober</a>, which enables our team to get testing results from quantum devices in hours instead of the days or weeks it used to take.</p><p>Others are likely benefiting from their own prior research and experience, as well. For example, <a href="https://www.quantinuum.com/" target="_blank">Quantinuum’s</a> recent research <a href="https://www.quantinuum.com/pressrelease/logical-qubits-start-outperforming-physical-qubits" target="_blank">showed the entanglement of logical qubits</a> in a fault-tolerant circuit using real-time quantum error correction. While still primitive, it’s an example of the type of progress needed in this critical field. For its part, Google has a new open-source library called <a href="https://quantumai.google/cirq" rel="noopener noreferrer" target="_blank">Cirq</a> for programming quantum computers. Along with similar libraries from IBM, Intel, and others, Cirq is helping drive development of improved quantum algorithms. And, as a final example, IBM’s 127-qubit processor, called <a href="https://research.ibm.com/blog/127-qubit-quantum-processor-eagle" rel="noopener noreferrer" target="_blank">Quantum Eagle</a>, shows steady progress toward upping the qubit count.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-right" data-rm-resized-container="25%" style="float: right;">
<img alt="A man holds a bumpy gold rectangle in front of a spindly machine." class="rm-shortcode rm-resized-image" data-rm-shortcode-id="2e57de474e52ab8607c49a0a860eeadf" data-rm-shortcode-name="rebelmouse-image" id="50104" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-holds-a-bumpy-gold-rectangle-in-front-of-a-spindly-machine.jpg?id=31720324&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The author shows Intel quantum-computing prototypes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Intel</small></p><p>There are also some key challenges that remain.</p><ul><li>First, we still need better devices and high-quality qubits. While the very best one- and two-qubit gates meet the needed threshold for fault tolerance, the community has yet to accomplish that on a much larger system.</li><li>Second, we’ve yet to see anyone propose an interconnect technology for quantum computers that is as elegant as how we wire up microprocessors today. Right now, each qubit requires multiple control wires. This approach is untenable as we strive to create a large-scale quantum computer.</li><li>Third, we need fast qubit control and feedback loops. Horse Ridge is a precursor for this, because we would expect latency to improve by having the control chip in the fridge and therefore closer to the qubit chip.</li><li>And finally, error correction. While there have been some recent indications of progress to correction and mitigation, no one has yet run an error-correction algorithm on a large group of qubits.</li></ul><p>With new research regularly showing novel approaches and advances, these are challenges we will overcome. For example, many in the industry are looking at how to integrate qubits and the controller on the same die to create quantum system-on-chips (SoCs).</p><p>But we’re still quite a way off from having a fault tolerant quantum computer. Over the next 10 years, Intel expects to be competitive (or pull ahead) of others in terms of qubit count and performance, but as I stated before, a system large enough to deliver compelling value won’t be realized for 10 to 15 years, by anyone. The industry needs to continue its evolution of qubit counts and quality improvement. After that, the next milestone should be the production of thousands of quality qubits (still several years away), and then scaling that to millions.</p><p>Let’s remember that it took Google 53 qubits to create an application that could accomplish a supercomputer function. If we want to explore new applications that go beyond today’s supercomputers, we’ll need to see system sizes that are orders of magnitude larger.</p><p>Quantum computing has come a long way in the past five years, but we still have a long way to go, and investors will need to fund it for the long term. Significant developments are happening in the lab, and they show immense promise for what could be possible in the future. For now, it’s important that we don’t get caught up in the hype but focus on real outcomes.</p><p><br/></p><p><em><strong>Correction 21 Sept. 2022: </strong>A previous version of this post stated incorrectly that the release of an announced 5,000-qubit quantum computer in 2020 did not happen. <a href="https://en.wikipedia.org/wiki/D-Wave_Systems#Pegasus" target="_blank">It did</a>. </em>Spectrum<em> regrets the error. </em><br/></p>]]></description><pubDate>Mon, 19 Sep 2022 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-quantum-week</guid><category>Quantum computers</category><category>Quantum processors</category><category>Quantum programming</category><category>Cryogenic ic</category><category>Qubits</category><category>Intel</category><category>Quantum computing</category><dc:creator>James S. Clarke</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-men-hold-a-large-gold-cylinder-that-is-suspended-by-several-narrow-gold-pipes.jpg?id=31720293&amp;width=980"></media:content></item><item><title>Coding Made AI—Now, How Will AI Unmake Coding?</title><link>https://spectrum.ieee.org/ai-code-generation-language-models</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-people-sit-at-monitors-coding.jpg?id=31720455&width=1245&height=700&coordinates=0%2C406%2C0%2C406"/><br/><br/><p>Are coders doomed? That question has been bouncing around computer-programming communities ever since OpenAI’s large language model, <a href="https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html" rel="noopener noreferrer" target="_blank">GPT-3</a>, surprised everyone with its ability to <a href="https://www.leadingedgeonly.com/article/gpt-3-innovations-in-web-design" rel="noopener noreferrer" target="_blank">create html websites</a> from simple written instructions.</p><p>In the months since, rapid-fire advances have led to <a href="https://machinelearningtokyo.com/2020/07/26/10-cool-gpt-3-demos/" target="_blank">systems that can write complete, albeit simple, computer programs</a> from natural-language descriptions—spoken or written human language—and automated coding assistants that speed the work of computer programmers. How far will artificial intelligence go in replacing or augmenting the work of human coders?</p><p>According to the experts <em>IEEE Spectrum </em>consulted, the bad news is coding as we know it may indeed be doomed. But the good news is computer programming and software development appears poised to remain a very human endeavor for the foreseeable future. In the meantime, AI-powered automated code generation will increasingly speed software development by allowing more code to be written in a shorter time.  <br/></p><p class="pull-quote">Programmers will not always need to learn a programming language. That will open software development to a much broader population.  </p><p>“I don’t believe AI is anywhere near replacing human developers,” said <a href="https://www.linkedin.com/in/vasi-philomin/" target="_blank">Vasi Philomin</a>, Amazon’s vice president for AI services, adding that AI tools will free coders from routine tasks, but the creative work of computer programming will remain.</p><p>If someone wants to become a developer, say, 10 years down the line, they won’t necessarily need to learn a programming language. Instead, they will need to understand the semantics, concepts, and logical sequences of building a computer program. That will open software development to a much broader population.</p><p>When the programming of electronic computers began in the 1940s, programmers wrote in numerical <a href="https://en.wikipedia.org/wiki/Machine_code" rel="noopener noreferrer" target="_blank">machine code</a>. It wasn’t until the mid-1950s that <a href="https://president.yale.edu/biography-grace-murray-hopper" rel="noopener noreferrer" target="_blank">Grace Hopper</a> and her team at the computer company <a href="https://www.si.edu/spotlight/bookkeeping-machines/remington-rand" rel="noopener noreferrer" target="_blank">Remington Rand</a> developed <a href="https://en.wikipedia.org/wiki/FLOW-MATIC" rel="noopener noreferrer" target="_blank">FLOW-MATIC</a>, which allowed programmers to use a limited English vocabulary to write programs.</p><p>Since then, programming has climbed a ladder of increasingly efficient languages that allow programmers to be more productive.</p><p>AI-written code is the cutting edge of a broader movement to allow people to write software without having to code at all. Already, with platforms like <a href="https://www.akkio.com/" target="_blank">Akkio</a>, people can build machine-learning models with simple drag, drop, and button-click features. Users of <a href="https://powerplatform.microsoft.com/en-us/" target="_blank">Microsoft’s Power Platform</a>, which includes a family of low-code products, can generate simple applications by just describing them.</p><p>In June, Amazon released <a href="https://aws.amazon.com/codewhisperer/" target="_blank">CodeWhisperer</a>, a coding assistant for programmers, like GitHub’s Copilot, which was first released in limited preview in June 2021. Both tools are based on <a href="https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai" target="_blank">large language models</a> (LLMs) that have been trained on massive code repositories. Both offer autocomplete suggestions as a programmer writes code or suggest executable instructions from simple natural-language phrases.</p><p class="pull-quote">“There needs to be some incremental refinement, some conversation between the human and the machine.”<br/>—Peter Schrammel, Diffblue</p><p>A <a href="https://venturebeat.com/ai/study-provides-insights-on-github-copilots-impact-on-developer-productivity/" target="_blank">GitHub survey</a> of 2,000 developers found that Copilot cuts in half the time it takes for certain coding tasks and raised overall developer satisfaction in their work.<br/></p><p>But to move beyond autocompletion, the problem is teaching the intent to the computer. Software requirements are usually vague, while natural language is notoriously imprecise.</p><p>“To resolve all these ambiguities in English written specification, there needs to be some incremental refinement, some conversation between the human and the machine,” said <a href="https://www.eye-on.ai/podcast-106" target="_blank">Peter Schrammel</a>, cofounder of <a href="https://www.diffblue.com/" rel="noopener noreferrer" target="_blank">Diffblue</a>, which automates the writing of unit tests for Java.</p><p>To address these problems, researchers at Microsoft have recently proposed adding a feedback mechanism to LLM-based code generation so that the computer asks the programmer for clarification of any ambiguities before generating code.<br/></p><p>The interactive system, called <a href="https://syncedreview.com/2022/08/18/microsoft-penn-u-uc-san-diegos-ticoder-framework-generates-code-with-90-4-consistency-to-user-intent/" rel="noopener noreferrer" target="_blank">TiCoder</a>, refines and formalizes user intent by generating what is called a “<a href="https://arxiv.org/abs/2208.05950" target="_blank">test-driven user-intent formalization</a>”—which attempts to use iterative feedback to divine the programmer’s algorithmic intent and then generate code that is consistent with the expressed intentions.</p><p>According to their paper, TiCoder improves the accuracy of automatically generated code by up to 85 percent from 48 percent, when evaluated on the <a href="https://arxiv.org/abs/2108.07732" target="_blank">Mostly Basic Programming Problems</a> (MBPP) benchmark. MBPP, meant to evaluate machine-generated code, consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers. <br/></p><p>A unit of code, which can be hundreds of lines long, is the smallest part of a program that can be maintained and executed independently. A suite of unit tests, typically consisting of dozens of unit tests, each of them between 10 and 20 lines of code, checks that the unit executes as intended, so that when you stack the units together, the program works as intended.  <br/></p><p>Unit tests are useful for debugging individual functions and for detecting errors when code is manually changed. But a unit test can also be used as the specification for the unit of code and can be used to guide programmers to write clean, bug-free code. While not many programmers pursue true test-driven development, in which the unit tests are written first, unit test and units are generally written together.</p><p class="pull-quote">Hand-coding software programs will increasingly be like hand-knitting sweaters.</p><p>According to a <a href="https://www.diffblue.com/Education/research_papers/2019-diffblue-developer-survey/" target="_blank">survey by Diffblue</a>, developers spend roughly 35 percent of their time writing quality-control tests (as opposed to writing code destined for production use), so there are significant productivity gains to be made just by automating a part of this.  <br/></p><p>Meanwhile, Github’s Copilot, Amazon’s CodeWhisperer, and AI programming assistant packages can be used as interactive auto-completion tools for writing unit tests. The programmer is given suggestions and picks the one that they think will work best. Diffblue’s system, called <a href="https://www.diffblue.com/products/" target="_blank">Diffblue Cover</a>, uses reinforcement learning to write unit tests automatically, with no human intervention.</p><p>Earlier this year, Google’s U.K.-based, artificial intelligence lab, <a href="https://www.deepmind.com/" target="_blank">DeepMind</a>, went further in fully automatic code generation with <a href="https://alphacode.deepmind.com/" rel="noopener noreferrer" target="_blank">AlphaCode</a>, a large language model that can write simple computer programs from natural-language instructions.</p><p>AlphaCode uses an encoder-decoder <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank">transformer architecture</a>, first encoding the natural-language description of the problem and then decoding the resulting vector into code for a solution.</p><p>The model was first trained on the <a href="https://github.com/git/git" target="_blank">GitHub code repository</a> until the model was able to produce reasonable-looking code.</p><p>To fine-tune the model, DeepMind used 15,000 pairs of natural-language problem descriptions and successful code solutions from past coding competitions to create a specialized data set of input-output examples.</p><p>Once AlphaCode was trained and tuned, it was tested against problems it hadn’t seen before.</p><p class="pull-quote">“I don’t believe AI is anywhere near replacing human developers. It will remove the mundane, boilerplate stuff that people have to do, and they can focus on higher-value things.”<br/>—Vasi Philomin, Amazon</p><p>The final step was to generate many solutions and then use a filtering algorithm to select the best one. “We created many different program possibilities by essentially sampling the language model almost a million times,” said <a href="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en" rel="noopener noreferrer" target="_blank">Oriol Vinyals</a>, who leads <a href="https://research.google/people/OriolVinyals/" target="_blank">DeepMind’s deep-learning team</a>.</p><p>To optimize the sample-selection process, DeepMind uses a clustering algorithm to divide the solutions into groups. The clustering process tends to group the working solutions together, making it easier to find a small set of candidates that are likely to work as well as those written by human programmers.</p><p>To test the system, DeepMind submitted 10 AlphaCode-written programs to a human coding competition on the popular <a href="https://codeforces.com/" rel="noopener noreferrer" target="_blank">Codeforces</a> platform where its solutions ranked among the top 54 percent.</p><p>“To generate a program, will you just write it in natural language, no coding required, and then the solution comes out at the other end?” <a href="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en" rel="noopener noreferrer" target="_blank">Vinyals</a> asked rhetorically in a recent interview. “I believe so.”</p><p>Vinyals and others caution that it will take time, possibly decades, to reach that goal. “We are still very far away from when a person would be able to tell a computer about the requirements for an arbitrary complex computer program, and have that automatically get coded,” said <a href="https://www.andrewng.org/" target="_blank">Andrew Ng</a>, a founder and CEO of Landing AI who is an AI pioneer and founding lead of <a href="https://research.google/teams/brain/" target="_blank">Google Brain</a>.<br/></p><p>But given the speed at which AI-code generation has advanced in a few short years, it seems inevitable that AI systems will eventually be able to write code from natural-language instructions. Hand-coding software programs will increasingly be like hand-knitting sweaters.<br/></p><p>To give natural-language instructions to a computer, developers will still need to understand some concepts of logic and functions and how to structure things. They will still need to study foundational programming, even if they don’t learn specific programming languages or write in computer code. That will, in turn, enable a wider range of programmers to create more and more varied kinds of software.  <br/></p><p>“I don’t believe AI is anywhere near replacing human developers,” Amazon’s Philomin said. “It will remove the mundane, boilerplate stuff that people have to do, and they can focus on higher-value things.”<br/></p><p>Diffblue’s Schrammel agrees that AI-automated code generation will allow software developers to focus on more difficult and creative tasks. But, he adds, there will at least need to be one interaction with a human to confirm what the machine has understood is what the human intended. </p><p>“Software developers will not lose their jobs because an automation tool replaces them,” he said. “There always will be more software that needs to be written.”</p><p><strong><em>Update 6 Oct. 2022: </em></strong><em>A previous version of this story stated that coding jobs of the future will be increasingly “AI-dependent.” Acting on expert feedback, we changed the statement to “AI-assisted” instead. <br/></em></p><p><em>This article appears in the November 2022 print issue as “AI Is Rewriting the Job of Coding.”<br/></em></p>]]></description><pubDate>Mon, 19 Sep 2022 12:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-code-generation-language-models</guid><category>Programming</category><category>Programming languages</category><category>Artificial intelligence</category><category>Gpt-3</category><category>Large language models</category><dc:creator>Craig S. Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-people-sit-at-monitors-coding.jpg?id=31720455&amp;width=980"></media:content></item><item><title>Domain-Wall Discovery Points Toward Self-Healing Circuits</title><link>https://spectrum.ieee.org/domain-wall</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conical-ferroelectric-domain-walls-in-lithium-niobate.jpg?id=31639003&width=1245&height=700&coordinates=0%2C93%2C0%2C94"/><br/><br/><p>Atomically thin materials such as <a data-linked-post="2650278756" href="https://spectrum.ieee.org/is-graphene-by-any-other-name-still-graphene" target="_blank">graphene</a> have drawn attention for how electrons can race in them at exceptionally quick speeds, leading to visions of advanced new electronics. Now scientists find that similar behavior can exist within two-dimensional sheets, known as domain walls, that are embedded within unusual crystalline materials. Moreover, unlike other atomically thin sheets, domain walls can easily be created, moved, and destroyed, which may lead the way for novel circuits that can instantly transform or be repaired on command.</p><p>In the new study, researchers investigated crystalline lithium niobate <a href="https://spectrum.ieee.org/antiferroelectric" target="_self">ferroelectric</a> film just 500 nanometers thick. Electric charges within materials separate into positive and negative poles, and ferroelectrics are materials in which these electric dipoles are generally oriented in the same direction. The electric dipoles in ferroelectrics are clustered in regions known as domains. These are separated by two-dimensional layers known as domain walls.</p><p>The amazing <a href="https://spectrum.ieee.org/memristor-6g-switches" target="_self">electronic properties</a> of two-dimensional materials such as <a href="https://spectrum.ieee.org/atomically-thin-circuits-made-from-graphene-and-molybdenite" target="_self">graphene and molybdenum disulfide</a> have led researchers to hope <a href="https://spectrum.ieee.org/the-most-complex-2d-microchip-yet" target="_self">they may allow Moore’s Law to continue</a> once it becomes impossible to make further progress using silicon. Researchers have also investigated similarly attractive behavior in exceptionally thin electrically conducting <a href="https://spectrum.ieee.org/the-etchasketchtm-of-microscopy-creates-single-electron-transistors" target="_self">heterointerfaces</a> between two different insulating materials, such as lanthanum aluminate and strontium titanate.</p><p>Domain walls are essentially homointerfaces between chemically identical regions of the same material. However, unlike any other 2D electronic material, applied electric or magnetic fields can readily create, move, and annihilate domain walls inside materials.</p><p>This unique quality of domain walls may potentially lead to novel “<a href="https://www.nature.com/articles/s41578-021-00375-z" target="_blank">domain wall electronics</a>” far more flexible and adaptable than current devices that rely on static components. One might imagine entire circuits “created in one instant, for one purpose, only to be wiped clean and rewritten in a different form, for a different purpose, in the next instant,” says study lead author <a href="https://pure.qub.ac.uk/en/persons/conor-mccluskey" target="_blank">Conor McCluskey</a>, a physicist at Queen’s University Belfast, in the United Kingdom. “Malleable domain-wall network architecture that can continually metamorphose could represent a kind of technological genie, granting wishes on demand for radical moment-to-moment changes in electronic function.”</p><p>However, scientists have found it difficult to examine domain walls in detail. The fact that domain walls are both very thin and buried under the surfaces of crystals makes them less easy to analyze “than regular 3D or even 2D materials,” McCluskey says.</p><p>In the new study, McCluskey and his colleagues focused on how the domain walls in the crystals they were investigating are shaped like cones. This geometry let them analyze the behavior of the domain walls using a relatively simple probe design.</p><p class="pull-quote">“Malleable domain-wall network architecture that can continually metamorphose could represent a kind of technological genie, granting wishes on demand for radical moment-to-moment changes in electronic function.” —<em>Conor McCluskey</em></p><p>The scientists found that electric-charge mobility was exceptionally fast at room temperature on average. These speeds may be “the highest room-temperature value in any oxide” and “at least comparable to that seen in graphene,” McCluskey says. The researchers detailed <u><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/adma.202204298" target="_blank">their findings</a></u> in the 11 August issue of the journal <em>Advanced Materials</em>.</p><p>Precise knowledge of such parameters “are needed for envisioning and building devices that work reliably,” McCluskey says. “The dream is that it could allow completely malleable or ephemeral nanocircuitry to be created, destroyed, and reformed from one moment to the next.”</p><p>One promising application for domain walls may be brain-mimicking <a href="https://spectrum.ieee.org/neuromorphic-computing-more-than-ai" target="_self">neuromorphic computing</a>, with neuromorphic devices playing the role of the synapses that link neurons together, McCluskey says.</p><p>“The brain works by forging pathways which have some memory about their history: If a particular synaptic pathway is used more frequently, it becomes stronger, making it easier for this pathway to be used in the future. The brain learns by forging these stronger pathways,” McCluskey says. “Some domain-wall systems can behave in the same way: If you apply a small voltage to walls in our particular system, they tilt and change slightly, increasing their conductivity and giving a higher current. The next pulse will produce a higher current, and so on and so on, as if they have some memory of their past.”</p><p>If domain walls can play the role of artificial synapses, “this could pave the way to a low-heat-production, low-power-consumption brainlike architecture for neuromorphic computing,” he adds.</p><p>However, although reconfigurable electronics based on domain walls are a tantalizing idea, in many ferroelectrics the domain walls conduct only marginally better than the rest of the material, and so they will likely not help support viable devices, McCluskey notes.</p><p>“This isn’t a problem for the system we have investigated, lithium niobate, as it has quite an astonishing ratio between the conductivity of the domain walls and the bulk material,” McCluskey says. However, lithium niobate does currently require large voltages to manipulate domain walls. Scaling these systems down in thickness for use with everyday voltages “is one major hurdle,” he notes. “We are working on it.”</p><p>Future experiments will explore why electric-charge mobility is so fast in domain walls. “Broadly speaking, the carrier mobility relies on two things—the number of times the charge carrier will scatter or bump into something on its journey through the material, and the so-called effective mass with which the carrier moves,” McCluskey says.</p><p>Electrons can deflect off defects in materials, as well as vibrations known as phonons. “It is possible the presence of a domain wall alters the defect or phonon concentrations locally, resulting in fewer scattering centers along the domain wall,” McCluskey says.</p><p>When it comes to the effective mass of a charge carrier such as an electron, “when we consider an electron moving through a crystal lattice, we need to consider it not as a free electron, such as one in vacuum, but as an electron moving through the solid crystalline environment,” McCluskey explains. “The electron feels the effect of the nearby atoms as it progresses, changing its energy as it moves closer or further away from any given atom.” This can essentially make an electron moving in a crystal lighter or heavier than a normal electron. The way in which domain walls disturb crystal lattices may in turn alter the electron’s effective mass, he says.</p><p>“Without further experiments, it’s impossible to say which of these contributions is more responsible for determining the carrier mobility in our system,” McCluskey says. “We hope that our study prompts a shift in focus toward characterizing the transport in domain-wall systems, which may be every bit as exciting as some of the other 2D functional materials systems at the forefront of research today.”</p>]]></description><pubDate>Sat, 10 Sep 2022 01:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/domain-wall</guid><category>Two-dimensional materials</category><category>Lithium niobate</category><category>Ferroelectric</category><category>Domain wall</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conical-ferroelectric-domain-walls-in-lithium-niobate.jpg?id=31639003&amp;width=980"></media:content></item><item><title>Introduction to Peer-to-Peer Streaming and GPU Processing in Data Acquisition Systems</title><link>https://go.teledynelecroy.com/ieee_p2p_webinar</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>Real-time digital signal processing is a vital part of many of today's data acquisition systems, and affordable graphics processing units (GPUs) offer a great complement to onboard field-programmable gate arrays (FPGAs).</p><p><span></span>Join Teledyne SP Devices for an introductory webinar about the basics and benefits of peer-to-peer streaming and GPU post-processing in data acquisition systems.</p><p><a href="https://go.teledynelecroy.com/ieee_p2p_webinar" target="_blank">Register now for this free webinar!</a></p><hr/><p><strong>Selected topics covered in this webinar:</strong></p><ul><li>Comparison between different data transfer methodologies</li><li>Processing differences between FPGA and GPU</li><li>Overview of solutions offered by Teledyne SP Devices</li></ul><p><strong>Who should attend?</strong> </p><p>Engineers that would like to learn more about data processing capabilities in high-performance data acquisition systems.</p><p><strong>What attendees will learn?</strong> </p><p>System-level requirements and achievable performance when using different data transfer methodologies between digitizers and graphics processing units (GPUs). A brief introduction to processing properties in FPGAs versus GPUs. Digitizer capabilities, operating system support, supported GPU models, etc.</p><p><strong>Presenter:</strong> Thomas Elter, Senior Field Applications Engineer</p>]]></description><pubDate>Wed, 07 Sep 2022 13:14:00 +0000</pubDate><guid>https://go.teledynelecroy.com/ieee_p2p_webinar</guid><category>Teledyne</category><category>Digital signal processing</category><category>Fpga</category><category>Gpu</category><category>Graphics processing unit</category><category>Signal processing</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Introduction to High-Level Synthesis (HLS) in Data Acquisition Systems</title><link>https://go.teledynelecroy.com/hls_IEEE_webinar_invitation</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=28133682&width=980"/><br/><br/><p>High-level synthesis (HLS) is used to develop firmware for field-programmable gate arrays (FPGAs) in languages such as C. </p><p>Join Teledyne SP Devices for an introductory webinar on HLS in the context of high-performance digitizers.</p><p><a href="https://go.teledynelecroy.com/hls_IEEE_webinar_invitation" target="_blank">Register now for this free webinar!</a></p><hr/><p><span></span></p><p><strong>Topics covered in this webinar:</strong></p><ul><li>Benefits of onboard FPGA signal processing</li><li>FPGA architecture and development basics</li><li>Programming languages and development tools</li><li>Application areas and signal processing examples</li></ul><p><strong>Who should attend? </strong></p><p><strong></strong>Developers that want to learn more about the possibilities and benefits of onboard digital signal processing in high-performance digitizers.</p><p><strong>What attendees will learn? </strong></p><p><strong></strong>An introduction to FPGA development for high-performance digitizers.</p><p><span></span></p><p><strong>Presenter:</strong> Thomas Elter, Senior Field Applications Engineer</p>]]></description><pubDate>Tue, 06 Sep 2022 16:06:06 +0000</pubDate><guid>https://go.teledynelecroy.com/hls_IEEE_webinar_invitation</guid><category>Fpga</category><category>Data acquisition</category><category>Programming languages</category><category>Teledyne</category><category>Type:webinar</category><dc:creator>Teledyne</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/28133682/origin.png"></media:content></item><item><title>Q&amp;A: Adele Goldberg on the Legacy of Smalltalk</title><link>https://spectrum.ieee.org/qa-adele-goldberg-on-the-legacy-of-smalltalk</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-old-computer-displaying-smalltalk.jpg?id=31291865&width=1245&height=700&coordinates=0%2C244%2C0%2C244"/><br/><br/><p><a href="https://smalltalkzoo.thechm.org/" rel="noopener noreferrer" target="_blank">Smalltalk</a> is one of the most influential programming languages, inspiring the <a href="https://en.wikipedia.org/wiki/Object-oriented_programming" rel="noopener noreferrer" target="_blank">object-oriented programming</a> paradigm; the world of <a href="https://spectrum.ieee.org/the-improbable-origins-of-powerpoint" target="_self">graphical user interfaces</a>; and live coding, which allows you to modify your code while it’s running and debug it on the fly. Some of the <a href="https://spectrum.ieee.org/top-programming-languages-2022" target="_self">top programming languages</a> of today—including Java and <a href="https://www.ruby-lang.org/en/about/" rel="noopener noreferrer" target="_blank">Ruby</a>—as well as languages such as Lisp and <a href="https://dl.acm.org/doi/10.1145/3386332" rel="noopener noreferrer" target="_blank">Objective-C</a> have   elements of Smalltalk in them. While not widely popular today, Smalltalk is still being used to power <a href="https://pharo.org/success/" rel="noopener noreferrer" target="_blank">applications</a> in <a href="https://www.cincomsmalltalk.com/main/successes/" rel="noopener noreferrer" target="_blank">various</a> <a href="https://gemtalksystems.com/about/customers/" rel="noopener noreferrer" target="_blank">industries</a>.</p><p>For Smalltalk’s 50th anniversary this year, we spoke with <a href="https://computerhistory.org/profile/adele-goldberg/" rel="noopener noreferrer" target="_blank">Adele Goldberg</a>, who codeveloped the language along with <a href="https://computerhistory.org/profile/alan-kay/" rel="noopener noreferrer" target="_blank">Alan Kay</a> and <a href="https://computerhistory.org/profile/dan-ingalls/" rel="noopener noreferrer" target="_blank">Dan Ingalls</a>.</p><p><strong><em>IEEE Spectrum:</em> What was the motivation behind creating Smalltalk?</strong></p><p><strong>Adele Goldberg:</strong> In the early ’70s, Alan Kay proposed that we should all be carrying around the handheld computer which he called the <a href="https://www.mprove.de/visionreality/media/Kay72a.pdf" rel="noopener noreferrer" target="_blank">Dynabook</a>. The question was, “What would you do with a handheld computer—something you can carry with you all the time? Would it be relegated to professionals or could anybody create on it?”</p><p>We were interested in understanding the kind of software that would allow what Alan liked to call “children of all ages” to create models of their understanding of how the world worked. It was more than applications—it was the development environment, and we called it a modeling environment, not a programming environment. That translated into the initial experiments on the simulation language, and that’s what motivated the particular <a href="https://spectrum.ieee.org/xerox-parc/smalltalk-history" target="_self">ideas behind Smalltalk</a>.</p><p><strong>What was your role in developing Smalltalk?</strong></p><p><strong>Goldberg:</strong> In 1973, Alan Kay was putting a group together at <a href="https://spectrum.ieee.org/xerox-parc" target="_self">Xerox at their new Palo Alto Research Center</a> and invited me to join. I was very involved in experimenting with various language constructs and what we would need as we moved from Smalltalk-72 to Smalltalk-80. By 1978, Allan had taken a sabbatical and did not return to Xerox; I then became the group manager and eventually a research-laboratory manager.</p><p>I was running a lab and helping direct what to work on, but I was very focused on getting the results out the door for broader use in the world. So, I worked with the team to get a version of Smalltalk that would run on multiple machines—not just Xerox proprietary workstations, but microprocessor technology that was becoming quite popular and available. Eventually, we got Xerox to let us spin out our own commercial company, and I was the founding CEO.</p><p><strong>When Smalltalk was developed, what do you think made it different from the existing programming languages at that time?</strong></p><p><strong>Goldberg:</strong> When you start thinking about how Smalltalk was different from the type of software that was then available, it was geared up to provide a components-based, reuse-based approach to software development—something that was very iterative and you could prototype and evolve toward an understanding of what it is you wanted to do. And because of the structure, object-oriented technology was a good way to rapidly make changes with predictability in terms of maintaining the reliability or stability of the system.</p><p><strong>Would you say during that time it was groundbreaking?</strong></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="A woman sits in front of a microphone" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="18c7cb060f8cece3ad65659f66c4086c" data-rm-shortcode-name="rebelmouse-image" id="96df5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-sits-in-front-of-a-microphone.jpg?id=31291864&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Adele Goldberg</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Computer History Museum</small></p><p><strong>Goldberg:</strong> You always build on the shoulders of other people. The whole approach that was the foundation of objects and message sending comes from <a href="https://en.wikipedia.org/wiki/Simula" rel="noopener noreferrer" target="_blank">Simula</a>. What we were focused on was not language alone, but how the language supports and influences that larger use in the development environment and the ability to just look at something running.</p><p>I remember people in the ’80s were arguing about low-level user-interface issues. In particular, when you select text, are you underlining, boxing the selected text, or reversing black and white to white and black? Back then, we had visitors in the lab, and one of my colleagues was doing the demonstrations. I could see that these two gentlemen were not happy, so I asked what’s wrong. They said that’s not the way to show a selection and explained what they wanted. My colleague showed a running text editor as an example of building media on top of the language environment, and he interrupted the active process, found where the highlighting action was, and changed it to do what they wanted. The entire tone of the room changed—you could feel it...   These guys were sitting up and then they relaxed.</p><p>We had the live flexibility to change things, and we were saying, “How can we empower you to have what you want?” But we got attacked for that too. It meant that people were dealing with a virtual machine and an interpretive system and they assumed it was inherently slow. We had to prove that wasn’t the case and that you could incrementally compile. I remember the days when one of my engineers was quilting and other people were doing other things while they waited for a compile to finish. It took a while, and you lose the immediacy. When you lose the immediacy, you lose the energy that goes into creativity. So, we were excited about the idea—the fun of “It’s always live; it’s always running, and you get what you want.”</p><p><strong>How has Smalltalk influenced the world of object-oriented programming?</strong></p><p><strong>Goldberg:</strong> One original mission of our research lab was education, but the other one was dealing with a new paradigm for software development. So the influence there is in software engineering and proving the case for switching from structured programming to object-oriented programming. That impact in the flow from the programming environment to software engineering meant that lots of work went into methodology. This new approach to software development changed how you organized the team, your planning, and your analysis of the problem.</p><p>Then, there have been new programming languages that either took an existing language and added objects or had a whole new approach to their idea of what it meant to have more of a components-based technology.</p><p>We get a lot of credit for graphical user interfaces. We were trying to figure out the best way to allow a person to touch that computer and say what they want to do with senses other than a keyboard. We certainly demonstrated a lot of that, and a lot of our applications were initial enablers for new ways of doing text editing or painting or building animations and integrating multimedia and graphics. Those applications were often the first time people would see something like overlapping windows, or the kind of spreadsheets or text editing we did that were objects and embedding objects—again, a components-based approach.</p><p><strong>Why does Smalltalk continue to be an influential programming language, even today?</strong></p><p><strong>Goldberg:</strong> I’m pretty confident that Smalltalk remains the only language system that does a couple of things differently. One is that everything really is an object. When we did Smalltalk-80, we made sure everything’s an object, all activities are done by message sending, all objects are instances of a class. Then, you should be able to build general hierarchies of objects and classes. It’s more consistent in the model of how things work, so it makes it easier for people to learn and take the risk and change.</p><p>Another big point is that you learn to write by reading, and it's the same with programming. Smalltalk-80 was specifically orchestrated to be readable—not just by looking at the object descriptions, but there’s the set of tools written in the language itself. People can essentially write whole new worlds in Smalltalk so that the experimentation flows better. You could say, “What are you and what do you do? Show me your definition. Show me who’s calling on you and who you call on.” All that comes out of an object-oriented approach.</p><p>It’s a fun place to play but also do serious work, which is why you still see people who are into Smalltalk.</p><p><strong>Do you hope Smalltalk will go on to live for another 50 years?</strong></p><p><strong>Goldberg:</strong> It’s not something I even think about. In the world of computer science, designing languages for specific needs and uses is part of the fun, so whether it’s called Smalltalk or not, I think it’s nice to see the paradigm living on.</p><p><em>This transcript has been edited for brevity and clarity.</em></p>]]></description><pubDate>Tue, 30 Aug 2022 15:23:00 +0000</pubDate><guid>https://spectrum.ieee.org/qa-adele-goldberg-on-the-legacy-of-smalltalk</guid><category>Programming languages</category><category>Coding</category><category>Smalltalk</category><category>Object-oriented programming</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-old-computer-displaying-smalltalk.jpg?id=31291865&amp;width=980"></media:content></item><item><title>Cutting-edge Analysis of EDGE Compute Nodes &amp; 5G</title><link>https://www.ansys.com/resource-center/webinar/cutting-edge-analysis-of-edge-compute-nodes-5g</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=26851525&width=980"/><br/><br/><p>NEDGE systems enable 5G/6G network providers to move cloud access, data processing, and storage closer to the consumer. This reduces the signal latency to levels that allow technologies such as autonomous vehicle control, AR/VR applications, eVTOL, emergency communications, smart factory operation, smart cities, large IoT systems, and more.</p><p><a href="https://www.ansys.com/resource-center/webinar/cutting-edge-analysis-of-edge-compute-nodes-5g" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a></p><p>In today's 5G systems, EDGE compute nodes are constructed with 5G NR mmWave and sub6 wireless connection capabilities built-in, with WiFi, Bluetooth, and GPS combined with a complex microserver.</p><p>The need for proximity makes designing EDGE nodes complex. Since many nodes are outside and must be weather sealed, you can't use airflow cooling systems. This webinar will show how you can use digital mission engineering software to model EDGE nodes in an extensive system and assess them for signal integrity, power integrity, thermal integrity, structural integrity, antenna systems, and large system deployment.</p>]]></description><pubDate>Mon, 29 Aug 2022 17:54:02 +0000</pubDate><guid>https://www.ansys.com/resource-center/webinar/cutting-edge-analysis-of-edge-compute-nodes-5g</guid><category>5g</category><category>Edge computing</category><category>Iot</category><category>Telecommunications</category><category>Type:webinar</category><category>Ansys</category><dc:creator>Ansys</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851525/origin.png"></media:content></item><item><title>Web3 Ride-Hailing App Drife Takes on Uber in India</title><link>https://spectrum.ieee.org/blockchain-ridehailing-app-drife-takes-on-uber-in-india</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/3-people-stand-at-a-car-with-the-drife-logo-on-it-smiling-at-the-camera.jpg?id=31180016&width=1245&height=700&coordinates=0%2C0%2C0%2C531"/><br/><br/><p>Ride-hailing in India is dominated by Uber and local rival <a href="https://www.olacabs.com/mobile" target="_blank">Ola</a>, but startup <a href="https://www.drife.io/" target="_blank">Drife </a>thinks blockchain technology could be the key to breaking up their duopoly.</p><p>CEO and cofounder <a href="https://www.linkedin.com/in/firdosh-sheikh/?originalSubdomain=ae" rel="noopener noreferrer" target="_blank">Firdosh Sheikh</a> used to be a power user of Uber, racking up more than 5,000 rides in total. But after speaking to drivers she discovered many were unhappy with the commissions the platform charged and the often opaque rules that governed which rides they get assigned. Riders also get a raw deal, she says, with little control over what they pay for their rides or who they travel with.</p><p>The problem, Sheikh decided, boiled down to having a middleman controlling the relationship between riders and drivers. So she set out to build a decentralized ride-hailing platform that puts control back into the hands of users via <a data-linked-post="2653906605" href="https://spectrum.ieee.org/cryptocurrency-blockchains-dont-need-to-be-energy-intensive" target="_blank">blockchain</a> technology. Last November, after three years of development, she and her cofounders launched the Drife app in the southern city of Bangalore. Today, they have more than 10,000 drivers and 100,000 riders signed up to the platform.</p><p>One of the main things that differentiates them from companies like Uber, says Sheikh, is that they don’t charge any commission. Drivers get to keep the entire fare and will instead pay a monthly subscription to use the platform, although the company is currently waiving this to encourage sign-ups.</p><p>“We don’t charge anything from the fare that you pay as a rider,” she says. “A centralized entity who has a profit motive in the fare that I pay as a rider will start manipulating the fare and exploiting it for their own profit motive, and that’s where both drivers and riders struggle.”</p><p>A base fare is set based on the class of vehicle and the distance of the trip, but after that riders can boost the amount they are willing to pay to attract more drivers. Drivers can also make counteroffers. The rider then chooses who to go with based either on price or driver ratings. This means pricing is purely market-driven, and because Drife isn’t taking a cut, fares should be cheaper than alternatives, says Sheikh.</p><p>The company also has its own cryptocurrency, called DRF. At present it can’t be used for much, but the token will play an important role in Drife’s expansion plans, says Sheikh. The company plans to operate on a franchise model, with local entrepreneurs bidding to run Drife operations in new cities in return for a share of subscription fees. But to apply for a franchise, they will need to purchase a large chunk of DRF tokens and lock them up for the duration of their contract. So far the company has received about 60 franchise requests from cities across the world, says Sheikh.</p><p>About 30 percent of DRF tokens have also been reserved for an “ecosystem fund,” which will be used for incentives and rewards for drivers and riders. Besides being tradable for real money, these tokens will also confer the right to vote in a decentralized autonomous organization (DAO)—a kind of member-run organization whose internal rules are encoded into a blockchain. Each city will have their own DAO, which will be responsible for choosing franchisees.</p><p class="pull-quote">“Nothing that we see today can work at the scale that we want to grow,” says Sheikh. “That’s why we’ve started our own side project where we’re working on a blockchain customized to our own needs.”</p><p>Crucially, users won’t need to take Drife’s word for any of this, as the entire system will be governed by <a href="https://spectrum.ieee.org/how-smart-contracts-work" target="_self">smart contracts</a>. These are software programs that live inside a blockchain and automate transactions according to predefined rules that are visible to everyone. “Nobody’s going to trust me if I say I don’t manipulate it unless I show them that I don’t have any power to manipulate that data,” says Sheikh. “That’s only possible through blockchain.”</p><p>In reality, however, Drife is building the plane while flying it. The firm’s smart-contract system is built on <a href="https://polygon.technology/" target="_blank">Polygon</a>, which is an extension of the Ethereum blockchain. But because they are constantly tweaking features and functionality, Sheikh admits that most of the time operations are actually running on a back-end server that mimics the processing the blockchain is supposed to do.</p><p>And many of the details about how the platform will work still need to be ironed out. How to ensure the DAO voting system isn’t dominated by those who hold the most tokens is a work in progress, says Sheikh. Most blockchains are also geared toward settling financial transactions, and it’s not clear if they can cope with the volume of real-world data involved in running a large-scale ride-hailing business. “Nothing that we see today can work at the scale that we want to grow,” says Sheikh. “That’s why we’ve started our own side project where we’re working on a blockchain customized to our own needs.”</p><p>It might still be some time before that becomes a problem. Despite a promising number of sign-ups, the company is currently doing only around 7,000 rides a week, compared to the millions done by Uber and Ola every day.<br/> </p><p>Mobility consultant <a href="https://www.linkedin.com/in/vinay-piparsania-6992779b/?original_referer=&originalSubdomain=in" rel="noopener noreferrer" target="_blank">Vinay Piparsania</a>, founder of MillenStrat Advisory & Research, says that blockchain technology holds considerable promise for disrupting the ride-hailing industry and breaking the monopolies of the big players. The biggest challenge for startups like Drife, though, is matching their financial and operational capabilities.</p><p>“Unfortunately, at this time, such driver-focused startups are much too small and fragmented to make the difference to the duopolic might of Uber and Ola,” he says. It’s a David and Goliath situation, Piparsania adds, so for the time being these companies should focus on “nibbling away in some key towns and categories by attracting and holding onto drivers, and actually demonstrating to riders that they can compete on delivery.”</p><p><em>This article appears in the November 2022 print issue as “Blockchain Ride-Hailing App Takes on Uber in India.”<br/></em></p>]]></description><pubDate>Fri, 26 Aug 2022 03:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/blockchain-ridehailing-app-drife-takes-on-uber-in-india</guid><category>Blockchain</category><category>Ride-hailing</category><category>Drife</category><category>Ethereum</category><category>Web3</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/3-people-stand-at-a-car-with-the-drife-logo-on-it-smiling-at-the-camera.jpg?id=31180016&amp;width=980"></media:content></item><item><title>Simulating for High-Speed Digital Insights</title><link>https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&amp;cmpid=ASC-2105674&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=306</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=27151193&width=980"/><br/><br/><p>The latest technology for serial links and memory interfaces is getting into the multi-gigabit range. We see them adopting multi-level modulations and more advanced data recovery methods. As a result creating a stable and compliant design is more challenging than ever before and standard signal integrity analysis is no longer sufficient.</p><p><a href="https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&cmpid=ASC-2105674&utm_source=ADSC&utm_medium=ASC&utm_campaign=306" rel="noopener noreferrer" target="_blank">Register now for this free webinar!</a></p><p>Keysight is offering a design flow, which gives you all the insights you need. In this webinar series, our experts will cover leading edge applications of Keysight's premier SerDes and Memory simulation platform, PathWave ADS, with respect to Signal Integrity, Power Integrity and EMI simulation and analysis.</p>]]></description><pubDate>Thu, 25 Aug 2022 19:25:56 +0000</pubDate><guid>https://connectlp.keysight.com/LP=30540?elqCampaignId=22564&amp;cmpid=ASC-2105674&amp;utm_source=ADSC&amp;utm_medium=ASC&amp;utm_campaign=306</guid><category>Keysight</category><category>Signal processing</category><category>Simulation</category><category>Type:webinar</category><dc:creator>Keysight</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/27151193/origin.png"></media:content></item><item><title>Clarivate: Innovators to Watch 2022 Report</title><link>https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;campaignid=7014N000002CbR8&amp;utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;utm_source=ieee&amp;utm_medium=</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=27150766&width=980"/><br/><br/><p>Since introducing the Top 100 Global Innovators list in 2012, each year Clarivate identifies the institutions and companies that sit at the very top of the global innovation ecosystem.<u></u><u></u></p><p><a href="https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&campaignid=7014N000002CbR8&utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&utm_source=ieee&utm_medium=" rel="noopener noreferrer" target="_blank">Download this free whitepaper now!</a></p><p>With each year's list arises the core question of which organizations could be candidates for entry in future years. In this report, we identify those potential future recipients using an overlay analysis focused on the fastest risers.</p><p>In Innovators to watch 2022 report, we identify 37 potential future recipients using an overlay analysis focused on fastest risers.</p><p>To read more on the updated selection process and see what companies and organizations are headed for the Top 100 Global Innovators, read the Innovators to watch 2022.</p>]]></description><pubDate>Thu, 25 Aug 2022 19:15:46 +0000</pubDate><guid>https://clarivate.com/lp/innovators-to-watch-2022/?campaignname=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;campaignid=7014N000002CbR8&amp;utm_campaign=Innovators_to_Watch_LeadGen_Jul_IPG_Global_2022&amp;utm_source=ieee&amp;utm_medium=</guid><category>Innovation</category><category>Type:whitepaper</category><category>Clarivate</category><dc:creator>Clarivate</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/27150766/origin.png"></media:content></item><item><title>5G Networks Are Worryingly Hackable</title><link>https://spectrum.ieee.org/5g-virtualization-increased-hackability</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-network-with-locks-around-it.jpg?id=31141770&width=1245&height=700&coordinates=0%2C187%2C0%2C188"/><br/><br/><p>Prominent tech firms like <a href="https://www.lightreading.com/service-provider-cloud/with-hacks-on-rise-microsoft-looks-to-secure-open-ran/d/d-id/779743" rel="noopener noreferrer" target="_blank">Microsoft</a> and <a href="https://www.fiercewireless.com/tech/nec-exec-open-ran-moving-pretty-fast-compared-prior-tech-shifts" rel="noopener noreferrer" target="_blank">NEC</a> have recently expressed concerns over the security and perhaps too-rapid adoption, respectively, of critical 5G technologies. Now German security researchers have given some substance to the industry’s fears and unease.</p><p>At a hacker conference held in the Netherlands last month, <a href="https://www.youtube.com/watch?v=LRQsFTmWa2w" rel="noopener noreferrer" target="_blank">Karsten Nohl</a>, founder of Berlin-based <a href="https://www.srlabs.de/" rel="noopener noreferrer" target="_blank">Security Research Labs</a>, outlined how his team had breached live 5G networks in a series of “red teaming” exercises—where hackers are hired by a company to test their defenses. In most cases they were able to take control of the network, he says, potentially allowing them to steal customer data or disrupt operations.</p><p>The hacks, <a href="https://media.ccc.de/v/mch2022-273-openran-5g-hacking-just-got-a-lot-more-interesting" rel="noopener noreferrer" target="_blank">revealed at the May Contain Hackers 2022 event (a.k.a. MCH2022</a>), were made possible thanks to poorly configured cloud technology, which is playing an increasingly important role in 5G networks. Nohl says many telcos are inexperienced in how to protect such systems, and his team found that operators had failed to apply basic cloud security techniques that could help mitigate hacks.</p><p class="pull-quote">The push toward Open RAN, virtualization, and “cloudifcation” unlocks more choice and functionality for 5G operators. It has also thrust them into the unfamiliar role of system integrator, suddenly responsible for securing the entire supply chain.</p><p>“5G has swept over telcos with all its implications, and nobody seems well prepared,” says Nohl. “We are introducing new technology into mobile networks, and those technologies can greatly enhance the security of our mobile networks. Or they can basically destroy any hacking resistance we’ve built up over the years. People are not aware of those choices.”</p><p>Mobile operators have traditionally relied on proprietary hardware from vendors like Ericsson, Nokia, and Huawei to build their networks. But in recent years, there has been a major push to “virtualize” network functions, which involves replicating key components in software so they can run on generic hardware, or even in the cloud. And the advent of 5G has only heightened the demand for virtualization, in particular when it comes to <a href="https://en.wikipedia.org/wiki/Radio_access_network" target="_blank">radio access networks (RANs)</a>—the part of the network involved in connecting end-user devices like cellphones to the network core.</p><p>Virtualization has a host of benefits, including the ability to deploy networks faster and more cheaply, to quickly upgrade networks, and even to dynamically reconfigure them in response to changing situations on the ground. The decoupling of hardware and software also prevents <a href="https://en.wikipedia.org/wiki/Vendor_lock-in" target="_blank">vendor lock-in</a> and allows network operators to mix and match components from different companies, something advocated for by the <a href="https://spectrum.ieee.org/the-cellular-industrys-clash-over-the-movement-to-remake-networks" target="_self">Open RAN movement</a>.</p><p>But these new capabilities are also making 5G networks more complex, says Nohl, which in turn necessitates the increasing use of automation to manage networks. And the ability to mix and match software and services from different companies means far more people are involved in the development pipeline. “The more stuff you have and the more moving parts, the more opportunities for mistakes, little misconfigurations,” says Nohl.</p><p>This makes it much easier to break into such virtualized networks than was previously possible. Among the entry points the team discovered included a backdoor-revealing API that had been posted publicly to the Internet as well as an old development site that had accidentally been left online. But the increased ease with which attackers can penetrate the networks is not in and of itself the main problem. “The really critical question is how difficult it is to break through from your initial foothold to something actually valuable within the network,” says Nohl.</p><p>His team found it was worryingly easy to move deeper into the networks they tested, thanks primarily to poorly configured “containers.” These are self-contained packages of software that bundle up an application and everything needed to run it—code, software libraries, and configuration files—so that it can be run on any hardware. Containers are a critical part of the cloud, because they allow different applications from different companies or departments to run alongside one another on the same servers. Containers are supposed to be isolated from one another, but if they are poorly configured it’s possible to break out and gain access to other containers or even to take control of the host system. In multiple instances Nohl and his team found misconfigured containers that allowed them to do just this.</p><p class="pull-quote">“I saw it many times when security teams were invited to the party when all is done and almost finished. The security guys have a very short time slot in order to fine-tune it—if they are actually allowed to touch it.”<br/>—Dmitry Kurbatov, SecurityGen</p><p>Some of the above difficulties could be attributed to the fact that telcos are inexperienced when it comes to cloud security, says Nohl. But they also may be taking shortcuts. Often operators are “lifting and shifting” preexisting software components into containers, Nohl said, but many of the settings designed to isolate containers from one another prevent the software from working as it should. Rather than rewriting code, developers often simply remove these protections, says Nohl. “Those shortcuts we see everywhere now,” he says.</p><p>“Network operators are having to move into a new operating model that’s significantly different than what they’ve done in the past,” says <a href="https://www.linkedin.com/in/erichanselman/" rel="noopener noreferrer" target="_blank">Eric Hanselman</a>, chief analyst at 451 Research. “The reality is that telcos have never had to deal with these levels of software development or low-level infrastructure management before. They always rely on their suppliers for this.”</p><p>While the shift toward Open RAN and the growing virtualization and “cloudifcation” of networks is unlocking more choice and functionality for operators, says Hanselman, it has also thrust them into the unfamiliar role of system integrator, responsible for securing the entire supply chain.</p><p><a href="https://www.linkedin.com/in/xcosta/" target="_blank">Xavier Costa Pérez</a>, head of 5G networks R&D at <a href="https://www.neclab.eu/" target="_blank">NEC Laboratories Europe</a>, disputes that operators are behind the curve when it comes to 5G security. While he admits that the transition to more virtualized networks entails inevitable risks, he says major players are investing heavily in security and partnering with cloud providers to tap into their security expertise. “I think the telco industry is very much aware that this can be a big issue,” he says. “It’s critical for survival, so I don’t think it’s taken lightly at all.”</p><p>It’s also important to remember that these kinds of highly virtualized networks still represent only a small portion of the 5G infrastructure today, probably less than 10 percent, says Costa Pérez. And all operators have backup 4G networks that they can switch to in the event of any problems.</p><p>Network operators aren’t complacent, says Ian Smith, security operations director at the industry body <a href="https://www.gsma.com/" target="_blank">GSMA</a>. “We know that maintaining security, especially for new network technologies, is an ongoing and evolving effort, and one to which the mobile industry is wholeheartedly committed.”</p><p>However, that hasn’t been the experience of <a href="https://www.linkedin.com/in/dmitry-kurbatov/?originalSubdomain=uk" rel="noopener noreferrer" target="_blank">Dmitry Kurbatov</a>, cofounder of telecom security startup SecurityGen. He has found that security often appears to be an afterthought, rather than being part of the development process from the start. “I saw it many times when security teams were invited to the party when all is done and almost finished.” he says. “The security guys have a very short time slot in order to fine-tune it—if they are actually allowed to touch it.”</p><p>Nonetheless, he’s optimistic about the shift to 5G. Previously, operators had little option but to trust vendors when it came to security, but now they will be able to take matters into their own hands. “You actually can have full visibility and control over [5G] systems and functions, which means now you have the chance as the network owner to be much more secure,” he says.</p><p>And even more important, the industry isn’t alone in going through the transition to the cloud, says John Carse, chief information security officer at the Japanese operator <a href="https://corp.mobile.rakuten.co.jp/english/" target="_blank">Rakuten Mobile</a>, which has been a champion of Open RAN principles. “This is a good thing because it means telecom doesn’t have a special problem to solve,” he says. “Telecom can benefit from adoption of techniques happening in all the industries surrounding it versus trying to overcome proprietary challenges.”</p>]]></description><pubDate>Wed, 24 Aug 2022 20:09:18 +0000</pubDate><guid>https://spectrum.ieee.org/5g-virtualization-increased-hackability</guid><category>Cybersecurity</category><category>Telecommunications</category><category>Open ran</category><category>5g</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-network-with-locks-around-it.jpg?id=31141770&amp;width=980"></media:content></item><item><title>Software Turns Promise Up for Offshore Wind</title><link>https://spectrum.ieee.org/vertical-axis-wind-turbine</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-shows-two-versions-of-a-turbine-design-in-sun-and-in-stormy-weather.jpg?id=31179195&width=1245&height=700&coordinates=0%2C200%2C0%2C201"/><br/><br/><p><span style="background-color: initial;">A radical new idea for offshore wind turbines would replace tall unwieldy towers that had blades on top with lightweight, towerless machines whose blades resemble the loops of a whisk. Now new software can help optimize these unusual designs to help make them a reality, researchers say.</span><br/></p><p>This new work comes as the U.S. government plans to boost <a href="https://spectrum.ieee.org/more-stagger-more-wind-power" target="_self">offshore wind energy</a>. In March, the White House announced <a href="https://www.energy.gov/articles/energy-secretary-granholm-announces-ambitious-new-30gw-offshore-wind-deployment-target" target="_blank">a national goal</a> to deploy 30 gigawatts of new offshore wind power by 2030. The federal government suggested this initiative could help power more than 10 million homes, support roughly 77,000 jobs, cut 78 million tonnes in carbon emissions, and spur US $12 billion in private investment per year. As part of this new plan, in <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2022/06/23/fact-sheet-biden-administration-launches-new-federal-state-offshore-wind-partnership-to-grow-american-made-clean-energy/" rel="noopener noreferrer" target="_blank">June</a>, the White House and eleven governors from along the East Coast launched a Federal-State Offshore Wind Implementation Partnership to further develop the offshore wind supply chain, including manufacturing facilities and port capabilities.</p><p>One reason offshore wind is attractive is the high demand for electricity on the coasts. People often live far away from where onshore wind is the strongest, and there is not enough space in cities for enough solar panels to power them, says <a href="https://energy.sandia.gov/programs/renewable-energy/water-power/water-power-personnel/ryan-coe/" rel="noopener noreferrer" target="_blank">Ryan Coe</a>, a mechanical engineer in Sandia National Laboratories’ water-power group in Albuquerque.</p><p class="pull-quote">The guy wires can be shortened or lengthened to adjust the height of the blades in response to wind conditions, much as modifying the amount of tension in a bowstring can help change the curve of a bow.</p><p>However, nearly 60 percent of the accessible offshore wind in the United States blows across water more than 60 meters deep. At such depths, it would prove very expensive to build a foundation for wind turbines on the seafloor. However, this deeper offshore wind energy remains attractive, as it roughly equals the entire annual electricity consumption in the United States.<br/></p><p>Wind turbines that can float above the seafloor could help play a key role in renewable energy. However, floating offshore wind turbines face their own challenges, such as their cost.</p><p>“Floating offshore wind is currently estimated to be three to five times more expensive than land-based wind in the United States,” says mechanical engineer <a href="https://energy.sandia.gov/programs/renewable-energy/wind-power/wind-energy-staff/brandon-ennis" target="_blank">Brandon Ennis</a>, Sandia’s offshore wind technical lead in Albuquerque.</p><p>The aim of the U.S. Department of Energy’s <a href="https://spectrum.ieee.org/underwater-manta-kites-tidal-power-harvesting" target="_self">Advanced Research Project Agency–Energy</a>’s <a href="https://arpa-e.energy.gov/technologies/programs/atlantis" target="_blank">ATLANTIS</a> project is to optimize the design of floating offshore wind turbines to maximize their power while minimizing their expense. The new, towerless design might help reduce both their mass and cost, Ennis says.</p><p>When it comes to land-based wind power, the turbines themselves represent about 65 percent of their levelized cost of energy—that is, their lifetime costs divided by energy production. In comparison, when it comes to floating offshore wind power, the turbines represent only about 20 percent of their levelized costs, Ennis says. Instead, their floating platforms are the biggest factors behind these costs.</p><p>“Given this difference in contribution to the levelized cost of energy of the turbine itself, it makes sense that the turbine design for floating offshore wind could be radically different than what is optimal on land,” Ennis says.</p><p>Most land-based wind turbines nowadays are <a href="https://en.wikipedia.org/wiki/Wind_turbine#Types" rel="noopener noreferrer" target="_blank">horizontal-axis machines</a>. They each possess a tower with blades that spin on a horizontal shaft, which cranks a generator behind the blades in the turbine’s nacelle, the box at the top of the turbine.</p><p>In contrast, the new design is a <a href="https://spectrum.ieee.org/are-vertical-axis-turbines-the-future-of-offshore-wind-power" target="_self">vertical-axis wind turbine</a> (VAWT). These can resemble <a href="https://en.wikipedia.org/wiki/Revolving_door" rel="noopener noreferrer" target="_blank">revolving doors</a>—they each possess blades that spin on a vertical shaft, with a generator below the blades. Placing a wind turbine’s massive generator at the base of the machine instead of high up on a tower makes it much less top heavy, which reduces the size and cost of the floating platform needed to keep it afloat.</p><p>One challenge when it comes to VAWTs is protecting them from extreme winds. Traditional horizontal-axis wind turbines (HAWTs) can rotate away from intense, damaging winds, but previous VAWTs catch wind from every direction.</p><p>The new design replaces the central vertical tower often used in previous VAWTs with a set of taut wires. These wires can be shortened or lengthened to adjust the height of the blades in response to wind conditions, much as modifying the amount of tension in a bowstring can help change the curve of a bow.</p><p>This new design can help maximize each wind turbine’s energy capture while controlling strain. In addition, replacing the shaft with wires further reduces the weight of the turbine, allowing the floating platform to be significantly smaller and less expensive.</p><p>Previous VAWTs with towers often weighed more than HAWTs. In contrast, the new towerless design may weigh less than traditional HAWT designs, Ennis says.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A photograph looking skyward at a white tower with a large loop around it." class="rm-shortcode" data-rm-shortcode-id="13c97d001d3cb427c6607ae4f8c50082" data-rm-shortcode-name="rebelmouse-image" id="b681c" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photograph-looking-skyward-at-a-white-tower-with-a-large-loop-around-it.jpg?id=31179342&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A historical photo shows Sandia’s experimental 34-meter-diameter vertical-axis wind turbine, built in Texas in the 1980s.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Randy Montoya/Sandia National Laboratories</small></p><p>Less mass above the water level requires less mass below the water level, leading to a lighter, cheaper platform, Ennis notes. “The optimal wind energy system would remove all mass and cost that is not directly capturing energy from the wind, and the towerless VAWT design concept moves toward that goal by reducing the turbine mass and the associated platform mass,” Ennis says. Sandia filed a patent application for the new design in 2020.</p><p>However, a key challenge with all VAWTs is how they were not the focus of research as the wind industry steadily grew for the past 30 years, Ennis says. As such, where HAWTs possess software to help in their design, VAWTS did not.</p><p>Now Sandia researchers have developed a new tool to model the way in which VAWTs and their floating platforms might respond to different wind and sea conditions. This new Offshore Wind Energy Simulator (<a href="https://arc.aiaa.org/doi/pdf/10.2514/1.J060476" rel="noopener noreferrer" target="_blank">OWENS</a>) can help optimize the design and control of these machines.</p><p>“Without a tool like ours, there cannot be a floating offshore VAWT industry, and it has been exciting to see our team advance that capability in such a short time compared to the historical development of design tools in the wind industry,” Ennis says.</p><p>The OWENS tool can also simulate land-based VAWTs. Moreover, with some modifications, it can model unconventional HAWTs, such as <a href="https://newsroom.ucla.edu/releases/innovative-biplane-design-next-generation-wind-turbines" rel="noopener noreferrer" target="_blank">bi-wing blades</a> or <a href="https://iopscience.iop.org/article/10.1088/1757-899X/1034/1/012042/meta" rel="noopener noreferrer" target="_blank">shrouded concepts</a>, as well as cross-flow marine hydrokinetic turbines, which are essentially VAWTs submerged in water instead of air, Ennis says.</p><p>Popular misinformation does exist concerning VAWTs, Ennis says. “People claim that VAWTs have lower aerodynamic performance,” he says. “That’s wrong. Lift-based VAWTs are predicted to have the ability to slightly exceed the maximum efficiency of a horizontal-axis wind turbine.”</p><p>Ennis does note that “VAWTs that were commercialized in the ’90s had fatigue issues. They also used aluminum blades with bolted joints, which is almost the worst material decision you could make for fatigue, and the design standards weren’t sufficient to characterize the operational life of a wind turbine.” However, modern designs can overcome these concerns, he says.</p><p>The researchers are validating OWENS with a land-based 34-meter-diameter VAWT built at Sandia in the ’80s. This research can eventually help certify VAWT models against pertinent design standards. They plan to submit their findings to the journal <em>Wind Energy Science</em>.</p><p>The scientists hope to have an optimized floating VAWT design by the end of the year, Ennis says. They next aim to have a small physical demo machine to verify its performance.</p><p>“Vertical-axis wind turbines offer some meaningful advantages over traditional horizontal-axis wind turbines, particularly for floating offshore wind energy,” Ennis says. “With our new design software, we are in a good position to evaluate just how significant these advantages might be for our towerless VAWT system.”</p>]]></description><pubDate>Wed, 24 Aug 2022 19:25:08 +0000</pubDate><guid>https://spectrum.ieee.org/vertical-axis-wind-turbine</guid><category>Offshore wind</category><category>Wind turbines</category><category>Wind energy</category><category>Arpa-e</category><category>Sandia national laboratories</category><category>Wind power</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-shows-two-versions-of-a-turbine-design-in-sun-and-in-stormy-weather.jpg?id=31179195&amp;width=980"></media:content></item><item><title>Top Programming Languages 2022</title><link>https://spectrum.ieee.org/top-programming-languages-2022</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-with-people-typing-on-laptop-surrounded-by-floating-windows.jpg?id=31173894&width=1245&height=700&coordinates=0%2C281%2C0%2C282"/><br/><br/><p>Welcome to <em>IEEE Spectrum</em>’s ninth annual ranking of the Top Programming Languages! This year we’ve revamped and streamlined our interactive ranking tool and made other changes under the hood, but the goal remains the same—to combine multiple metrics from different sources to estimate the relative popularity of different languages.</p><p>You can get into the details of our methodological changes below (the TL;DR is that we eliminated the need for folks to run a giant ball of floating-point math in their browser), but first let’s get into what the rankings tell us this year.</p><h3>IEEE Spectrum’s Top Programming Languages 2022</h3><br/><div class="flourish-embed flourish-chart" data-src="visualisation/10817270?1210395"><script src="https://public.flourish.studio/resources/embed.js"></script></div><p><a href="https://www.python.org/" rel="noopener noreferrer" target="_blank">Python</a> remains on top but is closely followed by <a href="https://www.open-std.org/jtc1/sc22/wg14/" rel="noopener noreferrer" target="_blank">C</a>. Indeed, the combined popularity of C and the big C-like languages—<a href="https://isocpp.org/" rel="noopener noreferrer" target="_blank">C++</a> and <a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/introduction" rel="noopener noreferrer" target="_blank">C#</a>—would outrank Python by some margin. <a href="https://www.java.com/en/download/help/whatis_java.html" rel="noopener noreferrer" target="_blank">Java</a> also remains popular, as does <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" rel="noopener noreferrer" target="_blank">Javascript</a>, the latter buoyed by the ever-increasing complexity of websites and in-browser tools (although it’s worth noting that in some quarters, the cool thing is now <a href="https://www.lowtechmagazine.com/2018/09/how-to-build-a-lowtech-website.html" rel="noopener noreferrer" target="_blank">deliberately stripped-down static sites</a> built with just <a href="https://www.w3.org/standards/webdesign/htmlcss" rel="noopener noreferrer" target="_blank">HTML and simple CSS</a>).</p><p>But among these stalwarts is the rising popularity of <a href="https://blog.ansi.org/2018/10/sql-standard-iso-iec-9075-2016-ansi-x3-135/#gref" target="_blank">SQL</a>. In fact, it’s at No. 1 in our Jobs ranking, which looks solely at metrics from the<a href="https://jobs.ieee.org/" target="_blank"> IEEE Job Site</a> and <a href="https://www.careerbuilder.com/" target="_blank">CareerBuilder</a>. Having looked through literally hundreds and hundreds of job listings in the course of compiling these rankings for you, dear reader, I can say that the strength of the SQL signal is not because there are a lot of employers looking for <em>just</em> SQL coders, in the way that they advertise for Java experts or C++ developers. They want a given language <em>plus</em> SQL. And lots of them want that “plus SQL.”</p><p class="pull-quote">It may not be the most glamorous language...but some experience with SQL is a valuable arrow to have in your quiver.</p><p><span></span>This is likely because so many applications today involve a front-end or middleware layer talking to a back-end database, often over a network to eliminate local resource constraints. Why reinvent the wheel and try to hack your own database and accompanying network interface protocol when <a href="http://troels.arvin.dk/db/rdbms/" target="_blank">so many SQL implementations</a> are available? Chances are there’s probably already one that fits your use case. And even when a networked back end isn’t practical, embedded and single-board computers can be found with enough oomph to run a <a href="https://www.sqlite.org/about.html" target="_blank">SQL database locally</a>. (For more on the rise of SQL, see our <a href="https://spectrum.ieee.org/the-rise-of-sql" target="_blank">accompanying article</a>.)</p><p>So it may not be the most glamorous language or what you’re going to use to implement the next Great Algorithm, but some experience with SQL is a valuable arrow to have in your quiver.</p><p>Looking at complete jobs listings also shows that if you’re interested in cyberwarfare (both offensive and defensive), then getting handy with assembly code is a pretty good in. Previously, I generally just associated assembly code with things like device drivers, tweaking <a href="https://0xax.gitbooks.io/linux-insides/content/Theory/linux-theory-3.html" target="_blank">the guts of operating systems</a>, or <a href="https://www.libhunt.com/l/assembly/topic/retrocomputing" target="_blank">retrocomputing</a>. But many of the job listings calling for expertise in assembly were posted by the kinds of low-profile cybersecurity contractors that orbit Washington, D.C., and even one government agency—the <a href="https://www.nsa.gov/" target="_blank">NSA</a>.</p><p>Job listings are of course not the only metrics we look at in <em>Spectrum</em>. A complete list of our <a href="https://spectrum.ieee.org/top-programming-languages-methods" target="_blank">sources is here</a>, but in a nutshell we look at nine metrics that we think are good proxies for measuring what languages people are programming in. Sources include GitHub, Google, Stack Overflow, Twitter, and <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" target="_blank">IEEE Xplore</a>. The raw data is normalized and weighted according to the different rankings offered—for example, the <em>Spectrum</em> default ranking is heavily weighted toward the interests of IEEE members, while Trending puts more weight on forums and social-media metrics.</p><p><div class="rblad-ieee_in_content"></div></p><p><span></span>In previous years, we allowed readers to bypass these preset rankings and create a custom ranking by adjusting the weights however they pleased. However, it turned out that very few people were taking advantage of this feature. Taking it out allows us to precompute the preset rankings instead of serving an app that contained the data from all the metrics and then computed the rankings in the browser on the fly. Quite apart from making the app large, and thus slower to load, we also ran into the problem that different browsers could produce slightly different results, thanks to variations in floating-point implementations! (This problem of different implementations giving different results was largely solved by the <a href="https://standards.ieee.org/ieee/754/6210/" target="_blank">IEEE-754 standard</a> for floating-point numbers, so it would be interesting to go back and find out which browser versions are noncompliant. But for now let’s just all agree to not run any, say, nuclear reactors with a Web app, okay?)<br/></p><p>Creating the rankings also pulls us into the typical dilemmas faced by any taxonomist—you might think you’ve got a straightforward and unambiguous way to classify things, but then edge cases and weird hybrids invariably slither into view. Plus there’s <a href="https://en.wikipedia.org/wiki/Lumpers_and_splitters" target="_blank">the eternal struggle between “lumping” and “splitting”</a>—is it best to focus on similarities and thus put multiple things under fewer labels overall, or focus on differences and break things up as much as possible and have more fine-grained labels?</p><p>For us this question starts with considering just what <em>is</em> a programming language. This causes a lot of folks to shout at us, especially with regard to HTML/CSS. Although not <a href="https://en.wikipedia.org/wiki/Turing_completeness" target="_blank">Turing-complete</a> except under highly artificial conditions, we <em>do</em> consider HTML/CSS a programming language because the tags in its source code are primarily intended as imperatives to do things such as “present this text in a table format” or “make this heading larger than the body text.” Another question that crops up is when do you decide that a superset or subset of one language has become distinct enough to be considered separately?</p><p><div class="rblad-ieee_in_content"></div></p><p>Generally, we let <a href="https://youtu.be/MwQNkECtskU?t=22" target="_blank">pragmatism be our guide</a>. You can argue that Arduino microcontrollers are programmed in a subset of Java (or C++), but if someone asked for help writing an Arduino project, giving them a book on Java would be of limited use. On the other hand, there are a lot of books with titles along the lines of <em>Writing Arduino Programs Made EZ</em>, so the Arduino language is listed separately. On the other hand, it doesn’t seem to make much sense to put, say, <a href="https://micropython.org/" target="_blank">MicroPython</a> and <a href="https://circuitpython.org/" target="_blank">CircuitPython</a> anywhere but firmly in the Python box, at least for now. Categorizations evolve. For example, previously we grouped <a href="https://www.typescriptlang.org/" target="_blank">Typescript</a> with JavaScript, but adoption has grown enough that it makes more sense to break it out.</p><p>These are all essentially subjective decisions, as are the weights we assign to different metrics, so your mileage may vary. We simply offer this up as our approach to a tricky problem—after all, no one can directly measure what languages people are programming in.</p>We hope you find them useful, and if you think we need to be making different judgement calls, leave us a comment telling us why!]]></description><pubDate>Tue, 23 Aug 2022 15:59:26 +0000</pubDate><guid>https://spectrum.ieee.org/top-programming-languages-2022</guid><category>Python</category><category>Sql</category><category>Top programming languages</category><category>Coding</category><category>Programming languages</category><category>Type:departments</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-with-people-typing-on-laptop-surrounded-by-floating-windows.jpg?id=31173894&amp;width=980"></media:content></item><item><title>How We Judge the Top Programming Languages</title><link>https://spectrum.ieee.org/top-programming-languages-methods</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.jpg?id=31142677&width=980"/><br/><br/><p>Our interactive ranking of the most popular programming languages was first created by data journalist <a href="http://www.nickdiakopoulos.com/" target="_blank">Nick Diakopoulos</a> in 2013. The current version is maintained by <em>IEEE Spectrum </em>senior editor Stephen Cass with development support from Preeti Kulkarni and Michael Novakovic. As no one can look over the shoulders of every programmer, we have chosen metrics that we believe are reasonable proxies of popularity. By combining metrics to synthesize a single ranking we hope to even out statistical fluctuations, and changing the weights given to different metrics as they’re combined lets us emphasize different aspects, such as what's popular with employers in our Jobs ranking. Data is gathered through a combination of manual collection and APIs and combined using an R script.</p><p>We originally started with a list of over 300 programming languages gathered from GitHub. We looked at the volume of results found on Google when we searched for each one using the template “X programming” where “X” is the name of the language. We then filtered out languages that had a very low number of search results, and followed that by going through the remaining entries by hand to narrow them down to the most interesting. Since then, each year we review the list as new languages find their footing and others slip into obscurity. </p><p>Our final set of 57 languages includes names familiar to most computer users, such as Java, stalwarts like Cobol and Fortran, and languages that thrive in niches, like Haskell. The Processing language was dropped from our rankings this year because its name is a common word even within programming. Its generic name makes it hard to separate out when the word “processing” is referring specifically to the language  (unlike, say, Python, which is a common word generally but nearly always refers to the language within a programming context). Before we scrubbed it from the list, Processing’s score, and thus its ranking, seemed artificially high for a niche language. We hope to attack this problem in next year’s rankings.</p><p>We gauged the popularity of languages using the following sources for a total of nine metrics. <br/></p><h3>Google Search</h3><p>We measured the number of hits for each language by using search for the template “X programming.” This number indicates the volume of online information resources about each programming language. We took the measurement in August 2022, so it represents a snapshot of the Web at that particular moment in time. This data was gathered manually. </p><h3>Twitter</h3><p>We measured the number of hits on Twitter for the template “X programming” for the 7.5 months from January 2022 to mid-August 2022 using the <a href="https://dev.twitter.com/rest/reference/get/search/tweets" target="_blank">Twitter Search</a> API. This number indicates the amount of chatter on social media for the language and reflects the sharing of online resources like news articles or books, as well as physical social activities such as hackathons.</p><h3>Stack Overflow</h3><p><a href="http://stackoverflow.com/" target="_blank">Stack Overflow</a> is a popular site where programmers can ask questions about coding. We measured the number of questions posted that mention each language for the 12 months ending August 2022. Each question is tagged with the languages under discussion, and these tags are used to tabulate our measurements using the <a href="https://api.stackexchange.com/docs" rel="noopener noreferrer" target="_blank">Stack Exchange API</a>.</p><h3>Reddit</h3><p><a href="http://www.reddit.com/" rel="noopener noreferrer" target="_blank">Reddit</a> is a news and information site where users post links and comments. On Reddit, we measured the number of posts mentioning each of the languages during the period spanning  September 2021 and August 2022, using the template “X programming” across any subreddit on the site. We collected data using the <a href="http://www.reddit.com/dev/api" rel="noopener noreferrer" target="_blank">Reddit API</a>.</p><h3>IEEE Xplore Digital Library</h3><p>IEEE maintains a digital library with over 3.6 million conference and journal articles covering a wide array of scientific and engineering disciplines. We measured the number of articles that mention each of the languages in the template “X programming” for the years 2021 and 2022. This metric captures the prevalence of the different programming languages as used and referenced in scholarship. We collected data using the <a href="https://developer.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Xplore API</a>.</p><h3>IEEE Jobs Site</h3><p>We measured the demand for different programming languages in job postings on the <a href="https://jobs.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Job Site</a>. The IEEE Jobs Site has a large number of non-U.S. listings. Because some of the languages we track could be ambiguous in plain text—such as D, Go, J, Ada, and R—we searched for job listings with those words in the job description and then manually examined listings. When the number of listings returned was greater than 500, 200 of the listings were examined as a sample, and the result was used to calculate the total number of matching jobs. The search was conducted in August 2022.</p><h3>CareerBuilder</h3><p>We measured the demand for different programming languages on the <a href="http://www.careerbuilder.com/" rel="noopener noreferrer" target="_blank">CareerBuilder</a> job site. CareerBuilder listings were those offered within the United States. Because there is no publicly available API, we manually searched for listings that included each language. Some of the languages we track could be ambiguous in plain text—such as Go, J, and R—so we manually inspected each listing to remove false positives (for example, listings looking for experience with the Americans With Disabilities Act rather than the Ada programming language.). When more than 200 results were returned, 200 of the listings were examined as a sample, and the result was used to calculate the total number of matching jobs. The search was conducted in August 2022.</p><h3>GitHub</h3><p><a href="https://github.com/explore" target="_blank">GitHub</a> is a public repository for many volunteer-driven open-source software projects, and so indicates what languages coders choose to work in when they have a personal choice. We use looked at two metrics from GitHub: repositories that have been “starred” by users, which reflects long-term interests, and the number of pull requests, which indicates current activity. We used data gathered by <a href="https://madnight.github.io/githut/#/pull_requests/2022/1" target="_blank">GitHut 2.0</a>, which measures the top 50 languages used by number of repositories tagged with that language and draws from GitHub's public API.  The data covers the first quarter of 2022.</p>]]></description><pubDate>Tue, 23 Aug 2022 15:59:10 +0000</pubDate><guid>https://spectrum.ieee.org/top-programming-languages-methods</guid><category>Top programming languages</category><category>Software</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/31142677/origin.jpg"></media:content></item></channel></rss>