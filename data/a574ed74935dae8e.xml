<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Berkeley Artificial Intelligence Research Blog</title>
    <description>The BAIR Blog</description>
    <link>http://bair.berkeley.edu/blog/</link>
    <atom:link href="http://bair.berkeley.edu/blog/feed.xml" rel="self" type="application/rss+xml" />
    
     
      <item>
        <title>Keeping Learning-Based Control Safe by Regulating Distributional Shift</title>
        <description>&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/header.jpg&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; To regulate the distribution shift experience by learning-based controllers, we seek a mechanism for constraining the agent to regions of high data density throughout its trajectory (left). Here, we present an approach which achieves this goal by combining features of density models (middle) and Lyapunov functions (right).&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In order to make use of machine learning and reinforcement learning in controlling real world systems, we must design algorithms which not only achieve good performance, but also interact with the system in a safe and reliable manner. Most prior work on safety-critical control focuses on maintaining the safety of the &lt;em&gt;physical  system&lt;/em&gt;, e.g. avoiding falling over for legged robots, or colliding into obstacles for autonomous vehicles. However, for learning-based controllers, there is another source of safety concern: because machine learning models are only optimized to output correct predictions on the training data, they are prone to outputting erroneous predictions when evaluated on out-of-distribution inputs. Thus, if an agent visits a state or takes an action that is very different from those in the training data, a learning-enabled controller may “exploit” the inaccuracies in its learned component and output actions that are suboptimal or even dangerous.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;To prevent these potential “exploitations” of model inaccuracies, we propose a new framework to reason about the safety of a learning-based controller with respect to its &lt;em&gt;training distribution&lt;/em&gt;. The central idea behind our work is to view the training data distribution as a safety constraint, and to draw on tools from control theory to control the distributional shift experienced by the agent during closed-loop control. More specifically, we’ll discuss how Lyapunov stability can be unified with density estimation to produce Lyapunov density models, a new kind of safety “barrier” function which can be used to synthesize controllers with guarantees of keeping the agent in regions of high data density. Before we introduce our new framework, we will first give an overview of existing techniques for guaranteeing physical safety via barrier function.&lt;/p&gt;

&lt;h1 id=&quot;guaranteeing-safety-via-barrier-functions&quot;&gt;Guaranteeing Safety via Barrier Functions&lt;/h1&gt;
&lt;p&gt;In control theory, a central topic of study is: given &lt;em&gt;known&lt;/em&gt; system dynamics, $s_{t+1}=f(s_t, a_t)$, and &lt;em&gt;known&lt;/em&gt; system constraints, $s \in C$, how can we design a controller that is guaranteed to keep the system within the specified constraints? Here, $C$ denotes the set of states that are deemed safe for the agent to visit. This problem is challenging because the specified constraints need to be satisfied over the agent’s entire trajectory horizon ($s_t \in C$  $\forall 0\leq t \leq T$). If the controller uses a simple “greedy” strategy of avoiding constraint violations in the next time step (not taking $a_t$ for which $f(s_t, a_t) \notin C$), the system may still end up in an “irrecoverable” state, which itself is considered safe, but will inevitably lead to an unsafe state in the future regardless of the agent’s future actions. In order to avoid visiting these “irrecoverable” states, the controller must employ a more “long-horizon” strategy which involves predicting the agent’s entire future trajectory to avoid safety violations at any point in the future (avoid $a_t$ for which all possible $\{ a_{\hat{t}} \}_{\hat{t}=t+1}^H$ lead to some $\bar{t}$ where $s_{\bar{t}} \notin C$ and $t&amp;lt;\bar{t} \leq T$). However, predicting the agent’s full trajectory at every step is extremely computationally intensive, and often infeasible to perform online during run-time.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_1.jpg&quot; width=&quot;40%&quot; /&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_2.jpg&quot; width=&quot;40%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; Illustrative example of a drone whose goal is to fly as straight as possible while avoiding obstacles. Using the “greedy” strategy of avoiding safety violations (left), the drone flies straight because there’s no obstacle in the next timestep, but inevitably crashes in the future because it can’t turn in time. In contrast, using the “long-horizon” strategy (right), the drone turns early and successfully avoids the tree, by considering the entire future horizon future of its trajectory.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Control theorists tackle this challenge by designing “barrier” functions, $v(s)$, to constrain the controller at each step (only allow $a_t$ which satisfy $v(f(s_t, a_t)) \leq 0$). In order to ensure the agent remains safe throughout its entire trajectory, the constraint induced by barrier functions ($v(f(s_t, a_t))\leq 0$) prevents the agent from visiting both unsafe states and irrecoverable states which inevitably lead to unsafe states in the future. This strategy essentially amortizes the computation of looking into the future for inevitable failures when designing the safety barrier function, which only needs to be done once and can be computed offline. This way, at runtime, the policy only needs to employ the greedy constraint satisfaction strategy on the barrier function $v(s)$ in order to ensure safety for all future timesteps.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_4.jpg&quot; width=&quot;50%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; The blue region denotes the of states allowed by the barrier function constraint, $\{s | v(s) \leq 0\}$. Using a “long-horizon” barrier function, the drone only needs to greedily ensure that the barrier function constraint $v(s) \leq 0$ is satisfied for the next state, in order to avoid safety violations for all future timesteps. &lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Here, we used the notion of a “barrier” function as an umbrella term to describe a number of different kinds of functions whose functionalities are to constrain the controller in order to make long-horizon guarantees. Some specific examples include &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-1-4757-3108-8_5&quot;&gt;control Lyapunov functions&lt;/a&gt; for guaranteeing stability, &lt;a href=&quot;https://arxiv.org/abs/1903.11199&quot;&gt;control barrier functions&lt;/a&gt; for guaranteeing general safety constraints, and the value function in &lt;a href=&quot;https://arxiv.org/abs/1709.07523&quot;&gt;Hamilton-Jacobi reachability&lt;/a&gt; for guaranteeing general safety constraints under external disturbances. More recently, there has also been &lt;a href=&quot;https://arxiv.org/abs/1705.08551&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1805.07708&quot;&gt;work&lt;/a&gt; on learning barrier functions, for settings where the system is unknown or where barrier functions are difficult to design. However, prior works in both traditional and learning-based barrier functions are mainly focused on making guarantees of physical safety. In the next section, we will discuss how we can extend these ideas to regulate the distribution shift experienced by the agent when using a learning-based controller.&lt;/p&gt;

&lt;h1 id=&quot;lyapunov-density-models&quot;&gt;Lyapunov Density Models&lt;/h1&gt;
&lt;p&gt;To prevent model exploitation due to distribution shift, many learning-based control algorithms constrain or regularize the controller to prevent the agent from taking low-likelihood actions or visiting low likelihood states, for instance in &lt;a href=&quot;https://arxiv.org/abs/2006.04779&quot;&gt;offline RL&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2005.13239&quot;&gt;model-based RL&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1606.03476&quot;&gt;imitation learning&lt;/a&gt;. However, most of these methods only constrain the controller with a single-step estimate of the data distribution, akin to the “greedy” strategy of keeping an autonomous drone safe by preventing actions which causes it to crash in the next timestep. As we saw in the illustrative figures above, this strategy is not enough to guarantee that the drone will not crash (or go out-of-distribution) in another future timestep.&lt;/p&gt;

&lt;p&gt;How can we design a controller for which the agent is guaranteed to stay in-distribution for its entire trajectory? Recall that barrier functions can be used to guarantee constraint satisfaction for all future timesteps, which is exactly the kind of guarantee we hope to make with regards to the data distribution. Based on this observation, we propose a new kind of barrier function: the Lyapunov density model (LDM), which merges the dynamics-aware aspect of a Lyapunov function with the data-aware aspect of a density model (it is in fact a generalization of both types of function). Analogous to how Lyapunov functions keeps the system from becoming physically unsafe, our Lyapunov density model keeps the system from going out-of-distribution.&lt;/p&gt;

&lt;p&gt;An LDM ($G(s, a)$) maps state and action pairs to negative log densities, where the values of $G(s, a)$ represent the best data density the agent is able to stay above throughout its trajectory. It can be intuitively thought of as a “dynamics-aware, long-horizon” transformation on a single-step density model ($E(s, a)$), where $E(s, a)$ approximates the negative log likelihood of the data distribution. Since a single-step density model constraint ($E(s, a) \leq -\log(c)$ where $c$ is a cutoff density) might still allow the agent to visit “irrecoverable” states which inevitably causes the agent to go out-of-distribution, the LDM transformation increases the value of those “irrecoverable” states until they become “recoverable” with respect to their updated value. As a result, the LDM constraint ($G(s, a) \leq -\log(c)$) restricts the agent to a smaller set of states and actions which excludes the “irrecoverable” states, thereby ensuring the agent is able to remain in high data-density regions throughout its entire trajectory.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/jason.jpg&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; Example of data distributions (middle) and their associated LDMs (right) for a 2D linear system (left). LDMs can be viewed as &quot;dynamics-aware, long-horizon&quot; transformations on density models. &lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;How exactly does this “dynamics-aware, long-horizon” transformation work? Given a data distribution $P(s, a)$ and dynamical system $s_{t+1} = f(s_t, a_t)$, we define the following as the LDM operator: $\mathcal{T}G(s, a) = \max\{-\log P(s, a), \min_{a’} G(f(s, a), a’)\}$. Suppose we initialize $G(s, a)$ to be $-\log P(s, a)$. Under one iteration of the LDM operator, the value of a state action pair, $G(s, a)$, can either remain at $-\log P(s, a)$ or increase in value, depending on whether the value at the best state action pair in the next timestep, $\min_{a’} G(f(s, a), a’)$, is larger than $-\log P(s, a)$. Intuitively, if the value at the best next state action pair is larger than the current $G(s, a)$ value, this means that the agent is unable to remain at the current density level regardless of its future actions, making the current state “irrecoverable” with respect to the current density level. By increasing the current the value of $G(s, a)$, we are “correcting” the LDM such that its constraints would not include “irrecoverable” states. Here, one LDM operator update captures the effect of looking into the future for one timestep. If we repeatedly apply the LDM operator on $G(s, a)$ until convergence, the final LDM will be free of “irrecoverable” states in the agent’s entire future trajectory.&lt;/p&gt;

&lt;p&gt;To use an LDM in control, we can train an LDM and learning-based controller on the same training dataset and constrain the controller’s action outputs with an LDM constraint ($G(s, a)) \leq -\log(c)$). Because the LDM constraint prevents both states with low density and “irrecoverable” states, the learning-based controller will be able to avoid out-of-distribution inputs throughout the agent’s entire trajectory. Furthermore, by choosing the cutoff density of the LDM constraint, $c$, the user is able to control the tradeoff between protecting against model error vs. flexibility for performing the desired task.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/hopper.gif&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; Example evaluation of ours and baseline methods on a hopper control task for different values of constraint thresholds (x- axis). On the right, we show example trajectories from when the threshold is too low (hopper falling over due to excessive model exploitation), just right (hopper successfully hopping towards target location), or too high (hopper standing still due to over conservatism). &lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;So far, we have only discussed the properties of a “perfect” LDM, which can be found if we had oracle access to the data distribution and dynamical system. In practice, though, we approximate the LDM using only data samples from the system. This causes a problem to arise: even though the role of the LDM is to prevent distribution shift, the LDM itself can also suffer from the negative effects of distribution shift, which degrades its effectiveness for preventing distribution shift. To understand the degree to which the degradation happens, we analyze this problem from both a theoretical and empirical perspective. Theoretically, we show even if there are errors in the LDM learning procedure, an LDM constrained controller is still able to maintain guarantees of keeping the agent in-distribution. Albeit, this guarantee is a bit weaker than the original guarantee provided by a perfect LDM, where the amount of degradation depends on the scale of the errors in the learning procedure. Empirically, we approximate the LDM using deep neural networks, and show that using a learned LDM to constrain the learning-based controller still provides performance improvements compared to using single-step density models on several domains.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ldm-control/bar.jpg&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt; Evaluation of our method (LDM) compared to constraining a learning-based controller with a density model, the variance over an ensemble of models, and no constraint at all on several domains including hopper, lunar lander, and glucose control. &lt;/i&gt;
&lt;/p&gt;

&lt;h1 id=&quot;conclusion-and-takeaways&quot;&gt;Conclusion and Takeaways&lt;/h1&gt;
&lt;p&gt;Currently, one of the biggest challenges in deploying learning-based controllers on real world systems is their potential brittleness to out-of-distribution inputs, and lack of guarantees on performance. Conveniently, there exists a large body of work in control theory focused on making guarantees about how systems evolve. However, these works usually focus on making guarantees with respect to physical safety requirements, and assume access to an accurate dynamics model of the system as well as physical safety constraints. The central idea behind our work is to instead view the training data distribution as a safety constraint. This allows us to make use of these ideas in controls in our design of learning-based control algorithms, thereby inheriting both the scalability of machine learning and the rigorous guarantees of control theory.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;This post is based on the paper “Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control”, presented at ICML 2022. You
find more details in &lt;a href=&quot;https://arxiv.org/abs/2206.10524&quot;&gt;our paper&lt;/a&gt; and on our &lt;a href=&quot;https://sites.google.com/berkeley.edu/ldm/&quot;&gt;website&lt;/a&gt;. We thank Sergey Levine, Claire Tomlin, Dibya Ghosh, Jason Choi, Colin Li, and Homer Walke for their valuable feedback on this blog post.&lt;/i&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 19 Sep 2022 02:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/09/19/ldm-control/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/09/19/ldm-control/</guid>
      </item>
     
    
     
      <item>
        <title>Reverse engineering the NTK: towards first-principles architecture design</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Reverse engineering the NTK: towards first-principles architecture design&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/reverse-engineering/rev_eng_fig1.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;ntk,kernels,infinite width,neural architecture design,activation function,ReLU&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;The BAIR Blog&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Jamie Simon&quot; /&gt;

&lt;p&gt;Deep neural networks have enabled technological wonders ranging from voice recognition to machine transition to protein engineering, but their design and application is nonetheless notoriously unprincipled.
The development of tools and methods to guide this process is one of the grand challenges of deep learning theory.
In &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;Reverse Engineering the Neural Tangent Kernel&lt;/a&gt;, we propose a paradigm for bringing some principle to the art of architecture design using recent theoretical breakthroughs: first design a good kernel function – often a much easier task – and then “reverse-engineer” a net-kernel equivalence to translate the chosen kernel into a neural network.
Our main theoretical result enables the design of activation functions from first principles, and we use it to create one activation function that mimics deep \(\textrm{ReLU}\) network performance with just one hidden layer and another that soundly outperforms deep \(\textrm{ReLU}\) networks on a synthetic task.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ntk-reveng/rev_eng_fig1.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
&lt;!-- &lt;small&gt; --&gt;
&lt;i&gt; &lt;b&gt;Kernels back to networks.&lt;/b&gt; Foundational works derived formulae that map from wide neural networks to their corresponding kernels. We obtain an inverse mapping, permitting us to start from a desired kernel and turn it back into a network architecture. &lt;/i&gt;
&lt;!-- &lt;/small&gt; --&gt;
&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;neural-network-kernels&quot;&gt;&lt;strong&gt;Neural network kernels&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The field of deep learning theory has recently been transformed by the realization that deep neural networks often become analytically tractable to study in the &lt;em&gt;infinite-width&lt;/em&gt; limit.
Take the limit a certain way, and the network in fact converges to an ordinary kernel method using either the architecture’s &lt;a href=&quot;https://arxiv.org/abs/1806.07572&quot;&gt;“neural tangent kernel” (NTK)&lt;/a&gt; or, if only the last layer is trained (a la random feature models), its &lt;a href=&quot;https://arxiv.org/abs/1711.00165&quot;&gt;“neural network Gaussian process” (NNGP) kernel&lt;/a&gt;.
Like the central limit theorem, these wide-network limits are often surprisingly good approximations even far from infinite width (often holding true at widths in the hundreds or thousands), giving a remarkable analytical handle on the mysteries of deep learning.&lt;/p&gt;

&lt;!-- Consider, for perspective, how other fields of engineering operate: we start with a description of a problem, procedurally design a structure or system that solves it, and build it.
We normally find that our system behaves close to how we predicted, and if it doesn’t, we can understand its failings.
Deep learning, by contrast, is basically [alchemy](https://www.youtube.com/watch?v=x7psGHgatGM): despite much research, practitioners still have almost no principled methods for neural architecture design, and SOTA systems are often full of hacks and hyperparameters we might not need if we understood what we were doing.
As a result, the development of new methods is often slow and expensive, and even when we find clever new ideas, we often don't understand why they work as well as they do. --&gt;

&lt;h3 id=&quot;from-networks-to-kernels-and-back-again&quot;&gt;&lt;strong&gt;From networks to kernels and back again&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The original works exploring this net-kernel correspondence gave formulae for going from &lt;em&gt;architecture&lt;/em&gt; to &lt;em&gt;kernel&lt;/em&gt;: given a description of an architecture (e.g. depth and activation function), they give you the network’s two kernels.
This has allowed great insights into the optimization and generalization of various architectures of interest.
However, if our goal is not merely to understand existing architectures but to design &lt;em&gt;new&lt;/em&gt; ones, then we might rather have the mapping in the reverse direction: given a &lt;em&gt;kernel&lt;/em&gt; we want, can we find an &lt;em&gt;architecture&lt;/em&gt; that gives it to us?
In this work, we derive this inverse mapping for fully-connected networks (FCNs), allowing us to design simple networks in a principled manner by (a) positing a desired kernel and (b) designing an activation function that gives it.&lt;/p&gt;

&lt;p&gt;To see why this makes sense, let’s first visualize an NTK.
Consider a wide FCN’s NTK \(K(x_1,x_2)\) on two input vectors \(x_1\) and \(x_2\) (which we will for simplicity assume are normalized to the same length).
For a FCN, this kernel is &lt;em&gt;rotation-invariant&lt;/em&gt; in the sense that \(K(x_1,x_2) = K(c)\), where \(c\) is the cosine of the angle between the inputs.
Since \(K(c)\) is a scalar function of a scalar argument, we can simply plot it.
Fig. 2 shows the NTK of a four-hidden-layer (4HL) \(\textrm{ReLU}\) FCN.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ntk-reveng/rev_eng_fig2.png&quot; width=&quot;65%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 2.&lt;/b&gt; The NTK of a 4HL $\textrm{ReLU}$ FCN as a function of the cosine between two input vectors $x_1$ and $x_2$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This plot actually contains much information about the learning behavior of the corresponding wide network!
The monotonic increase means that this kernel expects closer points to have more correlated function values.
The steep increase at the end tells us that the correlation length is not too large, and it can fit complicated functions.
The diverging derivative at \(c=1\) tells us about the smoothness of the function we expect to get.
Importantly, &lt;em&gt;none of these facts are apparent from looking at a plot of \(\textrm{ReLU}(z)\)&lt;/em&gt;!
We claim that, if we want to understand the effect of choosing an activation function \(\phi\), then the resulting NTK is actually more informative than \(\phi\) itself.
It thus perhaps makes sense to try to design architectures in “kernel space,” then translate them to the typical hyperparameters.&lt;/p&gt;

&lt;h3 id=&quot;an-activation-function-for-every-kernel&quot;&gt;&lt;strong&gt;An activation function for every kernel&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Our main result is a “reverse engineering theorem” that states the following:&lt;/p&gt;

&lt;p style=&quot;padding: 10px; border: 2px solid black;&quot;&gt;
&lt;b&gt;Thm 1:&lt;/b&gt; For any kernel $K(c)$, we can construct an activation function $\tilde{\phi}$ such that, when inserted into a &lt;i&gt;single-hidden-layer&lt;/i&gt; FCN, its infinite-width NTK or NNGP kernel is $K(c)$.
&lt;/p&gt;

&lt;p&gt;We give an explicit formula for \(\tilde{\phi}\) in terms of Hermite polynomials
(though we use a different functional form in practice for trainability reasons).
Our proposed use of this result is that, in problems with some known structure, it’ll sometimes be possible to write down a good kernel and reverse-engineer it into a trainable network with various advantages over pure kernel regression, like computational efficiency and the ability to learn features.
As a proof of concept, we test this idea out on the synthetic &lt;em&gt;parity problem&lt;/em&gt; (i.e., given a bitstring, is the sum odd or even?), immediately generating an activation function that dramatically outperforms \(\text{ReLU}\) on the task.&lt;/p&gt;

&lt;h3 id=&quot;one-hidden-layer-is-all-you-need&quot;&gt;&lt;strong&gt;One hidden layer is all you need?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Here’s another surprising use of our result.
The kernel curve above is for a 4HL \(\textrm{ReLU}\) FCN, but I claimed that we can achieve any kernel, including that one, with just one hidden layer.
This implies we can come up with some new activation function \(\tilde{\phi}\) that gives this “deep” NTK in a &lt;em&gt;shallow network&lt;/em&gt;!
Fig. 3 illustrates this experiment.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ntk-reveng/rev_eng_fig3.png&quot; width=&quot;60%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 3.&lt;/b&gt; Shallowification of a deep $\textrm{ReLU}$ FCN into a 1HL FCN with an engineered activation function $\tilde{\phi}$. &lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;Surprisingly, this “shallowfication” actually works.
The left subplot of Fig. 4 below shows a “mimic” activation function \(\tilde{\phi}\) that gives virtually the same NTK as a deep \(\textrm{ReLU}\) FCN.
The right plots then show train + test loss + accuracy traces for three FCNs on a standard tabular problem from the UCI dataset.
Note that, while the shallow and deep ReLU networks have very different behaviors, our engineered shallow mimic network tracks the deep network almost exactly!&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/ntk-reveng/rev_eng_fig4.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;
&lt;p style=&quot;margin-left:20%; margin-right:20%;&quot;&gt;
&lt;small&gt;
&lt;i&gt; &lt;b&gt;Fig 4.&lt;/b&gt; Left panel: our engineered &quot;mimic&quot; activation function, plotted with ReLU for comparison. Right panels: performance traces for 1HL ReLU, 4HL ReLU, and 1HL mimic FCNs trained on a UCI dataset. Note the close match between the 4HL ReLU and 1HL mimic networks.&lt;/i&gt;
&lt;/small&gt;
&lt;/p&gt;

&lt;p&gt;This is interesting from an engineering perspective because the shallow network uses fewer parameters than the deep network to achieve the same performance.
It’s also interesting from a theoretical perspective because it raises fundamental questions about the value of depth.
A common belief deep learning belief is that deeper is not only better but &lt;em&gt;qualitatively different&lt;/em&gt;: that deep networks will efficiently learn functions that shallow networks simply cannot.
Our shallowification result suggests that, at least for FCNs, this isn’t true: if we know what we’re doing, then depth doesn’t buy us anything.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;This work comes with lots of caveats.
The biggest is that our result only applies to FCNs, which alone are rarely state-of-the-art.
However, work on convolutional NTKs is &lt;a href=&quot;https://arxiv.org/abs/2112.05611&quot;&gt;fast progressing&lt;/a&gt;, and we believe this paradigm of designing networks by designing kernels is ripe for extension in some form to these structured architectures.&lt;/p&gt;

&lt;p&gt;Theoretical work has so far furnished relatively few tools for practical deep learning theorists.
We aim for this to be a modest step in that direction.
Even without a science to guide their design, neural networks have already enabled wonders.
Just imagine what we’ll be able to do with them once we finally have one.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is based on &lt;a href=&quot;https://arxiv.org/abs/2106.03186&quot;&gt;the paper&lt;/a&gt; “Reverse Engineering the Neural Tangent Kernel,” which is joint work with &lt;a href=&quot;https://www.sajant.com/&quot;&gt;Sajant Anand&lt;/a&gt; and &lt;a href=&quot;https://deweeselab.com/&quot;&gt;Mike DeWeese&lt;/a&gt;. We provide &lt;a href=&quot;https://github.com/james-simon/reverse-engineering&quot;&gt;code&lt;/a&gt; to reproduce all our results. We’d be delighted to field your questions or comments.&lt;/em&gt;&lt;/p&gt;

&lt;!-- [^1]: In case you're unfamiliar with kernels or kernel regression, a kernel is basically a similarity function between two samples generalizing the dot product, and kernel regression is just linear regression with the dot product replaced by the kernel function. --&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(It’s the belief of this author that deeper really is different for CNNs, and so studies aiming to understand the benefits of depth for generalization should focus on CNNs and other structured architectures.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 29 Aug 2022 09:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/08/29/reverse-engineering/</guid>
      </item>
     
    
     
      <item>
        <title>Why do Policy Gradient Methods work so well in Cooperative MARL? Evidence from Policy Representation</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Why do Policy Gradient Methods work so well in Cooperative MARL? Evidence from Policy Representation&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;assets/pg-ar/ar.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;multi-agent, reinforcement learning, policy gradient&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;The BAIR Blog&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Wei Fu, Chao Yu, Jiaqi Yang, Yi Wu&quot; /&gt;

&lt;p&gt;In cooperative multi-agent reinforcement learning (MARL), due to its &lt;em&gt;on-policy&lt;/em&gt; nature, policy gradient (PG) methods are typically believed to be less sample efficient than value decomposition (VD) methods, which are &lt;em&gt;off-policy&lt;/em&gt;. However, some &lt;a href=&quot;https://arxiv.org/abs/2103.01955&quot;&gt;recent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.09533&quot;&gt;empirical&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.07869&quot;&gt;studies&lt;/a&gt; demonstrate that with proper input representation and hyper-parameter tuning, multi-agent PG can achieve &lt;a href=&quot;http://bair.berkeley.edu/blog/2021/07/14/mappo/&quot;&gt;surprisingly strong performance&lt;/a&gt; compared to off-policy VD methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why could PG methods work so well?&lt;/strong&gt; In this post, we will present concrete analysis to show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, VD can be problematic and lead to undesired outcomes. By contrast, PG methods with individual policies can converge to an optimal policy in these cases. In addition, PG methods with auto-regressive (AR) policies can learn multi-modal policies.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/ar.png&quot; width=&quot;80%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 1: different policy representation for the 4-player permutation game.
&lt;/i&gt;
&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;ctde-in-cooperative-marl-vd-and-pg-methods&quot;&gt;CTDE in Cooperative MARL: VD and PG methods&lt;/h2&gt;

&lt;p&gt;Centralized training and decentralized execution (&lt;a href=&quot;https://arxiv.org/abs/1706.02275&quot;&gt;CTDE&lt;/a&gt;) is a popular framework in cooperative MARL. It leverages &lt;em&gt;global&lt;/em&gt; information for more effective training while keeping the representation of individual policies for testing. CTDE can be implemented via value decomposition (VD) or policy gradient (PG), leading to two different types of algorithms.&lt;/p&gt;

&lt;p&gt;VD methods learn local Q networks and a mixing function that mixes the local Q networks to a global Q function. The mixing function is usually enforced to satisfy the Individual-Global-Max (&lt;a href=&quot;https://arxiv.org/abs/1905.05408&quot;&gt;IGM&lt;/a&gt;) principle, which guarantees the optimal joint action can be computed by greedily choosing the optimal action locally for each agent.&lt;/p&gt;

&lt;p&gt;By contrast, PG methods directly apply policy gradient to learn an individual policy and a centralized value function for each agent. The value function takes as its input the global state (e.g., &lt;a href=&quot;https://arxiv.org/abs/2103.01955&quot;&gt;MAPPO&lt;/a&gt;) or the concatenation of all the local observations (e.g., &lt;a href=&quot;https://arxiv.org/abs/1706.02275&quot;&gt;MADDPG&lt;/a&gt;), for an accurate global value estimate.&lt;/p&gt;

&lt;h2 id=&quot;the-permutation-game-a-simple-counterexample-where-vd-fails&quot;&gt;The permutation game: a simple counterexample where VD fails&lt;/h2&gt;

&lt;p&gt;We start our analysis by considering a stateless cooperative game, namely the permutation game. In an $N$-player permutation game, each agent can output $N$ actions ${ 1,\ldots, N }$. Agents receive $+1$ reward  if their actions are mutually different, i.e., the joint action is a permutation over $1, \ldots, N$; otherwise, they receive $0$ reward. Note that there are $N!$ symmetric optimal strategies in this game.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/permutation_game.png&quot; width=&quot;70%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 2: the 4-player permutation game.
&lt;/i&gt;
&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/vd_pg.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;
Figure 3: high-level intuition on why VD fails in the 2-player permutation game.
    &lt;/i&gt;
&lt;/p&gt;
&lt;p&gt;Let us focus on the 2-player permutation game now and apply VD to the game. In this stateless setting, we use  $Q_1$ and $Q_2$ to denote  the local Q-functions, and use $Q_\textrm{tot}$ to denote  the global Q-function. The IGM principle requires that&lt;/p&gt;

\[\arg\max_{a^1,a^2}Q_\textrm{tot}(a^1,a^2)=\{\arg\max_{a^1}Q_1(a^1),\arg\max_{a^2}Q_2(a^2)\}.\]

&lt;p&gt;We prove that VD cannot represent the payoff of the 2-player permutation game by contradiction. If VD methods were able to represent the payoff, we would have&lt;/p&gt;

\[Q_\textrm{tot}(1, 2)=Q_\textrm{tot}(2,1)=1\quad \text{and}\quad Q_\textrm{tot}(1, 1)=Q_\textrm{tot}(2,2)=0.\]

&lt;p&gt;If either of these two agents has different local Q values (e.g. $Q_1(1)&amp;gt; Q_1(2)$), we have $\arg\max_{a^1}Q_1(a^1)=1$. Then according to the IGM principle, &lt;em&gt;any&lt;/em&gt; optimal joint action&lt;/p&gt;

\[(a^{1\star},a^{2\star})=\arg\max_{a^1,a^2}Q_\textrm{tot}(a^1,a^2)=\{\arg\max_{a^1}Q_1(a^1),\arg\max_{a^2}Q_2(a^2)\}\]

&lt;p&gt;satisfies $a^{1\star}=1$ and $a^{1\star}\neq 2$, so the joint action $(a^1,a^2)=(2,1)$ is sub-optimal, i.e., $Q_\textrm{tot}(2,1)&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;Otherwise, if $Q_1(1)=Q_1(2)$ and $Q_2(1)=Q_2(2)$, then&lt;/p&gt;

\[Q_\textrm{tot}(1, 1)=Q_\textrm{tot}(2,2)=Q_\textrm{tot}(1, 2)=Q_\textrm{tot}(2,1).\]

&lt;p&gt;As a result, value decomposition cannot represent the payoff matrix of the 2-player permutation game.&lt;/p&gt;

&lt;p&gt;What about PG methods? Individual policies can indeed represent an optimal policy for the permutation game. Moreover, stochastic gradient descent can guarantee PG to converge to one of these optima &lt;a href=&quot;https://arxiv.org/abs/1802.06175&quot;&gt;under mild assumptions&lt;/a&gt;. This suggests that, even though PG methods are less popular in MARL compared with VD methods, they can be preferable in certain cases that are common in real-world applications, e.g., games with multiple strategy modalities.&lt;/p&gt;

&lt;p&gt;We also remark that in the permutation game, in order to represent an optimal joint policy, each agent must choose distinct actions. &lt;strong&gt;Consequently, a successful implementation of PG must ensure that the policies are agent-specific.&lt;/strong&gt; This can be done by using either individual policies with unshared parameters (referred to as PG-Ind in our paper), or an agent-ID conditioned policy (&lt;a href=&quot;http://bair.berkeley.edu/blog/2021/07/14/mappo/&quot;&gt;PG-ID&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;pg-outperforms-existing-vd-methods-on-popular-marl-testbeds&quot;&gt;PG outperforms existing VD methods on popular MARL testbeds&lt;/h2&gt;

&lt;p&gt;Going beyond the simple illustrative example of the permutation game, we extend our study to popular and more realistic MARL benchmarks. In addition to StarCraft Multi-Agent Challenge (&lt;a href=&quot;https://github.com/oxwhirl/smac&quot;&gt;SMAC&lt;/a&gt;), where the effectiveness of PG and agent-conditioned policy input &lt;a href=&quot;http://bair.berkeley.edu/blog/2021/07/14/mappo/&quot;&gt;has been verified&lt;/a&gt;, we show new results in Google Research Football (&lt;a href=&quot;https://github.com/google-research/football&quot;&gt;GRF&lt;/a&gt;) and multi-player &lt;a href=&quot;https://github.com/deepmind/hanabi-learning-environment&quot;&gt;Hanabi Challenge&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/football.png&quot; width=&quot;48%&quot; /&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/hanabi.png&quot; width=&quot;45%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 4: (left) winning rates of PG methods on GRF; (right) best and average evaluation scores on Hanabi-Full.
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In GRF, PG methods outperform the state-of-the-art VD baseline (&lt;a href=&quot;https://arxiv.org/abs/2106.02195&quot;&gt;CDS&lt;/a&gt;) in 5 scenarios. Interestingly, we also notice that individual policies (PG-Ind) without parameter sharing achieve comparable, sometimes even higher winning rates, compared to agent-specific policies (PG-ID) in all 5 scenarios. We evaluate PG-ID in the full-scale Hanabi game with varying numbers of players (2-5 players) and compare them to &lt;a href=&quot;https://arxiv.org/abs/1912.02288&quot;&gt;SAD&lt;/a&gt;, a strong off-policy Q-learning variant in Hanabi, and Value Decomposition Networks (&lt;a href=&quot;https://arxiv.org/abs/1706.05296&quot;&gt;VDN&lt;/a&gt;). As demonstrated in the above table, PG-ID is able to produce results comparable to or better than the best and average rewards achieved by SAD and VDN with varying numbers of players using the same number of environment steps.&lt;/p&gt;

&lt;h2 id=&quot;beyond-higher-rewards-learning-multi-modal-behavior-via-auto-regressive-policy-modeling&quot;&gt;Beyond higher rewards: learning multi-modal behavior via auto-regressive policy modeling&lt;/h2&gt;

&lt;p&gt;Besides learning higher rewards, we also study how to learn multi-modal policies in cooperative MARL. Let’s go back to the permutation game. Although we have proved that PG can effectively learn an optimal policy, the strategy mode that it finally reaches can highly depend on the policy initialization. Thus, a natural question will be:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;i&gt;
Can we learn a single policy that can cover all the optimal modes?
    &lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In the decentralized PG formulation, the factorized representation of a joint policy can only represent one particular mode. Therefore, we propose an enhanced way to parameterize the policies for stronger expressiveness — the auto-regressive (AR) policies.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/permutation_ar.gif&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;
Figure 5: comparison between individual policies (PG) and auto-regressive  policies (AR) in the 4-player permutation game.
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Formally, we factorize the joint policy of $n$ agents into the form of&lt;/p&gt;

\[\pi(\mathbf{a} \mid \mathbf{o}) \approx \prod_{i=1}^n \pi_{\theta^{i}} \left( a^{i}\mid o^{i},a^{1},\ldots,a^{i-1} \right),\]

&lt;p&gt;where the action produced by agent $i$ depends on its own observation $o_i$ and all the actions from previous agents $1,\dots,i-1$. The auto-regressive factorization can represent &lt;em&gt;any&lt;/em&gt; joint policy in a centralized MDP. The &lt;em&gt;only&lt;/em&gt; modification to each agent’s policy is the input dimension, which is slightly enlarged by including previous actions; and the output dimension of each agent’s policy remains unchanged.&lt;/p&gt;

&lt;p&gt;With such a minimal parameterization overhead, AR policy substantially improves the representation power of PG methods. We remark that PG with AR policy (PG-AR) can simultaneously represent all optimal policy modes in the permutation game.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/heatmap.png&quot; width=&quot;70%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure: the heatmaps of actions for policies learned by PG-Ind (left) and PG-AR (middle), and the heatmap for rewards (right); while PG-Ind only converge to a specific mode in the 4-player permutation game, PG-AR successfully discovers all the optimal modes.
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In more complex environments, including SMAC and GRF, PG-AR can learn interesting emergent behaviors that require strong intra-agent coordination that may never be learned by PG-Ind.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/2m1z.gif&quot; width=&quot;45%&quot; /&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/pg-ar/3v1.gif&quot; width=&quot;45%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 6: (left) emergent behavior induced by PG-AR in SMAC and GRF. On the 2m_vs_1z map of SMAC, the marines keep standing and attack alternately while ensuring there is only one attacking marine at each timestep; (right) in the academy_3_vs_1_with_keeper scenario of GRF, agents learn a &quot;Tiki-Taka&quot; style behavior: each player keeps passing the ball to their teammates.
&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;discussions-and-takeaways&quot;&gt;Discussions and Takeaways&lt;/h2&gt;

&lt;p&gt;In this post, we provide a concrete analysis of VD and PG methods in cooperative MARL. First, we reveal the limitation on the expressiveness of popular VD methods, showing that they could not represent optimal policies even in a simple permutation game. By contrast, we show that PG methods are provably more expressive. We empirically verify the expressiveness advantage of PG on popular MARL testbeds, including SMAC, GRF, and Hanabi Challenge. We hope the insights from this work could benefit the community towards more general and more powerful cooperative MARL algorithms in the future.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This post is based on our paper: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (&lt;a href=&quot;https://arxiv.org/abs/2206.07505&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://sites.google.com/view/revisiting-marl&quot;&gt;website&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Jul 2022 02:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/07/10/pg-ar/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/07/10/pg-ar/</guid>
      </item>
     
    
     
      <item>
        <title>FIGS: Attaining XGBoost-level performance with the interpretability and speed of CART</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;FIGS: Attaining XGBoost-level performance =with the interpretability and speed of CART&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/figs/figs_intro.gif&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;figs,interpretability,trees,imodels&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;The BAIR Blog&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Chandan Singh, Yan Shuo Tan, Bin Yu&quot; /&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2201.11931&quot;&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/figs/figs_intro.gif&quot; width=&quot;90%&quot; /&gt;&lt;/a&gt;
&lt;br /&gt;
&lt;b&gt;FIGS (Fast Interpretable Greedy-tree Sums): &lt;/b&gt;&lt;i&gt;A method for building interpretable models by simultaneously growing an ensemble of decision trees in competition with one another.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Recent machine-learning advances have led to increasingly complex predictive models, often at the cost of interpretability. We often need interpretability, particularly in high-stakes applications such as in clinical decision-making; interpretable models help with all kinds of things, such as identifying errors, leveraging domain knowledge, and making speedy predictions.&lt;/p&gt;

&lt;p&gt;In this blog post we’ll cover &lt;a href=&quot;https://arxiv.org/abs/2201.11931&quot;&gt;FIGS&lt;/a&gt;, a new method for fitting an &lt;em&gt;interpretable model&lt;/em&gt; that takes the form of a sum of trees. Real-world experiments and theoretical results show that FIGS can effectively adapt to a wide range of structure in data, achieving state-of-the-art performance in several settings, all without sacrificing interpretability.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-figs-work&quot;&gt;How does FIGS work?&lt;/h2&gt;

&lt;p&gt;Intuitively, FIGS works by extending CART, a typical greedy algorithm for growing a decision tree, to consider growing a &lt;em&gt;sum&lt;/em&gt; of trees &lt;em&gt;simultaneously&lt;/em&gt; (see Fig 1). At each iteration, FIGS may grow any existing tree it has already started or start a new tree; it greedily selects whichever rule reduces the total unexplained variance (or an alternative splitting criterion) the most. To keep the trees in sync with one another, each tree is made to predict the &lt;em&gt;residuals&lt;/em&gt; remaining after summing the predictions of all other trees (see &lt;a href=&quot;https://arxiv.org/abs/2201.11931&quot;&gt;the paper&lt;/a&gt; for more details).&lt;/p&gt;

&lt;p&gt;FIGS is intuitively similar to ensemble approaches such as gradient boosting / random forest, but importantly since all trees are grown to compete with each other the model can adapt more to the underlying structure in the data. The number of trees and size/shape of each tree emerge automatically from the data rather than being manually specified.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/figs/figs_fitting.gif&quot; width=&quot;90%&quot; /&gt;&lt;/a&gt;
&lt;br /&gt;
&lt;b&gt;Fig 1. &lt;/b&gt;&lt;i&gt;High-level intuition for how FIGS fits a model.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;an-example-using-figs&quot;&gt;An example using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FIGS&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Using FIGS is extremely simple. It is easily installable through the &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;imodels package&lt;/a&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install imodels&lt;/code&gt;) and then can be used in the same way as standard scikit-learn models: simply import a classifier or regressor and use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt; methods. Here’s a full example of using it on a sample clinical dataset in which the target is risk of cervical spine injury (CSI).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;imodels&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FIGSClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_clean_dataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# prepare data (in this a sample clinical dataset)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_clean_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'csi_pecarn_pred'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.33&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# fit the model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FIGSClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_rules&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# initialize a model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# fit model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# discrete predictions: shape is (n_test, 1)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds_proba&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# predicted probabilities: shape is (n_test, n_classes)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# visualize the model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out.svg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dpi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This results in a simple model – it contains only 4 splits (since we specified that the model should have no more than 4 splits (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_rules=4&lt;/code&gt;). Predictions are made by dropping a sample down every tree, and &lt;i&gt;summing&lt;/i&gt; the risk adjustment values obtained from the resulting leaves of each tree. This model is extremely interpretable, as a physician can now (i) easily make predictions using the 4 relevant features and (ii) vet the model to ensure it matches their domain expertise. Note that this model is just for illustration purposes, and achieves ~84\% accuracy.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/figs/figs_csi_model_small.svg&quot; width=&quot;85%&quot; /&gt;&lt;/a&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Fig 2.&lt;/b&gt; Simple model learned by FIGS for predicting risk of cervical spinal injury. &lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;If we want a more flexible model, we can also remove the constraint on the number of rules (changing the code to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = FIGSClassifier()&lt;/code&gt;), resulting in a larger model (see Fig 3). Note that the number of trees and how balanced they are emerges from the structure of the data – only the total number of rules may be specified.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/figs/figs_csi_model_large.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Fig 3.&lt;/b&gt; Slightly larger model learned by FIGS for predicting risk of cervical spinal injury. &lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;how-well-does-figs-perform&quot;&gt;How well does FIGS perform?&lt;/h2&gt;

&lt;p&gt;In many cases when interpretability is desired, such as &lt;a href=&quot;https://arxiv.org/abs/2205.15135&quot;&gt;clinical-decision-rule modeling&lt;/a&gt;, FIGS is able to achieve state-of-the-art performance. For example, Fig 4 shows different datasets where FIGS achieves excellent performance, particularly when limited to using very few total splits.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/figs/figs_classification.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Fig 4.&lt;/b&gt; FIGS predicts well with very few splits. &lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;why-does-figs-perform-well&quot;&gt;Why does FIGS perform well?&lt;/h2&gt;

&lt;p&gt;FIGS is motivated by the observation that single decision trees often have splits that are repeated in different branches, which may occur when there is &lt;a href=&quot;https://proceedings.mlr.press/v151/shuo-tan22a/shuo-tan22a.pdf&quot;&gt;additive structure&lt;/a&gt; in the data. Having multiple trees helps to avoid this by disentangling the additive components into separate trees.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Overall, interpretable modeling offers an alternative to common black-box modeling, and in many cases can offer massive improvements in terms of efficiency and transparency without suffering from a loss in performance.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;This post is based on two papers: &lt;a href=&quot;https://arxiv.org/abs/2201.11931&quot;&gt;FIGS&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2205.15135&quot;&gt;G-FIGS&lt;/a&gt; – all code is available through the &lt;a href=&quot;https://github.com/csinva/imodels&quot;&gt;imodels package&lt;/a&gt;. This is joint work with &lt;a href=&quot;https://www.linkedin.com/in/nasseri/&quot;&gt;Keyan Nasseri&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/abhineet-agarwal-126171185/&quot;&gt;Abhineet Agarwal&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/james-pc-duncan/&quot;&gt;James Duncan&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/omer-ronen-48ba9412a/?originalSubdomain=il&quot;&gt;Omer Ronen&lt;/a&gt;, and &lt;a href=&quot;https://profiles.ucsf.edu/aaron.kornblith&quot;&gt;Aaron Kornblith&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 30 Jun 2022 02:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/06/30/figs/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/06/30/figs/</guid>
      </item>
     
    
     
      <item>
        <title>The Berkeley Crossword Solver</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;The Berkeley Crossword Solver&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig1.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;keyword1, keyword2&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;Building the world's best automated crossword solver&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak&quot; /&gt;

&lt;p&gt;We recently published the Berkeley Crossword Solver (BCS), the current state of the art for solving American-style crossword puzzles. The BCS combines neural question answering and probabilistic inference to achieve near-perfect performance on most American-style crossword puzzles, like the one shown below:&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig1.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 1: Example American-style crossword puzzle
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;An earlier version of the BCS, in conjunction with Dr.Fill, was the first computer program to outscore all human competitors in the world’s top crossword tournament. The most recent version is the current top-performing system on crossword puzzles from The New York Times, achieving 99.7% letter accuracy (see the &lt;a href=&quot;https://arxiv.org/abs/2205.09665&quot;&gt;technical paper&lt;/a&gt;, &lt;a href=&quot;https://berkeleycrosswordsolver.com&quot;&gt;web demo&lt;/a&gt;, and &lt;a href=&quot;https://github.com/albertkx/Berkeley-Crossword-Solver&quot;&gt;code release&lt;/a&gt;).&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Crosswords are challenging for humans and computers alike. Many clues are vague or underspecified and can’t be answered until crossing constraints are taken into account. While some clues are similar to factoid question answering, others require relational reasoning or understanding difficult wordplay.&lt;/p&gt;

&lt;p&gt;Here are a handful of example clues from our dataset (answers at the bottom of this post):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;They’re given out at Berkeley’s HAAS School (4)&lt;/li&gt;
  &lt;li&gt;Winter hrs. in Berkeley (3)&lt;/li&gt;
  &lt;li&gt;Domain ender that UC Berkeley was one of the first schools to adopt (3)&lt;/li&gt;
  &lt;li&gt;Angeleno at Berkeley, say (8)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;our-approach&quot;&gt;Our Approach&lt;/h1&gt;
&lt;p&gt;The BCS uses a two-step process to solve crossword puzzles. First, it generates a probability distribution over possible answers to each clue using a question answering (QA) model; second, it uses probabilistic inference, combined with local search and a generative language model, to handle conflicts between proposed intersecting answers.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig2.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 2: Architecture diagram of the Berkeley Crossword Solver
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;The BCS’s question answering model is based on DPR (Karpukhin et al., 2020), which is a bi-encoder model typically used to retrieve passages that are relevant to a given question. Rather than passages, however, our approach maps both questions and answers into a shared embedding space and finds answers directly. Compared to the previous state-of-the-art method for answering crossword clues, this approach obtained a 13.4% absolute improvement in top-1000 QA accuracy. We conducted a manual error analysis and found that our QA model typically performed well on questions involving knowledge, commonsense reasoning, and definitions, but it often struggled to understand wordplay or theme-related clues.&lt;/p&gt;

&lt;p&gt;After running the QA model on each clue, the BCS runs loopy belief propagation to iteratively update the answer probabilities in the grid. This allows information from high confidence predictions to propagate to more challenging clues. After belief propagation converges, the BCS obtains an initial puzzle solution by greedily taking the highest likelihood answer at each position.&lt;/p&gt;

&lt;p&gt;The BCS then refines this solution using a local search that tries to replace low confidence characters in the grid. Local search works by using a guided proposal distribution in which characters that had lower marginal probabilities during belief propagation are iteratively replaced until a locally optimal solution is found. We score these alternate characters using a character-level language model (ByT5, Xue et al., 2022), that handles novel answers better than our closed-book QA model.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig3.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 3: Example changes made by our local search procedure
&lt;/i&gt;
&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;
&lt;p&gt;We evaluated the BCS on puzzles from five major crossword publishers, including The New York Times. Our system obtains 99.7% letter accuracy on average, which jumps to 99.9% if you ignore puzzles that involve rare themes. It solves 81.7% of puzzles without a single mistake, which is a 24.8% improvement over the previous state-of-the-art system.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig4.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 4: Results compared to previous state-of-the-art Dr.Fill
&lt;/i&gt;
&lt;/p&gt;

&lt;h1 id=&quot;winning-the-american-crossword-puzzle-tournament&quot;&gt;Winning The American Crossword Puzzle Tournament&lt;/h1&gt;
&lt;p&gt;The American Crossword Puzzle Tournament (ACPT) is the largest and longest-running crossword tournament and is organized by Will Shortz, the New York Times crossword editor. Two prior approaches to computer crossword solving gained mainstream attention and competed in the ACPT: Proverb and Dr.Fill. Proverb is a 1998 system that ranked 213th out of 252 competitors in the tournament. Dr.Fill’s first competition was in ACPT 2012, and it ranked 141st out of 650 competitors. We teamed up with Dr.Fill’s creator Matt Ginsberg and combined an early version of our QA system with Dr.Fill’s search procedure to outscore all 1033 human competitors in the 2021 ACPT. Our joint submission solved all seven puzzles in under a minute, missing just three letters across two puzzles.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/crosswords/fig5.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 5: Results from the 2021 American Crossword Puzzle Tournament (ACPT)
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;We are really excited about the challenges that remain in crosswords, including handling difficult themes and more complex wordplay. To encourage future work, we are releasing a dataset of 6.4M question answer clues, a demo of the Berkeley Crossword Solver, and our code at &lt;a href=&quot;http://berkeleycrosswordsolver.com&quot;&gt;http://berkeleycrosswordsolver.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Answers to clues: MBAS, PST, EDU, INSTATER&lt;/p&gt;
</description>
        <pubDate>Fri, 20 May 2022 03:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/05/20/crosswords/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/05/20/crosswords/</guid>
      </item>
     
    
     
      <item>
        <title>Rethinking Human-in-the-Loop for Artificial Augmented Intelligence</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Rethinking Human-in-the-Loop for Artificial Augmented Intelligence&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/human-in-the-loop/image3.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;Human-in-the-loop, Artificial Augmented Intelligence, Real-world applications&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;It is time to rethink human-in-the-loop.&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Zhongqi Miao, Ziwei Liu&quot; /&gt;

&lt;!-- body --&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;http://bair.berkeley.edu/static/blog/human-in-the-loop/image3.png&quot; width=&quot;90%&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 1: In real-world applications, we think there exist a human-machine loop where humans and machines are mutually augmenting each other. We call it Artificial Augmented Intelligence.
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;How do we build and evaluate an AI system for real-world applications? In most AI research, the evaluation of AI methods involves a training-validation-testing process. The experiments usually stop when the models have good testing performance on the reported datasets because real-world data distribution is assumed to be modeled by the validation and testing data. However, real-world applications are usually more complicated than a single training-validation-testing process. The biggest difference is the ever-changing data. For example, wildlife datasets change in class composition all the time because of animal invasion, re-introduction, re-colonization, and seasonal animal movements. A model trained, validated, and tested on existing datasets can easily be broken when newly collected data contain novel species. Fortunately, we have out-of-distribution detection methods that can help us detect samples of novel species. However, when we want to expand the recognition capacity (i.e., being able to recognize novel species in the future), the best we can do is fine-tuning the models with new ground-truthed annotations. In other words, we need to incorporate human effort/annotations regardless of how the models perform on previous testing sets.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;inevitable-human-in-the-loop&quot;&gt;Inevitable human-in-the-loop&lt;/h1&gt;

&lt;p&gt;When human annotations are inevitable, real-world recognition systems become a never-ending loop of &lt;strong&gt;data collection → annotation → model fine-tuning&lt;/strong&gt; (Figure 2). As a result, the performance of one single step of model evaluation does not represent the actual generalization of the whole recognition system because the model will be updated with new data annotations, and a new round of evaluation will be conducted. With this loop in mind, we think that instead of building a model with &lt;strong&gt;&lt;em&gt;better testing performance&lt;/em&gt;&lt;/strong&gt;, focusing on &lt;strong&gt;&lt;em&gt;how much human effort can be saved&lt;/em&gt;&lt;/strong&gt; is a more generalized and practical goal in real-world applications.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;http://bair.berkeley.edu/static/blog/human-in-the-loop/image1.png&quot; height=&quot;&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 2: In the loop of data collection, annotation, and model update, the goal of optimization becomes minimizing the requirement of human annotation rather than single-step recognition performance.
&lt;/i&gt;
&lt;/p&gt;

&lt;h1 id=&quot;a-case-study-on-wildlife-recognition&quot;&gt;A case study on wildlife recognition&lt;/h1&gt;

&lt;p&gt;In the paper we published last year in Nature-Machine Intelligence [1], we discussed the incorporation of human-in-the-loop into wildlife recognition and proposed to examine human effort efficiency in model updates instead of simple testing performance. For demonstration, we designed a recognition framework that was a combination of active learning, semi-supervised learning, and human-in-the-loop (Figure 3). We also incorporated a time component into this framework to indicate that the recognition models did not stop at any single time step. Generally speaking, in the framework, at each time step, when new data are collected, a recognition model actively selects which data should be annotated based on a prediction confidence metric. Low-confidence predictions are sent for human annotation, and high-confidence predictions are trusted for downstream tasks or pseudo-labels for model updates.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;http://bair.berkeley.edu/static/blog/human-in-the-loop/image2.png&quot; height=&quot;&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 3: Here, we present an iterative recognition framework that can both maximize the utility of modern image recognition methods and minimize the dependence on manual annotations for model updating.  
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In terms of human annotation efficiency for model updates, we split the evaluation into 1) the percentage of high-confidence predictions on validation (i.e., saved human effort for annotation); 2) the accuracy of high-confidence predictions (i.e., reliability); and 3) the percentage of novel categories that are detected as low-confidence predictions (i.e., sensitivity to novelty). With these three metrics, the optimization of the framework becomes minimizing human efforts (i.e., to maximize high-confidence percentage) and maximizing model update performance and high-confidence accuracy.&lt;/p&gt;

&lt;p&gt;We reported a two-step experiment on a large-scale wildlife camera trap dataset collected from Mozambique National Park for demonstration purposes. The first step was an initialization step to initialize a model with only part of the dataset. In the second step, a new set of data with known and novel classes was applied to the initialized model. Following the framework, the model made predictions on the new dataset with confidence, where high-confidence predictions were trusted as pseudo-labels, and low-confidence predictions were provided with human annotations. Then, the model was updated with both pseudo-labels and annotations and ready for the future time steps. As a result, the percentage of high-confidence predictions on second step validation was 72.2%, the accuracy of high-confidence predictions was 90.2%, and the percentage of novel classes detected as low-confidence was 82.6%. In other words, our framework saved 72% of human effort on annotating all the second step data. As long as the model was confident, 90% of the predictions were correct. In addition, 82% of novel samples were successfully detected. Details of the framework and experiments can be found in the original paper.&lt;/p&gt;

&lt;h1 id=&quot;artificial-augmented-intelligence-a2i&quot;&gt;Artificial Augmented Intelligence (A&lt;sup&gt;2&lt;/sup&gt;I)&lt;/h1&gt;

&lt;p&gt;By taking a closer look at Figure 3, besides the &lt;strong&gt;data collection - human annotation - model update&lt;/strong&gt; loop, there is another &lt;strong&gt;human-machine&lt;/strong&gt; loop hidden in the framework (Figure 1). This is a loop where both humans and machines are constantly improving each other through model updates and human intervention. For example, when AI models cannot recognize novel classes, human intervention can provide information to expand the model’s recognition capacity. On the other hand, when AI models get more and more generalized, the requirement for human effort gets less. In other words, the use of human effort gets more efficient.&lt;/p&gt;

&lt;p&gt;In addition, the confidence-based human-in-the-loop framework we proposed is not limited to novel class detection but can also help with issues like long-tailed distribution and multi-domain discrepancies. As long as AI models feel less confident, human intervention comes in to help improve the model. Similarly, human effort is saved as long as AI models feel confident, and sometimes human errors can even be corrected (Figure 4). In this case, the relationship between humans and machines becomes synergistic. Thus, the goal of AI development changes from replacing human intelligence to mutually augmenting both human and machine intelligence. We call this type of AI: &lt;strong&gt;Artificial Augmented Intelligence (A&lt;sup&gt;2&lt;/sup&gt;I)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Ever since we started working on artificial intelligence, we have been asking ourselves, what do we create AI for? At first, we believed that, ideally, AI should fully replace human effort in simple and tedious tasks such as large-scale image recognition and car driving. Thus, we have been pushing our models to an idea called “human-level performance” for a long time. However, this goal of replacing human effort is intrinsically building up opposition or a mutually exclusive relationship between humans and machines. In real-world applications, the performance of AI methods is just limited by so many affecting factors like long-tailed distribution, multi-domain discrepancies, label noise, weak supervision, out-of-distribution detection, etc. Most of these problems can be somehow relieved with proper human intervention. The framework we proposed is just one example of how these separate problems can be summarized into high- versus low-confidence prediction problems and how human effort can be introduced into the whole AI system. We think it is not cheating or surrendering to hard problems. It is a more human-centric way of AI development, where the focus is on how much human effort is saved rather than how many testing images a model can recognize. Before the realization of Artificial General Intelligence (AGI), we think it is worthwhile to further explore the direction of machine-human interactions and A&lt;sup&gt;2&lt;/sup&gt;I such that AI can start making more impacts in various practical fields.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;http://bair.berkeley.edu/static/blog/human-in-the-loop/image4.png&quot; height=&quot;&quot; /&gt;
    &lt;br /&gt;
&lt;i&gt;
Figure 4: Examples of high-confidence predictions that did not match the original annotations. Many high-confidence predictions that were flagged as incorrect based on validation labels (provided by students and citizen scientists) were in fact correct upon closer inspection by wildlife experts.  
&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Acknowledgements: We thank all co-authors of the paper “Iterative Human and Automated Identification of Wildlife Images” for their contributions and discussions in preparing this blog. The views and opinions expressed in this blog are solely of the authors of this paper.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This blog post is based on the following paper which is published at Nature - Machine Intelligence:&lt;br /&gt;
[1] Miao, Zhongqi, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S. Palmer, Stella X. Yu, and Wayne M. Getz. “Iterative human and automated identification of wildlife images.” Nature Machine Intelligence 3, no. 10 (2021): 885-895.(Link to &lt;a href=&quot;https://arxiv.org/pdf/2105.02320.pdf&quot;&gt;Pre-print&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 03 May 2022 06:55:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/05/03/human-in-the-loop/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/05/03/human-in-the-loop/</guid>
      </item>
     
    
     
      <item>
        <title>Designing Societally Beneficial Reinforcement Learning Systems</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Designing Societally Beneficial Reinforcement Learning Systems&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/reward-reports/fb-exo.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;reinforcement learning, ethical AI, documentation&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;Designing Societally Beneficial Reinforcement Learning Systems&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Nathan Lambert, Aaron Snoswell, Sarah Dean, Thomas Krendl Gilbert, Tom Zick&quot; /&gt;

&lt;!-- body --&gt;

&lt;p&gt;Deep reinforcement learning (DRL) is transitioning from a research field focused on game playing to a technology with real-world applications. Notable examples include DeepMind’s work on &lt;a href=&quot;https://www.nature.com/articles/s41586-021-04301-9&quot;&gt;controlling a nuclear reactor&lt;/a&gt; or on improving &lt;a href=&quot;https://arxiv.org/abs/2202.06626&quot;&gt;Youtube video compression&lt;/a&gt;, or Tesla &lt;a href=&quot;https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=4802s&quot;&gt;attempting to use a method inspired by MuZero&lt;/a&gt; for autonomous vehicle behavior planning. But the exciting potential for real world applications of RL should also come with a healthy dose of caution - for example RL policies are well known to be vulnerable to &lt;a href=&quot;https://robotic.substack.com/p/rl-exploitation?s=w&quot;&gt;exploitation&lt;/a&gt;, and methods for safe and &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/&quot;&gt;robust policy development&lt;/a&gt; are an active area of research.&lt;/p&gt;

&lt;p&gt;At the same time as the emergence of powerful RL systems in the real world, the public and researchers are expressing an increased appetite for fair, aligned, and safe machine learning systems. The focus of these research efforts to date has been to account for shortcomings of datasets or supervised learning practices that can harm individuals. However the unique ability of RL systems to leverage temporal feedback in learning complicates the types of risks and safety concerns that can arise.&lt;/p&gt;

&lt;p&gt;This post expands on our recent &lt;a href=&quot;https://cltc.berkeley.edu/2022/02/08/reward-reports/&quot;&gt;whitepaper&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2204.10817&quot;&gt;research paper&lt;/a&gt;, where we aim to illustrate the different modalities harms can take when augmented with the temporal axis of RL. To combat these novel societal risks, we also propose a new kind of documentation for dynamic Machine Learning systems which aims to assess and monitor these risks both before and after deployment.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;whats-special-about-rl-a-taxonomy-of-feedback&quot;&gt;What’s Special About RL? A Taxonomy of Feedback&lt;/h1&gt;

&lt;p&gt;Reinforcement learning systems are often spotlighted for their ability to act in an environment, rather than passively make predictions. Other supervised machine learning systems, such as computer vision, consume data and return a prediction that can be used by some decision making rule. In contrast, the appeal of RL is in its ability to not only (a) directly model the impact of actions, but also to (b) improve policy performance automatically. These key properties of acting upon an environment, and learning within that environment can be understood as by considering the different types of feedback that come into play when an RL agent acts within an environment. We classify these feedback forms in a taxonomy of (1) Control, (2) Behavioral, and (3) Exogenous feedback. The first two notions of feedback, Control and Behavioral, are directly within the formal mathematical definition of an RL agent while Exogenous feedback is induced as the agent interacts with the broader world.&lt;/p&gt;

&lt;h2 id=&quot;1-control-feedback&quot;&gt;1. Control Feedback&lt;/h2&gt;

&lt;p&gt;First is control feedback - in the control systems engineering sense - where the action taken depends on the current measurements of the state of the system. RL agents choose actions based on an observed state according to a policy, which generates environmental feedback. For example, a thermostat turns on a furnace according to the current temperature measurement. Control feedback gives an agent the ability to react to unforeseen events (e.g. a sudden snap of cold weather) autonomously.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/fb-control.png&quot; width=&quot;60%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 1: Control Feedback.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;2-behavioral-feedback&quot;&gt;2. Behavioral Feedback&lt;/h2&gt;

&lt;p&gt;Next in our taxonomy of RL feedback is ‘behavioral feedback’: the trial and error learning that enables an agent to improve its policy through interaction with the environment. This could be considered the defining feature of RL, as compared to e.g. ‘classical’ control theory. Policies in RL can be defined by a set of parameters that determine the actions the agent takes in the future. Because these parameters are updated through behavioral feedback, these are actually a reflection of the data collected from executions of past policy versions. RL agents are not fully ‘memoryless’ in this respect–the current policy depends on stored experience, and impacts newly collected data, which in turn impacts future versions of the agent. To continue the thermostat example - a ‘smart home’ thermostat might analyze historical temperature measurements and adapt its control parameters in accordance with seasonal shifts in temperature, for instance to have a more aggressive control scheme during winter months.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/fb-behavioral.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 2: Behavioral Feedback.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;3-exogenous-feedback&quot;&gt;3. Exogenous Feedback&lt;/h2&gt;

&lt;p&gt;Finally, we can consider a third form of feedback external to the specified RL environment, which we call Exogenous (or ‘exo’) feedback. While RL benchmarking tasks may be static environments, every action in the real world impacts the dynamics of both the target deployment environment, as well as adjacent environments. For example, a news recommendation system that is optimized for clickthrough may change the way editors write headlines towards attention-grabbing  clickbait. In this RL formulation, the set of articles to be recommended would be considered part of the environment and expected to remain static, but exposure incentives cause a shift over time.&lt;/p&gt;

&lt;p&gt;To continue the thermostat example, as a ‘smart thermostat’ continues to adapt its behavior over time, the behavior of other adjacent systems in a household might change in response - for instance other appliances might consume more electricity due to increased heat levels, which could impact electricity costs. Household occupants might also change their clothing and behavior patterns due to different temperature profiles during the day. In turn, these secondary effects could also influence the temperature which the thermostat monitors, leading to a longer timescale feedback loop.&lt;/p&gt;

&lt;p&gt;Negative costs of these external effects will not be specified in the agent-centric reward function, leaving these external environments to be manipulated or exploited. Exo-feedback is by definition difficult for a designer to predict. Instead, we propose that it should be addressed by documenting the evolution of the agent, the targeted environment, and adjacent environments.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/fb-exo.png&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 3: Exogenous (exo) Feedback.&lt;/i&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;how-can-rl-systems-fail&quot;&gt;How can RL systems fail?&lt;/h1&gt;

&lt;p&gt;Let’s consider how two key properties can lead to failure modes specific to RL systems: direct action selection (via control feedback) and autonomous data collection (via behavioral feedback).&lt;/p&gt;

&lt;p&gt;First is decision-time safety. One current practice in RL research to create safe decisions is to augment the agent’s reward function with a penalty term for certain harmful or undesirable states and actions. For example, in a robotics domain we might penalize certain actions (such as extremely large torques) or state-action tuples (such as carrying a glass of water over sensitive equipment). However it is difficult to anticipate where on a pathway an agent may encounter a crucial action, such that failure would result in an unsafe event. This aspect of how reward functions interact with optimizers is especially problematic for deep learning systems, where numerical guarantees are challenging.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/decision.png&quot; width=&quot;90%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 4: Decision time failure illustration.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;As an RL agent collects new data and the policy adapts, there is a complex interplay between current parameters, stored data, and the environment that governs evolution of the system. Changing any one of these three sources of information will change the future behavior of the agent, and moreover these three components are deeply intertwined. This uncertainty makes it difficult to back out the cause of failures or successes.&lt;/p&gt;

&lt;p&gt;In domains where many behaviors can possibly be expressed, the RL specification leaves a lot of factors constraining behavior unsaid. For a robot learning locomotion over an uneven environment, it would be useful to know what signals in the system indicate it will learn to find an easier route rather than a more complex gait. In complex situations with less well-defined reward functions, these intended or unintended behaviors will encompass a much broader range of capabilities, which may or may not have been accounted for by the designer.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/behavior.png&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 5: Behavior estimation failure illustration.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;While these failure modes are closely related to control and behavioral feedback, Exo-feedback does not map as clearly to one type of error and introduces risks that do not fit into simple categories. Understanding exo-feedback requires that stakeholders in the broader communities (machine learning, application domains, sociology, etc.) work together on real world RL deployments.&lt;/p&gt;

&lt;h1 id=&quot;risks-with-real-world-rl&quot;&gt;Risks with real-world RL&lt;/h1&gt;

&lt;p&gt;Here, we discuss four types of design choices an RL designer must make, and how these choices can have an impact upon the socio-technical failures that an agent might exhibit once deployed.&lt;/p&gt;

&lt;h2 id=&quot;scoping-the-horizon&quot;&gt;Scoping the Horizon&lt;/h2&gt;

&lt;p&gt;Determining the timescale on which aRL agent can plan impacts the possible and actual behavior of that agent. In the lab, it may be common to tune the horizon length until the desired behavior is achieved. But in real world systems, optimizations will externalize costs depending on the defined horizon. For example, an RL agent controlling an autonomous vehicle will have very different goals and behaviors if the task is to stay in a lane,  navigate a contested intersection, or route across a city to a destination. This is true even if the objective (e.g. “minimize travel time”) remains the same.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/horizon.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 6: Scoping the horizon example with an autonomous vehicle.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;defining-rewards&quot;&gt;Defining Rewards&lt;/h2&gt;

&lt;p&gt;A second design choice is that of actually specifying the reward function to be maximized. This immediately raises the well-known risk of RL systems, reward hacking, where the designer and agent negotiate behaviors based on specified reward functions. In a deployed RL system, this often results in unexpected exploitative behavior – from &lt;a href=&quot;https://openai.com/blog/faulty-reward-functions/&quot;&gt;bizarre video game agents&lt;/a&gt; to &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/04/19/mbrl/&quot;&gt;causing errors in robotics simulators&lt;/a&gt;. For example, if an agent is presented with the problem of navigating a maze to reach the far side, a mis-specified reward might result in the agent avoiding the task entirely to minimize the time taken.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/reward-shaping.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 7: Defining rewards example with maze navigation.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;pruning-information&quot;&gt;Pruning Information&lt;/h2&gt;

&lt;p&gt;A common practice in RL research is to redefine the environment to fit one’s needs – RL designers make numerous explicit and implicit assumptions to model tasks in a way that makes them amenable to virtual RL agents. In highly structured domains, such as video games, this can be rather benign.However, in the real world redefining the environment amounts to changing the ways information can flow between the world and the RL agent. This can dramatically change the meaning of the reward function and offload risk to external systems. For example, an autonomous vehicle with sensors focused only on the road surface shifts the burden from AV designers to pedestrians. In this case, the designer is pruning out information about the surrounding environment that is actually crucial to robustly safe integration within society.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/info-shaping.png&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 8: Information shaping example with an autonomous vehicle.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;training-multiple-agents&quot;&gt;Training Multiple Agents&lt;/h2&gt;

&lt;p&gt;There is growing interest in the problem of &lt;a href=&quot;https://bair.berkeley.edu/blog/2021/07/14/mappo/&quot;&gt;multi-agent RL&lt;/a&gt;, but as an emerging research area, little is known about how learning systems interact within dynamic environments. When the relative concentration of autonomous agents increases within an environment, the terms these agents optimize for can actually re-wire norms and values encoded in that specific application domain. An example would be the changes in behavior that will come if the majority of vehicles are autonomous and communicating (or not) with each other. In this case, if the agents have autonomy to optimize toward a goal of minimizing transit time (for example), they could crowd out the remaining human drivers and heavily disrupt accepted societal norms of transit.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/multi-agent.png&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 9: The risks of multi-agency example on autonomous vehicles.&lt;/i&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;making-sense-of-applied-rl-reward-reporting&quot;&gt;Making sense of applied RL: Reward Reporting&lt;/h1&gt;

&lt;p&gt;In our recent &lt;a href=&quot;https://cltc.berkeley.edu/2022/02/08/reward-reports/&quot;&gt;whitepaper&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2204.10817&quot;&gt;research paper&lt;/a&gt;, we proposed &lt;a href=&quot;https://rewardreports.github.io/&quot;&gt;Reward Reports&lt;/a&gt;, a new form of ML documentation that foregrounds the societal risks posed by sequential data-driven optimization systems, whether explicitly constructed as an RL agent or &lt;a href=&quot;https://robotic.substack.com/p/ml-becomes-rl?s=w&quot;&gt;implicitly construed&lt;/a&gt; via data-driven optimization and feedback. Building on proposals to document datasets and models, we focus on reward functions: the objective that guides optimization decisions in feedback-laden systems. Reward Reports comprise questions that highlight the promises and risks entailed in defining what is being optimized in an AI system, and are intended as living documents that dissolve the distinction between ex-ante (design) specification and ex-post (after the fact) harm. As a result, Reward Reports provide a framework for ongoing deliberation and accountability before and after a system is deployed.&lt;/p&gt;

&lt;p&gt;Our proposed template for a Reward Reports consists of several sections, arranged to help the reporter themselves understand and document the system. A Reward Report begins with (1) system details that contain the information context for deploying the model. From there, the report documents (2) the optimization intent, which questions the goals of the system and why RL or ML may be a useful tool. The designer then documents (3) how the system may affect different stakeholders in the institutional interface. The next two sections contain technical details on (4) the system implementation and (5) evaluation. Reward reports conclude with (6) plans for system maintenance as additional system dynamics are uncovered.&lt;/p&gt;

&lt;p&gt;The most important feature of a Reward Report is that it allows documentation to evolve over time, in step with the temporal evolution of an online, deployed RL system! This is most evident in the change-log, which is we locate at the end of our Reward Report template:&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:center&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/reward-reports/rr-contents.png&quot; width=&quot;80%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 10: Reward Reports contents.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;what-would-this-look-like-in-practice&quot;&gt;What would this look like in practice?&lt;/h2&gt;

&lt;p&gt;As part of our research, we have developed a reward report &lt;a href=&quot;https://github.com/RewardReports/reward-reports&quot;&gt;LaTeX template, as well as several example reward reports&lt;/a&gt; that aim to illustrate the kinds of issues that could be managed by this form of documentation. These examples include the temporal evolution of the MovieLens recommender system, the DeepMind MuZero game playing system, and a hypothetical deployment of an RL autonomous vehicle policy for managing merging traffic, based on the &lt;a href=&quot;https://flow-project.github.io/&quot;&gt;Project Flow simulator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, these are just examples that we hope will serve to inspire the RL community–as more RL systems are deployed in real-world applications, we hope the research community will build on our ideas for Reward Reports and refine the specific content that should be included. To this end, we hope that you will join us at our (un)-workshop.&lt;/p&gt;

&lt;h2 id=&quot;work-with-us-on-reward-reports-an-unworkshop&quot;&gt;Work with us on Reward Reports: An (Un)Workshop!&lt;/h2&gt;

&lt;p&gt;We are hosting an “un-workshop” at the upcoming conference on Reinforcement Learning and Decision Making (&lt;a href=&quot;https://rldm.org/rldm-2022-workshops/&quot;&gt;RLDM&lt;/a&gt;) on June 11th from 1:00-5:00pm EST at Brown University, Providence, RI. We call this an un-workshop because we are looking for the attendees to help create the content! We will provide templates, ideas, and discussion as our attendees build out example reports. We are excited to develop the ideas behind Reward Reports with real-world practitioners and cutting-edge researchers.&lt;/p&gt;

&lt;p&gt;For more information on the workshop, visit the &lt;a href=&quot;https://rewardreports.github.io/workshop.html&quot;&gt;website&lt;/a&gt; or contact the organizers at &lt;a href=&quot;mailto:geese-org@lists.berkeley.edu&quot;&gt;geese-org@lists.berkeley.edu&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This post is based on the following papers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cltc.berkeley.edu/2022/02/08/reward-reports/&quot;&gt;Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems&lt;/a&gt; by Thomas Krendl Gilbert, Sarah Dean, Tom Zick, Nathan Lambert. Center for Long Term Cybersecurity Whitepaper Series 2022.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2204.10817&quot;&gt;Reward Reports for Reinforcement Learning&lt;/a&gt; by Thomas Krendl Gilbert, Sarah Dean, Nathan Lambert, Tom Zick and Aaron Snoswell. ArXiv Preprint 2022.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 29 Apr 2022 05:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/04/29/reward-reports/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/04/29/reward-reports/</guid>
      </item>
     
    
     
      <item>
        <title>Should I Use Offline RL or Imitation Learning?</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Should I Use Offline RL or Imitation?&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/maers/maers.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;reinforcement learning, data-driven learning, imitation, offline RL&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;When should you use offline RL?&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Aviral Kumar, Ilya Kostrikov, Sergey Levine&quot; /&gt;

&lt;!-- body --&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649378429759_Screenshot+2022-04-07+at+5.40.23+PM.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 1: Summary of our recommendations for when a practitioner should BC and various imitation learning style methods, and when they should use offline RL approaches.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Offline reinforcement learning allows learning policies from previously collected data, which has profound implications for applying RL in domains where running trial-and-error learning is impractical or dangerous, such as safety-critical settings like autonomous driving or medical treatment planning. In such scenarios, online exploration is simply too risky, but &lt;a href=&quot;https://arxiv.org/abs/2005.01643&quot;&gt;offline RL&lt;/a&gt; methods can learn effective policies from logged data collected by &lt;a href=&quot;https://arxiv.org/abs/2109.10813&quot;&gt;humans&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/2010.14500&quot;&gt;heuristically designed controllers&lt;/a&gt;.  Prior learning-based control methods have also approached learning from existing data as imitation learning: if the data is generally “good enough,” simply copying the behavior in the data can lead to good results, and if it’s not good enough, then filtering or reweighting the data and then copying can work well.  &lt;a href=&quot;https://arxiv.org/abs/2106.01345&quot;&gt;Several&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.03298&quot;&gt;recent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.09470&quot;&gt;works&lt;/a&gt; suggest that this is a viable alternative to modern offline RL methods.&lt;/p&gt;

&lt;p&gt;This brings about several questions: &lt;strong&gt;when should we use offline RL? Are there fundamental limitations to methods that rely on some form of imitation (BC, conditional BC, filtered BC) that offline RL addresses?&lt;/strong&gt; While it might be clear that offline RL should enjoy a large advantage over imitation learning when learning from diverse datasets that contain a lot of suboptimal behavior, we will also discuss how even cases that might seem BC-friendly can still allow offline RL to attain &lt;a href=&quot;https://arxiv.org/abs/2204.05618&quot;&gt;significantly better results&lt;/a&gt;. Our goal is to help explain when and why you should use each method and provide guidance to practitioners on the benefits of each approach. Figure 1 concisely summarizes our findings and we will discuss each component.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;methods-for-learning-from-offline-data&quot;&gt;Methods for Learning from Offline Data&lt;/h2&gt;

&lt;p&gt;Let’s start with a brief recap of various methods for learning policies from data that we will discuss. The learning algorithm is provided with an offline dataset \(\mathcal{D}\), consisting of trajectories \(\{\tau_i\}_{i=1}^N\) generated by some behavior policy. Most offline RL methods perform some sort of dynamic programming (e.g., Q-learning) updates on the provided data, aiming to obtain a value function. This typically requires adjusting for &lt;a href=&quot;https://arxiv.org/abs/2005.01643&quot;&gt;distributional&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.04779&quot;&gt;shift&lt;/a&gt; to work well, but when this is done properly, it leads to good results.&lt;/p&gt;

&lt;p&gt;On the other hand, methods based on imitation learning attempt to simply clone the actions observed in the dataset if the dataset is good enough, or perform some kind of filtering or conditioning to extract useful behavior when the dataset is not good. For instance, recent work &lt;a href=&quot;https://arxiv.org/abs/2106.01345&quot;&gt;filters trajectories&lt;/a&gt; based on their return, or directly &lt;a href=&quot;https://arxiv.org/abs/2106.08909&quot;&gt;filters individual transitions&lt;/a&gt; based on how advantageous these could be under the behavior policy and then clones them. Conditional BC methods are based on the idea that every transition or trajectory is optimal when conditioned on the right variable. This way, after conditioning, the data becomes optimal given the value of the conditioning variable, and in principle we could then condition on the desired task, such as a high reward value, and get a near-optimal trajectory. For example, a trajectory that attains a return of \(R_0\) is &lt;em&gt;optimal&lt;/em&gt; if our goal is to attain return \(R = R_0\) (&lt;a href=&quot;https://arxiv.org/abs/1912.13465&quot;&gt;RCPs&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.01345&quot;&gt;decision transformer&lt;/a&gt;); a trajectory that reaches goal \(g\) is optimal for reaching \(g=g_0\) (&lt;a href=&quot;https://openreview.net/forum?id=rALA0Xo6yNJ&quot;&gt;GCSL&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2112.10751&quot;&gt;RvS&lt;/a&gt;). Thus, one can perform perform reward-conditioned BC or goal-conditioned BC, and execute the learned policies with the desired value of return or goal during evaluation. This approach to offline RL bypasses learning value functions or dynamics models entirely, which can make it simpler to use. However, does it actually solve the general offline RL problem?&lt;/p&gt;

&lt;h2 id=&quot;what-we-already-know-about-rl-vs-imitation-methods&quot;&gt;What We Already Know About RL vs Imitation Methods&lt;/h2&gt;

&lt;p&gt;Perhaps a good place to start our discussion is to review the performance of offline RL and imitation-style methods on benchmark tasks. In the figure below, we review the performance of some recent methods for learning from offline data on a subset of the &lt;a href=&quot;https://arxiv.org/abs/2004.07219&quot;&gt;D4RL&lt;/a&gt; benchmark.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1650480950123_Screenshot+2022-04-20+at+11.54.39+AM.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Table 1: Dichotomy of empirical results on several tasks in D4RL. While imitation-style methods (decision transformer, %BC, one-step RL, conditional BC) perform at par with and can outperform offline RL methods (CQL, IQL) on the locomotion tasks, these methods simply break down on the more complex maze navigation tasks.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Observe in the table that while imitation-style methods perform at par with offline RL methods across the span of the locomotion tasks, offline RL approaches vastly outperform these methods (except, goal-conditioned BC, which we will discuss towards the end of this post) by a large margin on the antmaze tasks. &lt;strong&gt;What explains this difference?&lt;/strong&gt; As we will discuss in this blog post, methods that rely on imitation learning are often quite effective when the behavior in the offline dataset consists of some complete trajectories that perform well. This is true for most replay-buffer style datasets, and all of the locomotion datasets in D4RL are generated from replay buffers of online RL algorithms. In such cases, simply filtering good trajectories, and executing the mode of the filtered trajectories will work well. This explains why %BC, one-step RL and decision transformer work quite well. However, offline RL methods can vastly outperform BC methods when this stringent requirement is not met because they benefit from a form of “temporal compositionality” which enables them to learn from suboptimal data. This explains the enormous difference between RL and imitation results on the antmazes.&lt;/p&gt;

&lt;h2 id=&quot;offline-rl-can-solve-problems-that-conditional-filtered-or-weighted-bc-cannot&quot;&gt;Offline RL Can Solve Problems that Conditional, Filtered or Weighted BC Cannot&lt;/h2&gt;

&lt;p&gt;To understand why offline RL can solve problems that the aforementioned BC methods cannot, let’s ground our discussion in a simple, didactic example. Let’s consider the navigation task shown in the figure below, where the goal is to navigate from the starting location A to the goal location D in the maze. This is directly representative of several real-world decision-making scenarios in mobile robot navigation and provides an abstract model for an RL problem in domains such as robotics or recommender systems. Imagine you are provided with data that shows how the agent can navigate from location A to B and how it can navigate from C to E, but no single trajectory in the dataset goes from A to D. Obviously, the offline dataset shown below provides enough information for discovering a way to navigate to D: by combining different paths that cross each other at location E. But, can various offline learning methods find a way to go from A to D?&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649378795135_Screenshot+2022-04-07+at+5.46.30+PM.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 2: Illustration of the base case of temporal compositionality or stitching that is needed find optimal trajectories in various problem domains.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;It turns out that, while offline RL methods are able to discover the path from A to D, various imitation-style methods cannot. This is because offline RL algorithms can &lt;strong&gt;“stitch”&lt;/strong&gt; suboptimal trajectories together: while the trajectories \(\tau_i\) in the offline dataset might attain poor return, a better policy can be obtained by combining good segments of trajectories (A→E + E→D = A→D).  This ability to stitch segments of trajectories temporally is the hallmark of value-based offline RL algorithms that utilize Bellman backups, but cloning (a subset of) the data or trajectory-level sequence models are unable to extract this information, since such no single trajectory from A to D is observed in the offline dataset!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why should you care about stitching and these mazes?&lt;/strong&gt; One might now wonder if this stitching phenomenon is only useful in some esoteric edge cases or if it is an actual, practically-relevant phenomenon. Certainly stitching appears very explicitly in &lt;a href=&quot;https://arxiv.org/abs/2010.14500&quot;&gt;multi-stage robotic manipulation&lt;/a&gt; tasks and also in &lt;a href=&quot;https://arxiv.org/abs/2012.09812&quot;&gt;navigation tasks&lt;/a&gt;. However, stitching is not limited to just these domains — it turns out that the need for stitching implicitly appears even in tasks that do not appear to contain a maze. In practice, effective policies would often require finding an “extreme” but high-rewarding action, very different from an action that the behavior policy would prescribe, at &lt;em&gt;every&lt;/em&gt; state and learning to stitch such actions to obtain a policy that performs well overall. This form of &lt;em&gt;implicit&lt;/em&gt; stitching appears in many practical applications: for example, one might want to find an HVAC control policy that minimizes the carbon footprint of a building with a dataset collected from distinct control policies run historically in different buildings, each of which is suboptimal in one manner or the other. In this case, one can still get a much better policy by stitching extreme actions at every state. In general this implicit form of stitching is required in cases where we wish to find really good policies that maximize a continuous value (e.g., maximize rider comfort in autonomous driving;  maximize profits in automatic stock trading) using a dataset collected from a mixture of suboptimal policies (e.g., data from different human drivers; data from different human traders who excel and underperform under different situations) that never execute extreme actions at each decision. However, by stitching such extreme actions at each decision, one can obtain a much better policy. Therefore, naturally succeeding at many problems requires learning to either explicitly or implicitly stitch trajectories, segments or even single decisions, and offline RL is good at it.&lt;/p&gt;

&lt;p&gt;The next natural question to ask is: &lt;strong&gt;Can we resolve this issue by adding an RL-like component in BC methods?&lt;/strong&gt; One recently-studied approach is to perform a limited number of policy improvement steps beyond behavior cloning. That is, while full offline RL performs multiple rounds of policy improvement untill we find an optimal policy, one can just find a policy by running &lt;a href=&quot;https://arxiv.org/abs/2106.08909&quot;&gt;one step of policy improvement&lt;/a&gt; beyond behavioral cloning. This policy improvement is performed by incorporating some sort of a value function, and one might hope that utilizing some form of Bellman backup equips the method with the ability to “&lt;strong&gt;stitch&lt;/strong&gt;”. Unfortunately, even this approach is unable to fully close the gap against offline RL. This is because while the one-step approach can stitch trajectory segments, it would often end up stitching the wrong segments! One step of policy improvement only myopically improves the policy, without taking into account the impact of updating the policy on the future outcomes, the policy may fail to identify truly optimal behavior. For example, in our maze example shown below, it might appear better for the agent to find a solution that decides to go upwards and attain mediocre reward compared to going towards the goal, since under the behavior policy going downwards might appear highly suboptimal.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648609753495_Screenshot+2022-03-29+at+8.09.10+PM.png&quot; width=&quot;100%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 3: Imitation-style methods that only perform a limited steps of policy improvement may still fall prey to choosing suboptimal actions, because the optimal action assuming that the agent will follow the behavior policy in the future may actually not be optimal for the full sequential decision making problem.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;is-offline-rl-useful-when-stitching-is-not-a-primary-concern&quot;&gt;Is Offline RL Useful When Stitching is Not a Primary Concern?&lt;/h2&gt;

&lt;p&gt;So far, our analysis reveals that offline RL methods are better due to good “stitching” properties. But one might wonder, if stitching is critical when provided with good data, such as demonstration data in &lt;a href=&quot;https://arxiv.org/abs/2108.03298&quot;&gt;robotics&lt;/a&gt; or data from good policies in &lt;a href=&quot;https://arxiv.org/abs/1908.08796&quot;&gt;healthcare&lt;/a&gt;. However, in our &lt;a href=&quot;http://link here&quot;&gt;recent paper,&lt;/a&gt; we find that even when temporal compositionality is not a primary concern, offline RL does provide benefits over imitation learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Offline RL can teach the agent what to “not do”.&lt;/strong&gt; Perhaps one of the biggest benefits of offline RL algorithms is that running RL on noisy datasets generated from stochastic policies can not only teach the agent what it should do to maximize return, but also what shouldn’t be done and how actions at a given state would influence the chance of the agent ending up in undesirable scenarios in the future. In contrast, any form of conditional or weighted BC which only teach the policy “do X”, without explicitly discouraging particularly low-rewarding or unsafe behavior. This is especially relevant in open-world settings such as robotic manipulation in diverse settings or making decisions about patient admission in an ICU, where knowing what to not do very clearly is essential. In our &lt;a href=&quot;https://arxiv.org/abs/2204.05618&quot;&gt;paper&lt;/a&gt;, we quantify the gain of accurately inferring “what not to do and how much it hurts” and describe this intuition pictorially below. Often obtaining such noisy data is easy — one could augment expert demonstration data with additional “negatives” or “fake data” generated from a simulator (e.g., robotics, autonomous driving), or by first running an imitation learning method and creating a dataset for offline RL that augments data with evaluation rollouts from the imitation learned policy.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648634037765_Screenshot+2022-03-30+at+2.53.52+AM.png&quot; width=&quot;90%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 4: By leveraging noisy data, offline RL algorithms can learn to figure out what shouldn’t be done in order to explicitly avoid regions of low reward, and how the agent could be overly cautious much before that.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Is offline RL useful at all when I&lt;/strong&gt; &lt;strong&gt;actually&lt;/strong&gt; &lt;strong&gt;have near-expert demonstrations?&lt;/strong&gt;  As the final scenario, let’s consider the case where we actually have only near-expert demonstrations — perhaps, the perfect setting for imitation learning. In such a setting, there is no opportunity for stitching or leveraging noisy data to learn what not to do. Can offline RL still improve upon imitation learning? Unfortunately, one can show that, in the worst case, no algorithm can perform better than standard behavioral cloning. However, if the task admits some structure then offline RL policies can be more robust. For example, if there are multiple states where it is easy to identify a good action using reward information, offline RL approaches can quickly converge to a good action at such states, whereas a standard BC approach that does not utilize rewards may fail to identify a good action, leading to policies that are non-robust and fail to solve the task. Therefore, offline RL is a preferred option for tasks with an abundance of such “non-critical” states where long-term reward can easily identify a good action. An illustration of this idea is shown below, and we formally prove a theoretical result quantifying these intuitions in the &lt;a href=&quot;https://arxiv.org/abs/2204.05618&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648635140775_Screenshot+2022-03-30+at+3.12.16+AM.png&quot; width=&quot;90%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 5: An illustration of the idea of non-critical states: the abundance of states where reward information can easily identify good actions at a given state can help offline RL — even when provided with expert demonstrations —  compared to standard BC, that does not utilize any kind of reward information,&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;so-when-is-imitation-learning-useful&quot;&gt;So, When Is Imitation Learning Useful?&lt;/h2&gt;

&lt;p&gt;Our discussion has so far highlighted that offline RL methods can be robust and effective in many scenarios where conditional and weighted BC might fail. Therefore, we now seek to understand if conditional or weighted BC are useful in certain problem settings. This question is easy to answer in the context of standard behavioral cloning, if your data consists of expert demonstrations that you wish to mimic, standard behavioral cloning is a relatively simple, good choice.  However this approach fails when the data is noisy or suboptimal or when the task changes (e.g., when the distribution of initial states changes). And offline RL may still be preferred in settings with some structure (as we discussed above). Some failures of BC can be resolved by utilizing filtered BC — if the data consists of a mixture of good and bad trajectories, filtering trajectories based on return can be a good idea. Similarly, one could use one-step RL if the task does not require any form of stitching. However, in all of these cases, offline RL might be a better alternative especially if the task or the environment satisfies some conditions, and might be worth trying at least.&lt;/p&gt;

&lt;p&gt;Conditional BC performs well on a problem when one can obtain a conditioning variable well-suited to a given task. For example, empirical results on the antmaze domains from &lt;a href=&quot;https://arxiv.org/abs/2112.10751&quot;&gt;recent work&lt;/a&gt; indicate that conditional BC with a goal as a conditioning variable is quite effective in goal-reaching problems, however, conditioning on returns is not (compare Conditional BC (goals) vs Conditional BC (returns) in Table 1). Intuitively, this “well-suited” conditioning variable essentially enables stitching — for instance, a navigation problem naturally decomposes into a sequence of intermediate goal-reaching problems and then stitch solutions to a cleverly chosen subset of intermediate goal-reaching problems to solve the complete task. At its core, the success of conditional BC requires some domain knowledge about the compositionality structure in the task. On the other hand, offline RL methods extract the underlying stitching structure by running dynamic programming, and work well more generally. Technically, one could combine these ideas and utilize dynamic programming to learn a value function and then obtain a policy by running conditional BC with the value function as the conditioning variable, and this can work quite well (compare RCP-A to RCP-R &lt;a href=&quot;https://arxiv.org/abs/1912.13465&quot;&gt;here&lt;/a&gt;, where RCP-A uses a value function for conditioning; compare TT+Q and TT &lt;a href=&quot;https://arxiv.org/abs/2106.02039&quot;&gt;here&lt;/a&gt;)!&lt;/p&gt;

&lt;h1 id=&quot;empirical-results-comparing-offline-rl-and-bc&quot;&gt;Empirical Results Comparing Offline RL and BC&lt;/h1&gt;

&lt;p&gt;In our discussion so far, we have already studied settings such as the antmazes, where offline RL methods can significantly outperform imitation-style methods due to stitching. We will now quickly discuss some empirical results that compare the performance of offline RL and BC on tasks where we are provided with near-expert, demonstration data.&lt;/p&gt;

&lt;p style=&quot;text-align:center;float:right&quot;&gt;
&lt;img src=&quot;https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649381175454_image.png&quot; width=&quot;90%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;Figure 6: Comparing full offline RL (CQL) to imitation-style methods (One-step RL and BC) averaged over 7 Atari games, with expert demonstration data and noisy-expert data. Empirical details here.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;In our final experiment, we compare the performance of offline RL methods to imitation-style methods on an average over seven Atari games. We use &lt;a href=&quot;https://sites.google.com/view/cql-offline-rl&quot;&gt;conservative Q-learning&lt;/a&gt; (CQL) as our representative offline RL method. Note that naively running offline RL (“Naive CQL (Expert)”), without proper cross-validation to prevent overfitting and underfitting does not improve over BC. However, offline RL equipped with a reasonable cross-validation procedure (“Tuned CQL (Expert)”) is able to clearly improve over BC. This highlights the need for &lt;a href=&quot;https://arxiv.org/abs/2109.10813&quot;&gt;understanding how offline RL methods must be tuned&lt;/a&gt;, and at least, in part explains the poor performance of offline RL when learning from demonstration data in prior works. Incorporating a bit of noisy data that can inform the algorithm of what it shouldn’t do, further improves performance (“CQL (Noisy Expert)” vs “BC (Expert)”) within an identical data budget. Finally, note that while one would expect that while one step of policy improvement can be quite effective, we found that it is quite sensitive to hyperparameters and fails to improve over BC significantly. These observations validate the findings discussed earlier in the blog post. We discuss results on other domains in our &lt;a href=&quot;https://arxiv.org/abs/2204.05618&quot;&gt;paper&lt;/a&gt;, that we encourage practitioners to check out.&lt;/p&gt;

&lt;h1 id=&quot;discussion-and-takeaways&quot;&gt;Discussion and Takeaways&lt;/h1&gt;

&lt;p&gt;In this blog post, we aimed to understand if, when and why offline RL is a better approach for tackling a variety of sequential decision-making problems. Our discussion suggests that offline RL methods that learn value functions can leverage the benefits of stitching, which can be crucial in many problems. Moreover, there are even scenarios with expert or near-expert demonstration data, where running offline RL is a good idea. We summarize our recommendations for practitioners in Figure 1, shown right at the beginning of this blog post. We hope that our analysis improves the understanding of the benefits and properties of offline RL approaches.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This blog post is primarily based on the paper:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When Should Offline RL Be Preferred Over Behavioral Cloning?&lt;/strong&gt;&lt;br /&gt;
Aviral Kumar*, Joey Hong*, Anikait Singh, Sergey Levine       [&lt;a href=&quot;https://arxiv.org/abs/2204.05618&quot;&gt;arxiv&lt;/a&gt;].&lt;br /&gt;
In International Conference on Learning Representations (ICLR), 2022.&lt;/p&gt;

&lt;p&gt;In addition, the empirical results discussed in the blog post are taken from various papers, in particular from &lt;a href=&quot;https://arxiv.org/abs/2112.10751&quot;&gt;RvS&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2110.06169&quot;&gt;IQL&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Apr 2022 05:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/</guid>
      </item>
     
    
     
      <item>
        <title>Offline RL Made Easier: No TD Learning, Advantage Reweighting, or Transformers</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Offline RL Made Easier: No TD Learning, Advantage Reweighting, or Transformers&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/rvs/rvs-overview.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;offline reinforcement learning, supervised learning, deep learning, transformer&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;The BAIR Blog&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Scott Emmons, Ben Eysenbach, Sergey Levine&quot; /&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/rvs-kitchen.gif&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;A demonstration of the RvS policy we learn with just supervised learning and a depth-two MLP. It uses no TD learning, advantage reweighting, or Transformers!&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Offline reinforcement learning (RL) is conventionally approached using value-based methods based on temporal difference (TD) learning. However, many recent algorithms reframe RL as a supervised learning problem. These algorithms learn &lt;em&gt;conditional policies&lt;/em&gt; by conditioning on goal states (Lynch &lt;em&gt;et al.&lt;/em&gt;, 2019; Ghosh &lt;em&gt;et al.&lt;/em&gt;, 2021), reward-to-go (Kumar &lt;em&gt;et al.&lt;/em&gt;, 2019; Chen &lt;em&gt;et al.&lt;/em&gt;, 2021), or language descriptions of the task (Lynch and Sermanet, 2021).&lt;/p&gt;

&lt;p&gt;We find the simplicity of these methods quite appealing. If supervised learning is enough to solve RL problems, then offline RL could become widely accessible and (relatively) easy to implement. Whereas TD learning must delicately balance an actor policy with an ensemble of critics, these supervised learning methods train just one (conditional) policy, and nothing else!&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;So, how can we use these methods to effectively solve offline RL problems? Prior work puts forward a number of clever tips and tricks, but these tricks are sometimes contradictory, making it challenging for practitioners to figure out how to successfully apply these methods. For example, RCPs (Kumar &lt;em&gt;et al.&lt;/em&gt;, 2019) require carefully reweighting the training data, GCSL (Ghosh &lt;em&gt;et al.&lt;/em&gt;, 2021) requires iterative, online data collection, and Decision Transformer (Chen &lt;em&gt;et al.&lt;/em&gt;, 2021) uses a Transformer sequence model as the policy network.&lt;/p&gt;

&lt;p&gt;Which, if any, of these hypotheses are correct? Do we need to reweight our training data based on estimated advantages? Are Transformers necessary to get a high-performing policy? Are there other critical design decisions that have been left out of prior work?&lt;/p&gt;

&lt;p&gt;Our work aims to answer these questions by trying to identify the &lt;em&gt;essential elements&lt;/em&gt; of offline RL via supervised learning. We run experiments across 4 suites, 26 environments, and 8 algorithms. When the dust settles, we get competitive performance in every environment suite we consider using remarkably simple elements. The video above shows the complex behavior we learn using just supervised learning with a depth-two MLP – no TD learning, data reweighting, or Transformers!&lt;/p&gt;

&lt;h1 id=&quot;rl-via-supervised-learning&quot;&gt;RL via Supervised Learning&lt;/h1&gt;

&lt;p&gt;Let’s begin with an overview of the algorithm we study. While lots of prior work (Kumar &lt;em&gt;et al.&lt;/em&gt;, 2019; Ghosh &lt;em&gt;et al.&lt;/em&gt;, 2021; and Chen &lt;em&gt;et al.&lt;/em&gt;, 2021) share the same core algorithm, it lacks a common name. To fill this gap, we propose the term &lt;em&gt;RL via Supervised Learning (RvS)&lt;/em&gt;. We are not proposing any new algorithm but rather showing how prior work can be viewed from a unifying framework; see Figure 1.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/rvs-data.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Figure 1.&lt;/b&gt; (Left) A replay buffer of experience (Right) Hindsight relabelled training data&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;RL via Supervised Learning takes as input a replay buffer of experience including states, actions, and outcomes. The outcomes can be an arbitrary function of the trajectory, including a goal state, reward-to-go, or language description. Then, RvS performs hindsight relabeling to generate a dataset of state, action, and outcome triplets. The intuition is that the actions that are observed provide supervision for the outcomes that are reached. With this training dataset, RvS performs supervised learning by maximizing the likelihood of the actions given the states and outcomes. This yields a conditional policy that can condition on arbitrary outcomes at test time.&lt;/p&gt;

&lt;h1 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h1&gt;

&lt;p&gt;In our experiments, we focus on the following three key questions.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Which design decisions are critical for RL via supervised learning?&lt;/li&gt;
  &lt;li&gt;How well does RL via supervised learning actually work? We can do RL via supervised learning, but would using a different offline RL algorithm perform better?&lt;/li&gt;
  &lt;li&gt;What type of outcome variable should we condition on? (And does it even matter?)&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h1&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/rvs-architecture.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Our RvS architecture. A depth-two MLP suffices in every environment suite we consider.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;We get good performance using just a depth-two multi-layer perceptron. In fact, this is competitive with all previously published architectures we’re aware of, including a Transformer sequence model. We just concatenate the state and outcome before passing them through two fully-connected layers (see Figure 2). The keys that we identify are having a network with large capacity – we use width 1024 – as well as dropout in some environments. We find that this works well without reweighting the training data or performing any additional regularization.&lt;/p&gt;

&lt;h1 id=&quot;overall-performance&quot;&gt;Overall Performance&lt;/h1&gt;

&lt;p&gt;After identifying these key design decisions, we study the overall performance of RvS in comparison to previous methods. This blog post will overview results from two of the suites we consider in the paper.&lt;/p&gt;

&lt;h1 id=&quot;d4rl-gym&quot;&gt;D4RL Gym&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/gym-env.png&quot; width=&quot;40%&quot; hspace=&quot;20&quot; align=&quot;right&quot; /&gt;
The first suite is D4RL Gym, which contains the standard MuJoCo halfcheetah, hopper, and walker robots. The challenge in D4RL Gym is to learn locomotion policies from offline datasets of varying quality. For example, one offline dataset contains rollouts from a totally random policy. Another dataset contains rollouts from a “medium” policy trained partway to convergence, while another dataset is a mixture of rollouts from medium and expert policies.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/gym-performance.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Figure 3.&lt;/b&gt; Overall performance in D4RL Gym.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Figure 3 shows our results in D4RL Gym. RvS-R is our implementation of RvS conditioned on rewards (illustrated in Figure 2). On average across all 12 tasks in the suite, we see that RvS-R, which uses just a depth-two MLP, is competitive with Decision Transformer (DT; Chen &lt;em&gt;et al.&lt;/em&gt;, 2021). We also see that RvS-R is competitive with the methods that use temporal difference (TD) learning, including CQL-R (Kumar &lt;em&gt;et al.&lt;/em&gt;, 2020), TD3+BC (Fujimoto &lt;em&gt;et al.&lt;/em&gt;, 2021), and Onestep (Brandfonbrener &lt;em&gt;et al.&lt;/em&gt;, 2021). However, the TD learning methods have an edge because they perform especially well on the random datasets. This suggests that one might prefer TD learning over RvS when dealing with low-quality data.&lt;/p&gt;

&lt;h1 id=&quot;d4rl-antmaze&quot;&gt;D4RL AntMaze&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/antmaze-env.png&quot; width=&quot;27%&quot; hspace=&quot;20&quot; align=&quot;right&quot; /&gt;
The second suite is D4RL AntMaze. This suite requires a quadruped to navigate to a target location in mazes of varying size. The challenge of AntMaze is that many trajectories contain only pieces of the full path from the start to the goal location. Learning from these trajectories requires stitching together these pieces to get the full, successful path.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
&lt;img src=&quot;https://bair.berkeley.edu/static/blog/rvs/antmaze-performance.png&quot; width=&quot;70%&quot; /&gt;
&lt;br /&gt;
&lt;i&gt;&lt;b&gt;Figure 4.&lt;/b&gt; Overall performance in D4RL AntMaze.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Our AntMaze results in Figure 4 highlight the importance of the conditioning variable. Whereas conditioning RvS on rewards (RvS-R) was the best choice of the conditioning variable in D4RL Gym, we find that in D4RL AntMaze, it is much better to condition RvS on $(x, y)$ goal coordinates (RvS-G). When we do this, we see that RvS-G compares favorably to TD learning! This was surprising to us because TD learning explicitly performs dynamic programming using the Bellman equation.&lt;/p&gt;

&lt;p&gt;Why does goal-conditioning perform better than reward conditioning in this setting? Recall that AntMaze is designed so that simple imitation is not enough: optimal methods must stitch together parts of suboptimal trajectories to figure out how to reach the goal. In principle, TD learning can solve this with &lt;em&gt;temporal&lt;/em&gt; compositionality. With the Bellman equation, TD learning can combine a path from A to B with a path from B to C, yielding a path from A to C. RvS-R, along with other behavior cloning methods, does not benefit from this temporal compositionality. We hypothesize that RvS-G, on the other hand, benefits from &lt;em&gt;spatial compositionality&lt;/em&gt;. This is because, in AntMaze, the policy needed to reach one goal is similar to the policy needed to reach a nearby goal. We see correspondingly that RvS-G beats RvS-R.&lt;/p&gt;

&lt;p&gt;Of course, conditioning RvS-G on $(x, y)$ coordinates represents a form of prior knowledge about the task. But this also highlights an important consideration for RvS methods: the choice of conditioning information is critically important, and it may depend significantly on the task.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Overall, we find that in a diverse set of environments, RvS works well without needing any fancy algorithmic tricks (such as data reweighting) or fancy architectures (such as Transformers). Indeed, our simple RvS setup can match, and even outperform, methods that utilize (conservative) TD learning. The keys for RvS that we identify are model capacity, regularization, and the conditioning variable.&lt;/p&gt;

&lt;p&gt;In our work, we handcraft the conditioning variable, such as $(x, y)$ coordinates in AntMaze. Beyond the standard offline RL setup, this introduces an additional assumption, namely, that we have some prior information about the structure of the task. We think an exciting direction for future work would be to remove this assumption by automating the learning of the goal space.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;reproducing-experiments&quot;&gt;Reproducing Experiments&lt;/h1&gt;

&lt;p&gt;We packaged our &lt;a href=&quot;https://github.com/scottemmons/rvs&quot;&gt;open-source code&lt;/a&gt; so that it can automatically handle all the dependencies for you. After downloading the code, you can run these five commands to reproduce our experiments:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t rvs:latest .
docker run -it --rm -v $(pwd):/rvs rvs:latest bash
cd rvs
pip install -e .
bash experiments/launch_gym_rvs_r.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;This post is based on the paper:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.10751&quot;&gt;RvS: What is Essential for Offline RL via Supervised Learning?&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;http://scottemmons.com/&quot;&gt;Scott Emmons&lt;/a&gt;, &lt;a href=&quot;https://ben-eysenbach.github.io/&quot;&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href=&quot;https://www.kostrikov.xyz/&quot;&gt;Ilya Kostrikov&lt;/a&gt;, &lt;a href=&quot;https://people.eecs.berkeley.edu/~svlevine/&quot;&gt;Sergey Levine&lt;/a&gt;&lt;br /&gt;
International Conference on Learning Representations (ICLR), 2022&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2112.10751&quot;&gt;[Paper]&lt;/a&gt; &lt;a href=&quot;https://github.com/scottemmons/rvs&quot;&gt;[Code]&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Apr 2022 02:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/04/20/rvs/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/04/20/rvs/</guid>
      </item>
     
    
     
      <item>
        <title>Accelerating Ukraine Intelligence Analysis with Computer Vision on Synthetic Aperture Radar Imagery</title>
        <description>&lt;!-- twitter --&gt;
&lt;meta name=&quot;twitter:title&quot; content=&quot;Accelerating Ukraine Intelligence Analysis with CV on SAR Imagery&quot; /&gt;

&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;

&lt;meta name=&quot;twitter:image&quot; content=&quot;https://bair.berkeley.edu/static/blog/maers/maers.png&quot; /&gt;

&lt;meta name=&quot;keywords&quot; content=&quot;ukraine, russia, synthetic aperture radar, sar, semantic segmentation, geoint&quot; /&gt;

&lt;meta name=&quot;description&quot; content=&quot;Creating efficient EO and SAR machine learning models to aid imagery analysis for Ukraine.&quot; /&gt;

&lt;meta name=&quot;author&quot; content=&quot;Ritwik Gupta, Colorado Reed, Anna Rohrbach, Trevor Darrell&quot; /&gt;

&lt;!-- body --&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/ukraine-clouds-optim.gif&quot; width=&quot;120%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 1:&lt;/b&gt; Airmass measurements (clouds) over Ukraine from February 18, 2022 - March 01, 2022 from the SEVIRI instrument. Data accessed via the &lt;a href=&quot;https://view.eumetsat.int/productviewer?v=default&quot;&gt;EUMETSAT Viewer&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Satellite imagery is a critical source of information during the current invasion of Ukraine. Military strategists, journalists, and researchers use this imagery to make decisions, unveil violations of international agreements, and inform the public of the stark realities of war. With Ukraine experiencing a large amount of cloud cover and attacks often occuring during night-time, many forms of satellite imagery are hindered from seeing the ground. &lt;a href=&quot;https://earthdata.nasa.gov/learn/backgrounders/what-is-sar&quot;&gt;Synthetic Aperture Radar (SAR)&lt;/a&gt; imagery penetrates cloud cover, but requires special training to interpret. Automating this tedious task would enable real-time insights, but current computer vision methods developed on typical RGB imagery do not properly account for the phenomenology of SAR. This leads to suboptimal performance on this critical modality. Improving the access to and availability of SAR-specific methods, codebases, datasets, and pretrained models will benefit intelligence agencies, researchers, and journalists alike during this critical time for Ukraine.&lt;/p&gt;

&lt;p&gt;In this post, we present a baseline method and pretrained models that enable the interchangeable use of RGB and SAR for downstream classification, semantic segmentation, and change detection pipelines.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We live in a rapidly changing world, one that experiences natural disasters, civic upheaval, war, and all sorts of chaotic events which leave unpredictable—and often permanent—marks on the face of the planet. Understanding this change has historically been difficult. Surveyors were sent out to explore our new reality, and their distributed findings were often noisily integrated into a source of reality. Maintaining a constant state of vigilance has been a goal of mankind since we were able to conceive such a thought, all the way from when &lt;a href=&quot;https://time.com/longform/aerial-photography-drones-history/&quot;&gt;Nadar took the first aerial photograph&lt;/a&gt; to when &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0273117715001623&quot;&gt;Sputnik 1’s radio signals were used to analyze the ionosphere&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Vigilance, or to the French, &lt;em&gt;surveillance&lt;/em&gt;, has been a part of human history for millenia. As with any tool, it has been a double-edged sword. Historically, surveillance without checks and balances has been detrimental to society. Conversely, the proper and responsible surveillance has allowed us to learn deep truths about our world which have resulted in advances in the &lt;a href=&quot;https://www.nasa.gov/mission_pages/icebridge/instruments/index.html&quot;&gt;scientific&lt;/a&gt; and &lt;a href=&quot;https://web.archive.org/web/20211001071654/https://news.un.org/en/story/2006/04/176152-un-launches-new-enhanced-tool-use-satellite-data-fighting-hunger-poverty&quot;&gt;humanitarian&lt;/a&gt; domains. With the amount of satellites in orbit today, our understanding of the environment is updated almost daily. We have rapidly transitioned from having very little information to now having more data than we can meaningfully extract knowledge from. Storing this information, let alone understanding, is an engineering challenge that is of growing urgency.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-and-remote-sensing&quot;&gt;Machine Learning and Remote Sensing&lt;/h2&gt;

&lt;p&gt;With &lt;a href=&quot;https://datacenterfrontier.com/terabytes-from-space-satellite-imaging-is-filling-data-centers/&quot;&gt;hundreds of terabytes&lt;/a&gt; of data being downlinked from satellites to data centers every day, gaining knowledge and actionable insights from that data with manual processing has already become an impossible task. The most widely used form of remote sensing data is electro-optical (EO) satellite imagery. EO imagery is commonplace—anyone who has used Google Maps or similar mapping software has interacted with EO satellite imagery.&lt;/p&gt;

&lt;p&gt;Machine learning (ML) on EO imagery is used in a wide variety of scientific and commercial applications. From &lt;a href=&quot;https://journals.ametsoc.org/view/journals/hydr/17/3/jhm-d-15-0075_1.xml&quot;&gt;improving precipitation predictions&lt;/a&gt;, &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0924271618300479&quot;&gt;analyzing human slavery by identifying brick kilns&lt;/a&gt;, to &lt;a href=&quot;https://blog.google/products/maps/google-maps-101-ai-power-new-features-io-2021/&quot;&gt;classifying entire cities to improve traffic routing&lt;/a&gt;, the outputs of ML on EO imagery have been integrated into almost every facet of human society.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/bridge-eo-kyiv.jpeg&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;a href=&quot;https://www.cnn.com/europe/live-news/ukraine-russia-putin-news-03-03-22/h_ed1c79ce964585a1d044c2dd50e2997a&quot;&gt;&lt;b&gt;Figure 2:&lt;/b&gt; VHR EO imagery over the Kyiv region as acquired by Maxar on February 28, 2022&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Commonly used satellite constellations for EO imagery include the &lt;a href=&quot;https://landsat.gsfc.nasa.gov/&quot;&gt;Landsat&lt;/a&gt; series of satellites operated by the United States Geological Survey and the &lt;a href=&quot;https://sentinel.esa.int/web/sentinel/missions/sentinel-2&quot;&gt;Copernicus Sentinel-2&lt;/a&gt; constellation operated by the European Space Agency. These constellations provide imagery at resolutions between 10-60 meters which is good enough for many use cases, but preclude the observation of finer details.&lt;/p&gt;

&lt;h2 id=&quot;the-advent-of-very-high-resolution-commercial-electro-optical-satellite-imagery&quot;&gt;The Advent of Very High Resolution, Commercial Electro-Optical Satellite Imagery&lt;/h2&gt;

&lt;p&gt;Over the last few years, very high resolution (VHR) EO imagery has been made available through a variety of commercial sources. Ranging from between 0.3 - 2.0 meter resolution&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, companies such as &lt;a href=&quot;https://www.planet.com/&quot;&gt;Planet&lt;/a&gt;, &lt;a href=&quot;https://www.maxar.com/&quot;&gt;Maxar&lt;/a&gt;, &lt;a href=&quot;https://www.airbus.com/en/products-services/space/earth-observation&quot;&gt;Airbus&lt;/a&gt;, and others are providing extremely precise imagery with high revisit rates, &lt;a href=&quot;https://www.fastcompany.com/40498033/every-day-this-satellite-company-takes-a-snapshot-of-the-entire-planet&quot;&gt;imaging the entire planet every day&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maxar-ships.jpeg&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;a href=&quot;https://blog.maxar.com/earth-intelligence/2022/enhancing-maritime-domain-awareness-with-maxars-crows-nest-solution&quot;&gt;&lt;b&gt;Figure 3:&lt;/b&gt; An example of Maxar VHR EO imagery showing floating production, storage and off-loading units and a tanker&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;The increased resolution provided by VHR imagery enables a litany of downstream use cases. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/full/10.1002/ldr.1094&quot;&gt;Erosion can be detected at finer scales&lt;/a&gt;, and the &lt;a href=&quot;https://xview2.org/&quot;&gt;building damage can be classified after natural disasters&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Machine learning methods have had to adapt in response to VHR satellite imagery. With an increased acuity, the amount of pixels and the &lt;a href=&quot;http://xviewdataset.org/&quot;&gt;amount of classes that can be discerned&lt;/a&gt; has increased by orders of magnitude. Computer vision research has responded by &lt;a href=&quot;https://www.nature.com/articles/s41467-021-24638-z&quot;&gt;reducing the computational cost to learn efficient representation of satellite imagery&lt;/a&gt;, creating &lt;a href=&quot;https://arxiv.org/abs/2108.09186&quot;&gt;methods to alleviate the increased burden on labelers&lt;/a&gt;, and even &lt;a href=&quot;https://arxiv.org/abs/2111.08872&quot;&gt;engineering large software frameworks&lt;/a&gt; to allow computer vision practitioners to handle this abundant source of imagery.&lt;/p&gt;

&lt;p&gt;In general, existing computer vision methods on other, non-aerial RGB imagery &lt;a href=&quot;https://arxiv.org/abs/1510.00098&quot;&gt;transfer very well&lt;/a&gt; to satellite imagery. This has allowed commercial VHR imagery to be immediately useful with highly accurate results.&lt;/p&gt;

&lt;h2 id=&quot;the-problem-with-electro-optical-imagery&quot;&gt;The Problem with Electro-Optical Imagery&lt;/h2&gt;

&lt;p&gt;For highly turbulent and risky situations such as war and natural disasters, having constant, reliable access to the Earth is paramount.  Unfortunately, EO imagery cannot solve all of our surveillance needs. EO can only detect light sources during daytime, and as it turns out, &lt;a href=&quot;https://earthobservatory.nasa.gov/images/85843/cloudy-earth&quot;&gt;nearly 2/3rds of the Earth is covered by clouds at any given time&lt;/a&gt;. Unless you care about clouds, this blockage of the surface of the planet is problematic when understanding what happens on the ground is of critical importance. Machine learning methods attempt to sidestep this problem by &lt;a href=&quot;https://hal-enpc.archives-ouvertes.fr/hal-01832797/document&quot;&gt;predicting what the world would look like without clouds&lt;/a&gt;. However, the loss of information is fundamentally irrecoverable.&lt;/p&gt;

&lt;h2 id=&quot;synthetic-aperture-radar-imagery&quot;&gt;Synthetic Aperture Radar Imagery&lt;/h2&gt;

&lt;p&gt;Synthetic aperture radar (SAR) imagery is an active form of remote sensing in which a satellite transmits pulses of microwave radar waves down to the surface of the Earth. These radar waves reflect off the ground and any objects on it and are returned back to the satellite. By processing these pulses over time and space, a SAR image is formed where each pixel is the superposition of different radar scatters.&lt;/p&gt;

&lt;p&gt;Radar waves penetrate clouds, and since the satellite is actively producing the radar waves, it illuminates the surface of the Earth even during the night. Synthetic aperture radar has a wide variety of uses, being used to &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/134087&quot;&gt;estimate the roughness of the Earth&lt;/a&gt;, &lt;a href=&quot;https://unitar.org/about/news-stories/news/unosat-introduces-ai-its-flood-rapid-mapping-operations-benefit-national-disaster-management&quot;&gt;mapping the extent of flooding over large areas&lt;/a&gt;, and to &lt;a href=&quot;https://iuu.xview.us/&quot;&gt;detect the presence of illegal fishing vessels in protected waters&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are multiple SAR satellite constellations in operation at the moment. The &lt;a href=&quot;https://sentinel.esa.int/web/sentinel/missions/sentinel-1&quot;&gt;Copernicus Sentinel-1&lt;/a&gt; constellation provides imagery to the public at large with resolutions ranging from 10 - 80 meters (10 meter imagery being the most common. Most commercial SAR providers, such as &lt;a href=&quot;https://www.iceye.com/&quot;&gt;ICEYE&lt;/a&gt; and &lt;a href=&quot;https://www.capellaspace.com/&quot;&gt;Capella Space&lt;/a&gt;, provide imagery down to 0.5 meter resolution. In upcoming launches, other commercial vendors aim to produce SAR imagery with sub-0.5 meter resolution with high revisit rates as satellite constellations grow and government regulations evolve.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/sar-ukraine-belarus.jpg&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;a href=&quot;https://www.wired.co.uk/article/ukraine-russia-satellites&quot;&gt;&lt;b&gt;Figure 4:&lt;/b&gt; A VHR SAR image provided by Capella Space over the Ukraine-Belarus border&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;the-wacky-world-of-synthetic-aperture-radar-imagery&quot;&gt;The Wacky World of Synthetic Aperture Radar Imagery&lt;/h2&gt;

&lt;p&gt;While SAR imagery, at a quick glance, may look very similar to EO imagery, the underlying physics is quite different, which leads to many interesting effects in the imagery product which can be counterintuitive and incompatible with modern computer vision. Three common effects are termed polarization, layover, and multi-path effects.&lt;/p&gt;

&lt;p&gt;Radar antennas on SAR satellites often transmit polarized radar waves. The direction of polarization is the orientation of the wave’s electric field. Objects on the ground exhibit different responses to the different polarizations of radar waves. Therefore, SAR satellites often operate in dual or quad-polarization modes, broadcasting horizontally (H) or vertically (V) polarized waves and reading either polarization back, resulting in HH, HV, VH, and VV bands. You can contrast this with RGB bands in EO imagery, but the fundamental physics are different.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/sentinel1-vv-vh.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 5:&lt;/b&gt; Difference between VH (left) and VV (right) polarizations over the same region in Dnipro, Ukraine from Sentinel-1 radiometric terrain corrected imagery. As seen here, the radar returns in corresponding local regions can be different.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Layover is an effect in which radar beams reach the top of a structure before they reach the bottom, resulting in the top of the object being presented as overlapping with the bottom. This happens when objects are particularly tall. Visually, tall buildings appear as if they are laying on their side, while mountains will have their peaks intersecting with their bases.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/capella-layover.jpeg&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;a href=&quot;https://twitter.com/capellaspace/status/1367865023587049474/photo/1&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Example of layover in Capella’s VHR SAR imagery.&lt;/a&gt; The upper portion of the stadium is intersecting, seemingly, with the parking lot behind it.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Multi-path effects occur when radar waves reflect off of objects on the ground and incur multiple bounces before returning to the SAR sensor. Multi-path effects result in objects appearing in the imagery in various transformations in the resulting image. This effect can be seen everywhere in SAR imagery, but is particularly noticeable in urban areas, forests, and other dense environments.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/multipath.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;a href=&quot;https://discovery.ucl.ac.uk/id/eprint/10053908/&quot;&gt;&lt;b&gt;Figure 7:&lt;/b&gt; Example of a multi-path effect on a bridge from oblique SAR imagery&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Existing computer vision methods that are built on traditional RGB imagery are not built with these effects in mind. Object detectors trained on EO satellite imagery assume that a unique object will only appear once, or that the object will appear relatively similar in different contexts, rather than potentially mirrored or scattered or interwoven with surrounding objects. The very nature of occlusion and the vision principles underlying the assumptions of occlusion in EO imagery do not transfer to SAR. Taken together, existing computer vision techniques can transfer to SAR imagery, but with reduced performance and a set of systematic errors that can be addressed through SAR-specific methodology.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computer Vision on SAR Imagery for Ukraine&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Imagery analysts are currently relying on both EO and SAR imagery where available over Ukraine. When EO imagery is available, existing computer vision tooling built for that modality is used to expedite the process of intelligence gathering. However, when only SAR imagery is available, these toolchains cannot be used. Imagery analysts have to resort to manual analysis which is time consuming and can be prone to mistakes. This topic is being explored by some other institutions internationally, however, it still remains an understudied area with respect to the amount of data available.&lt;/p&gt;

&lt;p&gt;At Berkeley AI Research, we have created an initial set of methods and models that have learned robust representations for RGB, SAR, and co-registered RGB + SAR imagery from the publicly released &lt;a href=&quot;https://bigearth.net&quot;&gt;BigEarthNet-MM dataset&lt;/a&gt; and the data from &lt;a href=&quot;https://www.capellaspace.com/community/capella-open-data/&quot;&gt;Capella’s Open Data&lt;/a&gt;, which consists of both RGB and SAR imagery. As such, using our models, imagery analysts are able to interchangeably use RGB, SAR, or co-registered RGB+SAR imagery for downstream tasks such as image classification, semantic segmentation, object detection, or change detection.&lt;/p&gt;

&lt;p&gt;Given that SAR is a phenomenologically different data source than EO imagery, we have found that the Vision Transformer (ViT) is a particularly effective architecture for representation learning with SAR as it removes the scale and shift invariant inductive biases built into convolutional neural networks. Our top performing method, MAERS, for representation learning on RGB, SAR, and co-registered RGB + SAR builds upon the &lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;Masked Autoencoder&lt;/a&gt; (MAE) recently introduced by He et. al., where the network learns to encode the input data by taking a masked version of the data as input, encoding the data, and then learning to decode the data in such a way that it reconstructs the unmasked input data.&lt;/p&gt;

&lt;p&gt;Contrary to popular &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;classes of contrastive learning techniques&lt;/a&gt;, the MAE does not presuppose certain augmentation invariances in the data that may be incorrect for SAR features. Instead, it solely relies on reconstructing the original input, which is agnostic to RGB, SAR, or co-registered modalities. As shown in Figure 8, MAERS further extends MAE by learning independent input projection layers for RGB, SAR, and RGB+SAR channels, encoding the output of these projected layers using a shared ViT, and then decoding to the RGB, SAR, or RGB+SAR channels using independent output projection layers. The input projection layers and shared ViT can then be transferred to downstream tasks, such as object detection or change detection, where the input encoder can then take RGB, SAR, or RGB+SAR as input.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maers.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 8:&lt;/b&gt; (top) A visualization of MAERS to learn a joint representation and encoder that can be used for a (bottom) downstream task, such as object detection on either, or both, modalities.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;Learning representations for RGB, SAR, and co-registered modalities can benefit a range of downstream tasks, such as content-based image retrieval, classification, segmentation, and detection. To demonstrate the effectiveness of our learned representations, we perform experiments on the well-established benchmarks of 1) multi-label classification of co-registered EO and SAR scenes from the &lt;a href=&quot;https://bigearth.net/&quot;&gt;BigEarthNet-MM dataset&lt;/a&gt;, and 2) semantic segmentation on the VHR EO and SAR &lt;a href=&quot;https://spacenet.ai/sn6-challenge/&quot;&gt;SpaceNet 6 dataset&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;multi-label-classification-on-bigearth-mm&quot;&gt;Multi-Label Classification on BigEarth-MM&lt;/h2&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maers-benmm.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 9:&lt;/b&gt; (left) co-registered Sentinel-2 EO and Sentinel-1 SAR imagery are patchified and used to perform a multi-label classification task as specified by the BigEarth-MM challenge. A linear layer is added to our multi-modal encoder and then fine-tuned end-to-end.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;MAERS is initialized with a set of ImageNet weights for a ViT-Base encoder, followed by pretraining on the BigEarthNet-MM dataset for 20 epochs with RGB, SAR, and RGB+SAR imagery. We append a single linear layer to the MAERS encoder and learn the multi-label classification task by fine-tuning the entire model for 20 epochs (linear probing experiments obtain similar results, as we will show in our upcoming paper). Our results are shown in Table 1. MAERS with fine-tuning outperforms the best RGB+SAR results as presented in the BigEarthNet-MM paper, and show that adapting the State-of-the-Art MAE architecture for representation learning for RGB, SAR, and RGB+SAR input modalities leads to State-of-the-Art results.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maers-table.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Table 1:&lt;/b&gt; Reported per-class F2 scores on the test set of BigEarthNet-MM.&lt;/i&gt;
&lt;/p&gt;

&lt;h2 id=&quot;semantic-segmentation-on-vhr-eo-and-sar-spacenet-6&quot;&gt;Semantic Segmentation on VHR EO and SAR SpaceNet 6&lt;/h2&gt;

&lt;p&gt;We further experimented with transfer learning for a timely task that will aid imagery analysts aiming to understand the destruction in Ukraine: semantic segmentation of buildings footprints, which is a precursor task to performing building damage assessment. Building damage assessment is of direct interest to government officials, journalists, and human rights organizations aiming to understand the scope and severity of Russia’s attacks against infrastructure and civilian populations.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maers-vhr-sar-segmentation.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 10:&lt;/b&gt; Example of building SAR-based MAERS segmentation taken from SpaceNet6, where the image on the left shows the RGB image, and the image on the right shows the SAR image with overlaid segmentation results. The SAR image is displayed in false color with VV, VH, and VV/VH bands.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;For this experiment, we used the SpaceNet 6 dataset as an open and public benchmark to illustrate the effectiveness of our learned representations for building footprint detection with VHR SAR from Capella Space. We used this encoder in tandem with the &lt;a href=&quot;https://arxiv.org/abs/1807.10221&quot;&gt;UperNet&lt;/a&gt; architecture for semantic segmentation. Figure 11 shows the IoU performance of segmenting building footprints in a held-out validation component of the SpaceNet 6 with &lt;strong&gt;only SAR input imagery&lt;/strong&gt;, on a segmentation model that was trained to use either SAR or RGB imagery. The MAERS pretrained model leads to a ~13 point improvement compared to training the RGB+SAR model from scratch or adapting ImageNet weights with the exact same architecture.&lt;/p&gt;

&lt;p&gt;This demonstrates that MAERS can learn robust RGB+SAR representations that allow a practitioner to use EO or SAR imagery interchangeably to accomplish downstream tasks. It is important to note that the phenomenology of SAR imagery is not fully conducive for building segmentation and that using EO imagery for this task leads to IoU scores &amp;gt; 90. This leaves a substantial gap yet to be covered by SAR techniques, something we hope to cover in our following paper. However, getting this performance out of SAR is essential when environmental conditions are not conducive to EO imagery capture.&lt;/p&gt;

&lt;p style=&quot;text-align:center;&quot;&gt;
    &lt;img src=&quot;https://bair.berkeley.edu/static/blog/maers/maers-segmentation-iou.png&quot; width=&quot;100%&quot; /&gt;
    &lt;br /&gt;
    &lt;i&gt;&lt;b&gt;Figure 11:&lt;/b&gt; Building segmentation IoU on the SpaceNet 6 Challenge, using an UperNet segmentation model with a ViT backbone. MAERS pretraining leads to ~13 point gain in IoU performance compared to training from scratch or adapting ImageNet pretrained weights.&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;These results are preliminary, but compelling. We will follow up this effort with a publication with a detailed set of experiments and benchmarks. Furthermore, we will aid in the transition of our models to our humanitarian partners to enable them to perform change detection over residential and other civilian areas to enable better tracking of war crimes being committed in Ukraine.&lt;/p&gt;

&lt;p&gt;These models are created with the goal of increasing the efficacy of organizations involved in humanitarian missions that are keeping a watchful eye on the war in Ukraine. However, as with any technology, it is our responsibility to understand how this technology could be misused. Therefore, we have designed these models with input from partners who perform intelligence and imagery analysis in humanitarian settings. By taking into account their thoughts, comments, and critiques, we are releasing a capability we are confident will be used for the good of humanity and with processes which dictate their safe and responsible use.&lt;/p&gt;

&lt;h2 id=&quot;call-to-action&quot;&gt;Call to Action&lt;/h2&gt;

&lt;p&gt;As citizens of free democracies who develop technologies which help us make sense of the complicated, chaotic, and counter-intuitive world that we live in, we have a responsibility to act when acts of injustice occur. Our colleagues and friends in Ukraine are facing extreme uncertainties and danger. We possess skills in the cyber domain that can aid in the fight against Russian forces. By focusing our time and efforts, whether that be through targeted research or volunteering our time in &lt;a href=&quot;https://ukrainenow.org/&quot;&gt;helping keep track of processing times at border crossings&lt;/a&gt;, we can make a small dent in an otherwise difficult situation.&lt;/p&gt;

&lt;p&gt;We urge our fellow computer scientists to partner with government and humanitarian organizations and listen to their needs as difficult times persist. Simple things can make large differences.&lt;/p&gt;

&lt;h2 id=&quot;model-and-weights&quot;&gt;Model and Weights&lt;/h2&gt;

&lt;p&gt;The models are not being made publicly accessible at this time. We are releasing our models to qualified researchers and partners through this &lt;a href=&quot;https://forms.gle/8rB4wvzair1t8qqz9&quot;&gt;form&lt;/a&gt;. Full distribution will follow once we have completed a thorough assessment of our models.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thank you to &lt;a href=&quot;https://www.diu.mil/team/Steven-Butow&quot;&gt;Gen. Steve Butow&lt;/a&gt; and  &lt;a href=&quot;https://scholar.google.com/citations?user=bJ51bBQAAAAJ&amp;amp;hl=en&quot;&gt;Dr. Nirav Patel&lt;/a&gt; at the Department of Defense’s &lt;a href=&quot;https://diu.mil/&quot;&gt;Defense Innovation Unit&lt;/a&gt; for reviewing this post and providing their expertise on the future of commercial SAR constellations.&lt;/p&gt;

&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;It’s interesting to note that the definition of VHR imagery has changed over time. In the 80s, &lt;a href=&quot;https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JC093iC06p06735&quot;&gt;20 kilometer resolution was “VHR”&lt;/a&gt;. Perhaps, in the future, 0.3m resolution imagery will no longer be VHR. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 21 Mar 2022 05:00:00 -0700</pubDate>
        <link>http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/</link>
        <guid isPermaLink="true">http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/</guid>
      </item>
     
    
  </channel>
</rss>
