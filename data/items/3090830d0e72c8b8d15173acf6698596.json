{
  "title": "More on Outlier Detection",
  "description": "<p><em>Several features of outlier detection in real world performance monitoring data.</em></p>\n\n<p>Outlier score is useless. It could be defined as number of <a href=\"http://en.wikipedia.org/wiki/Standard_deviation\">standard deviations</a> from the <a href=\"http://en.wikipedia.org/wiki/Mean\">sample mean</a> to the value in question (or number of <a href=\"http://en.wikipedia.org/wiki/Interquartile_range\">IQRs</a> from <a href=\"http://en.wikipedia.org/wiki/Median\">median</a> depending on method used).  Due to extremely long tails of data distribution there would be outliers with scores 10 and 1000 but it doesn’t mean that one of them is 100 times worse than another. The only thing that seems to matter here is if the value is an outlier or not.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Mean\">Mean</a> and <a href=\"http://en.wikipedia.org/wiki/Standard_deviation\">standard deviation</a> values are misleading for non-gaussian distributions which are typical in performance monitoring data. <a href=\"http://en.wikipedia.org/wiki/Median\">Median</a> and <a href=\"http://en.wikipedia.org/wiki/Interquartile_range\">IQR</a> (or <a href=\"http://en.wikipedia.org/wiki/Interdecile_range\">interdecile range</a>) are more robust estimates for <a href=\"http://en.wikipedia.org/wiki/Central_tendency\">center</a> and <a href=\"http://en.wikipedia.org/wiki/Statistical_dispersion\">dispersion</a> of data.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Interquartile_range\">IQR</a> or <a href=\"http://en.wikipedia.org/wiki/Interdecile_range\">interdecile range</a> could be 0 for a flat metric with rare spikes. It makes methods based on these values useless.</p>\n\n<p>Many metrics have small number of unique values which skews calculation of quantilles (breaks <a href=\"http://www.edgarstat.com/tukeys_outliers_help.cfm\">IQR-based methods</a>) and produces steep steps in <a href=\"http://en.wikipedia.org/wiki/Empirical_distribution_function\">empirical CDF</a> (breaks <a href=\"http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\">KS-test</a>).</p>\n\n<p>Asymmetric distribution (typical for latency data) suggests using asymmetric ranges around median but it’s often impossible to calculate range on one side because there is only one value there (e.g. mostly constant metric with rare positive spikes).</p>\n\n<p>We can’t ignore outliers when reasoning about overall system performance. Imagine a single threaded service that has typical request latency about 1ms and one in 1000 requests takes 1s (outlier) to complete. If we drop the outlier (maybe because of <a href=\"http://www.azulsystems.com/sites/default/files/images/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf\">Coordinated Omission</a>) then the service seems to be capable of serving 1000 rps. In reality that service works for one second and then gets stuck for another second resulting in total performance less than 500 rps.</p>\n\n<p>There are several events which are outliers from practical point of view but outlier detection methods don’t handle them well (though they are quite simple to detect):</p>\n\n<ul>\n  <li>\n    <p>Metric appeared. Some libraries might start reporting metric only after application provides value for the first time. For rare events (like errors) it means that event’s counter will be undefined until the event happens for the first time.</p>\n  </li>\n  <li>\n    <p>Metric disappeared. It might mean that metric was removed with latest update of application or that there was no single appearance for some rare event within observation window.</p>\n  </li>\n  <li>\n    <p>Constant changed. Observation window contains only equal values and the last obtained one is not equal to previously seen values.</p>\n  </li>\n</ul>",
  "pubDate": "Mon, 08 Dec 2014 00:00:00 +0000",
  "link": "https://mabrek.github.io/blog/more-on-outliers/",
  "guid": "https://mabrek.github.io/blog/more-on-outliers/"
}