{
  "title": "From Arrow to pandas at 10 Gigabytes Per Second",
  "link": "",
  "published": "2016-12-27T08:00:00-08:00",
  "updated": "2016-12-27T08:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2016-12-27:/blog/high-perf-arrow-to-pandas/",
  "summary": "<p>In this post I discuss some recent work in Apache Arrow to accelerate\nconverting to pandas objects from general Arrow columnar memory.</p>",
  "content": "<p>In this post I discuss some recent work in Apache Arrow to accelerate\nconverting to pandas objects from general Arrow columnar memory.</p>\n\n\n<h2>Challenges constructing pandas DataFrame objects quickly</h2>\n<p>One of the difficulties in fast construction of pandas DataFrame object is that\nthe \"native\" internal memory structure is more complex than a dictionary or\nlist of one-dimensional NumPy arrays. I won't go into the reasons for this\ncomplexity, but it's something we're hoping to do away with as part of the\n<a href=\"http://www.slideshare.net/wesm/python-data-wrangling-preparing-for-the-future\">pandas 2.0 effort</a>. There are two layers of complexity:</p>\n<ul>\n<li>\n<p>pandas's memory representation for a particular data type may change\n  depending on the presence of null values. Boolean data becomes <code>dtype=object</code>\n  while integer data becomes <code>dtype=float64</code>.</p>\n</li>\n<li>\n<p>Upon calling <code>pandas.DataFrame</code>, internally pandas will \"consolidate\" the\n  input by copying into its internal two-dimensional <strong>block\n  structure</strong>. Constructing the precise block structure is the only true way to\n  do <strong>zero-copy</strong> DataFrame construction.</p>\n</li>\n</ul>\n<p>To give you an idea of the overhead introduced by consolidation, let's look at\na benchmark. Consider the setup code, in which we create a dict of 100\n<code>float64</code> arrays constituting a gigabyte of data:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyarrow</span> <span class=\"k\">as</span> <span class=\"nn\">pa</span>\n\n<span class=\"n\">type_</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">(</span><span class=\"s1\">&#39;float64&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">DATA_SIZE</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">&lt;&lt;</span> <span class=\"mi\">30</span><span class=\"p\">)</span>\n<span class=\"n\">NCOLS</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">NROWS</span> <span class=\"o\">=</span> <span class=\"n\">DATA_SIZE</span> <span class=\"o\">/</span> <span class=\"n\">NCOLS</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">(</span><span class=\"n\">type_</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">itemsize</span>\n\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s1\">&#39;c&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">):</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">NROWS</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">NCOLS</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n\n<p>Then, we create a DataFrame with <code>pd.DataFrame(data)</code>:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"nv\">timeit</span> <span class=\"nv\">df</span> <span class=\"o\">=</span> <span class=\"nv\">pd</span>.<span class=\"nv\">DataFrame</span><span class=\"ss\">(</span><span class=\"nv\">data</span><span class=\"ss\">)</span>\n<span class=\"mi\">10</span> <span class=\"nv\">loops</span>, <span class=\"nv\">best</span> <span class=\"nv\">of</span> <span class=\"mi\">3</span>: <span class=\"mi\">132</span> <span class=\"nv\">ms</span> <span class=\"nv\">per</span> <span class=\"k\">loop</span>\n</code></pre></div>\n\n<p>(For those who are counting, that's <strong>7.58 GB/second</strong> <em>just</em> to do an internal\nmemory copy.)</p>\n<p>An important thing to remember is that, here, we have already constructed\npandas's \"native\" memory representation (nulls would be <code>NaN</code> in the arrays),\nbut as a collection of 1D arrays.</p>\n<h2>Converting from Arrow columnar memory to pandas</h2>\n<p><a href=\"http://arrow.apache.org/\">Apache Arrow</a>, a project I've been heavily involved with since its genesis\nearly in 2016, is a language-agnostic in-memory columnar representation and\ncollection of tools for interprocess communication (IPC) . It supports nested\nJSON-like data and is designed as a building-block for creating fast analytics\nengines.</p>\n<p>Compared with pandas, Arrow has a more precise representation of null values in\na bitmap that is separate from the values. So, zero-copy conversion even to a\ndict-of-arrays representation suitable for pandas requires more effort.</p>\n<p>One of my major goals in working on Arrow is to use it as a high-bandwidth IO\npipe for the Python ecosystem. We can talk to the JVM, database systems, and\nmany different file formats by using Arrow as the columnar interchange\nformat. For this use case, it's important to be able to get back a pandas\nDataFrame as fast as possible.</p>\n<p>Over the last month I've done some engineering to construct pandas's native\ninternal block structure to achieve high bandwidth to Arrow memory. If you've\nbeen following the <a href=\"http://github.com/wesm/feather\">Feather file format</a>, this work is all closely\ninterrelated.</p>\n<p>Let's return to the same gigabyte of data from above, and be sure to add some\nnulls:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">[::</span><span class=\"mi\">5</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">nan</span>\n</code></pre></div>\n\n<p>Now, let's convert the DataFrame to an Arrow table, which constructs the Arrow\ncolumnar representation:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">table</span> <span class=\"o\">=</span> <span class=\"n\">pa</span><span class=\"o\">.</span><span class=\"n\">Table</span><span class=\"o\">.</span><span class=\"n\">from_pandas</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">table</span>\n<span class=\"o\">&lt;</span><span class=\"n\">pyarrow</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"o\">.</span><span class=\"n\">Table</span> <span class=\"n\">at</span> <span class=\"mh\">0x7f18ec65abd0</span><span class=\"o\">&gt;</span>\n</code></pre></div>\n\n<p>To go back to pandas-land, call the table's <code>to_pandas</code> method. This supports a\nmultithreaded conversion, so let's do a single-threaded conversion for\ncomparison:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">timeit</span> <span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">table</span><span class=\"o\">.</span><span class=\"n\">to_pandas</span><span class=\"p\">(</span><span class=\"n\">nthreads</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"mi\">10</span> <span class=\"n\">loops</span><span class=\"p\">,</span> <span class=\"n\">best</span> <span class=\"n\">of</span> <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"mi\">158</span> <span class=\"n\">ms</span> <span class=\"n\">per</span> <span class=\"n\">loop</span>\n</code></pre></div>\n\n<p>This is <strong>6.33 GB/s</strong>, or about 20% slower than purely memcpy-based\nconstruction. On my desktop, I can use all 4 cores to go even faster:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">timeit</span> <span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">table</span><span class=\"o\">.</span><span class=\"n\">to_pandas</span><span class=\"p\">(</span><span class=\"n\">nthreads</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"mi\">10</span> <span class=\"n\">loops</span><span class=\"p\">,</span> <span class=\"n\">best</span> <span class=\"n\">of</span> <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"mi\">103</span> <span class=\"n\">ms</span> <span class=\"n\">per</span> <span class=\"n\">loop</span>\n</code></pre></div>\n\n<p>At <strong>9.71 GB/s</strong>, this is not far from saturating the main memory bandwidth on\nmy consumer desktop hardware (but I am not an expert on this).</p>\n<p>The performance benefits of multithreading can be more dramatic on other\nhardware. While the performance ratio on my desktop is only <strong>1.53</strong>, on my\n(also quad-core) laptop it is 3.29.</p>\n<blockquote>\n<p>Note that numeric data is a best-case scenario; string or binary data would\ncome with additional overhead while pandas continues to use Python objects in\nits memory representation.</p>\n</blockquote>\n<h2>Implications and roadmap</h2>\n<p>Since Arrow arrays, record batches (multiple arrays of the same length), and\ntables (collections of record batches) can be easily zero-copy constructed from\nmany different sources, it is a flexible and efficient way to move tabular data\naround between systems. By having high speed conversion to pandas, we can pay a\nsmall conversion cost (5-10 GB/s is usually negligible compared with the IO\nperformance of other media) to obtain a fully-fledged pandas DataFrame.</p>\n<p>In a separate blog post, I'll go into some of the technical details of Arrow's\nlow-overhead (and as often as possible: zero-copy) C++ IO subsystem.</p>\n<p>As we forge ahead on the pandas 2.0 roadmap, we hope to further reduce the\noverhead (to zero, in some cases) in interacting with columnar memory like\nArrow. A simpler memory representation will also make it easier for other\napplications to interact with pandas at a low-level.</p>"
}