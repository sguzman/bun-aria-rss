{
  "title": "Inferencing the Transformer Model",
  "link": "https://machinelearningmastery.com/inferencing-the-transformer-model/",
  "comments": "https://machinelearningmastery.com/inferencing-the-transformer-model/#comments",
  "dc:creator": "Stefania Cristina",
  "pubDate": "Wed, 19 Oct 2022 13:03:42 +0000",
  "category": [
    "Attention",
    "attention",
    "inference",
    "natural language processing",
    "transformer"
  ],
  "guid": "https://machinelearningmastery.com/?p=13874",
  "description": "<p>Last Updated on November 2, 2022 We have seen how to train the Transformer model on a dataset of English and German sentence pairs and how to plot the training and validation loss curves to diagnose the model&#8217;s learning performance and decide at which epoch to run inference on the trained model. We are now [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/inferencing-the-transformer-model/\">Inferencing the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "content:encoded": "<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13874 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Inferencing+the+Transformer+Model&url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Inferencing+the+Transformer+Model&url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/inferencing-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p id=\"last-modified-info\">Last Updated on November 2, 2022</p>\n<p>We have seen how to <a href=\"https://machinelearningmastery.com/training-the-transformer-model/\">train the Transformer model</a> on a dataset of English and German sentence pairs and how to <a href=\"https://machinelearningmastery.com/?p=13879&preview=true\">plot the training and validation loss curves</a> to diagnose the model’s learning performance and decide at which epoch to run inference on the trained model. We are now ready to run inference on the trained Transformer model to translate an input sentence.</p>\n<p>In this tutorial, you will discover how to run inference on the trained Transformer model for neural machine translation.<span class=\"Apple-converted-space\"> </span></p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>How to run inference on the trained Transformer model</li>\n<li>How to generate text translations</li>\n</ul>\n<p>Let’s get started.<span class=\"Apple-converted-space\"> </span></p>\n<div id=\"attachment_13877\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-scaled.jpg\"><img aria-describedby=\"caption-attachment-13877\" loading=\"lazy\" class=\"wp-image-13877 size-large\" src=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-1024x683.jpg\" alt=\"\" width=\"1024\" height=\"683\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-1024x683.jpg 1024w, https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-300x200.jpg 300w, https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-768x512.jpg 768w, https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-1536x1024.jpg 1536w, https://machinelearningmastery.com/wp-content/uploads/2022/10/karsten-wurth-algc0FKHeMA-unsplash-2048x1366.jpg 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><p id=\"caption-attachment-13877\" class=\"wp-caption-text\">Inferencing the Transformer model<br />Photo by <a class=\"N2odk RZQOk Vk1a0 AsGGe pgmwB KHq0c\" href=\"https://unsplash.com/photos/algc0FKHeMA\">Karsten Würth</a>, some rights reserved.</p></div>\n<h2><b>Tutorial Overview</b></h2>\n<p>This tutorial is divided into three parts; they are:</p>\n<ul>\n<li>Recap of the Transformer Architecture</li>\n<li>Inferencing the Transformer Model</li>\n<li>Testing Out the Code</li>\n</ul>\n<h2><b>Prerequisites</b></h2>\n<p>For this tutorial, we assume that you are already familiar with:</p>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/the-transformer-model/\">The theory behind the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">An implementation of the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/training-the-transformer-model/\">Training the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/?p=13879&preview=true\">Plotting the training and validation loss curves for the Transformer model</a></li>\n</ul>\n<h2><b>Recap of the Transformer Architecture</b></h2>\n<p><a href=\"https://machinelearningmastery.com/the-transformer-model/\">Recall</a> having seen that the Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.</p>\n<div id=\"attachment_12821\" style=\"width: 379px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"><img aria-describedby=\"caption-attachment-12821\" loading=\"lazy\" class=\"wp-image-12821\" src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" alt=\"\" width=\"369\" height=\"520\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png 727w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-213x300.png 213w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png 768w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-1090x1536.png 1090w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png 1320w\" sizes=\"(max-width: 369px) 100vw, 369px\" /></a><p id=\"caption-attachment-12821\" class=\"wp-caption-text\">The encoder-decoder structure of the Transformer architecture <br />Taken from &#8220;<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>&#8220;</p></div>\n<p>In generating an output sequence, the Transformer does not rely on recurrence and convolutions.</p>\n<p>You have seen how to implement the complete Transformer model and subsequently train it on a dataset of English and German sentence pairs. Let&#8217;s now proceed to run inference on the trained model for neural machine translation.<span class=\"Apple-converted-space\"> </span></p>\n<p><strong>Kick-start your project</strong> with my book <a href=\"https://machinelearningmastery.com/transformer-models-with-attention/\">Building Transformer Models with Attention</a>. It provides <strong>self-study tutorials</strong> with <strong>working code</strong> to guide you into building a fully-working transformer models that can<br><em>translate sentences from one language to another</em>...</p>\n<h2><b>Inferencing the Transformer Model</b></h2>\n<p>Let’s start by creating a new instance of the <code>TransformerModel</code> class that was previously implemented in <a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">this tutorial</a>.<span class=\"Apple-converted-space\"> </span></p>\n<p>You will feed into it the relevant input arguments as specified in the paper of <a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al. (2017)</a> and the relevant information about the dataset in use:<span class=\"Apple-converted-space\"> </span></p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the dataset parameters\nenc_seq_length = 7  # Encoder sequence length\ndec_seq_length = 12  # Decoder sequence length\nenc_vocab_size = 2405  # Encoder vocabulary size\ndec_vocab_size = 3858  # Decoder vocabulary size\n\n# Create model\ninferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)</pre><p>Here, note that the last input being fed into the <code>TransformerModel</code> corresponded to the dropout rate for each of the <code>Dropout</code> layers in the Transformer model. These <code>Dropout</code> layers will not be used during model inferencing (you will eventually set the <code>training</code> argument to <code>False</code>), so you may safely set the dropout rate to 0.</p>\n<p>Furthermore, the <code>TransformerModel</code> class was already saved into a separate script named <code>model.py</code>. Hence, to be able to use the <code>TransformerModel</code> class, you need to include <code>from model import TransformerModel</code>.</p>\n<p>Next, let&#8217;s create a class, <code>Translate</code>, that inherits from the <code>Module</code> base class in Keras and assign the initialized inferencing model to the variable <code>transformer</code>:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">class Translate(Module):\n    def __init__(self, inferencing_model, **kwargs):\n        super(Translate, self).__init__(**kwargs)\n        self.transformer = inferencing_model\n        ...</pre><p>When you <a href=\"https://machinelearningmastery.com/training-the-transformer-model/\">trained the Transformer model</a>, you saw that you first needed to tokenize the sequences of text that were to be fed into both the encoder and decoder. You achieved this by creating a vocabulary of words and replacing each word with its corresponding vocabulary index.<span class=\"Apple-converted-space\"> </span></p>\n<p>You will need to implement a similar process during the inferencing stage before feeding the sequence of text to be translated into the Transformer model.<span class=\"Apple-converted-space\"> </span></p>\n<p>For this purpose, you will include within the class the following <code>load_tokenizer</code> method, which will serve to load the encoder and decoder tokenizers that <a href=\"https://machinelearningmastery.com/?p=13879&preview=true\">you would have generated and saved during the training stage</a>:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def load_tokenizer(self, name):\n    with open(name, 'rb') as handle:\n        return load(handle)</pre><p>It is important that you tokenize the input text at the inferencing stage using the same tokenizers generated at the training stage of the Transformer model since these tokenizers would have already been trained on text sequences similar to your testing data.<span class=\"Apple-converted-space\"> </span></p>\n<p>The next step is to create the class method, <code>call()</code>, that will take care to:</p>\n<ul>\n<li>Append the start (<START>) and end-of-string (<EOS>) tokens to the input sentence:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def __call__(self, sentence):\n    sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"</pre><p></p>\n<ul>\n<li>Load the encoder and decoder tokenizers (in this case, saved in the <code>enc_tokenizer.pkl</code> and <code>dec_tokenizer.pkl</code> pickle files, respectively):</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">enc_tokenizer = self.load_tokenizer('enc_tokenizer.pkl')\ndec_tokenizer = self.load_tokenizer('dec_tokenizer.pkl')</pre><p></p>\n<ul>\n<li>Prepare the input sentence by tokenizing it first, then padding it to the maximum phrase length, and subsequently converting it to a tensor:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">encoder_input = enc_tokenizer.texts_to_sequences(sentence)\nencoder_input = pad_sequences(encoder_input, maxlen=enc_seq_length, padding='post')\nencoder_input = convert_to_tensor(encoder_input, dtype=int64)</pre><p></p>\n<ul>\n<li>Repeat a similar tokenization and tensor conversion procedure for the <START> and <EOS> tokens at the output:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])\noutput_start = convert_to_tensor(output_start[0], dtype=int64)\n\noutput_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\noutput_end = convert_to_tensor(output_end[0], dtype=int64)</pre><p></p>\n<ul>\n<li>Prepare the output array that will contain the translated text. Since you do not know the length of the translated sentence in advance, you will initialize the size of the output array to 0, but set its <code>dynamic_size</code> parameter to <code>True</code> so that it may grow past its initial size. You will then set the first value in this output array to the <START> token:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\ndecoder_output = decoder_output.write(0, output_start)</pre><p></p>\n<ul>\n<li>Iterate, up to the decoder sequence length, each time calling the Transformer model to predict an output token. Here, the <code>training</code> input, which is then passed on to each of the Transformer’s <code>Dropout</code> layers, is set to <code>False</code> so that no values are dropped during inference. The prediction with the highest score is then selected and written at the next available index of the output array. The <code>for</code> loop is terminated with a <code>break</code> statement as soon as an <EOS> token is predicted:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">for i in range(dec_seq_length):\n\n    prediction = self.transformer(encoder_input, transpose(decoder_output.stack()), training=False)\n\n    prediction = prediction[:, -1, :]\n\n    predicted_id = argmax(prediction, axis=-1)\n    predicted_id = predicted_id[0][newaxis]\n\n    decoder_output = decoder_output.write(i + 1, predicted_id)\n\n    if predicted_id == output_end:\n        break</pre><p></p>\n<ul>\n<li>Decode the predicted tokens into an output list and return it:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">output = transpose(decoder_output.stack())[0]\noutput = output.numpy()\n\noutput_str = []\n\n# Decode the predicted tokens into an output list\nfor i in range(output.shape[0]):\n\n   key = output[i]\n   translation = dec_tokenizer.index_word[key]\n   output_str.append(translation)\n\nreturn output_str</pre><p>The complete code listing, so far, is as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from pickle import load\nfrom tensorflow import Module\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64, TensorArray, argmax, newaxis, transpose\nfrom model import TransformerModel\n\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the dataset parameters\nenc_seq_length = 7  # Encoder sequence length\ndec_seq_length = 12  # Decoder sequence length\nenc_vocab_size = 2405  # Encoder vocabulary size\ndec_vocab_size = 3858  # Decoder vocabulary size\n\n# Create model\ninferencing_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, 0)\n\n\nclass Translate(Module):\n    def __init__(self, inferencing_model, **kwargs):\n        super(Translate, self).__init__(**kwargs)\n        self.transformer = inferencing_model\n\n    def load_tokenizer(self, name):\n        with open(name, 'rb') as handle:\n            return load(handle)\n\n    def __call__(self, sentence):\n        # Append start and end of string tokens to the input sentence\n        sentence[0] = \"<START> \" + sentence[0] + \" <EOS>\"\n\n        # Load encoder and decoder tokenizers\n        enc_tokenizer = self.load_tokenizer('enc_tokenizer.pkl')\n        dec_tokenizer = self.load_tokenizer('dec_tokenizer.pkl')\n\n        # Prepare the input sentence by tokenizing, padding and converting to tensor\n        encoder_input = enc_tokenizer.texts_to_sequences(sentence)\n        encoder_input = pad_sequences(encoder_input, maxlen=enc_seq_length, padding='post')\n        encoder_input = convert_to_tensor(encoder_input, dtype=int64)\n\n        # Prepare the output <START> token by tokenizing, and converting to tensor\n        output_start = dec_tokenizer.texts_to_sequences([\"<START>\"])\n        output_start = convert_to_tensor(output_start[0], dtype=int64)\n\n        # Prepare the output <EOS> token by tokenizing, and converting to tensor\n        output_end = dec_tokenizer.texts_to_sequences([\"<EOS>\"])\n        output_end = convert_to_tensor(output_end[0], dtype=int64)\n\n        # Prepare the output array of dynamic size\n        decoder_output = TensorArray(dtype=int64, size=0, dynamic_size=True)\n        decoder_output = decoder_output.write(0, output_start)\n\n        for i in range(dec_seq_length):\n\n            # Predict an output token\n            prediction = self.transformer(encoder_input, transpose(decoder_output.stack()), training=False)\n\n            prediction = prediction[:, -1, :]\n\n            # Select the prediction with the highest score\n            predicted_id = argmax(prediction, axis=-1)\n            predicted_id = predicted_id[0][newaxis]\n\n            # Write the selected prediction to the output array at the next available index\n            decoder_output = decoder_output.write(i + 1, predicted_id)\n\n            # Break if an <EOS> token is predicted\n            if predicted_id == output_end:\n                break\n\n        output = transpose(decoder_output.stack())[0]\n        output = output.numpy()\n\n        output_str = []\n\n        # Decode the predicted tokens into an output string\n        for i in range(output.shape[0]):\n\n            key = output[i]\n            print(dec_tokenizer.index_word[key])\n\n        return output_str</pre><p></p>\n<h2><b>Testing Out the Code</b></h2>\n<p>In order to test out the code, let’s have a look at the <code>test_dataset.txt</code> file that you would have saved when <a href=\"https://machinelearningmastery.com/?p=13879&preview=true\">preparing the dataset for training</a>. This text file contains a set of English-German sentence pairs that have been reserved for testing, from which you can select a couple of sentences to test.</p>\n<p>Let’s start with the first sentence:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Sentence to translate\nsentence = ['im thirsty']</pre><p>The corresponding ground truth translation in German for this sentence, including the <START> and <EOS> decoder tokens, should be: <code><START> ich bin durstig <EOS></code>.</p>\n<p>If you have a look at the <a href=\"https://machinelearningmastery.com/?p=13879&preview=true\">plotted training and validation loss curves</a> for this model (here, you are training for 20 epochs), you may notice that the validation loss curve slows down considerably and starts plateauing at around epoch 16.<span class=\"Apple-converted-space\"> </span></p>\n<p>So let’s proceed to load the saved model’s weights at the 16th epoch and check out the prediction that is generated by the model:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Load the trained model's weights at the specified epoch\ninferencing_model.load_weights('weights/wghts16.ckpt')\n\n# Create a new instance of the 'Translate' class\ntranslator = Translate(inferencing_model)\n\n# Translate the input sentence\nprint(translator(sentence))</pre><p>Running the lines of code above produces the following translated list of words:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">['start', 'ich', 'bin', 'durstig', ‘eos']</pre><p>Which is equivalent to the ground truth German sentence that was expected (always keep in mind that since you are training the Transformer model from scratch, you may arrive at different results depending on the random initialization of the model weights).<span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s check out what would have happened if you had, instead, loaded a set of weights corresponding to a much earlier epoch, such as the 4th epoch. In this case, the generated translation is the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">['start', 'ich', 'bin', 'nicht', 'nicht', 'eos']</pre><p>In English, this translates to: <i>I in not not</i>, which is clearly far off from the input English sentence, but which is expected since, at this epoch, the learning process of the Transformer model is still at the very early stages.<span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s try again with a second sentence from the test dataset:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Sentence to translate\nsentence = ['are we done']</pre><p>The corresponding ground truth translation in German for this sentence, including the <START> and <EOS> decoder tokens, should be: <code><START> sind wir dann durch <EOS></code>.</p>\n<p>The model’s translation for this sentence, using the weights saved at epoch 16, is:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">['start', 'ich', 'war', 'fertig', 'eos']</pre><p>Which, instead, translates to: <i>I was ready</i>. While this is also not equal to the ground truth, it is <i>close</i> to its meaning.<span class=\"Apple-converted-space\"> </span></p>\n<p>What the last test suggests, however, is that the Transformer model might have required many more data samples to train effectively. This is also corroborated by the validation loss at which the validation loss curve plateaus remain relatively high.<span class=\"Apple-converted-space\"> </span></p>\n<p>Indeed, Transformer models are notorious for being very data hungry. <a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al. (2017)</a>, for example, trained their English-to-German translation model using a dataset containing around 4.5 million sentence pairs.<span class=\"Apple-converted-space\"> </span></p>\n<blockquote><p><i>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs…For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences…</i></p>\n<p style=\"text-align: right;\"><i>– </i><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017.</p>\n</blockquote>\n<p>They reported that it took them 3.5 days on 8 P100 GPUs to train the English-to-German translation model.<span class=\"Apple-converted-space\"> </span></p>\n<p>In comparison, you have only trained on a dataset comprising 10,000 data samples here, split between training, validation, and test sets.<span class=\"Apple-converted-space\"> </span></p>\n<p>So the next task is actually for you. If you have the computational resources available, try to train the Transformer model on a much larger set of sentence pairs and see if you can obtain better results than the translations obtained here with a limited amount of data.<span class=\"Apple-converted-space\"> </span></p>\n<h2><b>Further Reading</b></h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3><b>Books</b></h3>\n<ul>\n<li><a href=\"https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X\">Advanced Deep Learning with Python</a>, 2019</li>\n<li><a href=\"https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798\">Transformers for Natural Language Processing</a>, 2021</li>\n</ul>\n<h3><b>Papers</b></h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017</li>\n</ul>\n<h2><b>Summary</b></h2>\n<p>In this tutorial, you discovered how to run inference on the trained Transformer model for neural machine translation.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How to run inference on the trained Transformer model</li>\n<li>How to generate text translations</li>\n</ul>\n<p>Do you have any questions?<br />\nAsk your questions in the comments below, and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13874 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Inferencing+the+Transformer+Model&url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Inferencing+the+Transformer+Model&url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/inferencing-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/inferencing-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/inferencing-the-transformer-model/\">Inferencing the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "wfw:commentRss": "https://machinelearningmastery.com/inferencing-the-transformer-model/feed/",
  "slash:comments": 7
}