{
  "title": "Dual Generator Offline Reinforcement Learning. (arXiv:2211.01471v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01471",
  "description": "<p>In offline RL, constraining the learned policy to remain close to the data is\nessential to prevent the policy from outputting out-of-distribution (OOD)\nactions with erroneously overestimated values. In principle, generative\nadversarial networks (GAN) can provide an elegant solution to do so, with the\ndiscriminator directly providing a probability that quantifies distributional\nshift. However, in practice, GAN-based offline RL methods have not performed as\nwell as alternative approaches, perhaps because the generator is trained to\nboth fool the discriminator and maximize return -- two objectives that can be\nat odds with each other. In this paper, we show that the issue of conflicting\nobjectives can be resolved by training two generators: one that maximizes\nreturn, with the other capturing the ``remainder'' of the data distribution in\nthe offline dataset, such that the mixture of the two is close to the behavior\npolicy. We show that not only does having two generators enable an effective\nGAN-based offline RL method, but also approximates a support constraint, where\nthe policy does not need to match the entire data distribution, but only the\nslice of the data that leads to high long term performance. We name our method\nDASCO, for Dual-Generator Adversarial Support Constrained Offline RL. On\nbenchmark tasks that require learning from sub-optimal data, DASCO\nsignificantly outperforms prior methods that enforce distribution constraint.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1\">Quan Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aviral Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1\">Yevgen Chebotar</a>"
}