{
  "title": "Tracking the Progress in Natural Language Processing",
  "description": "Research in ML and NLP is moving at a tremendous pace, which is an obstacle for people wanting to enter the field. To make working with new tasks easier, this post introduces a resource that tracks the progress and state-of-the-art across many tasks in NLP.",
  "link": "http://ruder.io/tracking-progress-nlp/",
  "guid": "5c76ff7c1b9b0d18555b9eaa",
  "category": "natural language processing",
  "dc:creator": "Sebastian Ruder",
  "pubDate": "Fri, 22 Jun 2018 17:31:55 GMT",
  "media:content": "",
  "content:encoded": "<!--kg-card-begin: markdown--><!--kg-card-begin: markdown--><img src=\"http://ruder.io/content/images/2018/10/squad_progress.png\" alt=\"Tracking the Progress in Natural Language Processing\"><p>This post introduces a resource to track the progress and state-of-the-art across many tasks in NLP.</p>\n<p><a href=\"https://nlpprogress.com\">Go directly to the document tracking the progress in NLP</a>.</p>\n<p>Research in Machine Learning and in Natural Language Processing (NLP) is moving so fast these days, it is hard to keep up. This is an issue for people in the field, but it is an even bigger obstacle for people wanting to get into NLP and those seeking to make the leap from tutorials to reproducing papers and conducting their own research. Without expert guidance and prior knowledge, it can be a painstaking process to identify the most common datasets and the current state-of-the-art for your task of interest.</p>\n<p>A number of resources exist that could help with this process, but each has deficits: The Association of Computation Linguistics (ACL) has a <a href=\"https://aclweb.org/aclwiki/State_of_the_art\">wiki page</a> tracking the state-of-the-art, but the page is not maintained and contributing is not straightforward. The <a href=\"https://www.eff.org/ai/metrics\">Electronic Frontier Foundation</a> and the <a href=\"https://aiindex.org/\">AI Index</a> try to do something similar for all of AI but only cover a few language tasks. The <a href=\"http://lremap.elra.info/\">Language Resources and Evaluation (LRE) Map</a> collects language resources presented at LREC and other conferences, but does not allow to break them out by tasks or popularity. Similarly, the <a href=\"http://alt.qcri.org/semeval2018/index.php?id=tasks\">International Workshop on Semantic Evaluation (SemEval)</a> hosts a small number of tasks each year, which provide new datasets that typically have not been widely studied before. There are also resources that focus on <a href=\"http://rodrigob.github.io/are_we_there_yet/build/\">computer vision</a> and <a href=\"https://github.com/syhw/wer_are_we\">speech recognition</a> as well as <a href=\"https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems#nlp\">this repo</a>, which focuses on all of ML.</p>\n<p>As an alternative, I have created a <a href=\"https://github.com/sebastianruder/NLP-progress\">GitHub repository</a> that keeps track of the datasets and the current state-of-the-art for the most common tasks in NLP. The repository is kept as simple as possible to make maintenance and contribution easy. If I missed your favourite task or dataset or your new state-of-the-art result or if I made any error, you can simply submit a pull request.</p>\n<p>The aim is to have a comprehensive and up-to-date resource where everyone can see at a glance the state-of-the-art for the tasks they care about. Datasets, which already do a great job at tracking this such as <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">SQuAD</a> or <a href=\"https://nlp.stanford.edu/projects/snli/\">SNLI</a> using a public leaderboard will simply be referenced instead.</p>\n<p>My hope is that such a resource will give a broader sense of progress in the field than results in individual papers. It might also make it easier to identify tasks or areas where progress has been lacking. Another benefit is that such a resource may encourage serendipity: chancing upon an interesting new task or method. Finally, a positive by-product of having the state-of-the-art for each task easily accessible may be that it will be harder to justify (accidentally) comparing to weak baselines. For instance, the perplexity of the best baseline on the Penn Treebank varied dramatically across 10 language modeling papers submitted to ICLR 2018 (see below).</p>\n<figure>\n      <img src=\"http://ruder.io/content/images/2018/06/ppl_language_modeling_iclr_2018.png\" style=\"width: 70%\" title=\"Learning rate schedules with warm restarts\" alt=\"Tracking the Progress in Natural Language Processing\">\n<figcaption>Figure 1: Comparison of perplexity (PPL) of proposed model vs. PPL of best baseline across 10 language modeling papers submitted to ICLR 2018 (credit: <a href=\"https://twitter.com/AaronJaech/status/924678973497446400\">@AaronJaech</a>)</figcaption>\n</figure>\n<p>Credit for the cover image is due to the <a href=\"https://www.eff.org/ai/metrics#Reading-Comprehension\">Electronic Frontier Foundation</a>.</p>\n<!--kg-card-end: markdown--><!--kg-card-end: markdown-->"
}