{
  "title": "Dask Release 0.13.0",
  "link": "",
  "updated": "2017-01-03T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/01/03/dask-0.13.0",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>Dask just grew to version 0.13.0.  This is a signifcant release for arrays,\ndataframes, and the distributed scheduler.  This blogpost outlines some of the\nmajor changes since the last release November 4th.</p>\n\n<ol>\n  <li>Python 3.6 support</li>\n  <li>Algorithmic and API improvements for DataFrames</li>\n  <li>Dataframe to Array conversions for Machine Learning</li>\n  <li>Parquet support</li>\n  <li>Scheduling Performance and Worker Rewrite</li>\n  <li>Pervasive Visual Diagnostics with Embedded Bokeh Servers</li>\n  <li>Windows continuous integration</li>\n  <li>Custom serialization</li>\n</ol>\n\n<p>You can install new versions using Conda or Pip</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>conda install -c conda-forge dask distributed\n</code></pre></div></div>\n\n<p>or</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install dask[complete] distributed --upgrade\n</code></pre></div></div>\n\n<h2 id=\"python-36-support\">Python 3.6 Support</h2>\n\n<p>Dask and all necessary dependencies are now available on <a href=\"https://conda-forge.github.io/\">Conda\nForge</a> for Python 3.6.</p>\n\n<h2 id=\"algorithmic-and-api-improvements-for-dataframes\">Algorithmic and API Improvements for DataFrames</h2>\n\n<p>Thousand-core Dask deployments have become significantly more common in the\nlast few months.   This has highlighted scaling issues in some of the\nDask.array and Dask.dataframe algorithms, which were originally designed for\nsingle workstations.  Algorithmic and API changes can be grouped into the\nfollowing two categories:</p>\n\n<ol>\n  <li>Filling out the Pandas API</li>\n  <li>Algorithms that needed to be changed or added due to scaling issues</li>\n</ol>\n\n<p>Dask Dataframes now include a fuller set of the Pandas API, including the\nfollowing:</p>\n\n<ol>\n  <li>Inplace operations like <code class=\"language-plaintext highlighter-rouge\">df['x'] = df.y + df.z</code></li>\n  <li>The full Groupby-aggregate syntax like <code class=\"language-plaintext highlighter-rouge\">df.groupby(...).aggregate({'x': 'sum', 'y': ['min', max']})</code></li>\n  <li>Resample on dataframes as well as series</li>\n  <li>Pandas’ new rolling syntax <code class=\"language-plaintext highlighter-rouge\">df.x.rolling(10).mean()</code></li>\n  <li>And much more</li>\n</ol>\n\n<p>Additionally, collaboration with some of the larger Dask deployments has\nhighlighted scaling issues in some algorithms, resulting in the following improvements:</p>\n\n<ol>\n  <li>Tree reductions for groupbys, aggregations, etc.</li>\n  <li>Multi-output-partition aggregations for groupby-aggregations with millions of groups, drop_duplicates, etc..</li>\n  <li>Approximate algorithms for nunique</li>\n  <li>etc..</li>\n</ol>\n\n<p>These same collaborations have also yielded better handling of open file\ndescriptors, changes upstream to Tornado, and upstream changes to the\nconda-forge CPython recipe itself to increase the default file descriptor limit\non Windows up from 512.</p>\n\n<h2 id=\"dataframe-to-array-conversions\">Dataframe to Array Conversions</h2>\n\n<p>You can now convert Dask dataframes into Dask arrays.  This is mostly to\nsupport efforts of groups building statistics and machine learning\napplications, where this conversion is common.  For example you can load a\nterabyte of CSV or Parquet data, do some basic filtering and manipulation, and\nthen convert to a Dask array to do more numeric work like SVDs, regressions,\netc..</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'s3://...'</span><span class=\"p\">)</span>  <span class=\"c1\"># Read raw data\n</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">values</span>                 <span class=\"c1\"># Convert to dask.array\n</span>\n<span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">svd</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>    <span class=\"c1\"># Perform serious numerics\n</span></code></pre></div></div>\n\n<p>This should help machine learning and statistics developers generally, as many\nof the more sophisticated algorithms can be more easily implemented with the\nDask array model than can be done with distributed dataframes.  This change was\ndone specifically to support the nascent third-party\n<a href=\"https://github.com/moody-marlin/dask-glm\">dask-glm</a> project by <a href=\"https://github.com/moody-marlin/\">Chris\nWhite</a> at Capital One.</p>\n\n<p>Previously this was hard because Dask.array wanted to know the size of every\nchunk of data, which Dask dataframes can’t provide (because, for example, it is\nimpossible to lazily tell how many rows are in a CSV file without actually\nlooking through it).  Now that Dask.arrays have relaxed this requirement they\ncan also support other unknown shape operations, like indexing an array with\nanother array.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<h2 id=\"parquet-support\">Parquet Support</h2>\n\n<p>Dask.dataframe now supports <a href=\"https://parquet.apache.org/\">Parquet</a>, a columnar\nbinary store for tabular data commonly used in distributed clusters and the\nHadoop ecosystem.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'myfile.parquet'</span><span class=\"p\">)</span>                 <span class=\"c1\"># Read from Parquet\n</span>\n<span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_parquet</span><span class=\"p\">(</span><span class=\"s\">'myfile.parquet'</span><span class=\"p\">,</span> <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s\">'snappy'</span><span class=\"p\">)</span>  <span class=\"c1\"># Write to Parquet\n</span></code></pre></div></div>\n\n<p>This is done through the new\n<a href=\"http://fastparquet.readthedocs.io/en/latest/\">fastparquet</a> library, a\nNumba-accelerated version of the Pure Python\n<a href=\"https://github.com/jcrobak/parquet-python\">parquet-python</a>.  Fastparquet was\nbuilt and is maintained by <a href=\"https://github.com/martindurant\">Martin Durant</a>.\nIt’s also exciting to see the\n<a href=\"https://github.com/apache/parquet-cpp\">Parquet-cpp</a> project gain Python\nsupport through <a href=\"http://pyarrow.readthedocs.io/en/latest/\">Arrow</a> and work by\n<a href=\"http://wesmckinney.com/\">Wes McKinney</a> and <a href=\"https://github.com/xhochy\">Uwe\nKorn</a>.  Parquet has gone from inaccessible in Python\nto having multiple competing implementations, which is a wonderful and exciting\nchange for the “Big Data” Python ecosystem.</p>\n\n<h2 id=\"scheduling-performance-and-worker-rewrite\">Scheduling Performance and Worker Rewrite</h2>\n\n<p>The internals of the distributed scheduler and workers are significantly\nmodified.  Users shouldn’t experience much change here except for general\nperformance enhancement, more upcoming features, and much deeper visual\ndiagnostics through Bokeh servers.</p>\n\n<p>We’ve pushed some of the scheduling logic from the scheduler onto the workers.  This lets us do two things:</p>\n\n<ol>\n  <li>We keep a much larger backlog of tasks on the workers.  This allows workers\nto optimize and saturate their hardware more effectively.  As a result,\ncomplex computations end up being significantly faster.</li>\n  <li>We can more easily deliver on a rising number of requests for complex\nscheduling features.  For example, GPU users will be happy to learn that\nyou can now specify abstract resource constraints like “this task requires\na GPU” and “this worker has four GPUs” and the scheduler and workers will\nallocate tasks accordingly.  This is just one example of a feature that was\neasy to implement after the scheduler/worker redesign and is now available.</li>\n</ol>\n\n<h2 id=\"pervasive-visual-diagnostics-with-embedded-bokeh-servers\">Pervasive Visual Diagnostics with Embedded Bokeh Servers</h2>\n\n<p>While optimizing scheduler performance we built several new visual diagnostics\nusing <a href=\"http://bokeh.pydata.org/en/latest/\">Bokeh</a>.  There is now a Bokeh Server\nrunning <em>within</em> the scheduler and within every worker.</p>\n\n<p>Current Dask.distributed users will be familiar with the current diagnostic\ndashboards:</p>\n\n<p><a href=\"https://raw.githubusercontent.com/dask/dask-org/master/images/daskboard.gif\">\n    <img src=\"https://raw.githubusercontent.com/dask/dask-org/master/images/daskboard.gif\" alt=\"Dask Bokeh Plots\" width=\"60%\" /></a></p>\n\n<p>These plots provide intuition about the state of the cluster and the\ncomputations currently in flight.  These dashboards are generally well loved.</p>\n\n<p>There are now many more of these, though more focused on internal state and\ntimings that will be of interest to developers and power users than to a\ntypical users.  Here are a couple of the new pages (of which there are seven)\nthat show various timings and counters of various parts of the worker and\nscheduler internals.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/bokeh-counters.gif\">\n  <img src=\"https://mrocklin.github.io/blog/images/bokeh-counters.gif\" alt=\"Dask Bokeh counters page\" width=\"100%\" /></a></p>\n\n<p>The previous Bokeh dashboards were served from a separate process that queried\nthe scheduler periodically (every 100ms).  Now there are new Bokeh servers\nwithin every worker and a new Bokeh server <em>within</em> the scheduler process\nitself rather than in a separate process.  Because these servers are embedded\nthey have direct access to the state of the scheduler and workers which\nsignificantly reduces barriers for us to build out new visuals.  However, this\nalso adds some load to the scheduler, which can often be compute bound.  These\npages are available at new ports, 8788 for the scheduler and 8789 for the\nworker by default.</p>\n\n<h2 id=\"custom-serialization\">Custom Serialization</h2>\n\n<p>This is actually a change that occurred in the last release, but I haven’t\nwritten about it and it’s important, so I’m including it here.</p>\n\n<p>Previously inter-worker communication of data was accomplished with\nPickle/Cloudpickle and optional generic compression like LZ4/Snappy.  This was\nrobust and worked mostly fine, but left out some exotic data types and did not\nprovide optimal performance.</p>\n\n<p>Now we can serialize different types with special consideration.  This allows\nspecial types, like NumPy arrays, to pass through without unnecessary memory\ncopies and also allows us to use more exotic data-type specific compression\ntechniques like <a href=\"http://www.blosc.org/\">Blosc</a>.</p>\n\n<p>It also allows Dask to serialize some previously unserializable types.  In\nparticular this was intended to solve the Dask.array climate science\ncommunity’s concern about HDF5 and NetCDF files which (correctly) are\nunpicklable and so restricted to single-machine use.</p>\n\n<p>This is also the first step towards two frequently requested features (neither\nof these exist yet):</p>\n\n<ol>\n  <li>Better support for GPU-GPU specific serialization options.  We are now a\nlarge step closer to generalizing away our assumption of TCP Sockets as\nthe universal communication mechanism.</li>\n  <li>Passing data between workers of different runtime languages.  By embracing\nother protocols than Pickle we begin to allow for the communication of data\nbetween workers of different software environments.</li>\n</ol>\n\n<h2 id=\"whats-next\">What’s Next</h2>\n\n<p>So what should we expect to see in the future for Dask?</p>\n\n<ul>\n  <li><strong>Communication</strong>: Now that workers are more fully saturated we’ve found\nthat communication issues are arising more frequently as bottlenecks.  This\nmight be because everything else is nearing optimal or it might be\nbecause of the increased contention in the workers now that they are idle\nless often.  Many of our new diagnostics are intended to measure components\nof the communication pipeline.</li>\n  <li><strong>Third Party Tools</strong>: We’re seeing a nice growth of utilities like\n<a href=\"https://github.com/dask/dask-drmaa\">dask-drmaa</a> for launching clusters on\nDRMAA job schedulers (SGE, SLURM, LSF) and\n<a href=\"https://github.com/moody-marlin/dask-glm\">dask-glm</a> for solvers for GLM-like\nmachine-learning algorithms.  I hope that external projects like these\nbecome the main focus of Dask development going forward as Dask penetrates\nnew domains.</li>\n  <li><strong>Blogging</strong>:  I’ll be launching a few fun blog posts throughout the next\ncouple of weeks.  Stay tuned.</li>\n</ul>\n\n<h2 id=\"learn-more\">Learn More</h2>\n\n<p>You can install or upgrade using Conda or Pip</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>conda install -c conda-forge dask distributed\n</code></pre></div></div>\n\n<p>or</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install dask[complete] distributed --upgrade\n</code></pre></div></div>\n\n<p>You can learn more about Dask and its distributed scheduler at these websites:</p>\n\n<ul>\n  <li><a href=\"http://dask.pydata.org/en/latest/\">Dask Documentation</a></li>\n  <li><a href=\"http://distributed.readthedocs.io/en/latest/\">Distributed Scheduler Documentation</a></li>\n</ul>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Since the last main release the following developers have contributed to the\ncore Dask repostiory (parallel algorithms, arrays, dataframes, etc..)</p>\n\n<ul>\n  <li>Alexander C. Booth</li>\n  <li>Antoine Pitrou</li>\n  <li>Christopher Prohm</li>\n  <li>Frederic Laliberte</li>\n  <li>Jim Crist</li>\n  <li>Martin Durant</li>\n  <li>Matthew Rocklin</li>\n  <li>Mike Graham</li>\n  <li>Rolando (Max) Espinoza</li>\n  <li>Sinhrks</li>\n  <li>Stuart Archibald</li>\n</ul>\n\n<p>And the following developers have contributed to the Dask/distributed\nrepository (distributed scheduling, network communication, etc..)</p>\n\n<ul>\n  <li>Antoine Pitrou</li>\n  <li>jakirkham</li>\n  <li>Jeff Reback</li>\n  <li>Jim Crist</li>\n  <li>Martin Durant</li>\n  <li>Matthew Rocklin</li>\n  <li>rbubley</li>\n  <li>Stephan Hoyer</li>\n  <li>strets123</li>\n  <li>Travis E. Oliphant</li>\n</ul>"
}