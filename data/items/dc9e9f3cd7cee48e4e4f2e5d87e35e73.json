{
  "title": "Maximum-likelihood asymptotics",
  "link": "",
  "published": "2015-12-30T00:01:00-08:00",
  "updated": "2015-12-30T00:01:00-08:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2015-12-30:/maximum-likelihood-asymptotics",
  "summary": "<p>In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, <span class=\"math\">\\(N\\)</span>, and 2) They satisfy the <a href=\"http://efavdb.github.io/multivariate-cramer-rao-bound\">Cramer-Rao</a> lower bound for unbiased parameter estimates in this same limit &#8212; that is, they have the …</p>",
  "content": "<p>In this post, we review two facts about maximum-likelihood estimators: 1) They are consistent, meaning that they converge to the correct values given a large number of samples, <span class=\"math\">\\(N\\)</span>, and 2) They satisfy the <a href=\"http://efavdb.github.io/multivariate-cramer-rao-bound\">Cramer-Rao</a> lower bound for unbiased parameter estimates in this same limit &#8212; that is, they have the lowest possible variance of any unbiased estimator, in the <span class=\"math\">\\(N\\gg 1\\)</span>&nbsp;limit.</p>\n<h3>Introduction</h3>\n<p>We begin with a simple example maximum-likelihood inference problem: Suppose one has obtained <span class=\"math\">\\(N\\)</span> independent samples <span class=\"math\">\\(\\{x_1, x_2, \\ldots, x_N\\}\\)</span> from a Gaussian distribution of unknown mean <span class=\"math\">\\(\\mu\\)</span> and variance <span class=\"math\">\\(\\sigma^2\\)</span>. In order to obtain a maximum-likelihood estimate for these parameters, one asks which <span class=\"math\">\\(\\hat{\\mu}\\)</span> and <span class=\"math\">\\(\\hat{\\sigma}^2\\)</span> would be most likely to generate the samples observed. To find these, we first write down the probability of observing the samples, given our model. This is&nbsp;simply\n</p>\n<div class=\"math\">$$\nP(\\{x_1, x_2, \\ldots, x_N\\} \\vert \\mu, \\sigma^2) =\\\\ \\exp\\left [ \\sum_{i=1}^N \\left (-\\frac{1}{2} \\log (2 \\pi \\sigma^2) -\\frac{1}{2 \\sigma^2} (x_i - \\mu)^2\\right ) \\right ]. \\tag{1} \\label{1}\n$$</div>\n<p>\nTo obtain the maximum-likelihood estimates, we maximize (\\ref{1}): Setting its derivatives with respect to <span class=\"math\">\\(\\mu\\)</span> and <span class=\"math\">\\(\\sigma^2\\)</span> to zero and solving&nbsp;gives\n</p>\n<div class=\"math\">\\begin{align}\\label{mean}\n\\hat{\\mu} &amp;= \\frac{1}{N} \\sum_i x_i \\tag{2} \\\\\n\\hat{\\sigma}^2 &amp;= \\frac{1}{N} \\sum_i (x_i - \\hat{\\mu})^2. \\tag{3} \\label{varhat}\n\\end{align}</div>\n<p>\nThese are mean and variance values that would be most likely to generate our observation set <span class=\"math\">\\(\\{x_i\\}\\)</span>. Our solutions show that they are both functions of the random observation set. Because of this, <span class=\"math\">\\(\\hat{\\mu}\\)</span> and <span class=\"math\">\\(\\hat{\\sigma}^2\\)</span> are themselves random variables, changing with each sample set that happens to be observed. Their distributions can be characterized by their mean values, variances,&nbsp;etc.</p>\n<p>The average squared error of a parameter estimator is determined entirely by its bias and variance &#8212; see eq (2) of <a href=\"http://efavdb.github.io/bayesian-linear-regression\">prior post</a>. Now, one can show that the <span class=\"math\">\\(\\hat{\\mu}\\)</span> estimate of (\\ref{mean}) is unbiased, but this is not the case for the variance estimator (\\ref{varhat}) &#8212; one should (famously) divide by <span class=\"math\">\\(N-1\\)</span> instead of <span class=\"math\">\\(N\\)</span> here to obtain an unbiased estimator<span class=\"math\">\\(^1\\)</span>. This shows that maximum-likelihood estimators need not be unbiased. Why then are they so popular? One reason is that these estimators are guaranteed to be unbiased when <span class=\"math\">\\(N\\)</span>, the sample size, is large. Further, in this same limit, these estimators achieve the minimum possible variance for any unbiased parameter estimate &#8212; as set by the fundamental <a href=\"http://efavdb.github.io/multivariate-cramer-rao-bound\">Cramer-Rao</a> bound. The purpose of this post is to review simple proofs of these latter two facts about maximum-likelihood estimators<span class=\"math\">\\(^2\\)</span>.</p>\n<h3>Consistency</h3>\n<p>Let <span class=\"math\">\\(P(x \\vert \\theta^*)\\)</span> be some distribution characterized by a parameter <span class=\"math\">\\(\\theta^*\\)</span> that is unknown. We will show that the maximum-likelihood estimator converges to <span class=\"math\">\\(\\theta^*\\)</span> when <span class=\"math\">\\(N\\)</span> is large: As in (\\ref{1}), the maximum-likelihood solution is that <span class=\"math\">\\(\\theta\\)</span> that&nbsp;maximizes\n</p>\n<div class=\"math\">$$\\tag{4} \\label{4}\nJ \\equiv \\frac{1}{N}\\sum_{i=1}^N \\log P(x_i \\vert \\theta),\n$$</div>\n<p>\nwhere the <span class=\"math\">\\(\\{x_i\\}\\)</span> are the independent samples taken from <span class=\"math\">\\(P(x \\vert \\theta^*)\\)</span>. By the law of large numbers, when <span class=\"math\">\\(N\\)</span> is large, this average over the samples converges to its population mean. In other&nbsp;words,\n</p>\n<div class=\"math\">$$\\tag{5}\n\\lim_{N \\to \\infty}J \\rightarrow \\int_x P(x \\vert \\theta^*) \\log P(x \\vert \\theta) dx.\n$$</div>\n<p>\nWe will show that <span class=\"math\">\\(\\theta^*\\)</span> is the <span class=\"math\">\\(\\theta\\)</span> value that maximizes the above. We can do this directly,&nbsp;writing\n</p>\n<div class=\"math\">$$\n\\begin{align}\nJ(\\theta) - J(\\theta^*) &amp; = \\int_x P(x \\vert \\theta^*) \\log \\left ( \\frac{P(x \\vert \\theta) }{P(x \\vert \\theta^*)}\\right) \\\\\n&amp; \\leq \\int_x P(x \\vert \\theta^*) \\left ( \\frac{P(x \\vert \\theta) }{P(x \\vert \\theta^*)} - 1 \\right) \\\\\n&amp; = \\int_x P(x \\vert \\theta) - P(x \\vert \\theta^*) = 1 - 1 = 0. \\tag{6} \\label{6}\n\\end{align}\n$$</div>\n<p>\nHere, we have used <span class=\"math\">\\(\\log t \\leq t-1\\)</span> in the second line. Rearranging the above shows that <span class=\"math\">\\(J(\\theta^*) \\geq J(\\theta)\\)</span> for all <span class=\"math\">\\(\\theta\\)</span> &#8212; when <span class=\"math\">\\(N \\gg 1\\)</span>, meaning that <span class=\"math\">\\(J\\)</span> is maximized at <span class=\"math\">\\(\\theta^*\\)</span>. That is, the maximum-likelihood estimator <span class=\"math\">\\(\\hat{\\theta} \\to \\theta^*\\)</span> in this limit<span class=\"math\">\\(^3\\)</span>.</p>\n<h3>Optimal&nbsp;variance</h3>\n<p>To derive the variance of a general maximum-likelihood estimator, we will see how its average value changes upon introduction of a small Bayesian prior, <span class=\"math\">\\(P(\\theta) \\sim \\exp(\\Lambda \\theta)\\)</span>. The trick will be to evaluate the change in two separate ways &#8212; this takes a few lines, but is quite straightforward. In the first approach, we do a direct maximization: The quantity to be maximized is&nbsp;now\n</p>\n<div class=\"math\">$$ \\label{7}\nJ = \\sum_{i=1}^N \\log P(x_i \\vert \\theta) + \\Lambda \\theta. \\tag{7}\n$$</div>\n<p>\nBecause we take <span class=\"math\">\\(\\Lambda\\)</span> small, we can use a Taylor expansion to find the new solution,&nbsp;writing\n</p>\n<div class=\"math\">$$ \\label{8}\n\\hat{\\theta} = \\theta^* + \\theta_1 \\Lambda + O(\\Lambda^2). \\tag{8}\n$$</div>\n<p>\nSetting the derivative of (\\ref{7}) to zero, with <span class=\"math\">\\(\\theta\\)</span> given by its value in (\\ref{8}), we&nbsp;obtain\n</p>\n<div class=\"math\">$$\n\\sum_{i=1}^N \\partial_{\\theta} \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta^*} + \\\\ \\sum_{i=1}^N \\partial_{\\theta}^2 \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta^*} \\times \\theta_1 \\Lambda + \\Lambda + O(\\Lambda^2) = 0. \\tag{9} \\label{9}\n$$</div>\n<p>\nThe first term here goes to zero at large <span class=\"math\">\\(N\\)</span>, as above. Setting the terms at <span class=\"math\">\\(O(\\Lambda^1)\\)</span> to zero&nbsp;gives\n</p>\n<div class=\"math\">$$\n\\theta_1 = - \\frac{1}{ \\sum_{i=1}^N \\partial_{\\theta}^2 \\left . \\log P(x_i \\vert \\theta) \\right \\vert_{\\theta^*} }. \\tag{10} \\label{10}\n$$</div>\n<p>\nPlugging this back into (\\ref{8}) gives the first order correction to <span class=\"math\">\\(\\hat{\\theta}\\)</span> due to the perturbation. Next, as an alternative approach, we evaluate the change in <span class=\"math\">\\(\\theta\\)</span> by maximizing the <span class=\"math\">\\(P(\\theta)\\)</span> distribution, expanding about its unperturbed global maximum, <span class=\"math\">\\(\\theta^*\\)</span>: We write,&nbsp;formally,\n</p>\n<div class=\"math\">$$\\tag{11} \\label{11}\nP(\\theta) = e^{ - a_0 - a_2 (\\theta - \\theta^*)^2 - a_3 (\\theta - \\theta^*)^3 + \\ldots + \\Lambda \\theta}.\n$$</div>\n<p>\nDifferentiating to maximize (\\ref{11}), and again assuming a solution of form (\\ref{8}), we&nbsp;obtain\n</p>\n<div class=\"math\">$$\\label{12} \\tag{12}\n-2 a_2 \\times \\theta_1 \\Lambda + \\Lambda + O(\\Lambda^2) = 0 \\ \\ \\to \\ \\ \\theta_1 = \\frac{1}{2 a_2}.\n$$</div>\n<p>\nWe now require consistency between our two approaches, equating (\\ref{10}) and (\\ref{12}). This gives an expression for <span class=\"math\">\\(a_2\\)</span>. Plugging this back into (\\ref{11}) then gives (for the unperturbed&nbsp;distribution)\n</p>\n<div class=\"math\">$$\\tag{13} \\label{13}\nP(\\theta) = \\mathcal{N} \\exp \\left [ N \\frac{ \\langle \\partial_{\\theta}^2 \\left . \\log P(x, \\theta) \\right \\vert_{\\theta^*} \\rangle }{2} (\\theta - \\theta^*)^2 + \\ldots \\right].\n$$</div>\n<p>\nUsing this Gaussian approximation<span class=\"math\">\\(^4\\)</span>, we can now read off the large <span class=\"math\">\\(N\\)</span> variance of <span class=\"math\">\\(\\hat{\\theta}\\)</span>&nbsp;as\n</p>\n<div class=\"math\">$$\\tag{14} \\label{14}\nvar(\\hat{\\theta}) = - \\frac{1}{N} \\times \\frac{1}{\\langle \\partial_{\\theta}^2 \\left . \\log P(x, \\theta) \\right \\vert_{\\theta^*} \\rangle }.\n$$</div>\n<p>\nThis is the lowest possible value for any unbiased estimator, as set by the Cramer-Rao bound. The proof shows that maximum-likelihood estimators always saturate this bound, in the large <span class=\"math\">\\(N\\)</span> limit &#8212; a remarkable result. We discuss the intuitive meaning of the Cramer-Rao bound in a <a href=\"http://efavdb.github.io/multivariate-cramer-rao-bound\">prior post</a>.</p>\n<h3>Footnotes</h3>\n<p>[1] To see that (\\ref{varhat}) is biased, we just need to evaluate the average of <span class=\"math\">\\(\\sum_i (x_i - \\hat{\\mu})^2\\)</span>. This&nbsp;is</p>\n<div class=\"math\">$$\n\\overline{\\sum_i x_i^2 - 2 \\sum_{i,j} \\frac{x_i x_j}{N} + \\sum_{i,j,k} \\frac{x_j x_k}{N^2}} = N \\overline{x^2} - (N-1) \\overline{x}^2 - \\overline{x^2} \\\\\n= (N-1) \\left ( \\overline{x^2} - \\overline{x}^2 \\right) \\equiv (N-1) \\sigma^2.\n$$</div>\n<p>\nDividing through by <span class=\"math\">\\(N\\)</span>, we see that <span class=\"math\">\\(\\overline{\\hat{\\sigma}^2} = \\left(\\frac{N-1}{N}\\right)\\sigma^2\\)</span>. The deviation from the true variance <span class=\"math\">\\(\\sigma^2\\)</span> goes to zero at large <span class=\"math\">\\(N\\)</span>, but is non-zero for any finite <span class=\"math\">\\(N\\)</span>: The estimator is biased, but the bias goes to zero at large <span class=\"math\">\\(N\\)</span>.</p>\n<p>[2] The consistency proof is taken from lecture notes by D. Panchenko, see <a href=\"http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf\">here</a>. Professor Panchenko is quite famous for having proven the correctness of the Parisi ansatz in replica theory. Our variance proof is original &#8212; please let us know if you have seen it elsewhere. Note that it can also be easily extended to derive the covariance matrix of a set of maximum-likelihood estimators that are jointly distributed &#8212; we cover only the scalar case here, for&nbsp;simplicity.</p>\n<p>[3] The proof here actually only shows that there is no <span class=\"math\">\\(\\theta\\)</span> that gives larger likelihood than <span class=\"math\">\\(\\theta^*\\)</span> in the large <span class=\"math\">\\(N\\)</span> limit. However, for some problems, it is possible that more than one <span class=\"math\">\\(\\theta\\)</span> maximizes the likelihood. A trivial example is given by the case where the distribution is actually only a function of <span class=\"math\">\\((\\theta - \\theta_0)^2\\)</span>. In this case, both values <span class=\"math\">\\(\\theta_0 \\pm (\\theta^* - \\theta_0)\\)</span> will necessarily maximize the&nbsp;likelihood.</p>\n<p>[4] It&#8217;s a simple matter to carry this analysis further, including the cubic and higher order terms in the expansion (\\ref{11}). These lead to correction terms for (\\ref{14}), smaller in magnitude than that given there. These terms become important when <span class=\"math\">\\(N\\)</span> decreases in&nbsp;magnitude.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}