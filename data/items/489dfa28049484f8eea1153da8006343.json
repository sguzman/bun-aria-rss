{
  "title": "History of Monte Carlo Methods - Part 2",
  "link": "",
  "published": "2015-10-30T20:00:00+00:00",
  "updated": "2015-10-30T20:00:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-10-30:/sebastian/blog/history-of-monte-carlo-methods-part-2.html",
  "summary": "<p>This is the second part of a three part post.\nThe <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-1.html\">last part</a> covered the early history of Monte\nCarlo and the rejection sampling method.</p>\n<h1>Part 2</h1>\n<p>In this part we are going to look at importance sampling and sequential â€¦</p>",
  "content": "<p>This is the second part of a three part post.\nThe <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-1.html\">last part</a> covered the early history of Monte\nCarlo and the rejection sampling method.</p>\n<h1>Part 2</h1>\n<p>In this part we are going to look at importance sampling and sequential Monte\nCarlo.</p>\n<p>The video files <a href=\"https://1drv.ms/u/s!AniEhJbTwIdrkuMvpGcjY3NR12xbuw?e=YvUIAn\">are also available for offline\nviewing</a> in\nMP4/H.264, WebM/VP8, and WebM/VP9 formats.</p>\n<video width=\"639\" controls>\n<source src=\"https://onedrive.live.com/download?cid=6B87C0D396848478&resid=6B87C0D396848478%21307637&authkey=ACYR7JYwYI_xH54\"\n    type=\"video/mp4\">\nYour browser does not support the video tag.\n</video>\n\n<p><iframe\nsrc=\"https://onedrive.live.com/embed?cid=6B87C0D396848478&resid=6B87C0D396848478%21108434&authkey=AJ6I7fTgWMVe7s8&em=2\"\nwidth=\"639\" height=\"360\" frameborder=\"0\" scrolling=\"no\"></iframe>\n</p>\n<p>(Click on the slide to advance, or use the previous/next buttons.)</p>\n<h2>Transcript</h2>\n<p>(This is a slightly edited and link-annotated transcript of the audio in the\nabove video.  As this is spoken text, it is not as polished as my writing.)</p>\n<p><strong>Speaker</strong>: Okay, so rejection sampling works only for short chain lengths.\nAnd then, in light of this finding the next method, sequential importance\nsampling, was introduced independently by two groups.\n<a href=\"https://en.wikipedia.org/wiki/John_Hammersley\">John Michael Hammersley</a>,\nwho actually did his PhD here in Cambridge and then moved to Oxford to become\nprofessor, and by <a href=\"https://en.wikipedia.org/wiki/Marshall_Rosenbluth\">Marshall\nRosenbluth</a> and his wife\nArianna Rosenbluth. They did it independently. They called it different by\ndifferent names. I think Hammersley called it <em>inversely restricted sampling</em>\nand the Rosenbluth called it <em>biased sampling</em> and both these names did not\nreally stick. So now, nowadays it is called <em>sequential importance sampling</em>.\nIn different communities, it is also called the <em>growth method</em> or the\n<em>Rosenbluth method</em>.</p>\n<p>How does this method work? It is based on the idea that was suggested by the\naudience just before (in the first part). Remember when we are growing these\nchains step by step and we make the step and we would have to reject the\nsample? Well, we could just prevent making that step, all right. We could just\nsay, \"Look, there are two possibilities but you would not reject that sample.\nSo why not just take one of these?\"</p>\n<p>Of course the method is not fail-safe. So if we, in such a situation, no\nmatter what we do if we keep growing we will still run into trouble.\nRight, so the method is still myopic. We still only make one inference at a\ntime. But the real problem with this method is that we no longer sample\nuniformly from the set that interest us. In fact, we favor more compact\nconfigurations and what Hammersley, and Morton, and the Rosenbluth's realized\nis a method to systematically compensate for that bias. </p>\n<p>So let me first talk about how this in general is done and then how it is done\nin our specific example. So in general, we have this expectation expression\nthat we want to approximate. And what they said is, well, we assign one weight\nto each sample and if the weight would be one that would be the original\nexpectation. But we assign a weight to the sample and we choose the weight in\nsuch a way that we compensate for that bias, that we favor some\nconfigurations. So whenever we favor these configurations, we down-weight\nthem, and whenever some configurations is rare but we generate them, we\nup-weight them.</p>\n<p>In practice, we would generate a few samples and if every sample would have a\nweight, in this particular instance how it works is, well we just count the\nnumber of possibilities we have in each step. So we basically decompose a\nsampling distribution. So in the first step we have four possibilities, four\npoints free adjacent to it and the next one we have only three available. And\nso we just unroll basically our decisions. Here we only have two available,\ntwo choices available, all right? So because we grow the chain sequentially,\nthe probability of generating that particular configuration also decomposes\nsequentially.  So the final chain would have this probability of being\ngenerated. And what they simply do is say, \"Okay, this is the distribution by\nwhich we generate the sample, but we want to generate it at uniformly at\nrandom so we also decompose the weight and just build the weight as a\ninverse\". So when we weight these samples by weights we will systematically\nde-bias the sampling distribution, so we will get unbiased estimates of the\nexpectation that we are interested in.</p>\n<p>Let us take a look at where we were with the rejection sampler. So this is a\nlimit, what we could do with a rejection sampler. Now, with the growth method,\nwe can go to significantly longer chain length, to a chain length of 60 and\nthen again, the uncertainty estimates the confidence intervals blow up. So why\nis this? I mean why do we say uncertainty intervals blow up in this improved\nmethod? Well the thing is, the weights that we compute, they become very\nunbalanced. And even though we generate maybe a few thousand samples, only few\nof them will have significant share of the weights.</p>\n<p>So here is a visualization of that. So here, I grow 50 chains in parallel, one\nstep at a time, and I show you in each step the normalized weights. So in the\nbeginning, everything is uniform because in the beginning, everything has\nequal number of possibilities. But over time, as I grow more and more, as\nappend more elements, the weights become very unbalanced so that after 100\nsteps, actually five elements have weights significantly different from zero.\nAnd this means we actually do not have 50 samples on this case, we only have\nfive, and our estimates become very poor. And this only amplifies when you\nhave a few thousands ones. One way to measure the quality of the samples we\nhave generated is to ask, \"Okay, I have generated, say 5,000 samples with\nweights. How much are these worth in terms of computing expectations, in terms\nof unweighted samples?\"</p>\n<p>Because unweighted samples are optimal there is a quality measure that you can\ncompute that is an estimated quantity and it is called <a href=\"http://www.nowozin.net/sebastian/blog/effective-sample-size-in-importance-sampling.html\">effective sample\nsize</a>,\nwhich exactly measures the worthiness of a weighted sample set. And for this\nplot I have shown you now with 5,000 samples, you see that it drops and drops\nand drops until it is almost close to one. So that is a real problem.</p>\n<p>You guessed the next step would be another improved Monte Carlo method and\nindeed it improves on that, and it is generally known as\n<a href=\"http://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf\"><em>Sequential Monte Carlo Method</em></a>.\nThe idea is quite simple and natural and it has been reinvented\nmany times in different communities. So the <a href=\"http://scitation.aip.org/content/aip/journal/jcp/30/3/10.1063/1.1730021\">original\npaper</a>\nis from 1959 but has been reinvented in the signal processing community as\n<a href=\"https://en.wikipedia.org/wiki/Particle_filter\">particle filter</a>, it\nhas been reused and pioneered in computer vision by our Andrew Blake for\ntracking objects and their contours. And it is used across many different\ncommunities often under very different names. But generally, Sequential Monte\nCarlo is the preferred name. And the basic idea here is quite simple. The\nproblem is unbalanced weights. We have to prevent getting unbalanced weights\nin each step. So how we are going to do this is by introducing a process which\nremoves samples that have low weight and duplicate samples that have high\nweight. That is called re-sampling.</p>\n<p>So say, in one certain timestamp, we grow all the chains in parallel, we grow\nsay 50 chains in parallel. On this example, it is only six chains in parallel.\nAnd we have observed that the weights are unbalanced. Then remove some of the\nlow weight instances and we duplicate some of the high weight instances as\nshown here. The algorithm that corresponds to that is the same as before just\nweighted sequential importance sampling but we grow all the samples in\nparallel and monitor the weights. And if the weights are in trouble, if the\nweights become unbalanced, we enforce balanced weights again, by removing low\nweight samples and duplicating other.</p>\n<p><strong>Attendee</strong>: Question.</p>\n<p><strong>Speaker</strong>: Yes.</p>\n<p><strong>Attendee</strong>: In your little white chain example, the weights are going to be\nhigh when at every step you can take three choices, right? Like three times\nthree times three. And that is going to get bigger. The long ones do not go\nnear each other. So this is going to bias in favor of things that just sort of\ngo off into the distance, all of them curly wurly things, is that right?</p>\n<p><strong>Speaker</strong>: Right, exactly, because the sampling distribution biases towards\ncompact configurations the weights have to undo that bias by favoring the\nconfigurations that walk off and are no longer compact.</p>\n<p><strong>Attendee</strong>: But isn't that bad because then essentially we'll you dominated by\nall the ones that go off into the distance and we won't get any curly wurly\nones.</p>\n<p><strong>Speaker</strong>: It would be bad but remember that the sampling distribution that\nI had above the weight, the sampling distribution exactly has that opposing\nbias. So we generate from that distribution and we want the weights to\ncompensate for that bias. So the extra samples that we get are more compact\nthan they should be. And that is why we have to downweight these to get a low\nweight, right? And we have to upweight these samples that are not compact but\nthat go out into basically long chains.</p>\n<p><strong>Attendee</strong>: I did not get that, sorry. But never mind. I believe you.</p>\n<p><strong>Speaker</strong>: Okay. So if you do that re-sampling operation to compensate for\nthe unbalanced weight effect, and I plot again the effective sample size and\nwhenever we see that the effective sample size, in this case drops below\n2,500, I perform this re-sampling operation and reset the weights to uniform\nand I enforce this effective sample size to become 5,000 again. You see that\nbasically, I can control how unbalanced the weights become. Here is another\nvisualization in terms of the same plot that I had before and now with the red\narrows are indicated whenever I reset the weights to uniform, I perform his\nre-sampling operation so I always can ensure that my weights are close to the\nuniform distribution.</p>\n<p>So let us compare that again. This was a plot without re-sampling. You will\nsee that the uncertainty estimates indicate that our estimates are very\nunreliable. And this is basically with re-sampling. The whole family of\nSequential Monte Carlo approaches are really state of the art methods. This\nscales to almost no limits, so people have used generate chains with over a\nmillion bonds. It is state of the art for any kind of probabilistic model\nwhere you can sequentially decompose the model. For example, time series\nmodels, hidden Markov models, state space models, dynamic Bayesian networks,\nall these kind of models, these methods are applicable and highly efficient.</p>",
  "category": ""
}