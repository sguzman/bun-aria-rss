{
  "title": "LSTMs",
  "link": "https://shapeofdata.wordpress.com/2016/06/04/lstms/",
  "comments": "https://shapeofdata.wordpress.com/2016/06/04/lstms/#comments",
  "dc:creator": "Jesse Johnson",
  "pubDate": "Sat, 04 Jun 2016 13:01:21 +0000",
  "category": "Neural Networks",
  "guid": "http://shapeofdata.wordpress.com/?p=1485",
  "description": "In past posts, I&#8217;ve described how Recurrent Neural Networks (RNNs) can be used to learn patterns in sequences of inputs, and how the idea of unrolling can be used to train them. It turns out that there are some significant &#8230; <a href=\"https://shapeofdata.wordpress.com/2016/06/04/lstms/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>In past posts, I&#8217;ve described how <a href=\"https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks/\">Recurrent Neural Networks</a> (RNNs) can be used to learn patterns in sequences of inputs, and how the idea of <a href=\"https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/\">unrolling</a> can be used to train them. It turns out that there are some significant limitations to the types of patterns that a typical RNN can learn, due to the way their weight matrices are used. As a result, there has been a lot of interest in a variant of RNNs called Long Short-Term Memory networks (LSTMs). As I&#8217;ll describe below, LSTMs have more control than typical RNNs over what they remember, which allows them to learn much more complex patterns.</p>\n<p><span id=\"more-1485\"></span>Lets start with what I mean  by a &#8220;typical&#8221; RNN. In my post on RNNs, I mentioned three basic types of operations in the computation graph: 1) Outputs from nodes (which may be earlier or later in the network) can be concatenated into higher-dimensional vectors, for example to mix the current input vector with a vector that was calculated in a previous step. 2) Vectors can be multiplied by weight matrices, to combine and transform the concatenated vectors. 3) A fixed non-linear transformation can be applied to the output from each node in the computation graph.</p>\n<p>The limitation with the RNNs defined by these three operations stems from the fact that while the weight matrices are updated during the training phase, they&#8217;re fixed while each sequence of inputs is processed. So each step in the input sequence is combined with the information stored from earlier steps in the same way each time. In some sense, the network is forced to remember the same things about each step in every sequence, even though some steps may be more important, or contain different types of information, than others.</p>\n<p>An LSTM, on the other hand, is designed to be able to control what it remembers about each input, and to learn how to decide what to remember in the training phase. The key additional operation that goes into an LSTM is 4) Output vectors from nodes in the computation graph can by multiplied component-wise. In other words, the values in the first dimension of each vector are multiplied together to get the first dimension of a new vector. Then the second dimensions of the two vectors are multiplied, and so on.</p>\n<p>This is not a linear transformation, in the sense that you can&#8217;t get the same result by concatenating the two vectors, then multiplying by a weight matrix. Instead, this is more like treating one of the input vectors as a  weight matrix that you&#8217;ll multiply the other output vector by. But unlike the weight matrices in a typical RNN, this &#8220;weight matrix&#8221; vector is determined by a computation somewhere else in the network, so it&#8217;s determined when the new data is processed, rather than fixed throughout the evaluation phase.</p>\n<p>This &#8220;weight matrix&#8221; vector is in many ways not as impressive as one of the built-in weight matrices in a typical RNN. It&#8217;s equivalent to a matrix with the values of the vector along the diagonals, and the rest of the entries equal to zero. So it can&#8217;t do anything fancy to the other vector. Instead, you should think of it as a sort of filter that decides what parts of the other vector are important. In particular, if the &#8220;weight matrix&#8221; vector is zero in a given dimension then the result of multiplication will be zero in that dimension, no matter what the value was for that dimension in the other vector. If it&#8217;s close to 1, the output value is exactly equal to the value of the other vector in that dimension. (And a non-linear transformation is often applied to make sure the &#8220;weight matrix&#8221; values are very close to either 0 or 1.) So the &#8220;weight matrix&#8221; vector chooses what parts of a second vector get passed on to the next step. Because of this, the nodes in the computation graph where a &#8220;weight matrix&#8221; vector gets multiplied by a data vector is often called a <em>gate</em>.</p>\n<p>An LSTM uses this fancy new fourth operation to create three gates, illustrated in the Figure below. This shows the inside of a single cell in an LSTM, and we&#8217;ll see farther down how this cell gets hooked up on the outside.</p>\n<p>The LSTM cell has two inputs and two outputs. The output at the top (labeled <em>out</em>) is the actual RNN output, i.e. the output vector that you will use to evaluate and train the network. The output on the right (labeled <em>mem</em>) is the &#8220;memory&#8221; output, a vector that the LSTM wants to record for the next step. Similarly, the input on the bottom (labeled <em>in</em>) is the same as the input to a standard RNN, i.e. the next input vector in the sequence. The input on the right (also labeled <em>mem</em>) is the &#8220;memory&#8221; vector that was output from the LSTM cell during the previous step in the sequence. You should think of the LSTM as using the new input to update the value of the memory vector before passing it on to the next step, then using the new memory value to generate the actual output for the step.</p>\n<p><img data-attachment-id=\"1631\" data-permalink=\"https://shapeofdata.wordpress.com/2016/06/04/lstms/lstm/\" data-orig-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm.png\" data-orig-size=\"414,378\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"LSTM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=300\" data-large-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=414\" class=\" size-full wp-image-1631 alignleft\" src=\"https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=640\" alt=\"LSTM\" srcset=\"https://shapeofdata.files.wordpress.com/2016/06/lstm.png 414w, https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=150 150w, https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=300 300w\" sizes=\"(max-width: 414px) 100vw, 414px\"   />When a new input vector comes in through the bottom of the LSTM, it is multiplied by the weight matrix <img src=\"https://s0.wp.com/latex.php?latex=M_%7Bin%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_%7Bin%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_%7Bin%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_{in}\" class=\"latex\" />, which puts it into the same form as the memory vector. We want to combine this with the memory vector using gates like we described above. So the input vector is also concatenated with the previous cycle&#8217;s memory vector, and this is multiplied by three different weight matrices: <img src=\"https://s0.wp.com/latex.php?latex=M_R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_R&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_R\" class=\"latex\" /> controls what the cell &#8220;remembers&#8221; about the input, <img src=\"https://s0.wp.com/latex.php?latex=M_F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_F&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_F\" class=\"latex\" /> controls what the cell &#8220;forgets&#8221; from memory and <img src=\"https://s0.wp.com/latex.php?latex=M_%7Bout%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_%7Bout%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_%7Bout%7D&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_{out}\" class=\"latex\" /> controls what part of the current memory is output at the top. The results of each multiplication are fed through a non-linear transformation that I didn&#8217;t include in the Figure.</p>\n<p>Then these vectors are fed into gates, defined by our new network operation (indicated by a circle with a dot in it) as shown in the Figure. The middle gate filters the memory vector from the previous step and the bottom gate filters the transformed input vector. These two gated vectors are then added together to produce the memory vector for this step. In addition to becoming the memory vector that is sent to the next step in the LSTM, the memory vector is also filtered by the top gate to produce the actual output from the LSTM.</p>\n<p>The key step in this process is how the memory vector and the transformed input vector are independently gated before being added together. In the simplest setup, each of the &#8220;weight matrix&#8221; vectors would have all their values 0 or 1, and these would be complementary between the two gates, so that each dimension gets the value from one or the other. The values, which are calculated from both the current input and the current memory vector, would thus determine which of the dimensions in the memory vector should be passed on to the next step, and which should be replaced with the corresponding value from the transformed input vector. But in practice, the network gets to &#8220;learn&#8221; whatever behavior is most effective for producing the desired output patterns, so it may be much more complex.</p>\n<p>Speaking of which, lets quickly look at how an LSTM is trained, using the idea of <a href=\"https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/\">unrolling</a> that I described in my previous post. As you probably picked up from the discussion above, we hook up an LSTM externally by adding an edge from the right memory out, back around to the left memory in. This is shown on the left in the Figure below. This edge is a bit unwieldy to draw, wrapping around behind the cell. But once you unroll the network, as on the right of the Figure, it forms a nice, neat, horizontal edge from each step in the LSTM to the next.</p>\n<p><img data-attachment-id=\"1656\" data-permalink=\"https://shapeofdata.wordpress.com/2016/06/04/lstms/lstm_outside/\" data-orig-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png\" data-orig-size=\"524,168\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"LSTM_outside\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png?w=300\" data-large-file=\"https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png?w=524\" class=\" size-full wp-image-1656 aligncenter\" src=\"https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png?w=640\" alt=\"LSTM_outside\" srcset=\"https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png 524w, https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png?w=150 150w, https://shapeofdata.files.wordpress.com/2016/06/lstm_outside.png?w=300 300w\" sizes=\"(max-width: 524px) 100vw, 524px\"   /></p>\n<p>As with a standard RNN, you can use unrolling to understand the training process, by feeding the whole sequence of inputs to the network at once, and using back propagation to update the weight matrices based on the desired sequence of outputs.</p>\n<p>Note that LSTMs are fairly &#8220;shallow&#8221; as neural networks go, i.e. there aren&#8217;t that many layers of neurons. In fact, if you ignore the gates, there is a single lonely weight matrix between the input vector and the output vector. Of course, there are a number of ways one could modify an LSTM to make it more flexible, and a number of people have experimented with &#8220;deep LSTMs&#8221;. But for now, I&#8217;ll leave that for the readers&#8217; imaginations, or maybe a future post.</p>\n",
  "wfw:commentRss": "https://shapeofdata.wordpress.com/2016/06/04/lstms/feed/",
  "slash:comments": 5,
  "media:content": [
    {
      "media:title": "jessejohnson"
    },
    {
      "media:title": "LSTM"
    },
    {
      "media:title": "LSTM_outside"
    }
  ]
}