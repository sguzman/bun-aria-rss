{
  "title": "Training the Transformer Model",
  "link": "https://machinelearningmastery.com/training-the-transformer-model/",
  "comments": "https://machinelearningmastery.com/training-the-transformer-model/#comments",
  "dc:creator": "Stefania Cristina",
  "pubDate": "Wed, 12 Oct 2022 16:42:23 +0000",
  "category": [
    "Attention",
    "attention",
    "natural language processing",
    "training",
    "transformer"
  ],
  "guid": "https://35.82.237.216/?p=13585",
  "description": "<p>Last Updated on November 2, 2022 We have put together the complete Transformer model, and now we are ready to train it for neural machine translation. We shall use a training dataset for this purpose, which contains short English and German sentence pairs. We will also revisit the role of masking in computing the accuracy [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/training-the-transformer-model/\">Training the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "content:encoded": "<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13585 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Training+the+Transformer+Model&url=https://machinelearningmastery.com/training-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Training+the+Transformer+Model&url=https://machinelearningmastery.com/training-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/training-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/training-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p id=\"last-modified-info\">Last Updated on November 2, 2022</p>\n<p>We have put together the <a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking\">complete Transformer model</a>, and now we are ready to train it for neural machine translation. We shall use a training dataset for this purpose, which contains short English and German sentence pairs. We will also revisit the role of masking in computing the accuracy and loss metrics during the training process.<span class=\"Apple-converted-space\"> </span></p>\n<p>In this tutorial, you will discover how to train the Transformer model for neural machine translation.<span class=\"Apple-converted-space\"> </span></p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>How to prepare the training dataset</li>\n<li>How to apply a padding mask to the loss and accuracy computations</li>\n<li>How to train the Transformer model</li>\n</ul>\n<p>Let’s get started.<span class=\"Apple-converted-space\"> </span></p>\n<div id=\"attachment_13586\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><a href=\"http://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-scaled.jpg\"><img aria-describedby=\"caption-attachment-13586\" loading=\"lazy\" class=\"wp-image-13586 size-large\" src=\"http://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-1024x709.jpg\" alt=\"\" width=\"1024\" height=\"709\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-1024x709.jpg 1024w, https://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-300x208.jpg 300w, https://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-768x532.jpg 768w, https://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-1536x1063.jpg 1536w, https://machinelearningmastery.com/wp-content/uploads/2022/05/training_cover-2048x1417.jpg 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><p id=\"caption-attachment-13586\" class=\"wp-caption-text\">Training the transformer model<br />Photo by <a href=\"https://unsplash.com/photos/PGExULGintM\">v2osk</a>, some rights reserved.</p></div>\n<h2><b>Tutorial Overview</b></h2>\n<p>This tutorial is divided into four parts; they are:</p>\n<ul>\n<li>Recap of the Transformer Architecture</li>\n<li>Preparing the Training Dataset</li>\n<li>Applying a Padding Mask to the Loss and Accuracy Computations</li>\n<li>Training the Transformer Model</li>\n</ul>\n<h2><b>Prerequisites</b></h2>\n<p>For this tutorial, we assume that you are already familiar with:</p>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/the-transformer-model/\">The theory behind the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking\">An implementation of the Transformer model</a></li>\n</ul>\n<h2><b>Recap of the Transformer Architecture</b></h2>\n<p><a href=\"https://machinelearningmastery.com/the-transformer-model/\">Recall</a> having seen that the Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.</p>\n<div id=\"attachment_12821\" style=\"width: 379px\" class=\"wp-caption aligncenter\"><a href=\"http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"><img aria-describedby=\"caption-attachment-12821\" loading=\"lazy\" class=\"wp-image-12821\" src=\"http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" alt=\"\" width=\"369\" height=\"520\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png 727w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-213x300.png 213w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png 768w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-1090x1536.png 1090w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png 1320w\" sizes=\"(max-width: 369px) 100vw, 369px\" /></a><p id=\"caption-attachment-12821\" class=\"wp-caption-text\">The encoder-decoder structure of the Transformer architecture <br />Taken from &#8220;<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>&#8220;</p></div>\n<p>In generating an output sequence, the Transformer does not rely on recurrence and convolutions.</p>\n<p>You have seen how to implement the complete Transformer model, so you can now proceed to train it for neural machine translation.<span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s start first by preparing the dataset for training.<span class=\"Apple-converted-space\"> </span></p>\n<p><strong>Kick-start your project</strong> with my book <a href=\"https://machinelearningmastery.com/transformer-models-with-attention/\">Building Transformer Models with Attention</a>. It provides <strong>self-study tutorials</strong> with <strong>working code</strong> to guide you into building a fully-working transformer models that can<br><em>translate sentences from one language to another</em>...</p>\n<h2><b>Preparing the Training Dataset</b></h2>\n<p>For this purpose, you can refer to a previous tutorial that covers material about <a href=\"https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\">preparing the text data</a> for training.<span class=\"Apple-converted-space\"> </span></p>\n<p>You will also use a dataset that contains short English and German sentence pairs, which you may download <a href=\"https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl\">here</a>. This particular dataset has already been cleaned by removing non-printable and non-alphabetic characters and punctuation characters, further normalizing all Unicode characters to ASCII, and changing all uppercase letters to lowercase ones. Hence, you can skip the cleaning step, which is typically part of the data preparation process. However, if you use a dataset that does not come readily cleaned, you can refer to this <a href=\"https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\">this previous tutorial</a> to learn how to do so.<span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s proceed by creating the <code>PrepareDataset</code> class that implements the following steps:</p>\n<ul>\n<li>Loads the dataset from a specified filename.<span class=\"Apple-converted-space\"> </span></li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">clean_dataset = load(open(filename, 'rb'))</pre><p></p>\n<ul>\n<li>Selects the number of sentences to use from the dataset. Since the dataset is large, you will reduce its size to limit the training time. However, you may explore using the full dataset as an extension to this tutorial.</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">dataset = clean_dataset[:self.n_sentences, :]</pre><p></p>\n<ul>\n<li>Appends start (<START>) and end-of-string (<EOS>) tokens to each sentence. For example, the English sentence, <code>i like to run</code>, now becomes, <code><START> i like to run <EOS></code>. This also applies to its corresponding translation in German, <code>ich gehe gerne joggen</code>, which now becomes, <code><START> ich gehe gerne joggen <EOS></code>.</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">for i in range(dataset[:, 0].size):\n\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"</pre><p></p>\n<ul>\n<li>Shuffles the dataset randomly.<span class=\"Apple-converted-space\"> </span></li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">shuffle(dataset)</pre><p></p>\n<ul>\n<li>Splits the shuffled dataset based on a pre-defined ratio.</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">train = dataset[:int(self.n_sentences * self.train_split)]</pre><p></p>\n<ul>\n<li>Creates and trains a tokenizer on the text sequences that will be fed into the encoder and finds the length of the longest sequence as well as the vocabulary size.<span class=\"Apple-converted-space\"> </span></li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">enc_tokenizer = self.create_tokenizer(train[:, 0])\nenc_seq_length = self.find_seq_length(train[:, 0])\nenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])</pre><p></p>\n<ul>\n<li>Tokenizes the sequences of text that will be fed into the encoder by creating a vocabulary of words and replacing each word with its corresponding vocabulary index. The <START> and <EOS> tokens will also form part of this vocabulary. Each sequence is also padded to the maximum phrase length. <span class=\"Apple-converted-space\"> </span></li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\ntrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\ntrainX = convert_to_tensor(trainX, dtype=int64)</pre><p></p>\n<ul>\n<li>Creates and trains a tokenizer on the text sequences that will be fed into the decoder, and finds the length of the longest sequence as well as the vocabulary size.</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">dec_tokenizer = self.create_tokenizer(train[:, 1])\ndec_seq_length = self.find_seq_length(train[:, 1])\ndec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])</pre><p></p>\n<ul>\n<li>Repeats a similar tokenization and padding procedure for the sequences of text that will be fed into the decoder.</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\ntrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\ntrainY = convert_to_tensor(trainY, dtype=int64)</pre><p>The complete code listing is as follows (refer to <a href=\"https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\">this previous tutorial</a> for further details):</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from pickle import load\nfrom numpy.random import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64\n\n\nclass PrepareDataset:\n\tdef __init__(self, **kwargs):\n\t\tsuper(PrepareDataset, self).__init__(**kwargs)\n\t\tself.n_sentences = 10000  # Number of sentences to include in the dataset\n\t\tself.train_split = 0.9  # Ratio of the training data split\n\n\t# Fit a tokenizer\n\tdef create_tokenizer(self, dataset):\n\t\ttokenizer = Tokenizer()\n\t\ttokenizer.fit_on_texts(dataset)\n\n\t\treturn tokenizer\n\n\tdef find_seq_length(self, dataset):\n\t\treturn max(len(seq.split()) for seq in dataset)\n\n\tdef find_vocab_size(self, tokenizer, dataset):\n\t\ttokenizer.fit_on_texts(dataset)\n\n\t\treturn len(tokenizer.word_index) + 1\n\n\tdef __call__(self, filename, **kwargs):\n\t\t# Load a clean dataset\n\t\tclean_dataset = load(open(filename, 'rb'))\n\n\t\t# Reduce dataset size\n\t\tdataset = clean_dataset[:self.n_sentences, :]\n\n\t\t# Include start and end of string tokens\n\t\tfor i in range(dataset[:, 0].size):\n\t\t\tdataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n\t\t\tdataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n\n\t\t# Random shuffle the dataset\n\t\tshuffle(dataset)\n\n\t\t# Split the dataset\n\t\ttrain = dataset[:int(self.n_sentences * self.train_split)]\n\n\t\t# Prepare tokenizer for the encoder input\n\t\tenc_tokenizer = self.create_tokenizer(train[:, 0])\n\t\tenc_seq_length = self.find_seq_length(train[:, 0])\n\t\tenc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n\n\t\t# Encode and pad the input sequences\n\t\ttrainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n\t\ttrainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n\t\ttrainX = convert_to_tensor(trainX, dtype=int64)\n\n\t\t# Prepare tokenizer for the decoder input\n\t\tdec_tokenizer = self.create_tokenizer(train[:, 1])\n\t\tdec_seq_length = self.find_seq_length(train[:, 1])\n\t\tdec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n\n\t\t# Encode and pad the input sequences\n\t\ttrainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n\t\ttrainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n\t\ttrainY = convert_to_tensor(trainY, dtype=int64)\n\n\t\treturn trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size</pre><p>Before moving on to train the Transformer model, let’s first have a look at the output of the <code>PrepareDataset</code> class corresponding to the first sentence in the training dataset:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Prepare the training data\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n\nprint(train_orig[0, 0], '\\n', trainX[0, :])</pre><p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\"><START> did tom tell you <EOS> \n tf.Tensor([ 1 25  4 97  5  2  0], shape=(7,), dtype=int64)</pre><p>(Note: Since the dataset has been randomly shuffled, you will likely see a different output.)</p>\n<p>You can see that, originally, you had a three-word sentence (<i>did tom tell you</i>) to which you appended the start and end-of-string tokens. Then you proceeded to vectorize (you may notice that the <START> and <EOS> tokens are assigned the vocabulary indices 1 and 2, respectively). The vectorized text was also padded with zeros, such that the length of the end result matches the maximum sequence length of the encoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">print('Encoder sequence length:', enc_seq_length)</pre><p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Encoder sequence length: 7</pre><p>You can similarly check out the corresponding target data that is fed into the decoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">print(train_orig[0, 1], '\\n', trainY[0, :])</pre><p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\"><START> hat tom es dir gesagt <EOS> \n tf.Tensor([  1  14   5   7  42 162   2   0   0   0   0   0], shape=(12,), dtype=int64)</pre><p>Here, the length of the end result matches the maximum sequence length of the decoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">print('Decoder sequence length:', dec_seq_length)</pre><p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Decoder sequence length: 12</pre><p></p>\n<h2><b>Applying a Padding Mask to the Loss and Accuracy Computations</b></h2>\n<p><a href=\"https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras\">Recall</a> seeing that the importance of having a padding mask at the encoder and decoder is to make sure that the zero values that we have just appended to the vectorized inputs are not processed along with the actual input values.<span class=\"Apple-converted-space\"> </span></p>\n<p>This also holds true for the training process, where a padding mask is required so that the zero padding values in the target data are not considered in the computation of the loss and accuracy.</p>\n<p>Let’s have a look at the computation of loss first.<span class=\"Apple-converted-space\"> </span></p>\n<p>This will be computed using a sparse categorical cross-entropy loss function between the target and predicted values and subsequently multiplied by a padding mask so that only the valid non-zero values are considered. The returned loss is the mean of the unmasked values:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)</pre><p>For the computation of accuracy, the predicted and target values are first compared. The predicted output is a tensor of size (<i>batch_size</i>, <i>dec_seq_length</i>, <i>dec_vocab_size</i>) and contains probability values (generated by the softmax function on the decoder side) for the tokens in the output. In order to be able to perform the comparison with the target values, only each token with the highest probability value is considered, with its dictionary index being retrieved through the operation: <code>argmax(prediction, axis=2)</code>. Following the application of a padding mask, the returned accuracy is the mean of the unmasked values:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(math.equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)</pre><p></p>\n<h2><b>Training the Transformer Model</b></h2>\n<p>Let’s first define the model and training parameters as specified by <a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al. (2017)</a>:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 2\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1</pre><p>(Note: Only consider two epochs to limit the training time. However, you may explore training the model further as an extension to this tutorial.)</p>\n<p>You also need to implement a learning rate scheduler that initially increases the learning rate linearly for the first <i>warmup_steps</i> and then decreases it proportionally to the inverse square root of the step number. Vaswani et al. express this by the following formula:<span class=\"Apple-converted-space\"> </span></p>\n<p>$$\\text{learning_rate} = \\text{d_model}^{−0.5} \\cdot \\text{min}(\\text{step}^{−0.5}, \\text{step} \\cdot \\text{warmup_steps}^{−1.5})$$</p>\n<p>&nbsp;</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">class LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)</pre><p>An instance of the <code>LRScheduler</code> class is subsequently passed on as the <code>learning_rate</code> argument of the Adam optimizer:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)</pre><p>Next,  split the dataset into batches in preparation for training:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)</pre><p>This is followed by the creation of a model instance:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)</pre><p>In training the Transformer model, you will write your own training loop, which incorporates the loss and accuracy functions that were implemented earlier.<span class=\"Apple-converted-space\"> </span></p>\n<p>The default runtime in Tensorflow 2.0 is <i>eager execution</i>, which means that operations execute immediately one after the other. Eager execution is simple and intuitive, making debugging easier. Its downside, however, is that it cannot take advantage of the global performance optimizations that run the code using the <i>graph execution</i>. In graph execution, a graph is first built before the tensor computations can be executed, which gives rise to a computational overhead. For this reason, the use of graph execution is mostly recommended for large model training rather than for small model training, where eager execution may be more suited to perform simpler operations. Since the Transformer model is sufficiently large, apply the graph execution to train it.<span class=\"Apple-converted-space\"> </span></p>\n<p>In order to do so, you will use the <code>@function</code> decorator as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)</pre><p>With the addition of the <code>@function</code> decorator, a function that takes tensors as input will be compiled into a graph. If the <code>@function</code> decorator is commented out, the function is, alternatively, run with eager execution.<span class=\"Apple-converted-space\"> </span></p>\n<p>The next step is implementing the training loop that will call the <code>train_step</code> function above. The training loop will iterate over the specified number of epochs and the dataset batches. For each batch, the <code>train_step</code> function computes the training loss and accuracy measures and applies the optimizer to update the trainable model parameters. A checkpoint manager is also included to save a checkpoint after every five epochs:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">train_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n           \n    # Print epoch number and loss value at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n\n    # Save a checkpoint after every five epochs\n    if (epoch + 1) % 5 == 0:\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))</pre><p>An important point to keep in mind is that the input to the decoder is offset by one position to the right with respect to the encoder input. The idea behind this offset, combined with a look-ahead mask in the first multi-head attention block of the decoder, is to ensure that the prediction for the current token can only depend on the previous tokens.<span class=\"Apple-converted-space\"> </span></p>\n<blockquote><p><i>This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</i></p>\n<p style=\"text-align: right;\"><i>– </i><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017.<span class=\"Apple-converted-space\"> </span></p>\n</blockquote>\n<p>It is for this reason that the encoder and decoder inputs are fed into the Transformer model in the following manner:</p>\n<p style=\"text-align: center;\"><code>encoder_input = train_batchX[:, 1:]</code></p>\n<p style=\"text-align: center;\"><code>decoder_input = train_batchY[:, :-1]</code></p>\n<p>Putting together the complete code listing produces the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\nfrom keras.losses import sparse_categorical_crossentropy\nfrom model import TransformerModel\nfrom prepare_dataset import PrepareDataset\nfrom time import time\n\n\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 2\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n\n\n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n\n\n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n\n# Prepare the training and test splits of the dataset\ndataset = PrepareDataset()\ntrainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n\n# Prepare the dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n\n\n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n\n\n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n\n\n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n\n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)\n\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    start_time = time()\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n\n    # Print epoch number and loss value at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n\n    # Save a checkpoint after every five epochs\n    if (epoch + 1) % 5 == 0:\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n\nprint(\"Total time taken: %.2fs\" % (time() - start_time))</pre><p>Running the code produces a similar output to the following (you will likely see different loss and accuracy values because the training is from scratch, whereas the training time depends on the computational resources that you have available for training):</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Start of epoch 1\nEpoch 1 Step 0 Loss 8.4525 Accuracy 0.0000\nEpoch 1 Step 50 Loss 7.6768 Accuracy 0.1234\nEpoch 1 Step 100 Loss 7.0360 Accuracy 0.1713\nEpoch 1: Training Loss 6.7109, Training Accuracy 0.1924\n\nStart of epoch 2\nEpoch 2 Step 0 Loss 5.7323 Accuracy 0.2628\nEpoch 2 Step 50 Loss 5.4360 Accuracy 0.2756\nEpoch 2 Step 100 Loss 5.2638 Accuracy 0.2839\nEpoch 2: Training Loss 5.1468, Training Accuracy 0.2908\nTotal time taken: 87.98s</pre><p>It takes 155.13s for the code to run using eager execution alone on the same platform that is making use of only a CPU, which shows the benefit of using graph execution.<span class=\"Apple-converted-space\"> </span></p>\n<h2><b>Further Reading</b></h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3><b>Books</b></h3>\n<ul>\n<li><a href=\"https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X\">Advanced Deep Learning with Python</a>, 2019</li>\n<li><a href=\"https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798\">Transformers for Natural Language Processing</a>, 2021</li>\n</ul>\n<h3><b>Papers</b></h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017</li>\n</ul>\n<h3><b>Websites</b></h3>\n<ul>\n<li>Writing a training loop from scratch in Keras: <a href=\"https://keras.io/guides/writing_a_training_loop_from_scratch/\">https://keras.io/guides/writing_a_training_loop_from_scratch/</a></li>\n</ul>\n<h2><b>Summary</b></h2>\n<p>In this tutorial, you discovered how to train the Transformer model for neural machine translation.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How to prepare the training dataset</li>\n<li>How to apply a padding mask to the loss and accuracy computations</li>\n<li>How to train the Transformer model</li>\n</ul>\n<p>Do you have any questions?<br />\nAsk your questions in the comments below, and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13585 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Training+the+Transformer+Model&url=https://machinelearningmastery.com/training-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Training+the+Transformer+Model&url=https://machinelearningmastery.com/training-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/training-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/training-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/training-the-transformer-model/\">Training the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "wfw:commentRss": "https://machinelearningmastery.com/training-the-transformer-model/feed/",
  "slash:comments": 7
}