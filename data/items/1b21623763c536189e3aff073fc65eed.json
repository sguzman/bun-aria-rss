{
  "title": "Honest calibration assessment for binary outcome predictions. (arXiv:2203.04065v2 [math.ST] UPDATED)",
  "link": "http://arxiv.org/abs/2203.04065",
  "description": "<p>Probability predictions from binary regressions or machine learning methods\nought to be calibrated: If an event is predicted to occur with probability $x$,\nit should materialize with approximately that frequency, which means that the\nso-called calibration curve $p(\\cdot)$ should equal the identity, $p(x) = x$\nfor all $x$ in the unit interval. We propose honest calibration assessment\nbased on novel confidence bands for the calibration curve, which are valid only\nsubject to the natural assumption of isotonicity. Besides testing the classical\ngoodness-of-fit null hypothesis of perfect calibration, our bands facilitate\ninverted goodness-of-fit tests whose rejection allows for the sought-after\nconclusion of a sufficiently well specified model. We show that our bands have\na finite sample coverage guarantee, are narrower than existing approaches, and\nadapt to the local smoothness of the calibration curve $p$ and the local\nvariance of the binary observations. In an application to model predictions of\nan infant having a low birth weight, the bounds give informative insights on\nmodel calibration.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Dimitriadis_T/0/1/0/all/0/1\">Timo Dimitriadis</a>, <a href=\"http://arxiv.org/find/math/1/au:+Duembgen_L/0/1/0/all/0/1\">Lutz Duembgen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Henzi_A/0/1/0/all/0/1\">Alexander Henzi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Puke_M/0/1/0/all/0/1\">Marius Puke</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ziegel_J/0/1/0/all/0/1\">Johanna Ziegel</a>"
}