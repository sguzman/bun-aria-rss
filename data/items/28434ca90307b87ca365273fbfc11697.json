{
  "title": "How To Find Simple And Interesting Multi-Gigabytes Data Set",
  "link": "https://fullstackml.com/how-to-find-simple-and-interesting-multi-gigabytes-data-set-f7d9b42f525?source=rss----46e065078cc1---4",
  "guid": "https://medium.com/p/f7d9b42f525",
  "category": [
    "dataset",
    "big-data"
  ],
  "dc:creator": "Dmitry Petrov",
  "pubDate": "Sun, 18 Oct 2015 01:47:00 GMT",
  "atom:updated": "2017-03-06T05:55:25.922Z",
  "content:encoded": "<p>Many folks are very exited about big data. They like play, explore, work and study this frontier. Most likely these folks either work with or would like to play with large amount of data (hundreds of gigabytes or even terabytes). But here’s the thing, it’s not easy to find a multi-gigabytes dataset. Usually, these kinds of datasets are needed for experimentating with new data processing framework such as Apache Spark or data streaming tools like Apache Kafka. In this blog post I will describe and provide a link to simple and a powerful multi-gigabytes stackoverflow data set.</p><h3>1. Datasets for machine learning</h3><p>Lots of sources exist for machine learning problems. Kaggle is the best source for these problems and they offer lots of datasets presented with examples of code. <strong>Most of these data sets are clean and ready to use in your machine learning experiments.</strong></p><p>In a real data scientist’s life most likely you do not have the luxury of clean data and the size of the input data creates an additional big problem. University courses as well as online courses offer a limited viewpoint on data science and machine learning due to the fact they teach student to apply statistical and machine learning methods to a small amount of clean data. <strong>In reality, a data scientist spends the majority part of time by getting data and cleaning up that data.</strong> According to Hal Varian (Google’s chief economist) <a href=\"http://www.nytimes.com/2009/08/06/technology/06stats.html?_r=0\">“the sexiest job of the 21st century”</a> belongs to Statisticians (and I assume to Data Scientists). However, they perform “clean up” work most of the time.</p><p>In order to experiment with new data processing or data streaming tools, you need a large (larger than your computer can hold in memory) and an uncleaned datasets.</p><p>Large and uncleanrf datasets will allow you to get actual data processing or learn analytical skills. It turns out that this is not that easy to find.</p><h3>2. Datasets for processing</h3><p>Kdnuggets and Quora have pretty good lists of open repositories:</p><ol><li><a href=\"http://www.kdnuggets.com/datasets/index.html\">http://www.kdnuggets.com/datasets/index.html</a></li><li><a href=\"https://www.quora.com/What-kinds-of-large-datasets-open-to-the-public-do-you-analyze-the-mostly\">https://www.quora.com/What-kinds-of-large-datasets-open-to-the-public-do-you-analyze-the-mostly</a></li></ol><p>Most of these datasets from these lists are <strong>very small in size</strong> and for the most part, <strong>you need specific knowledge from a dataset specific business domain such as physics or healthcare.</strong> However, for learning and experimentation purposes, it would be nice to have a dataset from a well known business domain that all people are familiar with.</p><p><strong>Social network data is the best because people understand these datasets and they have intuition about the data</strong> which is important in the analytic process. You might use a social network API to extract your data sets. Unfortunately, your data set is not the best for sharing your analytical results with other people. It would be great to find a common social network dataset with an open license. And I’ve found one!</p><h3>3. Stackoverflow open dataset</h3><p>Stackoverflow data set is the only social open dataset that I was able to find. Stackoverflow.com is a question and answers web site about programming. This web site is especially useful when you have to write a code in a language you are not familiar with. This well known approach is called — stackoverflow driven development or SDD. I believe all people from the high-tech industry are familiar with stackoverflow and many of them have an account for this web site.</p><p>Stack Exchange Company (owner of stackoverflow.com) publishes stackexchange dataset under an open creative common license. You might find the freshest dataset on this page:</p><p><a href=\"https://archive.org/details/stackexchange\">Stack Exchange Data Dump : Stack Exchange, Inc. : Free Download, Borrow, and Streaming : Internet Archive</a></p><p>The dataset contains all stackexchange data including stackoverflow and the overall <strong>size of the archive is 27 gigabytes</strong>. <strong>The size of the uncompressed data is more than 1 terabyte.</strong></p><h3>4. How to download and extract the dataset?</h3><p><strong>However, this dataset is not easy to get.</strong> First, you need to upload the archive of the entire dataset. Please note that <strong>the downloading speed is very slow.</strong> They recommend using a bittorrent client to download the archive but often it has some issues. Without the bittorent, I made 3 attempts and spent 2 days to download this archive. Next, you need to<strong> unzip the large archive</strong>. Finally, you need to unzip the subset of data that you need (like stackoverflow-Posts or travel.stackexchange) using the<strong> 7z compressor</strong>. If you don’t have the 7z compressor, you need to find and install it to your machine.</p><p>After you download the archive from <a href=\"https://archive.org/details/stackexchange\">https://archive.org/details/stackexchange</a> extract all stackoverflow related archives and uncompress each of them (all archives which starts with stackovervlow.com):</p><ul><li>stackovervlow.com-Posts.7z</li><li>stackovervlow.com-PostsHistory.7z</li><li>stackovervlow.com-Comments.7z</li><li>stackovervlow.com-Badges.7z</li><li>stackovervlow.com-PostLinks.7z</li><li>stackovervlow.com-Tags.7z</li><li>stackovervlow.com-Users.7z</li><li>stackovervlow.com-Votes.7z</li></ul><p>As a result you will see a set of xml files with the same names.</p><h3>5. How to use the dataset?</h3><p>Let’s experiment with the dataset. <strong>The most interesting file is Posts.xml. This file contains 34Gb of uncompressed data,</strong> approximately 70% is Body text which is a text of questions from the web site. This amount of data, most likely, does not fit your memory. We might use an in-disk data manipulation or machine learning technology. This is a good chance to use Apache Spark and MLLib or your custom solution.</p><p>Let’s take a look how this stackoverflow question will look like in the file.</p><figure><img alt=\"Stackowerflow example\" src=\"https://cdn-images-1.medium.com/max/660/0*ZYMF7QS97u5Wuydt.\" /><figcaption>Stackowerflow example</figcaption></figure><p>In the file this post is presented by one single row. Note that because the text is HTML — the opening and closing p tags (<p> and </p>) are written as &lt;p&gt; and &lt;/p&gt; respectively.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5afed51e539799676cf964d49a2f400a/href\">https://medium.com/media/5afed51e539799676cf964d49a2f400a/href</a></iframe><p>I’ll provide Apache Spark code examples with this data set in the next blog post. My scenario will include two parts: preparing data or data manipulation and machine learning part. Both of these part I’ll use multi-gigabytes dataset as an input.</p><h3>Conclusion</h3><p>Stackoverflow dataset (<a href=\"https://archive.org/details/stackexchange\">https://archive.org/details/stackexchange</a>) is probably the simplest and most interesting open multi-gigabytes dataset you can find which fits machine learning, data processing scenarios and data streaming. <strong>Please share if you have any information about other simple open big dataset resources.</strong> This should help the community a lot.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f7d9b42f525\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://fullstackml.com/how-to-find-simple-and-interesting-multi-gigabytes-data-set-f7d9b42f525\">How To Find Simple And Interesting Multi-Gigabytes Data Set</a> was originally published in <a href=\"https://fullstackml.com\">FullStackML</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}