{
  "title": "Dynamic programming in reinforcement learning",
  "link": "",
  "published": "2020-03-28T12:00:00-07:00",
  "updated": "2020-03-28T12:00:00-07:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2020-03-28:/reinforcement-learning-dynamic-programming",
  "summary": "<h2>Background</h2>\n<p>We discuss how to use dynamic programming (<span class=\"caps\">DP</span>) to solve reinforcement learning (<span class=\"caps\">RL</span>) problems where we have a perfect model of the environment.  <span class=\"caps\">DP</span> is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&nbsp;problem …</p>",
  "content": "<h2>Background</h2>\n<p>We discuss how to use dynamic programming (<span class=\"caps\">DP</span>) to solve reinforcement learning (<span class=\"caps\">RL</span>) problems where we have a perfect model of the environment.  <span class=\"caps\">DP</span> is a general approach to solving problems by breaking them into subproblems that can be solved separately, cached, then combined to solve the overall&nbsp;problem.</p>\n<p>We’ll use a toy model, taken from [1], of a student transitioning between five states in college, which we also used in our <a href=\"https://efavdb.com/intro-rl-toy-example.html\">introduction</a> to <span class=\"caps\">RL</span>:</p>\n<p><img alt=\"student MDP\" src=\"https://efavdb.com/images/student_mdp.png\"></p>\n<p>The model (dynamics) of the environment describe the probabilities of receiving a reward <span class=\"math\">\\(r\\)</span> in the next state <span class=\"math\">\\(s'\\)</span> given the current state <span class=\"math\">\\(s\\)</span> and action <span class=\"math\">\\(a\\)</span> taken, <span class=\"math\">\\(p(s’, r | s, a)\\)</span>.  We can read these dynamics off the diagram of the student Markov Decision Process (<span class=\"caps\">MDP</span>), for&nbsp;example:</p>\n<p><span class=\"math\">\\(p(s'=\\text{CLASS2}, r=-2 | s=\\text{CLASS1}, a=\\text{study}) =&nbsp;1.0\\)</span></p>\n<p><span class=\"math\">\\(p(s'=\\text{CLASS2}, r=1 | s=\\text{CLASS3}, a=\\text{pub}) =&nbsp;0.4\\)</span></p>\n<p>If you&#8217;d like to jump straight to code, see this <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb\">jupyter notebook</a>.</p>\n<h3>The role of value functions in <span class=\"caps\">RL</span></h3>\n<p>The agent’s (student’s) policy maps states to actions, <span class=\"math\">\\(\\pi(a|s) := p(a|s)\\)</span>. \nThe goal is to find the optimal policy <span class=\"math\">\\(\\pi_*\\)</span> that will maximize the expected cumulative rewards, the discounted return <span class=\"math\">\\(G_t\\)</span>, in each state <span class=\"math\">\\(s\\)</span>.</p>\n<p>The value functions, <span class=\"math\">\\(v_{\\pi}(s)\\)</span> and <span class=\"math\">\\(q_{\\pi}(s, a)\\)</span>, in MDPs formalize this&nbsp;goal.</p>\n<div class=\"math\">\\begin{eqnarray}\nv_{\\pi}(s) &amp;=&amp; \\mathbb{E}_{\\pi}[G_t | S_t = s] \\\\\nq_{\\pi}(s, a) &amp;=&amp; \\mathbb{E}_{\\pi}[G_t | S_t = s, A_t = a]\n\\end{eqnarray}</div>\n<p>We want to be able to calculate the value function for an arbitrary policy, i.e. <em>prediction</em>, as well as use the value functions to find an optimal policy, i.e. the <em>control</em>&nbsp;problem.</p>\n<h2>Policy&nbsp;evaluation</h2>\n<p>Policy evaluation deals with the problem of calculating the value function for some arbitrary policy.  In our introduction to <span class=\"caps\">RL</span> <a href=\"https://efavdb.com/intro-rl-toy-example.html\">post</a>, we showed that the value functions obey self-consistent, recursive relations, that make them amenable to <span class=\"caps\">DP</span> approaches given a model of the&nbsp;environment.</p>\n<p>These recursive relations are the Bellman expectation equations, which write the value of each state in terms of an average over the values of its successor / neighboring states, along with the expected reward along the&nbsp;way.</p>\n<p>The Bellman expectation equation for <span class=\"math\">\\(v_{\\pi}(s)\\)</span>&nbsp;is</p>\n<div class=\"math\">\\begin{eqnarray}\\label{state-value-bellman} \\tag{1}\nv_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s’, r} p(s’, r | s, a) [r + \\gamma v_{\\pi}(s’) ],\n\\end{eqnarray}</div>\n<p>where <span class=\"math\">\\(\\gamma\\)</span> is the discount factor <span class=\"math\">\\(0 \\leq \\gamma \\leq 1\\)</span> that weights the importance of future vs. current returns. <strong><span class=\"caps\">DP</span> turns (\\ref{state-value-bellman}) into an update rule</strong> (\\ref{policy-evaluation}), <span class=\"math\">\\(\\{v_k(s’)\\} \\rightarrow v_{k+1}(s)\\)</span>, which iteratively converges towards the solution, <span class=\"math\">\\(v_\\pi(s)\\)</span>, for&nbsp;(\\ref{state-value-bellman}):</p>\n<div class=\"math\">\\begin{eqnarray}\\label{policy-evaluation} \\tag{2}\nv_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s’, r} p(s’, r | s, a) [r + \\gamma v_k(s’) ]\n\\end{eqnarray}</div>\n<p>Applying policy evaluation to our student model for an agent with a random policy, we arrive at the following state value function (see <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb\">jupyter notebook</a> for&nbsp;implementation):</p>\n<p><img alt=\"student MDP value function random policy\" src=\"https://efavdb.com/images/student_mdp_values_random_policy.png\"></p>\n<h2>Finding the optimal value functions and&nbsp;policy</h2>\n<h3>Policy&nbsp;iteration</h3>\n<p>We can evaluate the value functions for a given policy by turning the Bellman expectation equation (\\ref{state-value-bellman}) into an update equation with the iterative policy evaluation&nbsp;algorithm.</p>\n<p>But how do we use value functions to achieve our end goal of finding an optimal policy that corresponds to the optimal value&nbsp;functions?</p>\n<p>Imagine we know the value function for a policy.  If taking the greedy action, corresponding to taking <span class=\"math\">\\(\\text{arg} \\max_a q_{\\pi}(s,a)\\)</span>, from any state in that policy is not consistent with that policy, or, equivalently, <span class=\"math\">\\(\\max_a q_{\\pi}(s,a) &gt; v_\\pi(s)\\)</span>, then the policy is not optimal since we can improve the policy by taking the greedy action in that state and then onwards following the original&nbsp;policy.</p>\n<p>The <em>policy iteration</em> algorithm involves taking turns calculating the value function for a policy (policy evaluation) and improving on the policy (policy improvement) by taking the greedy action in each state for that value function until converging to <span class=\"math\">\\(\\pi_*\\)</span> and <span class=\"math\">\\(v_*\\)</span> (see [2] for pseudocode for policy&nbsp;iteration).</p>\n<h3>Value&nbsp;iteration</h3>\n<p>Unlike policy iteration, the value iteration algorithm does not require complete convergence of policy evaluation before policy improvement, and, in fact, makes use of just a single iteration of policy evaluation.  Just as policy evaluation could be viewed as turning the Bellman expectation equation into an update, value iteration turns the Bellman optimality equation into an&nbsp;update.</p>\n<p>In our previous <a href=\"https://efavdb.com/intro-rl-toy-example.html\">post</a> introducing <span class=\"caps\">RL</span> using the student example, we saw that the optimal value functions are the solutions to the Bellman optimality equation, e.g. for the optimal state-value&nbsp;function:</p>\n<div class=\"math\">\\begin{eqnarray}\\label{state-value-bellman-optimality} \\tag{3}\nv_*(s) &amp;=&amp; \\max_a q_{\\pi*}(s, a) \\\\\n    &amp;=&amp; \\max_a \\mathbb{E} [R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\\\\n    &amp;=&amp; \\max_a \\sum_{s’, r} p(s’, r | s, a) [r + \\gamma v_*(s’) ]\n\\end{eqnarray}</div>\n<p>As a <span class=\"caps\">DP</span> update equation, (\\ref{state-value-bellman-optimality})&nbsp;becomes:\n</p>\n<div class=\"math\">\\begin{eqnarray}\\label{value-iteration} \\tag{4}\nv_{k+1}(s) = \\max_a \\sum_{s’, r} p(s’, r | s, a) [r + \\gamma v_k(s’) ]\n\\end{eqnarray}</div>\n<p>Value iteration combines (truncated) policy evaluation with policy improvement in a single step; the state-value functions are updated with the averages of the value functions of the neighbor states that can occur from a greedy action, i.e. the action that maximizes the right hand side of&nbsp;(\\ref{value-iteration}).</p>\n<p>Applying value iteration to our student model, we arrive at the following optimal state value function, with the optimal policy delineated by red arrows (see <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb\">jupyter notebook</a>):</p>\n<p><img alt=\"student MDP optimal policy and value function\" src=\"https://efavdb.com/images/student_mdp_optimal_policy.png\"></p>\n<h2>Summary</h2>\n<p>We’ve discussed how to solve for (a) the value functions of an arbitrary policy, (b) the optimal value functions and optimal policy.  Solving for (a) involves turning the Bellman expectation equations into an update, whereas (b) involves turning the Bellman optimality equations into an update.  These algorithms are guaranteed to converge (see [1] for notes on how the contraction mapping theorem guarantees&nbsp;convergence).</p>\n<p>You can see the application of both policy evaluation and value iteration to the student model problem in this <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/02-dynamic-programming/student_MDP_dynamic_programming_solutions.ipynb\">jupyter notebook</a>.</p>\n<h2><a name=\"References\">References</a></h2>\n<p>[1] David Silver&#8217;s <span class=\"caps\">RL</span> Course Lecture 3 - Planning by Dynamic Programming (<a href=\"https://www.youtube.com/watch?v=Nd1-UUMVfz4\">video</a>,\n  <a href=\"https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf\">slides</a>)</p>\n<p>[2] Sutton and Barto -\n  <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\">Reinforcement Learning: An Introduction</a> - Chapter 4: Dynamic&nbsp;Programming</p>\n<p>[3] Denny Britz’s <a href=\"https://github.com/dennybritz/reinforcement-learning/tree/master/DP\">notes</a> on <span class=\"caps\">RL</span> and <span class=\"caps\">DP</span>, including crisp implementations in code of policy evaluation, policy iteration, and value iteration for the gridworld example discussed in&nbsp;[2].</p>\n<p>[4] Deep <span class=\"caps\">RL</span> Bootcamp Lecture 1: Motivation + Overview + Exact Solution Methods, by Pieter Abbeel (<a href=\"https://www.youtube.com/watch?v=qaMdN6LS9rA\">video</a>, <a href=\"https://drive.google.com/open?id=0BxXI_RttTZAhVXBlMUVkQ1BVVDQ\">slides</a>) - a very compressed&nbsp;intro.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    "",
    ""
  ]
}