{
  "title": "Towards Out-of-core ND-Arrays -- Multi-core Scheduling",
  "link": "",
  "updated": "2015-01-06T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/01/06/Towards-OOC-Scheduling",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr</strong> We show off a multi-threaded shared-memory task scheduler.  We share\ntwo techniques for space-constrained computing.  We end with pretty GIFs.</p>\n\n<p><em>Disclaimer: This post is on experimental buggy code.  This is not ready for public use.</em></p>\n\n<p><em>Disclaimer 2: This post is technical and intended for people who care about\ntask scheduling, not for traditional users.</em></p>\n\n<h2 id=\"setup\">Setup</h2>\n\n<p>My last two posts\n(<a href=\"http://matthewrocklin.com/blog/work/2014/12/27/Towards-OOC/\">post 1</a>,\n<a href=\"http://matthewrocklin.com/blog/work/2014/12/30/Towards-OOC-Frontend/\">post 2</a>)\nconstruct an ND-Array library out of a simple task scheduler, NumPy, and Blaze.</p>\n\n<p>In this post we discuss a more sophisticated scheduler.\nIn this post we outline a less elegent but more effective scheduler that uses\nmultiple threads and caching to achieve performance on an interesting class of\narray operations.</p>\n\n<p>We create scheduling policies to minimize the memory footprint of our\ncomputation.</p>\n\n<h2 id=\"example\">Example</h2>\n\n<p>First, we establish value by doing a hard thing well.  Given two large\narrays stored in HDF5:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">import</span> <span class=\"nn\">h5py</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">h5py</span><span class=\"p\">.</span><span class=\"n\">File</span><span class=\"p\">(</span><span class=\"s\">'myfile.hdf5'</span><span class=\"p\">)</span>\n<span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">create_dataset</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'A'</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4000</span><span class=\"p\">,</span> <span class=\"mi\">2000000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">,</span>\n                     <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">250</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">),</span> <span class=\"n\">fillvalue</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">create_dataset</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'B'</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4000</span><span class=\"p\">,</span> <span class=\"mi\">4000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">,</span>\n                     <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">250</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">),</span> <span class=\"n\">fillvalue</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">close</span><span class=\"p\">()</span></code></pre>\n</figure>\n\n<p>We do a transpose and dot product.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">from</span> <span class=\"nn\">blaze</span> <span class=\"kn\">import</span> <span class=\"n\">Data</span><span class=\"p\">,</span> <span class=\"n\">into</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask.obj</span> <span class=\"kn\">import</span> <span class=\"n\">Array</span>\n\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">h5py</span><span class=\"p\">.</span><span class=\"n\">File</span><span class=\"p\">(</span><span class=\"s\">'myfile.hdf5'</span><span class=\"p\">)</span>\n\n<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">[</span><span class=\"s\">'A'</span><span class=\"p\">],</span> <span class=\"n\">blockshape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'A'</span><span class=\"p\">)</span>\n<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">[</span><span class=\"s\">'B'</span><span class=\"p\">],</span> <span class=\"n\">blockshape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'B'</span><span class=\"p\">)</span>\n\n<span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>\n<span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span>\n\n<span class=\"n\">expr</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">)</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"s\">'myfile.hdf5::/C'</span><span class=\"p\">,</span> <span class=\"n\">expr</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>This uses all of our cores and can be done with only 100MB or so of ram.  This\nis impressive because neither the inputs, outputs, nor any intermediate stage\nof the computation can fit in memory.</p>\n\n<p>We failed to achieve this exactly (see note at bottom) but still, <em>in theory</em>,\nwe’re great!</p>\n\n<h2 id=\"avoid-intermediates\">Avoid Intermediates</h2>\n\n<p>To keep a small memory footprint we avoid holding on to unnecessary\nintermediate data.  The full computation graph of a smaller problem\nmight look like the following:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask/uninlined.png\" alt=\"Un-inlined dask\" /></p>\n\n<p>Boxes represent data, circles represent functions that run on that data ,\narrows specify which functions produce/consume which data.</p>\n\n<p>The top row of circles represent the actual blocked dot products (note the many\ndata dependence arrows originating from them).  The bottom row of circles\nrepresents  pulling blocks of data from the the <code class=\"language-plaintext highlighter-rouge\">A</code> HDF5 dataset to in-memory\nnumpy arrays.  The second row transposes the blocks from the first row and adds\nmore blocks from <code class=\"language-plaintext highlighter-rouge\">B</code>.</p>\n\n<p>Naively performed, this graph can be very bad; we replicate our data four times\nhere, once in each of the rows.  We pull out all of the chunks, transpose each\nof them, and then finally do a dot product.  If we couldn’t fit the original\ndata in memory then there is no way that this will work.</p>\n\n<h2 id=\"function-inlining\">Function Inlining</h2>\n\n<p>We resolve this in two ways.  First, we don’t cache intermediate values for\nfast-running functions (like <code class=\"language-plaintext highlighter-rouge\">np.transpose</code>).  Instead we inline fast functions\ninto tasks involving expensive functions (like <code class=\"language-plaintext highlighter-rouge\">np.dot</code>).\nWe may end up running the same quick function twice, but at least we\nwon’t have to store the result.  We trade computation for memory.</p>\n\n<p>The result of the graph above with all access and transpose operations\ninlined looks like the following:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask/inlined.png\" alt=\"inlined dask\" /></p>\n\n<p>Now our tasks nest (see below).  We run all functions within a nested task as\na single operation. (non-LISP-ers avert your eyes):</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"p\">(</span><span class=\"s\">'x_6'</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">):</span> <span class=\"p\">(</span><span class=\"n\">dotmany</span><span class=\"p\">,</span> <span class=\"p\">[(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">transpose</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">ndslice</span><span class=\"p\">,</span> <span class=\"s\">'A'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">)),</span>\n                          <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">transpose</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">ndslice</span><span class=\"p\">,</span> <span class=\"s\">'A'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))],</span>\n                          <span class=\"p\">[(</span><span class=\"n\">ndslice</span><span class=\"p\">,</span> <span class=\"s\">'B'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n                           <span class=\"p\">(</span><span class=\"n\">ndslice</span><span class=\"p\">,</span> <span class=\"s\">'B'</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)]),</span></code></pre>\n</figure>\n\n<p>This effectively shoves all of the storage responsibility back down to the HDF5\nstore.  We end up pulling out the same blocks multiple times, but repeated\ndisk access is inevitable on large complex problems.</p>\n\n<p>This is automatic.  Dask now includes an <code class=\"language-plaintext highlighter-rouge\">inline</code> function that does this\nfor you.  You just give it a set of “fast” functions to ignore, e.g.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>dsk2 = inline(dsk, [np.transpose, ndslice, add, mul, ...])\n</code></pre></div></div>\n\n<h2 id=\"scheduler\">Scheduler</h2>\n\n<p>Now that we have a nice dask to crunch on, we run those tasks with multiple\nworker threads.  This is the job of a <em>scheduler</em>.</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/threads.jpg\" align=\"right\" alt=\"Thread pool, courtesy of sewingmantra.com\" width=\"20%\" /></p>\n\n<p>We build and document such a scheduler\n<a href=\"https://github.com/ContinuumIO/dask/blob/master/dask/threaded.py\">here</a>.  It\ntargets a shared-memory single-process multi-threaded environment.  It replaces\nthe elegant 20 line reference solution with a large blob of ugly code\nfilled with locks and mutable state.  Still, it manages the computation sanely,\nperforms some critical optimizations, and uses all of our hardware cores\n(Moore’s law is dead!  Long live Moore’s law!)</p>\n\n<p>Many NumPy operations release the GIL and so are highly amenable to\nmulti-threading.  NumPy programs do not suffer the\nsingle-active-core-per-process limitation of most Python code.</p>\n\n<h2 id=\"approach\">Approach</h2>\n\n<p>We follow a fairly standard model.  We create a <code class=\"language-plaintext highlighter-rouge\">ThreadPool</code> with a fixed\nnumber of workers.  We analyze the dask to determine “ready to run” tasks.\nWe send a task to each of our worker threads.  As they finish they update the\nrun-state, marking jobs as finished, inserting their results into a shared\ncache, and then marking new jobs as ready based on the newly available data.\nThis update process is fully indexed and handled by the worker threads\nthemselves (with appropriate locks) making the overhead negligible and\nhopefully scalable to complex workloads.</p>\n\n<p>When a newly available worker selects a new ready task they often have several\nto choose from.  We have a choice.  The choice we make here is very powerful.\nThe next section talks about our selection policy:</p>\n\n<p><em>Select tasks that immediately free data resources on completion</em></p>\n\n<h2 id=\"optimizations\">Optimizations</h2>\n\n<p>Our policy to prefer tasks that free resources lets us run many computations in\na very small space.  We now show three expressions, their resulting schedules,\nand an animation showing the scheduler walk through those schedules.  These\nwere taken from live runs.</p>\n\n<h2 id=\"example-embarrassingly-parallel-computation\">Example: Embarrassingly parallel computation</h2>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask/embarrassing.gif\" align=\"right\" width=\"50%\" alt=\"Embarassingly parallel dask\" /></p>\n\n<p>On the right we show an animated GIF of the progress of the following\nembarrassingly parallel computation:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>expr = (((A + 1) * 2) ** 3)\n</code></pre></div></div>\n\n<p>Circles represent computations, boxes represent data.</p>\n\n<p>Red means actively taking up resources.  Red is bad.</p>\n\n<ul>\n  <li>Red circles: tasks currently executing in a thread</li>\n  <li>Red boxes: data currently residing in the cache occupying precious memory</li>\n</ul>\n\n<p>Blue means finished or released.  Blue is good.</p>\n\n<ul>\n  <li>Blue circles: finished tasks</li>\n  <li>Blue boxes: data released from memory because we no longer need it for any\ntask</li>\n</ul>\n\n<p>We want to turn all nodes blue while minimizing the number of red boxes we have\nat any given time.</p>\n\n<p>The policy to execute tasks that free resources causes “vertical” execution\nwhen possible.  In this example our approach is optimal because the number of\nred boxes is kept small throughout the computation.  We have one only red box\nfor each of our four threads.</p>\n\n<h2 id=\"example-more-complex-computation-with-reductions\">Example: More complex computation with Reductions</h2>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask/normalized-b.gif\" align=\"right\" width=\"35%\" alt=\"More complex dask\" /></p>\n\n<p>We show a more complex expression:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>expr = (B - B.mean(axis=0)) + (B.T / B.std())\n</code></pre></div></div>\n\n<p>This extends the class of expressions that we’ve seen so far to reductions and\nreductions along an axis.  The per-chunk reductions start at the bottom and\ndepend only on the chunk from which they originated.  These per-chunk results\nare then concatenated together and re-reduced with the large circles (zoom in\nto see the text <code class=\"language-plaintext highlighter-rouge\">concatenate</code> in there.)  The next level takes these (small)\nresults and the original data again (note the long edges back down the bottom\ndata resources) which result in per-chunk subtractions and divisions.</p>\n\n<p>From there on out the work is embarrassing, resembling the computation above.\nIn this case we have relatively little parallelism, so the frontier of red\nboxes covers the entire image; fortunately the dataset is small.</p>\n\n<h2 id=\"example-fail-case\">Example: Fail Case</h2>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask/fail-case.gif\" align=\"right\" width=\"50%\" alt=\"A case where our scheduling algorithm fails to avoid intermediates\" /></p>\n\n<p>We show a case where our greedy solution fails miserably:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>expr = (A.T.dot(B) - B.mean(axis=0))\n</code></pre></div></div>\n\n<p>The two large functions on the second level from the bottom are the computation\nof the <code class=\"language-plaintext highlighter-rouge\">mean</code>.  These are cheap and, once completed,\nallow each of the blocks of the dot product to terminate quickly and release\nmemory.</p>\n\n<p>Tragically these <code class=\"language-plaintext highlighter-rouge\">mean</code> computations execute at the last possible moment,\nkeeping all of the dot product result trapped in the cache.\nWe have almost a full row of red boxes at one point in the computation.</p>\n\n<p>In this case our greedy solution was short sighted; a slightly more global\nsolution would quickly select these large circles to run quickly.  Perhaps\nbetweenness centrality would resole this problem.</p>\n\n<h2 id=\"on-disk-caching\">On-disk caching</h2>\n\n<p>We’ll never build a good enough scheduler.  We need to be able to fall back\nto on-disk caching.  Actually this isn’t terrible.  High\nperformance SSDs get close to 1 GB/second throughput and in the complex cases\nwhere data-aware scheduling fails we probably compute slower than that anyway.</p>\n\n<p>I have a little disk-backed dictionary project,\n<a href=\"https://github.com/mrocklin/chest/\"><code class=\"language-plaintext highlighter-rouge\">chest</code></a>, for this but it’s immature.  In\ngeneral I’d like to see more projects implement the <code class=\"language-plaintext highlighter-rouge\">dict</code> interface with\ninteresting policies.</p>\n\n<h2 id=\"trouble-with-binary-data-stores\">Trouble with binary data stores</h2>\n\n<p>I have a confession, the first computation, the very large dot product,\nsometimes crashes my machine.  While then scheduler manages memory well I have\na memory leak somewhere.  I suspect that I use HDF5 improperly.</p>\n\n<p>I also tried doing this with <code class=\"language-plaintext highlighter-rouge\">bcolz</code>.  Sadly nd-chunking is not well\nsupported.  <a href=\"https://groups.google.com/forum/#!topic/bcolz/6pddtrKMqQc\">email thread\nhere</a> <a href=\"https://groups.google.com/forum/#!topic/bcolz/330-llmA3ps\">and\nhere</a>.</p>\n\n<h2 id=\"expression-scope\">Expression Scope</h2>\n\n<p>Blaze currently generates dasks for the following:</p>\n\n<ol>\n  <li>Elementwise operations (like <code class=\"language-plaintext highlighter-rouge\">+</code>, <code class=\"language-plaintext highlighter-rouge\">*</code>, <code class=\"language-plaintext highlighter-rouge\">exp</code>, <code class=\"language-plaintext highlighter-rouge\">log</code>, …)</li>\n  <li>Dimension shuffling like <code class=\"language-plaintext highlighter-rouge\">np.transpose</code></li>\n  <li>Tensor contraction like <code class=\"language-plaintext highlighter-rouge\">np.tensordot</code></li>\n  <li>Reductions like <code class=\"language-plaintext highlighter-rouge\">np.mean(..., axis=...)</code></li>\n  <li>All combinations of the above</li>\n</ol>\n\n<p>We also comply with the NumPy API on these operations..  At the time of writing\nnotable missing elements include the following:</p>\n\n<ol>\n  <li>Slicing (though this should be easy to add)</li>\n  <li>Solve, SVD, or any more complex linear algebra.  There are good solutions\nto this implemented in other linear algebra software (Plasma,\nFlame, Elemental, …) but I’m not planning to go down that path until\nlots of people start asking for it.</li>\n  <li>Anything that NumPy can’t do.</li>\n</ol>\n\n<p>I’d love to hear what’s important to the community.  Re-implementing all of\nNumPy is hard, re-implementing a few choice pieces of NumPy is relatively\nstraightforward.  Knowing what those few choices pieces are requires community\ninvolvement.</p>\n\n<h2 id=\"bigger-ideas\">Bigger ideas</h2>\n\n<p>My experience building dynamic schedulers is limited and my approach is likely\nsuboptimal.  It would be great to see other approaches.  None of the logic in\nthis post is specific to Blaze or even to NumPy.  To build a scheduler you only\nneed to understand the model of a graph of computations with data dependencies.</p>\n\n<p>If we were ambitious we might consider a distributed scheduler to execute these\ntask graphs across many nodes in a distributed-memory situation (like a\ncluster).  This is a hard problem but it would open up a different class of\ncomputational solutions.  The Blaze code to generate the dasks would not change\n; the graphs that we generate are orthogonal to our choice of scheduler.</p>\n\n<h2 id=\"help\">Help</h2>\n\n<p>I could use help with all of this, either as open source work (links below) or\nfor money.  Continuum has funding for employees and ample interesting problems.</p>\n\n<p><strong>Links:</strong></p>\n\n<ul>\n  <li><a href=\"http://dask.readthedocs.org/en/latest/\">Dask spec</a></li>\n  <li><a href=\"https://github.com/ContinuumIO/dask/blob/master/dask/threaded.py\">Scheduler implementation (with decent narrative documentation)</a></li>\n</ul>"
}