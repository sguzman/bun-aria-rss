{
  "title": "Thoughts on Mutual Information: Alternative Dependency Measures",
  "link": "http://artem.sobolev.name/posts/2019-09-15-thoughts-on-mutual-information-alternative-dependency-measures.html",
  "description": "<p>This posts finishes the discussion started in the <a href=\"/posts/2019-08-10-thoughts-on-mutual-information-more-estimators.html\">Thoughts on Mutual Information: More Estimators</a> with a consideration of alternatives to the Mutual Information.</p>\n<!--more-->\n<h2 id=\"mutual-information\">Mutual Information</h2>\n<p>Let’s step out a bit and take a critical look at the MI. One of its equivalent definitions says that it’s a KL-divergence between the joint distribution and the product of marginals: <span class=\"math display\">\\[\n\\text{MI}[p(x, z)] = D_{KL}(p(x, z) \\mid\\mid p(x) p(z))\n\\]</span></p>\n<p>Indeed, if the random variables <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(Z\\)</span> are independent, then the joint distribution <span class=\"math inline\">\\(p(x, z)\\)</span> factorizes as <span class=\"math inline\">\\(p(x) p(z)\\)</span>, and the KL (or any other divergence or distance between probability distributions) is equal to zero. Conversely, the more <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(Z\\)</span> are dependent, the further the joint <span class=\"math inline\">\\(p(x, z)\\)</span> deviates from the product of marginals <span class=\"math inline\">\\(p(x) p(z)\\)</span>.</p>\n<p>But why this particular choice of divergence?</p>\n<p>Why not <a href=\"https://threeplusone.com/pubs/on_jensenshannon.pdf\">Jeffreys divergence</a>, <a href=\"https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\">Jensen-Shannon divergence</a>, <a href=\"https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures\">Total Variation distance</a> or <a href=\"https://en.wikipedia.org/wiki/Wasserstein_metric\">Wasserstein distance</a>?</p>\n<p>The answer to this question lies in the entropic form of the MI: <span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n= D_{KL}(p(x, z) \\mid\\mid p(x) p(z))\n= \\mathbb{H}[p(x)] - \\E_{p(z)} \\mathbb{H}[p(x|z)]\n\\]</span></p>\n<p>The Mutual Information is equal to average reduction in entropy (a measure of uncertainty) of <span class=\"math inline\">\\(X\\)</span> when we know <span class=\"math inline\">\\(Z\\)</span>. Such information-theoretic interpretation is the main reason the MI is so widespread. However, there’s a major issue when <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(Z\\)</span> are continuous: these entropies become differential ones, and the differential entropy <a href=\"https://stats.stackexchange.com/a/256238\">does not enjoy the same uncertainty-measuring interpretation as the discrete one does</a>.</p>\n<p>One particular issue with the continuous Mutual Information is the following one: if <span class=\"math inline\">\\(\\text{Pr}[X = Z] = 1\\)</span>, then the MI attains its maximal value. In the discrete case this maximal value is equal to the entropy of <span class=\"math inline\">\\(X\\)</span> and finite, but in the continuous case it’s equal to <span class=\"math inline\">\\(+\\infty\\)</span> <a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a>. Moreover, imagine <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(Z\\)</span> are <span class=\"math inline\">\\(N\\)</span>-dimensional random vectors s.t. <span class=\"math inline\">\\(\\text{Pr}(X_1 = Z_1) = 1\\)</span> and the rest components are all independent random variables. Then, it’s easy to show that the MI <span class=\"math inline\">\\(I(X, Z) = +\\infty\\)</span> regardless of <span class=\"math inline\">\\(N\\)</span>! So if <span class=\"math inline\">\\(N\\)</span> is in billions these vectors are mostly independent, but one pesky component ruined it all, and we ended up with an infinite mutual “information”.</p>\n<p>I hope this convinced you there’s no information-theoretic interpretation in the continuous case to hold on to the particular choice of divergence between <span class=\"math inline\">\\(p(x, z)\\)</span> and <span class=\"math inline\">\\(p(x) p(z)\\)</span>, which means we’re free to explore alternatives…</p>\n<h2 id=\"f-divergences\"><span class=\"math inline\">\\(f\\)</span>-divergences</h2>\n<p>KL divergence is a special case of <a href=\"https://en.wikipedia.org/wiki/F-divergence\"><span class=\"math inline\">\\(f\\)</span>-divergences</a> given by <span class=\"math inline\">\\(f(t) = t \\log t\\)</span>. However, other choices are perfectly legal, too:</p>\n<ol style=\"list-style-type: decimal\">\n<li><a href=\"https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\">Jensen-Shannon divergence</a> corresponds to <span class=\"math inline\">\\(f(t) = t \\log t - (t+1) \\log \\tfrac{t+1}{2}\\)</span>.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures\">Total Variation distance</a> corresponds to <span class=\"math inline\">\\(f(t) = |t-1|\\)</span>.</li>\n<li>Jeffreys divergence is given by <span class=\"math inline\">\\(f(t) = \\tfrac{t-1}{2} \\log t\\)</span>.</li>\n<li>Reverse KL divergence is given by <span class=\"math inline\">\\(f(t) = -\\log t\\)</span>.</li>\n</ol>\n<p>So, one can consider <span class=\"math inline\">\\(f\\)</span>-Mutual Information defined as <span class=\"math display\">\\[\nI_f(X, Z) := D_f(p(x, z) \\mid\\mid p(x) p(z)) = \\E_{p(x) p(z)} f\\left(\\frac{p(x, z)}{p(x) p(z)}\\right)\n\\]</span></p>\n<p>In general, however, KL, Reverse KL and combinations thereof are the only <span class=\"math inline\">\\(f\\)</span>-divergences that are additive for independent random variables: if <span class=\"math inline\">\\(X_1 \\perp X_2\\)</span> under both <span class=\"math inline\">\\(p(x)\\)</span> and <span class=\"math inline\">\\(q(x)\\)</span>, then <span class=\"math display\">\\[\\text{KL}(p(x) \\mid\\mid q(x)) = \\text{KL}(p(x_1) \\mid\\mid q(x_1)) + \\text{KL}(p(x_2) \\mid\\mid q(x_2))\\]</span> And thus for <span class=\"math inline\">\\(X_1 \\perp X_2\\)</span> and <span class=\"math inline\">\\(Z_1 \\perp Z_2\\)</span> <span class=\"math display\">\\[\nI_f(X, Z)\n=\nI_f(X_1, Z_1)\n+\nI_f(X_2, Z_2)\n\\Leftrightarrow\n\\text{$f$ is a combination of KLs}\n\\]</span> Is such additivity important, though? Imagine having a sample set of independent objects <span class=\"math inline\">\\(X_1, \\dots, X_N\\)</span> used to extract corresponding representations <span class=\"math inline\">\\(Z_1, \\dots, Z_N\\)</span>. In general with <span class=\"math inline\">\\(f\\)</span>-MI you’re not allowed to use stochastic optimization / minibatching to work with <span class=\"math inline\">\\(I_f(X, Z)\\)</span>. This is counter-intuitive and not something we’d expect from a measure of <em>information</em>.</p>\n<p>That said, there some thing to keep in mind:</p>\n<ol style=\"list-style-type: decimal\">\n<li>In practice you probably can use <span class=\"math inline\">\\(\\sum_{n=1}^N I_f(X_n, Z_n)\\)</span> instead of <span class=\"math inline\">\\(I_f(X, Z)\\)</span> without having to suffer any consequences.</li>\n<li>In some special cases such additivity <em>does</em> hold. These are cases of KL divergence, Reverse KL divergence and any combination thereof.</li>\n</ol>\n<h3 id=\"lautum-information\">Lautum Information</h3>\n<p>Palomar and Verdu introduced the <a href=\"https://ieeexplore.ieee.org/document/4455754\">Lautum Information</a> (I particularly liked their naming: Lautum is Mutual backwards): an analogue of the Mutual Information with KL’s arguments swapped:</p>\n<p><span class=\"math display\">\\[\n\\text{LI}[p(x, z)] = D_{KL}(p(x) p(z) \\mid\\mid p(x, z))\n\\]</span></p>\n<p>It an be equivalently rewritten as</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\text{LI}[p(x, z)]\n&= \\E_{p(x) p(z)} \\log \\frac{p(x) p(z)}{p(x, z)}\n= \\E_{p(x) p(z)} \\log \\frac{p(z)}{p(z|x)} \\\\\n&= -\\E_{p(x) p(z)} \\log p(z|x) - \\mathbb{H}[p(z)]\n\\end{align*}\n\\]</span></p>\n<p>Notice that in general the first term is not an entropy, but rather a cross-entropy. Unfortunately, this cross-entropy term lacks intuitive information-theoretic interpretation – a distinctive feature of the Mutual Information.</p>\n<p>Another disadvantage of the Lautum Information is that even for discrete random variables it’s infinite when <span class=\"math inline\">\\(X = Z\\)</span>. Since one is sampling from the product of marginals <span class=\"math inline\">\\(p(x) p(z)\\)</span>, you’ll inevitably have some probability mass in the area where <span class=\"math inline\">\\(x \\not= z\\)</span>, but <span class=\"math inline\">\\(p(x, z)\\)</span> will be exactly zero in such regions, hence logarithm’s argument will be infinite. However, for continuous random variables, even the standard Mutual Information will be infinite in the case of a deterministic invertible dependency.</p>\n<p>But how do we estimate the LI? First, the good news is that its only entropy term comes with a minus sign, hence if you seek to lower bound the LI, then <a href=\"/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html\">the formal limitations theorem</a> does not apply. Unfortunately, I’m not aware of any good black-box bounds on the cross-entropy, so we’ll have to assume at least one conditional to be known, say, <span class=\"math inline\">\\(p(x|z)\\)</span>. For the other term we can use any of the plethora of bounds on the log marginal likelihood: <span class=\"math display\">\\[\n\\begin{align*}\n\\text{LI}[p(x, z)]\n&= -\\E_{p(x) p(z)} \\log p(x|z) + \\E_{p(x)} \\log p(x) \\\\\n&= -\\E_{p(x) p(z)} \\log p(x|z) + \\E_{p(x)} \\log q(x) + \\E_{p(x)} \\log \\frac{p(x)}{q(x)} \\\\\n&= -\\E_{p(x) p(z)} \\log p(x|z) + \\E_{p(x)} \\log q(x) + D_\\text{KL}(p(x) \\mid\\mid q(x) ) \\\\\n&\\ge \\E_{p(x) p(z)} \\log \\frac{q(x)}{p(x|z)}\n\\end{align*}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(q(x)\\)</span> is any (variational) distribution with the same support as <span class=\"math inline\">\\(p(x)\\)</span>. However, notice that we’ve already assumed <span class=\"math inline\">\\(p(x|z)\\)</span> to be known. With an additional assumption of being able to sample from this conditional we can use <a href=\"/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html#new-semi-implicit-hope\">SIVI</a> to give an (non-black-box) lower bound on the entropy of <span class=\"math inline\">\\(p(x)\\)</span>: <span class=\"math display\">\\[\n\\begin{align*}\n\\text{LI}[p(x, z)]\n&= \\E_{p(x)} \\log p(x) - \\E_{p(x) p(z)} \\log p(x|z) \\\\\n&\\le \\E_{p(x, z_0)} \\E_{p(z_{1:K})} \\log \\left( \\frac{1}{K+1} \\sum_{k=0}^K p(x|z_k) \\right) - \\E_{p(x) p(z)} \\log p(x|z) \\\\\n&= \\E_{p(x, z_0)} \\E_{p(z_{1:K})} \\left[ \\log \\left( \\frac{1}{K+1} \\sum_{k=0}^K p(x|z_k) \\right) - \\frac{1}{K} \\sum_{k=1}^K \\log p(x|z_k) \\right] \\\\\n\\end{align*}\n\\]</span></p>\n<p>Moreover, if the marginal distribution <span class=\"math inline\">\\(p(z)\\)</span> is known, <a href=\"(/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html#importance-weighted-hierarchical-variational-inference)\">IWHVI</a> provides a better estimate at the cost of introducing a variational distribution <span class=\"math inline\">\\(q(z|x)\\)</span>: <span class=\"math display\">\\[\n\\begin{align*}\n\\text{LI}[p(x, z)]\n&= \\E_{p(x)} \\log p(x) - \\E_{p(x) p(z)} \\log p(x|z) \\\\\n&\\le \\E_{p(x, z_0)} \\E_{q(z_{1:K}|x)} \\log \\left( \\frac{1}{K+1} \\sum_{k=0}^K \\frac{p(x, z_k)}{q(z_k|x)} \\right) - \\E_{p(x) p(z)} \\log p(x|z)\n\\end{align*}\n\\]</span></p>\n<p>Analogously, one can use <a href=\"/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html\">IWAE</a> bounds to arrive at the following lower bound on the LI: <span class=\"math display\">\\[\n\\begin{align*}\n\\text{LI}[p(x, z)]\n&= \\E_{p(x)} \\log p(x) + \\E_{p(x) p(z)} \\log p(x|z) \\\\\n&\\ge \\E_{p(x)} \\E_{q(z_{1:K}|x)} \\log \\left( \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x, z_k)}{q(z_k|x)} \\right) - \\E_{p(x) p(z)} \\log p(x|z)\n\\end{align*}\n\\]</span></p>\n<h2 id=\"wasserstein-distance\">Wasserstein Distance</h2>\n<p>An interesting alternative to both Forward and Reverse KLs is the <a href=\"https://en.wikipedia.org/wiki/Wasserstein_metric\">Wasserstein Distance aka Kantorovich–Rubinstein distance aka optimal transport distance</a>. Formally, Wasserstein-<span class=\"math inline\">\\(p\\)</span> metric <span class=\"math inline\">\\(W_p\\)</span> defined as <span class=\"math display\">\\[\nW_p(p(x), q(x)) := \\left( \\inf_{\\gamma \\in \\Gamma(p, q)} \\E_{\\gamma(x, y)} \\|x-y\\|_p^p \\right)^{1/p}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\Gamma(p, q)\\)</span> is a set of all possible joint distributions over <span class=\"math inline\">\\(x\\)</span> and <span class=\"math inline\">\\(y\\)</span> s.t. <span class=\"math inline\">\\(p(x)\\)</span> and <span class=\"math inline\">\\(q(y)\\)</span> are its respective marginals: <span class=\"math display\">\\[\n\\gamma \\in \\Gamma(p, q) \\Leftrightarrow\n\\int_{\\text{dom}(y)} \\gamma(x, y) dy = p(x),\n\\quad\n\\int_{\\text{dom}(x)} \\gamma(x, y) dx = q(y)\n\\]</span></p>\n<p>In particular, we’ll be considering Wasserstein-1 distance <span class=\"math inline\">\\(W_1\\)</span>: <span class=\"math display\">\\[\nW_1(p(x), q(x)) := \\inf_{\\gamma \\in \\Gamma(p, q)} \\E_{\\gamma(x, y)} \\|x-y\\|_1\n\\]</span></p>\n<p>The Wasserstein-1 distance lies at the heart of the <a href=\"https://arxiv.org/abs/1701.07875\">Wasserstein GAN</a>, and it was this paper that suggested to use Kantorovich–Rubinstein duality to estimate the distance in practice: <span class=\"math display\">\\[\nW_1(p(x), q(x)) \\ge \\E_{p(x)} f(x) - \\E_{q(x)} f(x)\n\\]</span> Where <span class=\"math inline\">\\(f\\)</span> is any 1-Lipschitz function. It also seems to be additive for independent random variables – a nice property to have.</p>\n<p>Now we can use this tractable lower bound to estimate the <a href=\"https://arxiv.org/abs/1903.11780\">Wasserstein Dependency Measure</a> <span class=\"math inline\">\\(W_1(p(x, z), p(x) p(z))\\)</span> – a Wasserstein analogue of the Mutual Information<a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>: <span class=\"math display\">\\[\nW_1(p(x, z), p(x) p(z)) \\ge \\E_{p(x, z)} f(x, z) - \\E_{p(x) p(z)} f(x, z)\n\\]</span></p>\n<p>You can notice that this lower bound is similar to the <a href=\"http://localhost:8000/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html#the-nguyen-wainwright-jordan-bound\">Nguyen Wainwright Jordan lower bound</a> on KL. Unfortunately, it’s not known whether this bound is efficient or if it also exhibits large variance. Thorough theoretical analysis of the Kantorovich–Rubinstein lower bound is an interesting research question.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The Mutual Information is far from being the only dependency measure, yet it’s the most intuitive one, with the intuition coming from the information theory. However, as I argued here, this nice interpretation goes out of the window once you introduce continuous random variables, so no need to stick to the particular choice of the MI, especially given that there’re lots of alternative dependency measures. With the <a href=\"/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html\">Formal Limitations</a> paper preventing us from having practical black-box lower bounds on the MI, I believe more and more researchers will study alternative dependence measures, and we already see some pioneering work, such as Wasserstein Dependency Measure.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>This is, of course, due to the fact that a real number requires an infinite amount of bits to be written exactly, allowing one to indeed store an infinite amount of information in a truly real number.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>Notably, authors of the paper did not name their measure something like Wasserstein Mutual Information. Technically speaking, only the KL-based MI can be called <em>something</em>-information since, as we’ve discussed already, it’s the only dependency measure that has information-theoretic interpretation. In that sense, Lautum Information should have been named differently.<a href=\"#fnref2\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Sun, 15 Sep 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-09-15-thoughts-on-mutual-information-alternative-dependency-measures.html",
  "dc:creator": "Artem"
}