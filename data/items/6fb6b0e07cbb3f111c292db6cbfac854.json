{
  "title": "Leveraging Fully Observable Policies for Learning under Partial Observability. (arXiv:2211.01991v1 [cs.RO])",
  "link": "http://arxiv.org/abs/2211.01991",
  "description": "<p>Reinforcement learning in partially observable domains is challenging due to\nthe lack of observable state information. Thankfully, learning offline in a\nsimulator with such state information is often possible. In particular, we\npropose a method for partially observable reinforcement learning that uses a\nfully observable policy (which we call a state expert) during offline training\nto improve online performance. Based on Soft Actor-Critic (SAC), our agent\nbalances performing actions similar to the state expert and getting high\nreturns under partial observability. Our approach can leverage the\nfully-observable policy for exploration and parts of the domain that are fully\nobservable while still being able to learn under partial observability. On six\nrobotics domains, our method outperforms pure imitation, pure reinforcement\nlearning, the sequential or parallel combination of both types, and a recent\nstate-of-the-art method in the same setting. A successful policy transfer to a\nphysical robot in a manipulation task from pixels shows our approach's\npracticality in learning interesting policies under partial observability.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baisero_A/0/1/0/all/0/1\">Andrea Baisero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1\">Christopher Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1\">Robert Platt</a>"
}