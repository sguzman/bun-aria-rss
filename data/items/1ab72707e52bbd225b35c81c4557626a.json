{
  "title": "Data Pipeline 4.4 Now Available",
  "link": "https://northconcepts.com/blog/2018/07/31/data-pipeline-4-4-now-available/",
  "comments": "https://northconcepts.com/blog/2018/07/31/data-pipeline-4-4-now-available/#respond",
  "pubDate": "Tue, 31 Jul 2018 10:50:04 +0000",
  "dc:creator": "Dele Taylor",
  "category": [
    "Data Pipeline",
    "News"
  ],
  "guid": "https://northconcepts.com/blog/?p=2064",
  "description": "<p> [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://northconcepts.com/blog/2018/07/31/data-pipeline-4-4-now-available/\">Data Pipeline 4.4 Now Available</a> appeared first on <a rel=\"nofollow\" href=\"https://northconcepts.com/blog\">Data Pipeline</a>.</p>\n",
  "content:encoded": "<p><img class=\"alignnone wp-image-2331 size-full\" src=\"https://northconcepts.com/blog/wp-content/uploads/2018/07/pexels-daniel-reche-3616959_text_new_4.4.jpg\" width=\"1280\" height=\"854\" srcset=\"https://northconcepts.com/blog/wp-content/uploads/2018/07/pexels-daniel-reche-3616959_text_new_4.4.jpg 1280w, https://northconcepts.com/blog/wp-content/uploads/2018/07/pexels-daniel-reche-3616959_text_new_4.4-300x200.jpg 300w, https://northconcepts.com/blog/wp-content/uploads/2018/07/pexels-daniel-reche-3616959_text_new_4.4-768x512.jpg 768w, https://northconcepts.com/blog/wp-content/uploads/2018/07/pexels-daniel-reche-3616959_text_new_4.4-1024x683.jpg 1024w\" sizes=\"(max-width: 1280px) 100vw, 1280px\" /></p>\n<p>Today we&#8217;re pleased announce the release of Data Pipeline version 4.4.  This update includes integration with Amazon S3, new features to better handle real-time data and aggregation, and new XML and JSON readers to speed up your development.</p>\n<p><span id=\"more-2064\"></span></p>\n<div class=\"text-center\"><a class=\"btn btn-primary\" href=\"https://northconcepts.com/pricing/\">Download Data Pipeline 4.4 now!</a></div>\n<h2>Amazon S3 Streaming Uploads & Downloads</h2>\n<p>We&#8217;ve introduced a new <code>AmazonS3FileSystem</code> class to help you interact with remote files stored on S3.  The class lets you read and write files with minimal memory overhead without first saving them to disk.  It also let&#8217;s you browse your buckets, folders, and files.</p>\n<h3>Read a File from Amazon S3</h3>\n<p></p><pre class=\"crayon-plain-tag\">import java.io.InputStream;\nimport java.io.InputStreamReader;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.NullWriter;\nimport com.northconcepts.datapipeline.csv.CSVReader;\nimport com.northconcepts.datapipeline.filesystem.amazons3.AmazonS3FileSystem;\nimport com.northconcepts.datapipeline.job.Job;\n\npublic class ReadFromAmazonS3 {\n\n    private static final String ACCESS_KEY = \"YOUR ACCESS KEY\";\n    private static final String SECRET_KEY = \"YOUR SECRET KEY\";\n\n    public static void main(String[] args) throws Throwable {\n        AmazonS3FileSystem s3 = new AmazonS3FileSystem();\n        s3.setBasicAWSCredentials(ACCESS_KEY, SECRET_KEY);\n        s3.open();\n        try {\n            InputStream inputStream = s3.readFile(\"datapipeline-bucket\", \"output/trades.csv\");\n\n            DataReader reader = new CSVReader(new InputStreamReader(inputStream));\n//            DataWriter writer = StreamWriter.newSystemOutWriter();\n            DataWriter writer = new NullWriter();\n            Job.run(reader, writer);\n            \n            System.out.println(\"Records read: \" + writer.getRecordCount());\n        } finally {\n            s3.close();\n        }\n    }\n\n}</pre><p>&nbsp;</p>\n<h3>Write a File to Amazon S3</h3>\n<p></p><pre class=\"crayon-plain-tag\">import java.io.File;\nimport java.io.OutputStream;\nimport java.io.OutputStreamWriter;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.csv.CSVReader;\nimport com.northconcepts.datapipeline.csv.CSVWriter;\nimport com.northconcepts.datapipeline.filesystem.amazons3.AmazonS3FileSystem;\nimport com.northconcepts.datapipeline.job.Job;\n\npublic class WriteToAmazonS3UsingMultipartStreaming {\n    \n    private static final String ACCESS_KEY = \"YOUR ACCESS KEY\";\n    private static final String SECRET_KEY = \"YOUR SECRET KEY\";\n\n    public static void main(String[] args) throws Throwable {\n        AmazonS3FileSystem s3 = new AmazonS3FileSystem();\n        s3.setBasicAWSCredentials(ACCESS_KEY, SECRET_KEY);\n//        s3.setDebug(true);\n        s3.open();\n        try {\n            // Create AWS S3 streaming, multi-part OutputStream \n            OutputStream outputStream = s3.writeMultipartFile(\"datapipeline-bucket\", \"output/trades.csv\");\n\n            DataReader reader = new CSVReader(new File(\"example/data/input/trades.csv\"))\n                    .setFieldNamesInFirstRow(true);\n                \n            DataWriter writer = new CSVWriter(new OutputStreamWriter(outputStream, \"utf-8\"))\n                    .setFieldNamesInFirstRow(true);\n            \n            Job.run(reader, writer);\n            \n            System.out.println(\"Done.\");\n        } finally {\n            s3.close();\n        }\n    }\n\n}</pre><p>&nbsp;</p>\n<h2>JSON Record Reader</h2>\n<p>Our existing <a href=\"https://northconcepts.com/docs/examples/read-a-json-stream/\">JSON reader</a> requires that you identify the fields ahead of time to add to your records.    But what if you want all the fields or don&#8217;t know what the fields are ahead of time?  The new <code>JsonRecordReader</code> is here to help.   You no longer need to specify any fields, just one or more record breaks and Data Pipeline will collect everything it finds into hierarchical records.</p><pre class=\"crayon-plain-tag\">import java.io.File;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.StreamWriter;\nimport com.northconcepts.datapipeline.job.Job;\nimport com.northconcepts.datapipeline.json.JsonRecordReader;\n\npublic class ReadJsonRecordsFromFile {\n\n    public static void main(String[] args) {\n        DataReader reader = new JsonRecordReader(new File(\"example/data/output/simple-json-to-file.json\"))\n                .addRecordBreak(\"/array/object\");\n        \n        DataWriter writer = StreamWriter.newSystemOutWriter();\n        \n        Job.run(reader, writer);\n    }\n}</pre><p>&nbsp;</p>\n<h2>XML Record Reader</h2>\n<p>Like the new JSON record reader, XML also has a similar addition.  If you need to collect all subbranches of an XML tree into records without specifying each field, the new <code>XmlRecordReader</code> is exactly what you need.</p><pre class=\"crayon-plain-tag\">import java.io.File;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.StreamWriter;\nimport com.northconcepts.datapipeline.job.Job;\nimport com.northconcepts.datapipeline.xml.XmlRecordReader;\n\npublic class ReadXmlRecordsFromFile {\n\n    public static void main(String[] args) {\n        DataReader reader = new XmlRecordReader(new File(\"example/data/output/simple-xml-to-file.xml\"))\n                .addRecordBreak(\"/records/record\");\n        \n        DataWriter writer = StreamWriter.newSystemOutWriter();\n        \n        Job.run(reader, writer);\n    }\n}</pre><p>&nbsp;</p>\n<h2>Buffered Reader</h2>\n<p>When dealing with real-time data, you sometimes need to collect out-of-order records together.  In a shipping system, you may need to process packages arriving on the same truck together, even if packages from multiple trucks are mixed together.  The new <code>BufferedReader</code> class uses a configurable strategy to determine when to release incoming records in each of its buffers downstream.</p><pre class=\"crayon-plain-tag\">import java.util.concurrent.TimeUnit;\n\nimport com.northconcepts.datapipeline.buffer.BufferStrategy;\nimport com.northconcepts.datapipeline.buffer.BufferedReader;\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.Record;\nimport com.northconcepts.datapipeline.core.StreamWriter;\nimport com.northconcepts.datapipeline.job.Job;\n\n\npublic class BufferRecordsByTimePeriodOrCount {\n\n    private static final int MAX_TRUCKS = 10;\n    private static final long MAX_PACKAGES = 200;\n    private static final int RECORD_DELAY_MILLISECONDS = 250;\n\n    public static void main(String[] args) {\n        \n        DataReader reader = new FakePackageReader(MAX_TRUCKS, MAX_PACKAGES, RECORD_DELAY_MILLISECONDS);\n        \n        // group records by \"truck_id\" and release all records for each \"truck_id\" downstream every 10 seconds or \n        // when 10 records for that \"truck_id\" have been collected\n        // and limit the internal buffer size to 100 records\n        \n        reader = new BufferedReader(reader, 100, \"truck_id\")\n                .setBufferStrategy(BufferStrategy.limitedTimeFromOpenOrLimitedRecords(TimeUnit.SECONDS.toMillis(10), 10))\n                ;\n        \n        DataWriter writer = StreamWriter.newSystemOutWriter();\n        \n        Job job = Job.run(reader, writer);\n        \n        System.out.println(job.getRecordsTransferred() + \"  -  \" + job.getRunningTimeAsString());\n    }\n\n    \n    //==================================================\n    \n    public static class FakePackageReader extends DataReader {\n        \n        private final int maxTrucks;\n        private final long maxPackages;\n        private long nextPackageId;\n        private final long recordDelay;\n        \n        public FakePackageReader(int maxTrucks, long maxPackages, long recordDelay) {\n            this.maxTrucks = maxTrucks;\n            this.maxPackages = maxPackages;\n            this.recordDelay = recordDelay;\n        }\n        \n        @Override\n        protected Record readImpl() throws Throwable {\n            if (nextPackageId >= maxPackages) {\n                return null;\n            }\n            \n            if (recordDelay > 0) {\n                Thread.sleep(recordDelay);\n            }\n            \n            Record record = new Record();\n            record.setField(\"package_id\", nextPackageId++);\n            record.setField(\"truck_id\", \"truck\" + nextPackageId % maxTrucks);\n            record.setField(\"amount\", nextPackageId + 0.01);\n            return record;\n        }\n        \n    }\n\n}</pre><p>&nbsp;</p>\n<h2>Group By Reader</h2>\n<p>If you need to summarize streaming data, <code>GroupByReader</code> can now produce output at a different rate than incoming data.  The <code>CreateWindowStrategy</code> and <code>CloseWindowStrategy</code> classes can now be used together to produce sliding windows that release summarized data even when no new records are available.  They can also be used to create complex windows based on a combination of factors including time and record counts.</p><pre class=\"crayon-plain-tag\">import java.util.concurrent.TimeUnit;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.Record;\nimport com.northconcepts.datapipeline.core.StreamWriter;\nimport com.northconcepts.datapipeline.group.CloseWindowStrategy;\nimport com.northconcepts.datapipeline.group.GroupByReader;\nimport com.northconcepts.datapipeline.job.Job;\n\n\npublic class GroupRecordsByTimePeriodOrCount {\n\n    private static final int MAX_ORDERS = 10;\n    private static final long MAX_TRANSACTIONS = 200;\n    private static final int RECORD_DELAY_MILLISECONDS = 250;\n\n    public static void main(String[] args) {\n        \n        DataReader reader = new FakeMessageQueueReader(MAX_ORDERS, MAX_TRANSACTIONS, RECORD_DELAY_MILLISECONDS);\n        \n        // group records by \"order_id\" and release them every 10 seconds even if no new records are received\n        reader = new GroupByReader(reader, \"order_id\")\n                .count(\"count_of_transactions\")\n                .sum(\"amount\")\n                .setCloseWindowStrategy(CloseWindowStrategy.limitedTime(TimeUnit.SECONDS.toMillis(10)))\n                ;\n        \n        \n        DataWriter writer = StreamWriter.newSystemOutWriter();\n        \n        Job job = Job.run(reader, writer);\n        \n        System.out.println(job.getRecordsTransferred() + \"  -  \" + job.getRunningTimeAsString());\n    }\n\n    \n    //==================================================\n    \n    public static class FakeMessageQueueReader extends DataReader {\n        \n        private final int maxOrders;\n        private final long maxTransactions;\n        private long nextTransactionId;\n        private final long recordDelay;\n        \n        public FakeMessageQueueReader(int maxOrders, long maxTransactions, long recordDelay) {\n            this.maxOrders = maxOrders;\n            this.maxTransactions = maxTransactions;\n            this.recordDelay = recordDelay;\n        }\n        \n        @Override\n        protected Record readImpl() throws Throwable {\n            if (nextTransactionId >= maxTransactions) {\n                return null;\n            }\n            \n            if (recordDelay > 0) {\n                Thread.sleep(recordDelay);\n            }\n            \n            Record record = new Record();\n            record.setField(\"transaction_id\", nextTransactionId++);\n            record.setField(\"order_id\", \"order\" + nextTransactionId % maxOrders);\n            record.setField(\"amount\", nextTransactionId + 0.01);\n            return record;\n        }\n        \n    }\n\n}</pre><p>&nbsp;</p>\n<h2>Bigrams, Trigrams, Ngrams</h2>\n<p>Data Pipeline has a new transformer for extracting every n-sequence of words in a body of text.  The new <code>Ngrams</code> class allows you to extract bigrams, trigrams, and other ngrams from any field in a data stream.   The following example finds the top 25 three-word sequences found in the headlines of several Canadian news websites.</p><pre class=\"crayon-plain-tag\">import java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.io.OutputStreamWriter;\nimport java.net.URL;\n\nimport com.northconcepts.datapipeline.core.DataReader;\nimport com.northconcepts.datapipeline.core.DataWriter;\nimport com.northconcepts.datapipeline.core.LimitReader;\nimport com.northconcepts.datapipeline.core.SequenceReader;\nimport com.northconcepts.datapipeline.core.SortingReader;\nimport com.northconcepts.datapipeline.csv.CSVWriter;\nimport com.northconcepts.datapipeline.group.GroupByReader;\nimport com.northconcepts.datapipeline.job.Job;\nimport com.northconcepts.datapipeline.transform.BasicFieldTransformer;\nimport com.northconcepts.datapipeline.transform.Ngrams;\nimport com.northconcepts.datapipeline.transform.TransformingReader;\nimport com.northconcepts.datapipeline.xml.XmlRecordReader;\n\npublic class ExtractBigramsTrigramsAndNgrams {\n    \n    private static final int NGRAMS = 3;  // bigram: 2; trigrams: 3; quadrigrams: 4;\n    private static final int TOP_PHRASES = 25;\n    \n    private static final String[] URLS = {\n            \"https://rss.cbc.ca/lineup/topstories.xml\",\n            \"https://rss.cbc.ca/lineup/world.xml\",\n            \"https://rss.cbc.ca/lineup/canada.xml\",\n            \"https://rss.cbc.ca/lineup/politics.xml\",\n            \"https://rss.cbc.ca/lineup/business.xml\",\n            \"https://rss.cbc.ca/lineup/health.xml\",\n            \"https://rss.cbc.ca/lineup/arts.xml\",\n            \"https://rss.cbc.ca/lineup/technology.xml\",\n            \"https://rss.cbc.ca/lineup/offbeat.xml\",\n            \"https://www.cbc.ca/cmlink/rss-cbcaboriginal\",\n            \n            \"https://globalnews.ca/feed/\",\n            \"https://globalnews.ca/canada/feed/\",\n            \"https://globalnews.ca/world/feed/\",\n            \"https://globalnews.ca/politics/feed/\",\n            \"https://globalnews.ca/money/feed/\",\n            \"https://globalnews.ca/health/feed/\",\n            \"https://globalnews.ca/entertainment/feed/\",\n            \"https://globalnews.ca/environment/feed/\",\n            \"https://globalnews.ca/tech/feed/\",\n            \"https://globalnews.ca/sports/feed/\",\n            \n            \"https://www.ctvnews.ca/rss/ctvnews-ca-top-stories-public-rss-1.822009\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-canada-public-rss-1.822284\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-world-public-rss-1.822289\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-entertainment-public-rss-1.822292\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-politics-public-rss-1.822302\",\n            \"https://www.ctvnews.ca/rss/lifestyle/ctv-news-lifestyle-1.3407722\",\n            \"https://www.ctvnews.ca/rss/business/ctv-news-business-headlines-1.867648\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-sci-tech-public-rss-1.822295\",\n            \"https://www.ctvnews.ca/rss/sports/ctv-news-sports-1.3407726\",\n            \"https://www.ctvnews.ca/rss/ctvnews-ca-health-public-rss-1.822299\",\n            \"https://www.ctvnews.ca/rss/autos/ctv-news-autos-1.867636\",\n            };\n\n    public static void main(String[] args) throws Throwable {\n        \n        SequenceReader sequenceReader = new SequenceReader();\n        \n        for (String url : URLS) {\n            BufferedReader input = new BufferedReader(new InputStreamReader(new URL(url).openStream(), \"UTF-8\"));\n            sequenceReader.add(new XmlRecordReader(input).addRecordBreak(\"/rss/channel/item\"));\n        }\n        \n        DataReader reader = sequenceReader;\n        \n        reader = new TransformingReader(reader)\n                .add(new BasicFieldTransformer(\"title\").lowerCase())\n                .add(new Ngrams(\"title\", \"phrase\", NGRAMS));\n        \n        reader = new GroupByReader(reader, \"phrase\")\n                .setExcludeNulls(true)\n                .count(\"count\", true);\n        \n        reader = new SortingReader(reader).desc(\"count\").asc(\"phrase\");\n        \n        reader = new LimitReader(reader, TOP_PHRASES);\n        \n        DataWriter writer = new CSVWriter(new OutputStreamWriter(System.out))\n                .setFieldNamesInFirstRow(true);\n   \n        Job.run(reader, writer);\n        \n    }\n    \n}</pre><p>&nbsp;</p>\n<h2>Multiple Field Support</h2>\n<p>The <code>BasicFieldTransformer</code>, <code>FieldTransformer</code>, and <code>FieldFilter</code> classes can now accept more than one field name.  You no longer have to duplicate lines of code to perform the same operations or filters on multiple fields, just pass them all into the constructors.</p>\n<h2>And More</h2>\n<p>Version 4.4 also includes improvements in the areas of:</p>\n<ul>\n<li>Multi-threading</li>\n<li>Debugging & diagnostic</li>\n<li>The expression language</li>\n<li>Job management and cancellation</li>\n<li>BigInteger and BigDecimal support</li>\n<li>Twitter data readers</li>\n<li>XPath expression handling</li>\n</ul>\n<p>See the <a href=\"https://northconcepts.com/changelog/\">changelog</a> for the complete list of updates.</p>\n<div class=\"text-center\"><a class=\"btn btn-primary\" href=\"https://northconcepts.com/pricing/\">Download Data Pipeline 4.4 now!</a></div>\n<p>The post <a rel=\"nofollow\" href=\"https://northconcepts.com/blog/2018/07/31/data-pipeline-4-4-now-available/\">Data Pipeline 4.4 Now Available</a> appeared first on <a rel=\"nofollow\" href=\"https://northconcepts.com/blog\">Data Pipeline</a>.</p>\n",
  "wfw:commentRss": "https://northconcepts.com/blog/2018/07/31/data-pipeline-4-4-now-available/feed/",
  "slash:comments": 0
}