{
  "title": "The Entropy of a Normal Distribution",
  "link": "",
  "published": "2015-06-13T23:30:00+01:00",
  "updated": "2015-06-13T23:30:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-06-13:/sebastian/blog/the-entropy-of-a-normal-distribution.html",
  "summary": "<p>The <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution\">multivariate normal\ndistribution</a>\nis one of the most important probability distributions for multivariate data.\nIn this post we will look at the entropy of this distribution and how to\nestimate the entropy given an iid sample.</p>\n<p>For a multivariate â€¦</p>",
  "content": "<p>The <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution\">multivariate normal\ndistribution</a>\nis one of the most important probability distributions for multivariate data.\nIn this post we will look at the entropy of this distribution and how to\nestimate the entropy given an iid sample.</p>\n<p>For a multivariate normal distribution in <span class=\"math\">\\(k\\)</span> dimensions in standard form with\nmean vector <span class=\"math\">\\(\\mathbf{\\mu} \\in \\mathbb{R}^k\\)</span> and covariance matrix\n<span class=\"math\">\\(\\mathbf{\\Sigma}\\)</span> we have the density function</p>\n<div class=\"math\">$$f(\\mathbb{x};\\mathbf{\\mu},\\mathbf{\\Sigma}) =\n    \\frac{1}{\\sqrt{(2\\pi)^k |\\mathbf{\\Sigma}|}}\n    \\exp\\left(-\\frac{1}{2} (\\mathbf{x}-\\mathbf{\\mu})^T\n        \\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\mathbf{\\mu})\\right).$$</div>\n<p>For this density, the differential entropy takes the simple form</p>\n<div class=\"math\">\\begin{equation}\nH = \\frac{k}{2} + \\frac{k}{2} \\log(2\\pi)\n    + \\frac{1}{2} \\log |\\mathbf{\\Sigma}|.\\label{eqn:Hnormal}\n\\end{equation}</div>\n<p>In practice we are often provided with a sample</p>\n<div class=\"math\">$$\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{\\Sigma}),\n    \\quad i=1,\\dots,n,$$</div>\n<p>without knowledge of <span class=\"math\">\\(\\mathbf{\\mu}\\)</span> nor <span class=\"math\">\\(\\mathbf{\\Sigma}\\)</span>.\nWe are then interested in estimating the entropy of the distribution from the\nsample.</p>\n<h2>Plugin Estimator</h2>\n<p>The simplest method to estimate the entropy is to first estimate the mean as\nthe empirical mean,</p>\n<div class=\"math\">$$\\hat{\\mathbf{\\mu}} =\n    \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i,$$</div>\n<p>and the sample covariance as</p>\n<div class=\"math\">$$\\hat{\\mathbf{\\Sigma}} =\n    \\frac{1}{n-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\mathbf{\\mu}})\n        (\\mathbf{x}_i - \\hat{\\mathbf{\\mu}})^T.$$</div>\n<p>Given these two estimates we simply use equation <span class=\"math\">\\((\\ref{eqn:Hnormal})\\)</span> on\n<span class=\"math\">\\(\\mathcal{N}(\\hat{\\mathbf{\\mu}},\\hat{\\mathbf{\\Sigma}})\\)</span>.  (We can also use\n<span class=\"math\">\\(\\mathcal{N}(\\mathbf{0},\\hat{\\mathbf{\\Sigma}})\\)</span> instead as the entropy is\ninvariant under translation.)</p>\n<p>This is called a <em>plugin estimate</em> because we first estimate parameters of a\ndistribution, then plug these into the analytic expression for the quantity of\ninterest.</p>\n<p>It turns out that the plugin estimator systematically underestimates the true\nentropy and that one can use improved estimators.\nThis is not special and plugin estimates are often biased or otherwise\ndeficient.\nIn case of the problem of estimating the entropy of an unknown normal\ndistribution however, the known results are especially beautiful.\nIn particular,</p>\n<ul>\n<li>there exist unbiased estimators,</li>\n<li>there exist an estimator that is a <a href=\"http://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator\">uniformly minimum variance unbiased\n  estimator</a>\n  (within a restricted class, see below),</li>\n<li>this estimator is also a (generalized) Bayesian estimator under the\n  squared-loss, with an improper prior distribution.</li>\n</ul>\n<p>Hence, for this case, a single estimator is satisfactory from both a Bayesian\nand frequentist viewpoint, and moreover it is easily computable.</p>\n<p>Great, we will look at this estimator, but first look at an earlier work that\nstudies a simpler case.</p>\n<h2>Ahmed and Gokhale, 1989</h2>\n<p>An optimal <a href=\"http://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator\">UMVUE\nestimator</a>\nfor the problem of a zero-mean Normal distribution\n<span class=\"math\">\\(\\mathcal{N}(\\mathbf{0},\\Sigma)\\)</span> has been found by <a href=\"http://ee364b.googlecode.com/svn-history/r24/trunk/references/00030996.pdf\">(Ahmed and Gokhale,\n1989)</a>.\nThis is a restricted case: while the entropy does not depend on the mean of\nthe distribution, it does affect the estimation of the sample covariance\nmatrix.</p>\n<p>For a sample their estimator is</p>\n<div class=\"math\">$$\\hat{H}_{\\textrm{AG}} =\n    \\frac{k}{2} \\log(e\\pi)\n    + \\frac{1}{2} \\log \\left|\\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^T\\right|\n    - \\frac{1}{2} \\sum_{j=1}^d \\psi\\left(\\frac{n+1-j}{2}\\right),$$</div>\n<p>where <span class=\"math\">\\(\\psi\\)</span> is the <a href=\"http://en.wikipedia.org/wiki/Digamma_function\">digamma\nfunction</a>.</p>\n<p>If you know the mean of your distribution (so you can center your data to\nensure <span class=\"math\">\\(\\mu=0\\)</span>), this estimator provides a big improvement over the plugin\nestimate.  Here is an example in mean squared error and bias, were <span class=\"math\">\\(\\Sigma\n\\sim \\textrm{Wishart}(\\nu,I_k)\\)</span> and <span class=\"math\">\\(\\mathbf{x}_i \\sim\n\\mathcal{N}(\\mathbf{0},\\Sigma)\\)</span>, with <span class=\"math\">\\(k=3\\)</span> and <span class=\"math\">\\(n=20\\)</span>.\nThe plot below shows a Monte Carlo result with <span class=\"math\">\\(80,000\\)</span> replicates.</p>\n<p><img alt=\"Ahmed and Gokhale estimator\" src=\"http://www.nowozin.net/sebastian/blog/images/normal-entropy-ag.svg\"></p>\n<p>As promised, we can observe a big improvement over the plugin estimate, and we\nalso see that the Ahmed Gokhale estimator is indeed unbiased.</p>\n<p>Here is a <a href=\"http://julialang.org/\">Julia</a> implementation.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">function</span> <span class=\"n\">entropy_ag</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n    <span class=\"c\"># X is a (k,n) matrix, samples in columns</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">k</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">n</span>\n        <span class=\"n\">C</span> <span class=\"o\">+=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">&#39;</span>\n    <span class=\"k\">end</span>\n    <span class=\"n\">H</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">k</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mf\">1.0</span> <span class=\"o\">+</span> <span class=\"n\">log</span><span class=\"p\">(</span><span class=\"nb\">pi</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">logdet</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">k</span>\n        <span class=\"n\">H</span> <span class=\"o\">-=</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">digamma</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">i</span><span class=\"p\">))</span>\n    <span class=\"k\">end</span>\n    <span class=\"n\">H</span>\n<span class=\"k\">end</span>\n</code></pre></div>\n\n<p>Because the case of a known mean is maybe less interesting, we go\nstraight to the general case.</p>\n<h2>Misra, Singh, and Demchuk, 2005</h2>\n<p>In <a href=\"http://www.sciencedirect.com/science/article/pii/S0047259X03001787\">(Misra, Singh, and Demchuk,\n2005)</a>\n(here is the\n<a href=\"http://www.researchgate.net/profile/Neeraj_Misra4/publication/23644689_Estimation_of_the_entropy_of_a_multivariate_normal_distribution/links/00b7d51df6ad392c72000000.pdf\">PDF</a>)\nthe authors do a thorough job of analyzing the general case.\nBeside a detailed bias and risk analysis the paper proposes two estimators for\nthe general case:</p>\n<ul>\n<li>An UMVUE estimator in a restricted class of estimators, that is a slight\n  variation of the Ahmed and Gokhale estimator;</li>\n<li>A shrinkage estimator in a larger class, which is proven to dominate the\n  UMVUE estimator in the restricted class.</li>\n</ul>\n<p>The authors are apparently unaware of the work of Ahmed and Gokhale.\nFor their UMVUE estimator <span class=\"math\">\\(\\hat{H}_{\\textrm{MSD}}\\)</span> they use the matrix</p>\n<div class=\"math\">$$S = \\sum_{i=1}^n (\\mathbf{x}_i-\\hat{\\mu})(\\mathbf{x}_i-\\hat{\\mu})^T$$</div>\n<p>and define</p>\n<div class=\"math\">\\begin{equation}\n\\hat{H}_{\\textrm{MSD}} =\n    \\frac{k}{2} \\log(e\\pi)\n    + \\frac{1}{2} \\log |S|\n    - \\frac{1}{2} \\sum_{j=1}^d \\psi\\left(\\frac{n-j}{2}\\right).\n    \\label{Hmsd}\n\\end{equation}</div>\n<p>Can you spot the difference to the Ahmed and Gokhale estimator?  There are\ntwo: the matrix <span class=\"math\">\\(S\\)</span> is centered using the <em>sample mean</em> <span class=\"math\">\\(\\hat{\\mu}\\)</span>, and, to\nadjust for the use of the sample mean for centering, the argument to the\ndigamma function is shifted by <span class=\"math\">\\(1/2\\)</span>.</p>\n<p>Here is a <a href=\"http://julialang.org/\">Julia</a> implementation.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">function</span> <span class=\"n\">entropy_msd</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n    <span class=\"c\"># X is a (k,n) matrix, samples in columns</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"n\">Xbar</span> <span class=\"o\">=</span> <span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">Xs</span> <span class=\"o\">=</span> <span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">Xbar</span>\n    <span class=\"n\">S</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">k</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">n</span>\n        <span class=\"n\">S</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">Xbar</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">Xbar</span><span class=\"p\">)</span><span class=\"o\">&#39;</span>\n    <span class=\"k\">end</span>\n\n    <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">k</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mf\">1.0</span> <span class=\"o\">+</span> <span class=\"n\">log</span><span class=\"p\">(</span><span class=\"nb\">pi</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">logdet</span><span class=\"p\">(</span><span class=\"n\">S</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">k</span>\n        <span class=\"n\">res</span> <span class=\"o\">-=</span> <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">digamma</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">i</span><span class=\"p\">))</span>\n    <span class=\"k\">end</span>\n    <span class=\"n\">res</span>\n<span class=\"k\">end</span>\n</code></pre></div>\n\n<h3>Outline of Derivation</h3>\n<p>The key result that is used for deriving both the MSD and the AG estimator is\na lemma due to <a href=\"http://www.stat.illinois.edu/statnews/wijsman_memoriam.htm\">Robert\nWijsman</a> from 1957\n(<a href=\"http://projecteuclid.org/euclid.aoms/1177706969\">PDF</a>).</p>\n<p>Wijsman proved a result relating the determinants of two matrices:\nthe covariance matrix <span class=\"math\">\\(\\Sigma\\)</span> of a multivariate Normal distribution, and\nthe empirical outer product matrix <span class=\"math\">\\(X X^T\\)</span> of a sample <span class=\"math\">\\(X \\in\n\\mathbb{R}^{n\\times k}\\)</span> from that Normal.\nIn Lemma 3 of the above paper he showed</p>\n<div class=\"math\">$$\\frac{|X X^T|}{|\\Sigma|} = \\prod_{i=1}^k \\chi_{n-i+1}^2.$$</div>\n<p>By taking the logarithm of this equation we can relate the central quantity in\nthe differential entropy, namely <span class=\"math\">\\(\\log |\\Sigma|\\)</span> to the log-determinant of the\nsample outer product matrix.</p>\n<p>The sample outer product matrix of a zero-mean multivariate Normal sample with\n<span class=\"math\">\\(n \\geq k\\)</span> is known to be distributed according to a Wishart distribution,\nwith many known analytic properties.\nBy using the known properties of the Wishart and <span class=\"math\">\\(\\chi^2\\)</span> distributions this\nallows the derivation and proving unbiasedness of the AG and MSD estimators.</p>\n<h3>Generalized Bayes</h3>\n<p>Misra, Singh, and Demchuk also show that their MSD estimator is the mean of a\nposterior that arises from a full Bayesian treatment with an improper prior.\nThis prior is shown to be, in (Theorem 2.3 in Misra et al., 2005),</p>\n<div class=\"math\">$$\\pi(\\mu,\\Sigma) = \\frac{1}{|\\Sigma|^{(k+1)/2}}$$</div>\n<p>This is a most satisfying result: a frequentist-optimal estimator in large\nclass of possible estimators is shown to be also a Bayes estimator for a\nsuitable matching prior.</p>\n<p>Because the posterior is proper for <span class=\"math\">\\(n \\geq k\\)</span>, one could also use the\nproposed prior to derive posterior credible regions for the entropy, and most\nlikely this is a good choice in that it could achieve good coverage\nproperties.</p>\n<h2>Brewster-Zidek estimator</h2>\n<p>Going even further, Misra and coauthors also show that while the MSD estimator\nis optimal in the class of <em>affine-equivariant</em> estimators, when one enlarges\nthe class of possible estimators there exist estimators which uniformly\ndominate the MSD estimator by achieving a lower risk.</p>\n<p>They propose a shrinkage estimator, termed <em>Brewster-Zidek estimator</em> which I\ngive here without further details.</p>\n<div class=\"math\">$$\\hat{H}_{BZ} = \\frac{k}{2} \\log(2 e \\pi)\n    + \\frac{1}{2} \\log |S + YY^T| + \\frac{1}{2}(\\log T - d(T))$$</div>\n<div class=\"math\">$$d(r) = \\frac{\\int_r^1 t^{\\frac{n-k}{2}-1} (1-t)^{\\frac{k}{2}-1}\n    \\left[\\log t + k \\log 2 + \\sum_{i=1}^k \\psi\\left(\\frac{n-i+1}{2}\\right)\n        \\right]\n    \\textrm{d}t}{\\int_r^1 t^{\\frac{n-k}{2}-1}(1-t)^{\\frac{k}{2}-1}\n    \\textrm{d}t}$$</div>\n<div class=\"math\">$$T = |S| |S+YY^T|^{-1}$$</div>\n<div class=\"math\">$$Y = \\sqrt{n} \\hat{\\mu}$$</div>\n<p>Here is a Julia implementation using numerical integration for evaluating\n<span class=\"math\">\\(d(r)\\)</span>.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">function</span> <span class=\"n\">entropy_bz</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n    <span class=\"c\"># X is a (p,n) matrix, samples in columns</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"n\">Bfun</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">t</span><span class=\"o\">^</span><span class=\"p\">((</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">^</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">function</span> <span class=\"n\">Afun</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">p</span><span class=\"o\">*</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">p</span>\n            <span class=\"n\">res</span> <span class=\"o\">+=</span> <span class=\"n\">digamma</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"k\">end</span>\n        <span class=\"n\">res</span> <span class=\"o\">*</span> <span class=\"n\">Bfun</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span>\n    <span class=\"k\">end</span>\n    <span class=\"n\">A</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">::</span><span class=\"kt\">Float64</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">quadgk</span><span class=\"p\">(</span><span class=\"n\">Afun</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">B</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">::</span><span class=\"kt\">Float64</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">quadgk</span><span class=\"p\">(</span><span class=\"n\">Bfun</span><span class=\"p\">,</span> <span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">d</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">B</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">)</span>\n\n    <span class=\"n\">Xbar</span> <span class=\"o\">=</span> <span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">Xs</span> <span class=\"o\">=</span> <span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">Xbar</span>\n    <span class=\"n\">S</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">:</span><span class=\"n\">n</span>\n        <span class=\"n\">S</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">Xbar</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"o\">:</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"n\">Xbar</span><span class=\"p\">)</span><span class=\"o\">&#39;</span>\n    <span class=\"k\">end</span>\n\n    <span class=\"n\">T</span> <span class=\"o\">=</span> <span class=\"n\">det</span><span class=\"p\">(</span><span class=\"n\">S</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"n\">det</span><span class=\"p\">(</span><span class=\"n\">S</span><span class=\"o\">+</span><span class=\"n\">Xs</span><span class=\"o\">*</span><span class=\"n\">Xs</span><span class=\"o\">&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">dBZ</span> <span class=\"o\">=</span> <span class=\"n\">logdet</span><span class=\"p\">(</span><span class=\"n\">S</span> <span class=\"o\">+</span> <span class=\"n\">Xs</span><span class=\"o\">*</span><span class=\"n\">Xs</span><span class=\"o\">&#39;</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">d</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">T</span><span class=\"p\">)</span>\n\n    <span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"nb\">pi</span><span class=\"p\">))</span><span class=\"o\">+</span><span class=\"n\">dBZ</span><span class=\"p\">)</span>\n<span class=\"k\">end</span>\n</code></pre></div>\n\n<h2>Shoot-out</h2>\n<p>Remember the zero-mean case?  Let us start with this case.\nI use <span class=\"math\">\\(k=3\\)</span> and <span class=\"math\">\\(n=20\\)</span> as before, and <span class=\"math\">\\(\\Sigma \\sim \\textrm{Wishart}(\\nu,I_k)\\)</span>.\nThen samples are generated as\n<span class=\"math\">\\(\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0},\\Sigma)\\)</span>.\nAll numbers are from <span class=\"math\">\\(80,000\\)</span> replications of the full procedure.</p>\n<p><img alt=\"Four estimators\" src=\"http://www.nowozin.net/sebastian/blog/images/normal-entropy-all-zeromean.svg\"></p>\n<p>What you can see from the above plot is that the AG estimator which is UMVUE\nfor this special case dominates the MSD estimator.\nBoth unbiased estimators are indeed unbiased.\nIn terms of risk the Brewster-Zidek estimator is indistinguishable from the\nMSD estimator.</p>\n<p>Now, what about <span class=\"math\">\\(\\mu \\neq \\mathbf{0}\\)</span>?\nHere, for the simulation the setting is as before, but the mean is\n<span class=\"math\">\\(\\mu \\sim \\mathcal{N}(\\mathbf{0},2I)\\)</span>, so that samples are distributed as\n<span class=\"math\">\\(\\mathbf{x}_i \\sim \\mathcal{N}(\\mu,\\Sigma)\\)</span>.</p>\n<p><img alt=\"Four estimators\" src=\"http://www.nowozin.net/sebastian/blog/images/normal-entropy-all.svg\"></p>\n<p>The result shows that the AG estimator becomes useless if its assumption is\nviolated, as is to be expected.  (Interestingly, if we were to try using the\nscaled sample covariance matrix <span class=\"math\">\\(n \\hat{\\Sigma}\\)</span> with the AG estimator it is\nreasonable but biased, that is, it has lost its UMVUE property.)\nThe MSD estimator and the Brewster-Zidek estimators are virtually\nindistinguishable and seem to be both unbiased in this case.</p>\n<h2>Conclusion</h2>\n<p>Estimating the entropy of a multivariate Normal distribution from a sample has\na satisfying solution, the MSD estimator <span class=\"math\">\\((\\ref{Hmsd})\\)</span>, which can be robustly\nused in all circumstances.  It is computationally efficient, and with\nsufficient samples, <span class=\"math\">\\(n \\geq k\\)</span>, the Bayesian interpretation also provides a\nproper posterior distribution over <span class=\"math\">\\(\\mu\\)</span> and <span class=\"math\">\\(\\Sigma\\)</span> which can be used to\nderive a posterior distribution over the entropy.</p>\n<p><em>Acknowledgements</em>.  I thank\n<a href=\"http://ei.is.tuebingen.mpg.de/person/jpeters\">Jonas Peters</a> for reading a\ndraft version of the article and providing feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}