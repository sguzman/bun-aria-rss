{
  "title": "Estimating Discrete Entropy, Part 2",
  "link": "",
  "published": "2015-02-21T19:00:00+00:00",
  "updated": "2015-02-21T19:00:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-02-21:/sebastian/blog/estimating-discrete-entropy-part-2.html",
  "summary": "<p>In the <a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html\">last part</a> we have looked at the\nbasic problem of discrete entropy estimation.\nIn this article we will see a number of proposals of improved estimators.</p>\n<h3>Miller Correction</h3>\n<p>In 1955 George Miller proposed a simple correction to the â€¦</p>",
  "content": "<p>In the <a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html\">last part</a> we have looked at the\nbasic problem of discrete entropy estimation.\nIn this article we will see a number of proposals of improved estimators.</p>\n<h3>Miller Correction</h3>\n<p>In 1955 George Miller proposed a simple correction to the naive plugin\nestimator <span class=\"math\">\\(\\hat{H}_N\\)</span> by adding the constant offset in the bias expression as\nfollows.</p>\n<div class=\"math\">$$\\hat{H}_M = \\hat{H}_N + \\frac{K-1}{2n}.$$</div>\n<p>This is an improvement over the plugin estimator but the added offset does not\ndepend on the distribution but only on the sample size.  We can do better.</p>\n<p>(A variant of the Miller estimator for the infinite alphabet case is the so\ncalled Miller-Madow estimator in which the quantity <span class=\"math\">\\(K\\)</span> is estimated from the\ndata as well.)</p>\n<h3>Jackknife Estimator</h3>\n<p>A classic method for bias correction is the\n<a href=\"http://en.wikipedia.org/wiki/Jackknife_resampling\">jackknife</a> <a href=\"http://en.wikipedia.org/wiki/Resampling_(statistics)\">resampling\nmethod</a> due to\n<a href=\"http://www.jstor.org/discover/10.2307/2332914\">(Quenouille, 1947)</a>, although\nthe somewhat catchy name is due to <a href=\"http://en.wikipedia.org/wiki/John_Tukey\">John\nTukey</a>.\n(The literature on the jackknife methodology is quite classic now.  A very\nreadable modern summary of the jackknife methodology can be found in\n<a href=\"http://www.stat.purdue.edu/~dasgupta/\">DasGupta's</a>\n<a href=\"http://www.springer.com/mathematics/probability/book/978-0-387-75970-8\">book</a>.\nAn older but still readable introduction is <a href=\"http://biomet.oxfordjournals.org/content/61/1/1.short\">(Miller,\n1974)</a>,\n<a href=\"http://www.csee.wvu.edu/~xinl/library/papers/math/statistics/jackknife.pdf\">PDF</a>.)</p>\n<p>In a nutshell, jackknife resampling methods are used to estimate bias and\nvariance of estimators.  They are typically simple to implement,\nand often computationally cheaper than the bootstrap.\nFor the bias reduction application, they often manage to reduce bias\nconsiderably, often knocking the bias down to <span class=\"math\">\\(O(n^{-2})\\)</span>.</p>\n<p>The use of jackknife bias estimation to improve entropy estimation was\nsuggested by <a href=\"http://www.jstor.org/discover/10.2307/1936227\">(Zahl, 1977)</a>.\nThe jackknife bias-corrected estimator of the plugin estimator is given as\nfollows.</p>\n<div class=\"math\">\\begin{equation}\n    \\hat{H}_{J} = n \\hat{H}_N - (n-1) \\hat{H}^{(\\cdot)}_N.\n    \\label{H:jackknife}\n\\end{equation}</div>\n<p>The quantity <span class=\"math\">\\(\\hat{H}^{(\\cdot)}_N\\)</span> is the average of <span class=\"math\">\\(n\\)</span> estimates\nobtained by leaving out a single observation.  Thus, writing\n<span class=\"math\">\\(\\mathbb{h}=(h_1,\\dots,h_K)\\)</span> for the histogram of bin counts on the full\nsample, and <span class=\"math\">\\(\\mathbb{h}_{\\setminus i}\\)</span> for the histogram with the count in\n<span class=\"math\">\\(X_i=k\\)</span> reduced by one, we have</p>\n<div class=\"math\">$$\\hat{H}^{\\setminus i}_N := \\hat{H}_N(\\mathbb{h}_{\\setminus i}),$$</div>\n<p>and the mean of these quantities,</p>\n<div class=\"math\">$$\\hat{H}^{(\\cdot)}_N :=\n    \\frac{1}{n} \\sum_{i=1}^n \\hat{H}^{\\setminus i}_N.$$</div>\n<p>Interestingly, normally it would be expensive to compute <span class=\"math\">\\(n\\)</span> leave-one-out\nestimates.  Here however, two tricks are possible: First, because the\nhistogram is a sufficient statistic, we need to compute only <span class=\"math\">\\(K\\)</span> holdout\nestimates instead of <span class=\"math\">\\(n\\)</span>.  Second, one can interleave computation in such a\nway that computing each holdout estimate is <span class=\"math\">\\(O(1)\\)</span>, reducing the overall\ncomputation of <span class=\"math\">\\((\\ref{H:jackknife})\\)</span> to <span class=\"math\">\\(O(K)\\)</span> runtime and no additional\nmemory over the plugin estimate.  In essence, the computational complexity is\ncomparable to that of the inexpensive plugin estimate, making the jackknife\nestimator computationally cheap.</p>\n<h3>Grassberger Estimator</h3>\n<p>Another proposal for an improved estimator is due to\n<a href=\"http://www.ucalgary.ca/complexity/people/faculty/peter\">Peter Grassberger</a>.\nIn <a href=\"http://arxiv.org/abs/physics/0307138\">(Grassberger, 2003)</a>\nhe derives two estimators based on an argument using analytic continuation\nwhich I have to admit is somewhat beyond my grasp.\nThe better of the two estimators is the following:</p>\n<div class=\"math\">$$\\hat{H}_G = \\log n - \\frac{1}{n} \\sum_{k=1}^K h_k G(h_k),$$</div>\n<p>where the logarithm of the original naive estimator <span class=\"math\">\\(\\hat{H}_N\\)</span> have been\nreplaced by a scalar function <span class=\"math\">\\(G\\)</span>, defined as</p>\n<div class=\"math\">$$G(h) = \\psi(h) + \\frac{1}{2} (-1)^{h}\n    \\left(\\psi(\\frac{h+1}{2}) - \\psi(\\frac{h}{2})\\right).$$</div>\n<p>The function <span class=\"math\">\\(\\psi\\)</span> is the <a href=\"http://en.wikipedia.org/wiki/Digamma_function\">digamma\nfunction</a>.\n(The function <span class=\"math\">\\(G(h)\\)</span> is the solution of <span class=\"math\">\\(G(h)=\\psi(h)+(-1)^h \\int^1_0\n\\frac{x^{h-1}}{x+1} \\textrm{d}x\\)</span> given as equation <span class=\"math\">\\((30)\\)</span> in <a href=\"http://arxiv.org/pdf/physics/0307138v2.pdf\">the\npaper</a>.)\nComputationally this estimator is almost as efficient as the original plugin\nestimator, because for integer arguments the digamma function can be\naccurately approximated by an <a href=\"http://en.wikipedia.org/wiki/Digamma_function#Computation_and_approximation\">efficient series\nexpansion</a>.</p>\n<p>When compared to the plugin estimator (in <a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html\">histogram count\nform</a>), we can see an upwards\ncorrection of this estimator but also an interesting difference between even\nand odd histogram counts.</p>\n<p><img alt=\"xlogx compared to xGx\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-2-gb.svg\"></p>\n<p>Unfortunately, the original derivation in Grassberger's paper is quite\ninvolved and beyond my full understanding.\nHowever, for practical purposes, among the computationally efficient\nestimators, the 2003 Grassberger estimator is probably the most useful and\nrobust estimator.</p>\n<h2>Experiment</h2>\n<p>The following plots show a simple evaluation of some popular discrete entropy\nestimators.  We assume a categorical distribution with <span class=\"math\">\\(K=64\\)</span> outcomes with\nthe probability vector <span class=\"math\">\\(\\mathbb{p}\\)</span> sampled from a symmetric Dirichlet\ndistribution with hyperparameter <span class=\"math\">\\(\\alpha \\in [0.25,5.0]\\)</span>.\nWe obtain <span class=\"math\">\\(n=100\\)</span> samples from the distribution and estimate the entropy based\non this sample.  For each <span class=\"math\">\\(\\alpha\\)</span> we repeat this procedure 5,000 times to\nestimate the root mean squared error (RMSE) and bias.</p>\n<p>We plot all the estimators discussed above, but also plot four additional\nestimators:</p>\n<ul>\n<li><a href=\"http://arxiv.org/abs/0811.3579\">Hausser-Strimmer estimator</a>, based on\na shrinkage estimate,</li>\n<li><a href=\"http://www.martinvinck.com/page2/assets/PRE_entropy00_subm_rev_00.pdf\">Polynomial estimator</a>\ndue to Vinck et al.; this is equivalent to <a href=\"http://math2.uncc.edu/~zzhang/papers/ZhangNUCO2012.pdf\">Zhiyi Zhang's\nestimator</a>,\nbut numerically simpler and more stable to evaluate.  (I only mention this\nhere because I have not found this mentioned elsewhere.)</li>\n<li>Two Bayesian estimators (Bayes and NSB), to be discussed in the next post.</li>\n</ul>\n<p><img alt=\"RMSE and bias experiments for seven different discrete entropy\nestimators\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-2-exp.svg\"></p>\n<p>These experiments are not fully representative because the Dirichlet prior\nmakes assumptions which may not be satisfied in your application; in\nparticular, this experiment considers the well-sampled case (<span class=\"math\">\\(N &gt; K\\)</span>), and\nsimple bias correction methods work well in this regime (this was pointed out\nto me by Ilya and Jonas); however, some clear trends are visible:</p>\n<ul>\n<li>The Plugin estimator fares badly on both RMSE and bias;</li>\n<li>The Miller estimator fares less badly, suggesting RMSE is affected mainly by\nbias, however, significant errors remain for small values of <span class=\"math\">\\(\\alpha\\)</span>;</li>\n<li>The Bayes estimator fares almost as bad as the plugin estimator, except for\n<span class=\"math\">\\(\\alpha=1/2\\)</span>.  More on this point in the next post;</li>\n<li>The Jackknife, Grassberger 2003, and NSB estimators provide excellent\nperformance throughout the whole range of <span class=\"math\">\\(\\alpha\\)</span> values.</li>\n<li>The performance of the Polynomial and Hausser estimates are mediocre.</li>\n</ul>\n<p>In the next part we will be looking at Bayesian estimators.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://www.memming.com/\">Il Memming Park</a>,\n<a href=\"http://ei.is.tuebingen.mpg.de/person/jpeters\">Jonas Peters</a>, and\n<a href=\"http://www.physics.emory.edu/home/people/faculty/nemenman-ilya.html\">Ilya Nemenman</a>\nfor reading a draft version of the article and providing very helpful feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}