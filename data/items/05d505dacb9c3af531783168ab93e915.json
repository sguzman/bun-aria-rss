{
  "title": "Interpreting the results of linear regression",
  "link": "",
  "published": "2016-06-29T14:54:00-07:00",
  "updated": "2016-06-29T14:54:00-07:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2016-06-29:/interpret-linear-regression",
  "summary": "<p>Our <a href=\"http://efavdb.github.io/linear-regression\">last post</a> showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &#8212; both in the estimates of individual linear regression coefficients and the …</p>",
  "content": "<p>Our <a href=\"http://efavdb.github.io/linear-regression\">last post</a> showed how to obtain the least-squares solution for linear regression and discussed the idea of sampling variability in the best estimates for the coefficients. In this post, we continue the discussion about uncertainty in linear regression &#8212; both in the estimates of individual linear regression coefficients and the quality of the overall&nbsp;fit.</p>\n<p>Specifically, we&#8217;ll discuss how to calculate the 95% confidence intervals and p-values from hypothesis tests that are output by many statistical packages like python&#8217;s statsmodels or R. An example with code is provided at the&nbsp;end.</p>\n<h2>Review</h2>\n<p>We wish to predict a scalar response variable <span class=\"math\">\\(y_i\\)</span> given a vector of predictors <span class=\"math\">\\(\\vec{x}_i\\)</span> of dimension <span class=\"math\">\\(K\\)</span>. In linear regression, we assume that <span class=\"math\">\\(y_i\\)</span> is a linear function of <span class=\"math\">\\(\\vec{x}_i\\)</span>, parameterized by a set of coefficients <span class=\"math\">\\(\\vec{\\beta}\\)</span> and an error term <span class=\"math\">\\(\\epsilon_i\\)</span>. The linear model (in matrix format and dropping the arrows over the vectors) for predicting <span class=\"math\">\\(N\\)</span> response variables&nbsp;is\n</p>\n<div class=\"math\">\\begin{align}\\tag{1}\ny = X\\beta + \\epsilon.\n\\end{align}</div>\n<p>The dimensions of each component are: dim(<span class=\"math\">\\(X\\)</span>) = (<span class=\"math\">\\(N\\)</span>,<span class=\"math\">\\(K\\)</span>), dim(<span class=\"math\">\\(\\beta\\)</span>) = (<span class=\"math\">\\(K\\)</span>,1), dim(<span class=\"math\">\\(y\\)</span>) = dim(<span class=\"math\">\\(\\epsilon\\)</span>) = (<span class=\"math\">\\(N\\)</span>,1), where <span class=\"math\">\\(N\\)</span> = # of examples, <span class=\"math\">\\(K\\)</span> = # of regressors / predictors, counting an optional intercept/constant&nbsp;term.</p>\n<p>The ordinary least-squares best estimator of the coefficients, <span class=\"math\">\\(\\hat{\\beta}\\)</span>, was <a href=\"http://efavdb.github.io/linear-regression\">derived last time</a>:\n</p>\n<div class=\"math\">\\begin{align}\\tag{2}\\label{optimal}\n\\hat{\\beta} = (X'X)^{-1}X'y,\n\\end{align}</div>\n<p>where the hat &#8220;^&#8221; denotes an estimator, not a true population&nbsp;parameter.</p>\n<p>(\\ref{optimal}) is a point estimate, but fitting different samples of data from the population will cause the best estimators to shift around. The amount of shifting can be explained by the variance-covariance matrix of <span class=\"math\">\\(\\hat{\\beta}\\)</span>, <a href=\"http://efavdb.github.io/linear-regression\">also derived</a> last time (independent of assumptions of&nbsp;normality):\n</p>\n<div class=\"math\">\\begin{align}\\tag{3}\\label{cov}\ncov(\\hat{\\beta}, \\hat{\\beta}) = \\sigma^2 (X'X)^{-1}.\n\\end{align}</div>\n<h2>Goodness of fit - <span class=\"math\">\\(R^2\\)</span></h2>\n<p>To get a better feel for (\\ref{cov}), it&#8217;s helpful to rewrite it in terms of the coefficient of determination <span class=\"math\">\\(R^2\\)</span>. <span class=\"math\">\\(R^2\\)</span> measures how much of the variation in the response variable <span class=\"math\">\\(y\\)</span> is explained by variation in the regressors <span class=\"math\">\\(X\\)</span> (as opposed to the unexplained variation from <span class=\"math\">\\(\\epsilon\\)</span>).</p>\n<p>The variation in <span class=\"math\">\\(y\\)</span>, i.e. the &#8220;total sum of squares&#8221; <span class=\"math\">\\(SST\\)</span>, can be partitioned into the sum of two terms, &#8220;regression sum of squares&#8221; and &#8220;error sum of squares&#8221;: <span class=\"math\">\\(SST = SSR + SSE\\)</span>.</p>\n<p>For convenience, let&#8217;s center <span class=\"math\">\\(y\\)</span> and <span class=\"math\">\\(X\\)</span> around their means, e.g. <span class=\"math\">\\(y \\rightarrow y - \\bar{y}\\)</span> so that the mean <span class=\"math\">\\(\\bar{y}=0\\)</span> for the centered variables.&nbsp;Then,\n</p>\n<div class=\"math\">\\begin{align}\\tag{4}\\label{SS}\nSST &amp;= \\sum_i^N (y - \\bar{y})^2 = y'y \\\\\nSSR &amp;= \\sum_i^N (X\\hat{\\beta} - \\bar{y})^2 = \\hat{y}'\\hat{y} \\\\\nSSE &amp;= \\sum_i^N (y - \\hat{y})^2 = e'e,\n\\end{align}</div>\n<p>where <span class=\"math\">\\(\\hat{y} \\equiv X\\hat{\\beta}\\)</span>. Then <span class=\"math\">\\(R^2\\)</span> is defined as the ratio of the regression sum of squares to the total sum of&nbsp;squares:\n</p>\n<div class=\"math\">\\begin{align}\\tag{5}\\label{R2}\nR^2 \\equiv \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n\\end{align}</div>\n<p><span class=\"math\">\\(R^2\\)</span> ranges between 0 and 1, with 1 being a perfect fit. According to (\\ref{cov}), the variance of a single coefficient <span class=\"math\">\\(\\hat{\\beta}_k\\)</span> is proportional to the quantity <span class=\"math\">\\((X'X)_{kk}^{-1}\\)</span>, where <span class=\"math\">\\(k\\)</span> denotes the kth diagonal element of <span class=\"math\">\\((X'X)^{-1}\\)</span>, and can be rewritten&nbsp;as\n</p>\n<div class=\"math\">\\begin{align}\\tag{6}\\label{cov2}\nvar(\\hat{\\beta}_k) &amp;= \\sigma^2 (X'X)_{kk}^{-1} \\\\ &amp;= \\frac{\\sigma^2}{(1 - R_k^2)\\sum_i^N (x_{ik} - \\bar{x}_k)^2},\n\\end{align}</div>\n<p>where <span class=\"math\">\\(R_k^2\\)</span> is the <span class=\"math\">\\(R^2\\)</span> in the regression of the kth variable, <span class=\"math\">\\(x_k\\)</span>, against the other predictors <a href=\"#A1\">[A1]</a>.</p>\n<p>The key observation from (\\ref{cov2}) is that the precision in the estimator decreases if the fit is made over highly correlated regressors, for which <span class=\"math\">\\(R_k^2\\)</span> approaches 1. This problem of multicollinearity in linear regression will be manifested in our simulated&nbsp;example.</p>\n<p>(\\ref{cov2}) is also consistent with the observation from our previous post that, all things being equal, the precision in the estimator increases if the fit is made over a direction of greater variance in the&nbsp;data.</p>\n<p>In the next section, <span class=\"math\">\\(R^2\\)</span> will again be useful for interpreting the behavior of one of our test&nbsp;statistics.</p>\n<h2>Calculating test&nbsp;statistics</h2>\n<p>If we assume that the vector of residuals has a multivariate normal distribution, <span class=\"math\">\\(\\epsilon \\sim N(0, \\sigma^2I)\\)</span>, then we can construct test statistics to characterize the uncertainty in the regression. In this section, we&#8217;ll&nbsp;calculate</p>\n<p>​(a) <strong>confidence intervals</strong> - random intervals around individual estimators <span class=\"math\">\\(\\hat{\\beta}_k\\)</span> that, if constructed for regressions over multiple samples, would contain the true population parameter, <span class=\"math\">\\(\\beta_k\\)</span>, a certain fraction, e.g. 95%, of the time.\n(b) <strong>p-value</strong> - the probability of events as extreme or more extreme than an observed value (a test statistic) occurring under the null hypothesis. If the p-value is less than a given significance level <span class=\"math\">\\(\\alpha\\)</span> (a common choice is <span class=\"math\">\\(\\alpha = 0.05\\)</span>), then the null hypothesis is rejected, e.g. a regression coefficient is said to be&nbsp;significant.</p>\n<p>From the assumption of the distribution of <span class=\"math\">\\(\\epsilon\\)</span>, it follows that <span class=\"math\">\\(\\hat{\\beta}\\)</span> has a multivariate normal distribution <a href=\"#A2\">[A2]</a>:\n</p>\n<div class=\"math\">\\begin{align}\\tag{7}\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X'X)^{-1}).\n\\end{align}</div>\n<p> To be explicit, a single coefficient, <span class=\"math\">\\(\\hat{\\beta}_k\\)</span>, is distributed&nbsp;as\n</p>\n<div class=\"math\">\\begin{align}\\tag{8}\n\\hat{\\beta}_k \\sim N(\\beta_k, \\sigma^2 (X'X)_{kk}^{-1}).\n\\end{align}</div>\n<p>This variable can be standardized as a&nbsp;z-score:\n</p>\n<div class=\"math\">\\begin{align}\\tag{9}\nz_k = \\frac{\\hat{\\beta}_k - \\beta_k}{\\sigma^2 (X'X)_{kk}^{-1}} \\sim N(0,1)\n\\end{align}</div>\n<p>In practice, we don&#8217;t know the population parameter, <span class=\"math\">\\(\\sigma^2\\)</span>, so we can&#8217;t use the z-score. Instead, we can construct a pivotal quantity, a t-statistic. The t-statistic for <span class=\"math\">\\(\\hat{\\beta}_k\\)</span> follows a t-distribution with n-K degrees of freedom <a href=\"#ref1\">[1]</a>,\n</p>\n<div class=\"math\">\\begin{align}\\tag{10}\\label{tstat}\nt_{\\hat{\\beta}_k} = \\frac{\\hat{\\beta}_k - \\beta_k}{s(\\hat{\\beta}_k)} \\sim t_{n-K},\n\\end{align}</div>\n<p> where <span class=\"math\">\\(s(\\hat{\\beta}_k)\\)</span> is the standard error of <span class=\"math\">\\(\\hat{\\beta}_k\\)</span>\n</p>\n<div class=\"math\">\\begin{align}\\tag{11}\ns(\\hat{\\beta}_k)^2 = \\hat{\\sigma}^2 (X'X)_{kk}^{-1},\n\\end{align}</div>\n<p> and <span class=\"math\">\\(\\hat{\\sigma}^2\\)</span> is the unbiased estimator of <span class=\"math\">\\(\\sigma^2\\)</span>\n</p>\n<div class=\"math\">\\begin{align}\\tag{12}\n\\hat{\\sigma}^2 = \\frac{\\epsilon'\\epsilon}{n - K}.\n\\end{align}</div>\n<h3>Confidence intervals around regression&nbsp;coefficients</h3>\n<p>The <span class=\"math\">\\((1-\\alpha)\\)</span> confidence interval around an estimator, <span class=\"math\">\\(\\hat{\\beta}_k \\pm \\Delta\\)</span>, is defined such that the probability of a random interval containing the true population parameter is <span class=\"math\">\\((1-\\alpha)\\)</span>:\n</p>\n<div class=\"math\">\\begin{align}\\tag{13}\nP[\\hat{\\beta}_k - \\Delta &lt; \\beta_k &lt; \\hat{\\beta}_k + \\Delta ] = 1 - \\alpha,\n\\end{align}</div>\n<p> where <span class=\"math\">\\(\\Delta = t_{1-\\alpha/2, n-K} s(\\hat{\\beta}_k)\\)</span>, and <span class=\"math\">\\(t_{1-\\alpha/2, n-K}\\)</span> is the <span class=\"math\">\\(\\alpha/2\\)</span>-level critical value for the t-distribution with <span class=\"math\">\\(n-K\\)</span> degrees of&nbsp;freedom.</p>\n<h3>t-test for the significance of a&nbsp;predictor</h3>\n<p>Directly related to the calculation of confidence intervals is testing whether a regressor, <span class=\"math\">\\(\\hat{\\beta}_k\\)</span>, is statistically significant. The t-statistic for the kth regression coefficient under the null hypothesis that <span class=\"math\">\\(x_k\\)</span> and <span class=\"math\">\\(y\\)</span> are independent follows a t-distribution with n-K degrees of freedom, c.f. (\\ref{tstat}) with <span class=\"math\">\\(\\beta_k = 0\\)</span>:\n</p>\n<div class=\"math\">\\begin{align}\\tag{14}\nt = \\frac{\\hat{\\beta}_k - 0}{s(\\hat{\\beta}_k)} \\sim t_{n-K}.\n\\end{align}</div>\n<p>We reject the null-hypothesis if <span class=\"math\">\\(P[t] &lt; \\alpha\\)</span>.</p>\n<p>According to (\\ref{cov2}), <span class=\"math\">\\(s(\\hat{\\beta}_k)\\)</span> increases with multicollinearity. Hence, the estimator must be more &#8220;extreme&#8221; in order to be statistically significant in the presence of&nbsp;multicollinearity.</p>\n<h3>F-test for the significance of the&nbsp;regression</h3>\n<p>Whereas the t-test considers the significance of a single regressor, the F-test evaluates the significance of the entire regression, where the null hypothesis is that <em>all</em> the regressors except the constant are equal to zero: <span class=\"math\">\\(\\hat{\\beta}_1 = \\hat{\\beta}_2 = ... = \\hat{\\beta}_{K-1} = 0\\)</span>.</p>\n<p>The F-statistic under the null hypothesis follows an F-distribution with {K-1, N-K} degrees of freedom <a href=\"#ref1\">[1]</a>:\n</p>\n<div class=\"math\">\\begin{align}\\tag{15}\\label{F}\nF = \\frac{SSR/(K-1)}{SSE/(N-K)} \\sim F_{K-1, N-K}.\n\\end{align}</div>\n<p>It is useful to rewrite the F-statistic in terms of <span class=\"math\">\\(R^2\\)</span> by substituting the expressions from (\\ref{<span class=\"caps\">SS</span>}) and&nbsp;(\\ref{R2}):\n</p>\n<div class=\"math\">\\begin{align}\\tag{16}\\label{F2}\nF = \\frac{(N-K) R^2}{(K-1) (1-R^2)}\n\\end{align}</div>\n<p>Notice how, for fixed <span class=\"math\">\\(R^2\\)</span>, the F-statistic decreases with an increasing number of predictors <span class=\"math\">\\(K\\)</span>. Adding uninformative predictors to the model will decrease the significance of the regression, which motivates parsimony in constructing linear&nbsp;models.</p>\n<h2>Example</h2>\n<p>With these formulas in hand, let&#8217;s consider the problem of predicting the weight of adult women using some simulated data (loosely based on reality). We&#8217;ll look at two models:\n(1) <strong>weight ~ height</strong>.\nAs expected, height will be a strong predictor of weight, corroborated by a significant p-value for the coefficient of height in the model.\n(2) <strong>weight ~ height + shoe size</strong>.\nHeight and shoe size are strongly correlated in the simulated data, while height is still a strong predictor of weight. We&#8217;ll find that neither of the predictors has a significant individual p-value, a consequence of&nbsp;collinearity.</p>\n<p>First, import some libraries. We use <code>statsmodels.api.OLS</code> for the linear regression since it contains a much more detailed report on the results of the fit than <code>sklearn.linear_model.LinearRegression</code>.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">statsmodels.api</span> <span class=\"k\">as</span> <span class=\"nn\">sm</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.stats</span> <span class=\"kn\">import</span> <span class=\"n\">t</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n</pre></div>\n\n\n<p>Next, set the population parameters for the simulated&nbsp;data.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">#</span> <span class=\"n\">height</span> <span class=\"p\">(</span><span class=\"n\">inches</span><span class=\"p\">)</span>\n<span class=\"n\">mean_height</span> <span class=\"o\">=</span> <span class=\"mi\">65</span>\n<span class=\"n\">std_height</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">.</span><span class=\"mi\">25</span>\n\n<span class=\"o\">#</span> <span class=\"n\">shoe</span> <span class=\"k\">size</span> <span class=\"p\">(</span><span class=\"n\">inches</span><span class=\"p\">)</span>\n<span class=\"n\">mean_shoe_size</span> <span class=\"o\">=</span> <span class=\"mi\">7</span><span class=\"p\">.</span><span class=\"mi\">5</span>\n<span class=\"n\">std_shoe_size</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">25</span>\n\n<span class=\"o\">#</span> <span class=\"n\">correlation</span> <span class=\"k\">between</span> <span class=\"n\">height</span> <span class=\"k\">and</span> <span class=\"n\">shoe</span> <span class=\"k\">size</span>\n<span class=\"n\">r_height_shoe</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">98</span> <span class=\"o\">#</span> <span class=\"n\">height</span> <span class=\"k\">and</span> <span class=\"n\">shoe</span> <span class=\"k\">size</span> <span class=\"k\">are</span> <span class=\"n\">highly</span> <span class=\"n\">correlated</span>\n<span class=\"o\">#</span> <span class=\"n\">covariance</span> <span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">w</span> <span class=\"n\">height</span> <span class=\"k\">and</span> <span class=\"n\">shoe</span> <span class=\"k\">size</span>\n<span class=\"n\">var_height_shoe</span> <span class=\"o\">=</span> <span class=\"n\">r_height_shoe</span><span class=\"o\">*</span><span class=\"n\">std_height</span><span class=\"o\">*</span><span class=\"n\">std_shoe_size</span>\n\n<span class=\"o\">#</span> <span class=\"n\">matrix</span> <span class=\"k\">of</span> <span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"n\">mu</span><span class=\"p\">,</span> <span class=\"k\">and</span> <span class=\"n\">covariance</span><span class=\"p\">,</span> <span class=\"n\">cov</span>\n<span class=\"n\">mu</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mean_height</span><span class=\"p\">,</span> <span class=\"n\">mean_shoe_size</span><span class=\"p\">)</span>\n<span class=\"n\">cov</span> <span class=\"o\">=</span> <span class=\"p\">[[</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">std_height</span><span class=\"p\">),</span> <span class=\"n\">var_height_shoe</span><span class=\"p\">],</span>\n<span class=\"p\">[</span><span class=\"n\">var_height_shoe</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">std_shoe_size</span><span class=\"p\">)]]</span>\n</pre></div>\n\n\n<p>Generate the simulated&nbsp;data:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">#</span> <span class=\"nb\">number</span> <span class=\"k\">of</span> <span class=\"k\">data</span> <span class=\"n\">points</span>\n<span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n\n<span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">85</span><span class=\"p\">)</span>\n<span class=\"o\">#</span> <span class=\"n\">height</span> <span class=\"k\">and</span> <span class=\"n\">shoe</span> <span class=\"k\">size</span>\n<span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">multivariate_normal</span><span class=\"p\">(</span><span class=\"n\">mu</span><span class=\"p\">,</span> <span class=\"n\">cov</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"o\">#</span> <span class=\"n\">height</span><span class=\"p\">,</span> <span class=\"n\">alone</span>\n<span class=\"n\">X0</span> <span class=\"o\">=</span> <span class=\"n\">X1</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">220</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">X0</span><span class=\"o\">*</span><span class=\"mi\">5</span><span class=\"p\">.</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>Below is the simulated data plotted against each other.\n<a href=\"https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png\"><img alt=\"scatterplots\" src=\"https://efavdb.com/wp-content/uploads/2016/06/scatter_height_weight_shoesize_cropped.png\"></a></p>\n<p>Fit the linear&nbsp;models:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">#</span> <span class=\"k\">add</span> <span class=\"k\">column</span> <span class=\"k\">of</span> <span class=\"n\">ones</span> <span class=\"k\">for</span> <span class=\"n\">intercept</span>\n<span class=\"n\">X0</span> <span class=\"o\">=</span> <span class=\"n\">sm</span><span class=\"p\">.</span><span class=\"n\">add_constant</span><span class=\"p\">(</span><span class=\"n\">X0</span><span class=\"p\">)</span>\n<span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">sm</span><span class=\"p\">.</span><span class=\"n\">add_constant</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">)</span>\n\n<span class=\"o\">#</span> <span class=\"ss\">&quot;OLS&quot;</span> <span class=\"n\">stands</span> <span class=\"k\">for</span> <span class=\"n\">Ordinary</span> <span class=\"n\">Least</span> <span class=\"n\">Squares</span>\n<span class=\"n\">sm0</span> <span class=\"o\">=</span> <span class=\"n\">sm</span><span class=\"p\">.</span><span class=\"n\">OLS</span><span class=\"p\">(</span><span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">X0</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">()</span>\n<span class=\"n\">sm1</span> <span class=\"o\">=</span> <span class=\"n\">sm</span><span class=\"p\">.</span><span class=\"n\">OLS</span><span class=\"p\">(</span><span class=\"n\">weight</span><span class=\"p\">,</span> <span class=\"n\">X1</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">()</span>\n</pre></div>\n\n\n<p>Look at the summary report, <code>sm0.summary()</code>, for the weight ~ height&nbsp;model.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">OLS Regression Results</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"err\">Dep. Variable: y R-squared: 0.788</span>\n<span class=\"c\">Model: OLS Adj. R-squared: 0.776</span>\n<span class=\"c\">Method: Least Squares F-statistic: 66.87</span>\n<span class=\"c\">Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.79e-07</span>\n<span class=\"c\">Time: 14:28:08 Log-Likelihood: -70.020</span>\n<span class=\"err\">No. Observations: 20 AIC: 144.0</span>\n<span class=\"err\">Df Residuals: 18 BIC: 146.0</span>\n<span class=\"err\">Df Model: 1</span>\n<span class=\"err\">Covariance Type: nonrobust</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"err\">coef std err t P&gt;|t| [95.0% Conf. Int.]</span>\n<span class=\"err\">------------------------------------------------------------------------------</span>\n<span class=\"err\">const -265.2764 49.801 -5.327 0.000 -369.905 -160.648</span>\n<span class=\"err\">x1 6.1857 0.756 8.178 0.000 4.596 7.775</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"c\">Omnibus: 0.006 Durbin-Watson: 2.351</span>\n<span class=\"err\">Prob(Omnibus): 0.997 Jarque-Bera (JB): 0.126</span>\n<span class=\"c\">Skew: 0.002 Prob(JB): 0.939</span>\n<span class=\"c\">Kurtosis: 2.610 Cond. No. 1.73e+03</span>\n<span class=\"err\">==============================================================================</span>\n</pre></div>\n\n\n<p>The height variable, <code>x1</code>, is significant according to the t-test, as is the intercept, denoted <code>const</code> in the report. Also, notice the coefficient used to simulate the dependence of weight on height (<span class=\"math\">\\(\\beta_1\\)</span> = 5.5), is contained in the 95% confidence interval of <code>x1</code>.</p>\n<p>Next, let&#8217;s look at the summary report, <code>sm1.summary()</code>, for the weight ~ height + shoe_size&nbsp;model.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">OLS Regression Results</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"err\">Dep. Variable: y R-squared: 0.789</span>\n<span class=\"c\">Model: OLS Adj. R-squared: 0.765</span>\n<span class=\"c\">Method: Least Squares F-statistic: 31.86</span>\n<span class=\"c\">Date: Wed, 29 Jun 2016 Prob (F-statistic): 1.78e-06</span>\n<span class=\"c\">Time: 14:28:08 Log-Likelihood: -69.951</span>\n<span class=\"err\">No. Observations: 20 AIC: 145.9</span>\n<span class=\"err\">Df Residuals: 17 BIC: 148.9</span>\n<span class=\"err\">Df Model: 2</span>\n<span class=\"err\">Covariance Type: nonrobust</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"err\">coef std err t P&gt;|t| [95.0% Conf. Int.]</span>\n<span class=\"err\">------------------------------------------------------------------------------</span>\n<span class=\"err\">const -333.1599 204.601 -1.628 0.122 -764.829 98.510</span>\n<span class=\"err\">x1 7.4944 3.898 1.923 0.071 -0.729 15.718</span>\n<span class=\"err\">x2 -2.3090 6.739 -0.343 0.736 -16.527 11.909</span>\n<span class=\"err\">==============================================================================</span>\n<span class=\"c\">Omnibus: 0.015 Durbin-Watson: 2.342</span>\n<span class=\"err\">Prob(Omnibus): 0.993 Jarque-Bera (JB): 0.147</span>\n<span class=\"c\">Skew: 0.049 Prob(JB): 0.929</span>\n<span class=\"c\">Kurtosis: 2.592 Cond. No. 7.00e+03</span>\n<span class=\"err\">==============================================================================</span>\n</pre></div>\n\n\n<p>Neither of the regressors <code>x1</code> and <code>x2</code> is significant at a significance level of <span class=\"math\">\\(\\alpha=0.05\\)</span>. In the simulated data, adult female weight has a positive linear correlation with height and shoe size, but the strong collinearity of the predictors (simulated with a correlation coefficient of 0.98) causes each variable to fail a t-test in the model &#8212; and even results in the wrong sign for the dependence on shoe&nbsp;size.</p>\n<p>Although the predictors fail individual t-tests, the overall regression <em>is</em> significant, i.e. the predictors are jointly informative, according to the&nbsp;F-test.</p>\n<p>Notice, however, that the p-value of the F-test has decreased compared to the simple linear model, as expected from (\\ref{F2}), since including the extra variable, shoe size, did not improve <span class=\"math\">\\(R^2\\)</span> but did increase <span class=\"math\">\\(K\\)</span>.</p>\n<p>Let&#8217;s manually calculate the standard error, t-statistics, F-statistic, corresponding p-values, and confidence intervals using the equations from&nbsp;above.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">#</span> <span class=\"n\">OLS</span> <span class=\"n\">solution</span><span class=\"p\">,</span> <span class=\"n\">eqn</span> <span class=\"k\">of</span> <span class=\"n\">form</span> <span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">b</span> <span class=\"o\">=&gt;</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"s1\">&#39;X)*beta_hat = X&#39;</span><span class=\"o\">*</span><span class=\"n\">y</span>\n<span class=\"n\">beta_hat</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">solve</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">X1</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">weight</span><span class=\"p\">))</span>\n\n<span class=\"o\">#</span> <span class=\"n\">residuals</span>\n<span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"n\">weight</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">,</span> <span class=\"n\">beta_hat</span><span class=\"p\">)</span>\n\n<span class=\"o\">#</span> <span class=\"n\">degrees</span> <span class=\"k\">of</span> <span class=\"n\">freedom</span> <span class=\"k\">of</span> <span class=\"n\">residuals</span>\n<span class=\"n\">dof</span> <span class=\"o\">=</span> <span class=\"n\">X1</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">X1</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n\n<span class=\"o\">#</span> <span class=\"n\">best</span> <span class=\"n\">estimator</span> <span class=\"k\">of</span> <span class=\"n\">sigma</span>\n<span class=\"n\">sigma_hat</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">epsilon</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">dof</span><span class=\"p\">)</span>\n\n<span class=\"o\">#</span> <span class=\"n\">standard</span> <span class=\"n\">error</span> <span class=\"k\">of</span> <span class=\"n\">beta_hat</span>\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">sigma_hat</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">X1</span><span class=\"p\">)),</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n\n<span class=\"o\">#</span> <span class=\"mi\">95</span><span class=\"o\">%</span> <span class=\"n\">confidence</span> <span class=\"n\">intervals</span>\n<span class=\"o\">#</span> <span class=\"o\">+/-</span><span class=\"n\">t_</span><span class=\"err\">{</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">alpha</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">-</span><span class=\"n\">K</span><span class=\"err\">}</span> <span class=\"o\">=</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"nb\">interval</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">alpha</span><span class=\"p\">,</span> <span class=\"n\">dof</span><span class=\"p\">)</span>\n<span class=\"n\">conf_intervals</span> <span class=\"o\">=</span> <span class=\"n\">beta_hat</span><span class=\"p\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">s</span><span class=\"p\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nb\">array</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">.</span><span class=\"nb\">interval</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">95</span><span class=\"p\">,</span> <span class=\"n\">dof</span><span class=\"p\">))</span>\n\n<span class=\"o\">#</span> <span class=\"n\">t</span><span class=\"o\">-</span><span class=\"k\">statistics</span> <span class=\"k\">under</span> <span class=\"k\">null</span> <span class=\"n\">hypothesis</span>\n<span class=\"n\">t_stat</span> <span class=\"o\">=</span> <span class=\"n\">beta_hat</span> <span class=\"o\">/</span> <span class=\"n\">s</span>\n\n<span class=\"o\">#</span> <span class=\"n\">p</span><span class=\"o\">-</span><span class=\"k\">values</span>\n<span class=\"o\">#</span> <span class=\"n\">survival</span> <span class=\"k\">function</span> <span class=\"n\">sf</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">CDF</span>\n<span class=\"n\">p_values</span> <span class=\"o\">=</span> <span class=\"n\">t</span><span class=\"p\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"k\">abs</span><span class=\"p\">(</span><span class=\"n\">t_stat</span><span class=\"p\">),</span> <span class=\"n\">dof</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">2</span>\n\n<span class=\"o\">#</span> <span class=\"n\">SSR</span> <span class=\"p\">(</span><span class=\"n\">regression</span> <span class=\"k\">sum</span> <span class=\"k\">of</span> <span class=\"n\">squares</span><span class=\"p\">)</span>\n<span class=\"n\">y_hat</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">,</span> <span class=\"n\">beta_hat</span><span class=\"p\">)</span>\n<span class=\"n\">y_mu</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n<span class=\"n\">mean_SSR</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">((</span><span class=\"n\">y_hat</span> <span class=\"o\">-</span> <span class=\"n\">y_mu</span><span class=\"p\">).</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">y_hat</span> <span class=\"o\">-</span> <span class=\"n\">y_mu</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"o\">#</span> <span class=\"n\">f</span><span class=\"o\">-</span><span class=\"n\">statistic</span>\n<span class=\"n\">f_stat</span> <span class=\"o\">=</span> <span class=\"n\">mean_SSR</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">sigma_hat</span><span class=\"p\">)</span>\n<span class=\"n\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;f-statistic:&#39;</span><span class=\"p\">,</span> <span class=\"n\">f_stat</span><span class=\"p\">,</span> <span class=\"s1\">&#39;\\n&#39;</span><span class=\"p\">)</span>\n\n<span class=\"o\">#</span> <span class=\"n\">p</span><span class=\"o\">-</span><span class=\"n\">value</span> <span class=\"k\">of</span> <span class=\"n\">f</span><span class=\"o\">-</span><span class=\"n\">statistic</span>\n<span class=\"n\">p_values_f_stat</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">sf</span><span class=\"p\">(</span><span class=\"k\">abs</span><span class=\"p\">(</span><span class=\"n\">f_stat</span><span class=\"p\">),</span> <span class=\"n\">dfn</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">beta_hat</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dfd</span><span class=\"o\">=</span><span class=\"n\">dof</span><span class=\"p\">)</span>\n<span class=\"n\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;p-value of f-statistic:&#39;</span><span class=\"p\">,</span> <span class=\"n\">p_values_f_stat</span><span class=\"p\">,</span> <span class=\"s1\">&#39;\\n&#39;</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p>The output values, below, from printing the manual calculations are consistent with the summary&nbsp;report:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">beta_hat</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">333</span><span class=\"p\">.</span><span class=\"mi\">15990097</span> <span class=\"mi\">7</span><span class=\"p\">.</span><span class=\"mi\">49444671</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">.</span><span class=\"mi\">30898743</span><span class=\"p\">]</span>\n\n<span class=\"n\">degrees</span> <span class=\"k\">of</span> <span class=\"n\">freedom</span> <span class=\"k\">of</span> <span class=\"n\">residuals</span><span class=\"p\">:</span> <span class=\"mi\">17</span>\n\n<span class=\"n\">sigma_hat</span><span class=\"p\">:</span> <span class=\"mi\">8</span><span class=\"p\">.</span><span class=\"mi\">66991550428</span>\n\n<span class=\"n\">standard</span> <span class=\"n\">error</span> <span class=\"k\">of</span> <span class=\"n\">beta_hat</span><span class=\"p\">:</span> <span class=\"p\">[</span> <span class=\"mi\">204</span><span class=\"p\">.</span><span class=\"mi\">60056111</span> <span class=\"mi\">3</span><span class=\"p\">.</span><span class=\"mi\">89776076</span> <span class=\"mi\">6</span><span class=\"p\">.</span><span class=\"mi\">73900599</span><span class=\"p\">]</span>\n\n<span class=\"n\">confidence</span> <span class=\"n\">intervals</span><span class=\"p\">:</span>\n<span class=\"p\">[[</span> <span class=\"o\">-</span><span class=\"mi\">7</span><span class=\"p\">.</span><span class=\"mi\">64829352</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">02</span> <span class=\"mi\">9</span><span class=\"p\">.</span><span class=\"mi\">85095501</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">01</span><span class=\"p\">]</span>\n<span class=\"p\">[</span> <span class=\"o\">-</span><span class=\"mi\">7</span><span class=\"p\">.</span><span class=\"mi\">29109662</span><span class=\"n\">e</span><span class=\"o\">-</span><span class=\"mi\">01</span> <span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">57180031</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">01</span><span class=\"p\">]</span>\n<span class=\"p\">[</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">65270473</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">01</span> <span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">19090724</span><span class=\"n\">e</span><span class=\"o\">+</span><span class=\"mi\">01</span><span class=\"p\">]]</span>\n\n<span class=\"n\">t</span><span class=\"o\">-</span><span class=\"k\">statistics</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">62834305</span> <span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">92275698</span> <span class=\"o\">-</span><span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">34263027</span><span class=\"p\">]</span>\n\n<span class=\"n\">p</span><span class=\"o\">-</span><span class=\"k\">values</span> <span class=\"k\">of</span> <span class=\"n\">t</span><span class=\"o\">-</span><span class=\"k\">statistics</span><span class=\"p\">:</span> <span class=\"p\">[</span> <span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">1218417</span> <span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">07142839</span> <span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">73607656</span><span class=\"p\">]</span>\n<span class=\"n\">f</span><span class=\"o\">-</span><span class=\"n\">statistic</span><span class=\"p\">:</span> <span class=\"mi\">31</span><span class=\"p\">.</span><span class=\"mi\">8556171105</span>\n\n<span class=\"n\">p</span><span class=\"o\">-</span><span class=\"n\">value</span> <span class=\"k\">of</span> <span class=\"n\">f</span><span class=\"o\">-</span><span class=\"n\">statistic</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">.</span><span class=\"mi\">77777555162</span><span class=\"n\">e</span><span class=\"o\">-</span><span class=\"mi\">06</span>\n</pre></div>\n\n\n<p>The full code is available as an <a href=\"https://github.com/EFavDB/linear-regression\">IPython notebook on github</a>.</p>\n<h2>Summary</h2>\n<p>Assuming a multivariate normal distribution for the residuals in linear regression allows us to construct test statistics and therefore specify uncertainty in our&nbsp;fits.</p>\n<p>A t-test judges the explanatory power of a predictor in isolation, although the standard error that appears in the calculation of the t-statistic is a function of the other predictors in the model. On the other hand, an F-test is a global test that judges the explanatory power of all the predictors together, and we&#8217;ve seen that parsimony in choosing predictors can improve the quality of the overall&nbsp;regression.</p>\n<p>We&#8217;ve also seen that multicollinearity can throw off the results of individual t-tests as well as obscure the interpretation of the signs of the fitted coefficients. A symptom of multicollinearity is when none of the individual coefficients are significant but the overall F-test is&nbsp;significant.</p>\n<h3>Reference</h3>\n<p>[1] Greene, W., Econometric Analysis, Seventh edition, Prentice Hall, 2011 - <a href=\"http://people.stern.nyu.edu/wgreene/MathStat/Outline.htm\">chapters available&nbsp;online</a></p>\n<h3>Appendix</h3>\n<p>[A1]\nWe specifically want the kth diagonal element from the inverse moment matrix, <span class=\"math\">\\((X'X)^{-1}\\)</span>. The matrix <span class=\"math\">\\(X\\)</span> can be <a href=\"https://en.wikipedia.org/wiki/Block_matrix\">partitioned</a>&nbsp;as </p>\n<div class=\"math\">$$[X_{(k)} \\vec{x}_k],$$</div>\n<p> where <span class=\"math\">\\(\\vec{x}_k\\)</span> is an N x 1 column vector containing the kth variable of each of the N samples, and <span class=\"math\">\\(X_{(k)}\\)</span> is the N x (K-1) matrix containing the rest of the variables and constant intercept. For convenience, let <span class=\"math\">\\(X_{(k)}\\)</span> and <span class=\"math\">\\(\\vec{x}_k\\)</span> be centered about their (column-wise)&nbsp;means.</p>\n<p>Matrix multiplication of the block-partitioned form of <span class=\"math\">\\(X\\)</span> with its transpose results in the following block&nbsp;matrix:\n</p>\n<div class=\"math\">\\begin{align}\n(X'X) =\n\\begin{bmatrix}\nX_{(k)}'X_{(k)} &amp; X_{(k)}'\\vec{x}_k \\\n\\vec{x}_k'X_{(k)} &amp; \\vec{x}_k'\\vec{x}_k\n\\end{bmatrix}\n\\end{align}</div>\n<p>The above matrix has four blocks, and <a href=\"https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion\">can be inverted blockwise</a> to obtain another matrix with four blocks. The lower right block corresponding to the kth diagonal element of the inverted matrix is a&nbsp;scalar:\n</p>\n<div class=\"math\">\\begin{align}\n(X'X)^{-1}_{kk} &amp;= [\\vec{x}_k'\\vec{x}_k - \\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\\vec{x}_k]^{-1} \\\\\n&amp;= \\left[\\vec{x}_k'\\vec{x}_k \\left( 1 - \\frac{\\vec{x}_k'X_{(k)}(X_{(k)}'X_{(k)})^{-1}X_{(k)}'\\vec{x}_k}{\\vec{x}_k'\\vec{x}_k} \\right)\\right]^{-1}\n\\end{align}</div>\n<p>Then the numerator of the fraction in the parentheses above can be&nbsp;simplified:\n</p>\n<div class=\"math\">\\begin{align}\n\\vec{x}_k'X_{(k)} ((X_{(k)}'X_{(k)})^{-1}X_{(k)}'\\vec{x}_k) &amp;= \\vec{x}_k' X_{(k)} \\hat{\\beta}_{(k)} \\\\\n&amp;= (X_{(k)}\\hat{\\beta}_{(k)} + \\epsilon_k)'X_{(k)}\\hat{\\beta}_{(k)} \\\\\n&amp;= \\hat{x}_k'\\hat{x}_k,\n\\end{align}</div>\n<p>where <span class=\"math\">\\(\\hat{\\beta}_{(k)}\\)</span> is the <span class=\"caps\">OLS</span> solution for the coefficients in the regression on the <span class=\"math\">\\(\\vec{x}_k\\)</span> by the remaining variables <span class=\"math\">\\(X_{(k)}\\)</span>: <span class=\"math\">\\(\\vec{x}_k = X_{(k)} \\beta_{(k)} + \\epsilon_k\\)</span>. In the last line, we used one of the constraints on the residuals &#8212; that the residuals and predictors are uncorrelated, <span class=\"math\">\\(\\epsilon_k'X_{(k)} = 0\\)</span>. Plugging in this simplification for the numerator and using the definition of <span class=\"math\">\\(R^2\\)</span> from (\\ref{R2}), we obtain our final&nbsp;result:\n</p>\n<div class=\"math\">\\begin{align}\n(X'X)^{-1}_{kk} &amp;= \\left[\\vec{x}_k'\\vec{x}_k \\left( 1 - \\frac{\\hat{x}_k'\\hat{x}_k}{\\vec{x}_k'\\vec{x}_k} \\right)\\right]^{-1} \\\\\n&amp;= \\left[\\vec{x}_k'\\vec{x}_k ( 1 - R_k^2 )\\right]^{-1}\n\\end{align}</div>\n<p>[A2]\n</p>\n<div class=\"math\">\\begin{align}\n\\hat{\\beta} &amp;= (X'X)^{-1}X'y \\\\\n&amp;= (X'X)^{-1}X'(X\\beta + \\epsilon) \\\\\n&amp;= \\beta + (X'X)^{-1}X'N(0, \\sigma^2I) \\\\\n&amp; \\sim N(\\beta, \\sigma^2 (X'X)^{-1})\n\\end{align}</div>\n<p> The last line is by properties of <a href=\"https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Affine_transformation\">affine transformations on multivariate normal distributions</a>.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    ""
  ]
}