{
  "title": "Functorial Manifold Learning. (arXiv:2011.07435v6 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2011.07435",
  "description": "<p>We adapt previous research on category theory and topological unsupervised\nlearning to develop a functorial perspective on manifold learning, also known\nas nonlinear dimensionality reduction. We first characterize manifold learning\nalgorithms as functors that map pseudometric spaces to optimization objectives\nand that factor through hierarchical clustering functors. We then use this\ncharacterization to prove refinement bounds on manifold learning loss functions\nand construct a hierarchy of manifold learning algorithms based on their\nequivariants. We express several popular manifold learning algorithms as\nfunctors at different levels of this hierarchy, including Metric\nMultidimensional Scaling, IsoMap, and UMAP. Next, we use interleaving distance\nto study the stability of a broad class of manifold learning algorithms. We\npresent bounds on how closely the embeddings these algorithms produce from\nnoisy data approximate the embeddings they would learn from noiseless data.\nFinally, we use our framework to derive a set of novel manifold learning\nalgorithms, which we experimentally demonstrate are competitive with the state\nof the art.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1\">Dan Shiebler</a> (University of Oxford)"
}