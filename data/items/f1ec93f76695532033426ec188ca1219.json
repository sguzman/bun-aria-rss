{
  "id": "tag:blogger.com,1999:blog-5547907074344788039.post-66549460551960139",
  "published": "2012-12-14T17:42:00.000-08:00",
  "updated": "2014-11-30T19:04:03.459-08:00",
  "title": "On the unreasonable effectiveness of data: why are more data better?",
  "content": "Update 11/30/14: I incorrectly named the authors of the \"unreasonable effectiveness study, now corrected.<br /><br />In the paper \"The unreasonable effectiveness of data\" [<a href=\"http://www.csee.wvu.edu/~gidoretto/courses/2011-fall-cp/reading/TheUnreasonable%20EffectivenessofData_IEEE_IS2009.pdf\">Halevy&nbsp;<i>et al</i>, 2009</a>], Halevy, Norvig and Pererira, all from Google, argue that interesting things happen when corpora get to web scale:<br /><blockquote class=\"tr_bq\">\"simple models and a lot of data trump more elaborate models based on less data\".</blockquote><div class=\"p2\">In that paper and the more detailed&nbsp;<a href=\"http://www.youtube.com/watch?v=yvDCzhbjYWs\">tech talk</a> given by Norvig, they demonstrate that&nbsp;when corpora get to hundreds of millions or trillions of training sample or words, very simple models with basic independence assumptions can outperform more complex models such as those based on carefully-crafted ontologies with smaller data.&nbsp;However, they provided relatively little explanation as to&nbsp;<i>why</i> more data are better. In this post, I want to attempt to think through that.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">I propose that there are several classes of problems and reasons for why more data are better.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">The first are <b>nearest neighbor type problems</b>.&nbsp;Halevy <i>et al</i>. mention \"James Hay and Alexei A. Efros addressed the task of scene completion: removing an unwanted, unsightly automobile or ex-spouse from a photograph and filling in the background with pixels taken from a large corpus of other photos.\" [<a href=\"http://graphics.cs.cmu.edu/projects/scene-completion/scene-completion.pdf\">Hays and Efros, 2007</a>&nbsp;]</div><div class=\"p1\"><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-oPQQoFrGJIs/UMvRm2ZyXTI/AAAAAAAAADw/6uQbzjCHv3o/s1600/high_res_square_teaser.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://2.bp.blogspot.com/-oPQQoFrGJIs/UMvRm2ZyXTI/AAAAAAAAADw/6uQbzjCHv3o/s400/high_res_square_teaser.jpg\" height=\"301\" width=\"400\" /></a></div><br /></div><div class=\"p3\"><br /></div><div class=\"p3\"></div><div class=\"p1\">Norvig presented the following schematic:&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p2\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-SCY_DiksZP4/UMvRxUp5c3I/AAAAAAAAAD4/_yECFNlk9Mg/s1600/norvig1.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://3.bp.blogspot.com/-SCY_DiksZP4/UMvRxUp5c3I/AAAAAAAAAD4/_yECFNlk9Mg/s400/norvig1.jpg\" height=\"262\" width=\"400\" /></a></div><br /></div><div class=\"p1\">and described it as a \"data threshold\" in which results went from really bad to really good.</div><div class=\"p2\"><br /></div><div class=\"p1\">I'm not convinced that there is any threshold or anything that resembles a phase transition.&nbsp;This seems to me to be a problem of finding the closest match. The more data, the closer the expected match.</div><div class=\"p2\"><br /></div><div class=\"p1\">Hays and Efros (2007) mention:</div><blockquote class=\"tr_bq\">\"Indeed, our initial experiments with the&nbsp;gist descriptor on a dataset of ten thousand images were very discouraging. However, increasing the image collection to two million&nbsp;yielded a qualitative leap in performance... Independently, Torralba et al. [2007] have&nbsp;observed a similar effect with a dataset of up to 70 million tiny&nbsp;(32x32) images...It takes a large amount of data for our method to succeed. We&nbsp;saw dramatic improvement when moving from ten thousand to two&nbsp;million images\" [Hays and Efros, 2007]</blockquote><div class=\"p2\">There is a large difference in those corpus sizes and a \"qualitative leap\" is not the same as a threshold (<i>sensu</i> phase transition).</div><div class=\"p2\"><br /></div><div class=\"p1\">More data can dramatically affect the metrics from simple effects. For instance, consider a sample of size <i>n</i> from a standard normal. How does the minimum of that sample vary with <i>n</i>?&nbsp;Let's create samples of different sizes and plot the minimum using the following R code</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">x&lt;-seq(1,7,0.5)</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">y&lt;-vector(mode=\"numeric\",length=length(x))</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">for (i in 1:length(x)){ y[i] &lt;- min(rnorm(10^(x[i]))) }</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">plot(x,y,xlab=\"Sample size, n (log10 scale)\",ylab=\"Minimum value of sample\",type=\"b\")</span></div><div class=\"p2\"><br /></div><div class=\"p2\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-uitu-xplmaE/UMvSBzWy4gI/AAAAAAAAAEA/VT_pb56jj-E/s1600/gaussian_min.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://2.bp.blogspot.com/-uitu-xplmaE/UMvSBzWy4gI/AAAAAAAAAEA/VT_pb56jj-E/s400/gaussian_min.jpg\" height=\"400\" width=\"400\" /></a></div><br /></div><div class=\"p1\">The minimum decreases loglinearly. This is a case of extrema from an unbounded tail.&nbsp;Perhaps more relevantly, here, for a minimization problem such as scene matching there is a lower bound: for all intents and purposes, a perfect match. For instance, perhaps someone else stood in the same exact tourist spot and took a picture of the exact same scene but without the obstructing car.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">I think this is what is happening in Norvig's schematic. At a certain corpus size we've found a really good match and a larger corpus doesn't and cannot improve results.</div><div class=\"p2\"><i></i><br /></div><div class=\"p1\"><i>In summary, for nearest neighbor type minimization problems with a non-negative distance function (meaning that the cost function has a lower bound of zero), that distance function will, on average, decrease monotonically with data or sample size.</i>&nbsp;</div><div class=\"p1\"><br /></div><div class=\"p1\">The second class are <b>counting</b> or <b>relative frequency&nbsp;</b>problems. These were the primary focus of Halevy<i> et al.</i> Norvig's presented a few examples. In segmentation, the task is to split a string such as \"cheapdealsandstuff.com\" into the most likely sequence of words. These strings are short enough to do a brute force in terms of possible partition, but for each partition we have to assess the likelihood of that partition. The simplest assumption is to assume independence among words. Thus, if <i>Pr(w)</i> is the probability of a word w given some corpus we can compute, say</div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">Pr(che,apdeals,andstuff) = Pr(che) * Pr(apdeals) * Pr(andstuff).</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">...</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">Pr(cheap,deals,and,stuff) = Pr(cheap) * Pr(deals) * Pr(and) * Pr(stuff).</span></div><div class=\"p2\"><br /></div><div class=\"p1\">One can of course also use n-grams, e.g. for bigrams: Pr(\"cheap deals\") * Pr(\"and stuff\")</div><div class=\"p2\"><br /></div><div class=\"p1\">A second example that Norvig covered was spell checking. Here we can take a misspelled word and compute the likelihood of the possible variants to suggest the most likely form.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">In both cases, we need a corpus that includes both common and uncommon phrases and we need counts of the occurrences of those phrases to compute relative frequency. The larger and more comprehensive the corpus, the better. I think that there are two statistical effects going on here:</div><ul class=\"ul1\"><li class=\"li1\">the larger the corpus, the better the quality of the estimate of the relative frequency. This is the <a href=\"http://en.wikipedia.org/wiki/Law_of_large_numbers\">law of large numbers</a>.</li><li class=\"li3\"><span class=\"s2\">the larger the corpus, the more likely it is to include unusual phrases, i.e. the long tail. T</span>his is an unbounded effect. The more of the web is indexed, there will always be new, never-seen-before phrases. The problem is exacerbated in that the distribution of words in the english language is a power law. (e.g.&nbsp;Zipf, G. The Psycho-Biology of Language. Houghton Mifflin, Boston, MA, 1935.). This means that the tail is especially long and thus especially large samples are needed to capture those rare phrases.</li></ul><div class=\"p1\">A third class are <b>estimating univariate distribution problems</b>. I recently encountered an interesting example while attending a <a href=\"http://www.slideshare.net/pskomoroch/developing-data-products\">tech talk</a> from <a href=\"http://www.datawrangling.com/\">Peter Skomoroch</a> from LinkedIn. He showed a plot of likelihood of a member having particular software-related job titles versus number of months since graduation. What we see from the data is that the distributions of \"Sr Software engineer\" and \"senior software engineer\" are almost identical, as one would expect as they are synonyms, as are \"CTO\" and \"chief technology officer\". This then presents an interesting way of identifying synonyms and so deduping the data rather than maintaing a huge master list of acronyms and abbreviations. This is only possible because of the scale of data that they have where the distribution they can generate is reliable and presumably close to the true underlying population distribution.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-6D6SknXZHBI/UMvP37emVCI/AAAAAAAAADo/duxe2D9dcWM/s1600/Slide1.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://2.bp.blogspot.com/-6D6SknXZHBI/UMvP37emVCI/AAAAAAAAADo/duxe2D9dcWM/s400/Slide1.jpg\" height=\"300\" width=\"400\" /></a></div><br />A fourth class are general <b>multivariate</b> or <b>correlational</b> problems in which we are trying to estimate the relationship among variables. This could be estimating the relationship <i>y = f(x)</i> or perhaps estimating the joint pdf of many variables. We might use this for word sense disambiguation (e.g. is the document referring to pike the fish or pike the pointed weapon?) or to build up a dossier of associated features or concepts about an entity (e.g. a company has an associated CEO, head office, tax ID and so on). Here we are interested in the correlations among words or phrases. The problem is that&nbsp;web documents are very high dimensional and embarking on high dimensional problems like these we are under the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>, that data become very sparse. Thus, one effect of larger samples is to increase the density of data across state space. Again, with larger samples we can estimate metrics such as location metrics (mean, median and other metrics of the center of distributions) more accurately. We can also estimate joint pdfs more accurately. Below is a simple example from the following code:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">par(mfrow=c(1,2))</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">plot(mvrnorm(100, mu = c(0, 0), Sigma = matrix(c(1, .9, .9, 1), 2)),xlab=\"X\",ylab=\"Y\",ylim=c(-4,4))</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">title(\"n = 100\")</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">plot(mvrnorm(10000, mu = c(0, 0), Sigma = matrix(c(1, .9, .9, 1), 2)),xlab=\"X\",ylab=\"Y\",ylim=c(-4,4))</span></div><div class=\"p1\"><span style=\"color: #9fc5e8; font-family: Courier New, Courier, monospace;\">title(\"n = 10000\")</span></div><div class=\"p2\"><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-DLjzLdNNoRc/UMvSYWSdafI/AAAAAAAAAEI/wFzmmmgaU6c/s1600/bivar.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://1.bp.blogspot.com/-DLjzLdNNoRc/UMvSYWSdafI/AAAAAAAAAEI/wFzmmmgaU6c/s400/bivar.jpg\" height=\"400\" width=\"400\" /></a></div><br /><br /></div><div class=\"p1\">At left is a small sample. It could easily be interpreted as linear. At right, with a larger sample, the true underlying bivariate normal is more obvious. Clearly,&nbsp;this is a trivial example. The point is that for higher dimensions you will much larger sample sizes to estimate the joint pdfs well.</div><div class=\"p1\"><br /></div><div class=\"p1\">This is clearly a cursory answer as to why more data are better. Quality data are still preferred. However, for many organizations, such as Google, Twitter, LinkedIn and Facebook where content is user-generated, is often free-form text and/or covers many domains (and so deep-cleaning and the use of ontologies is infeasible) then what we see that having very large datasets can compensate for the noise. It all evens out in the end and in the case of nearest neighbor problems, the solution will always be better.</div><div class=\"p1\"><br /></div><br />",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Carl Anderson",
    "uri": "http://www.blogger.com/profile/11930448254473684406",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 1
}