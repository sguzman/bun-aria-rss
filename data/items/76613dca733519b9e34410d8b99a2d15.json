{
  "title": "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation. (arXiv:2211.01587v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01587",
  "description": "<p>Recent advances in large-scale pre-training provide large models with the\npotential to learn knowledge from the raw text. It is thus natural to ask\nwhether it is possible to leverage these large models as knowledge bases for\ndownstream tasks. In this work, we answer the aforementioned question in\nunsupervised knowledge-grounded conversation. We explore various methods that\nbest elicit knowledge from large models. Our human study indicates that, though\nhallucinations exist, large models post the unique advantage of being able to\noutput common sense and summarize facts that cannot be directly retrieved from\nthe search engine. To better exploit such generated knowledge in dialogue\ngeneration, we treat the generated knowledge as a noisy knowledge source and\npropose the posterior-based reweighing as well as the noisy training strategy.\nEmpirical results on two benchmarks show advantages over the state-of-the-art\nmethods.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"
}