{
  "title": "Efficient Tabular Storage",
  "link": "",
  "updated": "2015-08-28T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/08/28/Storage",
  "content": "<p><strong>tl;dr: We discuss efficient techniques for on-disk storage of tabular\ndata, notably the following:</strong></p>\n\n<ul>\n  <li>Binary stores</li>\n  <li>Column stores</li>\n  <li>Categorical support</li>\n  <li>Compression</li>\n  <li>Indexed/Partitioned stores</li>\n</ul>\n\n<p><strong>We use NYCTaxi dataset for examples, and introduce a small project,\n<a href=\"https://github.com/blaze/castra\">Castra</a>.</strong></p>\n\n<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<h2 id=\"larger-than-memory-data-and-disk-io\">Larger than Memory Data and Disk I/O</h2>\n\n<p>We analyze large datasets (10-100GB) on our laptop by extending memory with\ndisk.  Tools like <a href=\"http://dask.pydata.org/en/latest/array.html\">dask.array</a> and\n<a href=\"http://dask.pydata.org/en/latest/dataframe.html\">dask.dataframe</a> make this\neasier for array and tabular data.</p>\n\n<p>Interaction times can improve significantly (from minutes to seconds) if we\nchoose to store our data on disk efficiently.  This is particularly important\nfor large data because we can no longer separately “load in our data” while we\nget a coffee and then iterate rapidly on our dataset once it’s comfortably\nin memory.</p>\n\n<p><em>Larger-than-memory datasets force interactive workflows to include the hard\ndrive.</em></p>\n\n<h2 id=\"csv-is-convenient-but-slow\">CSV is convenient but slow</h2>\n\n<p>CSV is great.  It’s human readable, accessible by every tool (even Excel!), and\npretty simple.</p>\n\n<p>CSV is also slow.  The <code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code> parser maxes out at 100MB/s\non simple data.  This doesn’t include any keyword arguments like datetime\nparsing that might slow it down further.  Consider the time to parse a 24GB\ndataset:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>24GB / (100MB/s) == 4 minutes\n</code></pre></div></div>\n\n<p>A four minute delay is too long for interactivity.  We need to operate in\nseconds rather than minutes otherwise people leave to work on something else.\nThis improvement from a few minutes to a few seconds is entirely possible if we\nchoose better formats.</p>\n\n<h2 id=\"example-with-csvs\">Example with CSVs</h2>\n\n<p>As an example lets play with the NYC Taxi dataset using\n<a href=\"http://http://dask.pydata.org/en/latest/dataframe.html\">dask.dataframe</a>, a\nlibrary that copies the Pandas API but operates in chunks off of disk.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'csv/trip_data_*.csv'</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                  <span class=\"n\">skipinitialspace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                  <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'pickup_datetime'</span><span class=\"p\">,</span> <span class=\"s\">'dropoff_datetime'</span><span class=\"p\">])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span></code></pre>\n</figure>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>medallion</th>\n      <th>hack_license</th>\n      <th>vendor_id</th>\n      <th>rate_code</th>\n      <th>store_and_fwd_flag</th>\n      <th>pickup_datetime</th>\n      <th>dropoff_datetime</th>\n      <th>passenger_count</th>\n      <th>trip_time_in_secs</th>\n      <th>trip_distance</th>\n      <th>pickup_longitude</th>\n      <th>pickup_latitude</th>\n      <th>dropoff_longitude</th>\n      <th>dropoff_latitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n      <td>BA96DE419E711691B9445D6A6307C170</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n      <td>2013-01-01 15:11:48</td>\n      <td>2013-01-01 15:18:10</td>\n      <td>4</td>\n      <td>382</td>\n      <td>1.0</td>\n      <td>-73.978165</td>\n      <td>40.757977</td>\n      <td>-73.989838</td>\n      <td>40.751171</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n      <td>2013-01-06 00:18:35</td>\n      <td>2013-01-06 00:22:54</td>\n      <td>1</td>\n      <td>259</td>\n      <td>1.5</td>\n      <td>-74.006683</td>\n      <td>40.731781</td>\n      <td>-73.994499</td>\n      <td>40.750660</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n      <td>2013-01-05 18:49:41</td>\n      <td>2013-01-05 18:54:23</td>\n      <td>1</td>\n      <td>282</td>\n      <td>1.1</td>\n      <td>-74.004707</td>\n      <td>40.737770</td>\n      <td>-74.009834</td>\n      <td>40.726002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n      <td>51EE87E3205C985EF8431D850C786310</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n      <td>2013-01-07 23:54:15</td>\n      <td>2013-01-07 23:58:20</td>\n      <td>2</td>\n      <td>244</td>\n      <td>0.7</td>\n      <td>-73.974602</td>\n      <td>40.759945</td>\n      <td>-73.984734</td>\n      <td>40.759388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n      <td>51EE87E3205C985EF8431D850C786310</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n      <td>2013-01-07 23:25:03</td>\n      <td>2013-01-07 23:34:24</td>\n      <td>1</td>\n      <td>560</td>\n      <td>2.1</td>\n      <td>-73.976250</td>\n      <td>40.748528</td>\n      <td>-74.002586</td>\n      <td>40.747868</td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"time-costs\">Time Costs</h3>\n\n<p>It takes a second to load the first few lines but 11 to 12 minutes to roll\nthrough the entire dataset.  We make a zoomable picture below of a random\nsample of the taxi pickup locations in New York City.  This example is taken\nfrom a <a href=\"https://github.com/blaze/dask-examples/blob/master/nyctaxi-storage.ipynb\">full example notebook here</a>.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">pickup_latitude</span> <span class=\"o\">&gt;</span> <span class=\"mi\">40</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span>\n         <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">pickup_latitude</span> <span class=\"o\">&lt;</span> <span class=\"mi\">42</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span>\n         <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">pickup_longitude</span> <span class=\"o\">&gt;</span> <span class=\"o\">-</span><span class=\"mi\">75</span><span class=\"p\">)</span> <span class=\"o\">&amp;</span>\n         <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">pickup_longitude</span> <span class=\"o\">&lt;</span> <span class=\"o\">-</span><span class=\"mi\">72</span><span class=\"p\">)]</span>\n\n<span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">frac</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">)</span>\n<span class=\"n\">pickup</span> <span class=\"o\">=</span> <span class=\"n\">sample</span><span class=\"p\">[[</span><span class=\"s\">'pickup_latitude'</span><span class=\"p\">,</span> <span class=\"s\">'pickup_longitude'</span><span class=\"p\">]]</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">pickup</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">bokeh.plotting</span> <span class=\"kn\">import</span> <span class=\"n\">figure</span><span class=\"p\">,</span> <span class=\"n\">show</span><span class=\"p\">,</span> <span class=\"n\">output_notebook</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"o\">=</span><span class=\"s\">\"Pickup Locations\"</span><span class=\"p\">)</span>\n<span class=\"n\">p</span><span class=\"p\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">pickup_longitude</span><span class=\"p\">,</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">pickup_latitude</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/e269d02ed15947ba58c5/raw/2c8620c11af14fcdd6e0948de1d6b398cf56001a/nyc-trip-pickup.html\" width=\"640\" height=\"450\"></iframe>\n\n<h3 id=\"eleven-minutes-is-a-long-time\">Eleven minutes is a long time</h3>\n\n<p>This result takes eleven minutes to compute, almost all of which is parsing CSV\nfiles.  While this may be acceptable for a single computation we invariably\nmake mistakes and start over or find new avenues in our data to explore.  Each\nstep in our thought process now takes eleven minutes, <em>ouch</em>.</p>\n\n<p>Interactive exploration of larger-than-memory datasets requires us to evolve\nbeyond CSV files.</p>\n\n<h2 id=\"principles-to-store-tabular-data\">Principles to store tabular data</h2>\n\n<p><em>What efficient techniques exist for tabular data?</em></p>\n\n<p>A good solution may have the following attributes:</p>\n\n<ol>\n  <li>Binary</li>\n  <li>Columnar</li>\n  <li>Categorical support</li>\n  <li>Compressed</li>\n  <li>Indexed/Partitioned</li>\n</ol>\n\n<p>We discuss each of these below.</p>\n\n<h3 id=\"binary\">Binary</h3>\n\n<p>Consider the text ‘1.23’ as it is stored in a CSV file and how it is stored as\na Python/C float in memory:</p>\n\n<ul>\n  <li>CSV:  <code class=\"language-plaintext highlighter-rouge\">1.23</code></li>\n  <li>C/Python float: <code class=\"language-plaintext highlighter-rouge\">0x3f9d70a4</code></li>\n</ul>\n\n<p>These look <em>very different</em>.  When we load <code class=\"language-plaintext highlighter-rouge\">1.23</code> from a CSV textfile\nwe need to translate it to <code class=\"language-plaintext highlighter-rouge\">0x3f9d70a4</code>; this takes time.</p>\n\n<p>A binary format stores our data on disk exactly how it will look in memory; we\nstore the bytes <code class=\"language-plaintext highlighter-rouge\">0x3f9d70a4</code> directly on disk so that when we load data\nfrom disk to memory no extra translation is necessary.  Our file is no longer\nhuman readable but it’s much faster.</p>\n\n<p>This gets more intense when we consider datetimes:</p>\n\n<ul>\n  <li>CSV: 2015-08-25 12:13:14</li>\n  <li>NumPy datetime representation: 1440529994000000  (as an integer)</li>\n</ul>\n\n<p>Every time we parse a datetime we need to compute how many microseconds it has\nbeen since the epoch.  This calculation needs to take into account things like\nhow many days in each month, and all of the intervening leap years.  This is\nslow.  A binary representation would record the integer directly on disk (as\n<code class=\"language-plaintext highlighter-rouge\">0x51e278694a680</code>) so that we can load our datetimes directly into memory\nwithout calculation.</p>\n\n<h3 id=\"columnar\">Columnar</h3>\n\n<p>Many analytic computations only require a few columns at a time, often only\none, e.g.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">passenger_counts</span><span class=\"p\">.</span><span class=\"n\">value_counts</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">().</span><span class=\"n\">sort_index</span><span class=\"p\">()</span>\n<span class=\"mi\">0</span>           <span class=\"mi\">3755</span>\n<span class=\"mi\">1</span>      <span class=\"mi\">119605039</span>\n<span class=\"mi\">2</span>       <span class=\"mi\">23097153</span>\n<span class=\"mi\">3</span>        <span class=\"mi\">7187354</span>\n<span class=\"mi\">4</span>        <span class=\"mi\">3519779</span>\n<span class=\"mi\">5</span>        <span class=\"mi\">9852539</span>\n<span class=\"mi\">6</span>        <span class=\"mi\">6628287</span>\n<span class=\"mi\">7</span>             <span class=\"mi\">30</span>\n<span class=\"mi\">8</span>             <span class=\"mi\">23</span>\n<span class=\"mi\">9</span>             <span class=\"mi\">24</span>\n<span class=\"mi\">129</span>            <span class=\"mi\">1</span>\n<span class=\"mi\">255</span>            <span class=\"mi\">1</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">passenger_count</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">int64</span></code></pre>\n</figure>\n\n<p>Of our 24 GB we may only need 2GB.  <em>Columnar</em> storage means storing each\ncolumn separately from the others so that we can read relevant columns\nwithout passing through irrelevant columns.</p>\n\n<p>Our CSV example fails at this.  While we only want two columns,\n<code class=\"language-plaintext highlighter-rouge\">pickup_datetime</code> and <code class=\"language-plaintext highlighter-rouge\">pickup_longitude</code>, we pass through all of our data to\ncollect the relevant fields.  The pickup location data is mixed with all the\nrest.</p>\n\n<h3 id=\"categoricals\">Categoricals</h3>\n\n<p><em>Categoricals encode repetitive text columns (normally very expensive) as\nintegers (very very cheap) in a way that is invisible to the user.</em></p>\n\n<p>Consider the following (mostly text) columns of our NYC taxi dataset:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">[[</span><span class=\"s\">'medallion'</span><span class=\"p\">,</span> <span class=\"s\">'vendor_id'</span><span class=\"p\">,</span> <span class=\"s\">'rate_code'</span><span class=\"p\">,</span> <span class=\"s\">'store_and_fwd_flag'</span><span class=\"p\">]].</span><span class=\"n\">head</span><span class=\"p\">()</span></code></pre>\n</figure>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>medallion</th>\n      <th>vendor_id</th>\n      <th>rate_code</th>\n      <th>store_and_fwd_flag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n      <td>CMT</td>\n      <td>1</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Each of these columns represents elements of a small set:</p>\n\n<ul>\n  <li>There are <strong>two</strong> vendor ids</li>\n  <li>There are <strong>twenty one</strong> rate codes</li>\n  <li>There are <strong>three</strong> store-and-forward flags (Y, N, missing)</li>\n  <li>There are about <strong>13000</strong> taxi medallions. (still a small number)</li>\n</ul>\n\n<p>And yet we store these elements in large and cumbersome dtypes:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"n\">df</span><span class=\"p\">[[</span><span class=\"s\">'medallion'</span><span class=\"p\">,</span> <span class=\"s\">'vendor_id'</span><span class=\"p\">,</span> <span class=\"s\">'rate_code'</span><span class=\"p\">,</span> <span class=\"s\">'store_and_fwd_flag'</span><span class=\"p\">]].</span><span class=\"n\">dtypes</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span>\n<span class=\"n\">medallion</span>             <span class=\"nb\">object</span>\n<span class=\"n\">vendor_id</span>             <span class=\"nb\">object</span>\n<span class=\"n\">rate_code</span>              <span class=\"n\">int64</span>\n<span class=\"n\">store_and_fwd_flag</span>    <span class=\"nb\">object</span>\n<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"nb\">object</span></code></pre>\n</figure>\n\n<p>We use <code class=\"language-plaintext highlighter-rouge\">int64</code> for rate code, which could easily have fit into an <code class=\"language-plaintext highlighter-rouge\">int8</code> an\nopportunity for an 8x improvement in memory use.  The object dtype used for\nstrings in Pandas and Python takes up a lot of memory and is quite slow:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">getsizeof</span><span class=\"p\">(</span><span class=\"s\">'CMT'</span><span class=\"p\">)</span>  <span class=\"c1\"># bytes\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"mi\">40</span></code></pre>\n</figure>\n\n<p>Categoricals replace the original column with a column of integers (of the\nappropriate size, often <code class=\"language-plaintext highlighter-rouge\">int8</code>) along with a small index mapping those integers to the\noriginal values.  I’ve <a href=\"http://matthewrocklin.com/blog/work/2015/06/18/Categoricals/\">written about categoricals\nbefore</a> so I\nwon’t go into too much depth here.  Categoricals increase both storage and\ncomputational efficiency by about 10x if you have text data that describes\nelements in a category.</p>\n\n<h3 id=\"compression\">Compression</h3>\n\n<p>After we’ve encoded everything well and separated our columns we find ourselves\nlimited by disk I/O read speeds.  Disk read bandwidths range from 100MB/s\n(laptop spinning disk hard drive) to 2GB/s (RAID of SSDs).  This read speed\nstrongly depends on how large our reads are.  The bandwidths given above\nreflect large sequential reads such as you might find when reading all of a\n100MB file in one go. Performance degrades for smaller reads.  Fortunately, for\nanalytic queries we’re often in the large sequential read case (hooray!)</p>\n\n<p>We reduce disk read times through compression.  Consider the datetimes of the\nNYC taxi dataset.  These values are repetitive and slowly changing; a perfect match for modern compression techniques.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ind</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">index</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span>  <span class=\"c1\"># this is on presorted index data (see castra section below)\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">ind</span>\n<span class=\"n\">DatetimeIndex</span><span class=\"p\">([</span><span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span> <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span> <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span> <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span> <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span> <span class=\"s\">'2013-01-01 00:00:00'</span><span class=\"p\">,</span>\n               <span class=\"p\">...</span>\n               <span class=\"s\">'2013-12-31 23:59:42'</span><span class=\"p\">,</span> <span class=\"s\">'2013-12-31 23:59:47'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-12-31 23:59:48'</span><span class=\"p\">,</span> <span class=\"s\">'2013-12-31 23:59:49'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-12-31 23:59:50'</span><span class=\"p\">,</span> <span class=\"s\">'2013-12-31 23:59:51'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-12-31 23:59:54'</span><span class=\"p\">,</span> <span class=\"s\">'2013-12-31 23:59:55'</span><span class=\"p\">,</span>\n               <span class=\"s\">'2013-12-31 23:59:57'</span><span class=\"p\">,</span> <span class=\"s\">'2013-12-31 23:59:57'</span><span class=\"p\">],</span>\n               <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'datetime64[ns]'</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sa\">u</span><span class=\"s\">'pickup_datetime'</span><span class=\"p\">,</span> <span class=\"n\">length</span><span class=\"o\">=</span><span class=\"mi\">169893985</span><span class=\"p\">,</span> <span class=\"n\">freq</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">tz</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<h4 id=\"benchmark-datetime-compression\">Benchmark datetime compression</h4>\n\n<p>We can use a modern compression library, like <code class=\"language-plaintext highlighter-rouge\">fastlz</code> or <a href=\"http://blosc.org/\">blosc</a> to compress this data at high speeds.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">36</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">blosc</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">37</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">compressed</span> <span class=\"o\">=</span> <span class=\"n\">blosc</span><span class=\"p\">.</span><span class=\"n\">compress_ptr</span><span class=\"p\">(</span><span class=\"n\">address</span><span class=\"o\">=</span><span class=\"n\">ind</span><span class=\"p\">.</span><span class=\"n\">values</span><span class=\"p\">.</span><span class=\"n\">ctypes</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">,</span>\n    <span class=\"p\">...:</span>                                       <span class=\"n\">items</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">ind</span><span class=\"p\">),</span>\n    <span class=\"p\">...:</span>                                       <span class=\"n\">typesize</span><span class=\"o\">=</span><span class=\"n\">ind</span><span class=\"p\">.</span><span class=\"n\">values</span><span class=\"p\">.</span><span class=\"n\">dtype</span><span class=\"p\">.</span><span class=\"n\">alignment</span><span class=\"p\">,</span>\n    <span class=\"p\">...:</span>                                       <span class=\"n\">clevel</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">3.22</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">332</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">3.55</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">512</span> <span class=\"n\">ms</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">40</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">ind</span><span class=\"p\">.</span><span class=\"n\">nbytes</span>  <span class=\"c1\"># compression ratio\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">40</span><span class=\"p\">]:</span> <span class=\"mf\">0.14296813539337488</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">41</span><span class=\"p\">]:</span> <span class=\"n\">ind</span><span class=\"p\">.</span><span class=\"n\">nbytes</span> <span class=\"o\">/</span> <span class=\"mf\">0.512</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>      <span class=\"c1\"># Compresson bandwidth (GB/s)\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">41</span><span class=\"p\">]:</span> <span class=\"mf\">2.654593515625</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">42</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">blosc</span><span class=\"p\">.</span><span class=\"n\">decompress</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">1.3</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">438</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">1.74</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">406</span> <span class=\"n\">ms</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">43</span><span class=\"p\">]:</span> <span class=\"n\">ind</span><span class=\"p\">.</span><span class=\"n\">nbytes</span> <span class=\"o\">/</span> <span class=\"mf\">0.406</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>      <span class=\"c1\"># Decompression bandwidth (GB/s)\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">43</span><span class=\"p\">]:</span> <span class=\"mf\">3.3476647290640393</span></code></pre>\n</figure>\n\n<p>We store 7x fewer bytes on disk (thus septupling our effective disk I/O) by\nadding an extra 3GB/s delay.  If we’re on a really nice Macbook pro hard\ndrive (~600MB/s) then this is a clear and substantial win.  The worse the hard\ndrive, the better this is.</p>\n\n\\[\\textrm{Effective bandwidth} = \\left(\\frac{1}{600 \\textrm{MB/s} \\times 7} + \\frac{1}{3000 \\textrm{MB/s}}\\right)^{-1} = 1750 \\textrm{MB/s}\\]\n\n<h4 id=\"but-sometimes-compression-isnt-as-nice\">But sometimes compression isn’t as nice</h4>\n\n<p>Some data is more or less compressable than others.  The following column of\nfloating point data does not compress as nicely.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">44</span><span class=\"p\">]:</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">pickup_latitude</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">().</span><span class=\"n\">values</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">45</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">compressed</span> <span class=\"o\">=</span> <span class=\"n\">blosc</span><span class=\"p\">.</span><span class=\"n\">compress_ptr</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">ctypes</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">dtype</span><span class=\"p\">.</span><span class=\"n\">alignment</span><span class=\"p\">,</span> <span class=\"n\">clevel</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">5.87</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">0</span> <span class=\"n\">ns</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">5.87</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">925</span> <span class=\"n\">ms</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">46</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">nbytes</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">46</span><span class=\"p\">]:</span> <span class=\"mf\">0.7518617315969132</span></code></pre>\n</figure>\n\n<p>This compresses more slowly and only provides marginal benefit.  Compression\nmay still be worth it on slow disk but this isn’t a huge win.</p>\n\n<p>The pickup_latitude column isn’t compressible because most of the information\nisn’t repetitive.  The numbers to the far right of the decimal point are more\nor less random.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>40.747868\n</code></pre></div></div>\n\n<p>Other floating point columns may compress well, particularly when they are\nrounded to small and meaningful decimal values.</p>\n\n<h4 id=\"compression-rules-of-thumb\">Compression rules of thumb</h4>\n\n<p>Optimal compression requires thought.  General rules of thumb include the\nfollowing:</p>\n\n<ul>\n  <li>Compress integer dtypes</li>\n  <li>Compress datetimes</li>\n  <li>If your data is slowly varying (e.g. sorted time series) then use a shuffle filter (default in blosc)</li>\n  <li>Don’t bother much with floating point dtypes</li>\n  <li>Compress categoricals (which are just integer dtypes)</li>\n</ul>\n\n<h4 id=\"avoid-gzip-and-bz2\">Avoid gzip and bz2</h4>\n\n<p>Finally, avoid gzip and bz2.  These are both very common and <em>very</em> slow.  If\ndealing with text data, consider <a href=\"https://github.com/google/snappy\">snappy</a>\n(also available via blosc.)</p>\n\n<h3 id=\"indexingpartitioning\">Indexing/Partitioning</h3>\n\n<p>One column usually dominates our queries.  In time-series data this is time.\nFor personal data this is the user ID.</p>\n\n<p>Just as column stores let us avoid irrelevant columns, partitioning our data\nalong a preferred index column lets us avoid irrelevant rows.  We may need the\ndata for the last month and don’t need several years’ worth.  We may\nneed the information for Alice and don’t need the information for Bob.</p>\n\n<p>Traditional relational databases provide indexes on any number of columns or\nsets of columns.  This is excellent if you are using a traditional relational\ndatabase.  Unfortunately the data structures to provide arbitrary indexes\ndon’t mix well with some of the attributes discussed above and we’re limited to\na single index that partitions our data into sorted blocks.</p>\n\n<h2 id=\"some-projects-that-implement-these-principles\">Some projects that implement these principles</h2>\n\n<p>Many modern distributed database storage systems designed for analytic queries\nimplement these principles well.  Notable players include\n<a href=\"http://docs.aws.amazon.com/redshift/latest/dg/welcome.html\">Redshift</a> and\n<a href=\"https://github.com/Parquet/parquet-format\">Parquet</a>.</p>\n\n<p>Additionally newer single-machine data stores like <a href=\"https://dato.com/products/create/docs/generated/graphlab.SFrame.html\">Dato’s\nSFrame</a>\nand <a href=\"http://bcolz.blosc.org/\">BColz</a> follow many of these principles.  Finally\nmany people have been doing this for a long time with custom use of libraries\nlike HDF5.</p>\n\n<p>It turns out that these principles are actually quite easy to implement with\nthe right tools (thank you #PyData) The rest of this post will talk about a\ntiny 500 line project, <a href=\"https://github.com/blaze/castra\">Castra</a>, that\nimplements these princples and gets good speedups on biggish Pandas data.</p>\n\n<h2 id=\"castra\">Castra</h2>\n\n<p>With these goals in mind we built <a href=\"https://github.com/blaze/castra\">Castra</a>, a\nbinary partitioned compressed columnstore with builtin support for categoricals\nand integration with both Pandas and dask.dataframe.</p>\n\n<h3 id=\"load-data-from-csv-files-sort-on-index-save-to-castra\">Load data from CSV files, sort on index, save to Castra</h3>\n\n<p>Here we load in our data from CSV files, sort on the pickup datetime column,\nand store to a castra file.  This takes about an hour (as compared to eleven\nminutes for a single read.)  Again, <a href=\"https://github.com/blaze/dask-examples/blob/master/nyctaxi-storage.ipynb\">you can view the full notebook here</a></p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'csv/trip_data_*.csv'</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                  <span class=\"n\">skipinitialspace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                  <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'pickup_datetime'</span><span class=\"p\">,</span> <span class=\"s\">'dropoff_datetime'</span><span class=\"p\">])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">set_index</span><span class=\"p\">(</span><span class=\"s\">'pickup_datetime'</span><span class=\"p\">,</span> <span class=\"n\">compute</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>    <span class=\"p\">.</span><span class=\"n\">to_castra</span><span class=\"p\">(</span><span class=\"s\">'trip.castra'</span><span class=\"p\">,</span> <span class=\"n\">categories</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">))</span></code></pre>\n</figure>\n\n<h3 id=\"profit\">Profit</h3>\n\n<p>Now we can take advantage of columnstores, compression, and binary\nrepresentation to perform analytic queries quickly.  Here is code to create a\nhistogram of trip distance.  The plot of the results follows below.</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask-trip-distance.gif\" /></p>\n\n<p>Note that this is especially fast because <a href=\"http://continuum.io/blog/pandas-releasing-the-gil\">Pandas now releases the\nGIL</a> on <code class=\"language-plaintext highlighter-rouge\">value_counts</code>\noperations (all <code class=\"language-plaintext highlighter-rouge\">groupby</code> operations really).  This takes around 20 seconds on\nmy machine on the last release of Pandas vs 5 seconds on the development\nbranch.  Moving from CSV files to Castra moved the bottleneck of our\ncomputation from disk I/O to processing speed, allowing improvements like\nmulti-core processing to really shine.</p>\n\n<p>We plot the result of the above computation with Bokeh below.  Note the spike\naround 20km.  This is around the distance from Midtown Manhattan to LaGuardia\nairport.</p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/e269d02ed15947ba58c5/raw/2c8620c11af14fcdd6e0948de1d6b398cf56001a/nyc-trip-distance.html\" width=\"830\" height=\"350\"></iframe>\n\n<p>I’ve shown Castra used above with\n<a href=\"http://dask.pydata.org/en/latest/dataframe.html\">dask.dataframe</a> but it works\nfine with straight Pandas too.</p>\n\n<h3 id=\"credit\">Credit</h3>\n\n<p>Castra was started by myself and Valentin Haenel (current maintainer of\n<a href=\"https://github.com/blosc/bloscpack/\">bloscpack</a> and\n<a href=\"http://bcolz.blosc.org/\">bcolz</a>) during an evening sprint following PyData\nBerlin.  Several bugfixes and refactors were followed up by Phil Cloud and Jim\nCrist.</p>\n\n<p>Castra is roughly 500 lines long.  It’s a tiny project which is both good and\nbad.  It’s being used experimentally and there are some heavy disclaimers in\nthe <a href=\"https://github.com/blaze/castra\">README</a>.  This post is not intended as a\nsales pitch for Castra, but rather to provide a vocabulary to talk about\nefficient tabular storage.</p>\n\n<p><em>Response to twitter traffic</em>: again, this blogpost is not saying “use Castra!”\nRather it says “don’t use CSVs!” and consider more efficient storage for\ninteractive use.  Other, more mature solutions exist or could be built.  Castra\nwas an experiment of “how fast can we get without doing too much work.”</p>"
}