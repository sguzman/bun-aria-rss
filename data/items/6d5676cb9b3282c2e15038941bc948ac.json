{
  "title": "2017 Outlook: pandas, Arrow, Feather, Parquet, Spark, Ibis",
  "link": "",
  "published": "2016-12-27T10:00:00-08:00",
  "updated": "2016-12-27T10:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2016-12-27:/blog/outlook-for-2017/",
  "summary": "<p>2017 is shaping up to be an exciting year in Python data development. In this\npost I'll give you a flavor of what to expect from my end. In follow up blog\nposts, I plan to go into more depth about how all the pieces fit together. I\nhave been a bit delinquent in blogging in 2016, since my hands have been quite\nfull doing development and working on the 2nd edition of <a href=\"http://shop.oreilly.com/product/0636920050896.do\"><em>Python for Data\nAnalysis</em></a>. I am going to do my best to write more in 2017.</p>",
  "content": "<p>2017 is shaping up to be an exciting year in Python data development. In this\npost I'll give you a flavor of what to expect from my end. In follow up blog\nposts, I plan to go into more depth about how all the pieces fit together. I\nhave been a bit delinquent in blogging in 2016, since my hands have been quite\nfull doing development and working on the 2nd edition of <a href=\"http://shop.oreilly.com/product/0636920050896.do\"><em>Python for Data\nAnalysis</em></a>. I am going to do my best to write more in 2017.</p>\n\n\n<h2>New position</h2>\n<p>After a productive <a href=\"http://wesmckinney.com/blog/thoughts-on-joining-cloudera/\">2 years with Cloudera</a>, a few months ago I transitioned\nto a software architect role at <a href=\"https://twosigma.com\">Two Sigma Investments</a>. Since leaving the\nquant finance world in 2010, I have observed the profound impact that open\nsource tools have had on the financial industry. I was refreshed to find that\nforward thinking institutions like Two Sigma have increased their engagement\nwith the open source ecosystem, releasing internally-developed tools and\ncontributing work back to established OSS projects.</p>\n<p>In my new role, I am working to solve data analysis problems that are\nwell-aligned with the open source software I've been developing over the last\ndecade:</p>\n<ul>\n<li>User-friendly API design</li>\n<li>High performance IO and data access</li>\n<li>Fast and expressive computational engines</li>\n</ul>\n<p>With regards to open source and business, there's a couple of interesting\ntrends happening right now, in my view:</p>\n<ul>\n<li>Companies not participating in open source (as users and/or developers) are\n  getting left behind</li>\n<li>Many of the best software engineers won't work for a company that forbids\n  them from working on open source projects (I certainly would not).</li>\n</ul>\n<h2>pandas 2.0</h2>\n<p>Over the last year, we have been publicly discussing a plan to improve the\ninternals of pandas to better suit the needs of today's data problems. I spoke\na bit about this <a href=\"http://www.slideshare.net/wesm/python-data-wrangling-preparing-for-the-future\">in a recent talk</a>.</p>\n<p>General speaking, the goals of pandas 2.0 are:</p>\n<ul>\n<li>Fixing design warts and accumulated technical debt from the last 9 years.</li>\n<li>Faster single-threaded performance</li>\n<li>More efficient memory management, less memory usage</li>\n<li>Improved performance and scalability through true multithreaded execution</li>\n</ul>\n<p>My goal is to deliver the same quality pandas user experience on 10x as much\ndata. pandas works well on 1GB of data, but less well on 10GB. This has to\nchange for pandas to remain a relevant tool in the future.</p>\n<p>In the meantime, the pandas team is toiling away with a major 0.20 release in\nthe works, followed by <strong>pandas 1.0</strong>, which will mark a period of API\nstability while we work on refactoring the pandas core.</p>\n<h2>Apache Arrow</h2>\n<p>Last February, we announced <a href=\"http://arrow.apache.org/\">Apache Arrow</a>, a collaboration amongst open\nsource data projects to establish a standard for high-performance in-memory\ncolumnar data structures and IO.</p>\n<p>Arrow development has been proceeding well since then. We recently reached a\nmajor development milestone by achieving full binary compatibility between the\ninitial Java and C++ library implementations. This is a critical step in\ndelivering high performance IO between the JVM (and systems like Spark) and\nC/C++ or Python-based systems.</p>\n<p>One of my goals in building Arrow's C++ libraries is to facilitate low-overhead\ncolumnar memory management and high performance, multithreaded IO in pandas\n2.0. When I can, I will write some posts about these tools and how they help.</p>\n<p>While pandas 2.0 is in development, we'll continue to do work to make Arrow a\nhigh speed \"data conduit\" for data en route to or from the current production\nversion of pandas.</p>\n<h2>Apache Parquet for Python</h2>\n<p>In 2016, we've worked to create a <a href=\"http://github.com/apache/parquet-cpp\">production-grade C++ library</a> for reading\nand writing the Apache Parquet file format. En route, I became a committer and\nthen a PMC member of Apache Parquet. <a href=\"https://github.com/xhochy\">Uwe Korn</a>, from Blue Yonder, has also\nbecome a Parquet committer.</p>\n<p>As Parquet is columnar file format designed for small size and IO efficiency,\nArrow is an in-memory columnar container ideal as a transport layer to and from\nParquet.</p>\n<p>To read and write Parquet files from Python using Arrow and parquet-cpp, you\ncan install <code>pyarrow</code> from conda-forge:</p>\n<div class=\"github\"><pre><span></span><code>conda install pyarrow -c conda-forge\n</code></pre></div>\n\n<p>Then, the code to read looks like:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">pyarrow.parquet</span> <span class=\"k\">as</span> <span class=\"nn\">pq</span>\n<span class=\"n\">arrow_table</span> <span class=\"o\">=</span> <span class=\"n\">pq</span><span class=\"o\">.</span><span class=\"n\">read_table</span><span class=\"p\">(</span><span class=\"s1\">&#39;path-to-data/0.parquet&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">arrow_table</span><span class=\"o\">.</span><span class=\"n\">to_pandas</span><span class=\"p\">()</span>\n</code></pre></div>\n\n<p>We'll be writing more documentation and blog posts about Parquet in the coming\nmonths.</p>\n<p>Continuum Analytics recently started developing a <a href=\"http://github.com/dask/fastparquet\">separate Parquet\nimplementation</a> for Python that uses Numba for accelerating encoding and\ndecoding routines. I'm glad to see more Python developers working on these\nproblems.</p>\n<h2>Feather file format</h2>\n<p>Earlier this year, I worked with <a href=\"http://hadley.nz\">Hadley Wickham</a> to design and deliver the\n<a href=\"http://github.com/wesm/feather\">Feather file format</a> for R and Python. Feather uses Apache Arrow's columnar\nrepresentation and sports a simple metadata specification that can handle the\nmain R and Python data types.</p>\n<p>As time has passed, as one might expect, quite a bit of code overlap has\ndeveloped between Feather's C++ and Python components and Apache Arrow's\nrespective C++ and Python libraries. To address this, I'm planning to merge the\nFeather implementation into the Arrow codebase, which will enable me to provide\nbetter performance and new features to Feather users.</p>\n<h2>Improving PySpark</h2>\n<p>PySpark, the Python API for <a href=\"http://spark.apache.org\">Apache Spark</a>, has well known performance\nissues (compared with Scala equivalents) around data serialization (Spark's\n<code>DataFrame.toPandas</code> and <code>sqlContext.createDataFrame</code>) and relatedly UDF\nevaluation (<code>rdd.map</code>, <code>rdd.mapPartition</code>, or <code>sqlContext.registerFunction</code>).</p>\n<p>In line with the above work on fast columnar IO for Python and the JVM using\nArrow, some of my Two Sigma colleagues and I are collaborating with IBM and the\nSpark community to accelerate PySpark using these new technologies. If you\nwould like to get involved with this, please let me know.</p>\n<p>The PySpark-Arrow work is progressing well, and we should have some interesting\nupdates to share in February at the <a href=\"https://spark-summit.org/east-2017/\">Spark Summit East conference</a>.</p>\n<h2>Update on Ibis</h2>\n<p>Last, but not least, I am still maintaining <a href=\"http://github.com/cloudera/ibis\">Ibis</a> and helping its users\nhow I can. I'm proud of the level of deep SQL semantic support that Ibis\nprovides its users. You can write very complex queries with pandas-like Python\ncode that is composable and reusable.</p>\n<p>As pandas 2.0 progresses, I am interested in building an in-memory backend for\nIbis. I had thought about doing this in the past, but decided it would be\nbetter to wait.</p>"
}