{
  "title": "Some simple ways to speed up convnet a little",
  "link": "https://mirror2image.wordpress.com/2014/12/28/some-simple-ways-to-speed-up-convnet-a-little/",
  "dc:creator": "mirror2image",
  "pubDate": "Sun, 28 Dec 2014 06:21:17 +0000",
  "category": [
    "computer vision",
    "Deep Learnig",
    "cnn",
    "convnet",
    "convolutional network",
    "Deep Learning"
  ],
  "guid": "http://mirror2image.wordpress.com/?p=1639",
  "description": "There are quite a few complex methods for making convolutional networks converge faster. Natural gradient, dual coordinate ascent, second order hessian free methods and more. However those methods are usually require considerable memory spending and extensive modifications of existing code if you already had some working method before. Here instead I&#8217;ll list some simple and [&#8230;]",
  "content:encoded": "<p>There are quite a few complex methods for making convolutional networks converge faster. Natural gradient, dual coordinate ascent, second order hessian free methods and more. However those methods are usually require considerable memory spending and extensive modifications of existing code if you already had some working method before.</p>\n<p>Here instead I&#8217;ll list some <em>simple</em> and lightweight methods, which don&#8217;t require extensive changes of already working code. Those methods works (I&#8217;ve tested them on CIFAR10), and at very worst don&#8217;t make convergence worse. You shouldn&#8217;t expect radical improvement from those methods &#8211; they are for <em>little </em>speed up.</p>\n<p>1. Start training on the part of the data and gradually  increase data size to full. For example for CIFAR10 I start with 1/5 of all the data and increase it by 1/5 each and time giving it 3 more epochs to train for each chunk. This trick was inspired by  <a href=\"http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf\">&#8220;curriculum learning&#8221;</a>. Resulting acceleration is small but still visible (on order of 10%).</p>\n<p>2. Random sampling. Running all the data batches in the same order theoretically can produce some small overfitting &#8211; coadaptaion of the data. Usually this effect is not visible, but just to be sure you can reshuffle data randomly each epoch, especially if it cost very little. I&#8217;m using simple reshuffling method based on the prime numbers table.<br />\nFor each minibatch k I take samples indexed by i: (i*prime)%data_size where i is from k*minibatch_size to (k+1)*minibatch_size, and prime taken form prime numbers table change each epoch. If prime is more than all prime factors of data_size all samples are indexed once.</p>\n<p>3. Gradient step size. The baseline method is to use simple momentum. Most popular method of gradient acceleration on top of momentum are RMSProp and Nesterov accelerated gadient (NAG).<br />\n<a href=\"http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\" title=\"nag\">NAG</a> simply change order in which momentum and gradient are applied to weights. RMSProp is normalization of the gradient, so that it should have approximately same norm. One of the variants <a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\" title=\"rms\">described here, which is similar to low pass filter</a>.Another possible implementation &#8211; gradient is divided by running  <a href=\"http://en.wikipedia.org/wiki/Root_mean_square\" title=\"root mean square\">root mean square</a> of it&#8217;s previous values. However in my tests on CIFAR10 neither NAG nor RMSprop or NAG+RMSProp show any visible improvement. </p>\n<p> Nevertheless in my tests a  simple modification of gradient step show better result then standard momentum. That is just a common optimization trick &#8211; discard or reduce those gradient step which increase error function. Straightforward implementation of it could be costly for convnet &#8211; to estimate cost function after gradient step require additional forward propagation. There is a workaround &#8211; estimate error on the next sample of the dataset and if error increased subtract part of the previous gradient. This is not precisely the same as reducing bad gradient, because we subtract offending gradient <em>after</em> we made next step, but because the step is small it still works. Essentially it could be seen as error-dependent gradient step </p>\n<p>4. <a href=\"http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\">Dropout. </a>Use of dropout require some care, especially if we want to insert dropout into networks built and tuned without dropout. Common recommendation is to increase number of filters proportionally to dropout value.<br />\nBut there are some hidden traps there: while dropout should improve <em>test </em>error(results on the samples not used in training) it make <em>training </em>error noticeably worse. In practice it may make worse even <em>test </em>error. Dropout also make convergence considerably more slow. There is non-obvious trick which help: continue iterations without changing learning rate for some times even after <em>training </em>error is not decreasing any more. Intuition behind this trick &#8211; dropout reduce <em>test </em>error, not <em>training </em>error. <em>Test </em>error decrease often very slow and noisy comparing to <em>training </em>error, so, just to be sure it may help to increase number of epoch even after both stop decreasing, without decreasing learning rate. Reducing learning rate after both training and test error went plateau for some epochs may produce better results.</p>\n<p>To be continued (may be)</p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}