{
  "title": "Nonsensical beer reviews via Markov chains",
  "link": "",
  "published": "2015-03-30T00:00:00-07:00",
  "updated": "2022-11-03T04:16:07-07:00",
  "author": {
    "name": "Greg Reda"
  },
  "id": "tag:www.gregreda.com,2015-03-30:/2015/03/30/beer-review-markov-chains/",
  "summary": "<p>I’ve had a bunch of beer reviews and ratings data sitting on my hard drive for about year. For a beer nerd like me, that’s a pretty cool dataset, yet I’ve let it collect digital dust.</p>\n<p>Fast forward to last week, where somehow I wound up in …</p>",
  "content": "<p>I’ve had a bunch of beer reviews and ratings data sitting on my hard drive for about year. For a beer nerd like me, that’s a pretty cool dataset, yet I’ve let it collect digital dust.</p>\n<p>Fast forward to last week, where somehow I wound up in the Wikipedia Death Spiral. You know what I mean - you click a link to a Wikipedia article, that article takes you to a new one, then you’re on another, and another … we’ve all been there. And it’s kind of awesome.</p>\n<p>Well, the rabbit hole led me to <a href=\"http://en.wikipedia.org/wiki/Markov_chain\">Markov chains</a>, which seemed like a good excuse to mess around with that beer review data.</p>\n<h2>What are Markov chains?</h2>\n<p>Markov chains are a random process that transitions to various states, where the “next state” is based on its probability distribution, given the current state.</p>\n<p>Imagine we have the following sequence of days, where S indicates it was sunny and R indicates it was rainy:</p>\n<blockquote>\n<p>S S R R S R S S R R R R S R S S S R</p>\n</blockquote>\n<p>Let’s pick a random beginning “state” - let’s just say it’s S (sunny). The next state is based <strong>only</strong> on the current state. Since our current state is S, we only need to look at observations immediately following a sunny day.</p>\n<p>To illustrate, let’s look at the weather pattern again, this time putting the observations to be considered in bold.</p>\n<blockquote>\n<p>S <strong>S</strong> <strong>R</strong> R S <strong>R</strong> S <strong>S</strong> <strong>R</strong> R R R S <strong>R</strong> S <strong>S</strong> <strong>S</strong> <strong>R</strong></p>\n</blockquote>\n<p>Even though there are 18 observations, only nine need to be considered for the possible next state. Of the nine, four are S and five are R, giving us a 44% (4/9) chance of the next state being sunny and a 55% (5/9) chance of it being rainy.</p>\n<p>Now, let’s assume our beginning state (S) transitioned to a second state of R (which it had a 55% chance of doing). Here are the states we need to consider for the possible third state:</p>\n<blockquote>\n<p>S S R <strong>R</strong> <strong>S</strong> R <strong>S</strong> S R <strong>R</strong> <strong>R</strong> <strong>R</strong> <strong>S</strong> R <strong>S</strong> S S R</p>\n</blockquote>\n<p>There’s an equal chance (4/8) the third state will be S or R.</p>\n<p>With a second-order Markov chain, the current state is two observations. Let’s assume a beginning state of SR and use the same weather sequence as above, again putting the possible next states in bold.</p>\n<blockquote>\n<p>S S R <strong>R</strong> S R <strong>S</strong> S R <strong>R</strong> R R S R <strong>S</strong> S S R</p>\n</blockquote>\n<p>This time there are only four observations to consider as possible “next states,” with an equal chance it’ll be S or R.</p>\n<p>Let’s assume the “next state” picked is R. Now our current (second) state is RR - the S from our beginning state is forgotten. The following are possible third states:</p>\n<blockquote>\n<p>S S R R <strong>S</strong> R S S R R <strong>R</strong> <strong>R</strong> <strong>S</strong> R S S S R</p>\n</blockquote>\n<p>Again, there’s an equal chance of our third state being S or R.</p>\n<p>We can continue picking “next states” and eventually we’ll have generated a random, yet probabilistic sequence of weather.</p>\n<p>These same principles can be used to generate a sentence from text data - pick a random beginning state (word) from the text and then pick the next word based on the likelihood of it occurring, given the current word. A first-order Markov sentence would have a one word current state, a second-order would have a two word current state, … and so on.</p>\n<p>The larger the corpus and the higher the order, the more sense these Markov generated sentences make. Good thing I have a lot of beer reviews.</p>\n<h2>The (mini) project</h2>\n<p>This seemed ripe for a Twitter bot, so I created <a href=\"https://twitter.com/BeerSnobSays\">BeerSnobSays</a>, which tweets nonsensical beer reviews generated via second-order Markov chains.</p>\n<p>Not everything it tweets makes much sense:</p>\n<blockquote>\n<p>dissipates about a finger of head and some mild spice interwoven and even beer at a local Greek restaurant.</p>\n<p>a big thumbs up though and there are plenty other choices that I was really no distinguishing characteristics that stand out.</p>\n<p>those who are looking for a beer best characteristic of this beer into the hype and the lager style that is unwelcome.</p>\n</blockquote>\n<p>But some of it is pretty funny:</p>\n<blockquote>\n<p>off by itself, the taste of apple juice colored brew with a nice warming alcohol bathes your noodle in its dryness.</p>\n<p>is almost like sour grains with a hint of booze in the finish, with sweet orange peels and pine sap.</p>\n<p>a charred woodiness and smoke can run into pineapple, oranges and citrusy oils with a clean alcohol sting at the bottom of the recipe.</p>\n<p>the berry aspect is evident but the tartness and dryness from the beer starts off surprisingly pleasant.</p>\n</blockquote>\n<p>I’m not sure if that last one’s from the bot or a famous poet.</p>\n<p>You can <a href=\"https://twitter.com/gjreda\">follow me</a> and <a href=\"https://twitter.com/BeerSnobSays\">BeerSnobSays</a> on Twitter. You can also find the code for the bot <a href=\"https://github.com/gjreda/beer-snob-says\">on GitHub</a>.</p>",
  "category": [
    "",
    "",
    "",
    ""
  ]
}