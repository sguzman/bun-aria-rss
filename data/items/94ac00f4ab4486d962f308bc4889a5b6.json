{
  "title": "Plotting the Training and Validation Loss Curves for the Transformer Model",
  "link": "https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/",
  "comments": "https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/#comments",
  "dc:creator": "Stefania Cristina",
  "pubDate": "Tue, 18 Oct 2022 18:37:09 +0000",
  "category": [
    "Attention",
    "attention",
    "natural language processing",
    "training loss",
    "transformer",
    "validation loss"
  ],
  "guid": "https://machinelearningmastery.com/?p=13879",
  "description": "<p>Last Updated on November 2, 2022 We have previously seen how to train the Transformer model for neural machine translation. Before moving on to inferencing the trained model, let us first explore how to modify the training code slightly to be able to plot the training and validation loss curves that can be generated during [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\">Plotting the Training and Validation Loss Curves for the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "content:encoded": "<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13879 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Plotting+the+Training+and+Validation+Loss+Curves+for+the+Transformer+Model&url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Plotting+the+Training+and+Validation+Loss+Curves+for+the+Transformer+Model&url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p id=\"last-modified-info\">Last Updated on November 2, 2022</p>\n<p>We have previously seen how to train the Transformer model for neural machine translation. Before moving on to inferencing the trained model, let us first explore how to modify the training code slightly to be able to plot the training and validation loss curves that can be generated during the learning process.<span class=\"Apple-converted-space\"> </span></p>\n<p>The training and validation loss values provide important information because they give us a better insight into how the learning performance changes over the number of epochs and help us diagnose any problems with learning that can lead to an underfit or an overfit model. They will also inform us about the epoch with which to use the trained model weights at the inferencing stage.</p>\n<p>In this tutorial, you will discover how to plot the training and validation loss curves for the Transformer model.<span class=\"Apple-converted-space\"> </span></p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>How to modify the training code to include validation and test splits, in addition to a training split of the dataset</li>\n<li>How to modify the training code to store the computed training and validation loss values, as well as the trained model weights</li>\n<li>How to plot the saved training and validation loss curves</li>\n</ul>\n<p>Let’s get started.<span class=\"Apple-converted-space\"> </span></p>\n<div id=\"attachment_13889\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover.jpg\"><img aria-describedby=\"caption-attachment-13889\" loading=\"lazy\" class=\"wp-image-13889 size-large\" src=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-1024x767.jpg\" alt=\"\" width=\"1024\" height=\"767\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-1024x767.jpg 1024w, https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-300x225.jpg 300w, https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-768x576.jpg 768w, https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-1536x1151.jpg 1536w, https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_cover-2048x1535.jpg 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><p id=\"caption-attachment-13889\" class=\"wp-caption-text\">Plotting the training and validation loss curves for the Transformer model<br />Photo by <a href=\"https://unsplash.com/photos/zS4lUqLEiNA\">Jack Anstey</a>, some rights reserved.</p></div>\n<h2><b>Tutorial Overview</b></h2>\n<p>This tutorial is divided into four parts; they are:</p>\n<ul>\n<li>Recap of the Transformer Architecture</li>\n<li>Preparing the Training, Validation, and Testing Splits of the Dataset</li>\n<li>Training the Transformer Model</li>\n<li>Plotting the Training and Validation Loss Curves</li>\n</ul>\n<h2><b>Prerequisites</b></h2>\n<p>For this tutorial, we assume that you are already familiar with:</p>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/the-transformer-model/\">The theory behind the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">An implementation of the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/training-the-transformer-model/\">Training the Transformer model</a></li>\n</ul>\n<h2><b>Recap of the Transformer Architecture</b></h2>\n<p><a href=\"https://machinelearningmastery.com/the-transformer-model/\">Recall</a> having seen that the Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.</p>\n<div id=\"attachment_12821\" style=\"width: 379px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"><img aria-describedby=\"caption-attachment-12821\" loading=\"lazy\" class=\"wp-image-12821\" src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" alt=\"\" width=\"369\" height=\"520\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png 727w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-213x300.png 213w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png 768w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-1090x1536.png 1090w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png 1320w\" sizes=\"(max-width: 369px) 100vw, 369px\" /></a><p id=\"caption-attachment-12821\" class=\"wp-caption-text\">The encoder-decoder structure of the Transformer architecture <br />Taken from &#8220;<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>&#8220;</p></div>\n<p>In generating an output sequence, the Transformer does not rely on recurrence and convolutions.</p>\n<p>You have seen how to train the complete Transformer model, and you shall now see how to generate and plot the training and validation loss values that will help you diagnose the model’s learning performance.<span class=\"Apple-converted-space\"> </span></p>\n<p><strong>Kick-start your project</strong> with my book <a href=\"https://machinelearningmastery.com/transformer-models-with-attention/\">Building Transformer Models with Attention</a>. It provides <strong>self-study tutorials</strong> with <strong>working code</strong> to guide you into building a fully-working transformer models that can<br><em>translate sentences from one language to another</em>...</p>\n<h2><b>Preparing the Training, Validation, and Testing Splits of the Dataset</b></h2>\n<p>In order to be able to include validation and test splits of the data, you will modify the code that <a href=\"https://machinelearningmastery.com/?p=13585&preview=true\">prepares the dataset</a> by introducing the following lines of code, which:</p>\n<ul>\n<li>Specify the size of the validation data split. This, in turn, determines the size of the training and test splits of the data, which we will be dividing into a ratio of 80:10:10 for the training, validation, and test sets, respectively:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">self.val_split = 0.1  # Ratio of the validation data split</pre><p></p>\n<ul>\n<li>Split the dataset into validation and test sets in addition to the training set:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">val = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\ntest = dataset[int(self.n_sentences * (1 - self.val_split)):]</pre><p></p>\n<ul>\n<li>Prepare the validation data by tokenizing, padding, and converting to a tensor. For this purpose, you will collect these operations into a function called <code>encode_pad</code>, as shown in the complete code listing below. This will avoid excessive repetition of code when performing these operations on the training data as well:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\nvalY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)</pre><p></p>\n<ul>\n<li>Save the encoder and decoder tokenizers into pickle files and the test dataset into a text file to be used later during the inferencing stage:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">self.save_tokenizer(enc_tokenizer, 'enc')\nself.save_tokenizer(dec_tokenizer, 'dec')\nsavetxt('test_dataset.txt', test, fmt='%s')</pre><p>The complete code listing is now updated as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from pickle import load, dump, HIGHEST_PROTOCOL\nfrom numpy.random import shuffle\nfrom numpy import savetxt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import convert_to_tensor, int64\n\nclass PrepareDataset:\n    def __init__(self, **kwargs):\n        super(PrepareDataset, self).__init__(**kwargs)\n        self.n_sentences = 15000  # Number of sentences to include in the dataset\n        self.train_split = 0.8  # Ratio of the training data split\n        self.val_split = 0.1  # Ratio of the validation data split\n\n    # Fit a tokenizer\n    def create_tokenizer(self, dataset):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(dataset)\n\n        return tokenizer\n\n    def find_seq_length(self, dataset):\n        return max(len(seq.split()) for seq in dataset)\n\n    def find_vocab_size(self, tokenizer, dataset):\n        tokenizer.fit_on_texts(dataset)\n\n        return len(tokenizer.word_index) + 1\n\n    # Encode and pad the input sequences\n    def encode_pad(self, dataset, tokenizer, seq_length):\n        x = tokenizer.texts_to_sequences(dataset)\n        x = pad_sequences(x, maxlen=seq_length, padding='post')\n        x = convert_to_tensor(x, dtype=int64)\n\n        return x\n\n    def save_tokenizer(self, tokenizer, name):\n        with open(name + '_tokenizer.pkl', 'wb') as handle:\n            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n\n    def __call__(self, filename, **kwargs):\n        # Load a clean dataset\n        clean_dataset = load(open(filename, 'rb'))\n\n        # Reduce dataset size\n        dataset = clean_dataset[:self.n_sentences, :]\n\n        # Include start and end of string tokens\n        for i in range(dataset[:, 0].size):\n            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n\n        # Random shuffle the dataset\n        shuffle(dataset)\n\n        # Split the dataset in training, validation and test sets\n        train = dataset[:int(self.n_sentences * self.train_split)]\n        val = dataset[int(self.n_sentences * self.train_split):int(self.n_sentences * (1-self.val_split))]\n        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n\n        # Prepare tokenizer for the encoder input\n        enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n        enc_seq_length = self.find_seq_length(dataset[:, 0])\n        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n\n        # Prepare tokenizer for the decoder input\n        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n        dec_seq_length = self.find_seq_length(dataset[:, 1])\n        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n\n        # Encode and pad the training input\n        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n\n        # Encode and pad the validation input\n        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n\n        # Save the encoder tokenizer\n        self.save_tokenizer(enc_tokenizer, 'enc')\n\n        # Save the decoder tokenizer\n        self.save_tokenizer(dec_tokenizer, 'dec')\n\n        # Save the testing dataset into a text file\n        savetxt('test_dataset.txt', test, fmt='%s')\n\n        return trainX, trainY, valX, valY, train, val, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size</pre><p></p>\n<h2><b>Training the Transformer Model</b></h2>\n<p>We shall introduce similar modifications to the code that <a href=\"https://machinelearningmastery.com/?p=13585&preview=true\">trains the Transformer model</a> to:</p>\n<ul>\n<li>Prepare the validation dataset batches:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">val_dataset = data.Dataset.from_tensor_slices((valX, valY))\nval_dataset = val_dataset.batch(batch_size)</pre><p></p>\n<ul>\n<li>Monitor the validation loss metric:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">val_loss = Mean(name='val_loss')</pre><p></p>\n<ul>\n<li>Initialize dictionaries to store the training and validation losses and eventually store the loss values in the respective dictionaries:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">train_loss_dict = {}\nval_loss_dict = {}\n\ntrain_loss_dict[epoch] = train_loss.result()\nval_loss_dict[epoch] = val_loss.result()</pre><p></p>\n<ul>\n<li>Compute the validation loss:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">loss = loss_fcn(decoder_output, prediction)\nval_loss(loss)</pre><p></p>\n<ul>\n<li>Save the trained model weights at every epoch. You will use these at the inferencing stage to investigate the differences in results that the model produces at different epochs.<span class=\"Apple-converted-space\">  </span>In practice, it would be more efficient to include a callback method that halts the training process based on the metrics that are being monitored during training and only then save the model weights:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># Save the trained model weights\ntraining_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")</pre><p></p>\n<ul>\n<li>Finally, save the training and validation loss values into pickle files:</li>\n</ul>\n<p></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">with open('./train_loss.pkl', 'wb') as file:\n    dump(train_loss_dict, file)\n\nwith open('./val_loss.pkl', 'wb') as file:\n    dump(val_loss_dict, file)</pre><p>The modified code listing now becomes:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, function\nfrom keras.losses import sparse_categorical_crossentropy\nfrom model import TransformerModel\nfrom prepare_dataset import PrepareDataset\nfrom time import time\nfrom pickle import dump\n\n\n# Define the model parameters\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_model = 512  # Dimensionality of model layers' outputs\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nn = 6  # Number of layers in the encoder stack\n\n# Define the training parameters\nepochs = 20\nbatch_size = 64\nbeta_1 = 0.9\nbeta_2 = 0.98\nepsilon = 1e-9\ndropout_rate = 0.1\n\n\n# Implementing a learning rate scheduler\nclass LRScheduler(LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n        super(LRScheduler, self).__init__(**kwargs)\n\n        self.d_model = cast(d_model, float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step_num):\n\n        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n        arg1 = step_num ** -0.5\n        arg2 = step_num * (self.warmup_steps ** -1.5)\n\n        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n\n\n# Instantiate an Adam optimizer\noptimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n\n# Prepare the training dataset\ndataset = PrepareDataset()\ntrainX, trainY, valX, valY, train_orig, val_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german.pkl')\n\nprint(enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n\n# Prepare the training dataset batches\ntrain_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\ntrain_dataset = train_dataset.batch(batch_size)\n\n# Prepare the validation dataset batches\nval_dataset = data.Dataset.from_tensor_slices((valX, valY))\nval_dataset = val_dataset.batch(batch_size)\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n\n\n# Defining the loss function\ndef loss_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of loss\n    padding_mask = math.logical_not(equal(target, 0))\n    padding_mask = cast(padding_mask, float32)\n\n    # Compute a sparse categorical cross-entropy loss on the unmasked values\n    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n\n    # Compute the mean loss over the unmasked values\n    return reduce_sum(loss) / reduce_sum(padding_mask)\n\n\n# Defining the accuracy function\ndef accuracy_fcn(target, prediction):\n    # Create mask so that the zero padding values are not included in the computation of accuracy\n    padding_mask = math.logical_not(equal(target, 0))\n\n    # Find equal prediction and target values, and apply the padding mask\n    accuracy = equal(target, argmax(prediction, axis=2))\n    accuracy = math.logical_and(padding_mask, accuracy)\n\n    # Cast the True/False values to 32-bit-precision floating-point numbers\n    padding_mask = cast(padding_mask, float32)\n    accuracy = cast(accuracy, float32)\n\n    # Compute the mean accuracy over the unmasked values\n    return reduce_sum(accuracy) / reduce_sum(padding_mask)\n\n\n# Include metrics monitoring\ntrain_loss = Mean(name='train_loss')\ntrain_accuracy = Mean(name='train_accuracy')\nval_loss = Mean(name='val_loss')\n\n# Create a checkpoint object and manager to manage multiple checkpoints\nckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\nckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=None)\n\n# Initialise dictionaries to store the training and validation losses\ntrain_loss_dict = {}\nval_loss_dict = {}\n\n# Speeding up the training process\n@function\ndef train_step(encoder_input, decoder_input, decoder_output):\n    with GradientTape() as tape:\n\n        # Run the forward pass of the model to generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=True)\n\n        # Compute the training loss\n        loss = loss_fcn(decoder_output, prediction)\n\n        # Compute the training accuracy\n        accuracy = accuracy_fcn(decoder_output, prediction)\n\n    # Retrieve gradients of the trainable variables with respect to the training loss\n    gradients = tape.gradient(loss, training_model.trainable_weights)\n\n    # Update the values of the trainable variables by gradient descent\n    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n\n    train_loss(loss)\n    train_accuracy(accuracy)\n\n\nfor epoch in range(epochs):\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    val_loss.reset_states()\n\n    print(\"\\nStart of epoch %d\" % (epoch + 1))\n\n    start_time = time()\n\n    # Iterate over the dataset batches\n    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = train_batchX[:, 1:]\n        decoder_input = train_batchY[:, :-1]\n        decoder_output = train_batchY[:, 1:]\n\n        train_step(encoder_input, decoder_input, decoder_output)\n\n        if step % 50 == 0:\n            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    # Run a validation step after every epoch of training\n    for val_batchX, val_batchY in val_dataset:\n\n        # Define the encoder and decoder inputs, and the decoder output\n        encoder_input = val_batchX[:, 1:]\n        decoder_input = val_batchY[:, :-1]\n        decoder_output = val_batchY[:, 1:]\n\n        # Generate a prediction\n        prediction = training_model(encoder_input, decoder_input, training=False)\n\n        # Compute the validation loss\n        loss = loss_fcn(decoder_output, prediction)\n        val_loss(loss)\n\n    # Print epoch number and accuracy and loss values at the end of every epoch\n    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f, Validation Loss %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result()))\n\n    # Save a checkpoint after every epoch\n    if (epoch + 1) % 1 == 0:\n\n        save_path = ckpt_manager.save()\n        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n\n        # Save the trained model weights\n        training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n\n        train_loss_dict[epoch] = train_loss.result()\n        val_loss_dict[epoch] = val_loss.result()\n\n# Save the training loss values\nwith open('./train_loss.pkl', 'wb') as file:\n    dump(train_loss_dict, file)\n\n# Save the validation loss values\nwith open('./val_loss.pkl', 'wb') as file:\n    dump(val_loss_dict, file)\n\nprint(\"Total time taken: %.2fs\" % (time() - start_time))</pre><p></p>\n<h2><b>Plotting the Training and Validation Loss Curves</b></h2>\n<p>In order to be able to plot the training and validation loss curves, you will first load the pickle files containing the training and validation loss dictionaries that you saved when training the Transformer model earlier.<span class=\"Apple-converted-space\"> </span></p>\n<p>Then you will retrieve the training and validation loss values from the respective dictionaries and graph them on the same plot.</p>\n<p>The code listing is as follows, which you should save into a separate Python script:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from pickle import load\nfrom matplotlib.pylab import plt\nfrom numpy import arange\n\n# Load the training and validation loss dictionaries\ntrain_loss = load(open('train_loss.pkl', 'rb'))\nval_loss = load(open('val_loss.pkl', 'rb'))\n\n# Retrieve each dictionary's values\ntrain_values = train_loss.values()\nval_values = val_loss.values()\n\n# Generate a sequence of integers to represent the epoch numbers\nepochs = range(1, 21)\n\n# Plot and label the training and validation loss values\nplt.plot(epochs, train_values, label='Training Loss')\nplt.plot(epochs, val_values, label='Validation Loss')\n\n# Add in a title and axes labels\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n# Set the tick locations\nplt.xticks(arange(0, 21, 2))\n\n# Display the plot\nplt.legend(loc='best')\nplt.show()</pre><p>Running the code above generates a similar plot of the training and validation loss curves to the one below:</p>\n<div id=\"attachment_13888\" style=\"width: 606px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1.png\"><img aria-describedby=\"caption-attachment-13888\" loading=\"lazy\" class=\"wp-image-13888 size-full\" src=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1.png\" alt=\"\" width=\"596\" height=\"457\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1.png 596w, https://machinelearningmastery.com/wp-content/uploads/2022/10/training_validation_loss_1-300x230.png 300w\" sizes=\"(max-width: 596px) 100vw, 596px\" /></a><p id=\"caption-attachment-13888\" class=\"wp-caption-text\">Line plots of the training and validation loss values over several training epochs</p></div>\n<p>Note that although you might see similar loss curves, they might not necessarily be identical to the ones above. This is because you are training the Transformer model from scratch, and the resulting training and validation loss values depend on the random initialization of the model weights.<span class=\"Apple-converted-space\"> </span></p>\n<p>Nonetheless, these loss curves give us a better insight into how the learning performance changes over the number of epochs and help us diagnose any problems with learning that can lead to an underfit or an overfit model.<span class=\"Apple-converted-space\"> </span></p>\n<p>For more details on using the training and validation loss curves to diagnose the learning performance of a model, you can refer to <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">this tutorial</a> by Jason Brownlee.<span class=\"Apple-converted-space\"> </span></p>\n<h2><b>Further Reading</b></h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3><b>Books</b></h3>\n<ul>\n<li><a href=\"https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X\">Advanced Deep Learning with Python</a>, 2019</li>\n<li><a href=\"https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798\">Transformers for Natural Language Processing</a>, 2021</li>\n</ul>\n<h3><b>Papers</b></h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017</li>\n</ul>\n<h3><b>Websites</b></h3>\n<ul>\n<li>How to use Learning Curves to Diagnose Machine Learning Model Performance, <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/</a></li>\n</ul>\n<h2><b>Summary</b></h2>\n<p>In this tutorial, you discovered how to plot the training and validation loss curves for the Transformer model.<span class=\"Apple-converted-space\"> </span></p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How to modify the training code to include validation and test splits, in addition to a training split of the dataset</li>\n<li>How to modify the training code to store the computed training and validation loss values, as well as the trained model weights</li>\n<li>How to plot the saved training and validation loss curves</li>\n</ul>\n<p>Do you have any questions?<br />\nAsk your questions in the comments below, and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13879 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Plotting+the+Training+and+Validation+Loss+Curves+for+the+Transformer+Model&url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Plotting+the+Training+and+Validation+Loss+Curves+for+the+Transformer+Model&url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/\">Plotting the Training and Validation Loss Curves for the Transformer Model</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "wfw:commentRss": "https://machinelearningmastery.com/plotting-the-training-and-validation-loss-curves-for-the-transformer-model/feed/",
  "slash:comments": 1
}