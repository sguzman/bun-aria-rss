{
  "title": "Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01568",
  "description": "<p>Large language models are now part of a powerful new paradigm in machine\nlearning. These models learn a wide range of capabilities from training on\nlarge unsupervised text corpora. In many applications, these capabilities are\nthen fine-tuned through additional training on specialized data to improve\nperformance in that setting. In this paper, we augment these models with an\nepinet: a small additional network architecture that helps to estimate model\nuncertainty and form an epistemic neural network (ENN). ENNs are neural\nnetworks that can know what they don't know. We show that, using an epinet to\nprioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same\nperformance while using 2x less data. We also investigate performance in\nsynthetic neural network generative models designed to build understanding. In\neach setting, using an epinet outperforms heuristic active learning schemes.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\">Ian Osband</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghari_S/0/1/0/all/0/1\">Seyed Mohammad Asghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1\">Benjamin Van Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1\">John Aslanides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>"
}