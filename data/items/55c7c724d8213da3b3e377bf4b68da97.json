{
  "title": "Becoming a Bayesian, Part 2",
  "link": "",
  "published": "2015-05-02T18:30:00+01:00",
  "updated": "2015-05-02T18:30:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-05-02:/sebastian/blog/becoming-a-bayesian-part-2.html",
  "summary": "<p>This post continues the previous post, <a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">part 1</a>,\noutlining my criticism towards a ''naive'' subjective Bayesian viewpoint:</p>\n<ol>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">The consequences of model misspecification</a>.</li>\n<li>The ''model first computation last'' approach, in this post.</li>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-3.html\">Denial of methods of classical statistics</a>.</li>\n</ol>\n<h2>The ''Model First â€¦</h2>",
  "content": "<p>This post continues the previous post, <a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">part 1</a>,\noutlining my criticism towards a ''naive'' subjective Bayesian viewpoint:</p>\n<ol>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">The consequences of model misspecification</a>.</li>\n<li>The ''model first computation last'' approach, in this post.</li>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-3.html\">Denial of methods of classical statistics</a>.</li>\n</ol>\n<h2>The ''Model First Computation Last'' approach</h2>\n<p>Without a model (not necessarily probabilistic) we cannot learn anything.\nThis is true for science, but it is also true for any machine learning system.\nThe model may be very general and make only a few general assumptions (e.g.\n''the physical laws remain constant over time and space''),\nor it may be highly specific (e.g. ''<span class=\"math\">\\(X \\sim \\mathcal{N}(\\mu,1)\\)</span>''),\nbut we need a model in order to relate observations to quantities of interest.</p>\n<p>But in contrast to science, when we build machine learning systems we are also\nengineers.  We build models not in isolation or on a piece of whiteboard, but\ninstead we build them to run on our current technology.</p>\n<p>Many <em>Bayesians</em> adhere to a strict separation of model and inference\nprocedure; that is, the model is independent of any inference procedure.\nThey argue convincingly that the goal of inference is to approximate the\nposterior under the assumed model, and that for each model there exist a large\nvariety of possible approximate inference methods that can be applied, such as\nMarkov chain Monte Carlo (MCMC), importance sampling, mean field, belief\npropagation, etc.\nBy selecting a suitable inference procedure, different accuracy and runtime\ntrade-offs can be realized.\nIn this viewpoint, the <em>model comes first and computation comes last</em>, once the\nmodel is in place.</p>\n<p>In practice this beautiful story does not play out very often.\nWhat is more common is that instead of spending time building and refining a\nmodel, time is spent on tuning the parameters of inference procedures, such\nas:</p>\n<ul>\n<li><em>MCMC</em>: Markov kernel, diagnostics, burn-in, possible extensions (annealing,\n  parallel tempering ladder, HMC parameters, etc.);</li>\n<li>Importance sampling: selecting the proposal distribution, effective sample\n  size, possible extensions (e.g. multiple importance sampling);</li>\n<li>Mean field and belief propagation: message initialization, schedule, damping\n  factor, convergence criterion.</li>\n</ul>\n<p>In fact, it seems to me, that many works describing novel models ultimately\nalso describe inference procedures that are required to make their models\nwork.\nI say this not to diminish the tremendeous progress we as a community have\nmade in probabilistic inference; it is just an observation that the separation\nof model and inference is not plug-and-play in practice.\n(Other pragmatic reasons for deviating from the subjective Bayesian viewpoint\nare provided in a <a href=\"http://projecteuclid.org/euclid.ba/1340371036\">paper by\nGoldstein</a>.)</p>\n<p>Suppose we have a probabilistic model and we are provided an\napproximate inference procedure for it.\nLet us draw a big box around these two components and call this the\n<em>effective model</em>, that is, the system that takes observations and produces\nsome probabilistic output.\nHow similar is this effective model to the model on our whiteboard?\nI know of only very few results, for example <a href=\"http://papers.nips.cc/paper/4649-the-bethe-partition-function-of-log-supermodular-graphical-models\">Ruozzi's analysis of the Bethe\napproximation</a>.</p>\n<p>Another practial example along these lines was given to me by Andrew Wilson is\nto compare an analytically tractable model such as a Gaussian process against\na richer but intractable model such as a Gaussian process with Student-T\nnoise.  The latter model is certainly more capable formally but requires\napproximate inference.\nIn this case, the approximate inference implicitly changes the model and it is\nnot clear at all whether it is it worth to give up analytic tractability.</p>\n<h3>Resource-Constrained Reasoning</h3>\n<p>It seems that when compared to machine learning, the field of artificial\nintelligence is somewhat ahead; in 1987 <a href=\"http://research.microsoft.com/en-us/um/people/horvitz/\">Eric\nHorvitz</a> had a nice\npaper at UAI on <a href=\"http://arxiv.org/abs/1304.2759\">reasoning and decision making under limited\nresources</a>.  When read liberally the problem\nof adhering to the orthodox (normative) view he described in 1987 seems to\nmirror the current issues faced by large scale probabilistic models used in\nmachine learning, namely that exact analysis in any but the simplest models is\nintractable and resource constraints are not made explicit in the model or the\ninference procedures.</p>\n<p>But some recent work is giving me new hopes that we will treat computation as\na first class citizen when building our models, here is some of that work from\nthe computer vision and natural language processing community:</p>\n<ul>\n<li><a href=\"http://stat.fsu.edu/~abarbu/\">Adrian Barbu's</a> <a href=\"http://stat.fsu.edu/~abarbu/papers/denoise_tip.pdf\">active random\nfields</a> from 2009, where\nhe explicitly considers the effects of using suboptimal inference procedure in\ngraphical models.</li>\n<li>Stoyanov, Ropson, and Eisner's work on <a href=\"http://www.cs.jhu.edu/~jason/papers/stoyanov+al.aistats11.pdf\">predicting with approximate\ninference\nprocedures</a> at\nAISTATS 2011; although this is an empirical risk minimization approach.</li>\n<li><a href=\"http://users.cecs.anu.edu.au/~jdomke/\">Justin Domke's</a> work on <a href=\"http://users.cecs.anu.edu.au/~jdomke/papers/2013pami.pdf\">unrolling\napproximate inference\nprocedures</a> and\ntraining the resulting models end-to-end using backpropagation.</li>\n</ul>\n<p>Cheng Soon Ong pointed me to <a href=\"https://www.aaai.org/Papers/Workshops/2002/WS-02-15/WS02-15-002.pdf\">work on anytime probabilistic\ninference</a>,\nwhich I am not familiar with, but the goal of having inference algorithms\nwhich adapt to the available resources is certainly desirable.  The <a href=\"http://en.wikipedia.org/wiki/Anytime_algorithm\">anytime\nsetting</a> is practically\nrelevant in many applications, particular in the real-time systems.</p>\n<p>All these works share the characteristic that they take a probabilistic model\nand approximate inference procedure and construct a new \"effective\nmodel\" by entangling the model and inference.\nBy doing so the resulting model is tractable by construction and retains to a\nlarge extent the specification of the original intractable model.\nHowever, the separation between model and inference procedure is lost.</p>\n<p>This is the first step towards a <em>computation first approach</em>, and I believe\nwe will see more machine learning works which recognize available\ncomputational primitives and resources as equally important to the model\nspecification itself.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://www.jancsary.net/\">Jeremy Jancsary</a>,\n<a href=\"http://files.is.tue.mpg.de/pgehler/\">Peter Gehler</a>,\n<a href=\"http://pub.ist.ac.at/~chl/\">Christoph Lampert</a>,\n<a href=\"http://www.cs.cmu.edu/~andrewgw/\">Andrew Wilson</a>, and\n<a href=\"http://www.ong-home.my/\">Cheng Soon-Ong</a> for feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}