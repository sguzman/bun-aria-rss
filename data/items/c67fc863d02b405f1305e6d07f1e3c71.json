{
  "title": "LDAOverflow with Online LDA",
  "link": "https://wellecks.wordpress.com/2014/10/26/ldaoverflow-with-online-lda/",
  "comments": "https://wellecks.wordpress.com/2014/10/26/ldaoverflow-with-online-lda/#respond",
  "dc:creator": "wellecks",
  "pubDate": "Sun, 26 Oct 2014 01:45:48 +0000",
  "category": [
    "artificial intelligence",
    "machine learning",
    "technology",
    "Latent Dirichlet Allocation",
    "Machine learning",
    "Online LDA",
    "topic modeling"
  ],
  "guid": "http://wellecks.wordpress.com/?p=847",
  "description": "In the past two posts (part I and part II), we used Latent Dirichlet Allocation (LDA) to discover topics for tweets, and visualized them. In this post, we&#8217;ll investigate using LDA on an 8gb dataset of around 8 million Stack Overflow posts. We&#8217;ll need to take a different approach; for the tweets we used a batch algorithm that worked [&#8230;]",
  "content:encoded": "<p>In the past two posts (<a href=\"https://wellecks.wordpress.com/2014/09/04/these-are-your-tweets-on-lda-part-i/\">part I</a> and <a href=\"https://wellecks.wordpress.com/2014/09/06/these-are-your-tweets-on-lda-part-ii/\">part II</a>), we used <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Latent Dirichlet Allocation</a> (LDA) to discover topics for tweets, and visualized them. In this post, we&#8217;ll investigate using LDA on an 8gb dataset of around 8 million <a href=\"http://stackoverflow.com/\">Stack Overflow</a> posts.</p>\n<p>We&#8217;ll need to take a different approach; for the tweets we used a batch algorithm that worked well for the relatively small dataset of around 5000 tweets, but would likely introduce performance issues when running on massive datasets. The batch algorithm also assumed that we have the entire training set at the start of training, making the approach unviable for streaming data, which we may receive <em>during</em> training. It&#8217;d be nice to train the model incrementally, so that we can train on a chunk of data, then resume training if we receive more data <em>without</em> have to retrain on the original chunk.</p>\n<p>In this post, we&#8217;ll look at <a href=\"https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf\">Online LDA</a>, a variation of &#8216;vanilla&#8217; LDA that can be trained incrementally in small batches. Online LDA is a good choice for large datasets since we only need to hold a very small subset of the dataset in memory at a given time, and a good fit for streaming data since we can continually feed in new data batches as we receive them. We&#8217;re also able to save the model state at a point in training, then resume later when we want to train on more data.</p>\n<p>First, we&#8217;ll jump into the math and look at the differences between online and batch LDA. Then we&#8217;ll use a python implementation of online LDA to discover topics for the Stack Overflow dataset. As usual, all of the associated code is <a href=\"https://github.com/wellecks/online_lda_python\">available on GitHub</a>.</p>\n<p><strong>Variations on Variational Bayes</strong></p>\n<p>For brevity this part will assume that you&#8217;ve read through the math background in the <a href=\"https://wellecks.wordpress.com/2014/09/04/these-are-your-tweets-on-lda-part-i/\">first LDA post</a>. I&#8217;ll also only highlight major parts; for the full story check out <a href=\"https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf\">Hoffman&#8217;s online LDA paper</a>.</p>\n<p>In LDA, our ultimate goal is to find the posterior distribution of latent topic variables after observing training data. However, computing this distribution is intractable, so we&#8217;re forced to approximate. One approximation approach is to use an optimization method called <a href=\"http://en.wikipedia.org/wiki/Variational_Bayesian_methods\">Variational Bayes</a>.</p>\n<p>In short, we approximate the true distribution by a simple distribution <img src=\"https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"q\" class=\"latex\" />, and associate parameters <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi,&#92; &#92;gamma,&#92; &#92;lambda\" class=\"latex\" /> with the original parameters <img src=\"https://s0.wp.com/latex.php?latex=z%2C%5C+%5Ctheta%2C%5C+%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z%2C%5C+%5Ctheta%2C%5C+%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z%2C%5C+%5Ctheta%2C%5C+%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"z,&#92; &#92;theta,&#92; &#92;beta\" class=\"latex\" /> respectively. Recall that <img src=\"https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"z\" class=\"latex\" /> gives the topic assignments for each word in each document , <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;theta\" class=\"latex\" /> gives the topic composition of each document, and <img src=\"https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;beta\" class=\"latex\" /> gives the word-topic probabilities for each word and each topic.</p>\n<p>Specifically, we have:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=q%28z_%7Bdi%7D%3Dk%29+%3D+%5Cphi_%7Bdw_%7Bdi%7Dk%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=q%28z_%7Bdi%7D%3Dk%29+%3D+%5Cphi_%7Bdw_%7Bdi%7Dk%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q%28z_%7Bdi%7D%3Dk%29+%3D+%5Cphi_%7Bdw_%7Bdi%7Dk%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"q(z_{di}=k) = &#92;phi_{dw_{di}k}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=q%28%5Ctheta_%7Bd%7D%29+%3D+dirichlet%28%5Ctheta_%7Bd%7D%3B+%5Cgamma_%7Bd%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=q%28%5Ctheta_%7Bd%7D%29+%3D+dirichlet%28%5Ctheta_%7Bd%7D%3B+%5Cgamma_%7Bd%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q%28%5Ctheta_%7Bd%7D%29+%3D+dirichlet%28%5Ctheta_%7Bd%7D%3B+%5Cgamma_%7Bd%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"q(&#92;theta_{d}) = dirichlet(&#92;theta_{d}; &#92;gamma_{d})\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=q%28%5Cbeta_%7Bk%7D%29+%3D+dirichlet%28%5Cbeta_%7Bk%7D%3B+%5Clambda_%7Bk%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=q%28%5Cbeta_%7Bk%7D%29+%3D+dirichlet%28%5Cbeta_%7Bk%7D%3B+%5Clambda_%7Bk%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=q%28%5Cbeta_%7Bk%7D%29+%3D+dirichlet%28%5Cbeta_%7Bk%7D%3B+%5Clambda_%7Bk%7D%29&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"q(&#92;beta_{k}) = dirichlet(&#92;beta_{k}; &#92;lambda_{k})\" class=\"latex\" /></p>\n<p style=\"text-align:left;\">Our goal is to estimate <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma%2C%5C+%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi,&#92; &#92;gamma,&#92; &#92;lambda\" class=\"latex\" />. In both batch and online LDA, we alternate between two steps:</p>\n<p style=\"text-align:left;padding-left:30px;\">1. <em>E-Step</em>: Estimate <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi%2C%5C+%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi,&#92; &#92;gamma\" class=\"latex\" /> using the current value of <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /></p>\n<p style=\"text-align:left;padding-left:30px;\">2. <em>M-Step</em>: Update <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> , using the current value of <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi\" class=\"latex\" /></p>\n<p>The core difference between batch and online LDA is in how these steps are carried out at the algorithmic level.</p>\n<p><b>Starting with Batch</b></p>\n<p>In batch Variational Bayes, we perform multiple passes over the <em>entire</em> dataset, checking each time for convergence. During each pass, the algorithm does an E-Step using the <em>entire</em> dataset. At a high level:</p>\n<pre><span style=\"text-decoration:underline;\"><b>E-Step</b></span>\nfor d = 1 to numDocs\n    initialize <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" />\n    repeat until change in <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi < &#92;epsilon\" class=\"latex\" />\n        update <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi\" class=\"latex\" />\n        update <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" /></pre>\n<p>Then the M-Step updates <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> using <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi\" class=\"latex\" /> values from <em>every</em> document:</p>\n<pre><span style=\"text-decoration:underline;\"><strong>M-Step\n</strong></span>update <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /></pre>\n<p>The specific updates are:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Bd%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Bd%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Bd%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi_{d,word_{i},k} &#92;propto e^{E_{q}(log &#92;theta_{d,k})+E_{q}(log&#92;beta_{k,word_{i}})}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bd%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7Dn_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bd%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7Dn_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bd%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7Dn_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma_{d,k} = &#92;alpha + &#92;sum_{word_{i}}&#92;phi_{d,word_{i},k}n_{d,word_{i}}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2B%5Csum_%7Bd%7Dn_%7Bd%2Cword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2B%5Csum_%7Bd%7Dn_%7Bd%2Cword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2B%5Csum_%7Bd%7Dn_%7Bd%2Cword_%7Bi%7D%7D%5Cphi_%7Bd%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{k,word_{i}}=&#92;eta +&#92;sum_{d}n_{d,word_{i}}&#92;phi_{d,word_{i},k}\" class=\"latex\" /></p>\n<p style=\"text-align:left;\">Where <img src=\"https://s0.wp.com/latex.php?latex=n_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=n_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n_%7Bd%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"n_{d,word_{i}}\" class=\"latex\" /> is the number of occurrences of <img src=\"https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"word_{i}\" class=\"latex\" /> in document <em>d</em>.</p>\n<p style=\"text-align:left;\"><strong>Going Online</strong></p>\n<p style=\"text-align:left;\">In online Variational Bayes, we only make a single sweep of the entire dataset, analyzing a chunk of documents at a time. A &#8216;chunk&#8217; could be a single document, 42 documents, or even the entire dataset. Let&#8217;s let a &#8216;chunk&#8217; be 1000 documents.</p>\n<p style=\"text-align:left;\">The online E-Step only uses the current chunk; instead of 8 million posts we now only have to hold 1000 in memory. The E-Step finds locally optimal values for <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma:\" class=\"latex\" /></p>\n<pre><span style=\"text-decoration:underline;\"><b>E-Step</b></span>\ninitialize <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" />\nrepeat until change in <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi+%3C+%5Cepsilon&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi < &#92;epsilon\" class=\"latex\" />\n    update <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi\" class=\"latex\" />\n    update <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" /></pre>\n<p>In the M-Step, we first compute <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda&#039;\" class=\"latex\" />, which is the value of <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> if we imagined that the entire dataset is made up of <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7BnumDocs%7D%7BchunkSize%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7BnumDocs%7D%7BchunkSize%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7BnumDocs%7D%7BchunkSize%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{numDocs}{chunkSize}\" class=\"latex\" /> copies of the current chunk. Then <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> is updated using a weighted sum of <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda&#039;\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%3A&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda:\" class=\"latex\" /></p>\n<pre><span style=\"text-decoration:underline;\"><strong>M-Step\n</strong></span>compute <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda&#039;\" class=\"latex\" />\nupdate <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /></pre>\n<p>The specific updates are:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Biter%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Biter%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D+%5Cpropto+e%5E%7BE_%7Bq%7D%28log+%5Ctheta_%7Biter%2Ck%7D%29%2BE_%7Bq%7D%28log%5Cbeta_%7Bk%2Cword_%7Bi%7D%7D%29%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi_{iter,word_{i},k} &#92;propto e^{E_{q}(log &#92;theta_{iter,k})+E_{q}(log&#92;beta_{k,word_{i}})}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma_%7Biter%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7Dn_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma_%7Biter%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7Dn_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma_%7Biter%2Ck%7D+%3D+%5Calpha+%2B+%5Csum_%7Bword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7Dn_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma_{iter,k} = &#92;alpha + &#92;sum_{word_{i}}&#92;phi_{iter,word_{i},k}n_{iter,word_{i}}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%27_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2BbatchSize%2An_%7Biter%2Cword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%27_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2BbatchSize%2An_%7Biter%2Cword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%27_%7Bk%2Cword_%7Bi%7D%7D%3D%5Ceta+%2BbatchSize%2An_%7Biter%2Cword_%7Bi%7D%7D%5Cphi_%7Biter%2Cword_%7Bi%7D%2Ck%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda&#039;_{k,word_{i}}=&#92;eta +batchSize*n_{iter,word_{i}}&#92;phi_{iter,word_{i},k}\" class=\"latex\" /></p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Clambda+%3D+%281-%5Crho_t%29%5Clambda%2B%5Crho_t%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda+%3D+%281-%5Crho_t%29%5Clambda%2B%5Crho_t%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda+%3D+%281-%5Crho_t%29%5Clambda%2B%5Crho_t%5Clambda%27&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda = (1-&#92;rho_t)&#92;lambda+&#92;rho_t&#92;lambda&#039;\" class=\"latex\" /></p>\n<p style=\"text-align:left;\">Where <img src=\"https://s0.wp.com/latex.php?latex=n_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=n_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n_%7Biter%2Cword_%7Bi%7D%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"n_{iter,word_{i}}\" class=\"latex\" /> is the number of occurrences of <img src=\"https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=word_%7Bi%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"word_{i}\" class=\"latex\" /> in the current iteration&#8217;s chunk of documents, and <img src=\"https://s0.wp.com/latex.php?latex=%5Crho_t+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho_t+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho_t+&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho_t \" class=\"latex\" /> is a weighting parameter.</p>\n<p style=\"text-align:left;\">We can see that unlike batch LDA, in online LDA we only need to hold a small chunk of the data at a time, and once we&#8217;re done analyzing it, we never need it again. As with batch, once we&#8217;ve estimated <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" />, we can find the most probable words for each topic by looking at the word probabilities in each row of <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" />.</p>\n<p style=\"text-align:left;\"><strong>Intuitions of the Inference</strong></p>\n<p style=\"text-align:left;\">If we squint and step back, LDA consists of using simple word counts in a clever way. The two parameters we ultimately care about are <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" />. How do these get updated during training?</p>\n<p style=\"text-align:left;\">Updates of <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" /> (the topic compositions for each document) are the prior <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> plus a weighted sum of word counts. The word counts are weighted by <img src=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Bword%2Ctopic%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cphi_%7Bword%2Ctopic%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cphi_%7Bword%2Ctopic%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;phi_{word,topic}\" class=\"latex\" />, the probability of assigning the word to the topic. Intuitively, if we count a lot of instances of &#8220;potato&#8221; in a document, and &#8220;potato&#8221; is likely to be assigned to topic 2, then it makes sense that the document has more of topic 2 in it than we previously thought.</p>\n<p style=\"text-align:left;\">Updates of <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Btopic%2Cword%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Btopic%2Cword%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Btopic%2Cword%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{topic,word}\" class=\"latex\" /> (the word-topic probabilities) use word counts weighted by the probability that the word will be assigned to the given topic. If &#8220;potato&#8221; shows up a lot in the dataset is likely to be assigned to topic 2, then it makes sense that <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7B2%2C+potato%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7B2%2C+potato%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7B2%2C+potato%7D&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{2, potato}\" class=\"latex\" /> should increase.</p>\n<p style=\"text-align:left;\"><strong>LDA Overflow</strong></p>\n<p style=\"text-align:left;\">Now it&#8217;s time to run Online LDA on the Stack Overflow dataset to discover topics without overflowing our memory. Stack Exchange kindly provides (and updates) <a href=\"https://archive.org/details/stackexchange\">a data dump of all of its user generated content</a>; I chose the <em>stackoverflow.com-Posts.7z</em> dataset.</p>\n<p style=\"text-align:left;\"><strong>Read, Clean, Parse, Repeat</strong></p>\n<p style=\"text-align:left;\">The data arrives as a 27gb XML behemoth. The first step is isolating the text from the <em>Title</em> and <em>Body</em> fields for each row. These fields will comprise a &#8216;document&#8217;, and our dataset will be formatted as a text file with one document per line.</p>\n<p style=\"text-align:left;\">Since the file is so large, we need to incrementally read the XML. We also filter out non alpha-numeric characters. Details for this process can be found in <a href=\"https://github.com/wellecks/online_lda_python/blob/master/xml_parse.py\">xml_parse.py</a>.</p>\n<p style=\"text-align:left;\">Once <a href=\"https://github.com/wellecks/online_lda_python/blob/master/xml_parse.py\">xml_parse.py</a> runs, we get an 8gb text file containing around 8,000,000 stack overflow documents (title and body content). A couple examples:</p>\n<pre style=\"text-align:left;\">Throw an error in a MySQL trigger If I have a trigger before the update on a table how can I throw an error that prevents the update on that table\n\nCompressing Decompressing Folders Files Does anyone know of a good way to compress or decompress files and folders in C quickly Handling large files might be necessary</pre>\n<p style=\"text-align:left;\"><strong>LDA by Hoffman</strong></p>\n<p style=\"text-align:left;\">We&#8217;ll use a Python implementation of online LDA written by Matt Hoffman, available on <a href=\"http://www.cs.princeton.edu/~mdhoffma/\">his webpage</a>. We need to adapt the high-level running script for our application; to do so I created a wrapper for running LDA called <a href=\"https://github.com/wellecks/online_lda_python/blob/master/online_lda.py\">online_lda.py</a>. Use</p>\n<pre style=\"text-align:left;\">python online_lda.py -h</pre>\n<p style=\"text-align:left;\">to see the various command line arguments.</p>\n<p style=\"text-align:left;\">I&#8217;ve also added more comments to <a href=\"https://github.com/wellecks/online_lda_python/blob/master/onlineldavb.py\">onlineldavb.py</a> on the repo in case you&#8217;d like to further inspect how the actual Online LDA algorithm is implemented.</p>\n<p style=\"text-align:left;\"><strong>Building a Vocabulary</strong></p>\n<p style=\"text-align:left;\">The LDA implementation assumes that we have a vocabulary file prior to training so that it can compactly represent documents as numeric word IDs. The vocabulary also allows us the algorithm to associate an index of <img src=\"https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;beta\" class=\"latex\" /> with a word ID and hence with a word.</p>\n<p style=\"text-align:left;\">We can generate a domain-specific vocabulary using the first 100,000 Stack Overflow posts, and supplement it with the vocabulary provided by Hoffman, which contains the most frequent English words. <a href=\"http://radimrehurek.com/gensim/\">Gensim</a> has a nice library for creating vocabularies. We filter out words that appear in fewer than 10 documents, since they are often &#8216;junk&#8217; words, and would probably not appear in the top words for a topic anyways since they appear so infrequently. Code for the vocabulary generation is found in <a href=\"https://github.com/wellecks/online_lda_python/blob/master/dictionary.py\">dictionary.py</a>.</p>\n<p style=\"text-align:left;\"><strong>Running</strong></p>\n<p style=\"text-align:left;\">Let&#8217;s kick it off!</p>\n<pre style=\"text-align:left;\">python online_lda.py dataset.txt vocabulary.txt</pre>\n<p style=\"text-align:left;\">The training took ~12 hours for a 100 topic model on my MacBook. The values of <img src=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cgamma&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;gamma\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> are output to files every 10,000 iterations and when the training completes. We can then use one of the <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" /> files to see the top 20 words for the top N topics. For instance, to print the top 2 topics with the final model, use:</p>\n<pre style=\"text-align:left;\">python printtopics.py vocabulary.txt lambda-final.dat 2</pre>\n<p style=\"text-align:left;\">giving:</p>\n<pre style=\"text-align:left;\"><span style=\"text-decoration:underline;\"><strong>topic 0</strong></span>\nsuspended:0.8356\nauthorization:0.0215\nentityset:0.0128\ntreemap:0.0094\nprofessionals:0.0086\nbest:0.0084\nfacts:0.0072\nspecial:0.0062\nsyntax:0.0056\nlisting:0.0051\nforwarding:0.0049\nwebparts:0.0047\nduration:0.0045\nvalued:0.0039\nhalts:0.0038\nbaggage:0.0034\nyeah:0.0034\nltaspdropdownlistgt:0.0033\ntwitter:0.0031\nliable:0.0030\n\n<strong><span style=\"text-decoration:underline;\">topic 1</span></strong>\nsupport:0.7800\nquarter:0.0380\nfig:0.0278\nluck:0.0160\n1gb:0.0142\nfuneral:0.0124\nvisiting:0.0109\nxiv:0.0071\nscreen:0.0063\ncommons:0.0046\nmonster:0.0040\nflash:0.0039\nfaculty:0.0037\ndesire:0.0031\ndetached:0.0030\nhandler:0.0028\nsay:0.0028\neveryday:0.0025\ndarker:0.0025\nscreen:0.0024</pre>\n<p style=\"text-align:left;\">The numbers are the word-topic probabilities from <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda&#038;bg=ffffff&#038;fg=404040&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda\" class=\"latex\" />.</p>\n<p style=\"text-align:left;\">We&#8217;ll use the approach from the first LDA post to create word clouds for two different topics:</p>\n<p style=\"text-align:left;\"><img loading=\"lazy\" data-attachment-id=\"873\" data-permalink=\"https://wellecks.wordpress.com/2014/10/26/ldaoverflow-with-online-lda/topic_cloud2/#main\" data-orig-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png\" data-orig-size=\"1656,828\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"topic_cloud2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=300\" data-large-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=525\" class=\"aligncenter size-large wp-image-873\" src=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=525&#038;h=262\" alt=\"topic_cloud2\" width=\"525\" height=\"262\" srcset=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=525&h=262 525w, https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=1048&h=524 1048w, https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=150&h=75 150w, https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=300&h=150 300w, https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=768&h=384 768w, https://wellecks.files.wordpress.com/2014/10/topic_cloud2.png?w=1024&h=512 1024w\" sizes=\"(max-width: 525px) 100vw, 525px\" /></p>\n<p style=\"text-align:left;\"><a href=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png\"><img loading=\"lazy\" data-attachment-id=\"872\" data-permalink=\"https://wellecks.wordpress.com/2014/10/26/ldaoverflow-with-online-lda/topic_cloud1/#main\" data-orig-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png\" data-orig-size=\"1466,1100\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"topic_cloud1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=300\" data-large-file=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=525\" class=\"aligncenter size-large wp-image-872\" src=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=525&#038;h=393\" alt=\"topic_cloud1\" width=\"525\" height=\"393\" srcset=\"https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=525&h=393 525w, https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=1048&h=786 1048w, https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=150&h=113 150w, https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=300&h=225 300w, https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=768&h=576 768w, https://wellecks.files.wordpress.com/2014/10/topic_cloud1.png?w=1024&h=768 1024w\" sizes=\"(max-width: 525px) 100vw, 525px\" /></a><strong>Conclusion</strong></p>\n<p style=\"text-align:left;\">We managed to find topics on a dataset of 8 million Stack Overflow posts by using Online LDA. Feel free to download the code and try it out on other datasets!</p>\n<p style=\"text-align:left;\"><strong>Credits & Links</strong></p>\n<p style=\"text-align:left;\">Much of the material is derived from <a href=\"https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf\">Hoffman&#8217;s paper</a> and his online LDA implementation. <a href=\"http://radimrehurek.com/gensim/models/ldamodel.html\">Gensim</a> and <a href=\"https://github.com/JohnLangford/vowpal_wabbit/wiki/Latent-Dirichlet-Allocation\">Vowpal Wabbit</a> also have implementations of online LDA.</p>\n",
  "wfw:commentRss": "https://wellecks.wordpress.com/2014/10/26/ldaoverflow-with-online-lda/feed/",
  "slash:comments": 0,
  "media:content": [
    {
      "media:title": "wellecks"
    },
    {
      "media:title": "topic_cloud2"
    },
    {
      "media:title": "topic_cloud1"
    }
  ]
}