{
  "title": "Caching",
  "link": "",
  "updated": "2015-08-03T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/08/03/Caching",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr: Caching improves performance under repetitive workloads.  Traditional\nLRU policies don’t fit data science well.  We propose a new caching policy.</strong></p>\n\n<h2 id=\"humans-repeat-stuff\">Humans Repeat Stuff</h2>\n\n<p>Consider the dataset that you’ve worked on most recently.  How many times have\nyou loaded it from disk into memory?  How many times have you repeated almost\nthe same computations on that data?</p>\n\n<p>Exploratory data science workloads involve repetition of very similar\ncomputations.  These computations share structure.  By caching frequently used\nresults (or parts of results) we may be able to considerably speed up\nexploratory data analysis.</p>\n\n<h2 id=\"caching-in-other-contexts\">Caching in other contexts</h2>\n\n<p>The web community loves caching.  Database backed web applications almost\nalways guard their database lookups with a system like <code class=\"language-plaintext highlighter-rouge\">memcached</code> which\ndevotes some amount of memory to caching frequent and recent queries.\nBecause humans visit mostly the same pages over and over again this can reduce\ndatabase load by an order of magnitude.  Even if humans visit different pages\nwith different inputs these pages often share many elements.</p>\n\n<h2 id=\"limited-caching-resources\">Limited caching resources</h2>\n\n<p>Given infinite memory we would cache every result that we’ve ever\ncomputed.  This would give us for instant recall on anything that wasn’t novel.\nSadly our memory resources are finite and so we evict cached results\nthat don’t seem to be worth keeping around.</p>\n\n<p>Traditionally we use a policy like Least Recently Used (LRU).  This policy\nevicts results that have not been requested for a long time.  This is cheap and\nworks well for web and systems applications.</p>\n\n<h2 id=\"lru-doesnt-fit-analytic-workloads\">LRU doesn’t fit analytic workloads</h2>\n\n<p>Unfortunately LRU doesn’t fit analytic workloads well.  Analytic workloads have\na large spread computation times and of storage costs.  While most web\napplication database queries take roughly the same amount of time\n(100ms-1000ms) and take up roughly the same amount of space to store (1-10kb),\nthe computation and storage costs of analytic computations can easily vary by\nmany orders of magnitude (spreads in the millions or billions are common.)</p>\n\n<p>Consider the following two common computations of a large NumPy array:</p>\n\n<ol>\n  <li><code class=\"language-plaintext highlighter-rouge\">x.std()  # costly to recompute, cheap to store</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">x.T      # cheap to recompute, costly to store</code></li>\n</ol>\n\n<p>In the first case, <code class=\"language-plaintext highlighter-rouge\">x.std()</code>, this might take a second on a large array\n(somewhat expensive) but takes only a few bytes to store.  This result is so\ncheap to store that we’re happy to keep it in our cache for a long time, even\nif its infrequently requested.</p>\n\n<p>In the second case, <code class=\"language-plaintext highlighter-rouge\">x.T</code> this is cheap to compute (just a metadata change in\nthe array) and executes in microseconds.  However the result might take\ngigabytes of memory to store.  We don’t want to keep this in our cache, even if\nit’s very frequently requested; it takes up all of the space for other\npotentially more useful (and smaller) results and we can recompute it trivially\nanyway.</p>\n\n<p>So we want to keep cached results that have the following properties:</p>\n\n<ol>\n  <li>Costly to recompute (in seconds)</li>\n  <li>Cheap to store (in bytes)</li>\n  <li>Frequently used</li>\n  <li>Recently used</li>\n</ol>\n\n<h2 id=\"proposed-caching-policy\">Proposed Caching Policy</h2>\n\n<p>Here is an alternative to LRU that respects the objectives stated above.</p>\n\n<p>Every time someone accesses an entry in our cache, we increment the score\nassociated to the entry with the following value</p>\n\n\\[\\textrm{score} = \\textrm{score} + \\frac{\\textrm{compute time}}{\\textrm{nbytes}} (1 + \\epsilon)^{t}\\]\n\n<p>Where <code class=\"language-plaintext highlighter-rouge\">compute time</code> is the time it took to compute the result in the first\nplace, <code class=\"language-plaintext highlighter-rouge\">nbytes</code> is the number of bytes that it takes to store the result,\n<code class=\"language-plaintext highlighter-rouge\">epsilon</code> is a small number that determines the halflife of what “recently”\nmeans, and <code class=\"language-plaintext highlighter-rouge\">t</code> is an auto-incrementing tick time increased at every access.</p>\n\n<p>This has units of inverse bandwidth (s/byte), gives more importance to new\nresults with a slowly growing exponential growth, and amplifies the score of\nfrequently requested results in a roughly linear fashion.</p>\n\n<p>We maintain these scores in a heap, keep track of the total number of bytes,\nand cull the cache as necessary to keep storage costs beneath a fixed budget.\nUpdates cost <code class=\"language-plaintext highlighter-rouge\">O(log(k))</code> for <code class=\"language-plaintext highlighter-rouge\">k</code> the number of elements in the cache.</p>\n\n<h2 id=\"cachey\">Cachey</h2>\n\n<p>I wrote this up into a tiny library called <code class=\"language-plaintext highlighter-rouge\">cachey</code>.  This is experimental\ncode and subject to wild API changes (including renaming.)</p>\n\n<p>The central object is a <code class=\"language-plaintext highlighter-rouge\">Cache</code> that includes asks for the following:</p>\n\n<ol>\n  <li>Number of available bytes to devote to the cache</li>\n  <li>Halflife on importance (the number of access that occur to reduce the\nimportance of a cached result by half) (default 1000)</li>\n  <li>A lower limit on costs to consider entry to the cache (default 0)</li>\n</ol>\n\n<h2 id=\"example\">Example</h2>\n\n<p>So here is the tiny example</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">cachey</span> <span class=\"kn\">import</span> <span class=\"n\">Cache</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">Cache</span><span class=\"p\">(</span><span class=\"n\">available_bytes</span><span class=\"o\">=</span><span class=\"mf\">1e9</span><span class=\"p\">)</span>  <span class=\"c1\"># 1 GB\n</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">put</span><span class=\"p\">(</span><span class=\"s\">'Hello'</span><span class=\"p\">,</span> <span class=\"s\">'world!'</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">'Hello'</span><span class=\"p\">)</span>\n<span class=\"s\">'world!'</span></code></pre>\n</figure>\n\n<h2 id=\"more-interesting-example\">More interesting example</h2>\n\n<p>The cache includes a <code class=\"language-plaintext highlighter-rouge\">memoize</code> decorator.  Lets memoize <code class=\"language-plaintext highlighter-rouge\">pd.read_csv</code>.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">from</span> <span class=\"nn\">cachey</span> <span class=\"kn\">import</span> <span class=\"n\">Cache</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">Cache</span><span class=\"p\">(</span><span class=\"mf\">1e9</span><span class=\"p\">)</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"n\">read_csv</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">memoize</span><span class=\"p\">(</span><span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'accounts.csv'</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">262</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">27.7</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">290</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">303</span> <span class=\"n\">ms</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'accounts.csv'</span><span class=\"p\">)</span>  <span class=\"c1\"># second read is free\n</span><span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">77</span> <span class=\"n\">µs</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">16</span> <span class=\"n\">µs</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">93</span> <span class=\"n\">µs</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">93</span> <span class=\"n\">µs</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">total_bytes</span> <span class=\"o\">/</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">available_bytes</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"mf\">0.096</span></code></pre>\n</figure>\n\n<p>So we create a new function <code class=\"language-plaintext highlighter-rouge\">read_csv</code> that operates exactly like\n<code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code> except that it holds on to recent results in <code class=\"language-plaintext highlighter-rouge\">c</code>, a cache.\nThis particular CSV file created a dataframe that filled a tenth of our\ncache space.  The more often we request this CSV file the more its score will\ngrow and the more likely it is to remain in the cache into the future.  If\nother memoized functions using this same cache produce more valuable results\n(costly to compute, cheap to store) and we run out of space then this result\nwill be evicted and we’ll have to recompute our result if we ask for\n<code class=\"language-plaintext highlighter-rouge\">read_csv('accounts.csv')</code> again.</p>\n\n<h2 id=\"memoize-everything\">Memoize everything</h2>\n\n<p>Just memoizing <code class=\"language-plaintext highlighter-rouge\">read_csv</code> isn’t very interesting.  The <code class=\"language-plaintext highlighter-rouge\">pd.read_csv</code>\nfunction operates at a constant data bandwidth of around 100 MB/s.  The caching\npolicies around <code class=\"language-plaintext highlighter-rouge\">cachey</code> really shine when they get to see <em>all of our\ncomputations</em>.  For example it could be that we don’t want to hold on to the\nresults of <code class=\"language-plaintext highlighter-rouge\">read_csv</code> because these take up a lot of space.  If we find\nourselves doing the same <code class=\"language-plaintext highlighter-rouge\">groupby</code> computations then we might prefer to use our\ngigabyte of caching space to store these both because</p>\n\n<ol>\n  <li>groupby computations take a long time to compute</li>\n  <li>groupby results are often very compact in memory</li>\n</ol>\n\n<h2 id=\"cachey-and-dask\">Cachey and Dask</h2>\n\n<p>I’m slowly working on integrating cachey into dask’s shared memory scheduler.</p>\n\n<p>Dask is in a good position to apply <code class=\"language-plaintext highlighter-rouge\">cachey</code> to many computations.  It can look\nat every task in a task graph and consider the result for inclusion into the\ncache.  We don’t need to explicitly <code class=\"language-plaintext highlighter-rouge\">memoize</code> every function we want to use,\n<code class=\"language-plaintext highlighter-rouge\">dask</code> can do this for us on the fly.</p>\n\n<p>Additionally dask has a nice view of our computation as a collection of\nsub-tasks.  Similar computations (like mean and variance) often share\nsub-tasks.</p>\n\n<h2 id=\"future-work\">Future Work</h2>\n\n<p>Cachey is new and untested but potentially useful now, particularly through the\n<code class=\"language-plaintext highlighter-rouge\">memoize</code> method shown above.\nIt’s a <a href=\"http://github.com/mrocklin/cachey\">small and simple codebase</a>.\nI would love to hear if people find this kind of caching policy valuable.</p>\n\n<p>I plan to think about the following in the near future:</p>\n\n<ol>\n  <li>How do we build a hierarchy of caches to share between memory and disk?</li>\n  <li>How do we cleanly integrate cachey into dask\n(see <a href=\"https://github.com/ContinuumIO/dask/pull/502\">PR #502</a>)\nand how do we make dask amenable to caching\n(see <a href=\"https://github.com/ContinuumIO/dask/pull/510\">PR #510</a>)?</li>\n</ol>"
}