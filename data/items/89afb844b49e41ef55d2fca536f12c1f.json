{
  "title": "out of distribution detection using focal loss",
  "link": "http://matpalm.com/blog/ood_using_focal_loss",
  "category": [
    "objax",
    "jax",
    "projects"
  ],
  "guid": "http://matpalm.com/blog/ood_using_focal_loss",
  "description": "out of distribution detection using focal loss",
  "content:encoded": "<h1>out of distribution (OOD) detection</h1>\n<p>OOD detection is an often overlooked part of many projects.\n   it's super important to understand when your model is operating on\n   data it's not familiar with!\n</p>\n<p>though this can be framed as the problem of detecting that an input\n   differs from training data, discussed more in this\n   <a href=\"http://matpalm.com/blog/de_concerns_for_ml/\">'data engineering concerns for machine learning products' talk</a>,\n   for this post we're going to look at the\n   problem more from the angle of the model needing to be able to express it's\n   own lack of confidence.\n</p>\n<p>a core theme of this post is treating OOD as a function of\n   the output of the model. when i've played with this idea before the\n   best result i've had is from using entropy as a proxy of confidence.\n   in the past it's not be in the context of OOD directly but instead\n   as a way of prioritising data annotation (i.e. uncertainty sampling for active learning)\n</p>\n<p>using entropy as a stable measure of confidence though requires a model be\n   well calibrated.\n</p>\n\n<h1>neural net calibration</h1>\n<p>neural nets are sadly notorious for not being well calibrated under the default ways\n   we train these days.\n</p>\n<p>the first approach i was shown to calibrate a neural net is to train your model\n   as normal, then finetune <em>just</em> the final classifier layer using a held out set;\n   the general idea being that this is effectively just a logistic regression on\n   features learnt by the\n   rest of the network and so should come with the better guarantees that logistic\n   regression provides us regarding calibration\n   (see this <a href=\"https://scikit-learn.org/stable/modules/calibration.html\">great overview</a>\n   from sklearn)\n   whereas this has always worked well for me it does require two steps to training,\n   as well as the need for a seperate held out set to be managed.\n</p>\n<p>another apporach to calibration i revisited this week comes from\n   this great overview paper from 2017\n   <a href=\"https://arxiv.org/abs/1706.04599\">on calibration of modern neural networks</a>.\n   it touches on platt scaling, which i've also used in the past successfully, but includes\n   something even simpler i didn't really notice until it was pointed out to me by a\n   colleague; don't bother tuning the entire last layer, just fit a single temperature\n   rescaling of the output logits i.e. what can be thought of as a single parameter\n   version of platt scaling. what a great simple idea!\n</p>\n<p>the main purpose of this post though is to reproduce some ideas around out of distribution\n   and calibration when you use focal loss. this was described in this great piece of work\n   <a href=\"https://torrvision.github.io/focal_calibration/\">calibrating deep neural networks using focal loss</a>. we'll implement that, but the other two as well for comparison.\n</p>\n<p>( as an aside, another great simple way of determining confidence is using an ensemble!\n   i explore this idea a bit in my post on <a href=\"http://matpalm.com/blog/ensemble_nets/\">ensemble nets</a>\n   where i train ensembles as a single model using jax vmap )\n</p>\n\n<h1>experiments</h1>\n\n<h2>dataset</h2>\n<p>let's start with\n   <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">cifar10</a>\n   and split the data up a bit differently than the standard splits...\n</p>\n<ol>\n <li>\n     we'll union the standard cifar10 <code>train</code> and <code>test</code> splits.\n </li>\n\n <li>\n     hold out instances labelled <code>automobile</code> & <code>cat</code> as a \"hard\" <code>ood</code> set ( i've intentionally chosen these two since i know they cause model confusion as observed in this <a href=\"https://keras.io/examples/vision/metric_learning/\">keras.io tute on metric learning</a> ).\n </li>\n\n <li>\n     split the remaining 8 classes into 0.7 <code>train</code> and 0.1 for each of <code>validation</code>, <code>calibration</code> and <code>test</code>.\n </li>\n</ol>\n<p>we'll additionally generate an \"easy\" <code>ood</code> set which will just be random images.\n</p>\n<p>in terms of how we'll use these splits...\n</p>\n<ul>\n <li>\n     <code>train</code> will be the main dataset to train against.\n </li>\n\n <li>\n     <code>validation</code> will be used during training for learning rate stepping, early stopping, etc.\n </li>\n\n <li>\n     <code>calibration</code> will be used when we want to tune a model in some way for calibration.\n </li>\n\n <li>\n     <code>test</code> will be used for final held out analysis.\n </li>\n\n <li>\n     <code>ood_hard</code> and <code>ood_easy</code> will be used as the representative out of distribution sets.\n </li>\n</ul>\n<table class='data'>\n<tr><td colspan=\"2\">examples images</td></tr>\n<tr><td>in distribution</td>\n<td><img src=\"/blog/imgs/2020/ood/eg.in_distribution.png\" /></td></tr>\n<tr><td>out of distribution (hard)</td>\n<td><img src=\"/blog/imgs/2020/ood/eg.ood_hard.png\" /></td></tr>\n<tr><td>out of distribution (easy)</td>\n<td><img src=\"/blog/imgs/2020/ood/eg.ood_easy.png\" /></td></tr>\n</table>\n\n\n<h2>method overview</h2>\n<ol>\n <li>\n     train a base model, <code>model_1</code>, on <code>train</code>, tuning against <code>validate</code>.\n </li>\n\n <li>\n     create <code>model_2</code> by finetuning the last layer of <code>model_1</code> against <code>calibrate</code>; i.e. the logisitic regression approach to calibration.\n </li>\n\n <li>\n     create <code>model_3</code> by just fitting a scalar temperture of last layer of <code>model_1</code>; i.e. the temperture scaling approach.\n </li>\n\n <li>\n     train some focal loss models from scratch against <code>train</code> using <code>validate</code> for tuning.\n </li>\n</ol>\n<p>at each step we'll check the entropy distributions across the various splits, including the easy and hard\n   ood sets\n</p>\n\n<h1>model_1: baseline</h1>\n<p>for a baseline we'll use a\n   <a href=\"https://objax.readthedocs.io/en/latest/objax/zoo.html#objax.zoo.resnet_v2.ResNet18\">resnet18 objax model</a>\n   trained against <code>train</code> using <code>validate</code> for simple early stopping.\n</p>\n<pre class=\"prettyprint\">\n# define model\nmodel = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)\n# train against all model vars\ntrainable_vars = model.vars()\n</pre>\n\n<pre class=\"prettyprint\">\n$ python3 <a href=\"https://github.com/matpalm/ood_focal_loss/blob/master/train_basic_model.py\">train_basic_model.py</a> --loss-fn cross_entropy --model-dir m1\n\nlearning_rate 0.001 validation accuracy 0.372 entropy min/mean/max 0.0005 0.8479 1.9910\nlearning_rate 0.001 validation accuracy 0.449 entropy min/mean/max 0.0000 0.4402 1.9456\nlearning_rate 0.001 validation accuracy 0.517 entropy min/mean/max 0.0000 0.3566 1.8013\nlearning_rate 0.001 validation accuracy 0.578 entropy min/mean/max 0.0000 0.3070 1.7604\nlearning_rate 0.001 validation accuracy 0.680 entropy min/mean/max 0.0000 0.3666 1.7490\nlearning_rate 0.001 validation accuracy 0.705 entropy min/mean/max 0.0000 0.3487 1.6987\nlearning_rate 0.001 validation accuracy 0.671 entropy min/mean/max 0.0000 0.3784 1.8276\nlearning_rate 0.0001 validation accuracy 0.805 entropy min/mean/max 0.0000 0.3252 1.8990\nlearning_rate 0.0001 validation accuracy 0.818 entropy min/mean/max 0.0000 0.3224 1.7743\nlearning_rate 0.0001 validation accuracy 0.807 entropy min/mean/max 0.0000 0.3037 1.8580\n</pre>\n\n<p>we can check the distribution of entropy values of this trained model against our various\n   splits\n</p>\n<img src=\"/blog/imgs/2020/ood/m1.entropy.png\" />\n\n<p>some observations...\n</p>\n<ul>\n <li>\n     highest confidence, i.e. lowest entropy, on training set.\n </li>\n\n <li>\n     equiv values on validation and test; both < training.\n </li>\n\n <li>\n     ood lowest of all; this is a good thing!\n </li>\n\n <li>\n     mean value of entropy on the validation set slowly drops over training.\n </li>\n</ul>\n<p>recall that a higher entropy => a more uniform distribution => less prediction confidence.\n</p>\n<p>as such our goal is to have the entropy of the <code>ood</code> set as high as possible without\n   dropping the <code>test</code> set too much.\n</p>\n\n<h1>model_2: fine tuning entire classifier layer</h1>\n<p>as mentioned before the main approach i'm familiar with is retraining the classifier\n   layer. we'll start with <code>model_1</code> and use the so-far-unseen <code>calibration</code> set for fine tuning it.\n</p>\n<pre class=\"prettyprint\">\n# define model\nmodel = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)\n# restore from model_1\nobjax.io.load_var_collection('m1/weights.npz', model.vars())\n# train against last layer only\nclassifier_layer = model[-1]\ntrainable_vars = classifier_layer.vars()\n</pre>\n\n<pre class=\"prettyprint\">\n$ python3 <a href=\"https://github.com/matpalm/ood_focal_loss/blob/master/train_model_2.py\">train_model_2.py</a> --input-model-dir m1 --output-model-dir m2\n\nlearning_rate 0.001 calibration accuracy 0.808 entropy min/mean/max 0.0000 0.4449 1.8722\nlearning_rate 0.001 calibration accuracy 0.809 entropy min/mean/max 0.0002 0.5236 1.9026\nlearning_rate 0.001 calibration accuracy 0.808 entropy min/mean/max 0.0004 0.5468 1.9079\nlearning_rate 0.001 calibration accuracy 0.811 entropy min/mean/max 0.0005 0.5496 1.9214\nlearning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0001 0.5416 1.9020\nlearning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0002 0.5413 1.9044\nlearning_rate 0.001 calibration accuracy 0.811 entropy min/mean/max 0.0003 0.5458 1.8990\nlearning_rate 0.001 calibration accuracy 0.814 entropy min/mean/max 0.0001 0.5472 1.9002\nlearning_rate 0.001 calibration accuracy 0.812 entropy min/mean/max 0.0001 0.5455 1.9124\nlearning_rate 0.001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5503 1.9092\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5485 1.9011\nlearning_rate 0.0001 calibration accuracy 0.814 entropy min/mean/max 0.0001 0.5476 1.9036\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0001 0.5483 1.9051\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5479 1.9038\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5482 1.9043\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5482 1.9059\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5479 1.9068\nlearning_rate 0.0001 calibration accuracy 0.813 entropy min/mean/max 0.0000 0.5480 1.9058\nlearning_rate 0.0001 calibration accuracy 0.812 entropy min/mean/max 0.0000 0.5488 1.9040\nlearning_rate 0.0001 calibration accuracy 0.814 entropy min/mean/max 0.0000 0.5477 1.9051\n</pre>\n\n<p>if we compare (top1) accuracy of <code>model_1</code> vs <code>model_2</code> we see a slight improvement\n   in <code>model_2</code>, attributable to the slight extra training i suppose (?)\n</p>\n<pre class=\"prettyprint\">\n$ python3 calculate_metrics.py --model m1/weights.npz\n\ntrain accuracy      0.970\nvalidate accuracy   0.807\ncalibrate accuracy  0.799\ntest accuracy       0.803\n\n$ python3 calculate_metrics.py --model m2/weights.npz\n\ntrain accuracy      0.980\nvalidate accuracy   0.816\ncalibrate accuracy  0.814\ntest accuracy       0.810\n\n</pre>\n\n<p>more importantly though; how do the entropy distributions looks?\n</p>\n<img src=\"/blog/imgs/2020/ood/m2.entropy.png\" />\n\n<p>some observations...\n</p>\n<ul>\n <li>\n     calibration accuracy and entropy during training shifted a bit, but not a lot.\n </li>\n\n <li>\n     again highest confidence on training.\n </li>\n\n <li>\n     again equiv values on validation and test; both < training.\n </li>\n\n <li>\n     ood difference a bit more distinct now.\n </li>\n</ul>\n\n<h1>model_3: fit a scalar temperature value to logits</h1>\n<p>in <code>model_3</code> we create a simple single parameter layer that represents rescaling,\n   append it to the pretrained resnet and train just that layer.\n</p>\n<pre class=\"prettyprint\">\nclass Temperature(objax.module.Module):\n    def __init__(self):\n        super().__init__()\n        self.temperature = objax.variable.TrainVar(jnp.array([1.0]))\n    def __call__(self, x):\n        return x / self.temperature.value\n\n# define model\nmodel = objax.zoo.resnet_v2.ResNet18(in_channels=3, num_classes=10)\n# restore from model_1\nobjax.io.load_var_collection('m1/weights.npz', model.vars())\n# add a temp rescaling layer\ntemperature_layer = layers.Temperature()\nmodel.append(temperature_layer)\n# train against just this layer\ntrainable_vars = temperature_layer.vars()\n</pre>\n\n<pre class=\"prettyprint\">\n$ python3 <a href=\"https://github.com/matpalm/ood_focal_loss/blob/master/train_model_3.py\">train_model_3.py</a> --input-model-dir m1 --output-model-dir m3\n\nlearning_rate 0.01 temp 1.4730 calibration accuracy 0.799 entropy min/mean/max 0.0006 0.5215 1.9348\nlearning_rate 0.01 temp 1.5955 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5828 1.9611\nlearning_rate 0.01 temp 1.6118 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5911 1.9643\nlearning_rate 0.01 temp 1.6162 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5933 1.9652\nlearning_rate 0.01 temp 1.6118 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5911 1.9643\nlearning_rate 0.01 temp 1.6181 calibration accuracy 0.799 entropy min/mean/max 0.0015 0.5943 1.9655\nlearning_rate 0.01 temp 1.6126 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5915 1.9645\nlearning_rate 0.01 temp 1.6344 calibration accuracy 0.799 entropy min/mean/max 0.0016 0.6025 1.9687\nlearning_rate 0.01 temp 1.6338 calibration accuracy 0.799 entropy min/mean/max 0.0016 0.6022 1.9686\nlearning_rate 0.01 temp 1.6018 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5860 1.9623\nlearning_rate 0.001 temp 1.6054 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5878 1.9630\nlearning_rate 0.001 temp 1.6047 calibration accuracy 0.799 entropy min/mean/max 0.0013 0.5874 1.9629\nlearning_rate 0.001 temp 1.6102 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5903 1.9640\nlearning_rate 0.001 temp 1.6095 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5899 1.9638\nlearning_rate 0.001 temp 1.6112 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5908 1.9642\nlearning_rate 0.001 temp 1.6145 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5924 1.9648\nlearning_rate 0.001 temp 1.6134 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5919 1.9646\nlearning_rate 0.001 temp 1.6144 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5924 1.9648\nlearning_rate 0.001 temp 1.6105 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5904 1.9640\nlearning_rate 0.001 temp 1.6151 calibration accuracy 0.799 entropy min/mean/max 0.0014 0.5927 1.9650\n\n</pre>\n\n<img src=\"/blog/imgs/2020/ood/m3.entropy.png\" />\n\n<p>some observations...\n</p>\n<ul>\n <li>\n     we're fitting a single value, so very fast!\n </li>\n\n <li>\n     as expected we don't see <em>any</em> change in accuracy during training compared to <code>model_1</code>. this is since the temp rescaling never changes the ordering of predictions. ( just this property alone could be enough reason to use this approach in some cases! )\n </li>\n\n <li>\n     slightly better than <code>model_2</code> in terms of raising the entropy of <code>ood</code>!\n </li>\n</ul>\n<p>not bad for fitting <em>a single</em> parameter :D\n</p>\n\n<h1>model_4: use focal loss</h1>\n<p>as a final experiment we'll use focal loss for training instead of cross entropy\n</p>\n<p>note: i rolled my own focal loss function which is always dangerous! it's not\n   impossible (i.e. it's likely) i've done something wrong in terms of numerical\n   stability; please let me know if you see a dumb bug :)\n</p>\n<pre class=\"prettyprint\"><code class=\"language-python\">def focal_loss_sparse(logits, y_true, gamma=1.0):\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    log_probs = log_probs[jnp.arange(len(log_probs)), y_true]\n    probs = jnp.exp(log_probs)\n    elementwise_loss = -1 * ((1 - probs)**gamma) * log_probs\n    return elementwise_loss\n</code></pre>\n\n<p>note that focal loss includes a gamma. we'll trial 1.0, 2.0 and 3.0 in line with\n   the experiments mentioned in\n   <a href=\"https://torrvision.github.io/focal_calibration/\">calibrating deep neural networks using focal loss</a>\n</p>\n\n<h2>gamma 1.0</h2>\n<pre class=\"prettyprint\">\n$ python3 train_basic_model.py --loss-fn focal_loss --gamma 1.0 --model-dir m4_1\n\nlearning_rate 0.001 validation accuracy 0.314 entropy min/mean/max 0.0193 1.0373 2.0371\nlearning_rate 0.001 validation accuracy 0.326 entropy min/mean/max 0.0000 0.5101 1.8546\nlearning_rate 0.001 validation accuracy 0.615 entropy min/mean/max 0.0000 0.6680 1.8593\nlearning_rate 0.001 validation accuracy 0.604 entropy min/mean/max 0.0000 0.5211 1.8107\nlearning_rate 0.001 validation accuracy 0.555 entropy min/mean/max 0.0001 0.5127 1.7489\nlearning_rate 0.0001 validation accuracy 0.758 entropy min/mean/max 0.0028 0.6008 1.8130\nlearning_rate 0.0001 validation accuracy 0.780 entropy min/mean/max 0.0065 0.6590 1.9305\nlearning_rate 0.0001 validation accuracy 0.796 entropy min/mean/max 0.0052 0.6390 1.8589\nlearning_rate 0.0001 validation accuracy 0.790 entropy min/mean/max 0.0029 0.5886 1.9151\n</pre>\n\n<img src=\"/blog/imgs/2020/ood/m4_1.entropy.png\" />\n\n\n<h2>gamma 2.0</h2>\n<pre class=\"prettyprint\">\n$ python3 train_basic_model.py --loss-fn focal_loss --gamma 2.0 --model-dir m4_2\n\nlearning_rate 0.001 validation accuracy 0.136 entropy min/mean/max 0.0001 0.2410 1.7906\nlearning_rate 0.001 validation accuracy 0.530 entropy min/mean/max 0.0000 0.8216 2.0485\nlearning_rate 0.001 validation accuracy 0.495 entropy min/mean/max 0.0003 0.7524 1.9086\nlearning_rate 0.001 validation accuracy 0.507 entropy min/mean/max 0.0000 0.6308 1.8460\nlearning_rate 0.001 validation accuracy 0.540 entropy min/mean/max 0.0000 0.5899 2.0196\nlearning_rate 0.001 validation accuracy 0.684 entropy min/mean/max 0.0001 0.6189 1.9029\nlearning_rate 0.001 validation accuracy 0.716 entropy min/mean/max 0.0000 0.6509 1.8822\nlearning_rate 0.001 validation accuracy 0.606 entropy min/mean/max 0.0001 0.7096 1.8312\nlearning_rate 0.0001 validation accuracy 0.810 entropy min/mean/max 0.0002 0.6208 1.9447\nlearning_rate 0.0001 validation accuracy 0.807 entropy min/mean/max 0.0004 0.6034 1.8420\n</pre>\n\n<img src=\"/blog/imgs/2020/ood/m4_2.entropy.png\" />\n\n\n<h2>gamma 3.0</h2>\n<pre class=\"prettyprint\">\n$ python3 train_basic_model.py --loss-fn focal_loss --gamma 3.0 --model-dir m4_3\n\nlearning_rate 0.001 validation accuracy 0.288 entropy min/mean/max 0.0185 1.1661 1.9976\nlearning_rate 0.001 validation accuracy 0.319 entropy min/mean/max 0.0170 1.0925 2.1027\nlearning_rate 0.001 validation accuracy 0.401 entropy min/mean/max 0.0396 0.9795 2.0361\nlearning_rate 0.001 validation accuracy 0.528 entropy min/mean/max 0.0001 0.8534 1.9115\nlearning_rate 0.001 validation accuracy 0.528 entropy min/mean/max 0.0002 0.7405 1.8509\nlearning_rate 0.0001 validation accuracy 0.748 entropy min/mean/max 0.0380 0.9204 1.9697\nlearning_rate 0.0001 validation accuracy 0.760 entropy min/mean/max 0.0764 0.9807 1.9861\nlearning_rate 0.0001 validation accuracy 0.772 entropy min/mean/max 0.0906 0.9646 2.0556\nlearning_rate 0.0001 validation accuracy 0.781 entropy min/mean/max 0.0768 0.8973 1.9264\nlearning_rate 0.0001 validation accuracy 0.780 entropy min/mean/max 0.0871 0.8212 1.9814\n</pre>\n\n<img src=\"/blog/imgs/2020/ood/m4_3.entropy.png\" />\n\n<p>some observations...\n</p>\n<ul>\n <li>\n     can see that immediately the model behaves more like <code>model_2</code> than <code>model_1</code> in terms of calibration.\n </li>\n\n <li>\n     increasing gamma really starts to seperate the <code>ood</code> data, but with a drop in overall <code>validation</code> accuracy. maybe the seperation isn't as much as i'd like, but happy to experiment more given this result.\n </li>\n</ul>\n<p>interesting!! focal loss is definitely going in my \"calibration toolkit\" :D\n</p>\n\n<h1>doing the <em>actual</em> detection though</h1>\n<p>recall the distribution plot of entropies from the <code>model_3</code> experiment\n</p>\n<img src=\"/blog/imgs/2020/ood/m3.entropy.png\" />\n\n<p>we can eyeball that the entropies values are low for in-distribution sets (train, validate,\n   calibrate and test) and high for the two ood sets (easy and hard)\n   but how do we <em>actually</em> turn this difference into a in-distribution classifier?\n   ( recall this is not a standard supervised learning problem, we only have the in-distribution\n   instances to train against and nothing from the ood sets at training time )\n</p>\n<p>amusingly i spent a bunch of time hacking around with all sorts of density estimation\n   techniques but it turned out the simplest baseline i started with was the best :/\n</p>\n<p>the baseline works on a strong, but valid enough, assumption of the entropies; the entropies\n   for in-distribution instances will be lower than the entropies for OOD instances.\n   given this assumption we can just do the simplest \"classifier\" of all; thresholding\n</p>\n<p>to get a sense of the precision/recall tradeoff of approach we can do the following...\n</p>\n<ul>\n <li>\n     fit a MinMax scaler to the <code>train</code> set; this scales all train values to (0, 1)\n </li>\n\n <li>\n     use this scaler to transform <code>test</code>, <code>ood_easy</code> and <code>ood_hard</code>; including (0, 1) clipping\n </li>\n\n <li>\n     \"flip\" all the values, <code>x = 1.0 - x</code>, since we want a low entropy to denote a positive instance of in-distribution\n </li>\n\n <li>\n     compare the performance of this \"classifier\" by considering <code>test</code> instances as positives and <code>ood</code> instances\nas negatives.\n </li>\n</ul>\n<p>with this simple approach we get the following ROC, P/R and reliability plots.\n</p>\n<img src=\"/blog/imgs/2020/ood/m3.roc_pr_reliability.thresholding.png\" />\n\n<p>i've included the attempts i made to use a kernel density estimator in <code>fit_kernel_density_to_entropy.py</code>\n   so if you manage to get a result better than this trivial baseline please\n   let me know :)\n</p>\n\n<h1>other options for detecting OOD</h1>\n<p>though this post is about using entropy as a way of measuring OOD there are other ways too.\n</p>\n<p>the simplest way i've handled OOD detection in the past is to include an\n   explicit OTHER label. this has worked in cases where\n   i've had data available to train against, e.g. data that was (somehow) sampled\n   but wasn't of interest for actual supervised problem at hand. but i can see\n   in lots of situations this wouldn't always be possible.\n</p>\n<p>another approach i'm still hacking on is doing density estimation on the inputs; this\n   related very closely to the diversity sampling ideas for active learning.\n</p>\n\n<h1>code</h1>\n<p>these experiments were written in\n   <a href=\"https://objax.readthedocs.io/\">objax</a>, an object oriented wrapper for\n   <a href=\"https://jax.readthedocs.io/\">jax</a>\n</p>\n<p><a href=\"https://github.com/matpalm/ood_focal_loss\">code available on github</a>\n</p>"
}