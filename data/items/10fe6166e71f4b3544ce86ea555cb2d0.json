{
  "title": "Big Data & Machine Learning Convergence",
  "link": "",
  "updated": "2014-08-22T16:21:00+02:00",
  "published": "2014-08-22T16:21:00+02:00",
  "author": {
    "name": "Mikio L. Braun",
    "uri": "http://mikiobraun.de",
    "email": "mikiobraun@gmail.com"
  },
  "id": "http://blog.mikiobraun.de/2014/08/big-data-machine-learning-convergence",
  "content": "<p><p>I recently had two pretty interesting discussions with students here at TU\nBerlin  which I think are representative with respect to how big the\ndivide between the machine learning community and the Big Data community still\nis.</p>\n\n<h2 id=\"linear-algebra-vs-functional-collections\">Linear Algebra vs. Functional collections</h2>\n\n<p>One student is working on implementing a boosting method I wrote a few years\nago using next-gen Big Data frameworks like <a href=\"http://flink.incubator.apache.org/\">Flink</a> and <a href=\"https://spark.apache.org/\">Spark</a> as\npart of his master thesis. I choose this algorithm because it the operations\ninvolved were quite simple: computing scalar products, vector differences, and\nnorms of vectors. Probably the most complex thing is to compute a cumulative\nsum.</p>\n\n<p>These are all operations which boil down to linear algebra, and the whole\nalgorithm is a few lines of code in pseudo-notation expressed in linear\nalgebra. I was wondering just how hard it would be to formulate this using a\nmore “functional collection” style API.</p>\n\n<p>For example, in order to compute the squared norm of a vector, you have to\nsquare each element and sum them up. In a language like C you’d do it like\nthis:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-c--\" data-lang=\"c++\"><span class=\"kt\">double</span> <span class=\"nf\">squaredNorm</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"kt\">double</span> <span class=\"n\">a</span><span class=\"p\">[])</span> <span class=\"p\">{</span>\n  <span class=\"kt\">double</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n  <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">n</span><span class=\"p\">;</span> <span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">s</span> <span class=\"o\">+=</span> <span class=\"n\">a</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">a</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">];</span>\n  <span class=\"p\">}</span>\n  <span class=\"k\">return</span> <span class=\"n\">s</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></figure>\n\n<p>In Scala, you’d express the same thing with</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-scala\" data-lang=\"scala\"><span class=\"k\">def</span> <span class=\"nf\">squaredNorm</span><span class=\"o\">(</span><span class=\"n\">a</span><span class=\"k\">:</span> <span class=\"kt\">Seq</span><span class=\"o\">[</span><span class=\"kt\">Double</span><span class=\"o\">])</span> <span class=\"k\">=</span>\n\t<span class=\"nv\">a</span><span class=\"o\">.</span><span class=\"py\">map</span><span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"k\">=&gt;</span> <span class=\"n\">x</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"o\">).</span><span class=\"py\">sum</span></code></pre></figure>\n\n<p>In a way, the main challenge here consists in breaking down these for-loops\ninto these sequence primitives provided by the language. Another example: the\nscalar product (sum of product of the corresponding elements of two vectors)\nwould become</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-scala\" data-lang=\"scala\"><span class=\"k\">def</span> <span class=\"nf\">scalarProduct</span><span class=\"o\">(</span><span class=\"n\">a</span><span class=\"k\">:</span> <span class=\"kt\">Seq</span><span class=\"o\">[</span><span class=\"kt\">Double</span><span class=\"o\">],</span> <span class=\"n\">b</span><span class=\"k\">:</span> <span class=\"kt\">Seq</span><span class=\"o\">[</span><span class=\"kt\">Double</span><span class=\"o\">])</span> <span class=\"k\">=</span>\n\t<span class=\"nv\">a</span><span class=\"o\">.</span><span class=\"py\">zip</span><span class=\"o\">(</span><span class=\"n\">b</span><span class=\"o\">).</span><span class=\"py\">map</span><span class=\"o\">(</span><span class=\"n\">ab</span> <span class=\"k\">=&gt;</span> <span class=\"nv\">ab</span><span class=\"o\">.</span><span class=\"py\">_1</span> <span class=\"o\">*</span> <span class=\"nv\">ab</span><span class=\"o\">.</span><span class=\"py\">_2</span><span class=\"o\">).</span><span class=\"py\">sum</span></code></pre></figure>\n\n<p>and so on.</p>\n\n<p>Now turning to a system like Flink or Spark which provides a very similar set\nof operations and is able to distribute them, it should be possible to use a\nsimilar approach. However, the first surprise was that in distributed systems,\nthere is no notion of order of a sequence. It’s really more of a collection of\nthings.</p>\n\n<p>So if you have to compute the scalar product between the vectors, you need to\nextend the stored data to include the index of each entry as well, and then\nyou first need to join the two sequences on the index to be able to perform\nthe map.</p>\n\n<p>The student is still half way through with this, but already it has cost\nconsiderable amount of mental work to rethink standard operations in the new\nnotation, and most importantly, have faith in the underlying system that it is\nable to perform things like joining vectors such that elements are aligned in\na smart fashion.</p>\n\n<p>I think the main message here is that machine learners really like to think in\nterms of matrices and vectors, not so much databases and query languages.\nThat’s the way algorithms are described in papers, that’s the way people\nthink, and the way people are trained, and it would be tremendously helpful if\nthere is a layer for that on top of Spark or Flink. There are already some\nactivites in that direction like <a href=\"https://spark.apache.org/docs/latest/mllib-basics.html\">distributed vectors in\nSpark</a> or the <a href=\"https://mahout.apache.org/users/sparkbindings/play-with-\nshell.html\">spark-shell\n in Mahout</a>, and I’m pretty interested to see whether how they will develop.</p>\n\n<h2 id=\"big-data-vs-big-computation\">Big Data vs. Big Computation</h2>\n\n<p>The other interesting discussion was with a Ph.D. student who works on\npredicting properties in solid state physics using machine learning. He\napparently didn’t knew too much about Hadoop and when I explained it to him he\nalso found it not appealing at all, although he is spending quite some compute\ntime on the groups cluster.</p>\n\n<p>There exists a medium sized cluster at TU Berlin for the machine learning group.\nIt consists of about 35 nodes, and hosts about 13TB of data for all kinds of\nresearch projects from the last ten or so years. But the cluster does not run\non Hadoop, it uses Sun’s <a href=\"http://www.univa.com/products/grid-engine.php\">gridengine</a>, which is now maintained by Univa.\nThere are historical reasons for that. Actually, the current infrastructure\ndeveloped over a number of years. So here is the short-history of distributed\ncomputing at the lab:</p>\n\n<p>Back in the early 2000s, people were still having desktop PCs under their\ndesks. At the time, people were doing most of their work on their own\ncomputer, although I think disk space was already shared over NFS (probably\nmainly for backup reasons). As people required more computing power, people\nstarted to log into other computers (of course, after asking whether that was\nok), in addition to several larger sized computers which were bought at the\ntime.</p>\n\n<p>That didn’t go well for a long time. First of all, manually finding computers\nwith resources to spare was pretty cumbersome, and oftentimes, your computer\nwould become very noisy although you weren’t doing any work yourself. So the\nnext step was to buy some rack servers, and put them into a server room, still\nwith the same centralized filesystem shared over NFS.</p>\n\n<p>The next step was to keep people from logging in to individual computers.\nInstead, gridengine was installed, which lets you submit jobs in the form of\nshell scripts to execute on the cluster when there were free resources. In a\nway, gridengine is like <a href=\"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\">YARN</a>, but restricted to shell scripts and\ninteractive shells. It has some more advanced capabilities, but people mostly\nsubmit it to run their programs somewhere in the cluster.</p>\n\n<div class=\"figure\">\n\t<img src=\"/images/tu-cluster.png\" />\n\tCompute cluster for machine learning research.\n</div>\n\n<p>Things have evolved a bit by now, for example, the NFS is now connected to a\nSAN over fibre channel, and there exist different slots for interactive and\nbatch jobs, but the structure is still the same, and it works. People use it\nfor matlab, native code, python, and many other things.</p>\n\n<p>I think the main reason that this system still works is that the jobs which\nare run here are mostly compute intensive and no so much data intensive.\nMostly the system is used to run large batches of model comparison, testing\nmany different variants on essentially the same data set.</p>\n\n<p>Most jobs follow the same principle: They initially load the data into memory\n(usually not more than a few hundred MB) and then compute for minutes to\nhours. In the end, the resulting model and some performance numbers are\nwritten to disk. Usually, the methods are pretty complex (this is ML research,\nafter all). Contrast this with “typical” Big Data settings where you have\nterabytes of data and run comparatively simple analysis methods or search on\nthem.</p>\n\n<p>The good message here is that scalable computing in the way it’s mostly\nrequired today is not that complicated. So this is less about\n<a href=\"http://en.wikipedia.org/wiki/Message_Passing_Interface\">MPI</a> and hordes of\ncompute workers, but more about support for managing long running computation\ntasks, dealing with issues of job dependency, snapshotting for failures, and\nso on.</p>\n\n<h2 id=\"big-data-to-complex-methods\">Big Data to Complex Methods?</h2>\n\n<p>The way I see it, Big Data has so far been driven mostly by the requirement to\ndeal with huge amount of data in a scalable fashion, while the methods were\nusually pretty simple (well, at least in terms to what is considered simple in\nmachine learning research).</p>\n\n<p>But eventually, more complex methods will also become relevant, such that\nscalable large scale computations will become more important, and possible\neven a combination of both. There already exists a large body of work for\nlarge scale computation, for example from people running large scale numerical\nsimulation in physics or meterology, but less so from database people.</p>\n\n<p>On the other hand, there is lots of potential for machine learners to open up\nnew possibilities to deal with vasts amount of data in an interactive fashion,\nsomething which is just plain impossible with a system like gridengine.</p>\n\n<p>As these two fields converge, work has to be done to provide the right set of\nmechanisms and abstractions. Right now I still think there is a considerable\ngap which we need to close over the next few years.</p>\n\n</p>\n   <p><a href=\"http://blog.mikiobraun.de/2014/08/big-data-machine-learning-convergence.html\">Click here for the full article</a>"
}