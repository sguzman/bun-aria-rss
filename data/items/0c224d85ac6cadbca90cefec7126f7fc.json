{
  "title": "Simpson&#8217;s Paradox and Deep Learning Metrics with Weightwatcher",
  "link": "https://calculatedcontent.com/2020/11/26/simpsons-paradox-and-deep-learning-metrics-with-weightwatcher/",
  "comments": "https://calculatedcontent.com/2020/11/26/simpsons-paradox-and-deep-learning-metrics-with-weightwatcher/#comments",
  "dc:creator": "Charles H Martin, PhD",
  "pubDate": "Fri, 27 Nov 2020 07:43:27 +0000",
  "category": "Uncategorized",
  "guid": "http://calculatedcontent.com/?p=13878",
  "description": "What is WeightWatcher ? The WeightWatcher tool is an open-source python package that can be used to predict the test &#8230; <a class=\"more-link\" href=\"https://calculatedcontent.com/2020/11/26/simpsons-paradox-and-deep-learning-metrics-with-weightwatcher/\">More</a>",
  "content:encoded": "\n<h5>What is WeightWatcher ?</h5>\n\n\n\n<p>The WeightWatcher tool is an open-source python package that can be used to predict the test accuracy of a series similar of Deep Neural Network (DNN) &#8212; without peeking at the test data.    </p>\n\n\n\n<p>WeightWatcher is based on research done in collaboration with UC Berkeley on the foundations of Deep Learning. We built this tool to help you analyze and debug your Deep Neural Networks.  </p>\n\n\n\n<p>It is easy to install and run; the tool will analyze your model and return both summary statistics and detailed metrics for each layer:</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png\"><img loading=\"lazy\" data-attachment-id=\"13907\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-25-at-8-28-33-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png\" data-orig-size=\"906,766\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-25-at-8.28.33-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=906\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=906\" alt=\"\" class=\"wp-image-13907\" width=\"408\" height=\"345\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=408 408w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=816 816w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-8.28.33-pm.png?w=768 768w\" sizes=\"(max-width: 408px) 100vw, 408px\" /></a></figure>\n\n\n\n<p>The <a href=\"https://github.com/CalculatedContent/WeightWatcher\">WeightWatcher github page</a> lists various papers and online presentations given at UC Berkeley and Stanford, and various conferences like ICML, KDD, etc.  There, and on this blog, you can find examples of how to use it.  </p>\n\n\n\n<p>This post describes how to select the metric for your models, and why.</p>\n\n\n\n<h5><strong>Different Metrics for Different Models</strong></h5>\n\n\n\n<p>You can use WeightWatcher to model a series of DNNs of either increasing size, or with different hyperparameters.  But you need different metrics &#8212; <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> vs. <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" /> &#8212; for different cases:</p>\n\n\n\n<ul><li><strong>alpha <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;;&#92;;\" class=\"latex\" /> : </strong> for different hyper-parameters settings (batch size, weight decay, &#8230;) on the same model</li><li><strong>weighted alpha: <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%5C%3B%5C%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}&#92;;&#92;;\" class=\"latex\" /> :</strong> for an architecture series like VGG11, VGG13, VGG16, VGG19 </li></ul>\n\n\n\n<p>But why do need 2 different alpha metrics?   To understand this, we need to understand</p>\n\n\n\n<h4><strong>Spectral Norms and Deep Learning   </strong></h4>\n\n\n\n<p>Traditional machine learning theory suggests that the test performance of a Deep Neural Network is correlated with the average log Spectral Norm. That is, the test error should be bounded by the average Spectral Norm, so the smaller norm, the smaller the test error. </p>\n\n\n\n<p>The Spectral Norm <img src=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}\" class=\"latex\" /> of an <img src=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"N&#92;times M\" class=\"latex\" /> matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> is just the (square root of the ) maximum eigenvalue <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{max}\" class=\"latex\" /> of its correlation matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}\" class=\"latex\" /></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3A%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5C%3B+%5Cmathbf%7BX%7D%5Cmathbf%7Be%7D_%7Bl%7D%3D+%5Clambda_%7Bl%7D%5Cmathbf%7Be%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3A%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5C%3B+%5Cmathbf%7BX%7D%5Cmathbf%7Be%7D_%7Bl%7D%3D+%5Clambda_%7Bl%7D%5Cmathbf%7Be%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3A%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5C%3B+%5Cmathbf%7BX%7D%5Cmathbf%7Be%7D_%7Bl%7D%3D+%5Clambda_%7Bl%7D%5Cmathbf%7Be%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}:= &#92;dfrac{1}{N}&#92;mathbf{W}^{T}&#92;mathbf{W},&#92;;&#92;;&#92;; &#92;mathbf{X}&#92;mathbf{e}_{l}= &#92;lambda_{l}&#92;mathbf{e}_{l}\" class=\"latex\" /></p>\n\n\n\n<p>We denote the (squared) Spectal Norm as:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D+%3A%3D+%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D+%3A%3D+%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D+%3A%3D+%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty} := &#92;lambda^{max}\" class=\"latex\" /></p>\n\n\n\n<p>Note:<a href=\"https://arxiv.org/abs/1810.01075\"> in earlier papers we (and others) also use:</a> <img src=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B2%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B2%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B2%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{2}\" class=\"latex\" /> </p>\n\n\n\n<p>WeightWatcher computes the log Spectral Norm for each layer, and defines:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle+%3A%3D+%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle+%3A%3D+%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle+%3A%3D+%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;log_{10}&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}&#92;rangle := &#92;dfrac{1}{N_L}&#92;sum_{l}^{N_L}&#92;log_{10}&#92;lambda^{max}_{l}\" class=\"latex\" /></p>\n\n\n\n<p>which we compute by averaging over all <img src=\"https://s0.wp.com/latex.php?latex=N_%7BL%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=N_%7BL%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=N_%7BL%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"N_{L}\" class=\"latex\" /> layer weight matrices.</p>\n\n\n\n<p>We compute the eigenvalues, or the Empirical Spectral Density (ESD), of each layer <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}\" class=\"latex\" /> by running SVD directly on the layer <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> or, for Conv2D layers, some matrix slice (see the Appendix).</p>\n\n\n\n<h5><strong>Spectral Norm Regularization</strong></h5>\n\n\n\n<p><a href=\"https://arxiv.org/pdf/1705.10941.pdf\">It has been suggested by Yoshida and Miyato</a> that the Spectral Norm would make a good regularizer for DNNs.  The basic idea is that the test data should look enough like the training data so that if we can say something about how the DNN performs on <em>perturbed</em> training data, that will also say something about the test performance.  </p>\n\n\n\n<p>Here is the text from the paper; let. me explain  how to interpret this in practical terms.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png\"><img data-attachment-id=\"13926\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-25-at-10-40-43-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png\" data-orig-size=\"2280,1118\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-25-at-10.40.43-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=1024\" alt=\"\" class=\"wp-image-13926\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=2048 2048w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-25-at-10.40.43-pm.png?w=768 768w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<p>We imagine the test data <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_{test}\" class=\"latex\" /> must look like the training data <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_{train}\" class=\"latex\" /> with some small perturbation <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{&#92;xi}\" class=\"latex\" />.  Let us write this as:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D%5Csim%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D%5Csim%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btest%7D%5Csim%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_{test}&#92;sim&#92;mathbf{x}_{train}+&#92;mathbf{&#92;xi}\" class=\"latex\" /></p>\n\n\n\n<p>As we train a DNN, we run several epochs of BackProp, which amounts to multiplying <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_{train}\" class=\"latex\" /> by a weight matrix  <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> at each layer, apply an activation function, and repeat until we get a label <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{y}_{train}\" class=\"latex\" /></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btrain%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"RELU(&#92;mathbf{W}&#92;mathbf{x}_{train}+&#92;mathbf{b})&#92;rightarrow&#92;cdots&#92;rightarrow&#92;mathbf{y}_{train}\" class=\"latex\" /></p>\n\n\n\n<p>To get an estimate, or bound, on the  test accuracy, we can then imagine applying the matrix multiply to the perturbed training point</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%28%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D%29%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%28%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D%29%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=RELU%28%5Cmathbf%7BW%7D%28%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D%29%2B%5Cmathbf%7Bb%7D%29%5Crightarrow%5Ccdots%5Crightarrow%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"RELU(&#92;mathbf{W}(&#92;mathbf{x}_{train}+&#92;mathbf{&#92;xi})+&#92;mathbf{b})&#92;rightarrow&#92;cdots&#92;rightarrow&#92;mathbf{y}_{test}\" class=\"latex\" /></p>\n\n\n\n<p>So if we can say something about how the DNN should perform on a perturbed training point <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_%7Btrain%7D%2B%5Cmathbf%7B%5Cxi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_{train}+&#92;mathbf{&#92;xi}\" class=\"latex\" />, we can say something about the test output <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D_%7Btest%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{y}_{test}\" class=\"latex\" />.  </p>\n\n\n\n<p><em>What can we say ? </em></p>\n\n\n\n<p>  When we apply an activation function <img src=\"https://s0.wp.com/latex.php?latex=f%7Bx%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=f%7Bx%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=f%7Bx%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"f{x}\" class=\"latex\" />, like a RELU, which acts pointwise on the data vector, and acts like an affine transformation.  So the RELU+weight matrix multiply is a bounded linear operator (at least piecewise) and therefore it can be bounded by it&#8217;s <a href=\"https://www.youtube.com/watch?v=K-yDVqijSYw\">Spectral Radius</a> <img src=\"https://s0.wp.com/latex.php?latex=%5Csigma%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Csigma%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csigma%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;sigma(&#92;mathbf{W})\" class=\"latex\" />.</p>\n\n\n\n<p>So we say that we want to learn a DNN model such that, at each layer, the action of the layer matrix-vector multiply is bounded by the Spectral Norm of its layer weight matrix.  This should, in theory, give good test performance.  By applying a Spectral Norm regularizer, we think we can make a DNN that is more robust to small changes in it&#8217;s input.  That is, we can make it perform better on random  perturbations the of training data , and, therefore, presumably, better on the test data.</p>\n\n\n\n<p><strong>From Bounds to a Regularizer</strong></p>\n\n\n\n<p>When we develop a mathematical bound , our first instinct is to develop a numerical regularizer <img src=\"https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Omega(&#92;mathbf{W}, &#92;mathbf{b})\" class=\"latex\" />.   That is, when solving our optimization problem&#8211;minimizing the DNN Energy function <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{E}_{DNN}(&#92;mathbf{X}, &#92;mathbf{Y})\" class=\"latex\" />&#8211;we want to prevent the solution from blowing up. Having a <em>mathematically rigorous</em> bound helps here since it seems to bound the BackProp optimization step on every iteration:</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=%5Cunderset%7B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%7D%7B%5Cmin%7D%5C%3B%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29%2B%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cunderset%7B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%7D%7B%5Cmin%7D%5C%3B%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29%2B%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cunderset%7B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%7D%7B%5Cmin%7D%5C%3B%5Cmathcal%7BE%7D_%7BDNN%7D%28%5Cmathbf%7BX%7D%2C+%5Cmathbf%7BY%7D%29%2B%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;underset{&#92;mathbf{W}, &#92;mathbf{b}}{&#92;min}&#92;;&#92;mathcal{E}_{DNN}(&#92;mathbf{X}, &#92;mathbf{Y})+&#92;Omega(&#92;mathbf{W}, &#92;mathbf{b})\" class=\"latex\" /></p>\n\n\n\n<p>Notice that since the regularizer <img src=\"https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5COmega%28%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Bb%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Omega(&#92;mathbf{W}, &#92;mathbf{b})\" class=\"latex\" /> appears in the optimization problem, it must be differentiable (either directly, or using some trick).</p>\n\n\n\n<p>A regularizer must also be easy to implement.  For example, <a href=\"https://arxiv.org/pdf/1901.11352.pdf\">we could also bound the Jacobian</a> <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{&#92;mathbf{J}}(&#92;mathbf{W})\" class=\"latex\" />, but this very expensive to compute, and it is difficult to apply even a norm bound of this  <img src=\"https://s0.wp.com/latex.php?latex=%28%5CVert%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%5Cmathbf%7BW%7D%29%5CVert%5E%7B2%7D_%7BF%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28%5CVert%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%5Cmathbf%7BW%7D%29%5CVert%5E%7B2%7D_%7BF%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5CVert%5Cmathcal%7B%5Cmathbf%7BJ%7D%7D%5Cmathbf%7BW%7D%29%5CVert%5E%7B2%7D_%7BF%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(&#92;Vert&#92;mathcal{&#92;mathbf{J}}&#92;mathbf{W})&#92;Vert^{2}_{F})\" class=\"latex\" />, on every step of BackProp.  It might also seem that the Spectral norm is hard to compute because one needs to run SVD, but there is a simple trick.  One can approximate the maximum eigenvalue <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda^{max}\" class=\"latex\" /> of <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}\" class=\"latex\" /> using the Power Method, by running it for say steps, and then simply add this to the SGD update step,.  There are many examples on github of this, and <a href=\"https://github.com/IShengFang/SpectralNormalizationKeras\">it has been applied, in particular, to GANs</a> and <a href=\"https://arxiv.org/pdf/1807.04720.pdf\">has been shown to work very well in large scale studies.</a></p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter\"><img src=\"https://discuss.pytorch.org/uploads/default/optimized/2X/d/d648f9e9c6547465033aeefbcaa751da5c461f22_2_690x310.png\" alt=\"image\" /></figure></div>\n\n\n\n<p>Also, since this expression is linear in <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" />, we can also readily plug this into TensorFlow /Keras or PyTorch and use autograd to compute the derivatives.  <a href=\"https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization\">It is available as a Tensforflow Addon</a>, and is part of the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html\">core pyTorch 1.7 package.</a></p>\n\n\n\n<p>Spectral Norm Regularization has not been widely used (outside say GANs) because it only works well for very deep networks.  See, however, this adaption for smaller DNNs called <a href=\"https://github.com/AntixK/mean-spectral-norm\">Mean Spectral Normalization</a>.</p>\n\n\n\n<h5><strong>From Worst-Case Bounds to Average Case Behavior</strong></h5>\n\n\n\n<p>What do we want from a theory of learning ?   With WeightWatcher, we have never sought. a rigorous bound. That&#8217;s not the goal of our theory. We do not seek a bound because this decribes the<em> worst-case behavior;</em>  we seek to understand <em>the average-case behavior</em> (However, what we can do repair the Spectral Norm as a metric , as shown below.)</p>\n\n\n\n<p>With the average-case, we hope to able to predict the generalization error of a DNN (and without peeking at the test data). And we mean this a very practical sense, applying to very large, production quality models, both in training and fine-tuning them.   </p>\n\n\n\n<p>So what&#8217;s the difference between having a bound and analyzing average-case behavior ?</p>\n\n\n\n<ul><li>With a mathematical bound,  we can bound the error&#8211;for a single model.  So this is alike a prediction, and , as shown above, we can use this to develop better regularizers.</li></ul>\n\n\n\n<ul><li>WIth the average-case behavior, we want to predict trends across many different models. Of both different depths (since deeper models usually perform better) and with different hyperparameter settings (batch size, momentum, weight decay, etc.)</li></ul>\n\n\n\n<p>It&#8217;s not at all obvious that we can expect a mathematical bound to be correlated with trends in the test accuracy of real-world DNNs.  It turns out, the Spectral Norm works pretty well&#8211;at least across pretrained DNNs of increasing depth.</p>\n\n\n\n<p>Here is an example, showing how the Spectral Norm performs on the VGG series</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png\"><img loading=\"lazy\" data-attachment-id=\"13974\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-26-at-10-21-43-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png\" data-orig-size=\"782,676\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-26-at-10.21.43-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=782\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=782\" alt=\"\" class=\"wp-image-13974\" width=\"506\" height=\"438\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=506 506w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.21.43-pm.png 782w\" sizes=\"(max-width: 506px) 100vw, 506px\" /></a><figcaption>Average Log Spectral Norm vs Top1 Test Accuracy for several  VGG models <br>(VGG11, VGG13, VGG16, VGG19, with/out Batchnorm, trained on Imagenet-1K)</figcaption></figure></div>\n\n\n\n<p>We see that the average log Spectral norm correlates quite well with the test accuracy of the DNN architecture series of the pretrained VGG models.  This is remarkable, since we do not have access to the training or the test data (or other information).  </p>\n\n\n\n<p><a href=\"https://arxiv.org/pdf/2002.06716.pdf\">We have used WeightWatcher </a>to analyze hundreds of pretrained models, of increasing depths, and using different data sets. Generally speaking, the average log Spectral Norm correlates well with the test accuracies of many different DNN series and for different data sets.  </p>\n\n\n\n<p>But not always. And that&#8217;s the rub.</p>\n\n\n\n<h5><strong>Simpson&#8217;s Paradox (or our Spectral surprise) </strong></h5>\n\n\n\n<p>Oddly, while the average log Spectral Norm <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;log_{10}&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}&#92;rangle\" class=\"latex\" /> is correlated with test error, when changing the depth of a DNN model,  it turns out to be <em>anti-correlated </em>with test error when varying the optimization hyper-parameters.     This is a classic example of Simpson&#8217;s paradox.</p>\n\n\n\n<p>We have noted this, and <a href=\"https://arxiv.org/abs/1912.02178\">it has also pointed by Bengio and co-workers</a>.  Indeed, an entire contest was recently set up to study this issue&#8211;the <a href=\"https://competitions.codalab.org/competitions/25301\">2020 NeurIPS Predicting Generalization challenge</a>.</p>\n\n\n\n<p>Below we can see the paradox by looking at predictions for ~100 small, pre-trained  VGG-like models, (provided by contest).  We use WeightWatcher (version ww0.4) to compute the average <code>log_spectral_norm</code>, and compare to. the reported test accuracies for the contest <code><strong>task2_v1</strong></code> set of baseline VVG-like models:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png\"><img data-attachment-id=\"13955\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-26-at-9-02-20-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png\" data-orig-size=\"1330,642\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-26-at-9.02.20-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=1024\" alt=\"\" class=\"wp-image-13955\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-9.02.20-pm.png 1330w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption><strong>task1_v4</strong>: 96 models trained on CIFAR10:&nbsp;<em>like VGG models</em></figcaption></figure></div>\n\n\n\n<p>For more details, please see the contest website details, and/or our <a href=\"https://charlesmartin14.github.io/contest-postmortem/intro.html\">contest post-mortem Jupyter Book </a>and paper on the contest (coming soon). </p>\n\n\n\n<p>Notice that the <font color=\"blue\"><code>2xx</code> models</font> have the best test accuracies, and, correspondingly, as a group, the smallest  <code>avg logspectralnorm</code>.  Smaller error correlates with the smaller norm metric.  Likewise, the <font color=\"teal\"><code>6xx</code> models</font> models have. the smallest Test Accuracies as. a group, and also, the largest <code>avg logspectralnorm</code>.  </p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png\"><img data-attachment-id=\"13950\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-21-at-9-21-04-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png\" data-orig-size=\"1448,1240\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-21-at-9.21.04-am\" data-image-description=\"<p>Simpson&#8217;s Paradox for the Spectral Norm metric, applied to a series of VGG-like models.</p>\n\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=1024\" alt=\"\" class=\"wp-image-13950\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-9.21.04-am.png 1448w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure></div>\n\n\n\n<p class=\"has-text-align-center\"><strong><em>This is a classic example of Simpson&#8217;s Paradox.</em></strong></p>\n\n\n\n<p>However, also note that, for each model group (<code>2xx, 10xx, 6xx, & 9xx</code>), we can draw, roughly, a straight line that shows most of the test accuracies in that group are anti-correlated with the <code>avg. log_spectral_norm</code> <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;log_{10}&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}&#92;rangle\" class=\"latex\" />.  Now the regression is not always great, and there are outliers, but we think the general trends hold well enough for this level of discussion (and we will drill into the details in our next paper).</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"alignright is-resized\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/220px-Simpsons_paradox_-_animation.gif\" alt=\"\" width=\"307\" height=\"218\" /><figcaption>Simpson&#8217;s Paradox</figcaption></figure></div>\n\n\n\n<p>This is a classic example of <a href=\"https://en.wikipedia.org/wiki/Simpson%27s_paradox#:~:text=Simpson's%20paradox%2C%20which%20also%20goes,when%20these%20groups%20are%20combined.\">Simpon&#8217;s Paradox</a>.  </p>\n\n\n\n<p>Here, we see a large trend, across the similar models, trained on the same dataset, but with different, depths. </p>\n\n\n\n<p>When looking closely at each model group, however, we see the reverse trend. </p>\n\n\n\n<p>This makes the Spectral Norm difficult to use as a general purpose metric for predicting test accuracies. <br></p>\n\n\n\n<p><strong>WeightWatcher to the Rescue</strong></p>\n\n\n\n<p>Using WeightWatcher, however, we can repair the average log Spectral Norm metric by computing it as a weighted average&#8211;weighted by the WeightWatcher <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> metric.</p>\n\n\n\n<p>Here is a similar plot on the same<strong> task2_v1 </strong>data,, but this time reporting the WeightWatcher average power law metric <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />.  Notice that <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> is well correlated within each model group, as expected when changing model hyperparameters.  Moreover, the <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> is not correlated with the Test Accuracy more broadly across different depths, nor is it correlated with the average log Spectral Norm (not shown).</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png\"><img data-attachment-id=\"13969\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-21-at-11-26-32-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png\" data-orig-size=\"1392,1300\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-21-at-11.26.32-am\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=1024\" alt=\"\" class=\"wp-image-13969\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-21-at-11.26.32-am.png 1392w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure></div>\n\n\n\n<p>WeightWatcher alpha tells us how correlated a single DNN model is.  And we can use to correct the average <code>log_spectral_norm</code> by simply taking a weighted average (called <code>alpha_weighted</code>):</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}:=&#92;dfrac{1}{N_L}&#92;sum_{l}^{N_L}&#92;alpha_{l}&#92;log_{10}&#92;lambda^{max}_{l}\" class=\"latex\" /></p>\n\n\n\n<p>If we look closely, we can see more in more detail how the  weighted alpha <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" /> corrects the average <code>log_spectral_norm </code>metric in the VGG architecture series.   Below we use WeightWatcher to plot the different metrics vs. the Top 1 Test Accuracy for the many different pre-trained VGG models.</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png\"><img loading=\"lazy\" data-attachment-id=\"13978\" data-permalink=\"https://calculatedcontent.com/screen-shot-2020-11-26-at-10-28-21-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png\" data-orig-size=\"2136,680\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2020-11-26-at-10.28.21-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=1024\" alt=\"\" class=\"wp-image-13978\" width=\"767\" height=\"243\" srcset=\"https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=763 763w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=1527 1527w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2020/11/screen-shot-2020-11-26-at-10.28.21-pm.png?w=768 768w\" sizes=\"(max-width: 767px) 100vw, 767px\" /></a><figcaption>WeightWatcher average metrics vs. Top1 Test Accuracy for pretrained VGG models<br>(VGG11, VGG13, VGG16, VGG19, with/out BatchNorm, trained on Imagenet-1K)</figcaption></figure>\n\n\n\n<p>Consider the plot on the far right, and, specifically, the <font color=\"pink\"> <strong>pink (BN-VGG-16)</strong> </font> and <font color=\"red\"><strong>red dots (VGG-19)</strong></font>, near test accuracy ~ 26.&nbsp;These are 2 models with both different depths (16 vs 19 layers) and different hyperparameter settings (BatchNorm or not).  The two models have nearly the same accuracy,  but a large variance between their average <code>log_spectral_norm</code>. Now consider the far left plot for average alpha <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" />, which shows that and <font color=\"red\"><strong>red</strong> </font> has a smaller alpha than the <font color=\"pink\"> <strong>pink</strong></font> .  The <font color=\"red\"><strong>VGG-19</strong> </font>  model is more strongly correlated than <font color=\"pink\"> <strong>BN_VGG-16</strong></font>.   The average alpha for the <font color=\"red\"><strong>red dot is ~3.5</strong></font>, whereas the <font color=\"pink\"><strong>pink dot ~ 3.85</strong></font>.  When we combine these 2 metrics, on the middle plot (alpha_weighted), the 2 models now appear much closer together. So <code>alpha_weighted</code> metric corrects the average <code>log_spectral_norm</code>, reducing the variance between similar models, and  making <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" /> more suitable to treating models of different depth and different hyperparameter settings.</p>\n\n\n\n<h4><strong>Summary</strong></h4>\n\n\n\n<p>The open source WeightWatcher tool provides metrics for Deep Neural Networks that allow the user to predict (trends in) the test accuracies of Deep Neural Networks without needing the test data. The different (power law) metrics, <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" /> apply to models with different hyperparameter settings, and different depths, resp.  Here, we explain why.</p>\n\n\n\n<p>The <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" /> average alpha metric describes the amount of correlation contained in the DNN weight matrices.  Smaller <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" /> correlates with better test accuracy for a single model with different hyperparemeter settings.  It is a unique metric, developed from the theory on strongly correlated systems from theoretical chemistry and physics</p>\n\n\n\n<p>The <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" /> average weighted alpha metric is suited for treating a series of models with different depths, like the VGG series: VGG11, VGG13, VGG16, VGG19.  It is a weighted average of the log Spectral Norm. </p>\n\n\n\n<p>To explain why  <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Calpha%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;alpha&#92;rangle\" class=\"latex\" /> works, we have reviewed the theory and application of Spectral Norm Regularization, and the use of the average log Spectral Norm  <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}&#92;rangle\" class=\"latex\" /> as a metric for predicting DNN test accuracies.</p>\n\n\n\n<p>While theory suggests that  <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clangle%5Clog_%7B10%7D%5CVert%5Cmathbf%7BW%7D%5CVert%5E%7B2%7D_%7B%5Cinfty%7D%5Crangle&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;langle&#92;log_{10}&#92;Vert&#92;mathbf{W}&#92;Vert^{2}_{&#92;infty}&#92;rangle\" class=\"latex\" /> might be able to predict the generalization performance of different pre-trained DNNs, in practice, it is correlated with test error for models with different depths, and anti-correlated for models trained with different hyperparameters.  This is a classic example of Simpson&#8217;s Paradox.</p>\n\n\n\n<p>We show that we can fix-up the average <code>log_spectral_norm</code>  (as provided in WeightWatcher) by using a weighted average, weighted by the  WeightWatcher power-law layer alpha <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> metric.  And this is exactly the WeightWatcher metric  <code>alpha_weighted</code>:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Cdfrac%7B1%7D%7BN_L%7D%5Csum_%7Bl%7D%5E%7BN_L%7D%5Calpha_%7Bl%7D%5Clog_%7B10%7D%5Clambda%5E%7Bmax%7D_%7Bl%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}:=&#92;dfrac{1}{N_L}&#92;sum_{l}^{N_L}&#92;alpha_{l}&#92;log_{10}&#92;lambda^{max}_{l}\" class=\"latex\" /></p>\n\n\n\n<p>Try it yourself on your own DNN models.</p>\n\n\n\n<p class=\"has-text-align-center\"><code>pip install weightwatcher </code></p>\n\n\n\n<p>And let me know how it goes.</p>\n\n\n\n<h4><strong>Appendix</strong></h4>\n\n\n\n<p><strong>Spectral Density of 2D Convolutional Layers</strong></p>\n\n\n\n<p>We can test this theory numerically using WeightWatcher.  Notice, however, that while it is obvious how to define <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%5E%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda^{max}\" class=\"latex\" /> for Dense matrix, there is some ambiguity doing this for a 2DConvolution and to make this work as a useful metric.  </p>\n\n\n\n<p>WeightWatcher has 2 methods for computing the SVD  of a Conv2D layer, depending on the version.  For a <img src=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k%5Ctimes+N%5Ctimes+M+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k%5Ctimes+N%5Ctimes+M+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%5Ctimes+k%5Ctimes+N%5Ctimes+M+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k&#92;times k&#92;times N&#92;times M \" class=\"latex\" /> Conv2D layer, the options are</p>\n\n\n\n<ul><li><strong>new version ww.0.4:</strong>  extract <img src=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k&#92;times k\" class=\"latex\" /> matrix slices of dimension <img src=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"N&#92;times M\" class=\"latex\" />,  and combine these to define the final <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}^{Conv2D}\" class=\"latex\" /> matrix, and run SVD on this.  This gives 1 ESD per Conv2D layer.</li><li><strong>old version ww2x</strong>: extract <img src=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k&#92;times k\" class=\"latex\" /> matricies <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}^{Conv2D}_{i}\" class=\"latex\" /> of dimension <img src=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=N%5Ctimes+M&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"N&#92;times M\" class=\"latex\" /> each, and run SVD on each <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BConv2D%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}^{Conv2D}_{i}\" class=\"latex\" />.  This gives <img src=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k%5Ctimes+k&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k&#92;times k\" class=\"latex\" />  ESDs per Conv2D layer, and the layer metrics are then averaged over all of these slices.   </li><li><strong>future version (maybe)</strong>:  Run SVD on the linear operator that defines the Conv2D transform.  This requires running SVD on the discrete FFT on each of the Conv2D  input-output channels, and then combining the resulting eigenvalues into 1 very large ESD.  This is quite slow and the numerical results were not as good in early experiments.</li></ul>\n\n\n\n<p>All three methods give slightly different layer Spectral Norms,  with ww0.4 being the best estimator so far.  The<code> ww2x=True </code>option is included for back compatibility with earlier papers.</p>\n\n\n\n<p></p>\n\n\n\n<p><br></p>\n",
  "wfw:commentRss": "https://calculatedcontent.com/2020/11/26/simpsons-paradox-and-deep-learning-metrics-with-weightwatcher/feed/",
  "slash:comments": 1,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "screen-shot-2020-11-21-at-9.21.04-am"
    },
    {
      "media:title": "charlesmartin14"
    },
    "",
    "",
    {
      "media:title": "image"
    },
    "",
    "",
    "",
    "",
    "",
    ""
  ]
}