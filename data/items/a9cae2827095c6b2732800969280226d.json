{
  "id": "tag:blogger.com,1999:blog-4446292666398344382.post-8841652459027202143",
  "published": "2016-12-16T05:19:00.000-08:00",
  "updated": "2017-01-27T17:47:54.207-08:00",
  "category": [
    "",
    ""
  ],
  "title": "Dialogue Workshop Recap",
  "content": "Most of the speakers have sent me their slides, which can be found on <a href=\"http://letsdiscussnips2016.weebly.com/schedule.html\">the schedule page</a>.  Overall the workshop was fun and enlightening.  Here are some major themes that I picked up upon.<br /><br /><b>Evaluation</b> There is no magic bullet, but check out <a href=\"https://github.com/pmineiro/ldlmd2016/blob/master/NIPSDec2016H.Hastie.pdf\">Helen's slides</a> for a nicely organized discussion of metrics.  Many different strategies were on display in the workshop:<br /><ul><li>Milica Gasic utilized crowdsourcing for some of her experiments.  She also indicated the incentives of crowdsourcing can lead to unnatural participant behaviours.</li><li>Nina Dethlefs used a combination of objective (BLEU) and subjective (“naturalness”) evaluation.</li><li>Vlad Serban has been a proponent of next utterance classification as a useful intrinsic metric.</li><li>Antoine Bordes (and the other FAIR folks) are heavily leveraging simulation and engineered tasks.</li><li>Jason Williams used imitation metrics (from hand labeled dialogs) as well as simulation.</li></ul>As Helen points out, computing metrics from customer behaviour is probably the gold standard for industrial task-oriented systems, but this is a scarce resource.  (Even within the company that has the customer relationship, by the way: at my current gig they will not let me flight something without demonstrating limited negative customer experience impact.)<br /><br />Those who have been around longer than I have experienced several waves of enthusiasm and pessimism regarding simulation for dialogue.  Overall I think the takeaway is that simulation can be useful tool, as long as one is cognizant of the limitations.<br /><br />Antoine quickly adapted his talk to Nina's with a fun slide that said “Yes, Nina, we are bringing simulation back.”  The FAIR strategy is something like this: “Here are some engineered dialog tasks that appear to require certain capabilities to perform well, such as multi-hop reasoning, interaction with a knowledge base, long-term memory, etc.  At the moment we have no system that can achieve 100% accuracy on these engineered tasks, so we will use these tasks to drive research into architectures and optimization strategies.  We also monitor performance other external tasks (e.g., DSTC) to see if our learning generalizes beyond the engineered task set.”  Sounds reasonable.<br /><br />Personally, as a result of the workshop, I'm going to invest more heavily in simulators in the near-term.<br /><br /><b>Leveraging Linguistics</b> Fernando Pereira had the killer comment about how linguistics is a descriptive theory which need not have explicit correspondence to implementation: &ldquo;when Mercury goes around the Sun, it is not running General Relativity.&rdquo;  Nonetheless, linguistics seems important not only for describing what behaviours a competent system must capture, but also for motivating and inspiring what kinds of automata we need to achieve it.  <br /><br />Augmenting or generating data sets seems like a natural way to leverage lingustics.  As an example, in the workshop I learned that 4 year old native English speakers are sensitive to proper vs. improper word order given simple sentences containing some nonsense words (but with morphological clues, such as capitalization and -ed suffix).  Consequently, I'm trying a next utterance classification run on a large dialog dataset where some of the negative examples are token-permuted versions of the true continuation, to see if this changes anything.<br /><br />Raquel Fernandez's talk focused on adult-child language interactions, and I couldn't help but think about potential relevance to training artificial systems.  In fact, current dialog systems are acting like the parent (i.e., the expert), e.g., by suggesting reformulations to the user.  But this laughable, because our systems are stupid: shouldn't we be acting like the child?<br /><br />The most extreme use of linguistics was the talk by Eshghi and Kalatzis, where they develop a custom incremental semantic parser for dialog and then use the resulting logical forms to drive the entire dialog process.  Once the parser is built, the amount of training data required is extremely minimal, but the parser is presumably built from looking at a large number of dialogs.<br /><br />Nina Dethlefs discussed some promising experiments with AMR.  I've been scared of AMR personally.  First, it is very expensive to get the annotations.  However, if that were the only problem, we could imagine a human-genome-style push to generate a large number of them.  The bigger problem is the relatively poor inter-annotator agreement (it was just Nina and her students, so they could come to agreement via side communication).  Nonetheless I could imagine a dialog system which is designed and built using a small number of prototypical semantic structures.  It might seem a bit artificial and constrained, but so does the graphical user interface with the current canonical set of UX elements, which users learn to productivity interact with.<br /><br />Angeliki Lazaridou's talk reminded me that communication is fundamentally a cooperative game, which explains why arguing on the internet is a waste of time. <br /><br /><b>Neural Networks: Game Changer?</b>  I asked variants of the following question to every panel: &ldquo;what problems have neural networks mitigated and what problems remain stubbornly unaddressed.&rdquo;  This was, essentially, the content of Marco Baroni's talk.  Overall I would say: there's enthusiasm now that we are no longer afraid of non-convex loss functions (along these lines, check out <a href=\"https://github.com/pmineiro/ldlmd2016/blob/master/letsdiscuss-gmemn2n-julienperez.pdf\">Julien Perez's slides</a>).<br /><br />However, we currently have only vague ideas on how to realize the competencies that are apparently required for high quality dialog.  I say <i>apparently</i> because the history of AI is full of practitioners assuming sufficient capabilities are necessary for some task, and recent advances in machine translation suggest that savant-parrots might be able to do surprisingly well.  In fact, during the discussion period there was some frustration that heuristic hand-coded strategies are still superior to machine learning based approaches, with the anticipation that this may continue to be true for the Alexa prize.  I'm positive about the existence of superior heuristics, however: not only do they provide a source of inspiration and ideas for data-driven approaches, but learning methods that combine imitation learning and reinforcement learning should be able to beneficially exploit them.<br /><br /><b>Entity Annotation</b> Consider the apparently simple and ubiquitous feature engineering strategy: add additional sparse indicator features which indicate semantic equivalence of tokens or token sequences.  So maybe “windows 10” and “windows anniversary edition” both get the same feature.  Jason Williams indicated his system is greatly improved by this, but he's trying to learn from $O(10)$ labeled dialogues, so I nodded.  Antoine Bordes indicated this helps on some bAbI dialog tasks, but those tasks only have $O(1000)$ dialogues, so again I nodded.  Then Vlad Serban indicated this helps for next utterance classification on the Ubuntu Dialog Corpus.  At this point I thought, &ldquo;wait, that's $O(10^5)$ dialogs.&rdquo;<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://4.bp.blogspot.com/-1C3sdb1DXSU/WFPenq4sgsI/AAAAAAAACd4/6C7UvpQqlTQBRwBm5r6s-7L7QFA0PRG9wCLcB/s1600/Blade_Runner_Intro_014.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"133\" src=\"https://4.bp.blogspot.com/-1C3sdb1DXSU/WFPenq4sgsI/AAAAAAAACd4/6C7UvpQqlTQBRwBm5r6s-7L7QFA0PRG9wCLcB/s320/Blade_Runner_Intro_014.jpg\" width=\"320\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Apparently, knowing a turtle and a tortoise are the same thing is tricky.</td></tr></tbody></table>In practice, I'm ok with manual feature engineering: it's how I paid the rent during the linear era.  But now I wonder: does it take much more data to infer such equivalences?  Will we never infer this, no matter how much data, given our current architectures?<br /><br /><b>Spelling</b> The speakers were roughly evenly split between “dialog” and “dialogue”.  I prefer the latter, as it has more panache.",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Paul Mineiro",
    "uri": "http://www.blogger.com/profile/05439062526157173163",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}