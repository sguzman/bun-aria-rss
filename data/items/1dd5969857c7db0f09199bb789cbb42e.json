{
  "title": "Styles of Truncated Backpropagation",
  "link": "",
  "published": "2016-07-19T00:00:00-04:00",
  "updated": "2016-07-19T00:00:00-04:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2016-07-19:/styles-of-truncated-backpropagation.html",
  "summary": "In my post on Recurrent Neural Networks in Tensorflow, I observed that Tensorflow's approach to truncated backpropagation (feeding in truncated subsequences of length n) is qualitatively different than \"backpropagating errors a maximum of n steps\". In this post, I explore the differences, and ask whether one approach is better than the other.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>In my post on <a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\">Recurrent Neural Networks in Tensorflow</a>, I observed that Tensorflow’s approach to truncated backpropagation (feeding in truncated subsequences of length n) is qualitatively different than “backpropagating errors a maximum of n steps”. In this post, I explore the differences, implement a truncated backpropagation algorithm in Tensorflow that maintains the distribution between backpropagated errors, and ask whether one approach is better than the other.</p>\n<p>I conclude that:</p>\n<ul>\n<li>Because a well-implemented evenly-distributed truncated backpropagation algorithm would run about as fast as full backpropagation over the sequence, and full backpropagation performs slightly better, it is most likely not worth implementing such an algorithm.</li>\n<li>The discussion and preliminary experiments in this post show that n-step Tensorflow-style truncated backprop (i.e., with num_steps = n) does not effectively backpropagate errors the full n-steps. Thus, if you are using Tensorflow-style truncated backpropagation and need to capture n-step dependencies, you may benefit from using a num_steps that is appreciably higher than n in order to effectively backpropagate errors the desired n steps.</li>\n</ul>\n<h3 id=\"differences-in-styles-of-truncated-backpropagation\">Differences in styles of truncated backpropagation</h3>\n<p>Suppose we are training an RNN on sequences of length 10,000. If we apply non-truncated backpropagation through time, the entire sequence is fed into the network at once, the error at time step 10,000 will be back propagated all the way back to time step 1. The two problems with this are that it is (1) expensive to backpropagate the error so many steps, and (2) due to vanishing gradients, backpropagated errors get smaller and smaller layer by layer, which makes further backpropagation insignificant.</p>\n<p>To deal with this, we might implement “truncated” backpropagation. A good description of truncated backpropagation is provided in Section 2.8.6 of <a href=\"http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf\">Ilya Sutskever’s Ph.D. thesis</a>:</p>\n<blockquote>\n<p>“[Truncated backpropagation] processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps…”</p>\n</blockquote>\n<p>Tensorflow-style truncated backpropagation uses k1 = k2 (= num_steps). See <a href=\"https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#truncated-backpropagation\">Tensorflow api docs</a>. The question this post addresses is whether setting k1 = 1 achieves better results. I will deem this “true” truncated backpropagation, since every error that can be backpropagated k2 steps is backpropagated the full k2 steps.</p>\n<p>To understand why these two approaches are qualitatively different, consider how they differ on sequences of length 49 with backpropagation of errors truncated to 7 steps. In both, every error is backpropagated to the weights at the current timestep. However, in Tensorflow-style truncated backpropagation, the sequence is broken into 7 subsequences, each of length 7, and only 7 over the errors are backpropagated 7 steps. In “true” truncated backpropagation, 42 of the errors can be backpropagated for 7 steps, and 42 are. This may lead to different results because the ratio of 7-step to 1-step errors used to update the weights is significantly different.</p>\n<p>To visualize the difference, here is how true truncated backpropagation looks on a sequence of length 6 with errors truncated to 3 steps:</p>\n<figure>\n<img src=\"https://r2rt.com/static/images/RNN_true_truncated_backprop.png\" alt=\"Diagram of True Truncated Backpropagation\" /><figcaption>Diagram of True Truncated Backpropagation</figcaption>\n</figure>\n<p>And here is how Tensorflow-style truncated backpropagation looks on the same sequence:</p>\n<figure>\n<img src=\"https://r2rt.com/static/images/RNN_tf_truncated_backprop.png\" alt=\"Diagram of Tensorflow Truncated Backpropagation\" /><figcaption>Diagram of Tensorflow Truncated Backpropagation</figcaption>\n</figure>\n<h3 id=\"experiment-design\">Experiment design</h3>\n<p>To compare the performance of the two algorithms, I write implement a “true” truncated backpropagation algorithm and compare results. The algorithms are compared on a vanilla-RNN, based on the one used in my prior post, <a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\">Recurrent Neural Networks in Tensorflow I</a>, except that I upgrade the task and model complexity, since the basic model from my prior post learned the simple patterns in the toy dataset very quickly. The task will be language modeling on the ptb dataset, and to match the increased complexity of this task, I add an embedding layer and dropout to the basic RNN model.</p>\n<p>I compare the best performance of each algorithm on the validation set after 20 epochs for the cases below. In each case, I use an AdamOptimizer (it does better than other optimizers in preliminary tests) and learning rates of 0.003, 0.001 and 0.0003.</p>\n<p><em>5-step truncated backpropagation</em></p>\n<ul>\n<li>True, sequences of length 20</li>\n<li>TF-style</li>\n</ul>\n<p><em>10-step truncated backpropagation</em></p>\n<ul>\n<li>True, sequences of length 30</li>\n<li>TF-style</li>\n</ul>\n<p><em>20-step truncated backpropagation</em></p>\n<ul>\n<li>True, sequences of length 40</li>\n<li>TF-style</li>\n</ul>\n<p><em>40-step truncated backpropagation</em></p>\n<ul>\n<li>TF-style</li>\n</ul>\n<h3 id=\"code\">Code</h3>\n<h4 id=\"imports-and-data-generators\">Imports and data generators</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np\n<span class=\"im\">import</span> tensorflow <span class=\"im\">as</span> tf\n<span class=\"op\">%</span>matplotlib inline\n<span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt\n<span class=\"im\">from</span> tensorflow.models.rnn.ptb <span class=\"im\">import</span> reader\n\n<span class=\"co\">#data from http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</span>\nraw_data <span class=\"op\">=</span> reader.ptb_raw_data(<span class=\"st\">&#39;ptb_data&#39;</span>)\ntrain_data, val_data, test_data, num_classes <span class=\"op\">=</span> raw_data\n\n<span class=\"kw\">def</span> gen_epochs(n, num_steps, batch_size):\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(n):\n        <span class=\"cf\">yield</span> reader.ptb_iterator(train_data, batch_size, num_steps)</code></pre></div>\n<h4 id=\"model\">Model</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_graph(num_steps,\n            bptt_steps <span class=\"op\">=</span> <span class=\"dv\">4</span>, batch_size <span class=\"op\">=</span> <span class=\"dv\">200</span>, num_classes <span class=\"op\">=</span> num_classes,\n            state_size <span class=\"op\">=</span> <span class=\"dv\">4</span>, embed_size <span class=\"op\">=</span> <span class=\"dv\">50</span>, learning_rate <span class=\"op\">=</span> <span class=\"fl\">0.01</span>):\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Builds graph for a simple RNN</span>\n\n<span class=\"co\">    Notable parameters:</span>\n<span class=\"co\">    num_steps: sequence length / steps for TF-style truncated backprop</span>\n<span class=\"co\">    bptt_steps: number of steps for true truncated backprop</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n\n    g <span class=\"op\">=</span> tf.get_default_graph()\n\n    <span class=\"co\"># placeholders</span>\n    x <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, <span class=\"va\">None</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;input_placeholder&#39;</span>)\n    y <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, <span class=\"va\">None</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;labels_placeholder&#39;</span>)\n    default_init_state <span class=\"op\">=</span> tf.zeros([batch_size, state_size])\n    init_state <span class=\"op\">=</span> tf.placeholder_with_default(default_init_state,\n                                             [batch_size, state_size], name<span class=\"op\">=</span><span class=\"st\">&#39;state_placeholder&#39;</span>)\n    dropout <span class=\"op\">=</span> tf.placeholder(tf.float32, [], name<span class=\"op\">=</span><span class=\"st\">&#39;dropout_placeholder&#39;</span>)\n\n    x_one_hot <span class=\"op\">=</span> tf.one_hot(x, num_classes)\n    x_as_list <span class=\"op\">=</span> [tf.squeeze(i, squeeze_dims<span class=\"op\">=</span>[<span class=\"dv\">1</span>]) <span class=\"cf\">for</span> i <span class=\"kw\">in</span> tf.split(<span class=\"dv\">1</span>, num_steps, x_one_hot)]\n\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;embeddings&#39;</span>):\n        embeddings <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;embedding_matrix&#39;</span>, [num_classes, embed_size])\n\n    <span class=\"kw\">def</span> embedding_lookup(one_hot_input):\n        <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;embeddings&#39;</span>, reuse<span class=\"op\">=</span><span class=\"va\">True</span>):\n            embeddings <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;embedding_matrix&#39;</span>, [num_classes, embed_size])\n            embeddings <span class=\"op\">=</span> tf.identity(embeddings)\n            g.add_to_collection(<span class=\"st\">&#39;embeddings&#39;</span>, embeddings)\n            <span class=\"cf\">return</span> tf.matmul(one_hot_input, embeddings)\n\n    rnn_inputs <span class=\"op\">=</span> [embedding_lookup(i) <span class=\"cf\">for</span> i <span class=\"kw\">in</span> x_as_list]\n\n    <span class=\"co\">#apply dropout to inputs</span>\n    rnn_inputs <span class=\"op\">=</span> [tf.nn.dropout(x, dropout) <span class=\"cf\">for</span> x <span class=\"kw\">in</span> rnn_inputs]\n\n    <span class=\"co\"># rnn_cells</span>\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;rnn_cell&#39;</span>):\n        W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W&#39;</span>, [embed_size <span class=\"op\">+</span> state_size, state_size])\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>, [state_size], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n\n    <span class=\"kw\">def</span> rnn_cell(rnn_input, state):\n        <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;rnn_cell&#39;</span>, reuse<span class=\"op\">=</span><span class=\"va\">True</span>):\n\n            W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W&#39;</span>, [embed_size <span class=\"op\">+</span> state_size, state_size])\n            W <span class=\"op\">=</span> tf.identity(W)\n            g.add_to_collection(<span class=\"st\">&#39;Ws&#39;</span>, W)\n\n            b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>, [state_size], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n            b <span class=\"op\">=</span> tf.identity(b)\n            g.add_to_collection(<span class=\"st\">&#39;bs&#39;</span>, b)\n\n            <span class=\"cf\">return</span> tf.tanh(tf.matmul(tf.concat(<span class=\"dv\">1</span>, [rnn_input, state]), W) <span class=\"op\">+</span> b)\n\n    state <span class=\"op\">=</span> init_state\n    rnn_outputs <span class=\"op\">=</span> []\n    <span class=\"cf\">for</span> rnn_input <span class=\"kw\">in</span> rnn_inputs:\n        state <span class=\"op\">=</span> rnn_cell(rnn_input, state)\n        rnn_outputs.append(state)\n\n    <span class=\"co\">#apply dropout to outputs</span>\n    rnn_outputs <span class=\"op\">=</span> [tf.nn.dropout(x, dropout) <span class=\"cf\">for</span> x <span class=\"kw\">in</span> rnn_outputs]\n\n    final_state <span class=\"op\">=</span> rnn_outputs[<span class=\"op\">-</span><span class=\"dv\">1</span>]\n\n    <span class=\"co\">#logits and predictions</span>\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;softmax&#39;</span>):\n        W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W_softmax&#39;</span>, [state_size, num_classes])\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b_softmax&#39;</span>, [num_classes], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n    logits <span class=\"op\">=</span> [tf.matmul(rnn_output, W) <span class=\"op\">+</span> b <span class=\"cf\">for</span> rnn_output <span class=\"kw\">in</span> rnn_outputs]\n    predictions <span class=\"op\">=</span> [tf.nn.softmax(logit) <span class=\"cf\">for</span> logit <span class=\"kw\">in</span> logits]\n\n    <span class=\"co\">#losses</span>\n    y_as_list <span class=\"op\">=</span> [tf.squeeze(i, squeeze_dims<span class=\"op\">=</span>[<span class=\"dv\">1</span>]) <span class=\"cf\">for</span> i <span class=\"kw\">in</span> tf.split(<span class=\"dv\">1</span>, num_steps, y)]\n    losses <span class=\"op\">=</span> [tf.nn.sparse_softmax_cross_entropy_with_logits(logit,label) <span class=\"op\">\\</span>\n              <span class=\"cf\">for</span> logit, label <span class=\"kw\">in</span> <span class=\"bu\">zip</span>(logits, y_as_list)]\n    total_loss <span class=\"op\">=</span> tf.reduce_mean(losses)\n\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Implementation of true truncated backprop using TF&#39;s high-level gradients function.</span>\n\n<span class=\"co\">    Because I add gradient-ops for each error, this are a number of duplicate operations,</span>\n<span class=\"co\">    making this a slow implementation. It would be considerably more effort to write an</span>\n<span class=\"co\">    efficient implementation, however, so for testing purposes, it&#39;s OK that this goes slow.</span>\n\n<span class=\"co\">    An efficient implementation would still require all of the same operations as the full</span>\n<span class=\"co\">    backpropagation through time of errors in a sequence, and so any advantage would not come</span>\n<span class=\"co\">    from speed, but from having a better distribution of backpropagated errors.</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n\n    embed_by_step <span class=\"op\">=</span> g.get_collection(<span class=\"st\">&#39;embeddings&#39;</span>)\n    Ws_by_step <span class=\"op\">=</span> g.get_collection(<span class=\"st\">&#39;Ws&#39;</span>)\n    bs_by_step <span class=\"op\">=</span> g.get_collection(<span class=\"st\">&#39;bs&#39;</span>)\n\n    <span class=\"co\"># Collect gradients for each step in a list</span>\n    embed_grads <span class=\"op\">=</span> []\n    W_grads <span class=\"op\">=</span> []\n    b_grads <span class=\"op\">=</span> []\n\n    <span class=\"co\"># Keeping track of vanishing gradients for my own curiousity</span>\n    vanishing_grad_list <span class=\"op\">=</span> []\n\n    <span class=\"co\"># Loop through the errors, and backpropagate them to the relevant nodes</span>\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(num_steps):\n        start <span class=\"op\">=</span> <span class=\"bu\">max</span>(<span class=\"dv\">0</span>,i<span class=\"op\">+</span><span class=\"dv\">1</span><span class=\"op\">-</span>bptt_steps)\n        stop <span class=\"op\">=</span> i<span class=\"op\">+</span><span class=\"dv\">1</span>\n        grad_list <span class=\"op\">=</span> tf.gradients(losses[i],\n                                 embed_by_step[start:stop] <span class=\"op\">+\\</span>\n                                 Ws_by_step[start:stop] <span class=\"op\">+\\</span>\n                                 bs_by_step[start:stop])\n        embed_grads <span class=\"op\">+=</span> grad_list[<span class=\"dv\">0</span> : stop <span class=\"op\">-</span> start]\n        W_grads <span class=\"op\">+=</span> grad_list[stop <span class=\"op\">-</span> start : <span class=\"dv\">2</span> <span class=\"op\">*</span> (stop <span class=\"op\">-</span> start)]\n        b_grads <span class=\"op\">+=</span> grad_list[<span class=\"dv\">2</span> <span class=\"op\">*</span> (stop <span class=\"op\">-</span> start) : ]\n\n        <span class=\"cf\">if</span> i <span class=\"op\">&gt;=</span> bptt_steps:\n            vanishing_grad_list.append(grad_list[stop <span class=\"op\">-</span> start : <span class=\"dv\">2</span> <span class=\"op\">*</span> (stop <span class=\"op\">-</span> start)])\n\n    grad_embed <span class=\"op\">=</span> tf.add_n(embed_grads) <span class=\"op\">/</span> (batch_size <span class=\"op\">*</span> bptt_steps)\n    grad_W <span class=\"op\">=</span> tf.add_n(W_grads) <span class=\"op\">/</span> (batch_size <span class=\"op\">*</span> bptt_steps)\n    grad_b <span class=\"op\">=</span> tf.add_n(b_grads) <span class=\"op\">/</span> (batch_size <span class=\"op\">*</span> bptt_steps)\n\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Training steps</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n\n    opt <span class=\"op\">=</span> tf.train.AdamOptimizer(learning_rate)\n    grads_and_vars_tf_style <span class=\"op\">=</span> opt.compute_gradients(total_loss, tf.trainable_variables())\n    grads_and_vars_true_bptt <span class=\"op\">=</span> <span class=\"op\">\\</span>\n        [(grad_embed, tf.trainable_variables()[<span class=\"dv\">0</span>]),\n         (grad_W, tf.trainable_variables()[<span class=\"dv\">1</span>]),\n         (grad_b, tf.trainable_variables()[<span class=\"dv\">2</span>])] <span class=\"op\">+</span> <span class=\"op\">\\</span>\n        opt.compute_gradients(total_loss, tf.trainable_variables()[<span class=\"dv\">3</span>:])\n    train_tf_style <span class=\"op\">=</span> opt.apply_gradients(grads_and_vars_tf_style)\n    train_true_bptt <span class=\"op\">=</span> opt.apply_gradients(grads_and_vars_true_bptt)\n\n    <span class=\"cf\">return</span> <span class=\"bu\">dict</span>(\n        train_tf_style <span class=\"op\">=</span> train_tf_style,\n        train_true_bptt <span class=\"op\">=</span> train_true_bptt,\n        gvs_tf_style <span class=\"op\">=</span> grads_and_vars_tf_style,\n        gvs_true_bptt <span class=\"op\">=</span> grads_and_vars_true_bptt,\n        gvs_gradient_check <span class=\"op\">=</span> opt.compute_gradients(losses[<span class=\"op\">-</span><span class=\"dv\">1</span>], tf.trainable_variables()),\n        loss <span class=\"op\">=</span> total_loss,\n        final_state <span class=\"op\">=</span> final_state,\n        x<span class=\"op\">=</span>x,\n        y<span class=\"op\">=</span>y,\n        init_state<span class=\"op\">=</span>init_state,\n        dropout<span class=\"op\">=</span>dropout,\n        vanishing_grads<span class=\"op\">=</span>vanishing_grad_list\n    )</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> reset_graph():\n    <span class=\"cf\">if</span> <span class=\"st\">&#39;sess&#39;</span> <span class=\"kw\">in</span> <span class=\"bu\">globals</span>() <span class=\"kw\">and</span> sess:\n        sess.close()\n    tf.reset_default_graph()</code></pre></div>\n<h3 id=\"some-quick-tests\">Some quick tests</h3>\n<h4 id=\"timing-test\">Timing test</h4>\n<p>As expected, my implementation of true BPTT is slow as there are duplicate operations being performed. An efficient implementation would run at roughly the same speed as the full backpropagation.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">reset_graph()\ng <span class=\"op\">=</span> build_graph(num_steps <span class=\"op\">=</span> <span class=\"dv\">40</span>, bptt_steps <span class=\"op\">=</span> <span class=\"dv\">20</span>)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n\nX, Y <span class=\"op\">=</span> <span class=\"bu\">next</span>(reader.ptb_iterator(train_data, batch_size<span class=\"op\">=</span><span class=\"dv\">200</span>, num_steps<span class=\"op\">=</span><span class=\"dv\">40</span>))</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"op\">%%</span>timeit\ngvs_bptt <span class=\"op\">=</span> sess.run(g[<span class=\"st\">&#39;gvs_true_bptt&#39;</span>], feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]:X, g[<span class=\"st\">&#39;y&#39;</span>]:Y, g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"dv\">1</span>})</code></pre></div>\n<pre><code>10 loops, best of 3: 173 ms per loop</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"op\">%%</span>timeit\ngvs_tf <span class=\"op\">=</span> sess.run(g[<span class=\"st\">&#39;gvs_tf_style&#39;</span>], feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]:X, g[<span class=\"st\">&#39;y&#39;</span>]:Y, g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"dv\">1</span>})</code></pre></div>\n<pre><code>10 loops, best of 3: 80.2 ms per loop</code></pre>\n<h4 id=\"vanshing-gradients-demonstration\">Vanshing gradients demonstration</h4>\n<p>To demonstrate the vanishing gradient problem, I collected this information. As you can see, the gradients vanish very quickly, decreasing by a factor of of about 3-4 at each step.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">vanishing_grads, gvs <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;vanishing_grads&#39;</span>], g[<span class=\"st\">&#39;gvs_true_bptt&#39;</span>]],\n                                feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]:X, g[<span class=\"st\">&#39;y&#39;</span>]:Y, g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"dv\">1</span>})\nvanishing_grads <span class=\"op\">=</span> np.array(vanishing_grads)\nweights <span class=\"op\">=</span> gvs[<span class=\"dv\">1</span>][<span class=\"dv\">1</span>]\n\n<span class=\"co\"># sum all the grads from each loss node</span>\nvanishing_grads <span class=\"op\">=</span> np.<span class=\"bu\">sum</span>(vanishing_grads, axis<span class=\"op\">=</span><span class=\"dv\">0</span>)\n\n<span class=\"co\"># now calculate the l1 norm at each bptt step</span>\nvanishing_grads <span class=\"op\">=</span> np.<span class=\"bu\">sum</span>(np.<span class=\"bu\">sum</span>(np.<span class=\"bu\">abs</span>(vanishing_grads),axis<span class=\"op\">=</span><span class=\"dv\">1</span>),axis<span class=\"op\">=</span><span class=\"dv\">1</span>)\n\nvanishing_grads</code></pre></div>\n<pre><code>array([  5.28676978e-08,   1.51207473e-07,   4.04591049e-07,\n         1.55859300e-06,   5.00411124e-06,   1.32292716e-05,\n         3.94736344e-05,   1.17605050e-04,   3.37805774e-04,\n         1.01710076e-03,   2.74375151e-03,   8.92040879e-03,\n         2.23708227e-02,   7.23497868e-02,   2.45202959e-01,\n         7.39126682e-01,   2.19093657e+00,   6.16793633e+00,\n         2.27248211e+01,   9.78200531e+01], dtype=float32)</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"bu\">len</span>(vanishing_grads) <span class=\"op\">-</span> <span class=\"dv\">1</span>):\n    <span class=\"bu\">print</span>(vanishing_grads[i<span class=\"op\">+</span><span class=\"dv\">1</span>] <span class=\"op\">/</span> vanishing_grads[i])</code></pre></div>\n<pre><code>2.86011\n2.67573\n3.85227\n3.21066\n2.64368\n2.98381\n2.97933\n2.87237\n3.0109\n2.69762\n3.25117\n2.50782\n3.23411\n3.38913\n3.01435\n2.96422\n2.81521\n3.68435\n4.30455</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plt.plot(vanishing_grads)</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/RNN_output_19_1.png\" alt=\"Plot of Vanishing Gradients\" /><figcaption>Plot of Vanishing Gradients</figcaption>\n</figure>\n<h4 id=\"quick-accuracy-test\">Quick accuracy test</h4>\n<p>A sanity check to make sure the true truncated backpropagation algorithm is doing the right thing.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"co\"># first test using bptt_steps &gt;= num_steps</span>\n\nreset_graph()\ng <span class=\"op\">=</span> build_graph(num_steps <span class=\"op\">=</span> <span class=\"dv\">7</span>, bptt_steps <span class=\"op\">=</span> <span class=\"dv\">7</span>)\nX, Y <span class=\"op\">=</span> <span class=\"bu\">next</span>(reader.ptb_iterator(train_data, batch_size<span class=\"op\">=</span><span class=\"dv\">200</span>, num_steps<span class=\"op\">=</span><span class=\"dv\">7</span>))\n\n<span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n    sess.run(tf.initialize_all_variables())\n    gvs_bptt, gvs_tf <span class=\"op\">=\\</span>\n        sess.run([g[<span class=\"st\">&#39;gvs_true_bptt&#39;</span>],g[<span class=\"st\">&#39;gvs_tf_style&#39;</span>]],\n                                feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]:X, g[<span class=\"st\">&#39;y&#39;</span>]:Y, g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"fl\">0.8</span>})\n\n<span class=\"co\"># assert embedding gradients are the same</span>\n<span class=\"cf\">assert</span>(np.<span class=\"bu\">max</span>(gvs_bptt[<span class=\"dv\">0</span>][<span class=\"dv\">0</span>] <span class=\"op\">-</span> gvs_tf[<span class=\"dv\">0</span>][<span class=\"dv\">0</span>]) <span class=\"op\">&lt;</span> <span class=\"fl\">1e-4</span>)\n<span class=\"co\"># assert weight gradients are the same</span>\n<span class=\"cf\">assert</span>(np.<span class=\"bu\">max</span>(gvs_bptt[<span class=\"dv\">1</span>][<span class=\"dv\">0</span>] <span class=\"op\">-</span> gvs_tf[<span class=\"dv\">1</span>][<span class=\"dv\">0</span>]) <span class=\"op\">&lt;</span> <span class=\"fl\">1e-4</span>)\n<span class=\"co\"># assert bias gradients are the same</span>\n<span class=\"cf\">assert</span>(np.<span class=\"bu\">max</span>(gvs_bptt[<span class=\"dv\">2</span>][<span class=\"dv\">0</span>] <span class=\"op\">-</span> gvs_tf[<span class=\"dv\">2</span>][<span class=\"dv\">0</span>]) <span class=\"op\">&lt;</span> <span class=\"fl\">1e-4</span>)</code></pre></div>\n<h3 id=\"experiment\">Experiment</h3>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">Train the network</span>\n<span class=\"co\">&quot;&quot;&quot;</span>\n\n<span class=\"kw\">def</span> train_network(num_epochs,\n                  num_steps,\n                  use_true_bptt,\n                  batch_size <span class=\"op\">=</span> <span class=\"dv\">200</span>,\n                  bptt_steps <span class=\"op\">=</span> <span class=\"dv\">7</span>,\n                  state_size <span class=\"op\">=</span> <span class=\"dv\">4</span>,\n                  learning_rate <span class=\"op\">=</span> <span class=\"fl\">0.01</span>,\n                  dropout <span class=\"op\">=</span> <span class=\"fl\">0.8</span>,\n                  verbose <span class=\"op\">=</span> <span class=\"va\">True</span>):\n\n    reset_graph()\n    tf.set_random_seed(<span class=\"dv\">1234</span>)\n    g <span class=\"op\">=</span> build_graph(num_steps <span class=\"op\">=</span> num_steps,\n                    bptt_steps <span class=\"op\">=</span> bptt_steps,\n                    state_size <span class=\"op\">=</span> state_size,\n                    batch_size <span class=\"op\">=</span> batch_size,\n                   learning_rate <span class=\"op\">=</span> learning_rate)\n    <span class=\"cf\">if</span> use_true_bptt:\n        train_step <span class=\"op\">=</span> g[<span class=\"st\">&#39;train_true_bptt&#39;</span>]\n    <span class=\"cf\">else</span>:\n        train_step <span class=\"op\">=</span> g[<span class=\"st\">&#39;train_tf_style&#39;</span>]\n    <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n        training_losses <span class=\"op\">=</span> []\n        val_losses <span class=\"op\">=</span> []\n        <span class=\"cf\">for</span> idx, epoch <span class=\"kw\">in</span> <span class=\"bu\">enumerate</span>(gen_epochs(num_epochs, num_steps, batch_size)):\n            training_loss <span class=\"op\">=</span> <span class=\"dv\">0</span>\n            steps <span class=\"op\">=</span> <span class=\"dv\">0</span>\n            training_state <span class=\"op\">=</span> np.zeros((batch_size, state_size))\n            <span class=\"cf\">for</span> X, Y <span class=\"kw\">in</span> epoch:\n                steps <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n                training_loss_, training_state, _ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;loss&#39;</span>],\n                                                      g[<span class=\"st\">&#39;final_state&#39;</span>],\n                                                      train_step],\n                                                  feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: X,\n                                                             g[<span class=\"st\">&#39;y&#39;</span>]: Y,\n                                                             g[<span class=\"st\">&#39;dropout&#39;</span>]: dropout,\n                                                             g[<span class=\"st\">&#39;init_state&#39;</span>]: training_state})\n                training_loss <span class=\"op\">+=</span> training_loss_\n            <span class=\"cf\">if</span> verbose:\n                <span class=\"bu\">print</span>(<span class=\"st\">&quot;Average training loss for Epoch&quot;</span>, idx, <span class=\"st\">&quot;:&quot;</span>, training_loss<span class=\"op\">/</span>steps)\n            training_losses.append(training_loss<span class=\"op\">/</span>steps)\n\n            val_loss <span class=\"op\">=</span> <span class=\"dv\">0</span>\n            steps <span class=\"op\">=</span> <span class=\"dv\">0</span>\n            training_state <span class=\"op\">=</span> np.zeros((batch_size, state_size))\n\n            <span class=\"cf\">for</span> X,Y <span class=\"kw\">in</span> reader.ptb_iterator(val_data, batch_size, num_steps):\n                steps <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n                val_loss_, training_state <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;loss&#39;</span>],\n                                                      g[<span class=\"st\">&#39;final_state&#39;</span>]],\n                                                  feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: X,\n                                                             g[<span class=\"st\">&#39;y&#39;</span>]: Y,\n                                                             g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"dv\">1</span>,\n                                                             g[<span class=\"st\">&#39;init_state&#39;</span>]: training_state})\n                val_loss <span class=\"op\">+=</span> val_loss_\n            <span class=\"cf\">if</span> verbose:\n                <span class=\"bu\">print</span>(<span class=\"st\">&quot;Average validation loss for Epoch&quot;</span>, idx, <span class=\"st\">&quot;:&quot;</span>, val_loss<span class=\"op\">/</span>steps)\n                <span class=\"bu\">print</span>(<span class=\"st\">&quot;***&quot;</span>)\n            val_losses.append(val_loss<span class=\"op\">/</span>steps)\n\n    <span class=\"cf\">return</span> training_losses, val_losses</code></pre></div>\n<h3 id=\"results\">Results</h3>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"co\"># Procedure to collect results</span>\n<span class=\"co\"># Note: this takes a few hours to run</span>\n\nbptt_steps <span class=\"op\">=</span> [(<span class=\"dv\">5</span>,<span class=\"dv\">20</span>), (<span class=\"dv\">10</span>,<span class=\"dv\">30</span>), (<span class=\"dv\">20</span>,<span class=\"dv\">40</span>), (<span class=\"dv\">40</span>,<span class=\"dv\">40</span>)]\nlrs <span class=\"op\">=</span> [<span class=\"fl\">0.003</span>, <span class=\"fl\">0.001</span>, <span class=\"fl\">0.0003</span>]\n<span class=\"cf\">for</span> bptt_step, lr <span class=\"kw\">in</span> ((x, y) <span class=\"cf\">for</span> x <span class=\"kw\">in</span> bptt_steps <span class=\"cf\">for</span> y <span class=\"kw\">in</span> lrs):\n    _, val_losses <span class=\"op\">=</span> <span class=\"op\">\\</span>\n        train_network(<span class=\"dv\">20</span>, bptt_step[<span class=\"dv\">0</span>], use_true_bptt<span class=\"op\">=</span><span class=\"va\">False</span>, state_size<span class=\"op\">=</span><span class=\"dv\">100</span>,\n                      batch_size<span class=\"op\">=</span><span class=\"dv\">32</span>, learning_rate<span class=\"op\">=</span>lr, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n    <span class=\"bu\">print</span>(<span class=\"st\">&quot;** TF STYLE **&quot;</span>, bptt_step, lr)\n    <span class=\"bu\">print</span>(np.<span class=\"bu\">min</span>(val_losses))\n    <span class=\"cf\">if</span> bptt_step[<span class=\"dv\">0</span>] <span class=\"op\">!=</span> <span class=\"dv\">0</span>:\n        _, val_losses <span class=\"op\">=</span> <span class=\"op\">\\</span>\n            train_network(<span class=\"dv\">20</span>, bptt_step[<span class=\"dv\">1</span>], use_true_bptt<span class=\"op\">=</span><span class=\"va\">True</span>, bptt_steps<span class=\"op\">=</span> bptt_step[<span class=\"dv\">0</span>],\n                          state_size<span class=\"op\">=</span><span class=\"dv\">100</span>, batch_size<span class=\"op\">=</span><span class=\"dv\">32</span>, learning_rate<span class=\"op\">=</span>lr, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n        <span class=\"bu\">print</span>(<span class=\"st\">&quot;** TRUE STYLE **&quot;</span>, bptt_step, lr)\n        <span class=\"bu\">print</span>(np.<span class=\"bu\">min</span>(val_losses))</code></pre></div>\n<p>Here are the results in a table:</p>\n<h5 id=\"minimum-validation-loss-achieved-in-20-epochs\">Minimum validation loss achieved in 20 epochs:</h5>\n<table>\n<tbody>\n<tr class=\"odd\">\n<td>BPTT Steps</td>\n<td>5</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>Learning Rate</td>\n<td>0.003</td>\n<td>0.001</td>\n<td>0.0003</td>\n</tr>\n<tr class=\"odd\">\n<td>True (20-seq)</td>\n<td><strong>5.12</strong></td>\n<td><strong>5.01</strong></td>\n<td>5.09</td>\n</tr>\n<tr class=\"even\">\n<td>TF Style</td>\n<td>5.21</td>\n<td>5.04</td>\n<td><strong>5.04</strong></td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>BPTT Steps</td>\n<td>10</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>Learning Rate</td>\n<td>0.003</td>\n<td>0.001</td>\n<td>0.0003</td>\n</tr>\n<tr class=\"even\">\n<td>True (30-seq)</td>\n<td><strong>5.07</strong></td>\n<td><strong>5.00</strong></td>\n<td>5.12</td>\n</tr>\n<tr class=\"odd\">\n<td>TF Style</td>\n<td>5.15</td>\n<td>5.03</td>\n<td><strong>5.05</strong></td>\n</tr>\n<tr class=\"even\">\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>BPTT Steps</td>\n<td>20</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>Learning Rate</td>\n<td>0.003</td>\n<td>0.001</td>\n<td>0.0003</td>\n</tr>\n<tr class=\"odd\">\n<td>True (40-seq)</td>\n<td><strong>5.05</strong></td>\n<td>5.00</td>\n<td>5.15</td>\n</tr>\n<tr class=\"even\">\n<td>TF Style</td>\n<td>5.11</td>\n<td><strong>4.99</strong></td>\n<td><strong>5.08</strong></td>\n</tr>\n<tr class=\"odd\">\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"even\">\n<td>BPTT Steps</td>\n<td>40</td>\n<td></td>\n<td></td>\n</tr>\n<tr class=\"odd\">\n<td>Learning Rate</td>\n<td>0.003</td>\n<td>0.001</td>\n<td>0.0003</td>\n</tr>\n<tr class=\"even\">\n<td>TF Style</td>\n<td>5.05</td>\n<td>4.99</td>\n<td>5.15</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"discussion\">Discussion</h3>\n<p>As you can see, true truncated backpropagation seems to have an advantage over Tensorflow-style truncated backpropagation when truncating errors at the same number of steps. However, this advantage completely disappears (and actually reverses) when comparing true truncated backpropagation to Tensorflow-style truncated backpropagation that uses the same sequence length.</p>\n<p>This suggests two things:</p>\n<ul>\n<li>Because a well-implemented true truncated backpropagation algorithm would run about as fast as full backpropagation over the sequence, and full backpropagation performs slightly better, it is most likely not worth implementing an efficient true truncated backpropagation algorithm.</li>\n<li>Since true truncated backpropagation outperforms Tensorflow-style truncated backpropagation when truncating errors to the same number of steps, we might conclude that Tensorflow-style truncated backpropagation does not effectively backpropagate errors the full n-steps. Thus, if you need to capture n-step dependencies with Tensorflow-style truncated backpropagation, you may benefit from using a num_steps that is appreciably higher than n in order to effectively backpropagate errors the desired n steps.</li>\n</ul>\n<p><strong>Edit</strong>: After writing this post, I discovered that this distinction between styles of truncated backpropagation is discussed in <a href=\"https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf\">Williams and Zipser (1992), Gradient-Based Learning Algorithms for Recurrent Networks and Their Computation Complexity</a>. The authors refer to the “true” truncated backpropagation as “truncated backpropagation” or BPTT(n) [or BPTT(n, 1)], whereas they refer to Tensorflow-style truncated backpropagation as “epochwise truncated backpropagation” or BPTT(n, n). They also allow for semi-epochwise truncated BPTT, which would do a backward pass more often than once per sequence, but less often than all possible times (i.e., in Ilya Sutskever’s language used above, this would be BPTT(k2, k1), where 1 &lt; k1 &lt; k2).</p>\n<p>In <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&amp;rep=rep1&amp;type=pdf\">Williams and Peng (1990), An Efficien Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories</a>, the authors conduct a similar experiment to the one in the post, and reach similar conclusions. In particular, Williams Peng write that: “The results of these experiments have been that the success rate of BPTT(2h; h) is essentially identical to that of BPTT(h)”. In other words, they compared “true” truncated backpropagation, with h steps of truncation, to BPTT(2h, h), which is similar to Tensorflow-style backpropagation and has 2h steps of truncation, and found that they performed similarly.</p>\n</body>\n</html>"
}