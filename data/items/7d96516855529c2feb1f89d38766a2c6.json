{
  "title": "How to Solve the Data Ingestion and Feature Store Component of the MLOps Stack",
  "link": "https://neptune.ai/blog/data-ingestion-and-feature-store-component-mlops-stack",
  "dc:creator": "Manuel Martin",
  "pubDate": "Tue, 13 Sep 2022 15:59:23 +0000",
  "category": [
    "ML Model Development",
    "MLOps",
    "mlops"
  ],
  "guid": "https://neptune.ai/?p=71407",
  "description": "<p>As every practitioner in the Data Science space knows, Data is the primary fuel for Machine Learning. A trustworthy data sourcing and high-quality data collection and processing can empower a vast range of potential ML use cases. But having a well-governed Data Warehouse requires a thorough devotion from every team in the organization to look [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://neptune.ai/blog/data-ingestion-and-feature-store-component-mlops-stack\">How to Solve the Data Ingestion and Feature Store Component of the MLOps Stack</a> appeared first on <a rel=\"nofollow\" href=\"https://neptune.ai\">neptune.ai</a>.</p>\n",
  "content:encoded": "\n<p>As every practitioner in the <em>Data Science</em> space knows, <strong>Data</strong> <strong>is the primary fuel for Machine Learning</strong>. A trustworthy data sourcing and high-quality data collection and processing can empower a vast range of potential ML use cases. But having a well-governed <a href=\"https://aws.amazon.com/data-warehouse/#:~:text=A%20data%20warehouse%20is%20a,typically%20on%20a%20regular%20cadence.\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Data Warehouse</a> requires a thorough devotion from every team in the organization to look after and curate every data point that they produce, ingest, analyze or exploit. <strong>Data quality responsibility spreads across everyone</strong>. It is not only dependent on the Data Engineering team.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img data-attachment-id=\"71390\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-1\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-1.png?fit=891%2C470&ssl=1\" data-orig-size=\"891,470\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-1.png?fit=300%2C158&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-1.png?fit=891%2C470&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-1.png?resize=668%2C353&#038;ssl=1\" alt=\"Data quality characteristics\" class=\"wp-image-71390\" width=\"668\" height=\"353\" data-recalc-dims=\"1\" /><figcaption><em>Main properties of Data Quality | <a href=\"https://www.aqclab.es/index.php/en/data-quality-iso-25012\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>The most common data architecture nowadays in organizations is <a href=\"https://en.wikipedia.org/wiki/Lambda_architecture\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Lambda Architecture</a>. It is characterized by having independent batch and streaming pipelines ingesting data into the Data Lake, which consists of a <em>landing</em> or <em>raw</em> stage where <a href=\"https://www.ibm.com/cloud/learn/elt\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">ELT</a><em> </em>processes dump raw data objects, such as events or database record dumps.&nbsp;</p>\n\n\n\n<p>This raw data is later ingested and wrangled into more organized Data Lake tables (Parquet files, for example), and then it is enriched to be ingested into the <em>Data Warehouse</em>. The data that gets into the DW is logically organised information for different business domains called <a href=\"https://www.oracle.com/autonomous-database/what-is-data-mart/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Data Marts</a>. These data marts are easily queried by Data Analysts and explored by Business Stakeholders. Each data mart could be related to different business units or product domains (<em>Marketing, Subscriptions, Registrations, Product, Users …)</em>.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img data-attachment-id=\"71391\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-2\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-2.png?fit=1999%2C979&ssl=1\" data-orig-size=\"1999,979\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-2.png?fit=300%2C147&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-2.png?fit=1024%2C501&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-2.png?resize=840%2C411&#038;ssl=1\" alt=\"Example of a typical Data Architecture\" class=\"wp-image-71391\" width=\"840\" height=\"411\" data-recalc-dims=\"1\" /><figcaption><em>Example of a typical Data Architecture in Google Cloud Platform&nbsp;| <a href=\"https://blog.miraclesoft.com/data-foundation-with-modernized-data-lake-data-warehouse/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>There are also other reference architecture patterns such as the <a href=\"https://hazelcast.com/glossary/kappa-architecture/#:~:text=What%20Is%20the%20Kappa%20Architecture,with%20a%20single%20technology%20stack.\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Kappa</a> or Delta, the latter getting a lot of traction with commercial products such as <a href=\"https://www.databricks.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Databricks</a> and <a href=\"https://delta.io/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Delta Lake</a>.&nbsp;</p>\n\n\n\n<p>These foundational data architectural patterns have paved the way for analytical workloads. <a href=\"https://en.wikipedia.org/wiki/Online_analytical_processing\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">OLAP</a> databases and processing engines for Big Data, such as <a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Spark</a> and <a href=\"https://www.dask.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Dask</a>, among others, have enabled the decoupling of the storage and computing hardware, allowing Data practitioners to interact with massive amounts of data for doing <em>Data Analytics </em>and <em>Data Science</em>.</p>\n\n\n\n<p>With the rise of <a href=\"/blog/mlops\" target=\"_blank\" rel=\"noreferrer noopener\">MLOps</a>, DataOps, and the importance of <em>Software Engineering </em>in production <em>Machine Learning, </em>different startups and products have emerged to solve the issue of serving features such as <a href=\"https://www.tecton.ai/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Tecton</a>, <a href=\"https://www.hopsworks.ai/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">HopsWorks</a>, <a href=\"https://feast.dev/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feast</a>, <a href=\"https://aws.amazon.com/sagemaker/feature-store/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">SageMaker Feature Store</a>, <a href=\"https://docs.databricks.com/applications/machine-learning/feature-store/index.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Databricks Feature Store</a>, <a href=\"https://cloud.google.com/vertex-ai/docs/featurestore\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Vertex AI Feature Store</a>…<em> </em>(check out <a href=\"https://www.featurestore.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Featurestore.org</a> to see all the players in this field).</p>\n\n\n\n<p>Furthermore, every company doing production data science at a considerable scale, if not using one of the tools named before, has built their in-house feature store (e.g., <a href=\"https://www.uber.com/blog/michelangelo-machine-learning-platform/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Uber was of the first to publish their own approach for building an ML platform</a>, followed by Airbnb).</p>\n\n\n\n<p>In this article, we will explain some of the concepts and issues that feature stores solve as if it was an in-house platform. This is because we think it is easier to understand the underlying components and the conceptual and technical relationships among them. We won’t dive deep into commercial products. </p>\n\n\n\n<p>We will also discuss the tension between build and buy, which is a hot topic among practitioners in the industry today and what’s the best way to approach this decision.</p>\n\n\n\n<div id=\"blog-cta-intext-block_6320a803d38d1\" class=\"blog-cta-intext\">\n  <h3 class=\"blog-cta-intext__title\">Bookmark for later</h3>\n  <div class=\"blog-cta-intext__content\"><p><a href=\"/blog/model-serving-component-mlops-stack\" target=\"_blank\" rel=\"noopener\">How to Solve the Model Serving Component of the MLOps Stack</a></p>\n</div>\n  </div>\n\n\n<h2>What is a feature store?</h2>\n\n\n\n<p>Last year, some <a href=\"https://www.datanami.com/2021/01/19/2021-the-year-of-the-feature-store/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">blogs and influential people in the ML world named 2021</a> as the year of the feature store. We will argue in the next sections the reason behind this. But then, what is a feature store?</p>\n\n\n\n<p>A short definition given by<a href=\"https://www.featurestore.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> Featurestore.org</a> is:</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p><em>“A </em><strong><em>data management layer for machine learning</em></strong><em> that allows to share & discover features and create more effective machine learning pipelines.”</em></p></blockquote>\n\n\n\n<p>That’s pretty accurate. To briefly expand on some details, feature stores are composed of a set of technological, architectural, conceptual, and semantic components that enable ML practitioners to create, ingest, discover and fetch features for doing offline experiments and developing online production services.</p>\n\n\n\n<h3>Components of a feature store</h3>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71392\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-3\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-3.png?fit=1706%2C1070&ssl=1\" data-orig-size=\"1706,1070\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-3\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-3.png?fit=300%2C188&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-3.png?fit=1024%2C642&ssl=1\" decoding=\"async\" width=\"1024\" height=\"642\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-3.png?resize=1024%2C642&#038;ssl=1\" alt=\"Components of a Feature Store\" class=\"wp-image-71392\" data-recalc-dims=\"1\"/><figcaption><em>Components of a feature store |<a href=\"https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> Source</a></em></figcaption></figure></div>\n\n\n<p>We should start defining what is a feature vector as it’s the core entity that feature stores deal with.</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Feature Vector: </strong>This is a data element that contains an entity identifier and a set of properties or characteristics that describe that element at a certain point in time. For example, the entity identifier can be a <strong>user ID</strong> and the properties could contain the following values: (<em>time_since_registration, n_purchases, ltv_value, is_free_trial, average_purchases_per_month, accumulated_purchases, last_purchase_ts etc)</em></li></ul>\n</div>\n\n\n<p>Let’s explain now which are the different storage components that host these feature vectors:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Offline Store: </strong>This is meant to be an analytical database that can ingest, store and serve feature vectors for offline workloads such as data science experiments or batch production jobs. In general, each row contains a feature vector uniquely identified by the entity ID and a given timestamp. This component is usually materialized as S3, Redshift, BigQuery, Hive, etc.</li></ul>\n</div>\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Online Store: </strong>Also referred to as <em>hot data</em>, this storage layer is meant to serve features for low latency prediction services. This database is now used to fetch features at millisecond speed. Redis, DynamoDB, or Cassandra are the common candidates to play this role. Key-Value databases are the best option as complex queries and join are not often needed at runtime.</li></ul>\n</div>\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Feature Catalog or Registry: </strong>Ideally, this is presented as a nice UI that enables features and training datasets discoverability.</li></ul>\n</div>\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Feature Store SDK: </strong>This is a Python library that abstracts access patterns for online and offline stores.</li></ul>\n</div>\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Metadata Management: </strong>This component is used to track access from different users or pipelines, ingestion processes, schema changes, and this type of information.</li></ul>\n</div>\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Offline and Online Serving API: </strong>This is a proxy service that sits in between the SDK and the online and feature hardware to facilitate feature access.</li></ul>\n</div>\n\n\n<p>In the following chronological diagram, we can see a summary of the key milestones around feature store since 2017, when Uber released its famous <a href=\"https://www.uber.com/blog/michelangelo-machine-learning-platform/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Michelangelo</a>. A couple of years later, after several commercial and OS products launched, we’ve already seen a wide acceptance of the concept of feature store by industry practitioners. Several organizations such as <a href=\"https://www.featurestore.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">featurestore.org</a> and <a href=\"https://mlops.community/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">mlops.community</a> have emerged in response to this.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71393\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-4\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-4.png?fit=1400%2C788&ssl=1\" data-orig-size=\"1400,788\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-4\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-4.png?fit=300%2C169&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-4.png?fit=1024%2C576&ssl=1\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-4.png?resize=1024%2C576&#038;ssl=1\" alt=\" Feature Store Milestones chart\" class=\"wp-image-71393\" data-recalc-dims=\"1\"/><figcaption><em>Feature Store Milestones | <a href=\"https://medium.com/data-for-ai/feature-store-milestones-cca2bafe6e9c\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source </a></em></figcaption></figure></div>\n\n\n<p>In relationship with MLOps, feature stores are themselves affected and affect other components of the stack such as the Data Warehouse, Data Lake, the data job schedulers, production databases, etc. as well. We will discuss this relationship in detail later, i.e., where does a feature store sit in the big picture of the MLOps framework?</p>\n\n\n\n<p>Now, let’s discuss some of the major issues that ML Engineers face around production feature engineering.</p>\n\n\n\n<h2>Hassles around feature store</h2>\n\n\n\n<h3>Standardization of features ingestion and fetching</h3>\n\n\n\n<p>Before the existence of a proper feature store, each data science team stored and fetched features using very different tools. These kinds of jobs have been treated traditionally as part of <em>Data Engineering</em> pipelines. Therefore, the libraries, SDKs, and tooling around these jobs are the ones used by data engineers. They can be quite diverse depending on the team’s expertise, maturity level, and background.</p>\n\n\n\n<p>For example, you could see the following situation in the same organization:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Team A:</strong> The team is not very knowledgeable in data engineering. They use bare pandas and SQL<strong> </strong>scripts with psycopg<strong> </strong>connectors to store offline features in Redshift and boto to store online features in DynamoDb.</li><li><strong>Team B:</strong> The team is mature and autonomous. They built a library for abstracting connections to several data sources using sqlalchemy<strong> </strong>or PySpark<strong> </strong>for big data jobs. They also have custom wrappers for sending data to DynamoDb and other <em>hot </em>databases.</li></ul>\n</div>\n\n\n<p>This is very typical in large organizations where the ML teams are not fully centralized, or ML cross-teams don’t exist.</p>\n\n\n\n<p>Teams operating with the same databases over different projects tend to build wrappers around them so that they can abstract the connectors and encapsulate common utilities or domain definitions. This problem is already solved by Team B. But Team A is not so skilled, and they might develop another in-house library to work with their features in a simpler way.&nbsp;</p>\n\n\n\n<p>This causes friction among teams because they want to impose their tool across the organization. It also lowers productivity levels across teams because each one is reinventing the wheel in its own manner, coupling developers to projects.</p>\n\n\n\n<p>By introducing a Feature Store SDK, both teams could leverage the same interface for interacting with Redshift and DynamoDb, and other data sources too. The learning curve will be steeper for Team A, but they will maintain the same standard for operating them. So overall productivity will be increased. This allows for better feature governance. SDKs usually hide other API calls to log user requests, and version datasets, allowing for rollbacks, etc.&nbsp;</p>\n\n\n\n<p>Most commercial feature stores provide specific SDKs for interacting with their central service. For example, in<a href=\"https://github.com/feast-dev/feast#5-build-a-training-dataset\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> the next snippet</a>, you could see how to build a dataset fetching features from Feast.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img data-attachment-id=\"71394\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-5\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-5.png?fit=873%2C677&ssl=1\" data-orig-size=\"873,677\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-5\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-5.png?fit=300%2C233&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-5.png?fit=873%2C677&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-5.png?resize=840%2C651&#038;ssl=1\" alt=\"Building a dataset \" class=\"wp-image-71394\" width=\"840\" height=\"651\" data-recalc-dims=\"1\" /><figcaption><em>Build a dataset fetching features from Feast | <a href=\"https://github.com/feast-dev/feast#5-build-a-training-dataset\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source </a></em></figcaption></figure></div>\n\n\n<p>This is not only valuable for standardizing feature store operations but also for abstracting the online and offline stores&#8217; hardware. Data Scientists don’t need to know if the offline store is a BigQuery or Redshift database. This is a great benefit as you could use a different source depending on the use case, data, etc.</p>\n\n\n\n<h3>Time-travel data</h3>\n\n\n\n<p>If we want to predict whether a user will buy a product or not, we have to build a dataset with features until that specific moment. <strong>We need to be very careful regarding not introducing future data as this can lead to </strong><a href=\"https://machinelearningmastery.com/data-leakage-machine-learning/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"><strong>Data Leakage</strong></a><strong>.</strong> But how?</p>\n\n\n\n<p>If we introduce future data into the training dataset with respect to each observation, the <em>Machine Learning</em> model will learn unreliable patterns. When putting the model into real-time production, it won’t have access to the same features (unless you can travel to the future), and its prediction capabilities will deteriorate.</p>\n\n\n\n<p>Coming back to the example of the product purchase prediction, let’s say you want to use specific characteristics about the users, for example, the number of items saved in the cart. The training dataset will contain events about users who saw and bought the product (positive label) and users who saw but didn’t buy the product (negative label). If you want to use the number of items in the cart as a feature, you would need to query specifically for the events that log every item added to the cart within the same session and just before the purchase/seen event.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71395\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-6\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-6.png?fit=1024%2C280&ssl=1\" data-orig-size=\"1024,280\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-6\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-6.png?fit=300%2C82&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-6.png?fit=1024%2C280&ssl=1\" decoding=\"async\" width=\"1024\" height=\"280\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-6.png?resize=1024%2C280&#038;ssl=1\" alt=\"Time Travel in ML\" class=\"wp-image-71395\" data-recalc-dims=\"1\"/><figcaption><em>Tecton: Time Travel in ML&nbsp;| <a href=\"https://www.tecton.ai/blog/time-travel-in-ml/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>Hence, when building such a dataset, we need to query specifically for the features that were available <strong>at that point in time</strong> with respect to each event. It’s necessary to have a representation of the world in which that event occurred.</p>\n\n\n\n<h4>How to have an accurate picture?</h4>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Log and wait</strong>: You just have to log specific features, such as <em>n_cumulative_items_in_the_cart,</em> and then we’ll know how many items the user had at that point in time. The main drawback is that this feature collection strategy needs time to gather enough data points for the use case. But on the other hand, it is easy to implement.<br></li><li><strong>Backfilling</strong><em>:</em> This technique basically aims to reconstruct the desired features at a given point in time. For example, by looking at logged events, we could add all the items added to the cart before each purchase. However, this might become very complex as we have to select the time window cutoff for every feature. These queries are commonly known as <strong>point-in-time</strong> joins.<br></li><li><strong>Snapshotting<em>:</em></strong> It is based on dumping the state of a production database periodically. This allows having features at any given point in time, with the drawback that the data changes between consecutive snapshots wouldn’t be available.</li></ul>\n</div>\n\n\n<h3>Features availability for production</h3>\n\n\n\n<p>Experienced ML engineers tend to think about what features are available at run time (online) when a new ML use case is proposed. Engineering the systems behind enabling features is the most time-consuming piece of the ML architecture in most cases.</p>\n\n\n\n<p>Having an up-to-date feature vector ready to be fed to ML models to make a prediction is not an easy task. Lots of components are involved, and special care is required to glue them all together.</p>\n\n\n\n<p>Features in production can come from very different sources. They can be fed to the algorithm within the request body parameters, they can be fetched from a specific API, retrieved from a SQL or NoSQL database, from a Kafka topic event, from a Key-Value store, or they can be computed and derived on-the-fly from other data. Each of them implies different levels of complexity and resource capacity.&nbsp;</p>\n\n\n\n<h4>What are these sources?</h4>\n\n\n<div class=\"custom-point-list\">\n<ol><li><strong>Request Body Parameters</strong></li></ol>\n</div>\n\n\n<p>This is the simplest way of receiving features for prediction. The responsibility of obtaining these features and passing them to the ML model is delegated to the client or consumer of the inference API Web Service. Nevertheless, this is not the most common way of feeding features. In fact, request parameters tend to contain unique identifiers that are needed to fetch feature vectors from other sources. These are usually user IDs, content IDs, timestamps, search queries, etc.</p>\n\n\n<div class=\"custom-point-list\">\n<ol start=\"2\"><li><strong>Databases</strong></li></ol>\n</div>\n\n\n<p>Depending on the evolvability requirements of the features schemas and latency, features can be live in different databases such as Cassandra, DynamoDb, Redis, PostgreSQL, or any other fast NoSQL or SQL database. Fetching these features from an online service is quite straightforward. You can use any Python library like boto for DynamoDb, pyredis for Redis, psycopg2 for PostgreSQL, mysql-connector-python for MySQL, cassandra-driver for Cassandra, and so on.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71396\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-7\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-7.png?fit=800%2C470&ssl=1\" data-orig-size=\"800,470\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-7\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-7.png?fit=300%2C176&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-7.png?fit=800%2C470&ssl=1\" decoding=\"async\" width=\"800\" height=\"470\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-7.png?resize=800%2C470&#038;ssl=1\" alt=\"Redis database\" class=\"wp-image-71396\" data-recalc-dims=\"1\"/><figcaption><em>Redis database | <a href=\"https://dishagroup.in/jvdd.aspx?iid=150612136-redis+machine+learning&cid=23\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source </a></em></figcaption></figure></div>\n\n\n<p>Each row in the database will have a primary key or index that will be available at runtime for each prediction request. The rest of the columns or values will be the features that you can use.</p>\n\n\n\n<p>To fill up these tables we can use different approaches depending on the nature of the features to compute:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Batch jobs:</strong> These are compute-intensive, heavy, and “slow”, that’s why they only serve a certain type of features defined by how <em>fresh</em> they need to be. When building different use cases, you realise that not every model needs real-time features. If you’re using the average rating of a product, you don’t need to compute the average every second. Most of the features like this just need a daily computation. If the feature requires higher update frequency than 1 day, you should start thinking about a batch job.</li></ul>\n</div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71397\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-8\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-8.png?fit=1024%2C341&ssl=1\" data-orig-size=\"1024,341\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-8\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-8.png?fit=300%2C100&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-8.png?fit=1024%2C341&ssl=1\" decoding=\"async\" width=\"1024\" height=\"341\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-8.png?resize=1024%2C341&#038;ssl=1\" alt=\" Batch processing example\" class=\"wp-image-71397\" data-recalc-dims=\"1\"/><figcaption><em>An example of a batch processing | <a href=\"https://datawhatnow.com/batch-processing-mapreduce/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>Talking about common tech stacks, old friends come into play for serving different purposes and scales:&nbsp;</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>Airflow + DBT or Python is a great first start to schedule and run these jobs.&nbsp;</li><li>If more scale is needed in terms of distributed memory, we can start thinking about Kubernetes Clusters to execute Spark or Dask jobs.&nbsp;</li></ul>\n</div>\n\n\n<p>Some alternatives for orchestration tools are Prefect, Dagster, Luigi, or Flyte. Have a look at a comparison of <a href=\"/blog/best-workflow-and-pipeline-orchestration-tools\">Data Science orchestration and workflow tools</a>.</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Streaming Ingestion:</strong><a href=\"https://medium.com/data-for-ai/building-real-time-ml-pipelines-with-a-feature-store-9f90091eeb4\"> </a>Features that need streaming or (near) real-time computations are time-sensitive. Common use cases that need real-time features are fraud detection, real-time product recommendation, predictive maintenance, dynamic pricing, voice assistants, chatbots, and more. For such use cases, we would need a very fast data transformation service.</li></ul>\n</div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71398\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-10\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-10.png?fit=1176%2C535&ssl=1\" data-orig-size=\"1176,535\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-10\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-10.png?fit=300%2C136&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-10.png?fit=1024%2C466&ssl=1\" decoding=\"async\" width=\"1024\" height=\"466\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-10.png?resize=1024%2C466&#038;ssl=1\" alt=\"Building ML pipeline with Feature\" class=\"wp-image-71398\" data-recalc-dims=\"1\"/><figcaption><em>Building ML pipeline with Feature | <a href=\"https://medium.com/data-for-ai/building-real-time-ml-pipelines-with-a-feature-store-9f90091eeb4\" target=\"_blank\" rel=\"noreferrer noopener\">Source</a></em></figcaption></figure></div>\n\n\n<p>There are two important dimensions to take into account here – <strong>frequency</strong> and <strong>complexity</strong>. For example, computing the “standard deviation of the current price versus the average monthly price” on an individual transaction is both a real-time and complex aggregation.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71399\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-11\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-11.png?fit=800%2C392&ssl=1\" data-orig-size=\"800,392\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-11\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-11.png?fit=300%2C147&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-11.png?fit=800%2C392&ssl=1\" decoding=\"async\" width=\"800\" height=\"392\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-11.png?resize=800%2C392&#038;ssl=1\" alt=\"Feature Store Streaming Ingestion\" class=\"wp-image-71399\" data-recalc-dims=\"1\"/><figcaption><em>Amazon SageMaker Feature Store Streaming Ingestion | <a href=\"https://aws.amazon.com/es/blogs/aws-spanish/ingesta-de-streaming-con-amazon-sagemaker-feature-store-para-tomar-decisiones-respaldadas-por-ml-casi-en-tiempo-real/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>Apart from having a streaming tool in place for collecting events (Kafka), we would also need a high-speed and scalable (to handle any volume of events per second) function-as-a-service (such as AWS Lambda) to read and process those events. More importantly, the transformation service needs to support aggregations, grouping, joins, custom functions, filters, sliding windows to calculate data over a given time period every X minutes or hours, etc.</p>\n\n\n\n<h2>Where does the feature store sit in the MLOps architecture?</h2>\n\n\n\n<p>The feature store is an inherent part of ML Platforms. As said previously, it has been a part of it since the first ML models were put in production, but it wasn’t until a few years ago when the concept acquired its own identity within the MLOps world.</p>\n\n\n\n<p>Features data sources can get tracked with Experiment Tracking tools such as Neptune, MLFlow, or SageMaker Experiments. That is, let’s say you’re training a fraud detection model and you’ve used some shared Features that another team has built. If you logged those features metadata as parameters, then they will be versioned along with your experiment results and code when tracking the experiments.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71400\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-12\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-12.png?fit=1628%2C570&ssl=1\" data-orig-size=\"1628,570\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-12\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-12.png?fit=300%2C105&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-12.png?fit=1024%2C359&ssl=1\" decoding=\"async\" width=\"1024\" height=\"359\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-12.png?resize=1024%2C359&#038;ssl=1\" alt=\" Orchestrating Spark ML Pipelines and MLflow for Production\" class=\"wp-image-71400\" data-recalc-dims=\"1\"/><figcaption>The Killer Feature Store: Orchestrating Spark ML Pipelines and MLflow for Production&nbsp;| <a href=\"https://www.slideshare.net/databricks/the-killer-feature-store-orchestrating-spark-ml-pipelines-and-mlflow-for-production\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></figcaption></figure></div>\n\n\n<p>Besides, they become a critical piece when the model is in the production stage. There are several components that need to be synchronised and closely monitored when being live. If one fails, predictions could degrade pretty quickly. These components are the features computation & ingestion pipelines and features consumption from the production services. The computation pipelines need to run at a specific frequency so that features’ freshness doesn’t affect the online predictions. E.g.: if a recommendation system needs to know the film you viewed yesterday, the feature pipeline should run before you go into the media streaming service again!</p>\n\n\n\n<h2>How to implement a feature store?</h2>\n\n\n\n<p>In this section, we will discuss different architectures that can be implemented for different stages and sizes of Data Science teams. In <a href=\"https://eugeneyan.com/writing/feature-stores/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">this very nice article</a>, you can see how the author uses the <em>Hierarchy of Needs </em>to very explicitly show which are the main pillars you need to solve. He places the <strong><em>Access</em></strong><em> </em>need, which encompasses transparency and lineage, as more foundational than <strong><em>Serving</em></strong>. I don’t completely agree as the features availability in production unlocks higher business value.</p>\n\n\n\n<p>The suggestions presented below will be based on AWS services (although they are easily interchangeable with other public cloud services).</p>\n\n\n\n<h3>The simplest solution</h3>\n\n\n\n<p>This architecture is based on managed services, which require less maintenance overhead and are better suited for small teams that can operate quickly.</p>\n\n\n\n<p>My initial setup would be Redshift as an offline store, DynamoDB as an online key value store, Airflow to manage batch feature computation jobs. Also, Pandas as data processing engine for both options. In this architecture, all feature computation pipelines are scheduled in Airflow and would need to ingest data by using Python scripts that fetch data from Redshift or S3, transforms it, and put it into DynamoDB for online services and then in Redshift again for the offline feature storage.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img data-attachment-id=\"71401\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-13\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-13.png?fit=1999%2C980&ssl=1\" data-orig-size=\"1999,980\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-13\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-13.png?fit=300%2C147&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-13.png?fit=1024%2C502&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-13.png?resize=850%2C416&#038;ssl=1\" alt=\"The initial setup chart \" class=\"wp-image-71401\" width=\"850\" height=\"416\" data-recalc-dims=\"1\" /><figcaption><em>The initial setup | Source: author</em></figcaption></figure></div>\n\n\n<h3>Medium-size feature store</h3>\n\n\n\n<p>If you’re already dealing with big data, near real-time needs for features, and reusability necessities across data science teams, then you are probably looking for more standardization across feature pipelines and some degree of reusability.</p>\n\n\n\n<p>In this situation, I would recommend starting using third-party feature store vendors when the data science team size is relatively big (let’s say, more than 8-10 data scientists). First, I would explore Feast as it’s the most used open-source solution out there, and it can work on top of existing infrastructure. You could use Redshift as an offline feature store and DynamoDB or Redis as an online feature store. The latter is faster for online prediction services with lower latency requirements. <a href=\"https://feast.dev/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feast </a>will help to catalogue and serve features through their SDK and web UI (still experimental, though). If you want a fully managed commercial tool, I implore you to try out <a href=\"https://www.tecton.ai/product/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Tecton</a>.</p>\n\n\n\n<p>Feature computation pipelines can now be developed using plain Python or Spark if there are big data requirements, leveraging <a href=\"https://rtd.feast.dev/en/master/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feast SDK</a> for managing data ingestion.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71402\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-14\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-14.png?fit=967%2C494&ssl=1\" data-orig-size=\"967,494\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-14\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-14.png?fit=300%2C153&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-14.png?fit=967%2C494&ssl=1\" decoding=\"async\" width=\"967\" height=\"494\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-14.png?resize=967%2C494&#038;ssl=1\" alt=\"\" class=\"wp-image-71402\" data-recalc-dims=\"1\"/><figcaption><em>Running Feast in production | <a href=\"https://docs.feast.dev/how-to-guides/running-feast-in-production\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p>It’s also pretty likely that at this size, there are some use cases with real-time features freshness necessities. In this case, we need a streaming service that ingests features directly into the online feature store. We could use Kinesis services and AWS Lambda to write feature vectors into Redis or DynamoDB directly. If window aggregations are needed, then Kinesis Data Analytics, KafkaSQL, or Spark Streaming might be reasonable options.</p>\n\n\n\n<h3>Enterprise-level feature store</h3>\n\n\n\n<p>At this stage, we assume the company has plenty of data scientists creating different types of models for different business or technical domains. One key principle when setting architectures for development teams of this size is to provide a reliable, scalable, secure, and standardized data platform. Therefore, SLAs, GDPR, Audit, and Access Control Lists are mandatory requirements to put in place. These are always important points to cover at every organization size, but in this case, they play a critical role.&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71403\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-15\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-15.png?fit=1080%2C754&ssl=1\" data-orig-size=\"1080,754\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-15\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-15.png?fit=300%2C209&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-15.png?fit=1024%2C715&ssl=1\" decoding=\"async\" width=\"1024\" height=\"715\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-15.png?resize=1024%2C715&#038;ssl=1\" alt=\"Feature Store\" class=\"wp-image-71403\" data-recalc-dims=\"1\"/><figcaption>feature store explained | <a href=\"https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source </a></figcaption></figure></div>\n\n\n<p>Most of the big players in the tech space have built their own feature stores according to their own needs, security principles, existing infrastructure, and managed availability themselves to avoid having a single point of failure if the service is fully managed.&nbsp;</p>\n\n\n\n<p>But if this is not the case and you’re running a public cloud-heavy workload, using AWS <a href=\"https://aws.amazon.com/sagemaker/feature-store/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">SageMaker Feature Store</a> or <a href=\"https://cloud.google.com/vertex-ai/docs/featurestore\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">GCP Vertex AI Feature Store</a> can be good options to start with. Their API is very similar to their open source counterparts, and if you’re already using SageMaker or Vertex, setting up their feature store services should be straightforward.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71404\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-16\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-16.png?fit=800%2C369&ssl=1\" data-orig-size=\"800,369\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-16\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-16.png?fit=300%2C138&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-16.png?fit=800%2C369&ssl=1\" decoding=\"async\" width=\"800\" height=\"369\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-16.png?resize=800%2C369&#038;ssl=1\" alt=\"Amazon SageMaker Feature Store \" class=\"wp-image-71404\" data-recalc-dims=\"1\"/><figcaption><em>Amazon SageMaker Feature Store for machine learning | <a href=\"http://Amazon SageMaker Feature Store for machine learning (ML)\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<p><a href=\"https://docs.databricks.com/applications/machine-learning/feature-store/index.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Databricks also offers an embedded Feature Store</a> service, which is also a good option and would be perfectly compatible with a tool like <a href=\"https://mlflow.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">MLFlow</a>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"71405\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-17\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-17.png?fit=1024%2C732&ssl=1\" data-orig-size=\"1024,732\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-17\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-17.png?fit=300%2C214&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-17.png?fit=1024%2C732&ssl=1\" decoding=\"async\" width=\"1024\" height=\"732\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-17.png?resize=1024%2C732&#038;ssl=1\" alt=\"\" class=\"wp-image-71405\" data-recalc-dims=\"1\"/><figcaption><em>Databricks Feature Store&nbsp;| <a href=\"https://www.databricks.com/it/product/feature-store\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source</a></em></figcaption></figure></div>\n\n\n<h2>The buy versus build question</h2>\n\n\n\n<p>The MLOps landscape has been dominated and shaped by big players such as Facebook, Netflix, Uber, Spotify, etc., throughout these years with their very influential staff engineers and blogs. But ML teams should be able to recognize the contexts in which they operate in their own organizations, teams, and business domains. A 200,000 users app doesn’t need the scale, standardization, and rigidity of a 20-million one. That’s why <a href=\"/blog/mlops-at-reasonable-scale\">MLOps at reasonable scale</a> is a hot topic that is sticking around senior practitioners not working at FAANG-like companies.</p>\n\n\n\n<div id=\"blog-cta-intext-block_6320a7c0d38d0\" class=\"blog-cta-intext\">\n  <h3 class=\"blog-cta-intext__title\">Read also </h3>\n  <div class=\"blog-cta-intext__content\"><p><a href=\"/blog/mlops-reasonable-scale-jacopo-tagliabue\" target=\"_blank\" rel=\"noopener\">Setting up MLOps at a Reasonable Scale With Jacopo Tagliabue</a></p>\n</div>\n  </div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img data-attachment-id=\"71406\" data-permalink=\"https://neptune.ai/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-mlops-stack-18\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-18.png?fit=1268%2C722&ssl=1\" data-orig-size=\"1268,722\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-18\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-18.png?fit=300%2C171&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-18.png?fit=1024%2C583&ssl=1\" decoding=\"async\" width=\"1024\" height=\"583\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/how-to-solve-the-data-ingestion-and-feature-store-component-of-the-MLOps-stack-18.png?resize=1024%2C583&#038;ssl=1\" alt=\"Graphic explanation of a feature store\" class=\"wp-image-71406\" data-recalc-dims=\"1\"/><figcaption><em>Explanation of a feature store | <a href=\"https://towardsdatascience.com/do-you-need-a-feature-store-35b90c3d8963\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Source </a></em></figcaption></figure></div>\n\n\n<h3>Who should build a feature store?</h3>\n\n\n\n<p>As mentioned at the start of this article, there’s a constant tussle between building a <em>feature store-like</em> platform in-house or buying a commercial or open source product like <em>Feast, Hopsworks,</em> or <em>Tecton</em>. This tension exists primarily because these products can be opinionated to some degree in their architecture and their SDKs. Thus, most of these tools need to have a central service to handle feature serving on top of online stores, which becomes a single point of failure for production ML services.&nbsp;</p>\n\n\n\n<p>In addition, some other products are full SaaS, becoming an uncertain critical piece for some teams. Thus, ML Engineers are skeptical to bet and adhere too early to one of these tools in their MLOps journey.&nbsp;</p>\n\n\n\n<p>It is very common that ML and Data Engineering teams share the same technology stack in small or medium size companies or startups. For that reason, migrating to a feature store might cause a big headache and expose some hidden costs. In terms of planning, legacy maintenance, operationality, duplicities, etc., it becomes another piece of infrastructure with specific SDKs which are different from the traditional Data Engineering ones.&nbsp;</p>\n\n\n\n<h3>Who should buy a feature store?</h3>\n\n\n\n<p>To extract the most value from a commercial feature store, your use cases and data science teams&#8217; setup need to be aligned with the core benefits that they provide. Products that are heavily reliant on real-time complex ML use cases such as recommendation systems, dynamic pricing, or fraud detection are the ones that can leverage these tools the most.&nbsp;</p>\n\n\n\n<p>A big team of Data Scientists is also a good reason to have a feature store, as it will increase productivity and features reusability. Apart from that, they usually provide a nice UI to discover and explore features. Nonetheless, commercial Feature Store SDKs and APIs provide a set of standards for a more homogeneous way of ingesting and retrieving features. And as a by-product, the data is governed, and reliable metadata is always logged.</p>\n\n\n\n<p>In the very wide variety of ML teams domains, the situation described above is not always met, and setting up these new commercial stacks is sometimes just a personal development desire of the engineers to stay up-to-date with respect to new technology.</p>\n\n\n\n<p>That’s why there are teams still who haven’t migrated to a full-packaged feature store and, instead, still rely on the existing data engineering stack for running their production feature engineering layer. This is totally valid, in my opinion.&nbsp;</p>\n\n\n\n<p>All in all, feature stores just add a convenient shell on top of the existing data engineering stack to provide unified access APIs, a nice UI to discover and govern feature sets, guarantee consistency between online and feature stores, etc. But all these features are not critical for every ML team&#8217;s use case.</p>\n\n\n\n<h2>Conclusion</h2>\n\n\n\n<p>I hope that this article has provided a broad view of what feature store are. But more importantly, the<strong> </strong>reason they’re necessary and the key components that need to be addressed when building one.&nbsp;&nbsp;</p>\n\n\n\n<p>Feature stores are necessary for levelling up the production services in the data science industry. But you need engineers behind them. The ML Engineer role is critical for dealing with feature pipelines as they are just a specific type of data transformation and ingestion process. Hybrid roles like that allow Data Scientists to focus more on the experimentation side and also guarantee high-quality deliverables.</p>\n\n\n\n<p>In addition, I paid special attention to explaining the <em>build versus buy</em> dilemma. From my personal experience, this question arises sooner or later within any mature ML team. I have tried to describe the situations in which they are key for achieving velocity and standardisation, but also left some thoughts on why context awareness is necessary regarding implementing this new technology. Experienced and senior roles should take into consideration the stage of the MLOps journey in which they operate.&nbsp;</p>\n\n\n\n<p>The feature store (commercial and open source) world is still young, and there is not yet a uniform and accepted way of dealing with all the different use cases and needs. So try all the approaches before settling down with one.</p>\n\n\n\n<h3>References</h3>\n\n\n<div class=\"custom-point-list\">\n<ol><li><a href=\"https://eugeneyan.com/writing/feature-stores/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feature Stores &#8211; A Hierarchy of Needs</a><strong>&nbsp;</strong></li><li><a href=\"https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feature Stores Explained: The Three Common Architectures | FeatureForm</a><strong>&nbsp;</strong></li><li><a href=\"https://www.moderndatastack.xyz/category/feature-store\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Feature Store &#8211; Modern Data Stack</a>&nbsp;</li><li><a href=\"https://www.tecton.ai/blog/time-travel-in-ml/#:~:text=Time-travel\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Back to the Future: Solving the time-travel problem in machine learning | Tecton</a></li><li><a href=\"https://maxhalford.github.io/blog/dataset-time-travel/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">An overview of dataset time travel • Max Halford</a></li><li><a href=\"https://www.youtube.com/watch?v=fU9hR3kiOK0\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">&#8220;Turning the database inside out with Apache Samza&#8221; by Martin Kleppmann</a></li><li><a href=\"https://www.wikiwand.com/en/Temporal_database\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Temporal database &#8211; Wikiwand</a></li><li><a href=\"https://medium.com/data-for-ai/building-real-time-ml-pipelines-with-a-feature-store-9f90091eeb4\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Building Real-Time ML Pipelines with a Feature Store | by Adi Hirschtein</a></li><li><a href=\"https://docs.feast.dev/how-to-guides/running-feast-in-production\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Running Feast in production</a></li><li><a href=\"https://www.slideshare.net/databricks/the-killer-feature-store-orchestrating-spark-ml-pipelines-and-mlflow-for-production\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">The Killer Feature Store: Orchestrating Spark ML Pipelines and MLflow for Production</a></li><li><a href=\"https://towardsdatascience.com/do-you-need-a-feature-store-35b90c3d8963\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Do you need a feature store?</a></li></ol>\n</div>\n\n\n\n<div id=\"author-box-new-format-block_62b32ad88674d\" class=\"article__footer article__author\">\n  <div class=\"article__authorImage\">\n          <img width=\"209\" height=\"230\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/5-Model-Deployment-Mistakes-That-Can-Really-Cost-You_8.png?fit=209%2C230&ssl=1\" class=\"article__authorImage-img\" alt=\"Manuel Martín\" decoding=\"async\" data-attachment-id=\"67679\" data-permalink=\"https://neptune.ai/5-model-deployment-mistakes-that-can-really-cost-you_8\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/5-Model-Deployment-Mistakes-That-Can-Really-Cost-You_8.png?fit=980%2C1076&ssl=1\" data-orig-size=\"980,1076\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"5 Model Deployment Mistakes That Can Really Cost You_8\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/5-Model-Deployment-Mistakes-That-Can-Really-Cost-You_8.png?fit=273%2C300&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/5-Model-Deployment-Mistakes-That-Can-Really-Cost-You_8.png?fit=933%2C1024&ssl=1\" />      </div>\n\n  <div class=\"article__authorContent\">\n          <h3 class=\"article__authorContent-name\">Manuel Martín</h3>\n    \n          <p class=\"article__authorContent-text\">Senior Machine Learning Engineer at Busuu. I like building ML systems and write about technical stuff.</p>\n    \n          <ul class=\"article__authorSocial\">\n        <li class=\"article__authorSocial-single article__authorSocial-name\">Follow me on</li>\n        \n                  <li class=\"article__authorSocial-single\"><a href=\"https://www.linkedin.com/in/manuelmart%C3%ADn11/\" class=\"article__authorSocial-lk\" target=\"_blank\"></a></li>\n        \n              </ul>\n    \n  </div>\n</div>\n\n\n<div class=\"is-layout-flow wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<p class=\"has-text-color\" style=\"color:#4455a6\"><strong>READ NEXT</strong></p>\n\n\n\n<h2>Real-World MLOps Examples: Model Development in Hypefactors</h2>\n\n\n\n<p class=\"has-small-font-size\">6 mins read | Author&nbsp;Stephen Oladele | Updated June 28th, 2022</p>\n\n\n<div id=\"block_5ffc75def9f8e\" class=\"separator separator-10\"></div>\n\n\n\n<p>In this first installment of the series “Real-world MLOps Examples,”&nbsp;<a href=\"https://www.linkedin.com/in/jules-belveze\" target=\"_blank\" rel=\"noreferrer noopener\">Jules Belveze</a>, an MLOps Engineer, will walk you through the model development process at&nbsp;<a href=\"https://hypefactors.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Hypefactors</a>, including the types of models they build, how they design their training pipeline, and other details you may find valuable. Enjoy the chat!</p>\n\n\n\n<h3 id=\"company-profile\">Company profile</h3>\n\n\n\n<p><a href=\"https://hypefactors.com/\" target=\"_blank\" rel=\"noreferrer noopener\">Hypefactors</a>&nbsp;provides an all-in-one media intelligence solution for managing PR and communications, tracking trust, product launches, and market and financial intelligence. They operate large data pipelines that stream in the world’s media data ongoingly in real-time. AI is used for many automations that were previously performed manually.</p>\n\n\n\n<h3 id=\"guest-introduction\">Guest introduction</h3>\n\n\n\n<h4>Could you introduce yourself to our readers?</h4>\n\n\n\n<p>Hey Stephen, thanks for having me! My name is Jules. I am 26. I was born and raised in Paris, I am currently living in Copenhagen.</p>\n\n\n\n<h4>Hey Jules! Thanks for the intro. Walk me through your background and how you got to Hypefactors.</h4>\n\n\n\n<p>I hold a Bachelor’s in statistics and probabilities and a Master’s in general engineering from universities in France. On top of that, I also graduated in Data Science with a focus on deep learning from Danish Technical University, Denmark. I’m fascinated by multilingual natural language processing (and therefore specialized in it). I also researched anomaly detection on high-dimensional time series during my graduate studies with Microsoft.&nbsp;</p>\n\n\n\n<p>Today, I work for a media intelligence tech company called Hypefactors, where I develop NLP models to help our users gain insights from the media landscape. What currently works for me is having the opportunity to carry out models from prototyping all the way to production. I guess you could call me a nerd, at least that’s how my friend describes me, as I spent most of my free time either coding or listening to disco vinyl.</p>\n\n\n\n<h3 id=\"model-development-at-hypefactors\">Model development at Hypefactors</h3>\n\n\n\n<h4>Could you elaborate on the types of models you build at Hypefactors?</h4>\n\n\n\n<p>Even though we also have computer vision models running in production, we mainly build&nbsp;<a href=\"https://neptune.ai/blog/category/natural-language-processing\" target=\"_blank\" rel=\"noreferrer noopener\">NLP (Natural Language Processing)</a>&nbsp;models for various use cases. We need to cover multiple countries and handle many languages. The multilingual aspect makes developing with “classical machine learning” approaches hard. We craft deep learning models on top of the&nbsp;<a href=\"https://github.com/huggingface/transformers\" target=\"_blank\" rel=\"noreferrer noopener\">transformer library</a>.&nbsp;</p>\n\n\n\n<p>We run all sorts of models in production, varying from span extraction or sequence classification to text generation. Those models are designed to serve different use cases, like topic classification, sentiment analysis, or summarisation.</p>\n\n\n<a class=\"button continous-post blue-filled\" href=\"/blog/mlops\" target=\"_blank\">\n    Continue reading -></a>\n\n\n\n<hr class=\"wp-block-separator has-css-opacity\"/>\n</div></div>\n<p>The post <a rel=\"nofollow\" href=\"https://neptune.ai/blog/data-ingestion-and-feature-store-component-mlops-stack\">How to Solve the Data Ingestion and Feature Store Component of the MLOps Stack</a> appeared first on <a rel=\"nofollow\" href=\"https://neptune.ai\">neptune.ai</a>.</p>\n",
  "post-id": 71407
}