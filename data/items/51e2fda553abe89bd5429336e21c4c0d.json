{
  "title": "Principal component analysis",
  "link": "",
  "published": "2015-12-05T22:22:00-08:00",
  "updated": "2015-12-05T22:22:00-08:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2015-12-05:/principal-component-analysis",
  "summary": "<p>We review the two essentials of principal component analysis (&#8220;<span class=\"caps\">PCA</span>&#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first <span class=\"math\">\\(k\\)</span> of these &#8212; listed in descending …</p>",
  "content": "<p>We review the two essentials of principal component analysis (&#8220;<span class=\"caps\">PCA</span>&#8221;): 1) The principal components of a set of data points are the eigenvectors of the correlation matrix of these points in feature space. 2) Projecting the data onto the subspace spanned by the first <span class=\"math\">\\(k\\)</span> of these &#8212; listed in descending eigenvalue order &#8212; provides the best possible <span class=\"math\">\\(k\\)</span>-dimensional approximation to the data, in the sense of captured&nbsp;variance.</p>\n<h3>Introduction</h3>\n<p>One way to introduce principal component analysis is to consider the problem of least-squares fits: Consider, for example, the figure shown below. To fit a line to this data, one might attempt to minimize the squared <span class=\"math\">\\(y\\)</span> residuals (actual minus fit <span class=\"math\">\\(y\\)</span> values). However, if the <span class=\"math\">\\(x\\)</span> and <span class=\"math\">\\(y\\)</span> values are considered to be on an equal footing, this <span class=\"math\">\\(y\\)</span>-centric approach is not quite appropriate. A natural alternative is to attempt instead to find the line that minimizes the <em>total squared projection error</em>: If <span class=\"math\">\\((x_i, y_i)\\)</span> is a data point, and <span class=\"math\">\\((\\hat{x}_i, \\hat{y}_i)\\)</span> is the point closest to it on the regression line (aka, its &#8220;projection&#8221; onto the line), we attempt to&nbsp;minimize\n</p>\n<div class=\"math\">$$\\tag{1} \\label{score}\nJ = \\sum_i (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2.\n$$</div>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2015/12/projection.png\"><img alt=\"margin around decision boundary\" src=\"https://efavdb.com/wp-content/uploads/2015/12/projection.png\"></a></p>\n<p>The summands here are illustrated in the figure: The dotted lines shown are the projection errors for each data point relative to the red line. The minimizer of (\\ref{score}) is the line that minimizes the sum of the squares of these&nbsp;values.</p>\n<p>Generalizing the above problem, one could ask which <span class=\"math\">\\(k\\)</span>-dimensional hyperplane passes closest to a set of data points in <span class=\"math\">\\(N\\)</span>-dimensions. Being able to identify the solution to this problem can be very helpful when <span class=\"math\">\\(N \\gg 1\\)</span>. The reason is that in high-dimensional, applied problems, many features are often highly-correlated. When this occurs, projection of the data onto a <span class=\"math\">\\(k\\)</span>-dimensional subspace can often result in a great reduction in memory usage (one moves from needing to store <span class=\"math\">\\(N\\)</span> values for each data point to <span class=\"math\">\\(k\\)</span>) with minimal loss of information (if the points are all near the plane, replacing them by their projections causes little distortion). Projection onto subspaces can also be very helpful for visualization: For example, plots of <span class=\"math\">\\(N\\)</span>-dimensional data projected onto a best two-dimensional subspace can allow one to get a feel for a dataset&#8217;s&nbsp;shape.</p>\n<p>At first glance, the task of actually minimizing (\\ref{score}) may appear daunting. However, it turns out this can be done easily using linear algebra. One need only carry out the following three&nbsp;steps:</p>\n<ul>\n<li>Preprocessing: If appropriate, shift features and normalize so that they all have mean <span class=\"math\">\\(\\mu = 0\\)</span> and variance <span class=\"math\">\\(\\sigma^2 = 1\\)</span>. The latter, scaling step is needed to account for differences in units, which may cause variations along one component to look artificially large or small relative to those along other components (eg, one raw component might be a measure in centimeters, and another in&nbsp;kilometers).</li>\n<li>Compute the covariance matrix. Assuming there are <span class=\"math\">\\(m\\)</span> data points, the <span class=\"math\">\\(i\\)</span>, <span class=\"math\">\\(j\\)</span> component of this matrix is given by:\n    <div class=\"math\">$$\\tag{2} \\label{2} \\Sigma_{ij}^2 = \\frac{1}{m}\\sum_{l=1}^m \\langle (f_{l,i} - \\mu_i) (f_{l,j} - \\mu_j) \\rangle\\\\ = \\langle x_i \\vert \\left (\\frac{1}{m} \\sum_{l=1}^m \\vert \\delta f_l \\rangle \\langle \\delta f_l \\vert \\right) \\vert x_j \\rangle.$$</div>\n    Note that, at right, we are using bracket notation for vectors. We make further use of this below &#8212; see footnote [1] at bottom for review. We&#8217;ve also written <span class=\"math\">\\(\\vert \\delta f_l \\rangle\\)</span> for the vector <span class=\"math\">\\(\\vert f_l \\rangle - \\sum_{i = 1}^n \\mu_i \\vert x_i \\rangle\\)</span> &#8212; the vector <span class=\"math\">\\(\\vert f_l \\rangle\\)</span> with the dataset&#8217;s centroid subtracted&nbsp;out.</li>\n<li>Project all feature vectors onto the <span class=\"math\">\\(k\\)</span> eigenvectors <span class=\"math\">\\(\\{\\vert v_j \\rangle\\)</span>, <span class=\"math\">\\(j = 1 ,2 \\ldots, k\\}\\)</span> of <span class=\"math\">\\(\\Sigma^2\\)</span> that have the largest eigenvalues <span class=\"math\">\\(\\lambda_j\\)</span>, writing\n    <div class=\"math\">$$\\tag{3} \\label{3}\n    \\vert \\delta f_i \\rangle \\approx \\sum_{j = 1}^k \\langle v_j \\vert \\delta f_i \\rangle \\times \\vert v_j\\rangle.\n    $$</div>\n    The term <span class=\"math\">\\(\\langle v_j \\vert \\delta f_i \\rangle\\)</span> above is the coefficient of the vector <span class=\"math\">\\(\\vert \\delta f_i \\rangle\\)</span> along the <span class=\"math\">\\(j\\)</span>-th principal component. If we set <span class=\"math\">\\(k = N\\)</span> above, (\\ref{3}) becomes an identity. However, when <span class=\"math\">\\(k &lt; N\\)</span>, the expression represents an approximation only, with the vector <span class=\"math\">\\(\\vert \\delta f_i \\rangle\\)</span> approximated by its projection into the subspace spanned by the largest <span class=\"math\">\\(k\\)</span> principal&nbsp;components.</li>\n</ul>\n<p>The steps above are all that are needed to carry out a <span class=\"caps\">PCA</span> analysis/compression of any dataset. We show in the next section why this solution will indeed provide the <span class=\"math\">\\(k\\)</span>-dimensional hyperplane resulting in minimal dataset projection&nbsp;error.</p>\n<h3>Mathematics of <span class=\"caps\">PCA</span></h3>\n<p>To understand <span class=\"caps\">PCA</span>, we proceed in three&nbsp;steps.</p>\n<ol>\n<li>Significance of a partial trace: Let <span class=\"math\">\\(\\{\\textbf{u}_j \\}\\)</span> be some arbitrary orthonormal basis set that spans our full <span class=\"math\">\\(N\\)</span>-dimensional space, and consider the sum\n    <div class=\"math\">\\begin{align}\\tag{4} \\label{4}\n    \\sum_{j = 1}^k \\Sigma^2_{jj} = \\frac{1}{m} \\sum_{i,j} \\langle u_j \\vert \\delta f_i \\rangle \\langle \\delta f_i \\vert u_j \\rangle\\\\ = \\frac{1}{m} \\sum_{i,j} \\langle \\delta f_i \\vert u_j \\rangle \\langle u_j \\vert \\delta f_i \\rangle\\\\ \\equiv \\frac{1}{m} \\sum_{i} \\langle \\delta f_i \\vert P \\vert \\delta f_i \\rangle.\n    \\end{align}</div>\n    To obtain the first equality here, we have used <span class=\"math\">\\(\\Sigma^2 = \\frac{1}{m} \\sum_{i} \\vert \\delta f_i \\rangle \\langle \\delta f_i \\vert\\)</span>, which follows from (\\ref{2}). To obtain the last, we have written <span class=\"math\">\\(P\\)</span> for the projection operator onto the space spanned by the first <span class=\"math\">\\(k\\)</span> <span class=\"math\">\\(\\{\\textbf{u}_j \\}\\)</span>. Note that this last equality implies that the partial trace is equal to the average squared length of the projected feature vectors &#8212; that is, the variance of the projected data&nbsp;set.</li>\n<li>Notice that the projection error is simply given by the total trace of <span class=\"math\">\\(\\Sigma^2\\)</span>, minus the partial trace above. Thus, minimization of the projection error is equivalent to maximization of the projected variance,&nbsp;(\\ref{4}).</li>\n<li>We now consider which basis maximizes (\\ref{4}). To do that, we decompose the <span class=\"math\">\\(\\{\\textbf{u}_i \\}\\)</span> in terms of the eigenvectors <span class=\"math\">\\(\\{\\textbf{v}_j\\}\\)</span> of <span class=\"math\">\\(\\Sigma^2\\)</span>, writing\n    <div class=\"math\">\\begin{align} \\tag{5} \\label{5}\n    \\vert u_i \\rangle = \\sum_j \\vert v_j \\rangle \\langle v_j \\vert u_i \\rangle \\equiv \\sum_j u_{ij} \\vert v_j \\rangle.\n    \\end{align}</div>\n    Here, we&#8217;ve inserted the identity in the <span class=\"math\">\\(\\{v_j\\}\\)</span> basis, and written <span class=\"math\">\\( \\langle v_j \\vert u_i \\rangle \\equiv u_{ij}\\)</span>. With these definitions, the partial trace becomes\n    <div class=\"math\">\\begin{align}\\tag{6} \\label{6}\n    \\sum_{i=1}^k \\langle u_i \\vert \\Sigma^2 \\vert u_i \\rangle = \\sum_{i,j,l} u_{ij}u_{il} \\langle v_j \\vert \\Sigma^2 \\vert v_l \\rangle \\\\= \\sum_{i=1}^k\\sum_{j} u_{ij}^2 \\lambda_j.\n    \\end{align}</div>\n    The last equality here follows from the fact that the <span class=\"math\">\\(\\{\\textbf{v}_i\\}\\)</span> are the eigenvectors of <span class=\"math\">\\(\\Sigma^2\\)</span> &#8212; we have also used the fact that they are orthonormal, which follows from the fact that <span class=\"math\">\\(\\Sigma^2\\)</span> is a real, symmetric matrix. The sum (\\ref{6}) is proportional to a weighted average of the eigenvalues of <span class=\"math\">\\(\\Sigma^2\\)</span>. We have a total mass of <span class=\"math\">\\(k\\)</span> to spread out amongst the <span class=\"math\">\\(N\\)</span> eigenvalues. The maximum mass that can sit on any one eigenvalue is one. This follows since <span class=\"math\">\\(\\sum_{i = 1}^k u_{ij}^2 \\leq \\sum_{i = 1}^N u_{ij}^2 =1\\)</span>, the latter equality following from the fact that <span class=\"math\">\\( \\sum_{i = 1}^N u_{ij}^2\\)</span> is an expression for the squared length of <span class=\"math\">\\(\\vert v_j\\rangle\\)</span> in the <span class=\"math\">\\(\\{u_i\\}\\)</span> basis. Under these constraints, the maximum possible average one can get in (\\ref{6}) occurs when all the mass sits on the largest <span class=\"math\">\\(k\\)</span> eigenvalues, with each of these eigenvalues weighted with mass one. This condition occurs if and only if the first <span class=\"math\">\\(k\\)</span> <span class=\"math\">\\(\\{\\textbf{u}_i\\}\\)</span> span the same space as that spanned by the first <span class=\"math\">\\(k\\)</span> <span class=\"math\">\\(\\{\\textbf{v}_j\\}\\)</span> &#8212; those with the <span class=\"math\">\\(k\\)</span> largest&nbsp;eigenvalues.</li>\n</ol>\n<p>That&#8217;s it for the mathematics of <span class=\"caps\">PCA</span>.</p>\n<h3>Footnotes</h3>\n<p>[1] <em>Review of bracket notation</em>: <span class=\"math\">\\(\\vert x \\rangle\\)</span> represents a regular vector, <span class=\"math\">\\(\\langle x \\vert\\)</span> is its transpose, and <span class=\"math\">\\(\\langle y \\vert x \\rangle\\)</span> represents the dot product of <span class=\"math\">\\(x\\)</span> and <span class=\"math\">\\(y\\)</span>. So, for example, when the term in parentheses at the right side of (\\ref{2}) acts on the vector <span class=\"math\">\\(\\vert x_j \\rangle\\)</span> to its right, you get <span class=\"math\">\\( \\frac{1}{m} \\sum_{k=1}^m \\vert \\delta f_k \\rangle \\left (\\langle \\delta f_k \\vert x_j \\rangle\\right).\\)</span> Here, <span class=\"math\">\\( \\left (\\langle \\delta f_k \\vert x_j \\rangle\\right)\\)</span> is a dot product, a scalar, and <span class=\"math\">\\(\\vert \\delta f_k \\rangle\\)</span> is a vector. The result is thus a weighted sum of vectors. In other words, the bracketed term (\\ref{2}) acts on a vector and returns a linear combination of other vectors. That means it is a matrix, as is any other object of form <span class=\"math\">\\(\\sum_i \\vert a_i \\rangle \\langle b_i \\vert\\)</span>. A special, important example is the identity matrix: Given any complete, orthonormal set of vectors <span class=\"math\">\\(\\{x_j\\}\\)</span>, the identity matrix <span class=\"math\">\\(I\\)</span> can be written as <span class=\"math\">\\(I = \\sum_i \\vert x_i \\rangle \\langle x_i \\vert\\)</span>. This identity is often used to make a change of&nbsp;basis.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}