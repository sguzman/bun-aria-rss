{
  "title": "Multivariate Cramer-Rao inequality",
  "link": "",
  "published": "2015-06-20T09:00:00-07:00",
  "updated": "2015-06-20T09:00:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2015-06-20:/multivariate-cramer-rao-bound",
  "summary": "<p>The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters <span class=\"math\">\\(\\vec{\\theta} = \\{\\theta_1, \\theta_2, \\ldots, \\theta_m \\}\\)</span> characterizing a probability distribution <span class=\"math\">\\(P(x) \\equiv P(x; \\vec{\\theta})\\)</span>, given only some samples <span class=\"math\">\\(\\{x_1, \\ldots, x_n\\}\\)</span> taken from <span class=\"math\">\\(P\\)</span>. Specifically, the inequality provides a rigorous lower …</p>",
  "content": "<p>The Cramer-Rao inequality addresses the question of how accurately one can estimate a set of parameters <span class=\"math\">\\(\\vec{\\theta} = \\{\\theta_1, \\theta_2, \\ldots, \\theta_m \\}\\)</span> characterizing a probability distribution <span class=\"math\">\\(P(x) \\equiv P(x; \\vec{\\theta})\\)</span>, given only some samples <span class=\"math\">\\(\\{x_1, \\ldots, x_n\\}\\)</span> taken from <span class=\"math\">\\(P\\)</span>. Specifically, the inequality provides a rigorous lower bound on the covariance matrix of any unbiased set of estimators to these <span class=\"math\">\\(\\{\\theta_i\\}\\)</span> values. In this post, we review the general, multivariate form of the inequality, including its significance and&nbsp;proof.</p>\n<h3>Introduction and theorem&nbsp;statement</h3>\n<p>The analysis of data very frequently requires one to attempt to characterize a probability distribution. For instance, given some random, stationary process that generates samples <span class=\"math\">\\(\\{x_i\\}\\)</span>, one might wish to estimate the mean <span class=\"math\">\\(\\mu\\)</span> of the probability distribution <span class=\"math\">\\(P\\)</span> characterizing this process. To do this, one could construct an estimator function <span class=\"math\">\\(\\hat{\\mu}(\\{x_i\\})\\)</span> &#8212; a function of some samples taken from <span class=\"math\">\\(P\\)</span> &#8212; that is intended to provide an approximation to <span class=\"math\">\\(\\mu\\)</span>. Given <span class=\"math\">\\(n\\)</span> samples, a natural choice is provided&nbsp;by\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\hat{\\mu}(\\{x_i\\}) = \\frac{1}{n}\\sum_{i = 1}^n x_i, \\tag{1}\n\\end{eqnarray}</div>\n<p>\nthe mean of the samples. This particular choice of estimator will always be unbiased given a stationary <span class=\"math\">\\(P\\)</span> &#8212; meaning that it will return the correct result, on average. However, each particular sample set realization will return a slightly different mean estimate. This means that <span class=\"math\">\\(\\hat{\\mu}\\)</span> is itself a random variable having its own distribution and&nbsp;width.</p>\n<p>More generally, one might be interested in a distribution characterized by a set of <span class=\"math\">\\(m\\)</span> parameters <span class=\"math\">\\(\\{\\theta_i\\}\\)</span>. Consistently good estimates to these values require estimators with distributions that are tightly centered around the true <span class=\"math\">\\(\\{\\theta_i\\}\\)</span> values. The Cramer-Rao inequality tells us that there is a fundamental limit to how tightly centered such estimators can be, given only <span class=\"math\">\\(n\\)</span> samples. We state the result&nbsp;below.</p>\n<p><b>Theorem:</b> The multivariate Cramer-Rao&nbsp;inequality</p>\n<p>Let <span class=\"math\">\\(P\\)</span> be a distribution characterized by a set of <span class=\"math\">\\(m\\)</span> parameters <span class=\"math\">\\(\\{\\theta_i\\}\\)</span>, and let <span class=\"math\">\\(\\{\\hat{\\theta_i}\\equiv \\hat{\\theta_i}(\\{x_i\\})\\}\\)</span> be an unbiased set of estimator functions for these parameters. Then, the covariance matrix (see definition below) for the <span class=\"math\">\\(\\hat{\\{\\theta_i\\}}\\)</span>&nbsp;satisfies,</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{2} \\label{cramer_rao_bound}\ncov(\\hat{\\theta}, \\hat{\\theta}) \\geq \\frac{1}{n} \\times \\frac{1}{ cov(\\nabla_{\\vec{\\theta}} \\log P(x),\\nabla_{\\vec{\\theta}} \\log P(x) )}.\n\\end{eqnarray}</div>\n<p>Here, the inequality holds in the sense that left side of the above equation, minus the right, is positive semi-definite. We discuss the meaning and significance of this equation in the next&nbsp;section.</p>\n<h3>Interpretation of the&nbsp;result</h3>\n<p>To understand (\\ref{cramer_rao_bound}), we must first review a couple of definitions. These&nbsp;follow.</p>\n<p><strong>Definition 1</strong>. Let <span class=\"math\">\\(\\vec{u}\\)</span> and <span class=\"math\">\\(\\vec{v}\\)</span> be two jointly-distributed vectors of stationary random variables. The covariance matrix of <span class=\"math\">\\(\\vec{u}\\)</span> and <span class=\"math\">\\(\\vec{v}\\)</span> is defined&nbsp;by\n</p>\n<div class=\"math\">$$\ncov(\\vec{u}, \\vec{v})_{ij} = \\overline{(u_{i}- \\overline{u_i})(v_{j}- \\overline{v_j})} \\equiv \\overline{\\delta u_{i} \\delta v_{j}}\\tag{3} \\label{cov},\n$$</div>\n<p>\nwhere we use overlines for averages. In words, (\\ref{cov}) states that <span class=\"math\">\\(cov(\\vec{u}, \\vec{v})_{ij}\\)</span> is the correlation function of the fluctuations of <span class=\"math\">\\(u_i\\)</span> and <span class=\"math\">\\(v_j\\)</span>.</p>\n<p><strong>Definition 2</strong>. A real, square matrix <span class=\"math\">\\(M\\)</span> is said to be positive semi-definite&nbsp;if\n</p>\n<div class=\"math\">$$\n\\vec{a}^T\\cdot M \\cdot \\vec{a} \\geq 0 \\tag{4} \\label{pd}\n$$</div>\n<p>\nfor all real vectors <span class=\"math\">\\(\\vec{a}\\)</span>. It is positive definite if the &#8220;<span class=\"math\">\\(\\geq\\)</span>&#8221; above can be replaced by a &#8220;<span class=\"math\">\\(&gt;\\)</span><span class=\"dquo\">&#8220;</span>.</p>\n<p>The interesting consequences of (\\ref{cramer_rao_bound}) follow from the following&nbsp;observation:</p>\n<p><strong>Observation</strong>. For any constant vectors <span class=\"math\">\\(\\vec{a}\\)</span> and <span class=\"math\">\\(\\vec{b}\\)</span>, we&nbsp;have\n</p>\n<div class=\"math\">$$\ncov(\\vec{a}^T\\cdot\\vec{u}, \\vec{b}^T \\cdot \\vec{v}) = \\vec{a}^T \\cdot cov(\\vec{u}, \\vec{v}) \\cdot \\vec{b}. \\tag{5} \\label{fact}\n$$</div>\n<p>\nThis follows from the definition&nbsp;(\\ref{cov}).</p>\n<p>Taking <span class=\"math\">\\(\\vec{a}\\)</span> and <span class=\"math\">\\(\\vec{b}\\)</span> to both be along <span class=\"math\">\\(\\hat{i}\\)</span> in (\\ref{fact}), and combining with (\\ref{pd}), we see that (\\ref{cramer_rao_bound}) implies&nbsp;that\n</p>\n<div class=\"math\">$$\n\\sigma^2(\\hat{\\theta}_i^2) \\geq \\frac{1}{n} \\times \\left (\\frac{1}{ cov(\\nabla_{\\vec{\\theta}} \\log P(x),\\nabla_{\\vec{\\theta}} \\log P(x) )} \\right)_{ii},\\tag{6}\\label{CRsimple}\n$$</div>\n<p>\nwhere we use <span class=\"math\">\\(\\sigma^2(x)\\)</span> to represent the variance of <span class=\"math\">\\(x\\)</span>. The left side of (\\ref{CRsimple}) is the variance of the estimator function <span class=\"math\">\\(\\hat{\\theta}_i\\)</span>, whereas the right side is a function of <span class=\"math\">\\(P\\)</span> only. This tells us that there is fundamental &#8212; distribution-dependent &#8212; lower limit on the uncertainty one can achieve when attempting to estimate <em>any parameter characterizing a distribution</em>. In particular, (\\ref{CRsimple}) states that the best variance one can achieve scales like <span class=\"math\">\\(O(1/n)\\)</span>, where <span class=\"math\">\\(n\\)</span> is the number of samples available<span class=\"math\">\\(^1\\)</span> &#8212; very&nbsp;interesting!</p>\n<p>Why is there a relationship between the left and right matrices in (\\ref{cramer_rao_bound})? Basically, the right side relates to the inverse rate at which the probability of a given <span class=\"math\">\\(x\\)</span> changes with <span class=\"math\">\\(\\theta\\)</span>: If <span class=\"math\">\\(P(x \\vert \\theta)\\)</span> is highly peaked, the gradient of <span class=\"math\">\\(P(x \\vert \\theta)\\)</span> will take on large values. In this case, a typical observation <span class=\"math\">\\(x\\)</span> will provide significant information relating to the true <span class=\"math\">\\(\\theta\\)</span> value, allowing for unbiased <span class=\"math\">\\(\\hat{\\theta}\\)</span> estimates that have low variance. In the opposite limit, where typical observations are not very <span class=\"math\">\\(\\theta\\)</span>-informative, unbiased <span class=\"math\">\\(\\hat{\\theta}\\)</span> estimates must have large variance<span class=\"math\">\\(^2\\)</span>.</p>\n<p>We now turn to the proof of&nbsp;(\\ref{cramer_rao_bound}).</p>\n<h3>Theorem&nbsp;proof</h3>\n<p>Our discussion here expounds on that in the <a href=\"http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/tutorials/mvahtmlframe74.html\">online text</a> of Cízek, Härdle, and Weron. We start by deriving a few simple lemmas. We state and derive these sequentially&nbsp;below.</p>\n<p><strong>Lemma 1</strong> Let <span class=\"math\">\\(T_j(\\{x_i\\}) \\equiv \\partial_{\\theta_j} \\log P(\\{x_i\\}; \\vec{\\theta})\\)</span> be a function of a set of independent sample values <span class=\"math\">\\(\\{x_i\\}\\)</span>. Then, the average of <span class=\"math\">\\(T_j(\\{x_i\\})\\)</span> is&nbsp;zero.</p>\n<p><em>Proof:</em> We obtain the average of <span class=\"math\">\\(T_j(\\{x_i\\})\\)</span> through integration over the <span class=\"math\">\\(\\{x_i\\}\\)</span>, weighted by <span class=\"math\">\\(P\\)</span>,\n</p>\n<div class=\"math\">$$\n\\int P(\\{x_i\\};\\vec{\\theta}) \\partial_{\\theta_j} \\log P(\\{x_i\\}; \\vec{\\theta}) d\\vec{x} = \\int P \\frac{\\partial_{\\theta_j} P}{P} d\\vec{x} = \\partial_{\\theta_j} \\int P d\\vec{x} = \\partial_{\\theta_j} 1 = 0. \\tag{7}\n$$</div>\n<p><strong>Lemma 2</strong>. The covariance matrix of an unbiased <span class=\"math\">\\(\\hat{\\theta}\\)</span> and <span class=\"math\">\\(\\vec{T}\\)</span> is the identity&nbsp;matrix.</p>\n<p><em>Proof:</em> Using (\\ref{cov}), the assumed fact that <span class=\"math\">\\(\\hat{\\theta}\\)</span> is unbiased, and Lemma 1, we&nbsp;have\n</p>\n<div class=\"math\">$$\\begin{align}\ncov \\left (\\hat{\\theta}(\\{x_i\\}), \\vec{T}(\\{x_i\\}) \\right)_{jk} &amp;= \\int P(\\{x_i\\}) (\\hat{\\theta}_j - \\theta_j ) \\partial_{\\theta_k} \\log P(\\{x_i\\}) d\\vec{x}\\\\ &amp; = \\int (\\hat{\\theta}_j - \\theta_j ) \\partial_{\\theta_k} P d\\vec{x} \\\\\n&amp;= -\\int P \\partial_{\\theta_k} (\\hat{\\theta}_j - \\theta_j ) d \\vec{x} \\tag{8}\n\\end{align}\n$$</div>\n<p>\nHere, we have integrated by parts in the last line. Now, <span class=\"math\">\\(\\partial_{\\theta_k} \\theta_j = \\delta_{jk}\\)</span>. Further, <span class=\"math\">\\(\\partial_{\\theta_k} \\hat{\\theta}_j = 0\\)</span>, since <span class=\"math\">\\(\\hat{\\theta}\\)</span> is a function of the samples <span class=\"math\">\\(\\{x_i\\}\\)</span> only. Plugging these results into the last line, we&nbsp;obtain\n</p>\n<div class=\"math\">$$\ncov \\left (\\hat{\\theta}, \\vec{T} \\right)_{jk} = \\delta_{jk} \\int P d\\vec{x} = \\delta_{jk}. \\tag{9}\n$$</div>\n<p><strong>Lemma 3</strong>. The covariance matrix of <span class=\"math\">\\(\\vec{T}\\)</span> is <span class=\"math\">\\(n\\)</span> times the covariance matrix of <span class=\"math\">\\(\\nabla_{\\vec{\\theta}} \\log P(x_1 ; \\vec{\\theta})\\)</span> &#8212; a single-sample version of <span class=\"math\">\\(\\vec{T}\\)</span>.</p>\n<p><em>Proof:</em> From the definition of <span class=\"math\">\\(\\vec{T}\\)</span>, we&nbsp;have\n</p>\n<div class=\"math\">$$\nT_j = \\partial_{\\theta_j} \\log P(\\{x_i\\}, \\vec{\\theta}) = \\sum_{i=1}^n \\partial_{\\theta_j} \\log P(x_i, \\vec{\\theta}), \\tag{10}\n$$</div>\n<p>\nwhere the last line follows from the fact that the <span class=\"math\">\\(\\{x_i\\}\\)</span> are independent, so that <span class=\"math\">\\(P(\\{x_i\\}, \\vec{\\theta}) = \\prod P(x_i; \\vec{\\theta})\\)</span>. The sum on the right side of the above equation is a sum of <span class=\"math\">\\(n\\)</span> independent, identically-distributed random variables. If follows that their covariance matrix is <span class=\"math\">\\(n\\)</span> times that for any&nbsp;individual.</p>\n<p><strong>Lemma 4</strong>. Let <span class=\"math\">\\(x\\)</span> and <span class=\"math\">\\(y\\)</span> be two scalar stationary random variables. Then, their correlation coefficient is defined to be <span class=\"math\">\\(\\rho \\equiv \\frac{cov(x,y)}{\\sigma(x) \\sigma(y)}\\)</span>. This&nbsp;satisfies\n</p>\n<div class=\"math\">$$\n-1 \\leq \\rho \\leq 1 \\label{c_c} \\tag{11}\n$$</div>\n<p><em>Proof:</em> Consider the variance of <span class=\"math\">\\(\\frac{x}{\\sigma(x)}+\\frac{y}{\\sigma(y)}\\)</span>. This&nbsp;is\n</p>\n<div class=\"math\">$$\n\\begin{align}\nvar \\left( \\frac{x}{\\sigma(x)}+\\frac{y}{\\sigma(y)} \\right) &amp;= \\frac{\\sigma^2(x)}{\\sigma^2(x)} + 2\\frac{ cov(x,y)}{\\sigma(x) \\sigma(y)} + \\frac{\\sigma^2(y)}{\\sigma^2(y)} \\\\\n&amp;= 2 + 2 \\frac{ cov(x,y)}{\\sigma(x) \\sigma(y)} \\geq 0. \\tag{12}\n\\end{align}\n$$</div>\n<p>\nThis gives the left side of (\\ref{c_c}). Similarly, considering the variance of <span class=\"math\">\\(\\frac{x}{\\sigma(x)}-\\frac{y}{\\sigma(y)}\\)</span> gives the right&nbsp;side.</p>\n<p>We&#8217;re now ready to prove the Cramer-Rao&nbsp;result.</p>\n<p><strong>Proof of Cramer-Rao inequality</strong>. Consider the correlation coefficient of the two scalars <span class=\"math\">\\(\\vec{a} \\cdot \\hat{\\theta}\\)</span> and <span class=\"math\">\\( \\vec{b} \\cdot \\vec{T}\\)</span>, with <span class=\"math\">\\(\\vec{a}\\)</span> and <span class=\"math\">\\(\\vec{b}\\)</span> some constant vectors. Using (\\ref{fact}) and Lemma 2, this can be written&nbsp;as\n</p>\n<div class=\"math\">$$\\begin{align}\n\\rho &amp; \\equiv \\frac{cov(\\vec{a} \\cdot \\hat{\\theta} ,\\vec{b} \\cdot \\vec{T})}{\\sqrt{var(\\vec{a} \\cdot \\hat{\\theta})var(\\vec{b} \\cdot \\vec{T})}} \\\\\n&amp;= \\frac{\\vec{a}^T \\cdot \\vec{b}}{\\left(\\vec{a}^T \\cdot cov(\\hat{\\theta}, \\hat{\\theta}) \\cdot \\vec{a} \\right)^{1/2} \\left( \\vec{b}^T \\cdot cov(\\vec{T},\\vec{T}) \\cdot \\vec{b} \\right)^{1/2}}\\leq 1. \\tag{13}\n\\end{align}\n$$</div>\n<p>\nThe last inequality here follows from Lemma 4. We can find the direction <span class=\"math\">\\(\\hat{b}\\)</span> where the bound above is most tight &#8212; at fixed <span class=\"math\">\\(\\vec{a}\\)</span> &#8212; by maximizing the numerator while holding the denominator fixed in value. Using a Lagrange multiplier to hold <span class=\"math\">\\(\\left( \\vec{b}^T \\cdot cov(\\vec{T},\\vec{T}) \\cdot \\vec{b} \\right) \\equiv 1\\)</span>, the numerator&#8217;s extremum occurs&nbsp;where\n</p>\n<div class=\"math\">$$\n\\vec{a}^T + 2 \\lambda \\vec{b}^T \\cdot cov(\\vec{T},\\vec{T}) = 0 \\ \\ \\to \\ \\ \\vec{b}^T = - \\frac{1}{2 \\lambda} \\vec{a}^T \\cdot cov(\\vec{T}, \\vec{T})^{-1}. \\tag{14}\n$$</div>\n<p>\nPlugging this form into the prior line, we&nbsp;obtain\n</p>\n<div class=\"math\">$$\n- \\frac{\\vec{a}^T \\cdot cov(\\vec{T},\\vec{T})^{-1} \\cdot \\vec{a}}{\\left(\\vec{a}^T \\cdot cov(\\hat{\\theta}, \\hat{\\theta}) \\cdot \\vec{a} \\right)^{1/2} \\left(\\vec{a}^T \\cdot cov(\\vec{T},\\vec{T})^{-1} \\cdot \\vec{a} \\right)^{1/2}}\\leq 1. \\tag{15}\n$$</div>\n<p>\nSquaring and rearranging terms, we&nbsp;obtain\n</p>\n<div class=\"math\">$$\n\\vec{a}^T \\cdot \\left (cov(\\hat{\\theta},\\hat{\\theta}) - cov(\\vec{T},\\vec{T})^{-1} \\right ) \\cdot \\vec{a} \\geq 0. \\tag{16}\n$$</div>\n<p>\nThis holds for any \\(\\vec{a}\\), implying that <span class=\"math\">\\(cov(\\hat{\\theta}, \\hat{\\theta}) - cov(\\vec{T},\\vec{T})^{-1}\\)</span> is positive semi-definite &#8212; see (\\ref{pd}). Applying Lemma 3, we obtain the result<span class=\"math\">\\(^3\\)</span>. <span class=\"math\">\\(\\blacksquare\\)</span></p>\n<p>Thank you for reading &#8212; we hope you&nbsp;enjoyed.</p>\n<p>[1] More generally, (\\ref{fact}) tells us that an observation similar to (\\ref{CRsimple}) holds for any linear combination of the <span class=\"math\">\\(\\{\\theta_i\\}\\)</span>. Notice also that the proof we provide here could also be applied to any individual <span class=\"math\">\\(\\theta_i\\)</span>, giving <span class=\"math\">\\(\\sigma^2(\\hat{\\theta}_i) \\geq 1/n \\times 1/\\langle(\\partial_{\\theta_i} \\log P)^2\\rangle\\)</span>. This is easier to apply than (\\ref{cramer_rao_bound}), but is less&nbsp;stringent.</p>\n<p>[2] It might be challenging to intuit the exact function that appears on the right side of <span class=\"math\">\\((\\ref{cramer_rao_bound})\\)</span>. However, the appearance of <span class=\"math\">\\(\\log P\\)</span><span class=\"quo\">&#8216;</span>s does make some intuitive sense, as it allows the derivatives involved to measure rates of change relative to typical values, <span class=\"math\">\\(\\nabla_{\\theta} P / P\\)</span>.</p>\n<p>[3] The discussion here covers the &#8220;standard proof&#8221; of the Cramer-Rao result. Its brilliance is that it allows one to work with scalars. In contrast, when attempting to find my own proof, I began with the fact that all covariance matrices are positive definite. Applying this result to the covariance matrix of a linear combination of <span class=\"math\">\\(\\hat{\\theta}\\)</span> and <span class=\"math\">\\(\\vec{T}\\)</span>, one can quickly get to results similar in form to the Cramer-Rao bound, but not quite identical. After significant work, I was eventually able to show that <span class=\"math\">\\(\\sqrt{cov(\\hat{\\theta},\\hat{\\theta})} - 1/\\sqrt{cov(\\vec{T},\\vec{T}) } \\geq 0\\)</span>. However, I have yet to massage my way to the final result using this approach &#8212; the difficulty being that the matrices involved don&#8217;t commute. By working with scalars from the start, the proof here cleanly avoids all such&nbsp;issues.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}