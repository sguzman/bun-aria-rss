{
  "title": "Additional Strategies for Confronting the Partition Function",
  "link": "",
  "published": "2018-10-29T22:00:00-04:00",
  "updated": "2018-10-29T22:00:00-04:00",
  "author": {
    "name": "Will Wolf"
  },
  "id": "tag:willwolf.io,2018-10-29:/2018/10/29/additional-strategies-partition-function/",
  "summary": "<p>Stochastic maximum likelihood, contrastive divergence, negative contrastive estimation and negative sampling for improving or avoiding the computation of the gradient of the log-partition function. (Oof, that's a mouthful.)</p>",
  "content": "<p>In the <a href=\"https://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/\">previous post</a> we introduced Boltzmann machines and the infeasibility of computing the gradient of its log-partition function <span class=\"math\">\\(\\nabla\\log{Z}\\)</span>. To this end, we explored one strategy for its approximation: Gibbs sampling. Gibbs sampling is a viable alternative because the expression for our gradient simplifies to an expectation over the model distribution, which can be approximated with Monte Carlo samples.</p>\n<p>In this post, we'll highlight the imperfections of this approximate approach itself, then present more preferable alternatives.</p>\n<h1>Pitfalls of Gibbs sampling</h1>\n<p>To refresh, the two gradients we seek to compute in a reasonable amount of time are:</p>\n<div class=\"math\">$$\n\\nabla_{w_{i, j}}\\log{Z} = \\mathop{\\mathbb{E}}_{x \\sim p_{\\text{model}}} [x_i  x_j]\\\\\n\\nabla_{b_{i}}\\log{Z} = \\mathop{\\mathbb{E}}_{x \\sim p_{\\text{model}}} [x_i]\n$$</div>\n<p>Via Gibbs sampling, we approximate each by:</p>\n<ol>\n<li>Burning in a Markov chain with respect to our model, then selecting <span class=\"math\">\\(n\\)</span> samples from this chain</li>\n<li>Evaluating both functions (<span class=\"math\">\\(x_i  x_j\\)</span>, and <span class=\"math\">\\(x_i\\)</span>) at these samples</li>\n<li>Taking the average of each</li>\n</ol>\n<p>Concretely:</p>\n<div class=\"math\">$$\n\\nabla_{w_{i, j}}\\log{Z} \\approx \\frac{1}{N}\\sum\\limits_{k=1}^N x^{(k)}_i  x^{(k)}_j\\quad\\text{where}\\quad x^{(k)} \\sim p_{\\text{model}}\\\\\n\\nabla_{b_{i}}\\log{Z} \\approx \\frac{1}{N}\\sum\\limits_{k=1}^N x^{(k)}_i\\quad\\text{where}\\quad x^{(k)} \\sim p_{\\text{model}}\n$$</div>\n<p><strong>We perform this sampling process at each gradient step.</strong></p>\n<h2>The cost of burning in each chain</h2>\n<p>Initializing a Markov chain at a random sample incurs a non-trivial \"burn-in\" cost. If paying this cost at each gradient step, it begins to add up. How can we do better?</p>\n<p><strong>In the remainder of the post, we'll explore two new directives for approximating the negative phase more cheaply, and the algorithms they birth.</strong></p>\n<h1>Directive #1: Cheapen the burn-in process</h1>\n<h2>Stochastic maximum likelihood</h2>\n<p>SML assumes the premise: let's initialize our chain at a point already close to the model's true distribution—reducing or perhaps eliminating the cost of burn-in altogether.  <strong>In this vein, at what sample do we initialize the chain?</strong></p>\n<p>In SML, we simply initialize at the terminal value of the previous chain (i.e. the one we manufactured to compute the gradients of the previous mini-batch). <strong>As long as the model has not changed significantly since, i.e. as long as the previous parameter update (gradient step) was not too large, this sample should exist in a region of high probability under the current model.</strong></p>\n<h3>Implications</h3>\n<p>Per the expression for the full log-likelihood gradient, e.g. <span class=\"math\">\\(\\nabla_{w_{i, j}}\\log{\\mathcal{L}} = \\mathop{\\mathbb{E}}_{x \\sim p_{\\text{data}}} [x_i  x_j] - \\mathop{\\mathbb{E}}_{x \\sim p_{\\text{model}}} [x_i  x_j]\\)</span>, the negative phase works to \"reduce the probability of the points in which the model strongly, yet wrongly, believes\".<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup> Since we approximate this term at each parameter update with samples <em>roughly from</em> the current model's true distribution, <strong>we do not encroach on this foundational task.</strong></p>\n<h2>Contrastive divergence</h2>\n<p>Alternatively, in the contrastive divergence algorithm, we initialize the chain at each gradient step with a <em>random sample</em> from the data distribution.</p>\n<h3>Implications</h3>\n<p>With no guarantee that the data distribution resembles the model distribution, we may systematically fail to sample, and thereafter \"suppress,\" points that are incorrectly likely under the latter (as they do not appear in the former!). <strong>This incurs the growth of \"spurious modes\"</strong> in our model, aptly named.<sup id=\"fnref2:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup></p>\n<h2>In summary</h2>\n<p>Cheapening the burn-in phase indeed gives us a more efficient training routine. Moving forward, what are some even more aggressive strategies we might explore?</p>\n<h1>Directive #2: Skip the computation of <span class=\"math\">\\(Z\\)</span> altogether</h1>\n<p>Canonically, we write the log-likelihood of our Boltzmann machine as follows:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\log{\\mathcal{L}(x)}\n&amp;= \\log{\\frac{\\exp{(H(x))}}{Z}}\\\\\n&amp;= \\log{\\big(\\exp{(H(x))}\\big)} - \\log{Z}\\\\\n&amp;= H(x) - \\log{Z}\n\\end{align*}\n$$</div>\n<p>Instead, what if we simply wrote this as:</p>\n<div class=\"math\">$$\n\\log{\\mathcal{L}(x)} = H(x) - c\n$$</div>\n<p>or, more generally:</p>\n<div class=\"math\">$$\n\\log{p_{\\text{model}}(x)} = \\log{\\tilde{p}_{\\text{model}}(x; \\theta)} - c\n$$</div>\n<p>and estimated <span class=\"math\">\\(c\\)</span> as a parameter?</p>\n<p><strong>Immediately, we remark that if we optimize this model with maximum likelihood, our algorithm will, trivially, make <span class=\"math\">\\(c\\)</span> arbitrarily negative.</strong> In other words, the easiest way to increase <span class=\"math\">\\(\\log{p_{\\text{model}}(x)}\\)</span> is to decrease <span class=\"math\">\\(c\\)</span>.</p>\n<p>How might we better phrase this problem?</p>\n<h2>Noise contrastive estimation</h2>\n<p>Ingeniously, NCE proposes an alternative:</p>\n<ol>\n<li>Posit two distributions: the model, and a noise distribution</li>\n<li>Given a data point, predict the distribution (i.e. binary classification) from which this point was generated</li>\n</ol>\n<p>Let's unpack this a bit.</p>\n<p>Under an (erroneous) MLE formulation, we would optimize the following objective:</p>\n<div class=\"math\">$$\n\\theta, c = \\underset{\\theta, c}{\\arg\\max}\\  \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log{p_{\\text{model}}}(x)]\n$$</div>\n<p>Under NCE, we're going to replace two pieces so as to perform the binary classification task described above (with 1 = \"model\", and 0 = \"noise\").</p>\n<p>First, let's swap <span class=\"math\">\\(\\log{p_{\\text{model}}}(x)\\)</span> with <span class=\"math\">\\(\\log{p_{\\text{joint}}}(y = 0\\vert x)\\)</span>, where:</p>\n<div class=\"math\">$$\np_{\\text{joint}}(x\\vert y) =\n\\begin{cases}\np_{\\text{noise}}(x)\\quad y = 0\\\\\np_{\\text{model}}(x)\\quad y = 1\\\\\n\\end{cases}\n$$</div>\n<div class=\"math\">$$\np_{\\text{joint}}(x, y)\n= p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x) + p_{\\text{joint}}(y = 1)p_{\\text{model}}(x)\n$$</div>\n<div class=\"math\">$$\np_{\\text{joint}}(y = 0\\vert x)\n= \\frac{p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x)}{p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x) + p_{\\text{joint}}(y = 1)p_{\\text{model}}(x)}\n$$</div>\n<p>Finally:</p>\n<div class=\"math\">$$\n\\theta, c = \\underset{\\theta, c}{\\arg\\max}\\  \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log{p_{\\text{joint}}(y = 0\\vert x)}]\n$$</div>\n<p>From here, we need to update <span class=\"math\">\\(x \\sim p_{\\text{data}}\\)</span> to include <span class=\"math\">\\(y\\)</span>. We'll do this in two pedantic steps.</p>\n<p>First, let's write:</p>\n<div class=\"math\">$$\n\\theta, c = \\underset{\\theta, c}{\\arg\\max}\\  \\mathbb{E}_{x, y=0\\ \\sim\\ p_{\\text{noise}}} [\\log{p_{\\text{joint}}(y\\vert x)}]\n$$</div>\n<p>This equation:</p>\n<ol>\n<li>Builds a classifier that discriminates between samples generated from the model distribution and noise distribution <strong>trained only on samples from the latter.</strong> (Clearly, this will not make for an effective classifier.)</li>\n<li>To train this classifier, we note that the equation asks us to maximize the likelihood of the noise samples under the noise distribution—where the noise distribution itself has no actual parameters we intend to train!</li>\n</ol>\n<p>In solution, we trivially expand our expectation to one over both noise samples, and data samples. In doing so, in predicting <span class=\"math\">\\(\\log{p_{\\text{joint}}(y = 1\\vert x)} = 1 - \\log{p_{\\text{joint}}(y = 0\\vert x)}\\)</span>, <strong>we'll be maximizing the likelihood of the data under the model.</strong></p>\n<div class=\"math\">$$\n\\theta, c = \\underset{\\theta, c}{\\arg\\max}\\  \\mathbb{E}_{x, y\\ \\sim\\ p_{\\text{train}}} [\\log{p_{\\text{joint}}(y \\vert x)}]\n$$</div>\n<p>where:</p>\n<div class=\"math\">$$\np_{\\text{train}}(x\\vert y) =\n\\begin{cases}\np_{\\text{noise}}(x)\\quad y = 0\\\\\np_{\\text{data}}(x)\\quad y = 1\\\\\n\\end{cases}\n$$</div>\n<p>As a final step, we'll expand our object into something more elegant:</p>\n<div class=\"math\">$$\n\\begin{align*}\np_{\\text{joint}}(y = 0\\vert x)\n&amp;= \\frac{p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x)}{p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x) + p_{\\text{joint}}(y = 1)p_{\\text{model}}(x)}\\\\\n&amp;= \\frac{1}{1 + \\frac{p_{\\text{joint}}(y = 1)p_{\\text{model}}(x)}{p_{\\text{joint}}(y = 0)p_{\\text{noise}}(x)}}\\\\\n\\end{align*}\n$$</div>\n<p>Assuming <em>a priori</em> that <span class=\"math\">\\(p_{\\text{joint}}(x, y)\\)</span> is <span class=\"math\">\\(k\\)</span> times more likely to generate a noise sample, i.e. <span class=\"math\">\\(\\frac{p_{\\text{joint}}(y = 1)}{p_{\\text{joint}}(y = 0)} = \\frac{1}{k}\\)</span>:</p>\n<div class=\"math\">$$\n\\begin{align*}\np_{\\text{joint}}(y = 0\\vert x)\n&amp;= \\frac{1}{1 + \\frac{p_{\\text{model}}(x)}{p_{\\text{noise}}(x)\\cdot k}}\\\\\n&amp;= \\frac{1}{1 + \\exp\\big(\\log{\\frac{p_{\\text{model}}(x)}{{p_{\\text{noise}}(x)\\cdot k}}}\\big)}\\\\\n&amp;= \\sigma\\bigg(-\\log{\\frac{p_{\\text{model}}(x)}{{p_{\\text{noise}}(x)\\cdot k}}}\\bigg)\\\\\n&amp;= \\sigma\\bigg(\\log{k} + \\log{p_{\\text{noise}}(x)} - \\log{p_{\\text{model}}(x)}\\bigg)\\\\\np_{\\text{joint}}(y = 1\\vert x)\n&amp;= 1 - \\sigma\\bigg(\\log{k} + \\log{p_{\\text{noise}}(x)} - \\log{p_{\\text{model}}(x)}\\bigg)\n\\end{align*}\n$$</div>\n<p>Given a joint training distribution over <span class=\"math\">\\((X_{\\text{data}}, y=1)\\)</span> and <span class=\"math\">\\((X_{\\text{noise}}, y=0)\\)</span>, this is the target we'd like to maximize.</p>\n<h2>Implications</h2>\n<p>For our training data, <strong>we require the ability to sample from our noise distribution.</strong></p>\n<p>For our target, <strong>we require the ability to compute the likelihood of some data under our noise distribution.</strong></p>\n<p>Therefore, these criteria do place practical restrictions on the types of noise distributions that we're able to consider.</p>\n<h2>Extensions</h2>\n<p>We briefly alluded to the fact that our noise distribution is non-parametric. However, there is nothing stopping us from evolving this distribution and giving it trainable parameters, then updating these parameters such that it generates increasingly \"optimal\" samples.</p>\n<p>Of course, we would have to design what \"optimal\" means. One interesting approach is called <a href=\"https://arxiv.org/abs/1805.03642\">Adversarial Contrastive Estimation\n</a>, wherein the authors adapt the noise distribution to generate increasingly \"harder negative examples, which forces the main model to learn a better representation of the data.\"<sup id=\"fnref:2\"><a class=\"footnote-ref\" href=\"#fn:2\">2</a></sup></p>\n<h2>Negative sampling</h2>\n<p>Negative sampling is the same as NCE except:</p>\n<ol>\n<li>We consider noise distributions whose likelihood we cannot evaluate</li>\n<li>To accommodate, we simply set <span class=\"math\">\\(p_{\\text{noise}}(x) = 1\\)</span></li>\n</ol>\n<p>Therefore:</p>\n<div class=\"math\">$$\n\\begin{align*}\np_{\\text{joint}}(y = 0\\vert x)\n&amp;= \\frac{1}{1 + \\frac{p_{\\text{model}}(x)}{p_{\\text{noise}}(x)\\cdot k}}\\\\\n&amp;= \\frac{1}{1 + \\frac{p_{\\text{model}}(x)}{ k}}\\\\\n&amp;= \\frac{1}{1 + \\exp\\big(\\log\\frac{p_{\\text{model}}(x)}{ k}\\big)}\\\\\n&amp;=\\sigma(-\\log\\frac{p_{\\text{model}}(x)}{ k})\\\\\n&amp;=\\sigma(\\log{k} - \\log{p_{\\text{model}}(x)})\\\\\np_{\\text{joint}}(y = 1\\vert x)\n&amp;= 1 - \\sigma(\\log{k} - \\log{p_{\\text{model}}(x)})\n\\end{align*}\n$$</div>\n<h2>In code</h2>\n<p>Since I learn best by implementing things, let's play around. Below, we train Boltzmann machines via noise contrastive estimation and negative sampling.</p>\n<h2>Load data</h2>\n<p>For this exercise, we'll fit a Boltzmann machine to the <a href=\"https://www.kaggle.com/zalando-research/fashionmnist\">Fashion MNIST</a> dataset.</p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/additional-strategies-partition-function/output_3_0.png\"/></p>\n<h2>Define model</h2>\n<p>Below, as opposed to in the previous post, I offer a vectorized implementation of the Boltzmann energy function.</p>\n<p>This said, the code is still imperfect: especially re: the line in which I iterate through data points individually to compute the joint likelihood.</p>\n<p>Finally, in <code>Model._H</code>, I divide by 1000 to get this thing to train. The following is only a toy exercise (like many of my posts); I did not spend much time tuning parameters.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">z</span><span class=\"p\">))</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Model</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">n_units</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">manual_seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">,</span> <span class=\"n\">n_units</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"o\">.</span><span class=\"n\">xavier_uniform_</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Parameter</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">FloatTensor</span><span class=\"p\">([</span><span class=\"mf\">1.</span><span class=\"p\">]))</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">diagonal_mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"o\">~</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">byte</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_units</span> <span class=\"o\">=</span> <span class=\"n\">n_units</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_likelihood</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"o\">=</span><span class=\"n\">log</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_unnormalized_likelihood</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_H</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_H</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">H</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">diagonal_mask</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">triu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">weight</span> <span class=\"o\">*</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">ger</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)))</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">H</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">_likelihood</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n        <span class=\"sd\">\"\"\"</span>\n<span class=\"sd\">        :param x: a vector of shape (n_units,) or (n, n_units),</span>\n<span class=\"sd\">            where the latter is a matrix of multiple data points</span>\n<span class=\"sd\">            for which to compute the joint likelihood</span>\n\n<span class=\"sd\">        :return: the likelihood, or log-likelihood if `log=True`</span>\n<span class=\"sd\">        \"\"\"</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">n_units</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span> <span class=\"ow\">and</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">):</span>\n            <span class=\"k\">raise</span><span class=\"p\">(</span><span class=\"s1\">'Please pass 1 or more points of `n_units` dimensions'</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># compute unnormalized likelihoods</span>\n        <span class=\"n\">multiple_samples</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">2</span>\n        <span class=\"k\">if</span> <span class=\"n\">multiple_samples</span><span class=\"p\">:</span>\n            <span class=\"n\">likelihood</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_unnormalized_likelihood</span><span class=\"p\">(</span><span class=\"n\">point</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">point</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">]</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">likelihood</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_unnormalized_likelihood</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)]</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">log</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">lik</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">lik</span> <span class=\"ow\">in</span> <span class=\"n\">likelihood</span><span class=\"p\">])</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">lik</span> <span class=\"o\">/</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">c</span> <span class=\"k\">for</span> <span class=\"n\">lik</span> <span class=\"ow\">in</span> <span class=\"n\">likelihood</span><span class=\"p\">])</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">sample</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">init_sample</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"n\">burn_in</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">,</span> <span class=\"n\">every_n</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">:</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">burn_in</span> <span class=\"o\">&gt;</span> <span class=\"n\">n_samples</span><span class=\"p\">:</span>\n            <span class=\"n\">n_samples</span> <span class=\"o\">+=</span> <span class=\"n\">burn_in</span>\n\n        <span class=\"n\">init_sample</span> <span class=\"o\">=</span> <span class=\"n\">init_sample</span> <span class=\"ow\">or</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"p\">)</span>\n        <span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">init_sample</span><span class=\"p\">]</span>\n\n        <span class=\"k\">def</span> <span class=\"nf\">_gibbs_step</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">):</span>\n            <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"nb\">sum</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">sample</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">))</span> <span class=\"k\">if</span> <span class=\"n\">j</span> <span class=\"o\">!=</span> <span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">p</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"p\">):</span>\n            <span class=\"n\">sample</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>  <span class=\"c1\"># make copy</span>\n            <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">):</span>\n                <span class=\"n\">sample</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">_gibbs_step</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"o\">=</span><span class=\"n\">sample</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"p\">)</span>\n            <span class=\"n\">samples</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">(</span><span class=\"n\">sample</span><span class=\"p\">)</span> <span class=\"p\">)</span>\n\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">sample</span> <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">sample</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">burn_in</span><span class=\"p\">:])</span> <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"n\">every_n</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n</pre></div>\n<h2>Noise contrastive estimation</h2>\n<p>Train a model using noise contrastive estimation. For our noise distribution, we'll start with a diagonal multivariate Gaussian, from which we can sample, and whose likelihood we can evaluate (as of PyTorch 0.4!).</p>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># define model, noise distribution</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">)</span>\n<span class=\"n\">noise</span> <span class=\"o\">=</span> <span class=\"n\">MultivariateNormal</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">),</span> <span class=\"n\">covariance_matrix</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># define classifier. we add a multiplicative constant to make training more stable.</span>\n<span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">([</span><span class=\"n\">k</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">noise</span><span class=\"o\">.</span><span class=\"n\">log_prob</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mi\">10000</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># define noise generator</span>\n<span class=\"n\">noise_sample</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">noise</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">sample_shape</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Size</span><span class=\"p\">([</span><span class=\"n\">BATCH_SIZE</span> <span class=\"o\">*</span> <span class=\"n\">k</span><span class=\"p\">]))</span>\n<span class=\"n\">noiseloader</span> <span class=\"o\">=</span> <span class=\"p\">(</span> <span class=\"p\">(</span><span class=\"n\">noise_sample</span><span class=\"p\">(),</span> <span class=\"n\">_</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">))</span> <span class=\"p\">)</span>\n\n<span class=\"c1\"># define optimizer, loss</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=.</span><span class=\"mi\">01</span><span class=\"p\">)</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">BCELoss</span><span class=\"p\">()</span>\n</pre></div>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">train_model</span><span class=\"p\">(</span><span class=\"n\">classifier</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">noiseloader</span><span class=\"p\">,</span> <span class=\"n\">n_batches</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">'inf'</span><span class=\"p\">),</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">):</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">noise</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">noiseloader</span><span class=\"p\">)):</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">&lt;</span> <span class=\"n\">n_batches</span><span class=\"p\">:</span>\n\n            <span class=\"c1\"># points from data distribution</span>\n            <span class=\"n\">X_data</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">data</span>\n            <span class=\"n\">X_data</span> <span class=\"o\">=</span> <span class=\"n\">X_data</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n_units</span><span class=\"p\">)</span>\n            <span class=\"n\">y_data</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">FloatTensor</span><span class=\"p\">([</span><span class=\"mf\">1.</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">X_data</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))])</span>\n\n            <span class=\"c1\"># points from noise distribution</span>\n            <span class=\"n\">X_noise</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">noise</span>\n            <span class=\"n\">X_noise</span> <span class=\"o\">=</span> <span class=\"n\">X_noise</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n_units</span><span class=\"p\">)</span>\n            <span class=\"n\">y_noise</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">FloatTensor</span><span class=\"p\">([</span><span class=\"mf\">0.</span> <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">X_noise</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))])</span>\n\n            <span class=\"c1\"># stack into single input</span>\n            <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">X_data</span><span class=\"p\">,</span> <span class=\"n\">X_noise</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n_units</span><span class=\"p\">)</span>\n            <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cat</span><span class=\"p\">([</span><span class=\"n\">y_data</span><span class=\"p\">,</span> <span class=\"n\">y_noise</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n            <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n            <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n\n            <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n            <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">10</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'Batch: </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s1\"> | Loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">verbose</span><span class=\"p\">:</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'weights.mean(): </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'bias.mean(): </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'c: </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'weights.grad.mean(): </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">weight</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'bias.grad.mean(): </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">params</span><span class=\"o\">.</span><span class=\"n\">bias</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n                    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">'c.grad: </span><span class=\"si\">{</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">grad</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"si\">:</span><span class=\"s1\">.3</span><span class=\"si\">}</span><span class=\"s1\">'</span><span class=\"p\">)</span>\n</pre></div>\n<h2>Train model</h2>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">classifier</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">noiseloader</span><span class=\"p\">,</span> <span class=\"n\">n_batches</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre></div>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">0</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.305</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">10</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0887</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">20</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0794</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">30</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0603</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">40</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0525</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">50</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0503</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">60</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0414</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">70</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.038</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">80</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.034</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">90</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0312</span>\n</pre></div>\n<h2>Negative sampling</h2>\n<p>Next, we'll try negative sampling using some actual images as negative samples</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">noiseset</span> <span class=\"o\">=</span> <span class=\"n\">dset</span><span class=\"o\">.</span><span class=\"n\">mnist</span><span class=\"o\">.</span><span class=\"n\">MNIST</span><span class=\"p\">(</span><span class=\"n\">root</span> <span class=\"o\">=</span> <span class=\"s1\">'data/mnist'</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">)</span>\n<span class=\"n\">noiseloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">noiseset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">BATCH_SIZE</span> <span class=\"o\">*</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># get some random training images</span>\n<span class=\"n\">dataiter</span> <span class=\"o\">=</span> <span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">noiseloader</span><span class=\"p\">)</span>\n<span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">dataiter</span><span class=\"o\">.</span><span class=\"n\">next</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># show images</span>\n<span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">make_grid</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">))</span>\n</pre></div>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/additional-strategies-partition-function/output_12_0.png\"/></p>\n<h2>Train model</h2>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># define model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">n_units</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># define classifier</span>\n<span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">([</span><span class=\"n\">k</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">log</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">())</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=.</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># i had to change this learning rate to get this to train</span>\n\n<span class=\"c1\"># train</span>\n<span class=\"n\">train_model</span><span class=\"p\">(</span><span class=\"n\">classifier</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">trainloader</span><span class=\"p\">,</span> <span class=\"n\">noiseloader</span><span class=\"p\">,</span> <span class=\"n\">n_batches</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n</pre></div>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">0</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.304</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">10</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.027</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">20</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0111</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">30</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.00611</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">40</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.00505</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">50</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.00318</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">60</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.00284</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">70</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0029</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">80</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0023</span>\n<span class=\"n\">Batch</span><span class=\"o\">:</span> <span class=\"mi\">90</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.00217</span>\n</pre></div>\n<h1>Sampling</h1>\n<p>Once more, the (ideal) goal of this model is to fit a function <span class=\"math\">\\(p(x)\\)</span> to some data, such that we can:</p>\n<ol>\n<li>Evaluate its likelihood (wherein it actually tells us that data on which the model was fit is more likely than data on which it was not)</li>\n<li>Draw realistic samples</li>\n</ol>\n<p>From a Boltzmann machine, our primary strategy for drawing samples is via Gibbs sampling. It's slow, and I do not believe it's meant to work particularly well. Let's draw 5 samples and see how we do.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">%%</span><span class=\"n\">time</span>\n\n<span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">30</span><span class=\"p\">,</span> <span class=\"n\">burn_in</span><span class=\"o\">=</span><span class=\"mi\">25</span><span class=\"p\">,</span> <span class=\"n\">every_n</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</pre></div>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">CPU times: user 4min 10s, sys: 4.09 s, total: 4min 14s</span>\n<span class=\"err\">Wall time: 4min 17s</span>\n</pre></div>\n<p>Takes forever!</p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/additional-strategies-partition-function/output_18_0.png\"/></p>\n<p>Nothing great. These samples are highly correlated, if perfectly identical, as expected.</p>\n<p>To generate better images, we'll have to let this run for a lot longer and \"thin\" the chain (taking every <code>every_n</code> samples, where <code>every_n</code> is on the order of 1, 10, or 100, roughly).</p>\n<h1>Summary</h1>\n<p>In this post, we discussed four additional strategies for both speeding up, as well as outright avoiding, the computation of the gradient of the log-partition function <span class=\"math\">\\(\\nabla\\log{Z}\\)</span>.</p>\n<p>While we only presented toy models here, these strategies see successful application in larger undirected graphical models, as well as directed conditional models for <span class=\"math\">\\(p(y\\vert x)\\)</span>. One key example of the latter is a language model; though the partition function is a sum over distinct values of <span class=\"math\">\\(y\\)</span> (labels) instead of configurations of <span class=\"math\">\\(x\\)</span> (inputs), it can still be intractable to compute! This is because there are as many distinct values of <span class=\"math\">\\(y\\)</span> as there are tokens in the given language's vocabulary, which is typically on the order of millions.</p>\n<p>Thanks for reading.</p>\n<h2>Code</h2>\n<p>The <a href=\"https://github.com/cavaunpeu/boltzmann-machines\">repository</a> and <a href=\"https://nbviewer.jupyter.org/github/cavaunpeu/boltzmann-machines/blob/master/boltzmann-machines-part-2.ipynb\">rendered notebook</a> for this project can be found at their respective links.</p>\n<h2>References</h2>\n<div class=\"footnote\">\n<hr/>\n<ol>\n<li id=\"fn:1\">\n<p>@book{Goodfellow-et-al-2016,\ntitle={Deep Learning},\nauthor={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\npublisher={MIT Press},\nnote={\\url{http://www.deeplearningbook.org}},\nyear={2016}\n} <a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">↩</a><a class=\"footnote-backref\" href=\"#fnref2:1\" title=\"Jump back to footnote 1 in the text\">↩</a></p>\n</li>\n<li id=\"fn:2\">\n<p><a href=\"https://arxiv.org/abs/1805.03642\">Adversarial Contrastive Estimation</a> <a class=\"footnote-backref\" href=\"#fnref:2\" title=\"Jump back to footnote 2 in the text\">↩</a></p>\n</li>\n</ol>\n</div>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}