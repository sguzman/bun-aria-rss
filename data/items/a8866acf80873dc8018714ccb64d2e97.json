{
  "title": "Statistical methods that only work if you don&#8217;t use them (more precisely, they only work well if you avoid using them in the cases where they will fail)",
  "link": "https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/",
  "comments": "https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/#comments",
  "dc:creator": "Andrew",
  "pubDate": "Sat, 22 Oct 2022 13:47:29 +0000",
  "category": [
    "Bayesian Statistics",
    "Decision Theory",
    "Miscellaneous Statistics"
  ],
  "guid": "https://statmodeling.stat.columbia.edu/?p=47611",
  "description": "Here are a couple examples. 1. Bayesian inference You conduct an experiment to estimate a parameter theta. Your experiment produces an unbiased estimate theta_hat with standard error 1.0 (on some scale). Assume the experiment is clean enough that you&#8217;re ok &#8230; <a href=\"https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>Here are a couple examples.</p>\n<p><strong>1.  Bayesian inference</strong></p>\n<p>You conduct an experiment to estimate a parameter theta.  Your experiment produces an unbiased estimate theta_hat with standard error 1.0 (on some scale).  Assume the experiment is clean enough that you&#8217;re ok with the data model, theta_hat ~ normal(theta, 1.0).  Now suppose that theta_hat happens to equal 1, and further suppose you are doing Bayesian inference with a uniform prior on theta (or, equivalently, a very weak prior such as normal(0, 10)).  Then your posterior distribution is theta ~ normal(1, 1), and the posterior probability is 84% that theta is greater than zero.  (See section 3 <a href=\"http://www.stat.columbia.edu/~gelman/research/published/physics.pdf\">here</a>.)  You then should be willing to bet that theta is greater than zero with 5-1 odds (assuming you&#8217;re not planning to bet against anyone with private information about theta).  OK, maybe not 5-1 because you&#8217;re concerned about the <a href=\"https://statmodeling.stat.columbia.edu/2015/06/19/in-which-a-complete-stranger-offers-me-a-bet/\">jack of spades</a>, etc.  So, 4-1.</p>\n<p>Still, it&#8217;s a problem, if you&#8217;re willing to routinely offer 4-1 bets on data that are consistent with noise (which is one way to put it when you observe an estimate that&#8217;s 1 standard error away from zero).  Go around offering those bets to people and you&#8217;ll soon lose all your money.</p>\n<p>OK, yeah, the problem is with the flat prior.  But that&#8217;s something that people do!  And the flat-prior-analysis isn&#8217;t terrible; it can summarize the data in useful ways, even if you shouldn&#8217;t &#8220;believe&#8221; or lay bets on all its implications.</p>\n<p>In practice, what do we do?  We use the Bayesian inference selectively, carefully.  We report the 95% posterior interval for theta, (-1, 3), but we don&#8217;t talk about the posterior probability that theta is positive, and we don&#8217;t publicize those 5-1 odds.  Similarly, when an estimate is two standard errors away from zero, we consider it as representing some evidence for a positive effect, but we wouldn&#8217;t bet at 39-1 odds.  In a sense, we&#8217;re acting as if we have a high <a href=\"http://www.stat.columbia.edu/~gelman/research/published/default_prior_zwet.pdf\">prior probability</a> that theta is close to zero&#8212;but not quite, as we&#8217;re still giving out that (-1, 3) interval.  We&#8217;re being incoherent, which is fine&#8212;you know what they say about foolish consistency&#8212;but in any case we should be aware of this incoherence.</p>\n<p><strong>2.  Null hypothesis significance testing</strong></p>\n<p>You have a <a href=\"http://www.stat.columbia.edu/~gelman/research/published/kanazawa.pdf\">hypothesis</a> that beauty is related to the sex ratio of babies, so you find some data and compare the proportion of girl births from attractive and unattractive parents.  You&#8217;ve heard about this thing called <a href=\"http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf\">forking paths</a> so you preregister your study.  Fortunately, you still find something statistically significant!  OK, it&#8217;s not quite what you preregistered, and it goes in the wrong direction, and it&#8217;s only significant at the 10% level, but <a href=\"https://statmodeling.stat.columbia.edu/2018/05/29/exposure-forking-paths-affects-support-publication/\">that&#8217;s still enough</a> to get the paper published in a top journal and have it receive two awards.</p>\n<p>OK, yeah, the problem is the famous incoherence of classical statistical practice.  What exactly is that p-value?  Is it a measure of evidence (p less than 0.01 is strong evidence, p less than 0.05 is ok evidence, p less than 0.1 is weak evidence, anything else counts as no evidence at all), or is it a hard rule (p less than 0.05 gets converted into Yes, p more than 0.05 becomes No)?  It&#8217;s not clear.  The procedure described in the paragraph immediately above corresponds to treating the p-value as evidence, and this leads to obvious problems so maybe you should just use the hard rule, but that puts you in the uncomfortable position of making strong statements based on noisy data.</p>\n<p>In practice, what do we do?  It&#8217;s a mix.  We use that hard 5% rule on the preregistered hypothesis&#8212;ok, not always, as indicated by the above link, but often&#8212;and then we also report p-values as evidence.  Again, incoherence is not in itself a problem, but it can lead to a worst-of-both-worlds situation as we&#8217;ve seen in Psychological Science, PNAS, etc., of a literatures that drift based on some mixture of speculation and noise.</p>\n<p><strong>3.  Where are we, then?</strong></p>\n<p>I&#8217;m not saying Bayes is wrong or even that null hypothesis significance testing is wrong.  These methods have their place.  What I&#8217;m saying is that they depend on assumptions, and we don&#8217;t always check these assumptions.</p>\n<p>To put it another way, Bayesian methods and null hypothesis significance testing methods work&#8212;really, they work for solving engineering problems and increasing our scientific understanding, I&#8217;m not just saying they &#8220;work&#8221; to get papers published&#8212;but the way to get them to work is to use them judiciously, to walk around all the land mines.  The good news is that you can use fake-data simulation to find out where those land mines are.</p>\n",
  "wfw:commentRss": "https://statmodeling.stat.columbia.edu/2022/10/22/statistical-methods-that-only-work-if-you-dont-use-them/feed/",
  "slash:comments": 26
}