{
  "title": "Improve price performance of your model training using Amazon SageMaker heterogeneous clusters",
  "link": "https://aws.amazon.com/blogs/machine-learning/improve-price-performance-of-your-model-training-using-amazon-sagemaker-heterogeneous-clusters/",
  "dc:creator": "Gili Nachum",
  "pubDate": "Thu, 27 Oct 2022 16:41:15 +0000",
  "category": [
    "Amazon SageMaker",
    "Artificial Intelligence"
  ],
  "guid": "4074eda066f3ecbe4b8b8bf6add2af4c827e9212",
  "description": "This post is co-written with Chaim Rand from Mobileye. Certain machine learning (ML) workloads, such as training computer vision models or reinforcement learning, often involve combining the GPU- or accelerator-intensive task of neural network model training with the CPU-intensive task of data preprocessing, like image augmentation. When both types of tasks run on the same […]",
  "content:encoded": "<p><em>This post is co-written with Chaim Rand from Mobileye.</em></p> \n<p>Certain machine learning (ML) workloads, such as training computer vision models or reinforcement learning, often involve combining the GPU- or accelerator-intensive task of neural network model training with the CPU-intensive task of data preprocessing, like image augmentation. When both types of tasks run on the same instance type, the data preprocessing gets bottlenecked on CPU, leading to lower GPU utilization. This issue becomes worse with time as the throughput of newer generations of GPUs grows at a steeper pace than that of CPUs.</p> \n<p>To address this issue, in July 2022, we <a href=\"https://aws.amazon.com/about-aws/whats-new/2022/07/announcing-heterogeneous-clusters-amazon-sagemaker-model-training/\" target=\"_blank\" rel=\"noopener noreferrer\">launched</a> heterogeneous clusters for <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker</a> model training, which enables you to launch training jobs that use different instance types in a single job. This allows offloading parts of the data preprocessing pipeline to <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html\" target=\"_blank\" rel=\"noopener noreferrer\">compute-optimized</a> instance types, whereas the deep neural network (DNN) task continues to run on <a href=\"https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing\" target=\"_blank\" rel=\"noopener noreferrer\">GPU or accelerated computing</a> instance types. <strong>Our benchmarks show up to 46% price performance benefit after enabling heterogeneous clusters in a CPU-bound TensorFlow computer vision model training.</strong></p> \n<p>For a similar use case, <a href=\"https://www.mobileye.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Mobileye</a>, an autonomous vehicle technologies development company, had this to share:</p> \n<p style=\"padding-left: 40px\"><em>“By moving CPU-bound deep learning computer vision model training to run over multiple instance types (CPU and GPU/ML accelerators), using a <code>tf.data.service</code> based solution we’ve built, we managed to reduce time to train by 40% while reducing the cost to train by 30%. We’re excited about heterogeneous clusters allowing us to run this solution on Amazon SageMaker.” </em></p> \n<p style=\"padding-left: 40px\">— AI Engineering, Mobileye</p> \n<p>In this post, we discuss the following topics:</p> \n<ul> \n <li>How heterogeneous clusters help remove CPU bottlenecks</li> \n <li>When to use heterogeneous clusters, and other alternatives</li> \n <li>Reference implementations in PyTorch and TensorFlow</li> \n <li>Performance benchmark results</li> \n <li>Heterogeneous clusters at Mobileye</li> \n</ul> \n<p>AWS’s <a href=\"https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing\" target=\"_blank\" rel=\"noopener noreferrer\">accelerated computing instance</a> family includes accelerators from AWS custom chips (<a href=\"https://aws.amazon.com/machine-learning/inferentia/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Inferentia</a>, <a href=\"https://aws.amazon.com/machine-learning/trainium/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Trainium</a>), NVIDIA (<a href=\"https://aws.amazon.com/nvidia/\" target=\"_blank\" rel=\"noopener noreferrer\">GPUs</a>), and <a href=\"https://aws.amazon.com/ec2/instance-types/dl1/\" target=\"_blank\" rel=\"noopener noreferrer\">Gaudi accelerators</a> from Habana Labs (an Intel company). Note that in this post, we use the terms GPU and accelerator interchangeably.</p> \n<h2>How heterogeneous clusters remove data processing bottlenecks</h2> \n<p>Data scientists who train deep learning models aim to maximize training cost-efficiency and minimize training time. To achieve this, one basic optimization goal is to have high GPU utilization, the most expensive and scarce resource within the <a href=\"http://aws.amazon.com/ec2\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instance. This can be more challenging with ML workloads that combine the classic GPU-intensive neural network model’s forward and backward propagation with CPU-intensive tasks, such as data processing and augmentation in computer vision or running an environment simulation in reinforcement learning. These workloads can end up being CPU bound, where having more CPU would result in higher throughput and faster and cheaper training as existing accelerators are partially idle. In some cases, CPU bottlenecks can be solved by switching to another instance type with a higher CPU:GPU ratio. However, there are situations where switching to another instance type may not be possible due to the instance family’s architecture, storage, or networking dependencies.</p> \n<p>In such situations, you have to increase the amount of CPU power by mixing instance types: instances with GPUs together with CPU. Summed together, this results in an overall higher CPU:GPU ratio. Until recently, SageMaker training jobs were limited to having instances of a single chosen instance type. With SageMaker heterogeneous clusters, data scientists can easily run a training job with multiple instance types, which enables offloading some of the existing CPU tasks from the GPU instances to dedicated compute-optimized CPU instances, resulting in higher GPU utilization and faster and more cost-efficient training. Moreover, with the extra CPU power, you can have preprocessing tasks that were traditionally done offline as a preliminary step to training become part of your training job. This makes it faster to iterate and experiment over both data preprocessing and DNN training assumptions and hyperparameters.</p> \n<p>For example, consider a powerful GPU instance type, ml.p4d.24xlarge (96 vCPU, 8 x NVIDIA <a href=\"https://www.nvidia.com/en-us/data-center/a100/\" target=\"_blank\" rel=\"noopener noreferrer\">A100</a> GPUs), with a CPU:GPU ratio of 12:1. Let’s assume your training job needs 20 vCPUs to preprocess enough data to keep one GPU 100% utilized. Therefore, to keep all 8 GPUs 100% utilized, you need a 160 vCPUs instance type. However, ml.p4d.24xlarge is short of 64 vCPUs, or 40%, limiting GPU utilization to 60%, as depicted on the left of the following diagram. Would adding another ml.p4d.24xlarge instance help? No, because the job’s CPU:GPU ratio would remain the same.</p> \n<p>With heterogeneous clusters, we can add two ml.c5.18xlarge (72 vCPU), as shown on the right of the diagram. The net total vCPU in this cluster is 210 (96+2*72), leading to a CPU:GPU ratio to 30:1. Each of these compute-optimized instances will be offloaded with a data preprocessing CPU-intensive task, and will allow efficient GPU utilization. Despite the extra cost of the ml.c5.18xlarge, the higher GPU utilization allows faster processing, and therefore higher price performance benefits.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image001.png\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-44759\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image001.png\" alt=\"\" width=\"1393\" height=\"553\"></a></p> \n<h2>When to use heterogeneous clusters, and other alternatives</h2> \n<p>In this section, we explain how to identify a CPU bottleneck, and discuss solving it using instance type scale up vs. heterogeneous clusters.</p> \n<p>The quick way to identify a CPU bottleneck is to monitor CPU and GPU <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-jobs\" target=\"_blank\" rel=\"noopener noreferrer\">utilization metrics</a> for SageMaker training jobs in <a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon CloudWatch</a>. You can access these views from the <a href=\"http://aws.amazon.com/console\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Management Console</a> within the training job page’s instance metrics hyperlink. Pick the relevant metrics and switch from 5-minute to 1-minute resolution. Note that the scale is 100% per vCPU or GPU, so the utilization rate for an instance with 4 vCPUs/GPUs could be as high as 400%. The following figure is one such example from CloudWatch metrics, where CPU is approximately 100% utilized, indicating a CPU bottleneck, whereas GPU is underutilized.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image003.jpg\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-44760\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image003.jpg\" alt=\"\" width=\"1540\" height=\"298\"></a></p> \n<p>For detailed diagnosis, run the training jobs with <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profile-training-jobs.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Debugger</a> to profile resource utilization status, statistics, and framework operations, by adding a profiler configuration when you construct a SageMaker estimator using the SageMaker Python SDK. After you submit the training job, review the resulting <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profiling-report.html#debugger-profiling-report-walkthrough-cpu-bottlenecks\" target=\"_blank\" rel=\"noopener noreferrer\">profiler report</a> for CPU bottlenecks.</p> \n<p>If you conclude that your job could benefit from a higher CPU:GPU compute ratio, first consider scaling up to another instance type in the same instance family, if one is available. For example, if you’re training your model on ml.g5.8xlarge (32 vCPUs, 1 GPU), consider scaling up to ml.g5.16xlarge (64 vCPUs, 1 GPU). Or, if you’re training your model using multi-GPU instance ml.g5.12xlarge (48 vCPUs, 4 GPUs), consider scaling up to ml.g5.24xlarge (96 vCPUs, 4 GPUs). Refer to the <a href=\"https://aws.amazon.com/ec2/instance-types/g5/\" target=\"_blank\" rel=\"noopener noreferrer\">G5</a> instance family specification for more details.</p> \n<p>Sometimes, scaling up isn’t an option, because there is no instance type with a higher vCPU:GPU ratio in the same instance family. For example, if you’re training the model on ml.trn1.32xlarge, ml.p4d.24xlarge, or <a id=\"_Int_LTKxDbZb\" target=\"_blank\" rel=\"noopener noreferrer\">ml.g5.48xlarge, you should consider heterogeneous clusters for SageMaker model training.</a></p>\n<a id=\"_Int_LTKxDbZb\" target=\"_blank\" rel=\"noopener noreferrer\"> </a>\n<p><a id=\"_Int_LTKxDbZb\" target=\"_blank\" rel=\"noopener noreferrer\">Besides scaling up, we’d like to note that there are additional alternatives to a heterogeneous cluster, like NVIDIA </a><a href=\"https://developer.nvidia.com/dali\" target=\"_blank\" rel=\"noopener noreferrer\">DALI</a>, which offloads image preprocessing to the GPU. For more information, refer to <a href=\"https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851\" target=\"_blank\" rel=\"noopener noreferrer\">Overcoming Data Preprocessing Bottlenecks with TensorFlow Data Service, NVIDIA DALI, and Other Methods</a>.</p> \n<p>To simplify decision-making, refer to the following flowchart.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image005.png\"><img loading=\"lazy\" class=\"size-full wp-image-44761 aligncenter\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/ML-11594-image005.png\" alt=\"\" width=\"469\" height=\"913\"></a></p> \n<h2>How to use SageMaker heterogeneous clusters</h2> \n<p>To get started quickly, you can directly jump to the TensorFlow or PyTorch examples provided as part of this post.</p> \n<p>In this section, we walk you through how to use a SageMaker heterogeneous cluster with a simple example. We assume that you already know how to train a model with the SageMaker Python SDK and the Estimator class. If not, refer to <a href=\"https://sagemaker.readthedocs.io/en/stable/overview.html\" target=\"_blank\" rel=\"noopener noreferrer\">Using the SageMaker Python SDK</a> before continuing.</p> \n<p>Prior to this feature, you initialized the training job’s Estimator class with the <code>InstanceCount</code> and InstanceType parameters, which implicitly assumes you only have a single instance type (a homogeneous cluster). With the release of heterogeneous clusters, we introduced the new <code>sagemaker.instance_group.InstanceGroup</code> class. This represents a group of one or more instances of a specific instance type, designed to carry a logical role (like data processing or neural network optimization. You can have two or more groups, and specify a custom name for each instance group, the instance type, and the number of instances for each instance group. For more information, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html#train-heterogeneous-cluster-configure-pysdk\" target=\"_blank\" rel=\"noopener noreferrer\">Using the SageMaker Python SDK</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html#train-heterogeneous-cluster-configure-api\" target=\"_blank\" rel=\"noopener noreferrer\">Using the Low-Level SageMaker APIs</a>.</p> \n<p>After you have defined the instance groups, you need to modify your training script to read the SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html#train-heterogeneous-cluster-modify-training-script-query-env-var\" target=\"_blank\" rel=\"noopener noreferrer\">training environment information</a> that includes heterogeneous cluster configuration. The configuration contains information such as the current instance groups, the current hosts in each group, and in which group the current host resides with their ranking. You can build logic in your training script to assign the instance groups to certain training and data processing tasks. In addition, your training script needs to take care of inter-instance group communication or distributed data loading mechanisms (for example, <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/experimental/service\" target=\"_blank\" rel=\"noopener noreferrer\">tf.data.service</a> in TensorFlow or generic <a href=\"https://grpc.io/docs/languages/python/basics/\" target=\"_blank\" rel=\"noopener noreferrer\">gRPC client-server</a>) or any other framework (for example, <a href=\"https://spark.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Spark</a>).</p> \n<p>Let’s go through a simple example of launching a heterogeneous training job and reading the environment configuration at runtime.</p> \n<ol> \n <li>When defining and launching the training job, we configure two instance groups used as arguments to the SageMaker estimator: <pre><code class=\"lang-python\">from sagemaker.instance_group import InstanceGroup\ndata_group = InstanceGroup(\"data_group\", \"ml.c5.18xlarge\", 2)\ndnn_group = InstanceGroup(\"dnn_group\", \"ml.p4d.24xlarge\", 1)\n\nfrom sagemaker.pytorch import PyTorch\nestimator = PyTorch(...,\n    entry_point='launcher.py',\n    instance_groups=[data_group, dnn_group]\n)</code></pre> </li> \n <li>On the entry point training script (named <code>launcher.py</code>), we read the heterogeneous cluster configuration to whether the instance will run the preprocessing or DNN code: \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\">from sagemaker_training import environment\nenv = environment.Environment()\nif env.current_instance_group == 'data_group': ...;</code></pre> \n  </div> </li> \n</ol> \n<p>With this, let’s summarize the tasks SageMaker does on your behalf, and the tasks that you are responsible for.</p> \n<p>SageMaker performs the following tasks:</p> \n<ol> \n <li>Provision different instance types according to instance group definition.</li> \n <li>Provision input channels on all or specific instance groups.</li> \n <li>Distribute training scripts and dependencies to instances.</li> \n <li>Set up an MPI cluster on a specific instance group, if defined.</li> \n</ol> \n<p>You are responsible for the following tasks:</p> \n<ol> \n <li>Modify your start training job script to specify instance groups.</li> \n <li>Implement a distributed data pipeline (for example, <code>tf.data.service</code>).</li> \n <li>Modify your entry point script (see <code>launcher.py</code> in the example notebook) to be a single entry point that will run on all the instances, detect which instance group it’s running in, and trigger the relevant behavior (such as data processing or DNN optimization).</li> \n <li>When the training loop is over, you must make sure that your entry point process exits on all instances across all instance groups. This is important because SageMaker waits for all the instances to finish processing before it marks the job as complete and stops billing. The <code>launcher.py</code> script in the TensorFlow and PyTorch example notebooks provides a reference implementation of signaling data group instances to exit when DNN group instances finish their work.</li> \n</ol> \n<h2>Example notebooks for SageMaker heterogeneous clusters</h2> \n<p>In this section, we provide a summary of the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/training/heterogeneous-clusters\" target=\"_blank\" rel=\"noopener noreferrer\">example notebooks</a> for both TensorFlow and PyTorch ML frameworks. In the notebooks, you can find the implementation details, walkthroughs on how the code works, code snippets that you could reuse in your training scripts, flow diagrams, and cost-comparison analysis.</p> \n<p>Note that in both examples, you shouldn’t expect the model to converge in a meaningful way. Our intent is only to measure the data pipeline and neural network optimization throughput expressed in epoch/step time. You must benchmark with your own model and dataset to produce price performance benefits that match your workload.</p> \n<h3>Heterogeneous cluster using a tf.data.service based distributed data loader (TensorFlow)</h3> \n<p>This <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/heterogeneous-clusters/tf.data.service.sagemaker/hetero-tensorflow-restnet50.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">notebook</a> demonstrates how to implement a heterogeneous cluster for SageMaker training using TensorFlow’s <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/experimental/service\" target=\"_blank\" rel=\"noopener noreferrer\"><code>tf.data.service</code></a> based distributed data pipeline. We train a deep learning computer vision model <a href=\"https://www.TensorFlow.org/api_docs/python/tf/keras/applications/ResNet50\" target=\"_blank\" rel=\"noopener noreferrer\">Resnet50</a> that requires CPU-intensive data augmentation. It uses <a href=\"https://github.com/horovod/horovod\" target=\"_blank\" rel=\"noopener noreferrer\">Horvod</a> for multi-GPU distributed data parallelism.</p> \n<p>We run the workload in two configurations: first as a homogeneous cluster, single ml.p4d.24xlarge instance, using a standard <code>tf.data</code> pipeline that showcases CPU bottlenecks leading to lower GPU utilization. In the second run, we switch from a single instance type to two instance groups using a SageMaker heterogeneous cluster. This run offloads some of the data processing to additional CPU instances (using <code>tf.data.service</code>).</p> \n<p>We then compare the homogeneous and heterogeneous configurations and find key price performance benefits. As shown in the following table, the heterogeneous job (86ms/step) is 2.2 times faster to train than the homogeneous job (192ms/step), making it 46% cheaper to train a model.</p> \n<table border=\"1px\"> \n <tbody> \n  <tr style=\"background-color: #000000\"> \n   <td><span style=\"color: #ffffff\"><strong>Example 1 (TF) </strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>ml.p4d.24xl</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>ml.c5.18xl</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Price per Hour*</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Average Step Time</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Cost per Step</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Price Performance Improvement</strong></span></td> \n  </tr> \n  <tr> \n   <td>Homogeneous</td> \n   <td>1</td> \n   <td>0</td> \n   <td>$37.688</td> \n   <td>192 ms</td> \n   <td>$0.201</td> \n   <td><span style=\"color: #ffffff\">.</span></td> \n  </tr> \n  <tr> \n   <td>Heterogeneous</td> \n   <td>1</td> \n   <td>2</td> \n   <td>$45.032</td> \n   <td>86 ms</td> \n   <td>$0.108</td> \n   <td>46%</td> \n  </tr> \n </tbody> \n</table> \n<p>* Price per hour is based on us-east-1 <a href=\"https://aws.amazon.com/sagemaker/pricing/\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker on-demand pricing</a></p> \n<p>This speedup is made possible by utilizing the extra vCPU, provided by the data group, and faster preprocessing. See the <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/heterogeneous-clusters/tf.data.service.sagemaker/hetero-tensorflow-restnet50.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">notebook</a> for more details and graphs.</p> \n<h3>Heterogeneous cluster using a gRPC client-server based distributed data loader (PyTorch)</h3> \n<p>This <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/heterogeneous-clusters/pt.grpc.sagemaker/hetero-pytorch-mnist.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">notebook</a> demonstrates a sample workload using a heterogeneous cluster for SageMaker training using a gRPC client-server based distributed data loader. This example uses a single GPU. We use the PyTorch model based on the following <a href=\"https://github.com/pytorch/examples/tree/main/mnist\" target=\"_blank\" rel=\"noopener noreferrer\">official MNIST example</a>. The training code has been modified to be heavy on data preprocessing. We train this model in both homogeneous and heterogeneous cluster modes, and compare price performance.</p> \n<p>In this example, we assumed the workload can’t benefit from multiple GPUs, and has dependency on a specific GPU architecture (NVIDIA <a href=\"https://www.nvidia.com/en-gb/data-center/tesla-v100/\" target=\"_blank\" rel=\"noopener noreferrer\">V100</a>). We ran both homogeneous and heterogeneous training jobs, and found key price performance benefits, as shown in the following table. The heterogeneous job (1.19s/step) is 6.5 times faster to train than the homogeneous job (0.18s/step), making it 77% cheaper to train a model.</p> \n<table border=\"1px\"> \n <tbody> \n  <tr style=\"background-color: #000000\"> \n   <td><span style=\"color: #ffffff\"><strong>Example 2 (PT)</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>ml.p3.2xl</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>ml.c5.9xl</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Price per Hour*</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Average Step Time</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Cost per Step</strong></span></td> \n   <td><span style=\"color: #ffffff\"><strong>Price Performance Improvement</strong></span></td> \n  </tr> \n  <tr> \n   <td>Homogeneous</td> \n   <td>1</td> \n   <td>0</td> \n   <td>$3.825</td> \n   <td>1193 ms</td> \n   <td>$0.127</td> \n   <td><span style=\"color: #ffffff\">.</span></td> \n  </tr> \n  <tr> \n   <td>Heterogeneous</td> \n   <td>1</td> \n   <td>1</td> \n   <td>$5.661</td> \n   <td>184 ms</td> \n   <td>$0.029</td> \n   <td>77%</td> \n  </tr> \n </tbody> \n</table> \n<p>* Price per hour is based on us-east-1 <a href=\"https://aws.amazon.com/sagemaker/pricing/\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker on-demand pricing</a></p> \n<p>This is possible because with a higher CPU count, we could use 32 data loader workers (compared to 8 with ml.p3.2xlarge) to preprocess the data and kept GPU close to 100% utilized at frequent intervals. See the <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/training/heterogeneous-clusters/pt.grpc.sagemaker/hetero-pytorch-mnist.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">notebook</a> for more details and graphs.</p> \n<h2>Heterogeneous clusters at Mobileye</h2> \n<p>Mobileye, an Intel company, develops Advanced Driver Assistance Systems (ADAS) and autonomous vehicle technologies with the goal of revolutionizing the transportation industry, making roads safer, and saving lives. These technologies are enabled using sophisticated computer vision (CV) models that are trained using SageMaker on large amounts of data stored in <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Simple Storage Service</a> (Amazon S3). These models use state-of-the-art deep learning neural network techniques.</p> \n<p>We noticed that for one of our CV models, the CPU bottleneck was primarily caused by heavy data preprocessing leading to underutilized GPUs. For this specific workload, we started looking at alternative solutions, evaluated distributed data pipeline technologies with heterogeneous clusters based on EC2 instances, and came up with reference implementations for both <a href=\"https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a> and <a href=\"https://towardsdatascience.com/overcoming-ml-data-preprocessing-bottlenecks-with-grpc-ca30fdc01bee\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>. The release of the SageMaker heterogeneous cluster allows us to run this and similar workloads on SageMaker to achieve improved price performance benefits.</p> \n<h2>Considerations</h2> \n<p>With the launch of the heterogeneous cluster feature, SageMaker offers a lot more flexibility in mixing and matching instance types within your training job. However, consider the following when using this feature:</p> \n<ul> \n <li>The heterogeneous cluster feature is available through SageMaker <a href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> and <a href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a> framework estimator classes. Supported frameworks are PyTorch v1.10 or later and TensorFlow v2.6 or later.</li> \n <li>All instance groups share the same Docker image.</li> \n <li>All instance groups share the same training script. Therefore, your training script should be modified to detect which instance group it belongs to and fork runs accordingly.</li> \n <li>The training instances hostnames (for example, alog-1, algo-2, and so on) are randomly assigned, and don’t indicate which instance group they belong to. To get the instance’s role, we recommend getting its instance group membership during runtime. This is also relevant when reviewing logs in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/logging-cloudwatch.html\" target=\"_blank\" rel=\"noopener noreferrer\">CloudWatch</a>, because the log stream name <code>[training-job-name]/algo-[instance-number-in-cluster]-[epoch_timestamp]</code> has the hostname.</li> \n <li>A distributed training strategy (usually an MPI cluster) can be applied only to one instance group.</li> \n <li>SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html\" target=\"_blank\" rel=\"noopener noreferrer\">Managed Warm Pools</a> and SageMaker <a href=\"https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/\" target=\"_blank\" rel=\"noopener noreferrer\">Local Mode</a> cannot currently be used with heterogeneous cluster training.</li> \n</ul> \n<h2>Conclusion</h2> \n<p>In this post, we discussed when and how to use the heterogeneous cluster feature of SageMaker training. We demonstrated a 46% price performance improvement on a real-world use case and helped you get started quickly with distributed data loader (<code>tf.data.service</code> and gRPC client-server) implementations. You can use these implementations with minimal code changes in your existing training scripts.</p> \n<p>To get started, try out our <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/training/heterogeneous-clusters\" target=\"_blank\" rel=\"noopener noreferrer\">example notebooks</a>. To learn more about this feature, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/train-heterogeneous-cluster.html\" target=\"_blank\" rel=\"noopener noreferrer\">Train Using a Heterogeneous Cluster</a>.</p> \n<hr> \n<h3>About the authors</h3> \n<p style=\"clear: both\"><strong> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/02/15/Gili-Nachum.png\"><img loading=\"lazy\" class=\"size-full wp-image-33120 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/02/15/Gili-Nachum.png\" alt=\"\" width=\"100\" height=\"133\"></a>Gili Nachum</strong> is a senior AI/ML Specialist Solutions Architect who works as part of the EMEA Amazon Machine Learning team. Gili is passionate about the challenges of training deep learning models, and how machine learning is changing the world as we know it. In his spare time, Gili enjoy playing table tennis.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/Hrushi-HeadShot-2019.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-44806 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/Hrushi-HeadShot-2019.jpg\" alt=\"\" width=\"100\" height=\"129\"></a>Hrushikesh Gangur</strong> is a principal solutions architect for AI/ML startups with expertise in both ML Training and AWS Networking. He helps startups in Autonomous Vehicle, Robotics, CV, NLP, MLOps, ML Platform, and Robotics Process Automation technologies to run their business efficiently and effectively on AWS. Prior to joining AWS, Hrushikesh acquired 20+ years of industry experience primarily around Cloud and Data platforms.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/gal-oshri.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-44787 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/gal-oshri.jpg\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Gal Oshri </strong>is a Senior Product Manager on the Amazon SageMaker team. He has 7 years of experience working on Machine Learning tools, frameworks, and services.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/Chaim-rand-headshot.jpeg\"><img loading=\"lazy\" class=\"wp-image-44807 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/Chaim-rand-headshot.jpeg\" alt=\"\" width=\"100\" height=\"100\"></a>Chaim Rand</strong> is a machine learning algorithm developer working on deep learning and computer vision technologies for Autonomous Vehicle solutions at Mobileye, an Intel Company. Check out his <a href=\"https://chaimrand.medium.com/\" target=\"_blank\" rel=\"noopener noreferrer\">blogs</a>.</p>"
}