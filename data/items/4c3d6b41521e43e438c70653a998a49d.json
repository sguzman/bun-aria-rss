{
  "title": "Multiclass Classification using Clatern",
  "link": "",
  "published": "2015-02-26T00:00:00+00:00",
  "updated": "2015-02-26T00:00:00+00:00",
  "id": "/clatern/2015/02/26/multiclass-classification-using-clatern",
  "content": "<p><a href=\"https://github.com/rinuboney/clatern\">Clatern</a> is a machine learning library for Clojure, in the works. This is a short tutorial on performing multiclass classification using Clatern.</p>\n\n<h4 id=\"importing-the-libraries\">Importing the libraries</h4>\n\n<p>The libraries required for the tutorial are core.matrix, Incanter and Clatern. Importing them,</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"nf\">require</span><span class=\"w\"> </span><span class=\"o\">'</span><span class=\"p\">[</span><span class=\"n\">clojure.core.matrix</span><span class=\"w\"> </span><span class=\"no\">:as</span><span class=\"w\"> </span><span class=\"n\">m</span><span class=\"p\">])</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"nf\">use</span><span class=\"w\"> </span><span class=\"o\">'</span><span class=\"p\">(</span><span class=\"nf\">incanter</span><span class=\"w\"> </span><span class=\"n\">core</span><span class=\"w\"> </span><span class=\"n\">datasets</span><span class=\"w\"> </span><span class=\"n\">charts</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"nf\">use</span><span class=\"w\"> </span><span class=\"o\">'</span><span class=\"p\">(</span><span class=\"nf\">clatern</span><span class=\"w\"> </span><span class=\"n\">logistic-regression</span><span class=\"w\"> </span><span class=\"n\">knn</span><span class=\"p\">))</span></code></pre></figure>\n\n<p><em>NOTE: This tutorial requires Incanter 2.0 (aka Incanter 1.9.0). This is because both Incanter 2.0 and Clatern are integrated with core.matrix.</em></p>\n\n<h3 id=\"dataset\">Dataset</h3>\n<p>This tutorial uses the popular Iris flower dataset. The dataset is available here: https://archive.ics.uci.edu/ml/datasets/Iris. For this tutorial, we’ll use Incanter to load the dataset.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">get-dataset</span><span class=\"w\"> </span><span class=\"no\">:iris</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"nf\">view</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"p\">)</span></code></pre></figure>\n\n<p><img src=\"https://camo.githubusercontent.com/6e7e613199cfb729b52792639c7b24ace67585e8/687474703a2f2f696e63616e7465722e6f72672f696d616765732f6578616d706c65732f697269735f646174612e6a7067\" alt=\"iris\" /></p>\n\n<p>Now converting the dataset into a matrix, where non-numeric columns are converted to either numeric codes or dummy variables, using the to-matrix function.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">iris-mat</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">to-matrix</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"nf\">view</span><span class=\"w\"> </span><span class=\"n\">iris-mat</span><span class=\"p\">)</span></code></pre></figure>\n\n<p><img src=\"https://camo.githubusercontent.com/1fa4972cc40ded5570931f7f567d1c595f010a47/687474703a2f2f696e63616e7465722e6f72672f696d616765732f6578616d706c65732f697269735f6d61742e6a7067\" alt=\"iris-mat\" /></p>\n\n<p>Now let’s split the dataset into a training set and a test set,</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"o\">'</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/order</span><span class=\"w\"> </span><span class=\"n\">iris-mat</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">shuffle</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"w\"> </span><span class=\"mi\">150</span><span class=\"p\">))))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">train-mat</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">take</span><span class=\"w\"> </span><span class=\"mi\">120</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"o\">'</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">test-mat</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">drop</span><span class=\"w\"> </span><span class=\"mi\">120</span><span class=\"w\"> </span><span class=\"n\">iris</span><span class=\"o\">'</span><span class=\"p\">))</span></code></pre></figure>\n\n<p>Splitting the training and test set into features and labels,</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">X-train</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/select</span><span class=\"w\"> </span><span class=\"n\">train-mat</span><span class=\"w\"> </span><span class=\"no\">:all</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\">]))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">y-train</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/get-column</span><span class=\"w\"> </span><span class=\"n\">train-mat</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">X-test</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/select</span><span class=\"w\"> </span><span class=\"n\">test-mat</span><span class=\"w\"> </span><span class=\"no\">:all</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"mi\">2</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\">]))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">y-test</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/get-column</span><span class=\"w\"> </span><span class=\"n\">test-mat</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"p\">))</span></code></pre></figure>\n\n<h3 id=\"logistic-regression\">Logistic Regression</h3>\n\n<p>Here comes the interesting part - training a classifier using the data. First, let’s try the logistic regression model. Gradient descent is a learning algorithm for the logistic regression model. The syntax of gradient descent is,</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"nf\">gradient-descent</span><span class=\"w\"> </span><span class=\"n\">X</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"no\">:alpha</span><span class=\"w\"> </span><span class=\"n\">alpha</span><span class=\"w\"> </span><span class=\"no\">:lambda</span><span class=\"w\"> </span><span class=\"n\">lambda</span><span class=\"w\"> </span><span class=\"no\">:num-iters</span><span class=\"w\"> </span><span class=\"n\">num-iters</span><span class=\"p\">)</span></code></pre></figure>\n\n<p>where,<br />\n<em>X</em> is input data,<br />\n<em>y</em> is target data,<br />\n<em>alpha</em> is the learning rate,<br />\n<em>lambda</em> is the regularization parameter, and<br />\n<em>num-iters</em> is the number of iterations.</p>\n\n<p>alpha(default = 0.1), lambda(default = 1) and num-iters(default = 100) are optional.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">lr-h</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">gradient-descent</span><span class=\"w\"> </span><span class=\"n\">X-train</span><span class=\"w\"> </span><span class=\"n\">y-train</span><span class=\"w\"> </span><span class=\"no\">:lambda</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"n\">e-4</span><span class=\"w\"> </span><span class=\"no\">:num-iters</span><span class=\"w\"> </span><span class=\"mi\">200</span><span class=\"p\">))</span></code></pre></figure>\n\n<p>That’s it. Here, gradient-descent is a function in the clojure.logistic-regression namespace. It trains on the provided data and returns a hypothesis in the logistic regression model. Now, <strong>lr-h</strong> is a function that can classify an input vector.</p>\n\n<h3 id=\"k-nearest-neighbors\">K Nearest Neighbors</h3>\n\n<p>Next, let’s try the k nearest neighbors model. There is actually no training phase for this model. It can be directly used. The syntax for knn is,</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"nf\">knn</span><span class=\"w\"> </span><span class=\"n\">X</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"n\">v</span><span class=\"w\"> </span><span class=\"no\">:k</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"p\">)</span></code></pre></figure>\n\n<p>where,<br />\n<em>X</em> is input data,<br />\n<em>y</em> is target data,<br />\n<em>v</em> is new input to be classified, and<br />\n<em>k</em> is the number of neighbours(optional, default = 3)</p>\n\n<p>Let’s define a function to perform kNN on our dataset.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">knn-h</span><span class=\"w\"> </span><span class=\"o\">#</span><span class=\"p\">(</span><span class=\"nf\">knn</span><span class=\"w\"> </span><span class=\"n\">X-train</span><span class=\"w\"> </span><span class=\"n\">y-train</span><span class=\"w\"> </span><span class=\"n\">%</span><span class=\"p\">))</span></code></pre></figure>\n\n<p>Similar to the logistic regression hypothesis, now <strong>knn-h</strong> can be used to classify an input vector.</p>\n\n<h3 id=\"classification\">Classification</h3>\n\n<p>Both <strong>lr-h</strong> and <strong>knn-h</strong> are functions that take input feature vectors and classify them. So to classify a whole dataset, the function is mapped to all rows of the dataset.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">lr-preds</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"w\"> </span><span class=\"n\">lr-h</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/rows</span><span class=\"w\"> </span><span class=\"n\">X-test</span><span class=\"p\">)))</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"n\">knn-preds</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"w\"> </span><span class=\"n\">lr-h</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/rows</span><span class=\"w\"> </span><span class=\"n\">X-test</span><span class=\"p\">)))</span></code></pre></figure>\n\n<p>Now <strong>lr-preds</strong> and <strong>knn-preds</strong> contains the classifications made by logistic regression and knn on the orignal dataset, respectively.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>So which model performs better here? Let’s write a function to assess the classification accuracy</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"k\">defn</span><span class=\"w\"> </span><span class=\"n\">accuracy</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"n\">X</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"p\">]</span><span class=\"w\">\n  </span><span class=\"p\">(</span><span class=\"nb\">*</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">/</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">apply</span><span class=\"w\"> </span><span class=\"nb\">+</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"w\"> </span><span class=\"o\">#</span><span class=\"p\">(</span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">=</span><span class=\"w\"> </span><span class=\"n\">%1</span><span class=\"w\"> </span><span class=\"n\">%2</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"w\"> </span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">m/rows</span><span class=\"w\"> </span><span class=\"n\">X</span><span class=\"p\">))))</span><span class=\"w\"> \n        </span><span class=\"p\">(</span><span class=\"nf\">m/row-count</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"p\">))</span><span class=\"w\">\n     </span><span class=\"mf\">100.0</span><span class=\"p\">))</span></code></pre></figure>\n\n<p>Now let’s evaluate both the classifiers:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-clojure\" data-lang=\"clojure\"><span class=\"p\">(</span><span class=\"nf\">accuracy</span><span class=\"w\"> </span><span class=\"n\">lr-h</span><span class=\"w\"> </span><span class=\"n\">X-test</span><span class=\"w\"> </span><span class=\"n\">y-test</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\">; 92.53</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"nf\">accuracy</span><span class=\"w\"> </span><span class=\"n\">knn-h</span><span class=\"w\"> </span><span class=\"n\">X-test</span><span class=\"w\"> </span><span class=\"n\">y-test</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\">; 96.13</span></code></pre></figure>\n\n<p>The accuracy of the models could vary highly depending on the shuffling of the dataset. These are values I averaged over 100 runs. Both models perform well on this datatset. So, that’s it for multiclass classification using Clatern. More work on Clatern to follow soon. So, keep an eye out :-)</p>",
  "author": {
    "name": ""
  },
  "category": "",
  "summary": "Clatern is a machine learning library for Clojure, in the works. This is a short tutorial on performing multiclass classification using Clatern. Importing the libraries"
}