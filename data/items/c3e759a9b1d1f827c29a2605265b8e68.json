{
  "title": "Apache Airflow Part 2 &#8212; Connections, Hooks, reading and writing to Postgres, and XComs",
  "link": "https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/",
  "comments": "https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/#comments",
  "dc:creator": "Jack Schultz",
  "pubDate": "Mon, 20 Apr 2020 14:15:34 +0000",
  "category": [
    "Airflow",
    "Data",
    "General",
    "How To",
    "Python",
    "Apache Airflow",
    "dag",
    "database",
    "ETL",
    "postgres",
    "sqlalchemy",
    "xcom"
  ],
  "guid": "http://bigishdata.com/?p=1685",
  "description": "In part 1, we went through have have basic DAGs that read, logged, and write to custom files, and got an overall sense of file location and places in Airflow. A lot of the work was getting Airflow running locally, &#8230; <a href=\"https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>In <a href=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/\">part 1</a>, we went through have have basic DAGs that read, logged, and write to custom files, and got an overall sense of file location and places in Airflow. A lot of the work was getting Airflow running locally, and then at the end of the post, a quick start in having it do work.</p>\n<p>In part 2 here, we&#8217;re going to look through and start some read and writes to a database, and show how tasks can run together in a directed, acyclical manner. Even though they&#8217;ll both be with a single database, you can think of stretching them out in other situations.</p>\n<p>Again, this post will assume you&#8217;re going through and writing this code on your own with some copy paste. If you&#8217;re on the path of using what I have written, checkout the github repo <a href=\"https://github.com/jackschultz/bigishdata-airflow\">here</a>.</p>\n<h3>Creating database</h3>\n<p>This post is going to go through and write to postgres. We already created a database for Airflow itself to use, but we want to leave that alone.</p>\n<p>So before we get to any of the python code, go and create the new database, add a new user with password, and then create the <code>dts</code> (short name for datetimes, since that&#8217;s all we&#8217;re doing here) table.</p>\n<pre>bigishdata=> create table dts (id serial primary key, run_time timestamp, execution_time timestamp);\nCREATE TABLE\nbigishdata=> \\dt\n            List of relations\n Schema | Name  | Type  |     Owner\n--------+-------+-------+----------------\n public | dts   | table | bigishdatauser\n(1 rows)\n\nbigishdata=# \\d dts\n                                          Table \"public.dts\"\n     Column     |            Type             | Collation | Nullable |             Default\n----------------+-----------------------------+-----------+----------+---------------------------------\n id             | integer                     |           | not null | nextval('dts_id_seq'::regclass)\n run_time       | timestamp without time zone |           |          |\n execution_time | timestamp without time zone |           |          |\nIndexes:\n    \"dts_pkey\" PRIMARY KEY, btree (id)</pre>\n<h3>Adding the Connection</h3>\n<p>Connections is well named term you&#8217;ll see all over in Airflow speak. They&#8217;re defined as &#8220;<a href=\"https://airflow.apache.org/docs/stable/concepts.html?highlight=connection#connections\">[t]he <span class=\"highlighted\">connection</span> information to external systems</a>&#8221; which could mean usernames, passwords, ports, etc. for when you want to connect to things like databases, AWS, Google Cloud, various data lakes or warehouses. Anything that requires information to connect to, you&#8217;ll be able to put that information in a Connection.</p>\n<p>With <code>airflow webserver</code> running, go to the UI, find the Admin dropdown on the top navbar, and click Connections. Like example DAGs, you&#8217;ll see many default Connections, which are really great to see what information is needed for those connections, and also to see what connections are available and what platforms you can move data to and from.</p>\n<p>Take the values for the database we&#8217;re using here &#8212; the (local)host, schema (meaning database name), login (username), password, port &#8212; and put that into the form shown below. At the top, you&#8217;ll see Conn Id, and in that input create a name for the connection. This name is clearly important, and you&#8217;ll see that we use that in order to say which Connection we want.</p>\n<p><img loading=\"lazy\" data-attachment-id=\"1691\" data-permalink=\"https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/screen-shot-2020-03-29-at-4-40-22-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png\" data-orig-size=\"1626,1352\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 4.40.22 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=584\" class=\"alignnone wp-image-1691\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=391&#038;h=325\" alt=\"Screen Shot 2020-03-29 at 4.40.22 PM.png\" width=\"391\" height=\"325\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=391&h=325 391w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=782&h=650 782w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=150&h=125 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=300&h=249 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-4.40.22-pm.png?w=768&h=639 768w\" sizes=\"(max-width: 391px) 100vw, 391px\" /></p>\n<p>When you save this, you can go to the Airflow database, find the <code>connection</code> table, and you can see the see the values you inputted in that form. You&#8217;ll also probably see that your password is there in plain text. For this post, I&#8217;m not going to talk about encrypting it, but you&#8217;re <a href=\"https://airflow.apache.org/docs/stable/howto/secure-connections.html\">able to do that</a>, and should, of course.</p>\n<p>One more thing to look at is in the source code, the <a href=\"https://github.com/apache/airflow/blob/master/airflow/models/connection.py\">Connection model</a>, <a href=\"https://github.com/apache/airflow/blob/master/airflow/www/forms.py#L101\">form</a>, and <a href=\"https://github.com/apache/airflow/blob/master/airflow/www/views.py#L2236\">view</a>. It&#8217;s a flask app! And great to see the source code to get a much better understanding for something like adding information for a connection.</p>\n<h3>Hooks</h3>\n<p>In order to use the information in a Connection, we use what is called a Hook. A Hook takes the information in the Connection, and hooks you up with the service that you created the Connection with. Another nicely named term.</p>\n<p><span id=\"more-1685\"></span></p>\n<p>The postgres hook we&#8217;ll be using is defined as follow.</p>\n<pre>from airflow.hooks.postgres_hook import PostgresHook\npg_hook = PostgresHook(postgres_conn_id='postgres_bigishdata')</pre>\n<p>You can take a look at the <a href=\"https://github.com/apache/airflow/blob/master/airflow/providers/postgres/hooks/postgres.py\">source code</a> for the Hook and you&#8217;ll see that the PostgresHook inherits from <a href=\"https://github.com/apache/airflow/blob/master/airflow/hooks/dbapi_hook.py\">DbApiHook</a>, which has functions such as <code>run</code>, <code>insert_rows</code>, and <code>get_cursor</code>, that all come from SqlAlchemy functionality. Another example of how seeing the code there makes me so much more comfortable using the hook and trusting that you know what the code is doing and how to use it.</p>\n<p>The readability and straightforwardness of the code is one of the best things I&#8217;ve found with Airflow.</p>\n<h3>DAG</h3>\n<p>With the Connection and Hook taken care of, we&#8217;ll write the DAG tasks that instead of writing to the file like the first time, will write these datetimes to the database. For me, I copied the <code>writing_to_file.py</code> to <code><a href=\"https://github.com/jackschultz/bigishdata-airflow/blob/master/dags/write_to_pg.py\">writing_to_pg.py</a></code> and changed the <code>python_callable</code> to</p>\n<pre>from airflow.hooks.postgres_hook import PostgresHook\npg_hook = PostgresHook(postgres_conn_id='postgres_bigishdata') \n....\n\ndef write_to_pg(**kwargs):\n    execution_time = kwargs['ts']\n    run_time = dt.datetime.utcnow()\n    print('Writing to pg', runtime, execution_time)\n    dts_insert = 'insert into dts (runtime, execution_time) values (%s, %s)'\n    pg_hook.run(dts_insert, parameters=(runtime, execution_time,))\n\n....\n\ndag = DAG('writing_to_pg',  # note the difference of the DAG name. This is important to keep separate for now, and also in the future with versions.\n          default_args=default_args,\n          start_date=dt.datetime.now(),\n          schedule_interval=dt.timedelta(seconds=10)\n          )\n....</pre>\n<p>In the function, We get the times, print them to go to the log, write the SQL insert statement, and call run with the statement and the tupled parameters.</p>\n<p>Restart the scheduler, which will then pickup the new DAG and put it in the DAG table of the Airflow database. Now if you go back to the main DAG page in the Airflow UI, you should see <code>writing_to_pg</code> show up. Go ahead and turn on the task, and go to psql and <code>select * from dts</code> and keep watching that as the tasks run. You&#8217;ll start to see them show up every 10 seconds. After a minute, your query will show something like this, again, with the execution times exactly 10 seconds apart, runtime slightly different (and yes, more than 13 seconds apart. Keep that in mind still).</p>\n<p><img loading=\"lazy\" data-attachment-id=\"1696\" data-permalink=\"https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/screen-shot-2020-03-30-at-6-28-48-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png\" data-orig-size=\"906,362\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-30 at 6.28.48 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=584\" class=\"alignnone wp-image-1696\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=493&#038;h=197\" alt=\"Screen Shot 2020-03-30 at 6.28.48 PM.png\" width=\"493\" height=\"197\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=493&h=197 493w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=150&h=60 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=300&h=120 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png?w=768&h=307 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-6.28.48-pm.png 906w\" sizes=\"(max-width: 493px) 100vw, 493px\" /></p>\n<p>That last paragraph was pretty in depth on how to get the DAG to run, so don&#8217;t feel bad if it didn&#8217;t start running correctly right away, but you&#8217;ll get there.</p>\n<h3>Downstream DAG</h3>\n<p>We&#8217;re finally to the point where we can have a downstream operator instead of the single task. Look back to what was done in <a href=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/\">part 1</a> and so far in part 2. All this work and setup to write an incredibly simple function. Now you can see why people don&#8217;t always feel the need to run Airflow. That said, keep going.</p>\n<p>The task we&#8217;ll create will, after the upstream <code>writing_to_pg</code> runs, take that new row, format the <code>run_time</code>, and update the row with the <code>formatted_run_time</code>. We&#8217;re still going simple.</p>\n<p>First is to add the <code>formatted_run_time column</code> to the <code>dts</code> table.</p>\n<pre>bigishdata=> alter table dts add column formatted_run_time varchar;\nALTER TABLE\nbigishdata=> \\d dts\n                                            Table \"public.dts\"\n       Column       |            Type             | Collation | Nullable |             Default\n--------------------+-----------------------------+-----------+----------+---------------------------------\n id                 | integer                     |           | not null | nextval('dts_id_seq'::regclass)\n run_time           | timestamp without time zone |           |          |\n execution_time     | timestamp without time zone |           |          |\n formatted_run_time | character varying           |           |          |\nIndexes:\n    \"dts_pkey\" PRIMARY KEY, btree (id)</pre>\n<p>In this case, we&#8217;re going to add SqlAlchemy object to represent the <code>dts</code>, instead of using sql string statements. This will give us back an object with a query, and make it a little clearer.</p>\n<p>In <code><a href=\"https://github.com/jackschultz/bigishdata-airflow/blob/master/dags/write_to_pg.py\">writing_to_pg_dag.py</a></code>, add the following.</p>\n<pre>....\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.orm import sessionmaker\n\n....\nBase = declarative_base()\n\nclass Dts(Base):\n    __tablename__ = 'dts'\n\n    id = Column(Integer, primary_key=True)\n    run_time = Column(DateTime)\n    execution_time = Column(DateTime)\n    formatted_runtime = Column(String)\n\n    def __repr__(self):\n        return f\"<Dts(id={self.id}, runtime={self.runtime})>\"\n....</pre>\n<p>Now we&#8217;re going to create the <code>python_callable</code>. In this function, we&#8217;re going to query for the most recent <code>dts</code> row that doesn&#8217;t have a formatted <code>run_time</code>, do the formatting, and then rewrite to the database. So below the SqlAlchemy object you created, add the following. What we&#8217;re going to do is <code>get_sqlalchemy_engine</code> through the PostgresHook, (which is a subclass of the <a href=\"https://airflow.readthedocs.io/en/stable/_modules/airflow/hooks/dbapi_hook.html\">DbApiHook</a>,) get a session from that, and do the query and update.</p>\n<pre># These variables can go anywhere, but we're making them globals in the file for now.\n\nfrom sqlalchemy.orm import sessionmaker\n\n...\n\npg_hook = PostgresHook(postgres_conn_id='postgres_bigishdata')\nengine = pg_hook.get_sqlalchemy_engine()\nSession = sessionmaker(bind=engine)\nsession = Session() #session for querying\n\n...\n\ndef format_dt(ds, **kwargs):\n    recent_dt = session.query(Dts).filter(Dts.runtime.isnot(None)).order_by(Dts.runtime.desc()).first()\n    print(recent_dt) # put in the logs\n    formatted_runtime = recent_dt.runtime.strftime('%m/%d/%Y %H:%M:%S')\n    recent_dt.formatted_runtime = formatted_runtime\n    session.add(recent_dt)\n    session.commit()</pre>\n<p>What we have here is a function that uses a SqlAlchemy session to query for an object, format the time, add, and commit.</p>\n<p>The last step of this formatting is to add the operator to the DAG.</p>\n<pre>...\n\nformat_run_time_pg_operator = PythonOperator(task_id='format_runtime_pg',\n                                             python_callable=format_run_time_pg,\n                                             provide_context=True,\n                                             dag=dag)\n\nwrite_to_pg_operator >> format_run_time_pg_operator</pre>\n<p>With the scheduler running, go back to the UI and turn on the <code>writing_to_pg</code> DAG again. What will happen is the datetimes will be set as they were before but when we query, right away we&#8217;ll see the formatted date time (and also when I was writing this after work).</p>\n<p><img data-attachment-id=\"1704\" data-permalink=\"https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/screen-shot-2020-03-30-at-7-01-00-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png\" data-orig-size=\"1198,502\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-30 at 7.01.00 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=584\" class=\"alignnone size-full wp-image-1704\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=584\" alt=\"Screen Shot 2020-03-30 at 7.01.00 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.01.00-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<h3>XCom</h3>\n<p>I&#8217;m guessing you noticed this, but the query above is asking for the most recent value that was stored in the databases, which is pretty dumb and not useful.</p>\n<p>Imagine it&#8217;s a table for runtime logs of many different services, and you want to put that log, and then after, do something else with the log. We can have one task for inserting the value, but the next task will want to know exactly which value to continue using. This is where <a href=\"https://airflow.apache.org/docs/stable/concepts.html?highlight=xcom#concepts-xcom\">XComs</a> come in to play.</p>\n<p>XComs &#8220;<a href=\"https://airflow.apache.org/docs/stable/concepts.html?highlight=xcom#concepts-xcom\">let tasks exchange messages, allowing more nuanced forms of control and shared state</a>&#8220;. But when <a href=\"https://airflow.apache.org/docs/1.10.1/concepts.html?highlight=xcom#operators\">reading more about these,</a> you&#8217;ll see that Airflow doesn&#8217;t really like XComs being used.</p>\n<blockquote><p>This is a subtle but very important point: in general, if two operators need to share information, like a filename or small amount of data, you should consider combining them into a single operator. If it absolutely can’t be avoided, Airflow does have a feature for operator cross-communication called <span class=\"highlighted\">XCom</span> that is described elsewhere in this document.</p></blockquote>\n<p>I&#8217;ll tend to disagree with this however. For example, with web scraping, I want to get the file and put it in some directory, local or s3. Then I want to go through the info in that html. I don&#8217;t want to have that all in one Operator because then, what&#8217;s the point? If each task is thought about as moving data from one place to another, then sure, we can say we want it from a webservice to a database, but there are step in between, and it should be considered more than ok to pass values between tasks.</p>\n<p>Anyway, back to the code. Let&#8217;s copy the <code>write_to_pg.py</code> file, rename it as <code><a href=\"https://github.com/jackschultz/bigishdata-airflow/blob/master/dags/write_to_pg_xcom.py\">write_to_pg_xcom.py</a></code>, and change the code around with two new <code>PythonOperator</code>s. First will add the times, commit them, and then use <code>xcom_push</code> to pass the new id of that row in the database. The second Operator will <code>xcom_pull</code> that id, query for that row, format the time, and update.</p>\n<pre>def write_to_pg_xcom(**kwargs):\n    run_time = dt.datetime.utcnow()\n    execution_time = kwargs['ts']\n    print('Writing to pg', run_time, execution_time)\n    new_dt = Dts(run_time=run_time, execution_time=execution_time)\n    session.add(new_dt)\n    session.commit()  # Flushing means commit and refresh, so new_dt has the assigned id\n    print(new_dt)\n    kwargs['ti'].xcom_push(key='inserted_id', value=new_dt.id)\n\n\ndef format_run_time_pg_xcom(**kwargs):\n    inserted_id = kwargs['ti'].xcom_pull(task_ids='write_to_pg_xcom', key='inserted_id')\n    print(inserted_id)\n    recent_dt = session.query(Dts).get(inserted_id)\n    print(recent_dt)  # put in the logs\n    formatted_run_time = recent_dt.run_time.strftime('%m/%d/%Y %H:%M:%S')\n    recent_dt.formatted_run_time = formatted_run_time\n    session.commit()\n\n....\n\nwrite_to_pg_xcom_operator = PythonOperator(task_id='write_to_pg_xcom',\n                                           python_callable=write_to_pg_xcom,\n                                           provide_context=True,\n                                           dag=dag)\nformat_run_time_pg_xcom_operator = PythonOperator(task_id='format_run_time_pg_xcom',\n                                                  python_callable=format_run_time_pg_xcom,\n                                                  provide_context=True,\n                                                  dag=dag)\n\nwrite_to_pg_xcom_operator >> format_run_time_pg_xcom_operator</pre>\n<p>Go back to the UI, click the switch to start the run, wait for a little for the tasks to start running, then go and query the DB, and reload the tree page in the UI and you&#8217;ll see the tasks running and completing.</p>\n<p>As a final reference to XCom intro, if you go back to the Airflow database, find the <code>xcom</code> table, you can see that the info is being stored.</p>\n<p><img data-attachment-id=\"1709\" data-permalink=\"https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/screen-shot-2020-03-30-at-7-41-15-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png\" data-orig-size=\"2036,890\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-30 at 7.41.15 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=584\" class=\"alignnone size-full wp-image-1709\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=584\" alt=\"Screen Shot 2020-03-30 at 7.41.15 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-30-at-7.41.15-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>For this task push, <code>value</code> column is where the database <code>id</code> is being stored. It&#8217;s a bytea column of the <a href=\"https://docs.python.org/3/library/pickle.html\">pickled</a> value, you pushed, and converting to bytes. Keep this in mind about sending large amounts of information between tasks. You can do tons of things with the data, so almost always better to put the data somewhere and send id or filename to the next task.</p>\n<h3>Summary</h3>\n<p>At the start, we had a DAG with a single task that wrote the run and execution times to a file. Continuing to the end of this post, we created a DAG with two tasks, one writes the times to Postgres, and the other reads those times and writes them back.</p>\n<p>Next, in post 3, we&#8217;re going to make a bigger jump. I&#8217;m a <a href=\"https://bigishdata.com/2016/09/27/getting-song-lyrics-from-geniuss-api-scraping/\">big</a> <a href=\"https://bigishdata.com/2017/06/06/web-scraping-with-python-part-two-library-overview-of-requests-urllib2-beautifulsoup-lxml-scrapy-and-more/\">fan</a> of <a href=\"https://bigishdata.com/2018/06/15/us-open-clustering-part-1-gathering-and-understanding-the-data/\">webscraping</a>, so we&#8217;re going to scrape fake blog posts (as if they had deeper meaning than what I write), save the html in s3, then get the post information like titles and posted at dates, structure it, and put it in postgres.</p>\n",
  "wfw:commentRss": "https://bigishdata.com/2020/04/20/apache-airflow-part-2-connections-hooks-reading-and-writing-to-postgres-and-xcoms/feed/",
  "slash:comments": 1,
  "media:content": [
    {
      "media:title": "jackschultz23"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 4.40.22 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-30 at 6.28.48 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-30 at 7.01.00 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-30 at 7.41.15 PM.png"
    }
  ]
}