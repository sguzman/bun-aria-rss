{
  "title": "Towards Out-of-core ND-Arrays -- Benchmark MatMul",
  "link": "",
  "updated": "2015-01-14T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/01/14/Towards-OOC-MatMul",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr</strong> We benchmark dask on an out-of-core dot product.  We also compare and\nmotivate the use of an optimized BLAS.</p>\n\n<p><strong>Here are the results</strong></p>\n\n<table>\n  <thead>\n  <tr>\n    <th>Performance (GFLOPS)</th>\n    <th>In-Memory</th>\n    <th>On-Disk</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Reference BLAS</th>\n      <td>6</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>OpenBLAS one thread</th>\n      <td>11</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>OpenBLAS four threads</th>\n      <td>22</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>Disclaimer: This post is on experimental buggy code.  This is not ready for public use.</em></p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>This is the fourth in a sequence of posts constructing an out-of-core nd-array\nusing NumPy, Blaze, and dask.  You can view these posts here:</p>\n\n<ol>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2014/12/27/Towards-OOC/\">Simple task scheduling</a>,</li>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2014/12/30/Towards-OOC-Frontend/\">Frontend usability</a></li>\n  <li><a href=\"http://matthewrocklin.com/blog/work/2015/01/06/Towards-OOC-Scheduling/\">A multi-threaded scheduler</a></li>\n</ol>\n\n<p>We now give performance numbers on out-of-core matrix-matrix multiplication.</p>\n\n<h2 id=\"matrix-matrix-multiplication\">Matrix-Matrix Multiplication</h2>\n\n<p>Dense matrix-matrix multiplication is compute-bound, not I/O bound.\nWe spend most of our time doing arithmetic and relatively little time shuffling\ndata around.  As a result we may be able to read <em>large</em> data from disk without\nperformance loss.</p>\n\n<p>When multiplying two $n\\times n$ matrices we read $n^2$ bytes but perform $n^3$\ncomputations.  There are $n$ computations to do per byte so, relatively\nspeaking, I/O is cheap.</p>\n\n<p>We normally measure speed for single CPUs in Giga Floating Point Operations\nPer Second (GFLOPS).  Lets look at how my laptop does on single-threaded\nin-memory matrix-matrix multiplication using NumPy.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">timeit</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>  <span class=\"c1\"># Matrix-matrix multiplication\n</span><span class=\"mi\">10</span> <span class=\"n\">loops</span><span class=\"p\">,</span> <span class=\"n\">best</span> <span class=\"n\">of</span> <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"mi\">176</span> <span class=\"n\">ms</span> <span class=\"n\">per</span> <span class=\"n\">loop</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mi\">1000</span> <span class=\"o\">**</span> <span class=\"mi\">3</span> <span class=\"o\">/</span> <span class=\"mf\">0.176</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>  <span class=\"c1\"># n cubed computations / seconds / billion\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mf\">5.681818181818182</span></code></pre>\n</figure>\n\n<p>OK, so NumPy’s matrix-matrix multiplication clocks in at 6 GFLOPS more or\nless.  The <code class=\"language-plaintext highlighter-rouge\">np.dot</code> function ties in to the <code class=\"language-plaintext highlighter-rouge\">GEMM</code> operation in the <code class=\"language-plaintext highlighter-rouge\">BLAS</code>\nlibrary on my machine.  Currently my <code class=\"language-plaintext highlighter-rouge\">numpy</code> just uses reference BLAS. (you can\ncheck this with <code class=\"language-plaintext highlighter-rouge\">np.show_config()</code>.)</p>\n\n<h2 id=\"matrix-matrix-multiply-from-disk\">Matrix-Matrix Multiply From Disk</h2>\n\n<p>For matrices too large to fit in memory we compute the solution one part at a\ntime, loading blocks from disk when necessary.  We parallelize this with\nmultiple threads.  Our last post demonstrates how NumPy+Blaze+Dask automates\nthis for us.</p>\n\n<p>We perform a simple numerical experiment, using HDF5 as our on-disk store.</p>\n\n<p>We install stuff</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>conda <span class=\"nb\">install</span> <span class=\"nt\">-c</span> blaze blaze\n<span class=\"nv\">$ </span>pip <span class=\"nb\">install </span>git+https://github.com/ContinuumIO/dask</code></pre>\n</figure>\n\n<p>We set up a fake dataset on disk</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">h5py</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">h5py</span><span class=\"p\">.</span><span class=\"n\">File</span><span class=\"p\">(</span><span class=\"s\">'myfile.hdf5'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">create_dataset</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'A'</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">200000</span><span class=\"p\">,</span> <span class=\"mi\">4000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                                <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">250</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">),</span> <span class=\"n\">fillvalue</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">create_dataset</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'B'</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4000</span><span class=\"p\">,</span> <span class=\"mi\">4000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>                                <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">250</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">),</span> <span class=\"n\">fillvalue</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>We tell Dask+Blaze how to interact with that dataset</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">dask.obj</span> <span class=\"kn\">import</span> <span class=\"n\">Array</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">blaze</span> <span class=\"kn\">import</span> <span class=\"n\">Data</span><span class=\"p\">,</span> <span class=\"n\">into</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"s\">'myfile.hdf5::/A'</span><span class=\"p\">,</span> <span class=\"n\">blockshape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">))</span>  <span class=\"c1\"># dask things\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"n\">Array</span><span class=\"p\">,</span> <span class=\"s\">'myfile.hdf5::/B'</span><span class=\"p\">,</span> <span class=\"n\">blockshape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>  <span class=\"c1\"># Blaze things\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>We compute our desired result, storing back onto disk</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">into</span><span class=\"p\">(</span><span class=\"s\">'myfile.hdf5::/result'</span><span class=\"p\">,</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">))</span>\n<span class=\"mi\">2</span><span class=\"nb\">min</span> <span class=\"mi\">49</span><span class=\"n\">s</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mi\">200000</span> <span class=\"o\">*</span> <span class=\"mi\">4000</span> <span class=\"o\">*</span> <span class=\"mi\">4000</span> <span class=\"o\">/</span> <span class=\"mi\">169</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>\n<span class=\"mf\">18.934911242</span></code></pre>\n</figure>\n\n<p>18.9 GFLOPS, roughly 3 times faster than the in-memory solution.  At first\nglance this is confusing - shouldn’t we be slower coming from disk?  Our\nspeedup is due to our use of four cores in parallel.  This is good, we don’t\nexperience much slowdown coming from disk.</p>\n\n<p>It’s as if all of our hard drive just became memory.</p>\n\n<h2 id=\"openblas\">OpenBLAS</h2>\n\n<p>Reference BLAS is slow; it was written long ago.  OpenBLAS is a modern\nimplementation.  I installed OpenBLAS with my system installer (<code class=\"language-plaintext highlighter-rouge\">apt-get</code>) and\nthen reconfigured and rebuilt numpy.  OpenBLAS supports many cores.  We’ll show\ntimings with one and with four threads.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span><span class=\"nb\">export </span><span class=\"nv\">OPENBLAS_NUM_THREADS</span><span class=\"o\">=</span>1\n<span class=\"nv\">$ </span>ipython</code></pre>\n</figure>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'f8'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">timeit</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"mi\">10</span> <span class=\"n\">loops</span><span class=\"p\">,</span> <span class=\"n\">best</span> <span class=\"n\">of</span> <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"mf\">89.8</span> <span class=\"n\">ms</span> <span class=\"n\">per</span> <span class=\"n\">loop</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mi\">1000</span> <span class=\"o\">**</span> <span class=\"mi\">3</span> <span class=\"o\">/</span> <span class=\"mf\">0.0898</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>  <span class=\"c1\"># compute GFLOPS\n</span><span class=\"mf\">11.135857461024498</span></code></pre>\n</figure>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span><span class=\"nb\">export </span><span class=\"nv\">OPENBLAS_NUM_THREADS</span><span class=\"o\">=</span>4\n<span class=\"nv\">$ </span>ipython</code></pre>\n</figure>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">timeit</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"mi\">10</span> <span class=\"n\">loops</span><span class=\"p\">,</span> <span class=\"n\">best</span> <span class=\"n\">of</span> <span class=\"mi\">3</span><span class=\"p\">:</span> <span class=\"mf\">46.3</span> <span class=\"n\">ms</span> <span class=\"n\">per</span> <span class=\"n\">loop</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"mi\">1000</span> <span class=\"o\">**</span> <span class=\"mi\">3</span> <span class=\"o\">/</span> <span class=\"mf\">0.0463</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span>  <span class=\"c1\"># compute GFLOPS\n</span><span class=\"mf\">21.598272138228943</span></code></pre>\n</figure>\n\n<p>This is about four times faster than reference.  If you’re not already\nparallelizing in some other way (like with <code class=\"language-plaintext highlighter-rouge\">dask</code>) then you should use a modern\nBLAS like OpenBLAS or MKL.</p>\n\n<h2 id=\"openblas--dask\">OpenBLAS + dask</h2>\n\n<p>Finally we run on-disk our experiment again, now with OpenBLAS.  We do this\nboth with OpenBLAS running with one thread and with many threads.</p>\n\n<p>We’ll skip the code (it’s identical to what’s above) and give a comprehensive\ntable of results below.</p>\n\n<p>Sadly the out-of-core solution doesn’t improve much by using OpenBLAS.\nAcutally when both OpenBLAS and dask try to parallelize we <em>lose</em> performance.</p>\n\n<h2 id=\"results\">Results</h2>\n\n<table>\n  <thead>\n  <tr>\n    <th>Performance (GFLOPS)</th>\n    <th>In-Memory</th>\n    <th>On-Disk</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Reference BLAS</th>\n      <td>6</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>OpenBLAS one thread</th>\n      <td>11</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>OpenBLAS four threads</th>\n      <td>22</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><strong>tl:dr</strong> When doing compute intensive work, don’t worry about using disk, just\ndon’t use two mechisms of parallelism at the same time.</p>\n\n<h2 id=\"main-take-aways\">Main Take-Aways</h2>\n\n<ol>\n  <li>We don’t lose much by operating from disk in compute-intensive tasks</li>\n  <li>Actually we can improve performance when an optimized BLAS isn’t avaiable.</li>\n  <li>Dask doesn’t benefit much from an optimized BLAS.  This is sad and surprising.  I expected performance to scale with single-core in-memory performance.  Perhaps this is indicative of some other limiting factor</li>\n  <li>One shouldn’t extrapolate too far with these numbers.  They’re only relevant for highly compute-bound operations like matrix-matrix multiply</li>\n</ol>\n\n<p>Also, thanks to Wesley Emeneker for finding where we were leaking memory,\nmaking results like these possible.</p>"
}