{
  "title": "Tensor Networks",
  "link": "",
  "published": "2018-06-19T23:40:00-05:00",
  "updated": "2018-06-19T23:40:00-05:00",
  "author": {
    "name": "Brandon Brown"
  },
  "id": "tag:outlace.com,2018-06-19:/TensorNets1.html",
  "summary": "<p>In this post we explore tensor networks, their mathematical properties, and implement a tensor network as a machine learning model for a toy problem.</p>",
  "content": "<h1>On Deep Tensor Networks and the Nature of Non-Linearity</h1>\n<p><strong>Abstract</strong>: We cover what tensor networks are, how they work, and go off on a tangent discussion about linearity to derive insight into how tensor networks can learn non-linear transformations without an explicit nonlinear activation function like traditional neural networks. We demonstrate how a simple tensor network can perform reasonably well on the FashionMNIST dataset using PyTorch's <code>einsum</code> function.</p>\n<p><img src=\"images/TensorNetwork/Tensor_Network.png\" width=400px></p>\n<p>Deep learning algorithms (neural networks) in their simplest form are\njust a sequence of two operations composed to some depth: a linear transformation (i.e. a matrix-vector multiplication) followed by the element-wise application of a reasonably well-behaved non-linear function (called the activation function). Together the linear transformation and the non-linear function are called a \"layer\" of the network, and the composition of many layers forms a deep neural network. Both the depth and the non-linearity of deep neural networks are crucial for their generalizability and learning power.</p>\n<p>The most credible explanation for why depth matters in neural networks is that depth models hierarchy, i.e. the idea that some data are at a higher and lower levels of abstraction. \"My Jack Terrier dog Spot\" is a concrete instance of the more abstract concept of \"dog\" or even \"animal.\" Hence depth naturally models these kinds of hierarchical relationships in data, and it turns out pretty much all of the data we care about has a hierarchical structure (e.g. letters -&gt; words -&gt; sentences, or musical notes -&gt; chords -&gt; bars -&gt; songs). Another word for hierarchy is composition; complex things are often composed of simpler things, which are composed of yet simpler things. Hence <em>deep</em> neural networks have an (inductive) bias toward learning these compositional/hierarchical relationships, which is exactly what we want them to do.</p>\n<p>Okay so depth is important. But why is the non-linear \"activation\" function so important for deep learning? \nAn almost tautological answer is that if we want our neural network to be able to model non-linear relationships, it must have some non-linearity itself.</p>\n<p>Another reason is that without the non-linear functions, a sequence of just linear transformation layers\ncan always be \"collapsed\" into a single linear transformation. This is basic linear algebra; the product\nof any sequence of matrices (i.e. matrix composition, hence composition of linear transformations)\ncan always be reduced to a single matrix/linear transformation. So without the non-linear function in each layer\nwe forfeit the compositionality (depth) property of deep neural networks, which allow hierarchical abstraction as \nyou go deeper in the network.</p>\n<p>Since we generally use \"bias units\" in the linear portion of a layer, every layer can actually perform\naffine transformations (linear transformation + a translation). We know from linear algebra that\naffine transformations can only uniformly stretch and shrink (scale), rotate, sheer and translate vectors in a vector space. If we think of some data of interest as a point cloud in some N-dimensional space, then if it is non-random data, it will have some sort of shape.</p>\n<p>For example, some fairly non-complex periodic system might produce data points that lie on a 2D circle or a higher-dimensional loop. The typical goal for a supervised neural network is to map this point cloud onto some other point cloud living in a different space with a different shape. When we one-hot encode the output of a neural network then we're mapping our point cloud onto essentially orthonormal unit basis vectors in a different space. Quite literally, we're transforming some geometric shape (or space) into a different shape. The task of the neural network then is to figure how to construct a new shape (the output/target space) from a starting shape (the input data point cloud) using only the tools of affine transformations and (usually) a single type of non-linear function, such as the rectified linear unit (ReLU) or sigmoid.</p>\n<div style=\"display:table\">\n    <div style=\"display: table-row;\">\n    <div style=\"float:left;display: table-cell; margin-right:50px; \"><img src=\"images/TensorNetwork/sigmoid_activation.png\" width=350px></div>\n    <div style=\"display: table-cell; \"><img src=\"images/TensorNetwork/relu_activation.png\" width=350px></div>\n   </div>\n</div>\n\n<p>On the left is a graph of the sigmoid activation function. Clearly it is a curved function and hence not linear. Sigmoid has mostly fallen out of favor and ReLU has become the de facto standard activation function. The right figure is the graph for ReLU. ReLU is just <span class=\"math\">\\(max(0,x)\\)</span>, and has two \"pieces,\" the flat line for when <span class=\"math\">\\(x \\lt 0\\)</span> and the sloped line for when <span class=\"math\">\\(x \\geq 0\\)</span>. So both of ReLU's parts are lines, why does it count as a non-linear function? A standard, but unintuitive, mathematical definition for a linear function is a function that has the following properties:</p>\n<p>(Definition 1.1: Linear Function)\n</p>\n<div class=\"math\">$$\nf : X \\rightarrow Y  \\\\ \n\\text{$f$ is a function from some domain X to some codomain Y} \\\\ \nf(x_1 + x_2) = f(x_1) + f(x_2) \\\\\nf(a \\times x) = a \\times f(x)\n$$</div>\n<p>If you check to see if ReLU has these properties you'll find that it fails both properties, and hence it is not a linear function. For example, <span class=\"math\">\\(relu(-1+1) \\neq relu(-1) + relu(1)\\)</span> and <span class=\"math\">\\(relu(-2 * 5) \\neq -2*relu(5)\\)</span>.</p>\n<p>One interesting consequence of linearity is that input data cannot be copied and used in the function expression. For example, if you have a non-linear function like <span class=\"math\">\\(f(x) = x^2\\)</span>, it is non-linear because it violates the definition of linearity above, but also the input <span class=\"math\">\\(x\\)</span> is copied. In a handwavy way, linear functions seem to follow some sort of conservation of data law.</p>\n<p>However, consider a function like <span class=\"math\">\\(f(x,y) = x*y\\)</span>, it is called multi-linear because it is linear with respect to each of its input variables. Interestingly, if <span class=\"math\">\\(y = x\\)</span> (always) then this function behaves exactly like <span class=\"math\">\\(f(x)=x^2\\)</span> but it's just in a different form. More generally, if <span class=\"math\">\\(f(x,y) = x*y\\)</span> but y is merely correlated with x, say <span class=\"math\">\\(y = x + e\\)</span> (e = noise) then you'll get <span class=\"math\">\\(f(x,y) = x(x+e) = x^2 + ex\\)</span>, which for a sufficiently small <span class=\"math\">\\(e\\)</span> will behave nearly the same as <span class=\"math\">\\(f(x) = x^2\\)</span>. So just a linear function of two variables that are highly correlated will produce non-linear behavior. Hence, if you have say two separate_linear_ dynamic systems that start off completely un-entangeled with each having their own maximum degrees of freedom, if they start to become correlated/entangled with each other by interaction, then when taken together as a composite system, it may exhibit non-linear dynamics.</p>\n<h3>A brief detour into calculus</h3>\n<p>Let's take a bit of a detour into calculus, because thinking of linear vs nonlinear got me rethinking some basic calculus from high school. You know that the derivative of a function <span class=\"math\">\\(f(x)\\)</span> at a point <span class=\"math\">\\(a\\)</span> is the slope of the tangent line at that point. More generally for multivariate and vector-valued functions, the derivative is thought of as the best linear approximation to that function. You've seen the definition of the derivative in terms of limits or in non-standard analysis as an algebraic operation with an extended real number system (the hyperreals).</p>\n<p>You know how to differentiate simple functions such as <span class=\"math\">\\(f(x) = x^3\\)</span> by following some derivative rules. For example, in this case we apply the rule: \"take the exponent of <span class=\"math\">\\(x\\)</span> and multiply it by <span class=\"math\">\\(x\\)</span> and then reduce the power of <span class=\"math\">\\(x\\)</span> by 1.\" So we calculate <span class=\"math\">\\(\\frac{df}{dx} = 3x^2\\)</span> (where <span class=\"math\">\\(\\frac{df}{dx}\\)</span> is Leibniz notation, read as \"the derivative of the function <span class=\"math\">\\(f\\)</span> with respect to variable <span class=\"math\">\\(x\\)</span>\"). In a a formula we declare that if <span class=\"math\">\\(f(x)=x^{r}\\)</span>, then <span class=\"math\">\\(df/dx = rx^{r-1}\\)</span>. And of course we've <em>memorized</em> a bunch of other simple rules, which together form a powerful toolset we can use to compute the derivative of many functions.</p>\n<p>Since differentiation is all about linear approximations, what new insights might we come to if we think of nonlinearity as involving copying (or deleting) data?\nWell, let's walk through what would happen if we took a non-linear function and re-construed it as a linear function?</p>\n<p>Take our simple function <span class=\"math\">\\(f(x) = x^3\\)</span> and expand it to make explicit the copying that is happening, <span class=\"math\">\\(f(x) = x \\cdot x \\cdot x\\)</span>. This function makes 3 copies of <span class=\"math\">\\(x\\)</span> and then reacts them together (multiplication) to produce the final result. The copying operation is totally hidden with traditional function notation, but it happened.</p>\n<p>What does this have to do with calculus? Well let's convert <span class=\"math\">\\(f(x)=x\\cdot x \\cdot x\\)</span> into a multi-linear function, i.e. <span class=\"math\">\\(f(x,y,z) = x\\cdot y \\cdot z\\)</span>, except that we know <span class=\"math\">\\(x = y = z\\)</span> (or that they're highly correlated). It is multi-linear because if we assumed data independence, then the function is linear with respect to each variable. For example, <span class=\"math\">\\(f(5+3,y,z) = 8yz\\)</span> which is the same as <span class=\"math\">\\(f(5,y,z) + f(3,y,z) = 5yz + 3yz = 8yz\\)</span>. But since we know our data is not independent, technically this function <em>isn't</em> linear because we can't independently control for each variable. But let's for a momement pretend we didn't know our input data is correlated in this way.</p>\n<p>So remember, we already remember from school that the derivative of <span class=\"math\">\\(f(x)=x^3\\)</span> is <span class=\"math\">\\(\\frac{df}{dx} = 3x^2\\)</span>. We can't take \"the derivative\" of our new multi-linear function anymore, we can only take partial derivatives with respect to each variable. To take the partial derivative of <span class=\"math\">\\(x\\)</span>, we assume the other variables <span class=\"math\">\\(y,z\\)</span> are held as constants, and then it becomes a simple linear equation. We do the same for <span class=\"math\">\\(y,z\\)</span>.</p>\n<div class=\"math\">$$ \n\\begin{align}\n\\frac{\\partial{f}}{\\partial{x}} = yz &amp;&amp; \\frac{\\partial{f}}{\\partial{y}} = xz &amp;&amp; \\frac{\\partial{f}}{\\partial{z}} = yx \n\\end{align}\n$$</div>\n<p>But wait, we can't really hold the other variables constant because we know they're perfectly correlated (actually equal). What we're getting at is a case of the total derivative (see &lt; https://en.wikipedia.org/wiki/Total_derivative &gt;). In this particular case, since <span class=\"math\">\\(x = y = z\\)</span>, we just need to combine (sum) all these partial derivatives together and make all the variables the same, say <span class=\"math\">\\(x\\)</span>.</p>\n<div class=\"math\">$$ \\frac{df}{dx} = yz + xz + yx = xx + xx + xx = 3x^2 $$</div>\n<p>Exactly what we get with through the traditional route. Or consider a slightly more complicated function:\n</p>\n<div class=\"math\">$$\nf(x) = 3x^3+x^2 \\rightarrow \\frac{df}{dx} = 9x^2 + 2x \\\\\nf(a,b,c,d,e) = 3abc+de \\rightarrow \\frac{df}{d\\{abcde\\}} = 3bc+3ac+3ab + e + d \\rightarrow 9x^2 + 2x\n$$</div>\n<p>Hence an intuition non-linearity as involving copying makes computing derivatives more intuitive, at least for me.</p>\n<h3>Tensor Networks</h3>\n<p>In the tensor algebra, a scalar is a 0-tensor, a vector is a 1-tensor, a matrix is a 2-tensor, and higher order tensors don't generally have names. But in order for the linear algebra operation we just did, we had to promote a 0-tensor to a 1-tensor. Tensors are often notated by labeled indices, as if they were containers and we find elements of the container by an addressing mechanism. A 0-tensor (scalar) isn't a container, it is the \"atomic ingredient\" of higher tensors, hence it does not have indices. Once you box together a bunch of scalars, you get a 1-tensor (a vector), and now to locate the individual scalars in the box, we label each one with a positive integer. </p>\n<p>If we have a vector <span class=\"math\">\\(A = \\langle{a,b,c}\\rangle\\)</span>, where <span class=\"math\">\\(a,b,c\\)</span> are scalars, then we could label them <span class=\"math\">\\(1,2,3\\)</span> in order. Hence we could refer to the <span class=\"math\">\\(i'th\\)</span> element of <span class=\"math\">\\(A\\)</span> as <span class=\"math\">\\(A_i\\)</span> or <span class=\"math\">\\(A(i)\\)</span>. So <span class=\"math\">\\(A_1 = a\\)</span> or <span class=\"math\">\\(A(3) = c\\)</span>. Now we can box together a bunch of 1-tensors (vectors) and we'll get a 2-tensor (matrix). A matrix <span class=\"math\">\\(M(i,j)\\)</span> hence would have two indices, so we need to supply two numbers to find a single scalar in the matrix. If we supply a partial address, such as <span class=\"math\">\\(M(1,j)\\)</span> then this would return a 1-tensor, whereas <span class=\"math\">\\(M(1,3)\\)</span> would return a 0-tensor. We can box together a bunch of 2-tensors to get a 3-tensor and so on. Importantly, anytime you box some <span class=\"math\">\\(k\\)</span>-tensors together to form a higher order tensor, they must be of the same size.</p>\n<p>Tensors are in a sense compositional mathematical objects. Scalars are \"made of\" nothing. Vectors are \"made of\" scalars, matrices are made of vectors, 3-tensors are made of 2-tensors. Perhaps this suggests that tensors have even more power to represent compositionality in data than do conventional neural networks, which usually only represent depth-wise hierarchy.</p>\n<p>If we have the natural compositionality of individual tensors, we can network them together to form (deep) tensor networks! As we'll soon see, however, there is no explicit non-linear activation function application in a tensor network, everything appears perfectly linear. Yet we Networking in a neural network is nothing more impressive than matrix multiplication. Tensors have a generalization of matrix multiplication called <strong>tensor contraction</strong>.</p>\n<h4>Tensor Contraction</h4>\n<p>Take a 2-tensor (matrix) and multiply it with a vector (1-tensor).</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">]])</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code>array([-2,  5])\n</code></pre></div>\n\n<p>The matrix on the left is the identity matrix, so it didn't do anything to our vector, but what's the computation happening and how can it be generalized to tensors?</p>\n<p>The answer is tensor (or index) contraction. We'll denote a matrix <span class=\"math\">\\(A_{rt}\\)</span> and a vector <span class=\"math\">\\(B_t\\)</span>. Multiplying them together is a simple operation really.</p>\n<div class=\"math\">$$ C_r = \\sum_{t}A_{rt}\\cdot B_t $$</div>\n<p>This equation is saying that a tensor contraction between matrix matrix <span class=\"math\">\\(A_{rt}\\)</span> and a vector <span class=\"math\">\\(B_t\\)</span> results in a new vector <span class=\"math\">\\(C_r\\)</span> (because it only has one index), and each element of <span class=\"math\">\\(C_r\\)</span> indexed by <span class=\"math\">\\(r\\)</span> is determined by taking the sum of the product of each subset of <span class=\"math\">\\(A_{rt}\\)</span> with <span class=\"math\">\\(B_t\\)</span> for every value of <span class=\"math\">\\(t\\)</span>. Often times the summation symbol is ommitted, so we express a tensor contraction just by juxtaposition, <span class=\"math\">\\(A_{rt}B_{t}\\)</span>. This often goes by the name <strong>Einstein summation notation</strong>, so whenever you juxtapose two tensors that have at least one shared index it denotes a tensor contraction.</p>\n<p>A simpler example is easier to calculate. The inner product of two vectors returns a scalar.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">])</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code><span class=\"mf\">10</span>\n</code></pre></div>\n\n<p>This result is from <span class=\"math\">\\(1*3+2*2+3*1\\)</span>. Let's see why using a tensor contraction. We'll define two vectors <span class=\"math\">\\(F_h = \\langle{1,2,3}\\rangle, G_i = \\langle{3,2,1}\\rangle $. We can take the inner product of two vectors when they're of equal length, in which case they are, so we can change the index label of $F_h\\)</span> to <span class=\"math\">\\(F_i\\)</span>. Now they have the same indices, we can do the tensor summation.</p>\n<div class=\"math\">$$ H = \\sum_i F_i\\cdot G_i $$</div>\n<p>It should be clear that we just multiply together corresponding elements then sum them all up, getting the inner product that numpy computed for us. Note that once we've done the sum over products, the matching indices have been fully contracted and the resulting tensor will thus have 2 less indices than the combined tensor. That is, combining the two vectors we get a tensor with two indices (a 2-tensor), but once we contract them, we get a 0-tensor.</p>\n<p>There is a convenient graphical representation of tensors that is really helpful in reasoning about tensor contractions and tensor networks. All you do is represent a tensor as some simple geometric shape, we'll use a square, and then for each index the tensor has, you draw that many number of \"legs\" or <strong>strings</strong> emanating from the box. For example, here is a matrix and a 3-tensor with 2 and 3 strings, respectively:</p>\n<div style=\"display:table\">\n    <div style=\"display: table-row;\">\n    <div style=\"float:left;display: table-cell;\"><img src=\"images/TensorNetwork/matrix1.png\" width=200px></div>\n    <div style=\"display: table-cell;\"><img src=\"images/TensorNetwork/3tensor1.png\" width=200px></div>\n   </div>\n</div>\n\n<p>Each index has been labeled. Now we can form tensor networks by connecting these tensors if they have compatible indices. For example, here is that inner product we just worked out:</p>\n<p><img src=\"images/TensorNetwork/inner_product1.png\"></p>\n<p>And here is a matrix vector multiplication:</p>\n<p><img src=\"images/TensorNetwork/matrix_vector_mult.png\" width=300px>\nYou can see the vector on the left since it only has one string exiting from it whereas the matrix on the right has two strings. Once they share a string (an index), that represents a contraction. Notice that the number of remaining open-ended strings represents the rank or order of the tensor that results after the contraction. The matrix-vector multiplication diagram has 1 open string, so we know this contraction returns a new vector.</p>\n<p>Since a simple feedforward neural network is nothing more than a series of matrix multiplications with non-linear activation functions, we can represent this easily diagrammatically: </p>\n<p><img src=\"images/TensorNetwork/nn3.png\" width=370px></p>\n<p>Where the <span class=\"math\">\\(\\sigma\\)</span> (sigma) symbol represents the non-linear function. If we removed that, we would have a tensor network. But remember, this whole post is about non-linearity and tensor networks, so how do we get back the non-linearity in a tensor network?</p>\n<p>Copying.</p>\n<p>All we need to do is violate the no-cloning rule in the topology of the tensor network, and it will be able to learn non-linear functions. Consider the following two tensor networks. One has two component tensors that are both 3-tensors. Since there are 2 open strings, we know the result is a 2-tensor, however, you can also think of this as a directional network in which we plug in an input vector (or matrix if it's a minibatch) on the left and the network produces an output vector on the right.\n<br /><br /></p>\n<div style=\"display:table\">\n    <div style=\"display: table-row;\">\n    <div style=\"float:left;display: table-cell; margin-right:50px; margin-bottom:25px;\"><img src=\"images/TensorNetwork/tensornet1.png\" width=350px></div>\n    <div style=\"display: table-cell; \"><img src=\"images/TensorNetwork/tensornet2.png\" width=350px></div>\n   </div>\n</div>\n\n<p>The tensor network on the left has a tensor <span class=\"math\">\\(A\\)</span> can can produce two strings from 1 input string, so it has the ability to copying its input, however, both copies get passed to a single tensor <span class=\"math\">\\(B\\)</span>, so this network cannot produce non-linear behavior because both copies will be entangled and cannot be transformed independently by tensor <span class=\"math\">\\(B\\)</span>. In contrast, the tensor network on the right <em>can</em> produce non-linear behavior (as we'll soon show) and that's because tensor <span class=\"math\">\\(A\\)</span> can produce two copies of its input and each copy gets independently transformed by two different tensors <span class=\"math\">\\(B,C\\)</span>, which then pass their result to tensor <span class=\"math\">\\(D\\)</span> which computes the final result.</p>\n<p>Ready to see some non-linearity arise from what appears to be a purely linear network? Let's see if we can train the tensor network on the right to learn the ReLU non-linear activation function. That would surely be a sign it can do something non-linear. It turns out numpy, PyTorch and TensorFlow all have functions called <strong>einsum</strong> that can compute tensor contractions as we've discussed. Let's see.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.optim</span> <span class=\"k\">as</span> <span class=\"nn\">optim</span>\n<span class=\"kn\">from</span> <span class=\"nn\">matplotlib</span> <span class=\"kn\">import</span> <span class=\"n\">pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n</code></pre></div>\n\n<p>Here we define the tensor network by first setting up our individual tensor components. Then we define a function that will accept some input tensor (in this case a scalar) and will connect up the tensors into a network by defining the tensor contractions that will happen.</p>\n<p>The notation used for the <em>einsum</em> function in PyTorch, you write the indices of the involved tensors in a string and then pass the actual tensor objects as the second argument in a tuple. In the string, all the tensor indices for each involved tensor should be listed together (as single characters), then a comma for the next tensor with all of its indices and so on. Then you write '-&gt;' to indicate the resultant tensor with its indices. </p>\n<p>Take the first string we see, 'sa,abc-&gt;sbc'. This means we will contract two tensors, the first has two indices, the second has three. We label the indices in the way we want the indices to contract. In this case, the 's' index represents the batch size and the 'a' index is the actual data. So we want to contract the data with the 2nd tensor, so we label its first index as 'a' as well. The resulting tensor indices will be whatever indices were not contracted.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">b4</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">tensorNet1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">r1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sa,abc-&gt;sbc&#39;</span><span class=\"p\">,(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">b1</span><span class=\"p\">))</span>\n    <span class=\"n\">r2</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sbc,bd-&gt;sd&#39;</span><span class=\"p\">,(</span><span class=\"n\">r1</span><span class=\"p\">,</span><span class=\"n\">b2</span><span class=\"p\">))</span>\n    <span class=\"n\">r3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sbc,bd-&gt;sd&#39;</span><span class=\"p\">,(</span><span class=\"n\">r1</span><span class=\"p\">,</span><span class=\"n\">b3</span><span class=\"p\">))</span>\n    <span class=\"n\">r4</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sd,sd,df-&gt;sf&#39;</span><span class=\"p\">,(</span><span class=\"n\">r2</span><span class=\"p\">,</span><span class=\"n\">r3</span><span class=\"p\">,</span><span class=\"n\">b4</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">r4</span>\n</code></pre></div>\n\n<p>If you're familiar with PyTorch, this is just a simple training loop. Normalizing the data seems to be important empirically for tensor contractions. Each tensor in the tensor network is a \"trainable object.\" So if our tensor network contains a 3-tensor with indices of size <span class=\"math\">\\(10\\times 5\\times 10\\)</span> then this tensor has <span class=\"math\">\\(10 * 5 * 10 = 500\\)</span> total number of parameters. The network we've just defined above has a total of <span class=\"math\">\\(1*10*10 + 10*10 + 10*10 + 10*1 = 3,000\\)</span> parameters.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">([</span><span class=\"n\">b1</span><span class=\"p\">,</span><span class=\"n\">b2</span><span class=\"p\">,</span><span class=\"n\">b3</span><span class=\"p\">,</span><span class=\"n\">b4</span><span class=\"p\">],</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n<span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">()</span>\n<span class=\"n\">losses</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">5000</span><span class=\"p\">):</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">tensorNet1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n    <span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">500</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Epoch: </span><span class=\"si\">{}</span><span class=\"s2\"> | Loss: </span><span class=\"si\">{}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">))</span>\n    <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">0</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">286692.1875</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">500</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">949.7288818359375</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">1000</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.07024684548377991</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">1500</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.025008967146277428</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">2000</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.02917313575744629</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">2500</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.019280623644590378</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">3000</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.03735805302858353</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">3500</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.039074432104825974</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">4000</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.0267774797976017</span>\n<span class=\"n\">Epoch</span><span class=\"o\">:</span> <span class=\"mi\">4500</span> <span class=\"o\">|</span> <span class=\"n\">Loss</span><span class=\"o\">:</span> <span class=\"mf\">0.026810143142938614</span>\n</code></pre></div>\n\n<p>The loss went steadily down, but let's see if it reliably learns the ReLU function. Remember, we're expecting that it will set negative numbers to 0 (or close to 0) and leave positive numbers as their original value (or at least close to their original values).</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">t1</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"c1\">#Some test data</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">)</span>\n<span class=\"n\">tensorNet1</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code>tensor([[-0.3854],\n        [-0.6216],\n        [ 0.2522],\n        [-0.6337]])\n\n\n\n\n\ntensor([[ 0.1582],\n        [ 0.4116],\n        [ 0.0678],\n        [ 0.4277]])\n</code></pre></div>\n\n<p>Well it learned something! It actually looks like it learned how to square the numbers, rather than the relu function, but still, it's a nonlinear function. And if you tried this same training task using the \"<em>linear</em> topology\" from the diagram on the right above, you would be completely unable to learn this non-linear function no matter how you tune the hyperparameters and no matter how big the tensors are. The wiring is everything. The non-linearity arises from the network structure (global) not the individual tensor contractions (locally linear).</p>\n<p>To me, this is quite interesting. It has been shown that certain kinds of tensor networks are directly related to convolutional neural networks with max pooling, see &lt; https://arxiv.org/abs/1603.00162 &gt;</p>\n<p>The fact that the nonlinearity is due to the network itself suggests that you can dynamically tune how nonlinear the network is, by controlling the degree of data copying that the network can do. I think the generality and flexibility of tensor networks can allow you to design networks with just the right amount of inductive bias given your data. You can design tree networks, grid networks, or any topology you can think of as long as the contractions are possible.</p>\n<p><img src=\"images/TensorNetwork/tensor_topologies.png\" /></p>\n<h3>Are Tensor Networks useful?</h3>\n<p>Okay, so that's all theoretically very interesting, but are tensor networks useful? Are they better than neural networks at anything? Well, tensor networks arose in the Physics community as a powerful tool to simulate quantum systems, so they're definitely useful in that domain. Unfortunately the jury is still out on whether they can be \"better\" than neural networks. So far, traditional deep networks continue to dominate. </p>\n<p>So far, I can't tell either from my own experiments with tensor networks or from the literature. However, because the components of tensor networks, tensors, are just high-order structured data, they have shown to be much more interpretable as parameters than the weights of a neural network that get rammed through activation functions. One small benefit I've noticed (but take it with a big grain of salt) is that I can use a much higher learning rate with a tensor network than a neural network without causing divergence during training.</p>\n<p>Tensor networks can be very useful in a method called <strong>tensor decompositions</strong>. The idea is that if you have a huge matrix of data, say 1,000,000 x 1,000,000 entries, you can decompose it into a the contraction of a series of smaller tensors, such that when contracted they will sufficiently approximate the original matrix. It turns out that when you train a tensor decomposition network to approximate your data, it will often learn rather interpretable features.</p>\n<p><img src=\"images/TensorNetwork/tensornet5.png\" /></p>\n<p>Below, I've included some code for a linear (i.e. it cannot learn a non-linear function) tensor network that can be trained to classify FashionMNIST clothes. A 2-layer convolutional neural network with about 500,000 parameters can achieve over 92% accuracy, whereas this simple linear tensor network can achieve about 88.5% accuracy (but quite quickly). The reason for using a linear tensor network is mostly because a decently sized tensor network with non-linear topology (e.g. a hierarchical tree like the figure in the beginning of the post) would be too much code, and a bit too confusing to read with raw einsum notation. </p>\n<p>I do include a slightly more complex non-linear topology at the very end that does achieve up to 94% train/ 90% test accuracy, which is getting competitive conventional neural networks. Whether more complex topologies of tensor networks can achieve even better results is left as an exercise for the reader.</p>\n<h3>Training a (Linear) Tensor Network on Fashion MNIST</h3>\n<h4>Setup a training function</h4>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">[],</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n\n    <span class=\"n\">criterion</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span>\n    <span class=\"n\">train_loss</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">train_accu</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n        <span class=\"c1\"># trainning</span>\n        <span class=\"n\">ave_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n        <span class=\"n\">correct_cnt</span><span class=\"p\">,</span> <span class=\"n\">ave_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n        <span class=\"n\">total_cnt</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n        <span class=\"k\">for</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">):</span>\n            <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">target</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">target</span>\n            <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">shape</span><span class=\"p\">:</span>\n                <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"o\">*</span><span class=\"n\">shape</span><span class=\"p\">))</span>\n            <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n            <span class=\"n\">ave_loss</span> <span class=\"o\">=</span> <span class=\"n\">ave_loss</span> <span class=\"o\">*</span> <span class=\"mf\">0.9</span> <span class=\"o\">+</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mf\">0.1</span>\n            <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">pred_label</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">total_cnt</span> <span class=\"o\">+=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n            <span class=\"n\">correct_cnt</span> <span class=\"o\">+=</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">pred_label</span> <span class=\"o\">==</span> <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">())</span>\n            <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">correct_cnt</span> <span class=\"o\">/</span> <span class=\"n\">total_cnt</span>\n            <span class=\"n\">train_loss</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n            <span class=\"n\">train_accu</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">acc</span><span class=\"p\">)</span>\n            <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n            <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n            <span class=\"sd\">&#39;&#39;&#39;if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):</span>\n<span class=\"sd\">                print(&#39;==&gt;&gt;&gt; epoch: {}, batch index: {}, train loss: {:.6f}, accuracy: {}&#39;.format(</span>\n<span class=\"sd\">                    epoch, batch_idx+1, ave_loss, acc))&#39;&#39;&#39;</span>\n        <span class=\"c1\"># testing</span>\n        <span class=\"n\">correct_cnt</span><span class=\"p\">,</span> <span class=\"n\">ave_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n        <span class=\"n\">total_cnt</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n        <span class=\"k\">for</span> <span class=\"n\">batch_idx</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"p\">):</span>\n            <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">shape</span><span class=\"p\">:</span>\n                <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"o\">*</span><span class=\"n\">shape</span><span class=\"p\">))</span>\n            <span class=\"n\">out</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">criterion</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"p\">,</span> <span class=\"n\">target</span><span class=\"p\">)</span>\n            <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">pred_label</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">out</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"n\">total_cnt</span> <span class=\"o\">+=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n            <span class=\"n\">correct_cnt</span> <span class=\"o\">+=</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">pred_label</span> <span class=\"o\">==</span> <span class=\"n\">target</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">())</span>\n            <span class=\"c1\"># smooth average</span>\n            <span class=\"n\">ave_loss</span> <span class=\"o\">=</span> <span class=\"n\">ave_loss</span> <span class=\"o\">*</span> <span class=\"mf\">0.9</span> <span class=\"o\">+</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mf\">0.1</span>\n            <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">correct_cnt</span> <span class=\"o\">*</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">total_cnt</span>\n            <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">batch_idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">or</span> <span class=\"p\">(</span><span class=\"n\">batch_idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"p\">):</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;==&gt;&gt;&gt; epoch: </span><span class=\"si\">{}</span><span class=\"s1\">, batch index: </span><span class=\"si\">{}</span><span class=\"s1\">, test loss: </span><span class=\"si\">{:.6f}</span><span class=\"s1\">, acc: </span><span class=\"si\">{:.3f}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span> \\\n                      <span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">batch_idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">ave_loss</span><span class=\"p\">,</span> <span class=\"n\">acc</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">train_loss</span><span class=\"p\">,</span> <span class=\"n\">train_accu</span>\n</code></pre></div>\n\n<h4>Load up the FashionMNIST Data</h4>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.datasets</span> <span class=\"k\">as</span> <span class=\"nn\">dset</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torchvision.transforms</span> <span class=\"k\">as</span> <span class=\"nn\">transforms</span>\n\n<span class=\"n\">transform</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">(),</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Normalize</span><span class=\"p\">((</span><span class=\"mf\">0.5</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">,))])</span>\n\n<span class=\"n\">train_set</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">FashionMNIST</span><span class=\"p\">(</span><span class=\"s2\">&quot;fashion_mnist&quot;</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">)</span>\n<span class=\"n\">test_set</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">FashionMNIST</span><span class=\"p\">(</span><span class=\"s2\">&quot;fashion_mnist&quot;</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">transform</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">train_loader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span>\n                 <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">train_set</span><span class=\"p\">,</span>\n                 <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span>\n                 <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span><span class=\"n\">drop_last</span><span class=\"o\">=</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">test_loader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span>\n                <span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">test_set</span><span class=\"p\">,</span>\n                <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span>\n                <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">drop_last</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;==&gt;&gt;&gt; total trainning batch number: </span><span class=\"si\">{}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">)))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;==&gt;&gt;&gt; total testing batch number: </span><span class=\"si\">{}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">test_loader</span><span class=\"p\">)))</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code>==&gt;&gt;&gt; total trainning batch number: 600\n==&gt;&gt;&gt; total testing batch number: 100\n</code></pre></div>\n\n<h4>Define the Tensor Network</h4>\n<h5>Total Num. Parameters: 784 * 25 * 25 + 25 * 25 * 10 = 496,250</h5>\n<p><img src=\"images/TensorNetwork/tensornet4a.png\" width=350px></p>\n<p>In this case I labeled each string with its dimension size, which is often called the <strong>bond dimension</strong>. As you'll see below, the 4 interior strings all have a bond dimension of 25.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">25</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span>\n<span class=\"n\">B</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">tensorNet2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sa,abc-&gt;sbc&#39;</span><span class=\"p\">,(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">A</span><span class=\"p\">)))</span>\n        <span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sbc,bct-&gt;st&#39;</span><span class=\"p\">,(</span><span class=\"n\">C</span><span class=\"p\">,</span><span class=\"n\">B</span><span class=\"p\">)))</span>\n        <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"ne\">Exception</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Error: </span><span class=\"si\">{}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">))</span>\n</code></pre></div>\n\n<h4>Train</h4>\n<div class=\"highlight\"><pre><span></span><code><span class=\"o\">%%</span><span class=\"n\">time</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">tensorNet2</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">A</span><span class=\"p\">,</span><span class=\"n\">B</span><span class=\"p\">],</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,))</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code><span class=\"o\">/</span><span class=\"nv\">Users</span><span class=\"o\">/</span><span class=\"nv\">brandonbrown</span><span class=\"o\">/</span><span class=\"nv\">anaconda3</span><span class=\"o\">/</span><span class=\"nv\">envs</span><span class=\"o\">/</span><span class=\"nv\">deeprl</span><span class=\"o\">/</span><span class=\"nv\">lib</span><span class=\"o\">/</span><span class=\"nv\">python3</span>.<span class=\"mi\">6</span><span class=\"o\">/</span><span class=\"nv\">site</span><span class=\"o\">-</span><span class=\"nv\">packages</span><span class=\"o\">/</span><span class=\"nv\">ipykernel</span><span class=\"o\">/</span><span class=\"nv\">__main__</span>.<span class=\"nv\">py</span>:<span class=\"mi\">7</span>: <span class=\"nv\">UserWarning</span>: <span class=\"nv\">Implicit</span> <span class=\"nv\">dimension</span> <span class=\"nv\">choice</span> <span class=\"k\">for</span> <span class=\"nv\">softmax</span> <span class=\"nv\">has</span> <span class=\"nv\">been</span> <span class=\"nv\">deprecated</span>. <span class=\"nv\">Change</span> <span class=\"nv\">the</span> <span class=\"k\">call</span> <span class=\"nl\">to</span> <span class=\"k\">include</span> <span class=\"nv\">dim</span><span class=\"o\">=</span><span class=\"nv\">X</span> <span class=\"nv\">as</span> <span class=\"nv\">an</span> <span class=\"nv\">argument</span>.\n<span class=\"o\">/</span><span class=\"nv\">Users</span><span class=\"o\">/</span><span class=\"nv\">brandonbrown</span><span class=\"o\">/</span><span class=\"nv\">anaconda3</span><span class=\"o\">/</span><span class=\"nv\">envs</span><span class=\"o\">/</span><span class=\"nv\">deeprl</span><span class=\"o\">/</span><span class=\"nv\">lib</span><span class=\"o\">/</span><span class=\"nv\">python3</span>.<span class=\"mi\">6</span><span class=\"o\">/</span><span class=\"nv\">site</span><span class=\"o\">-</span><span class=\"nv\">packages</span><span class=\"o\">/</span><span class=\"nv\">ipykernel</span><span class=\"o\">/</span><span class=\"nv\">__main__</span>.<span class=\"nv\">py</span>:<span class=\"mi\">20</span>: <span class=\"nv\">UserWarning</span>: <span class=\"nv\">invalid</span> <span class=\"nv\">index</span> <span class=\"nv\">of</span> <span class=\"nv\">a</span> <span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"nv\">dim</span> <span class=\"nv\">tensor</span>. <span class=\"nv\">This</span> <span class=\"nv\">will</span> <span class=\"nv\">be</span> <span class=\"nv\">an</span> <span class=\"nv\">error</span> <span class=\"nv\">in</span> <span class=\"nv\">PyTorch</span> <span class=\"mi\">0</span>.<span class=\"mi\">5</span>. <span class=\"nv\">Use</span> <span class=\"nv\">tensor</span>.<span class=\"nv\">item</span><span class=\"ss\">()</span> <span class=\"nv\">to</span> <span class=\"nv\">convert</span> <span class=\"nv\">a</span> <span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"nv\">dim</span> <span class=\"nv\">tensor</span> <span class=\"nv\">to</span> <span class=\"nv\">a</span> <span class=\"nv\">Python</span> <span class=\"nv\">number</span>\n<span class=\"o\">/</span><span class=\"nv\">Users</span><span class=\"o\">/</span><span class=\"nv\">brandonbrown</span><span class=\"o\">/</span><span class=\"nv\">anaconda3</span><span class=\"o\">/</span><span class=\"nv\">envs</span><span class=\"o\">/</span><span class=\"nv\">deeprl</span><span class=\"o\">/</span><span class=\"nv\">lib</span><span class=\"o\">/</span><span class=\"nv\">python3</span>.<span class=\"mi\">6</span><span class=\"o\">/</span><span class=\"nv\">site</span><span class=\"o\">-</span><span class=\"nv\">packages</span><span class=\"o\">/</span><span class=\"nv\">ipykernel</span><span class=\"o\">/</span><span class=\"nv\">__main__</span>.<span class=\"nv\">py</span>:<span class=\"mi\">45</span>: <span class=\"nv\">UserWarning</span>: <span class=\"nv\">invalid</span> <span class=\"nv\">index</span> <span class=\"nv\">of</span> <span class=\"nv\">a</span> <span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"nv\">dim</span> <span class=\"nv\">tensor</span>. <span class=\"nv\">This</span> <span class=\"nv\">will</span> <span class=\"nv\">be</span> <span class=\"nv\">an</span> <span class=\"nv\">error</span> <span class=\"nv\">in</span> <span class=\"nv\">PyTorch</span> <span class=\"mi\">0</span>.<span class=\"mi\">5</span>. <span class=\"nv\">Use</span> <span class=\"nv\">tensor</span>.<span class=\"nv\">item</span><span class=\"ss\">()</span> <span class=\"nv\">to</span> <span class=\"nv\">convert</span> <span class=\"nv\">a</span> <span class=\"mi\">0</span><span class=\"o\">-</span><span class=\"nv\">dim</span> <span class=\"nv\">tensor</span> <span class=\"nv\">to</span> <span class=\"nv\">a</span> <span class=\"nv\">Python</span> <span class=\"nv\">number</span>\n\n\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">0</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">204465</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">795</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">1</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">199457</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">812</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">2</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">196803</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">823</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">3</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">194834</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">827</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">4</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">193722</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">837</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">5</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">191949</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">839</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">6</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">191317</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">844</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">7</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">190564</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">847</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">8</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">190050</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">850</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">9</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">189429</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">853</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">10</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">188894</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">856</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">11</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">188893</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">857</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">12</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">188916</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">859</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">13</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">187949</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">859</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">14</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">187448</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">861</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">15</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">187306</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">864</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">16</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">187102</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">863</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">17</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">187094</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">864</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">18</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">186368</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">866</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">19</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">186422</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">865</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">20</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">186429</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">867</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">21</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">185773</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">867</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">22</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">186021</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">867</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">23</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">185484</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">868</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">24</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">186032</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">870</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">25</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">185131</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">871</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">26</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">185170</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">872</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">27</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">185003</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">869</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">28</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184911</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">873</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">29</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184867</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">874</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">30</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184924</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">874</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">31</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184951</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">874</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">32</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184638</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">875</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">33</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184458</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">874</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">34</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184469</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">874</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">35</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184160</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">875</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">36</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183932</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">876</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">37</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184098</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">875</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">38</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184113</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">39</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183863</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">40</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183771</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">41</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">184383</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">877</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">42</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183535</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">43</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183703</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">44</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183593</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">45</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183371</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">878</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">46</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183289</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">880</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">47</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183360</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">879</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">48</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183175</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">881</span>\n<span class=\"o\">==&gt;&gt;&gt;</span> <span class=\"nv\">epoch</span>: <span class=\"mi\">49</span>, <span class=\"nv\">batch</span> <span class=\"nv\">index</span>: <span class=\"mi\">100</span>, <span class=\"nv\">test</span> <span class=\"nv\">loss</span>: <span class=\"mi\">2</span>.<span class=\"mi\">183238</span>, <span class=\"nv\">acc</span>: <span class=\"mi\">0</span>.<span class=\"mi\">880</span>\n<span class=\"nv\">CPU</span> <span class=\"nv\">times</span>: <span class=\"nv\">user</span> <span class=\"mi\">40</span><span class=\"nv\">min</span> <span class=\"mi\">35</span><span class=\"nv\">s</span>, <span class=\"nv\">sys</span>: <span class=\"mi\">2</span><span class=\"nv\">min</span> <span class=\"mi\">23</span><span class=\"nv\">s</span>, <span class=\"nv\">total</span>: <span class=\"mi\">42</span><span class=\"nv\">min</span> <span class=\"mi\">58</span><span class=\"nv\">s</span>\n<span class=\"nv\">Wall</span> <span class=\"nv\">time</span>: <span class=\"mi\">16</span><span class=\"nv\">min</span> <span class=\"mi\">17</span><span class=\"nv\">s</span>\n</code></pre></div>\n\n<p>Not too bad right?</p>\n<h3>Simple Non-Linear Tensor Network</h3>\n<h4>Total Params: 784*20^2+4*20^2+20^4+20^3 = 479,200</h4>\n<p><img src=\"images/TensorNetwork/tensornet4.png\" width=500px></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">nl1</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># a,b,c</span>\n<span class=\"n\">nl2</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># c,e</span>\n<span class=\"n\">nl3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># d,f</span>\n<span class=\"n\">nl4</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># e,f,g</span>\n<span class=\"n\">nl5</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># c,e</span>\n<span class=\"n\">nl6</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># d,f</span>\n<span class=\"n\">nl7</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">requires_grad</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span> <span class=\"c1\"># e,f,g</span>\n<span class=\"k\">def</span> <span class=\"nf\">tensorNet3</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">r1</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;sb,bcd-&gt;scd&#39;</span><span class=\"p\">,(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">nl1</span><span class=\"p\">)))</span>\n    <span class=\"n\">r2</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;scd,ce-&gt;se&#39;</span><span class=\"p\">,(</span><span class=\"n\">r1</span><span class=\"p\">,</span><span class=\"n\">nl2</span><span class=\"p\">)))</span>\n    <span class=\"n\">r3</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;scd,df-&gt;sf&#39;</span><span class=\"p\">,(</span><span class=\"n\">r1</span><span class=\"p\">,</span><span class=\"n\">nl3</span><span class=\"p\">)))</span>\n    <span class=\"n\">r4</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;se,sf,efgh-&gt;sgh&#39;</span><span class=\"p\">,(</span><span class=\"n\">r2</span><span class=\"p\">,</span><span class=\"n\">r3</span><span class=\"p\">,</span><span class=\"n\">nl4</span><span class=\"p\">)))</span>\n    <span class=\"n\">r5</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;scd,ce-&gt;se&#39;</span><span class=\"p\">,(</span><span class=\"n\">r4</span><span class=\"p\">,</span><span class=\"n\">nl5</span><span class=\"p\">)))</span>\n    <span class=\"n\">r6</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;scd,df-&gt;sf&#39;</span><span class=\"p\">,(</span><span class=\"n\">r4</span><span class=\"p\">,</span><span class=\"n\">nl6</span><span class=\"p\">)))</span>\n    <span class=\"n\">r7</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">normalize</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">einsum</span><span class=\"p\">(</span><span class=\"s1\">&#39;se,sf,efg-&gt;sg&#39;</span><span class=\"p\">,(</span><span class=\"n\">r5</span><span class=\"p\">,</span><span class=\"n\">r6</span><span class=\"p\">,</span><span class=\"n\">nl7</span><span class=\"p\">)))</span>\n    <span class=\"k\">return</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">r7</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<div class=\"highlight\"><pre><span></span><code><span class=\"o\">%%</span><span class=\"n\">time</span>\n<span class=\"n\">loss1b</span><span class=\"p\">,</span> <span class=\"n\">acc1b</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">model1b</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">nl1</span><span class=\"p\">,</span> <span class=\"n\">nl2</span><span class=\"p\">,</span> <span class=\"n\">nl3</span><span class=\"p\">,</span> <span class=\"n\">nl4</span><span class=\"p\">,</span> <span class=\"n\">nl5</span><span class=\"p\">,</span> <span class=\"n\">nl6</span><span class=\"p\">,</span> <span class=\"n\">nl7</span><span class=\"p\">],</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,))</span>\n</code></pre></div>\n\n<p>Unfortunately, this non-linear tensor network performs significantly better with the training data (getting over 94% accuracy at 50 epochs) but the test accuracy tops out at around 88%, similar to the linear network. This just demonstrates that with the added non-linear ability, we can much more easily overfit the data. One thing to try is to add trainable bias tensors into the network as we have in neural networks.</p>\n<h3>Conclusion</h3>\n<p>Tensor Networks can be seen as a generalization of neural networks. Tensor Networks are being actively studied and improved and I believe it is likely they will be a great tool in your machine learning toolkit.</p>\n<h3>References</h3>\n<ul>\n<li>Yau, D. (2015). Operads of Wiring Diagrams. Retrieved from http://arxiv.org/abs/1512.01602</li>\n<li>Maina, S. A. (2017). Graphical Linear Algebra.</li>\n<li>Cichocki, A. (2014). Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions, 1–30. http://doi.org/abs/1403.2048</li>\n<li>Genovese, F. (2017). The Way of the Infinitesimal. Retrieved from http://arxiv.org/abs/1707.00459</li>\n<li>Mangan, T. (2008). A gentle introduction to tensors. J. Biomedical Science and Engineering, 1, 64–67. http://doi.org/10.1016/S0004-3702(98)00053-8</li>\n<li>Kissinger, A., &amp; Quick, D. (2015). A first-order logic for string diagrams, 171–189. http://doi.org/10.4230/LIPIcs.CALCO.2015.171</li>\n<li>Barber, A. G. (1997). Linear Type Theories , Semantics and Action Calculi. Computing. Retrieved from http://hdl.handle.net/1842/392</li>\n<li>Girard, J.-Y. (1995). Linear Logic: its syntax and semantics. Advances in Linear Logic, 1–42. http://doi.org/10.1017/CBO9780511629150.002</li>\n<li>Wadler, P., Val, A., &amp; Arr, V. A. (1990). Philip Wadler University of Glasgow, (April), 1–21.</li>\n<li>Martin, C. (2004). Tensor Decompositions Workshop Discussion Notes. Palo Alto, CA: American Institute of Mathematics (AIM), 1–27. Retrieved from http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:Tensor+Decompositions+Workshop+Discussion+Notes#0</li>\n<li>Kasai, H. (2017). Fast online low-rank tensor subspace tracking by CP decomposition using recursive least squares from incomplete observations, 1–21. Retrieved from http://arxiv.org/abs/1709.10276</li>\n<li>Zhang, Y., Zhou, G., Zhao, Q., Cichocki, A., &amp; Wang, X. (2016). Fast nonnegative tensor factorization based on accelerated proximal gradient and low-rank approximation. Neurocomputing, 198, 148–154. http://doi.org/10.1016/j.neucom.2015.08.122</li>\n<li>Smith, S., Beri, A., &amp; Karypis, G. (2017). Constrained Tensor Factorization with Accelerated AO-ADMM. Proceedings of the International Conference on Parallel Processing, 111–120. http://doi.org/10.1109/ICPP.2017.20</li>\n<li>Stoudenmire, E. M., &amp; Schwab, D. J. (2016). Supervised Learning with Tensor Networks. Advances in Neural Information Processing Systems 29 (NIPS 2016), (Nips), 4799–4807. Retrieved from https://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networks.pdf%0Ahttps://papers.nips.cc/paper/6211-supervised-learning-with-tensor-networksAdvances in Neural Information Processing Systems 29 (NIPS 2016)%0Ahttp://arxiv.org/abs/1605.05775</li>\n<li>Rabanser, S., Shchur, O., &amp; Günnemann, S. (2017). Introduction to Tensor Decompositions and their Applications in Machine Learning, 1–13. Retrieved from http://arxiv.org/abs/1711.10781</li>\n<li>Zafeiriou, S. (2009). Discriminant nonnegative tensor factorization algorithms. IEEE Transactions on Neural Networks, 20(2), 217–235. http://doi.org/10.1109/TNN.2008.2005293</li>\n<li>Cohen, J. E., &amp; Gillis, N. (2018). Dictionary-Based Tensor Canonical Polyadic Decomposition. IEEE Transactions on Signal Processing, 66(7), 1876–1889. http://doi.org/10.1109/TSP.2017.2777393</li>\n<li>Phan, A. H., &amp; Cichocki, A. (2010). Tensor decompositions for feature extraction and classification of high dimensional datasets. Nonlinear Theory and Its Applications, IEICE, 1(1), 37–68. http://doi.org/10.1587/nolta.1.37</li>\n<li>Kossaifi, J., Lipton, Z. C., Khanna, A., Furlanello, T., &amp; Anandkumar, A. (2017). Tensor Contraction &amp; Regression Networks, 1–10. Retrieved from http://arxiv.org/abs/1707.08308</li>\n<li>Ecognition, R., Lipton, Z. C., &amp; Anandkumar, A. (2018). D Eep a Ctive L Earning, 1–15.</li>\n<li>Stoudenmire, E. M. (2017). Learning Relevant Features of Data with Multi-scale Tensor Networks, 1–12. http://doi.org/10.1088/2058-9565/aaba1a</li>\n</ul>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    ""
  ]
}