{
  "title": "Deploy BLOOM-176B and OPT-30B on Amazon SageMaker with large model inference Deep Learning Containers and DeepSpeed",
  "link": "https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/",
  "dc:creator": "Rupinder Grewal",
  "pubDate": "Fri, 04 Nov 2022 16:45:57 +0000",
  "category": [
    "Advanced (300)",
    "Amazon SageMaker",
    "Artificial Intelligence"
  ],
  "guid": "7341bea5feb83eb3ce1c4ff9993a32f5ed4e0169",
  "description": "The last few years have seen rapid development in the field of deep learning. Although hardware has improved, such as with the latest generation of accelerators from NVIDIA and Amazon, advanced machine learning (ML) practitioners still regularly encounter issues deploying their large deep learning models for applications such as natural language processing (NLP). In an […]",
  "content:encoded": "<p>The last few years have seen rapid development in the field of deep learning. Although hardware has improved, such as with the latest generation of accelerators from NVIDIA and Amazon, advanced machine learning (ML) practitioners still regularly encounter issues deploying their large deep learning models for applications such as natural language processing (NLP).</p> \n<p>In an earlier post, we discussed <a href=\"https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/\" target=\"_blank\" rel=\"noopener\">capabilities and configurable settings</a> in <a href=\"https://aws.amazon.com/sagemaker/deploy/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker model deployment</a> that can make inference with these large models easier. Today, we announce a new <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> Deep Learning Container (DLC) that you can use to get started with large model inference in a matter of minutes. This <a href=\"https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\" target=\"_blank\" rel=\"noopener\">DLC</a> packages some of the most popular open-source libraries for model parallel inference, such as DeepSpeed and Hugging Face Accelerate.</p> \n<p>In this post, we use a new SageMaker large model inference DLC to deploy two of the most popular large NLP models: BigScience’s <a href=\"https://huggingface.co/bigscience/bloom\" target=\"_blank\" rel=\"noopener\">BLOOM-176B</a> and Meta’s <a href=\"https://huggingface.co/facebook/opt-30b\" target=\"_blank\" rel=\"noopener\">OPT-30B</a> from the Hugging Face repository. In particular, we use Deep Java Library (DJL) serving and tensor parallelism techniques from DeepSpeed to achieve 0.1 second latency per token in a text generation use case.</p> \n<p>You can find our complete example notebooks in our <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/nlp/realtime/llm\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p> \n<h2>Large model inference techniques</h2> \n<p>Language models have recently exploded in both size and popularity. With easy access from model zoos such as Hugging Face and improved accuracy and performance in NLP tasks such as classification and text generation, practitioners are increasingly reaching for these large models. However, large models are often too big to fit within the memory of a single accelerator. For example, the BLOOM-176B model can require more than 350 gigabytes of accelerator memory, which far exceeds the capacity of hardware accelerators available today. This necessitates the use of &nbsp;model parallel techniques from libraries like DeepSpeed and Hugging Face Accelerate to distribute a model across multiple accelerators for inference. In this post, we use the <a href=\"https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\" target=\"_blank\" rel=\"noopener\">SageMaker large model inference container</a> to generate and compare latency and throughput performance using these two open-source libraries.</p> \n<p>DeepSpeed and Accelerate use different techniques to optimize large language models for inference. The key difference is DeepSpeed’s <a href=\"https://www.deepspeed.ai/2021/03/15/inference-kernel-optimization.html\" target=\"_blank\" rel=\"noopener\">use of optimized kernels</a>. These kernels can dramatically improve inference latency by reducing bottlenecks in the computation graph of the model. Optimized kernels can be difficult to develop and are typically specific to a particular model architecture; DeepSpeed supports popular large models such as OPT and BLOOM with these optimized kernels. In contrast, Hugging Face’s Accelerate library doesn’t include optimized kernels at the time of writing. As we discuss in our results section, this difference is responsible for much of the performance edge that DeepSpeed has over Accelerate.</p> \n<p>A second difference between DeepSpeed and Accelerate is the type of model parallelism. Accelerate uses pipeline parallelism to partition a model between the hidden layers of a model, whereas DeepSpeed uses tensor parallelism to partition the layers themselves. Pipeline parallelism is a flexible approach that supports more model types and can improve throughput when larger batch sizes are used. Tensor parallelism requires more communication between GPUs because model layers can be spread across multiple devices, but can improve inference latency by engaging multiple GPUs simultaneously. You can learn more about parallelism techniques in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\" target=\"_blank\" rel=\"noopener\">Introduction to Model Parallelism</a> and <a href=\"https://huggingface.co/transformers/v4.9.2/parallelism.html\" target=\"_blank\" rel=\"noopener\">Model Parallelism</a>.</p> \n<h2>Solution overview</h2> \n<p>To effectively host large language models, we need features and support in the following key areas:</p> \n<ul> \n <li><strong>Building and testing solutions </strong>– Given the iterative nature of ML development, we need the ability to build, rapidly iterate, and test how the inference endpoint will behave when these models are hosted, including the ability to fail fast. These models can typically be hosted only on larger instances like p4dn or g5, and given the size of the models, it can take a while to spin up an inference instance and run any test iteration. Local testing usually has constraints because you need a similar instance in size to test, and these models aren’t easy to obtain.</li> \n <li><strong>Deploying and running at scale </strong>– The model files need to be loaded onto the inference instances, which presents a challenge in itself given the size. Tar / Un-Tar as an example for the Bloom-176B takes about 1 hour to create and another hour to load. We need an alternate mechanism to allow easy access to the model files.</li> \n <li><strong>Loading the model as singleton </strong>– For a multi-worker process, we need to ensure the model gets loaded only once so we don’t run into race conditions and further spend unnecessary resources. In this post, we show a way to load directly from <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3). However, this only works if we use the default settings of the DJL. Furthermore, any scaling of the endpoints needs to be able to spin up in a few minutes, which calls for reconsidering how the models might be loaded and distributed.</li> \n <li><strong>Sharding frameworks </strong>– These models typically need to be , usually by a tensor parallelism mechanism or by pipeline sharding as the typical sharding techniques, and we have advanced concepts like ZeRO sharding built on top of tensor sharding. For more information about sharding techniques, refer to <a href=\"https://huggingface.co/transformers/v4.9.2/parallelism.html\" target=\"_blank\" rel=\"noopener\">Model Parallelism</a>. To achieve this, we can have various combinations and use frameworks from NIVIDIA, DeepSpeed, and others. This needs the ability to test BYOC or use 1P containers and iterate over solutions and run benchmarking tests. You might also want to test various hosting options like asynchronous, serverless, and others.</li> \n <li><strong>Hardware selection </strong>– Your choice in hardware is determined by all the aforementioned points and further traffic patterns, use case needs, and model sizes.</li> \n</ul> \n<p>In this post, we use DeepSpeed’s optimized kernels and tensor parallelism techniques to host BLOOM-176B and OPT-30B on SageMaker. We also compare results from Accelerate to demonstrate the performance benefits of optimized kernels and tensor parallelism. For more information on DeepSpeed and Accelerate, refer to <a href=\"https://arxiv.org/pdf/2207.00032.pdf\" target=\"_blank\" rel=\"noopener\">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a> and <a href=\"https://huggingface.co/blog/bloom-inference-pytorch-scripts\" target=\"_blank\" rel=\"noopener\">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</a>.</p> \n<p>We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about the DJL and DJLServing, refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/\" target=\"_blank\" rel=\"noopener\">Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference</a>.</p> \n<p>It’s worth noting that optimized kernels can result in precision changes and a modified computation graph, which could theoretically result in changed model behavior. Although this could occasionally change the inference outcome, we do not expect these differences to materially impact the basic evaluation metrics of a model. Nevertheless, practitioners are advised to confirm the model outputs are as expected when using these kernels.</p> \n<p>The following steps demonstrate how to deploy a BLOOM-176B model in SageMaker using DJLServing and a SageMaker large model inference container. The complete example is also available in our <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/nlp/realtime/llm\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p> \n<h2>Using the DJLServing SageMaker DLC image</h2> \n<p>Use the following code to use the DJLServing SageMaker DLC image after replacing the region with your specific region you are running the notebook in:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-code\">763104351884.dkr.ecr.<span style=\"color: #ff0000\">&lt;region&gt;</span>.amazonaws.com/djl-inference:0.19.0-deepspeed0.7.3-cu113\n# example uri might be like 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.19.0-deepspeed0.7.3-cu113</code></pre> \n  </div> \n </div> \n</div> \n<h2>Create our model file</h2> \n<p>First, we create a file called <code>serving.properties</code> that contains only one line of code. This tells the DJL model server to use the DeepSpeed engine. The file contains the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">engine=DeepSpeed</code></pre> \n</div> \n<p><code>serving.properties</code> is a file defined by DJLServing that is used to configure per-model configuration.</p> \n<p>Next, we create our <code>model.py</code> file, which defines the code needed to load and then serve the model. In our code, we read in the <code>TENSOR_PARALLEL_DEGREE</code> environment variable (the default value is 1). This sets the number of devices over which the tensor parallel modules are distributed. Note that DeepSpeed provides a few built-in partition definitions, including one for BLOOM models. We use it by specifying <code>replace_method</code> and <code>relpace_with_kernel_inject</code>. If you have a customized model and need DeepSpeed to partition effectively, you need to change <code>relpace_with_kernel_inject</code> to <code>false</code> and add <code>injection_policy</code> to make the runtime partition work. For more information, refer to <a href=\"https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference\" target=\"_blank\" rel=\"noopener\">Initializing for Inference</a>. For our example, we used the pre-partitioned BLOOM model on DeepSpeed.</p> \n<p>Secondly, in the <code>model.py</code> file, we also load the model from Amazon S3 after the endpoint has been spun up. The model is loaded into the <code>/tmp</code> space on the container because SageMaker maps the <code>/tmp</code> to the <a href=\"http://aws.amazon.com/ebs\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Block Store</a> (Amazon EBS) volume that is mounted when we specify the endpoint creation parameter <code>VolumeSizeInGB</code>. For instances like p4dn, which come pre-built with the volume instance, we can continue to leverage the <code>/tmp</code> on the container. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from djl_python import Input, Output\nimport os\nimport deepspeed\nimport torch\nimport torch.distributed as dist\nimport sys\nimport subprocess\nimport time\nfrom glob import glob\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nfrom transformers.models.opt.modeling_opt import OPTDecoderLayer\n\npredictor = None\n\ndef check_config():\n    local_rank = os.getenv('LOCAL_RANK')\n    \n    if not local_rank:\n        return False\n    return True\n    \ndef get_model():\n\n    if not check_config():\n        raise Exception(\"DJL:DeepSpeed configurations are not default. This code does not support non default configurations\") \n    \n    tensor_parallel = int(os.getenv('TENSOR_PARALLEL_DEGREE', '1'))\n    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n    model_dir = \"/tmp/model\"\n    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n    print(f\"rank: {local_rank}\")\n    if local_rank == 0:\n        if f\"{model_dir}/DONE\" not in glob(f\"{model_dir}/*\"):\n            print(\"Starting Model downloading files\")\n            try:\n                proc_run = subprocess.run(\n                    [\"aws\", \"s3\", \"cp\", \"--recursive\", f\"s3://{bucket}/{key_prefix}\", model_dir]\n                )\n                print(\"Model downloading finished\")\n                # write file when download complete. Could use dist.barrier() but this makes it easier to check if model is downloaded in case of retry\n                with open(f\"{model_dir}/DONE\", \"w\") as f:\n                    f.write(\"download_complete\")\n                    \n                proc_run.check_returncode() # to throw the error in case there was one\n                \n            except subprocess.CalledProcessError as e:\n                print ( \"Model download failed: Error:\\nreturn code: \", e.returncode, \"\\nOutput: \", e.stderr )\n                raise # FAIL FAST  \n                               \n    dist.barrier()\n                \n    \n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    \n    # has to be FP16 as Int8 model loading not yet supported\n    with deepspeed.OnDevice(dtype=torch.float16, device=\"meta\"):\n        model = AutoModelForCausalLM.from_config(\n            AutoConfig.from_pretrained(model_dir), torch_dtype=torch.bfloat16\n        )\n    model = model.eval()\n    \n    model = deepspeed.init_inference(\n        model,\n        mp_size=tensor_parallel,\n        dtype=torch.int8,\n        base_dir = model_dir,\n        checkpoint=os.path.join(model_dir, \"ds_inference_config.json\"),\n        replace_method='auto',\n        replace_with_kernel_inject=True\n    )\n\n    model = model.module\n    dist.barrier()\n    return model, tokenizer</code></pre> \n</div> \n<p>DJLServing manages the runtime installation on any pip packages defined in <code>requirement.txt</code>. This file will have:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">awscli\nboto3</code></pre> \n</div> \n<p>We have created a directory called <code>code</code> and the <code>model.py</code>, <code>serving.properties</code>, and <code>requirements.txt</code> files are already created in this directory. To view the files, you can run the following code from the terminal:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">mkdir -p code\ncat code/model.py \ncat code/serving.properties \ncat code/requirements.txt </code></pre> \n</div> \n<p>The following figure shows the structure of the <code>model.tar.gz</code>.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/ml-11619-img01.jpg\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-45337\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/ml-11619-img01.jpg\" alt=\"\" width=\"1196\" height=\"251\"></a></p> \n<p>Lastly, we create the model file and upload it to Amazon S3:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">tar cvfz model.tar.gz code\ns3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)</code></pre> \n</div> \n<h2>Download and store the model from Hugging Face (Optional)</h2> \n<p>We have provided the steps in this section in case you want to download the model to Amazon S3 and use it from there. The steps are provided in the Jupyter file on GitHub. The following screenshot shows a snapshot of the steps.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/ML-11619-image003.jpg\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-45338\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/ML-11619-image003.jpg\" alt=\"\" width=\"834\" height=\"386\"></a></p> \n<h2>Create a SageMaker model</h2> \n<p>We now create a <a href=\"https://sagemaker.readthedocs.io/en/stable/api/inference/model.html\" target=\"_blank\" rel=\"noopener\">SageMaker model</a>. We use the <a href=\"http://aws.amazon.com/ecr/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Container Registry</a> (Amazon ECR) image provided by and the model artifact from the previous step to create the SageMaker model. In the model setup, we configure <code>TENSOR_PARALLEL_DEGREE=8</code>, which means the model is partitioned along 8 GPUs. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">PrimaryContainer={\n        \"Image\": inference_image_uri,\n        \"ModelDataUrl\": s3_code_artifact,\n        \"Environment\": {\n            \"MODEL_S3_BUCKET\": bucket,\n            \"MODEL_S3_PREFIX\": s3_model_prefix,\n            \"TENSOR_PARALLEL_DEGREE\": \"8\",\n},</code></pre> \n</div> \n<p>After you run the preceding cell in the Jupyter file, you see output similar to the following:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">{\n    \"ModelArn\": \"arn:aws:sagemaker:us-east-1:<span style=\"color: #ff0000\">&lt;account_id&gt;</span>:model/bloom-djl-ds-<span style=\"color: #ff0000\">&lt;date_time&gt;</span>\"\n}</code></pre> \n</div> \n<h2>Create a SageMaker endpoint</h2> \n<p>You can use any instances with multiple GPUs for testing. In this demo, we use a p4d.24xlarge instance. In the following code, note how we set the <code>ModelDataDownloadTimeoutInSeconds</code>, <code>ContainerStartupHealthCheckTimeoutInSeconds</code>, and <code>VolumeSizeInGB</code> parameters to accommodate the large model size. The <code>VolumeSizeInGB</code> parameter is applicable to GPU instances supporting the EBS volume attachment.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">endpoint_config_response = sm_client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            \"VariantName\": \"variant1\",\n            \"ModelName\": model_name,\n            \"InstanceType\": \"ml.p4d.24xlarge\",\n            \"InitialInstanceCount\": 1,\n            #\"VolumeSizeInGB\" : 200,\n            \"ModelDataDownloadTimeoutInSeconds\": 2400,\n            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n        },\n    ],\n)'</code></pre> \n</div> \n<p>Lastly, we create a SageMaker endpoint:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">create_endpoint_response = sm_client.create_endpoint(\n    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n)</code></pre> \n</div> \n<p>You see it printed out in the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">{\n    \"EndpointArn\": \"arn:aws:sagemaker:us-east-1:<span style=\"color: #ff0000\">&lt;aws-account-id&gt;</span>:endpoint/bloom-djl-ds-<span style=\"color: #ff0000\">&lt;date_time&gt;</span>\"\n}</code></pre> \n</div> \n<p>Starting the endpoint might take a while. You can try a few more times if you run into the <code>InsufficientInstanceCapacity</code> error, or you can raise a request to AWS to increase the limit in your account.</p> \n<h2>Performance tuning</h2> \n<p>If you intend to use this post and accompanying notebook with a different model, you may want to explore some of the tunable parameters that SageMaker, DeepSpeed, and the DJL offer. Iteratively experimenting with these parameters can have a material impact on the latency, throughput, and cost of your hosted large model. To learn more about tuning parameters such as number of workers, degree of tensor parallelism, job queue size, and others, refer to <a href=\"https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/configurations.md\" target=\"_blank\" rel=\"noopener\">DJL Serving configurations</a> and <a href=\"https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/\" target=\"_blank\" rel=\"noopener\">Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference</a>.</p> \n<h2>Results</h2> \n<p>In this post, we used DeepSpeed to host BLOOM-176B and OPT-30B on SageMaker ML instances. The following table summarizes our performance results, including a comparison with Hugging Face’s Accelerate. Latency reflects the number of milliseconds it takes to produce a 256-token string four times (<code>batch_size=4</code>) from the model. Throughput reflects the number of tokens produced per second for each test. For Hugging Face Accelerate, we used the library’s default loading with GPU memory mapping. For DeepSpeed, we used its faster checkpoint loading mechanism.</p> \n<table border=\"1px\"> \n <tbody> \n  <tr style=\"background-color: #000000\"> \n   <td><span style=\"color: #ffffff\">Model</span></td> \n   <td><span style=\"color: #ffffff\">Library</span></td> \n   <td><span style=\"color: #ffffff\">Model Precision</span></td> \n   <td><span style=\"color: #ffffff\">Batch Size</span></td> \n   <td><span style=\"color: #ffffff\">Parallel Degree</span></td> \n   <td><span style=\"color: #ffffff\">Instance</span></td> \n   <td><span style=\"color: #ffffff\">Time to Load</span><br> <span style=\"color: #ffffff\">(s)</span></td> \n   <td colspan=\"3\"><span style=\"color: #ffffff\">Latency (4 x 256 Token Output)</span></td> \n   <td>.</td> \n  </tr> \n  <tr style=\"background-color: #000000\"> \n   <td>.</td> \n   <td>.</td> \n   <td>.</td> \n   <td>.</td> \n   <td>.</td> \n   <td>.</td> \n   <td>.</td> \n   <td><span style=\"color: #ffffff\">P50</span><br> <span style=\"color: #ffffff\">(ms)</span></td> \n   <td><span style=\"color: #ffffff\">P90</span><br> <span style=\"color: #ffffff\">(ms)</span></td> \n   <td><span style=\"color: #ffffff\">P99</span><br> <span style=\"color: #ffffff\">(ms)</span></td> \n   <td><span style=\"color: #ffffff\">Throughput</span><br> <span style=\"color: #ffffff\">(tokens/sec)</span></td> \n  </tr> \n  <tr> \n   <td>BLOOM-176B</td> \n   <td>DeepSpeed</td> \n   <td>INT8</td> \n   <td>4</td> \n   <td>8</td> \n   <td>p4d.24xlarge</td> \n   <td>74.9</td> \n   <td>27,564</td> \n   <td>27,580</td> \n   <td>32,179</td> \n   <td>37.1</td> \n  </tr> \n  <tr> \n   <td>BLOOM-176B</td> \n   <td>Accelerate</td> \n   <td>INT8</td> \n   <td>4</td> \n   <td>8</td> \n   <td>p4d.24xlarge</td> \n   <td>669.4</td> \n   <td>92,694</td> \n   <td>92,735</td> \n   <td>103,292</td> \n   <td>11.0</td> \n  </tr> \n  <tr> \n   <td>OPT-30B</td> \n   <td>DeepSpeed</td> \n   <td>FP16</td> \n   <td>4</td> \n   <td>4</td> \n   <td>g5.24xlarge</td> \n   <td>239.4</td> \n   <td>11,299</td> \n   <td>11,302</td> \n   <td>11,576</td> \n   <td>90.6</td> \n  </tr> \n  <tr> \n   <td>OPT-30B</td> \n   <td>Accelerate</td> \n   <td>FP16</td> \n   <td>4</td> \n   <td>4</td> \n   <td>g5.24xlarge</td> \n   <td>533.8</td> \n   <td>63,734</td> \n   <td>63,737</td> \n   <td>67,605</td> \n   <td>16.1</td> \n  </tr> \n </tbody> \n</table> \n<p>From a latency perspective, DeepSpeed is about 3.4 times faster for BLOOM-176B and 5.6 times faster for OPT-30B than Accelerate. DeepSpeed’s optimized kernels are responsible for much of this difference in latency. Given these results, we recommend using DeepSpeed over Accelerate if your model of choice is supported.</p> \n<p>It’s also worth noting that model loading times with DeepSpeed were much shorter, making it a better option if you anticipate needing to quickly scale up your number of endpoints. Accelerate’s more flexible pipeline parallelism technique may be a better option if you have models or model precisions that aren’t supported by DeepSpeed.</p> \n<p>These results also demonstrate the difference in latency and throughput of different model sizes. In our tests, OPT-30B generates 2.4 times the number of tokens per unit time than BLOOM-176B on an instance type that is more than three times cheaper. On a price per unit throughput basis, OPT-30B on a g5.24xl instance is 8.9 times better than BLOOM-176B on a p4d.24xl instance. If you have strict latency, throughput, or cost limitations, consider using the smallest model possible that will still achieve functional requirements.</p> \n<h2>Clean up</h2> \n<p>As part of best practices it is always recommended to delete idle instances. The below code shows you how to delete the instances.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\"># - Delete the end point\nsm_client.delete_endpoint(EndpointName=endpoint_name)\n\n# - In case the end point failed we still want to delete the model\nsm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\nsm_client.delete_model(ModelName=model_name)</code></pre> \n</div> \n<p>Optionally delete the model check point from your S3</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">!aws s3 rm --recursive s3://&lt;your_bucket&gt;/{s3_model_prefix}</code></pre> \n</div> \n<h2>Conclusion</h2> \n<p>In this post, we demonstrated how to use SageMaker large model inference containers to host two large language models, BLOOM-176B and OPT-30B. We used DeepSpeed’s model parallel techniques with multiple GPUs on a single SageMaker ML instance.</p> \n<p>For more details about Amazon SageMaker and its large model inference capabilities, refer to <a href=\"https://aws.amazon.com/about-aws/whats-new/2022/09/amazon-sagemaker-deploying-large-models-volume-size-timeout-quotas/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker now supports deploying large models through configurable volume size and timeout quotas</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html\" target=\"_blank\" rel=\"noopener\">Real-time inference</a>.</p> \n<hr> \n<h3>About the authors</h3> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/simon-zamarin.jpg\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"wp-image-44796 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/simon-zamarin.jpg\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Simon Zamarin</strong> is an AI/ML Solutions Architect whose main focus is helping customers extract value from their data assets. In his spare time, Simon enjoys spending time with family, reading sci-fi, and working on various DIY house projects.</p> \n<p style=\"clear: both\"><strong><img loading=\"lazy\" class=\"alignleft wp-image-42978\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/09/21/Rupinder-Grewal-.jpg\" alt=\"\" width=\"100\" height=\"89\"> Rupinder Grewal</strong>&nbsp;is a Sr Ai/ML Specialist Solutions Architect with AWS. He currently focuses on serving of models and MLOps on SageMaker. Prior to this role he has worked as Machine Learning Engineer building and hosting models. Outside of work he enjoys playing tennis and biking on mountain trails.</p> \n<p style=\"clear: both\"><strong><img loading=\"lazy\" class=\"size-full wp-image-16803 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/10/06/frank-liu-100.jpg\" alt=\"\" width=\"100\" height=\"134\">Frank Liu </strong>is a Software Engineer for AWS Deep Learning. He focuses on building innovative deep learning tools for software engineers and scientists. In his spare time, he enjoys hiking with friends and family.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/Alan-Tan.png\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"size-full wp-image-41417 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/Alan-Tan.png\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Alan Tan </strong>is a Senior Product Manager with SageMaker leading efforts on large model inference. He’s passionate about applying Machine Learning to the area of Analytics. Outside of work, he enjoys the outdoors.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/04/14/Dhawal.png\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"size-full wp-image-35169 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/04/14/Dhawal.png\" alt=\"\" width=\"100\" height=\"97\"></a>Dhawal Patel</strong> is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing, and Artificial Intelligence. He focuses on Deep learning including NLP and Computer Vision domains. He helps customers achieve high performance model inference on SageMaker.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/qingla.png\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"size-full wp-image-41416 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/qingla.png\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Qing Lan </strong>is a Software Development Engineer in AWS. He has been working on several challenging products in Amazon, including high performance ML inference solutions and high performance logging system. Qing’s team successfully launched the first Billion-parameter model in Amazon Advertising with very low latency required. Qing has in-depth knowledge on the infrastructure optimization and Deep Learning acceleration.</p> \n<p style=\"clear: both\"><strong><img loading=\"lazy\" class=\"size-full wp-image-19110 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/11/28/Qingwei-Li.jpg\" alt=\"\" width=\"100\" height=\"133\">Qingwei Li</strong>&nbsp;is a Machine Learning Specialist at Amazon Web Services. He received his Ph.D. in Operations Research after he broke his advisor’s research grant account and failed to deliver the Nobel Prize he promised. Currently he helps customers in the financial service and insurance industry build machine learning solutions on AWS. In his spare time, he likes reading and teaching.</p> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/Robert-Van-Dusen.png\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"size-full wp-image-41418 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/08/22/Robert-Van-Dusen.png\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Robert Van Dusen</strong> is a Senior Product Manager with Amazon SageMaker. He leads deep learning model optimization for applications such as large model inference.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/sidharth.png\" target=\"_blank\" rel=\"noopener\"><img loading=\"lazy\" class=\"size-full wp-image-45359 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/11/03/sidharth.png\" alt=\"\" width=\"100\" height=\"100\"></a>Siddharth Venkatesan</strong> is a Software Engineer in AWS Deep Learning. He currently focusses on building solutions for large model inference. Prior to AWS he worked in the Amazon Grocery org building new payment features for customers world-wide. Outside of work, he enjoys skiing, the outdoors, and watching sports.</p>"
}