{
  "title": "Dask and Scikit-Learn -- Model Parallelism",
  "link": "",
  "updated": "2016-07-12T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/07/12/dask-learn-part-1",
  "content": "<p><em>This post was written by Jim Crist.  The original post lives at\n<a href=\"http://jcrist.github.io/dask-sklearn-part-1.html\">http://jcrist.github.io/dask-sklearn-part-1.html</a>\n(with better styling)</em></p>\n\n<p>This is the first of a series of posts discussing some recent experiments\ncombining <a href=\"http://dask.pydata.org/en/latest/\">dask</a> and\n<a href=\"http://scikit-learn.org/stable/\">scikit-learn</a>. A small (and extremely alpha)\nlibrary has been built up from these experiments, and can be found\n<a href=\"https://github.com/jcrist/dask-learn\">here</a>.</p>\n\n<p>Before we start, I would like to make the following caveats:</p>\n\n<ul>\n  <li>I am not a machine learning expert. Do not consider this a guide on how to do\nmachine learning, the usage of scikit-learn below is probably naive.</li>\n  <li>All of the code discussed here is in flux, and shouldn’t be considered stable\nor robust. That said, if you know something about machine learning and want\nto help out, I’d be more than happy to receive issues or pull requests :).</li>\n</ul>\n\n<p>There are several ways of parallelizing algorithms in machine learning. Some\nalgorithms can be made to be data-parallel (either across features or across\nsamples). In this post we’ll look instead at model-parallelism (use same data\nacross different models), and dive into a daskified implementation of\n<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\">GridSearchCV</a>.</p>\n\n<h2 id=\"what-is-grid-search\">What is grid search?</h2>\n\n<p>Many machine learning algorithms have <em>hyperparameters</em> which can be tuned to\nimprove the performance of the resulting estimator. A <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search\">grid\nsearch</a>\nis one way of optimizing these parameters — it works by doing a parameter\nsweep across a cartesian product of a subset of these parameters (the “grid”),\nand then choosing the best resulting estimator. Since this is fitting many\nindependent estimators across the same set of data, it can be fairly easily\nparallelized.</p>\n\n<h2 id=\"grid-search-with-scikit-learn\">Grid search with scikit-learn</h2>\n\n<p>In scikit-learn, a grid search is performed using the <code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code> class, and\ncan (optionally) be automatically parallelized using\n<a href=\"https://pythonhosted.org/joblib/index.html\">joblib</a>.</p>\n\n<p>This is best illustrated with an example. First we’ll make an example dataset\nfor doing classification against:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_classification</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_classification</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">10000</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                           <span class=\"n\">n_redundant</span><span class=\"o\">=</span><span class=\"mi\">250</span><span class=\"p\">,</span>\n                           <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>To solve this classification problem, we’ll create a pipeline of a <code class=\"language-plaintext highlighter-rouge\">PCA</code> and a\n<code class=\"language-plaintext highlighter-rouge\">LogisticRegression</code>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">linear_model</span><span class=\"p\">,</span> <span class=\"n\">decomposition</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.pipeline</span> <span class=\"kn\">import</span> <span class=\"n\">Pipeline</span>\n\n<span class=\"n\">logistic</span> <span class=\"o\">=</span> <span class=\"n\">linear_model</span><span class=\"p\">.</span><span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>\n<span class=\"n\">pca</span> <span class=\"o\">=</span> <span class=\"n\">decomposition</span><span class=\"p\">.</span><span class=\"n\">PCA</span><span class=\"p\">()</span>\n<span class=\"n\">pipe</span> <span class=\"o\">=</span> <span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s\">'pca'</span><span class=\"p\">,</span> <span class=\"n\">pca</span><span class=\"p\">),</span>\n                       <span class=\"p\">(</span><span class=\"s\">'logistic'</span><span class=\"p\">,</span> <span class=\"n\">logistic</span><span class=\"p\">)])</span>\n</code></pre></div></div>\n\n<p>Both of these classes take several hyperparameters, we’ll do a grid-search\nacross only a few of them:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">#Parameters of pipelines can be set using ‘__’ separated parameter names:\n</span><span class=\"n\">grid</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">pca__n_components</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">250</span><span class=\"p\">],</span>\n            <span class=\"n\">logistic__C</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">1e4</span><span class=\"p\">],</span>\n            <span class=\"n\">logistic__penalty</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'l1'</span><span class=\"p\">,</span> <span class=\"s\">'l2'</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>Finally, we can create an instance of <code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code>, and perform the grid\nsearch. The parameter <code class=\"language-plaintext highlighter-rouge\">n_jobs=-1</code> tells joblib to use as many processes as I\nhave cores (8).</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">estimator</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipe</span><span class=\"p\">,</span> <span class=\"n\">grid</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">5.3</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">243</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">5.54</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">21.6</span> <span class=\"n\">s</span>\n</code></pre></div></div>\n\n<p>What happened here was:</p>\n\n<ul>\n  <li>An estimator was created for each parameter combination and test-train set\n(scikit-learn’s grid search also does cross validation across 3-folds by\ndefault).</li>\n  <li>Each estimator was fit on its corresponding set of training data</li>\n  <li>Each estimator was then scored on its corresponding set of testing data</li>\n  <li>The best set of parameters was chosen based on these scores</li>\n  <li>A new estimator was then fit on <em>all</em> of the data, using the best parameters</li>\n</ul>\n\n<p>The corresponding best score, parameters, and estimator can all be found as\nattributes on the resulting object:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_score_</span>\n<span class=\"mf\">0.89290000000000003</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_params_</span>\n<span class=\"p\">{</span><span class=\"s\">'logistic__C'</span><span class=\"p\">:</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"s\">'logistic__penalty'</span><span class=\"p\">:</span> <span class=\"s\">'l2'</span><span class=\"p\">,</span> <span class=\"s\">'pca__n_components'</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">}</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_estimator_</span>\n<span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s\">'pca'</span><span class=\"p\">,</span> <span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"n\">copy</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">whiten</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"s\">'logistic'</span><span class=\"p\">,</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">dual</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n        <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">intercept_scaling</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n        <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"s\">'ovr'</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s\">'l2'</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"s\">'liblinear'</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">warm_start</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">))])</span><span class=\"o\">&lt;</span><span class=\"n\">div</span> <span class=\"n\">class</span><span class=\"o\">=</span><span class=\"n\">md_output</span><span class=\"o\">&gt;</span>\n\n    <span class=\"p\">{</span><span class=\"s\">'logistic__C'</span><span class=\"p\">:</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"s\">'logistic__penalty'</span><span class=\"p\">:</span> <span class=\"s\">'l2'</span><span class=\"p\">,</span> <span class=\"s\">'pca__n_components'</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"grid-search-with-dask-learn\">Grid search with dask-learn</h2>\n\n<p>Here we’ll repeat the same fit using dask-learn. I’ve tried to match the\nscikit-learn interface as much as possible, although not everything is\nimplemented. Here the only thing that really changes is the <code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code>\nimport. We don’t need the <code class=\"language-plaintext highlighter-rouge\">n_jobs</code> keyword, as this will be parallelized across\nall cores by default.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">dklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span> <span class=\"k\">as</span> <span class=\"n\">DaskGridSearchCV</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">destimator</span> <span class=\"o\">=</span> <span class=\"n\">DaskGridSearchCV</span><span class=\"p\">(</span><span class=\"n\">pipe</span><span class=\"p\">,</span> <span class=\"n\">grid</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">16.3</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">1.89</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">18.2</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">5.63</span> <span class=\"n\">s</span>\n</code></pre></div></div>\n\n<p>As before, the best score, parameters, and estimator can all be found as\nattributes on the object. Here we’ll just show that they’re equivalent:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">best_score_</span> <span class=\"o\">==</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_score_</span>\n<span class=\"bp\">True</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">best_params_</span> <span class=\"o\">==</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_params_</span>\n<span class=\"bp\">True</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">best_estimator_</span>\n<span class=\"n\">Pipeline</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"o\">=</span><span class=\"p\">[(</span><span class=\"s\">'pca'</span><span class=\"p\">,</span> <span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"n\">copy</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">whiten</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"s\">'logistic'</span><span class=\"p\">,</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"n\">class_weight</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">dual</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n        <span class=\"n\">fit_intercept</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">intercept_scaling</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n        <span class=\"n\">multi_class</span><span class=\"o\">=</span><span class=\"s\">'ovr'</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"s\">'l2'</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">solver</span><span class=\"o\">=</span><span class=\"s\">'liblinear'</span><span class=\"p\">,</span> <span class=\"n\">tol</span><span class=\"o\">=</span><span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">warm_start</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">))])</span><span class=\"o\">&lt;</span><span class=\"n\">div</span> <span class=\"n\">class</span><span class=\"o\">=</span><span class=\"n\">md_output</span><span class=\"o\">&gt;</span>\n\n    <span class=\"p\">{</span><span class=\"s\">'logistic__C'</span><span class=\"p\">:</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span> <span class=\"s\">'logistic__penalty'</span><span class=\"p\">:</span> <span class=\"s\">'l2'</span><span class=\"p\">,</span> <span class=\"s\">'pca__n_components'</span><span class=\"p\">:</span> <span class=\"mi\">50</span><span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"why-is-the-dask-version-faster\">Why is the dask version faster?</h2>\n\n<p>If you look at the times above, you’ll note that the dask version was <code class=\"language-plaintext highlighter-rouge\">~4X</code>\nfaster than the scikit-learn version. This is not because we have optimized any\nof the pieces of the <code class=\"language-plaintext highlighter-rouge\">Pipeline</code>, or that there’s a significant amount of\noverhead to <code class=\"language-plaintext highlighter-rouge\">joblib</code> (on the contrary, joblib does some pretty amazing things,\nand I had to construct a contrived example to beat it this badly). The reason\nis simply that the dask version is doing less work.</p>\n\n<p>This maybe best explained in pseudocode. The scikit-learn version of the above\n(in serial) looks something like (pseudocode):</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"ow\">in</span> <span class=\"n\">cv</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'pca__n_components'</span><span class=\"p\">]:</span>\n        <span class=\"k\">for</span> <span class=\"n\">C</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'logistic__C'</span><span class=\"p\">]:</span>\n            <span class=\"k\">for</span> <span class=\"n\">penalty</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'logistic__penalty'</span><span class=\"p\">]:</span>\n                <span class=\"c1\"># Create and fit a PCA on the input data\n</span>                <span class=\"n\">pca</span> <span class=\"o\">=</span> <span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"n\">n</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Transform both the train and test data\n</span>                <span class=\"n\">X_train2</span> <span class=\"o\">=</span> <span class=\"n\">pca</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n                <span class=\"n\">X_test2</span> <span class=\"o\">=</span> <span class=\"n\">pca</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Create and fit a LogisticRegression on the transformed data\n</span>                <span class=\"n\">logistic</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"n\">penalty</span><span class=\"p\">)</span>\n                <span class=\"n\">logistic</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train2</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Score the total pipeline\n</span>                <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">logistic</span><span class=\"p\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test2</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Save the score and parameters\n</span>                <span class=\"n\">scores_and_params</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">score</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Find the best set of parameters (for some definition of best)\n</span><span class=\"n\">find_best_parameters</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This is looping through a cartesian product of the cross-validation sets and\nall the parameter combinations, and then creating and fitting a new estimator\nfor each combination. While embarassingly parallel, this can also result in\nrepeated work, as earlier stages in the pipeline are refit multiple times on\nthe same parameter + data combinations.</p>\n\n<p>In contrast, the dask version hashes all inputs (forming a sort of <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle\nDAG</a>), resulting in the intermediate\nresults being shared. Keeping with the pseudocode above, the dask version might\nlook like:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"ow\">in</span> <span class=\"n\">cv</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'pca__n_components'</span><span class=\"p\">]:</span>\n        <span class=\"c1\"># Create and fit a PCA on the input data\n</span>        <span class=\"n\">pca</span> <span class=\"o\">=</span> <span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"n\">n</span><span class=\"p\">).</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n        <span class=\"c1\"># Transform both the train and test data\n</span>        <span class=\"n\">X_train2</span> <span class=\"o\">=</span> <span class=\"n\">pca</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n        <span class=\"n\">X_test2</span> <span class=\"o\">=</span> <span class=\"n\">pca</span><span class=\"p\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">C</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'logistic__C'</span><span class=\"p\">]:</span>\n            <span class=\"k\">for</span> <span class=\"n\">penalty</span> <span class=\"ow\">in</span> <span class=\"n\">grid</span><span class=\"p\">[</span><span class=\"s\">'logistic__penalty'</span><span class=\"p\">]:</span>\n                <span class=\"c1\"># Create and fit a LogisticRegression on the transformed data\n</span>                <span class=\"n\">logistic</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"o\">=</span><span class=\"n\">penalty</span><span class=\"p\">)</span>\n                <span class=\"n\">logistic</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train2</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Score the total pipeline\n</span>                <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">logistic</span><span class=\"p\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test2</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n                <span class=\"c1\"># Save the score and parameters\n</span>                <span class=\"n\">scores_and_params</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">((</span><span class=\"n\">score</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">penalty</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Find the best set of parameters (for some definition of best)\n</span><span class=\"n\">find_best_parameters</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This can still be parallelized, but in a less straightforward manner - the\ngraph is a bit more complicated than just a simple map-reduce pattern.\nThankfully the <a href=\"http://dask.pydata.org/en/latest/scheduler-overview.html\">dask\nschedulers</a> are well\nequipped to handle arbitrary graph topologies. Below is a GIF showing how the\ndask scheduler (the threaded scheduler specifically) executed the grid search\nperformed above. Each rectangle represents data, and each circle represents a\ntask. Each is categorized by color:</p>\n\n<ul>\n  <li>\n    <p>Red means actively taking up resources. These are tasks executing in a thread,\nor intermediate results occupying memory</p>\n  </li>\n  <li>\n    <p>Blue means finished or released. These are already finished tasks, or data\nthat’s been released from memory because it’s no longer needed</p>\n  </li>\n</ul>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/grid_search_schedule.gif\" alt=\"Dask Graph Execution\" style=\"width:100%\" /></p>\n\n<p>Looking at the trace, a few things stand out:</p>\n\n<ul>\n  <li>\n    <p>We do a good job sharing intermediates. Each step in a pipeline is only fit\nonce given the same parameters/data, resulting in some intermediates having\nmany dependent tasks.</p>\n  </li>\n  <li>\n    <p>The scheduler does a decent job of quickly finishing up tasks required to\nrelease data. This doesn’t matter as much here (none of the intermediates\ntake up much memory), but for other workloads this is very useful. See Matt\nRocklin’s <a href=\"http://matthewrocklin.com/blog/work/2015/01/06/Towards-OOC-Scheduling\">excellent blogpost\nhere</a>\nfor more discussion on this.</p>\n  </li>\n</ul>\n\n<h2 id=\"distributed-grid-search-using-dask-learn\">Distributed grid search using dask-learn</h2>\n\n<p>The <a href=\"http://dask.pydata.org/en/latest/scheduler-overview.html\">schedulers</a> used\nin dask are configurable. The default (used above) is the threaded scheduler,\nbut we can just as easily swap it out for the distributed scheduler. Here I’ve\njust spun up two local workers to demonstrate, but this works equally well\nacross multiple machines.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Executor</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"c1\"># Create an Executor, and set it as the default scheduler\n</span><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">exc</span> <span class=\"o\">=</span> <span class=\"n\">Executor</span><span class=\"p\">(</span><span class=\"s\">'10.0.0.3:8786'</span><span class=\"p\">,</span> <span class=\"n\">set_as_default</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">exc</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Executor</span><span class=\"p\">:</span> <span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">\"10.0.0.3:8786\"</span> <span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"mi\">2</span> <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"o\">&gt;</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">1.69</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">433</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">2.12</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">7.66</span> <span class=\"n\">s</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">1.69</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">433</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">2.12</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">7.66</span> <span class=\"n\">s</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"p\">(</span><span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">best_score_</span> <span class=\"o\">==</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_score_</span> <span class=\"ow\">and</span>\n<span class=\"p\">...</span>  <span class=\"n\">destimator</span><span class=\"p\">.</span><span class=\"n\">best_params_</span> <span class=\"o\">==</span> <span class=\"n\">estimator</span><span class=\"p\">.</span><span class=\"n\">best_params_</span><span class=\"p\">)</span>\n<span class=\"bp\">True</span>\n</code></pre></div></div>\n\n<p>Note that this is slightly slower than the threaded execution, so it doesn’t\nmake sense for this workload, but for others it might.</p>\n\n<h2 id=\"what-worked-well\">What worked well</h2>\n\n<ul>\n  <li>\n    <p>The <a href=\"https://github.com/jcrist/dask-learn/blob/master/dklearn/grid_search.py\">code for doing\nthis</a>\nis quite short.  There’s also an implementation of\n<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html\"><code class=\"language-plaintext highlighter-rouge\">RandomizedSearchCV</code></a>,\nwhich is only a few extra lines (hooray for good class hierarchies!).\nInstead of working with dask graphs directly, both implementations use\n<a href=\"http://dask.pydata.org/en/latest/delayed.html\">dask.delayed</a> wherever\npossible, which also makes the code easy to read.</p>\n  </li>\n  <li>\n    <p>Due to the internal hashing used in dask (which is extensible!), duplicate\ncomputations are avoided.</p>\n  </li>\n  <li>\n    <p>Since the graphs are separated from the scheduler, this works both locally\nand distributed with only a few extra lines.</p>\n  </li>\n</ul>\n\n<h2 id=\"caveats-and-what-could-be-better\">Caveats and what could be better</h2>\n\n<ul>\n  <li>\n    <p>The scikit-learn api makes use of mutation (<code class=\"language-plaintext highlighter-rouge\">est.fit(X, y)</code> mutates <code class=\"language-plaintext highlighter-rouge\">est</code>),\nwhile dask collections are mostly immutable. After playing around with a few\ndifferent ideas, I settled on dask-learn estimators being immutable (except\nfor grid-search, more on this in a bit). This made the code easier to reason\nabout, but does mean that you need to do <code class=\"language-plaintext highlighter-rouge\">est = est.fit(X, y)</code> when working\nwith dask-learn estimators.</p>\n  </li>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code> posed a different problem. Due to the <code class=\"language-plaintext highlighter-rouge\">refit</code> keyword, the\nimplementation can’t be done in a single pass over the data. This means that\nwe can’t build a single graph describing both the grid search and the refit,\nwhich prevents it from being done lazily. I debated removing this keyword,\nbut decided in the end to make <code class=\"language-plaintext highlighter-rouge\">fit</code> execute immediately. This means that\nthere’s a bit of a disconnect between <code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code> and the other classes in\nthe library, which I don’t like. On the other hand, it does mean that this\nversion of <code class=\"language-plaintext highlighter-rouge\">GridSearchCV</code> could be a drop-in for the sckit-learn one.</p>\n  </li>\n  <li>\n    <p>The approach presented here is nice, but is really <em>only beneficial when\nthere’s duplicate work to be avoided, and that duplicate work is expensive</em>.\nRepeating the above with only a single estimator (instead of a pipeline)\nresults in identical (or slightly worse) performance than joblib. Similarly,\nif the repeated steps are cheap the difference in performance is much smaller\n(try the above using\n<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\">SelectKBest</a>\ninstead of <code class=\"language-plaintext highlighter-rouge\">PCA</code>).</p>\n  </li>\n  <li>\n    <p>The ability to swap easily from local to distributed execution is nice, but\n<a href=\"http://distributed.readthedocs.io/en/latest/joblib.html\">distributed also contains a joblib\nfrontend</a> that can\ndo this just as easily.</p>\n  </li>\n</ul>\n\n<h2 id=\"help\">Help</h2>\n\n<p>I am not a machine learning expert. Is any of this useful? Do you have\nsuggestions for improvements (or better yet PRs for improvements :))? Please\nfeel free to reach out in the comments below, or <a href=\"https://github.com/jcrist/dask-learn\">on\ngithub</a>.</p>\n\n<p><em>This work is supported by <a href=\"http://continuum.io/\">Continuum Analytics</a> and the\n<a href=\"http://www.darpa.mil/program/XDATA\">XDATA</a> program as part of the <a href=\"http://blaze.pydata.org/\">Blaze\nProject</a>.</em></p>"
}