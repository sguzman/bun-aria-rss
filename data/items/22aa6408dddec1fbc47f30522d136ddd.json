{
  "title": "Bayesian Statistics: MCMC",
  "link": "",
  "published": "2016-08-07T18:37:00-07:00",
  "updated": "2016-08-07T18:37:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2016-08-07:/metropolis",
  "summary": "<p>We review the Metropolis algorithm &#8212; a simple Markov Chain Monte Carlo (<span class=\"caps\">MCMC</span>) sampling method &#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&nbsp;provided.</p>\n<h2>Introduction</h2>\n<p>One of the central aims of statistics is to identify good methods for fitting models to data. One way to â€¦</p>",
  "content": "<p>We review the Metropolis algorithm &#8212; a simple Markov Chain Monte Carlo (<span class=\"caps\">MCMC</span>) sampling method &#8212; and its application to estimating posteriors in Bayesian statistics. A simple python example is&nbsp;provided.</p>\n<h2>Introduction</h2>\n<p>One of the central aims of statistics is to identify good methods for fitting models to data. One way to do this is through the use of Bayes&#8217; rule: If <span class=\"math\">\\(\\textbf{x}\\)</span> is a vector of <span class=\"math\">\\(k\\)</span> samples from a distribution and <span class=\"math\">\\(\\textbf{z}\\)</span> is a vector of model parameters, Bayes&#8217; rule&nbsp;gives\n</p>\n<div class=\"math\">\\begin{align} \\tag{1} \\label{Bayes}\np(\\textbf{z} \\vert \\textbf{x}) = \\frac{p(\\textbf{x} \\vert \\textbf{z}) p(\\textbf{z})}{p(\\textbf{x})}.\n\\end{align}</div>\n<p>\nHere, the probability at left, <span class=\"math\">\\(p(\\textbf{z} \\vert \\textbf{x})\\)</span> &#8212; the &#8220;posterior&#8221; &#8212; is a function that tells us how likely it is that the underlying true parameter values are <span class=\"math\">\\(\\textbf{z}\\)</span>, given the information provided by our observations <span class=\"math\">\\(\\textbf{x}\\)</span>. Notice that if we could solve for this function, we would be able to identify which parameter values are most likely &#8212; those that are good candidates for a fit. We could also use the posterior&#8217;s variance to quantify how uncertain we are about the true, underlying parameter&nbsp;values.</p>\n<p>Bayes&#8217; rule gives us a method for evaluating the posterior &#8212; now our goal: We need only evaluate the right side of (\\ref{Bayes}). The quantities shown there&nbsp;are</p>\n<p><span class=\"math\">\\(p(\\textbf{x} \\vert \\textbf{z})\\)</span> &#8212; This is the probability of seeing <span class=\"math\">\\(\\textbf{x}\\)</span> at fixed parameter values <span class=\"math\">\\(\\textbf{z}\\)</span>. Note that if the model is specified, we can often immediately write this part down. For example, if we have a Normal distribution model, specifying <span class=\"math\">\\(\\textbf{z}\\)</span> means that we have specified the Normal&#8217;s mean and variance. Given these, we can say how likely it is to observe any <span class=\"math\">\\(\\textbf{x}\\)</span>.</p>\n<p><span class=\"math\">\\(p(\\textbf{z})\\)</span> &#8212; the &#8220;prior&#8221;. This is something we insert by hand before taking any data. We choose its form so that it covers the values we expect are reasonable for the parameters in&nbsp;question.</p>\n<p><span class=\"math\">\\(p(\\textbf{x})\\)</span> &#8212; the denominator. Notice that this doesn&#8217;t depend on <span class=\"math\">\\(\\textbf{z}\\)</span>, and so represents a normalization constant for the&nbsp;posterior.</p>\n<p>It turns out that the last term above can sometimes be difficult to evaluate analytically, and so we must often resort to numerical methods for estimating the posterior. Monte Carlo sampling is one of the most common approaches taken for doing this. The idea behind Monte Carlo is to take many samples <span class=\"math\">\\(\\{\\textbf{z}_i\\}\\)</span> from the posterior (\\ref{Bayes}). Once these are obtained, we can approximate population averages by averages over the samples. For example, the true posterior average <span class=\"math\">\\(\\langle\\textbf{z} \\rangle \\equiv \\int \\textbf{z} p(\\textbf{z} \\vert \\textbf{x}) d \\textbf{z}\\)</span> can be approximated by <span class=\"math\">\\(\\overline{\\textbf{z}} \\equiv \\frac{1}{N}\\sum_i \\textbf{z}_i\\)</span>, the sample average. By the law of large numbers, the sample averages are guaranteed to approach the distribution averages as <span class=\"math\">\\(N \\to \\infty\\)</span>. This means that Monte Carlo can always be used to obtain very accurate parameter estimates, provided we take <span class=\"math\">\\(N\\)</span> sufficiently large &#8212; and that we can find a convenient way to sample from the posterior. In this post, we review one simple variant of Monte Carlo that allows for posterior sampling: the Metropolis&nbsp;algorithm.</p>\n<h2>Metropolis&nbsp;Algorithm</h2>\n<h3>Iterative&nbsp;Procedure</h3>\n<p>Metropolis is an iterative, try-accept algorithm. We initialize the algorithm by selecting a parameter vector <span class=\"math\">\\(\\textbf{z}\\)</span> at random. Following this, we repeatedly carry out the following two steps to obtain additional posterior&nbsp;samples:</p>\n<ol>\n<li>Identify a next candidate sample <span class=\"math\">\\(\\textbf{z}_j\\)</span> via some random process. This candidate selection step can be informed by the current sample&#8217;s position, <span class=\"math\">\\(\\textbf{z}_i\\)</span>. For example, one could require that the next candidate be selected from those parameter vectors a given step-size distance from the current sample, <span class=\"math\">\\(\\textbf{z}_j \\in \\{\\textbf{z}_k: \\vert \\textbf{z}_i - \\textbf{z}_k \\vert = \\delta \\}\\)</span>. However, while the candidate selected can depend on the current sample, it must not depend on any prior history of the sampling process. Whatever the process chosen (there&#8217;s some flexibility here), we write <span class=\"math\">\\(t_{i,j}\\)</span> for the rate of selecting <span class=\"math\">\\(\\textbf{z}_j\\)</span> as the next candidate given the current sample is <span class=\"math\">\\(\\textbf{z}_i\\)</span>.</li>\n<li>Once a candidate is identified, we either accept or reject it via a second random process. If it is accepted, we mark it down as the next sample, then go back to step one, using the current sample to inform the next candidate selection. Otherwise, we mark the current sample down again, taking it as a repeat sample, and then use it to return to candidate search step, as above. Here, we write <span class=\"math\">\\(A_{i,j}\\)</span> for the rate of accepting <span class=\"math\">\\(\\textbf{z}_j\\)</span>, given that it was selected as the next candidate, starting from <span class=\"math\">\\(\\textbf{z}_i\\)</span>.</li>\n</ol>\n<h3>Selecting the trial and acceptance&nbsp;rates</h3>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg\"><img alt=\"Untitled-1\" src=\"https://efavdb.com/wp-content/uploads/2016/08/Untitled-1.jpg\"></a></p>\n<p>In order to ensure that our above process selects samples according to the distribution (\\ref{Bayes}), we need to appropriately set the <span class=\"math\">\\(\\{t_{i,j}\\}\\)</span> and <span class=\"math\">\\(\\{A_{i,j}\\}\\)</span> values. To do that, note that at equilibrium one must see the same number of hops from <span class=\"math\">\\(\\textbf{z}_i\\)</span> to <span class=\"math\">\\(\\textbf{z}_j\\)</span> as hops from <span class=\"math\">\\(\\textbf{z}_j\\)</span> from <span class=\"math\">\\(\\textbf{z}_i\\)</span> (if this did not hold, one would see a net shifting of weight from one to the other over time, contradicting the assumption of equilibrium). If <span class=\"math\">\\(\\rho_i\\)</span> is the fraction of samples the process takes from state <span class=\"math\">\\(i\\)</span>, this condition can be written&nbsp;as\n</p>\n<div class=\"math\">\\begin{align} \\label{inter}\n\\rho_i t_{i,j} A_{i,j} = \\rho_j t_{j,i} A_{j,i} \\tag{3}\n\\end{align}</div>\n<p>\nTo select a process that returns the desired sampling weight, we solve for <span class=\"math\">\\(\\rho_i\\)</span> over <span class=\"math\">\\(\\rho_j\\)</span> in (\\ref{inter}) and then equate this to the ratio required by (\\ref{Bayes}). This&nbsp;gives\n</p>\n<div class=\"math\">\\begin{align} \\tag{4} \\label{cond}\n\\frac{\\rho_i}{\\rho_j} = \\frac{t_{j,i} A_{j,i}}{t_{i,j} A_{i,j}}\n\\equiv \\frac{p(\\textbf{x} \\vert \\textbf{z}_i)p(\\textbf{z}_i)}{p(\\textbf{x} \\vert \\textbf{z}_j)p(\\textbf{z}_j)}.\n\\end{align}</div>\n<p>\nNow, the single constraint above is not sufficient to pin down all of our degrees of freedom. In the Metropolis case, we choose the following working balance: The trial rates between states are set equal, <span class=\"math\">\\(t_{i,j} = t_{j,i}\\)</span> (but remain unspecified &#8212; left to the discretion of the coder on a case-by-case basis), and we&nbsp;set\n</p>\n<div class=\"math\">$$ \\tag{5}\nA_{i,j} = \\begin{cases}\n1, &amp; \\text{if } p(\\textbf{z}_j \\vert \\textbf{x}) &gt; p(\\textbf{z}_i \\vert \\textbf{x}) \\\\\n\\frac{p(\\textbf{x} \\vert \\textbf{z}_j)p(\\textbf{z}_j)}{p(\\textbf{x} \\vert \\textbf{z}_i)p(\\textbf{z}_i)} \\equiv \\frac{p(\\textbf{z}_j \\vert \\textbf{x})}{p(\\textbf{z}_i \\vert \\textbf{x})}, &amp; \\text{else}.\n\\end{cases}\n$$</div>\n<p>\nThis last equation says that we choose to always accept a candidate sample if it is more likely than the current one. However, if the candidate is less likely, we only accept a fraction of the time &#8212; with rate equal to the relative probability ratio of the two states. For example, if the candidate is only <span class=\"math\">\\(80%\\)</span> as likely as the current sample, we accept it <span class=\"math\">\\(80%\\)</span> of the time. That&#8217;s it for Metropolis &#8212; a simple <span class=\"caps\">MCMC</span> algorithm, guaranteed to satisfy (\\ref{cond}), and to therefore equilibrate to (\\ref{Bayes})! An example&nbsp;follows.</p>\n<h3>Coding&nbsp;example</h3>\n<p>The following python snippet illustrates the Metropolis algorithm in action. Here, we take 15 samples from a Normal distribution of variance one and true mean also equal to one. We pretend not to know the mean (but assume we do know the variance), assume a uniform prior for the mean, and then run the algorithm to obtain two hundred thousand samples from the mean&#8217;s posterior. <a href=\"https://efavdb.com/wp-content/uploads/2016/08/result-1.png\"><img alt=\"result\" src=\"https://efavdb.com/wp-content/uploads/2016/08/result-1.png\"></a> The histogram at right summarizes the results, obtained by dropping the first 1% of the samples (to protect against bias towards the initialization value). Averaging over the samples returns a mean estimate of <span class=\"math\">\\(\\mu \\approx 1.4 \\pm 0.5\\)</span> (95% confidence interval), consistent with the true value of <span class=\"math\">\\(1\\)</span>.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">%</span><span class=\"n\">matplotlib</span> <span class=\"n\">inline</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"c1\"># Take some samples</span>\n<span class=\"n\">true_mean</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"n\">true_mean</span><span class=\"p\">,</span> <span class=\"kp\">size</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">)</span>\n<span class=\"n\">total_samples</span> <span class=\"o\">=</span> <span class=\"mi\">200000</span>\n\n<span class=\"c1\"># Function used to decide move acceptance</span>\n<span class=\"k\">def</span> <span class=\"nf\">posterior_numerator</span><span class=\"p\">(</span><span class=\"n\">mu</span><span class=\"p\">):</span>\n    <span class=\"kp\">prod</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">X</span><span class=\"p\">:</span>\n        <span class=\"kp\">prod</span> <span class=\"o\">*=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"kp\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mu</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"kp\">prod</span>\n\n<span class=\"c1\"># Initialize MCMC, then iterate</span>\n<span class=\"n\">z1</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"n\">posterior_samples</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">z1</span><span class=\"p\">]</span>\n\n<span class=\"k\">while</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">posterior_samples</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"n\">total_samples</span><span class=\"p\">:</span>\n    <span class=\"n\">z_current</span> <span class=\"o\">=</span> <span class=\"n\">posterior_samples</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">z_candidate</span> <span class=\"o\">=</span> <span class=\"n\">z_current</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span>\n    <span class=\"n\">rel_prob</span> <span class=\"o\">=</span> <span class=\"n\">posterior_numerator</span><span class=\"p\">(</span>\n        <span class=\"n\">z_candidate</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">posterior_numerator</span><span class=\"p\">(</span><span class=\"n\">z_current</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">rel_prob</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">posterior_samples</span><span class=\"o\">.</span><span class=\"kp\">append</span><span class=\"p\">(</span><span class=\"n\">z_candidate</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">trial_toss</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">trial_toss</span> <span class=\"o\">&lt;</span> <span class=\"n\">rel_prob</span><span class=\"p\">:</span>\n            <span class=\"n\">posterior_samples</span><span class=\"o\">.</span><span class=\"kp\">append</span><span class=\"p\">(</span><span class=\"n\">z_candidate</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">posterior_samples</span><span class=\"o\">.</span><span class=\"kp\">append</span><span class=\"p\">(</span><span class=\"n\">z_current</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Drop some initial samples and thin</span>\n<span class=\"n\">thinned_samples</span> <span class=\"o\">=</span> <span class=\"n\">posterior_samples</span><span class=\"p\">[</span><span class=\"mi\">2000</span><span class=\"p\">:]</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">hist</span><span class=\"p\">(</span><span class=\"n\">thinned_samples</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&quot;Histogram of MCMC samples&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</pre></div>\n\n\n<h3>Summary</h3>\n<p>To summarize, we have reviewed the application of <span class=\"caps\">MCMC</span> to Bayesian statistics. <span class=\"caps\">MCMC</span> is a general tool for obtaining samples from a probability distribution. It can be applied whenever one can conveniently specify the relative probability of two states &#8212; and so is particularly apt for situations where only the normalization constant of a distribution is difficult to evaluate, precisely the problem with the posterior (\\ref{Bayes}). The method entails carrying out an iterative try-accept algorithm, where the rates of trial and acceptance can be adjusted, but must be balanced so that the equilibrium distribution that results approaches the desired form. The key equation enabling us to strike this balance is (\\ref{inter}) &#8212; the zero flux condition (aka the <em>detailed balance</em> condition to physicists) that holds between states at&nbsp;equilibrium.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}