{
  "title": "Efficiently Store Pandas DataFrames",
  "link": "",
  "updated": "2015-03-16T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/03/16/Fast-Serialization",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr</strong> We benchmark several options to store Pandas DataFrames to disk.\nGood options exist for numeric data but text is a pain.  Categorical dtypes\nare a good option.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>For\n<a href=\"http://matthewrocklin.com/blog/work/2015/03/11/Towards-OOC-Frame/\">dask.frame</a>\nI need to read and write Pandas DataFrames to disk.  Both disk bandwidth and\nserialization speed limit storage performance.</p>\n\n<ul>\n  <li>Disk bandwidth, between 100MB/s and 800MB/s for a notebook hard drive, is\nlimited purely by hardware.  Not much we can do here except buy better\ndrives.</li>\n  <li>Serialization cost though varies widely by library and context.  We can be\nsmart here.  Serialization is the conversion of a Python variable (e.g.\nDataFrame) to a stream of bytes that can be written raw to disk.</li>\n</ul>\n\n<p>Typically we use libraries like <code class=\"language-plaintext highlighter-rouge\">pickle</code> to serialize Python objects.  For\ndask.frame we <em>really</em> care about doing this quickly so we’re going to also\nlook at a few alternatives.</p>\n\n<h2 id=\"contenders\">Contenders</h2>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">pickle</code> - The standard library pure Python solution</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">cPickle</code> - The standard library C solution</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">pickle.dumps(data, protocol=2)</code> - pickle and cPickle support multiple\nprotocols.  Protocol 2 is good for numeric data.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">json</code> - using the standardlib <code class=\"language-plaintext highlighter-rouge\">json</code> library, we encode the values and\nindex as lists of ints/strings</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">json-no-index</code> - Same as above except that we don’t encode the index of the\nDataFrame, e.g. <code class=\"language-plaintext highlighter-rouge\">0, 1, ...</code>\nWe’ll find that JSON does surprisingly well on pure text data.</li>\n  <li><a href=\"http://msgpack.org/\">msgpack</a> - A binary JSON alternative</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">CSV</code> - The venerable <code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code> and <code class=\"language-plaintext highlighter-rouge\">DataFrame.to_csv</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">hdfstore</code> - Pandas’ custom HDF5 storage format</li>\n</ul>\n\n<p>Additionally we mention but don’t include the following:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">dill</code> and <code class=\"language-plaintext highlighter-rouge\">cloudpickle</code>- formats commonly used for function\nserialization.  These perform about the same as <code class=\"language-plaintext highlighter-rouge\">cPickle</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">hickle</code> - A pickle interface over HDF5.  This does well on NumPy data but\ndoesn’t support Pandas DataFrames well.</li>\n</ul>\n\n<h2 id=\"experiment\">Experiment</h2>\n\n<p><em>Disclaimer: We’re about to issue performance numbers on a toy dataset.  You\nshould not trust that what follows generalizes to your data.  You should\nlook at your own data and run benchmarks yourself.  My benchmarks lie.</em></p>\n\n<p>We create a DataFrame with two columns, one with numeric data, and one with\ntext.  The text column has repeated values (1000 unique values, each repeated\n1000 times) while the numeric column is all unique.  This is fairly typical of\ndata that I see in the wild.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s\">'text'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000000</span><span class=\"p\">)],</span>\n                   <span class=\"s\">'numbers'</span><span class=\"p\">:</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1000000</span><span class=\"p\">)})</span></code></pre>\n</figure>\n\n<p>Now we time the various <code class=\"language-plaintext highlighter-rouge\">dumps</code> and <code class=\"language-plaintext highlighter-rouge\">loads</code> methods of the different\nserialization libraries and plot the results below.</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/serialize.png\" alt=\"Time costs to serialize numeric data\" width=\"100%\" /></p>\n\n<p>As a point of reference writing the serialized result to disk and reading it\nback again should take somewhere between 0.05s and 0.5s on standard hard\ndrives.  We want to keep serialization costs below this threshold.</p>\n\n<p><em>Thank you to <a href=\"http://stanford.edu/~mwaskom/\">Michael Waskom</a> for making those\ncharts\n(see <a href=\"https://twitter.com/mrocklin/status/577204291418509312\">twitter conversation</a>\nand his <a href=\"http://nbviewer.ipython.org/gist/mwaskom/886b4e5cb55fed35213d\">alternative\ncharts</a>)</em></p>\n\n<p><em>Gist to recreate plots here:\n<a href=\"https://gist.github.com/mrocklin/4f6d06a2ccc03731dd5f\">https://gist.github.com/mrocklin/4f6d06a2ccc03731dd5f</a></em></p>\n\n<p><em>Further Disclaimer:  These numbers average from multiple repeated calls to\n<code class=\"language-plaintext highlighter-rouge\">loads/dumps</code>.  Actual performance in the wild is likely worse.</em></p>\n\n<h2 id=\"observations\">Observations</h2>\n\n<p>We have good options for numeric data but not for text.  This is unfortunate;\nserializing ASCII text should be cheap.  We lose here because we store text in\na Series with the NumPy dtype ‘O’ for generic Python objects.  We don’t have a\ndedicated variable length string dtype.  This is tragic.</p>\n\n<p>For numeric data the successful systems systems record a small amount of\nmetadata and then dump the raw bytes.  The main takeaway from this is that you\nshould use the <code class=\"language-plaintext highlighter-rouge\">protocol=2</code> keyword argument to pickle.  This option isn’t\nwell known but strongly impacts preformance.</p>\n\n<p><em>Note: Aaron Meurer notes in the comments that for Python 3 users <code class=\"language-plaintext highlighter-rouge\">protocol=3</code>\nis already default.  Python 3 users can trust the default <code class=\"language-plaintext highlighter-rouge\">protocol=</code> setting\nto be efficient and should not specify <code class=\"language-plaintext highlighter-rouge\">protocol=2</code>.</em></p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/serialize-subset.png\" alt=\"Time costs to serialize numeric data\" width=\"50%\" align=\"right\" /></p>\n\n<h3 id=\"some-thoughts-on-text\">Some thoughts on text</h3>\n\n<ol>\n  <li>\n    <p>Text should be <em>easy</em> to serialize.  It’s already text!</p>\n  </li>\n  <li>\n    <p>JSON-no-index serializes the text values of the dataframe (not the integer\nindex) as a list of strings.  <em>This assumes that the data are strings</em> which is\nwhy it’s able to outperform the others, even though it’s not an optimized\nformat.  This is what we would gain if we had a <code class=\"language-plaintext highlighter-rouge\">string</code> dtype rather than\nrelying on the NumPy Object dtype, <code class=\"language-plaintext highlighter-rouge\">'O'</code>.</p>\n  </li>\n  <li>\n    <p>MsgPack is surpsingly fast compared to cPickle</p>\n  </li>\n  <li>\n    <p>MsgPack is oddly unbalanced, it can dump text data very quickly but takes a\nwhile to load it back in.  Can we improve msgpack load speeds?</p>\n  </li>\n  <li>\n    <p>CSV text loads are fast.  Hooray for <code class=\"language-plaintext highlighter-rouge\">pandas.read_csv</code>.</p>\n  </li>\n</ol>\n\n<h3 id=\"some-thoughts-on-numeric-data\">Some thoughts on numeric data</h3>\n\n<ol>\n  <li>\n    <p>Both <code class=\"language-plaintext highlighter-rouge\">pickle(..., protocol=2)</code> and <code class=\"language-plaintext highlighter-rouge\">msgpack</code> dump raw bytes.\nThese are well below disk I/O speeds.  Hooray!</p>\n  </li>\n  <li>\n    <p>There isn’t much reason to compare performance below this level.</p>\n  </li>\n</ol>\n\n<h2 id=\"categoricals-to-the-rescue\">Categoricals to the Rescue</h2>\n\n<p>Pandas recently added support for <a href=\"http://pandas-docs.github.io/pandas-docs-travis/categorical.html\">categorical\ndata</a>.  We\nuse categorical data when our values take on a fixed number of possible options\nwith potentially many repeats (like stock ticker symbols.)  We enumerate these\npossible options (<code class=\"language-plaintext highlighter-rouge\">AAPL: 1, GOOG: 2, MSFT: 3, ...</code>) and use those numbers\nin place of the text.  This works well when there are many more\nobservations/rows than there are unique values.  Recall that in our case we\nhave one million rows but only one thousand unique values.  This is typical for\nmany kinds of data.</p>\n\n<p>This is great!  We’ve shrunk the amount of text data by a factor of a thousand,\nreplacing it with cheap-to-serialize numeric data.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'text'</span><span class=\"p\">].</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s\">'category'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">text</span>\n<span class=\"mi\">0</span>      <span class=\"mi\">0</span>\n<span class=\"mi\">1</span>      <span class=\"mi\">1</span>\n<span class=\"mi\">2</span>      <span class=\"mi\">2</span>\n<span class=\"mi\">3</span>      <span class=\"mi\">3</span>\n<span class=\"p\">...</span>\n<span class=\"mi\">999997</span>    <span class=\"mi\">997</span>\n<span class=\"mi\">999998</span>    <span class=\"mi\">998</span>\n<span class=\"mi\">999999</span>    <span class=\"mi\">999</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">Length</span><span class=\"p\">:</span> <span class=\"mi\">1000000</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">category</span>\n<span class=\"n\">Categories</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"nb\">object</span><span class=\"p\">):</span> <span class=\"p\">[</span><span class=\"mi\">0</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10</span> <span class=\"o\">&lt;</span> <span class=\"mi\">100</span> <span class=\"p\">...</span> <span class=\"mi\">996</span> <span class=\"o\">&lt;</span> <span class=\"mi\">997</span> <span class=\"o\">&lt;</span> <span class=\"mi\">998</span> <span class=\"o\">&lt;</span> <span class=\"mi\">999</span><span class=\"p\">]</span></code></pre>\n</figure>\n\n<p>Lets consider the costs of doing this conversion and of serializing it\nafterwards relative to the costs of just serializing it.</p>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Serialize Original Text</th>\n      <td> 1.042523</td>\n    </tr>\n    <tr>\n      <th>Convert to Categories</th>\n      <td> 0.072093</td>\n    </tr>\n    <tr>\n      <th>Serialize Categorical Data</th>\n      <td> 0.028223</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>When our data is amenable to categories then it’s cheaper to\nconvert-then-serialize than it is to serialize the raw text.  Repeated\nserializations are just pure-win.  Categorical data is good for other reasons\ntoo; computations on object dtype in Pandas generally happen at Python speeds.\nIf you care about performance then categoricals are definitely something to\nroll in to your workflow.</p>\n\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n\n<ol>\n  <li>Several excellent serialization options exist, each with different\nstrengths.</li>\n  <li>A combination of good serialization support for numeric data and\nPandas categorical dtypes enable efficient serialization and storage of\nDataFrames.</li>\n  <li>Object dtype is bad for PyData.  String dtypes would be nice.  I’d like to\nshout out to <a href=\"https://github.com/libdynd/dynd-python\">DyND</a> a possible NumPy\nreplacement that would resolve this.</li>\n  <li>MsgPack provides surprisingly good performance over custom Python\nsolutions, why is that?</li>\n  <li>I suspect that we could improve performance by special casing Object dtypes\nand assuming that they contain only text.</li>\n</ol>"
}