{
  "title": "FastText vs. Word2vec: A Quick Comparison",
  "link": "https://kavita-ganesan.com/fasttext-vs-word2vec/",
  "comments": "https://kavita-ganesan.com/fasttext-vs-word2vec/#respond",
  "dc:creator": "Kavita Ganesan",
  "pubDate": "Thu, 08 Jul 2021 20:54:29 +0000",
  "category": [
    "AI Implementation",
    "Machine Learning",
    "Neural Embeddings"
  ],
  "guid": "https://kavita-ganesan.com/?p=5990",
  "description": "One of the questions that often comes up is what&#8217;s the difference between fastText&#160;and Word2Vec? Aren&#8217;t they both the same? Yes and no. They are conceptually the same, but there is a minor difference—fastText&#160;operates at a character level but Word2Vec operates at a word level. Why this difference? Before we dive into fastText&#160;, let&#8217;s quickly &#8230;<p class=\"read-more\"> <a class=\"\" href=\"https://kavita-ganesan.com/fasttext-vs-word2vec/\"> <span class=\"screen-reader-text\">FastText vs. Word2vec: A Quick Comparison</span> Read More »</a></p>",
  "content:encoded": "\n<p>One of the questions that often comes up is what&#8217;s the difference between fastText&nbsp;and Word2Vec? Aren&#8217;t they both the same?</p>\n\n\n\n<p>Yes and no. They are conceptually the same, but there is a minor difference—fastText&nbsp;operates at a <em>character </em>level but Word2Vec operates at a <em>word </em>level. Why this difference?</p>\n\n\n\n<p>Before we dive into fastText&nbsp;, let&#8217;s quickly recap what Word2Vec is. With Word2Vec, we train a neural network with a single hidden layer to predict a&nbsp;<strong>target word</strong>&nbsp;based on its&nbsp;<strong>context</strong>&nbsp;(<strong>neighboring words</strong>). The assumption is that&nbsp;<em>the meaning of a word can be inferred by the company it keeps</em>. Under the hood, when it comes to training you could use two different neural architectures to achieve this—<a href=\"https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/\"><em>CBOW</em> and <em>SkipGram</em></a>.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-1024x538.png\" alt=\"Difference between skipgram and cbow - word2vec\" class=\"wp-image-4613\" width=\"785\" height=\"412\" srcset=\"https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-1024x538.png 1024w, https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-300x158.png 300w, https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-150x79.png 150w, https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-768x403.png 768w, https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-1536x806.png 1536w, https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png 2048w\" sizes=\"(max-width: 785px) 100vw, 785px\" /><figcaption>CBOW vs. SkipGram</figcaption></figure></div>\n\n\n\n<p>And as you know, after the training phase using either architecture, you can use the learned vectors in creative ways. For example, for recommendations, synonyms extraction, and more. The <strong>SkipGram </strong>architecture from Word2Vec was taken one level deeper, to operate at a character n-gram level—essentially using a bag of character n-grams. This is fastText.</p>\n\n\n\n<p></p>\n\n\n\n<h2>What is a character n-gram?</h2>\n\n\n\n<p>A character n-gram is a set of co-occurring characters within a given window. It&#8217;s very similar to word <a href=\"https://kavita-ganesan.com/what-are-n-grams/\">n-grams</a>, only that the window size is at the character level. And a <em>bag </em>of character n-grams in the fastText case means a word is represented by a sum of its character n-grams. If <code>n=</code>2, and your word is <em><strong><code>this</code> </strong></em>your resulting n-grams would be:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\">\n<t\nth\nhi\nis\ns>\nthis\n\n</pre></div>\n\n\n<p>The last item is a special sequence. Here&#8217;s a visual example of how the neighboring word <em>this </em>is represented in learning to predict the word <em>visual</em> based on the sentence &#8220;<strong>this</strong> is a <strong>visual</strong> example&#8221; (remember: the meaning of a word, is inferred by the company it keeps).</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img decoding=\"async\" loading=\"lazy\" src=\"https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText-1024x794.png\" alt=\"Word2Vec vs. fastText\" class=\"wp-image-4696\" width=\"710\" height=\"551\" srcset=\"https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText-1024x794.png 1024w, https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText-300x233.png 300w, https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText-150x116.png 150w, https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText-768x596.png 768w, https://kavita-ganesan.com/wp-content/uploads/SkipGram-with-subword-information-character-n-gram-size2.-Also-known-as-FastText.png 1436w\" sizes=\"(max-width: 710px) 100vw, 710px\" /><figcaption>Skip-Gram with character n-gram information (character n-gram size=2). </figcaption></figure></div>\n\n\n\n<p>The intuition behind fastText&nbsp;is that by using a bag of character n-grams, you can learn representations for&nbsp;morphologically rich languages. </p>\n\n\n\n<p>For example, in languages such as German, certain phrases are expressed as a single word. The phrase <em>table tennis,</em> for instance, is written in as Tischtennis. In plain vanilla Word2Vec, you&#8217;ll learn the representation of <code>tennis</code> and <code>tischtennis</code> separately. This makes it harder to infer that <code>tennis</code> and <code>tischtennis</code> are in fact related. </p>\n\n\n\n<p>However, by learning the character n-gram representation of these words,&nbsp;<code>tennis</code>&nbsp;and&nbsp;<code>tischtennis</code>&nbsp;will now share overlapping n-grams, making them closer in vector space. And thus, would make it easier to surface related concepts. </p>\n\n\n\n<p>Another use of character n-gram representation is to infer the meaning of unseen words. For example, if you are looking for the similarity of&nbsp;<code>courageous</code> and your corpora does not carry this word, you can still infer its meaning from its subwords such as&nbsp;<code>courage</code>.</p>\n\n\n\n<h2>Some Interesting Tidbits</h2>\n\n\n\n<ul><li>From the original <a href=\"https://arxiv.org/abs/1607.04606\">fastText paper, </a>the authors found that the use of character n-grams was more useful in morphologically rich languages such as Arabic, German and Russian than for English (evaluated using rank correlation with human judgment). I can attest to this as I did try subword information for English similarity and the results were not as good as using CBOW.  (See my <a href=\"https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/\">CBOW vs. SkipGram</a> article)</li><li>The authors found that using n-grams with <code><em>n>=3</em></code> and <code><em>n<=6</em></code> worked best. But the The optimal n-gram size really depends on the task and language and should be tuned appropriately. </li><li>For analogy tasks, subword information significantly improved syntactic analogy tasks but did not help with semantic (meaning) analogy tasks. </li></ul>\n\n\n\n<h2>Summing up fastText vs. Word2Vec</h2>\n\n\n\n<p>In summary, conceptually Word2Vec and fastText have the same goal: to learn vector representations of words. But unlike Word2Vec, which under the hood uses words to predict words, fastText operates at a more granular level with character n-grams. Where words are represented by the sum of the character n-gram vectors.</p>\n\n\n\n<p>Is fastText better than Word2Vec? In my opinion, no. It does better on some tasks and maybe in non-English languages. But for tasks in English, I&#8217;ve found Word2Vec to be just as good or better. </p>\n\n\n\n<h2>Learn More About Word Embeddings: </h2>\n\n\n\n<ul class=\"has-background\" style=\"background-color:#fff9f9\"><li><a href=\"https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/\">Comparison between CBOW, SkipGram and SkipGramSI</a></li><li><a href=\"https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\">Gensim Word2Vec Tutorial: An End-to-End Example</a></li><li><a href=\"https://kavita-ganesan.com/easily-access-pre-trained-word-embeddings-with-gensim/\">Easily Access Pre-trained Word Embeddings with Gensim</a></li></ul>\n",
  "wfw:commentRss": "https://kavita-ganesan.com/fasttext-vs-word2vec/feed/",
  "slash:comments": 0,
  "post-id": 5990
}