{
  "title": "Machine Learning Classics: The Perceptron",
  "link": "https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/",
  "comments": "https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/#comments",
  "dc:creator": "datasciencelab",
  "pubDate": "Fri, 10 Jan 2014 22:01:57 +0000",
  "category": [
    "Experiments",
    "linear classifier",
    "machine learning",
    "perceptron",
    "supervised learning"
  ],
  "guid": "http://datasciencelab.wordpress.com/?p=539",
  "description": "An important problem in statistics and machine learning is that of classification, which is the task of identifying the category that an observation belongs to, on the basis of a training set of data containing other instances. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a [&#8230;]",
  "content:encoded": "<p>An important problem in statistics and machine learning is that of classification, which is the task of identifying the category that an observation belongs to, on the basis of a training set of data containing other instances.</p>\n<blockquote><p>In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering or cluster analysis, and involves grouping data into categories based on some measure of inherent similarity (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space). (<a href=\"http://en.wikipedia.org/wiki/Classification_(machine_learning)\">Wikipedia</a>)</p></blockquote>\n<p>At The Data Science Lab, we have already reviewed some basics of <em>unsupervised</em> classification with the <a title=\"Clustering With K-Means in Python\" href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">Lloyd algorithm for k-means clustering</a> and have investigated <a title=\"Finding the K in K-Means Clustering\" href=\"https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/\">how to find the appropriate number of clusters</a>. Today&#8217;s post will be devoted to a classical machine learning algorithm for <em>supervised</em> classification: the perceptron learning algorithm.</p>\n<h3>Theory behind the perceptron</h3>\n<p>The perceptron learning algorithm was invented in the late 1950s by <a href=\"http://en.wikipedia.org/wiki/Frank_Rosenblatt\">Frank Rosenblatt</a>. It belongs to the class of linear classifiers, this is, for a data set classified according to binary categories (which we will assume to be labeled +1 and -1), the classifier seeks to divide the two classes by a linear separator. The separator is a <em>(n-1)</em>-dimensional hyperplane in a <em>n</em>-dimensional space, in particular it is a line in the plane and a plane in the 3-dimensional space.</p>\n<p>Our data set will be assumed to consist of <em>N</em> observations characterized by <em>d</em> features or attributes, <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n+%3D+%28x_1%2C+%5Cldots%2C+x_d%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n+%3D+%28x_1%2C+%5Cldots%2C+x_d%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n+%3D+%28x_1%2C+%5Cldots%2C+x_d%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_n = (x_1, &#92;ldots, x_d)\" class=\"latex\" /> for <img src=\"https://s0.wp.com/latex.php?latex=n+%3D+%281%2C+%5Cldots%2C+N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=n+%3D+%281%2C+%5Cldots%2C+N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n+%3D+%281%2C+%5Cldots%2C+N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"n = (1, &#92;ldots, N)\" class=\"latex\" />. The problem of binary classifying these data points can be translated to that of finding a series of weights <img src=\"https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=w_i&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"w_i\" class=\"latex\" /> such that all vectors verifying</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3C+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3C+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3C+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle &#92;sum_{i=1}^d w_i x_i < b\" class=\"latex\" /></p>\n<p>are assigned to one of the classes whereas those verifying</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3E+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3E+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5Ed+w_i+x_i+%3E+b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle &#92;sum_{i=1}^d w_i x_i > b\" class=\"latex\" /></p>\n<p>are assigned to the other, for a given threshold value <img src=\"https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"b\" class=\"latex\" />. If we rename <img src=\"https://s0.wp.com/latex.php?latex=b+%3D+w_0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=b+%3D+w_0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=b+%3D+w_0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"b = w_0\" class=\"latex\" /> and introduce an artificial coordinate <img src=\"https://s0.wp.com/latex.php?latex=x_0+%3D+1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=x_0+%3D+1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=x_0+%3D+1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"x_0 = 1\" class=\"latex\" /> in our vectors <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_n\" class=\"latex\" />, we can write the perceptron separator formula as</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28%5Csum_%7Bi%3D0%7D%5Ed+w_i+x_i%5Cright%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28+%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28%5Csum_%7Bi%3D0%7D%5Ed+w_i+x_i%5Cright%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28+%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+h%28%5Cmathbf%7Bx%7D%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28%5Csum_%7Bi%3D0%7D%5Ed+w_i+x_i%5Cright%29+%3D+%5Cmathrm%7Bsign%7D%5Cleft%28+%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle h(&#92;mathbf{x}) = &#92;mathrm{sign}&#92;left(&#92;sum_{i=0}^d w_i x_i&#92;right) = &#92;mathrm{sign}&#92;left( &#92;mathbf{w}^{&#92;mathbf{T}}&#92;mathbf{x}&#92;right)\" class=\"latex\" /></p>\n<p>Note that <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}^{&#92;mathbf{T}}&#92;mathbf{x}\" class=\"latex\" /> is notation for the <a href=\"http://en.wikipedia.org/wiki/Scalar_product\">scalar product</a> between vectors <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}\" class=\"latex\" />. Thus the problem of classifying is that of finding the vector of weights <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}\" class=\"latex\" /> given a training data set of <em>N</em> vectors <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}\" class=\"latex\" /> with their corresponding labeled classification vector <img src=\"https://s0.wp.com/latex.php?latex=%28y_1%2C+%5Cldots%2C+y_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28y_1%2C+%5Cldots%2C+y_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28y_1%2C+%5Cldots%2C+y_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(y_1, &#92;ldots, y_N)\" class=\"latex\" />.</p>\n<h3>The perceptron learning algorithm (PLA)</h3>\n<p>The learning algorithm for the perceptron is online, meaning that instead of considering the entire data set at the same time, it only looks at one example at a time, processes it and goes on to the next one. The algorithm starts with a guess for the vector <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}\" class=\"latex\" /> (without loss of generalization one can begin with a vector of zeros). <a href=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png\"><img data-attachment-id=\"555\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/perceptron_update/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png\" data-orig-size=\"289,293\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"perceptron_update\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png?w=289\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png?w=289\" class=\"alignright size-full wp-image-555\" alt=\"perceptron_update\" src=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png?w=830\" srcset=\"https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png 289w, https://datasciencelab.files.wordpress.com/2014/01/perceptron_update.png?w=148 148w\" sizes=\"(max-width: 289px) 100vw, 289px\"   /></a>It then assesses how good of a guess that is by comparing the predicted labels with the actual, correct labels (remember that those are available for the training test, since we are doing supervised learning). As long as there are misclassified points, the algorithm corrects its guess for the weight vector by updating the weights in the correct direction, until all points are correctly classified.</p>\n<p>That direction is as follows: given a labeled training data set, if <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}\" class=\"latex\" /> is the guessed weight vector and <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{x}_n\" class=\"latex\" /> is an incorrectly classified point with <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D_n+%5Cneq+y_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D_n+%5Cneq+y_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%5E%7B%5Cmathbf%7BT%7D%7D%5Cmathbf%7Bx%7D_n+%5Cneq+y_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}^{&#92;mathbf{T}}&#92;mathbf{x}_n &#92;neq y_n\" class=\"latex\" />, then the weight <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w}\" class=\"latex\" /> is updated to <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D+%2B+y_n+%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D+%2B+y_n+%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D+%2B+y_n+%5Cmathbf%7Bx%7D_n&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{w} + y_n &#92;mathbf{x}_n\" class=\"latex\" />. This is illustrated in the plot on the right, taken from <a href=\"http://www.mblondel.org/journal/2010/10/31/kernel-perceptron-in-python/\">this clear article on the perceptron</a>.</p>\n<p>A nice feature of the perceptron learning rule is that if there exist a set of weights that solve the problem (i.e. if the data is linearly separable), then the perceptron will find these weights.</p>\n<h3>A python implementation of the perceptron</h3>\n<p>For our python implementation we will use a trivial example on two dimensions: within the <img src=\"https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D%5Ctimes%5B-1%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D%5Ctimes%5B-1%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D%5Ctimes%5B-1%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"[-1,1]&#92;times[-1,1]\" class=\"latex\" /> space, we define two random points and draw the line that joins them. The general equation of a line given two points in it, <img src=\"https://s0.wp.com/latex.php?latex=%28x_1%2C+y_1%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28x_1%2C+y_1%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x_1%2C+y_1%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(x_1, y_1)\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%28x_2%2C+y_2%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28x_2%2C+y_2%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x_2%2C+y_2%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(x_2, y_2)\" class=\"latex\" />, is <img src=\"https://s0.wp.com/latex.php?latex=A+%2B+Bx+%2B+Cy+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A+%2B+Bx+%2B+Cy+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A+%2B+Bx+%2B+Cy+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A + Bx + Cy = 0\" class=\"latex\" /> where <img src=\"https://s0.wp.com/latex.php?latex=A%2C+B%2C+C&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A%2C+B%2C+C&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A%2C+B%2C+C&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A, B, C\" class=\"latex\" /> can be written in terms of the two points. Defining a vector <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D+%3D+%28A%2C+B%2C+C%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D+%3D+%28A%2C+B%2C+C%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D+%3D+%28A%2C+B%2C+C%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{V} = (A, B, C)\" class=\"latex\" />, any point <img src=\"https://s0.wp.com/latex.php?latex=%28x%2Cy%29+&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28x%2Cy%29+&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28x%2Cy%29+&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(x,y) \" class=\"latex\" /> belongs to the line if <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D%5E%5Cmathrm%7BT%7D%5Cmathrm%7Bx%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D%5E%5Cmathrm%7BT%7D%5Cmathrm%7Bx%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7BV%7D%5E%5Cmathrm%7BT%7D%5Cmathrm%7Bx%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{V}^&#92;mathrm{T}&#92;mathrm{x} = 0\" class=\"latex\" />, where <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bx%7D+%3D+%281%2Cx%2Cy%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bx%7D+%3D+%281%2Cx%2Cy%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bx%7D+%3D+%281%2Cx%2Cy%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{x} = (1,x,y)\" class=\"latex\" />. Points for which the dot product is positive fall on one side of the line, negatives fall on the other.</p>\n<p>This procedure automatically divides the plane linearly in two regions. We randomly choose <em>N</em> points in this space and classify them as +1 or -1 according to the dividing line defined before. The <code>Perceptron</code> class defined below is initialized exactly in this way. The perceptron learning algorithm is implemented in the <code>pla</code> function, and the classification error, defined as the fraction of misclassified points, is in <code>classification_error</code>. The code is as follows:</p>\n<pre class=\"brush: python; title: ; notranslate\">\nimport numpy as np\nimport random\nimport os, subprocess\n\nclass Perceptron:\n    def __init__(self, N):\n        # Random linearly separated data\n        xA,yA,xB,yB = [random.uniform(-1, 1) for i in range(4)]\n        self.V = np.array([xB*yA-xA*yB, yB-yA, xA-xB])\n        self.X = self.generate_points(N)\n\n    def generate_points(self, N):\n        X = []\n        for i in range(N):\n            x1,x2 = [random.uniform(-1, 1) for i in range(2)]\n            x = np.array([1,x1,x2])\n            s = int(np.sign(self.V.T.dot(x)))\n            X.append((x, s))\n        return X\n\n    def plot(self, mispts=None, vec=None, save=False):\n        fig = plt.figure(figsize=(5,5))\n        plt.xlim(-1,1)\n        plt.ylim(-1,1)\n        V = self.V\n        a, b = -V[1]/V[2], -V[0]/V[2]\n        l = np.linspace(-1,1)\n        plt.plot(l, a*l+b, 'k-')\n        cols = {1: 'r', -1: 'b'}\n        for x,s in self.X:\n            plt.plot(x[1], x[2], cols[s]+'o')\n        if mispts:\n            for x,s in mispts:\n                plt.plot(x[1], x[2], cols[s]+'.')\n        if vec != None:\n            aa, bb = -vec[1]/vec[2], -vec[0]/vec[2]\n            plt.plot(l, aa*l+bb, 'g-', lw=2)\n        if save:\n            if not mispts:\n                plt.title('N = %s' % (str(len(self.X))))\n            else:\n                plt.title('N = %s with %s test points' \\\n                          % (str(len(self.X)),str(len(mispts))))\n            plt.savefig('p_N%s' % (str(len(self.X))), \\\n                        dpi=200, bbox_inches='tight')\n\n    def classification_error(self, vec, pts=None):\n        # Error defined as fraction of misclassified points\n        if not pts:\n            pts = self.X\n        M = len(pts)\n        n_mispts = 0\n        for x,s in pts:\n            if int(np.sign(vec.T.dot(x))) != s:\n                n_mispts += 1\n        error = n_mispts / float(M)\n        return error\n\n    def choose_miscl_point(self, vec):\n        # Choose a random point among the misclassified\n        pts = self.X\n        mispts = []\n        for x,s in pts:\n            if int(np.sign(vec.T.dot(x))) != s:\n                mispts.append((x, s))\n        return mispts[random.randrange(0,len(mispts))]\n\n    def pla(self, save=False):\n        # Initialize the weigths to zeros\n        w = np.zeros(3)\n        X, N = self.X, len(self.X)\n        it = 0\n        # Iterate until all points are correctly classified\n        while self.classification_error(w) != 0:\n            it += 1\n            # Pick random misclassified point\n            x, s = self.choose_miscl_point(w)\n            # Update weights\n            w += s*x\n            if save:\n                self.plot(vec=w)\n                plt.title('N = %s, Iteration %s\\n' \\\n                          % (str(N),str(it)))\n                plt.savefig('p_N%s_it%s' % (str(N),str(it)), \\\n                            dpi=200, bbox_inches='tight')\n        self.w = w\n\n    def check_error(self, M, vec):\n        check_pts = self.generate_points(M)\n        return self.classification_error(vec, pts=check_pts)\n</pre>\n<p>To start a run of the perceptron with 20 data points and visualize the initial configuration we simply initialize the <code>Perceptron</code> class and call the <code>plot</code> function:</p>\n<pre class=\"brush: python; title: ; notranslate\">\np = Perceptron(20)\np.plot()\n</pre>\n<p><a href=\"https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png\"><img loading=\"lazy\" data-attachment-id=\"582\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/p_n20_o/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png\" data-orig-size=\"454,442\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"p_N20_o\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png?w=454\" class=\"wp-image-582 alignright\" alt=\"p_N20_o\" src=\"https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png?w=255&#038;h=255\" width=\"255\" height=\"255\" /></a><br />\nOn the right is the plane that we obtain, divided in two by the black line. Red points are labeled as +1 while blue ones are -1. The purpose of the perceptron learning algorithm is to &#8220;learn&#8221; a linear classifier that correctly separates red from blue points given the labeled set of 20 points shown in the figure. This is, we want to learn the black line as faithfully as possible.</p>\n<p>The call to <code>p.pla()</code> runs the algorithm and stores the final weights learned in <code>p.w</code>. To save a plot of each of the iterations, we can use the option <code>p.pla(save=True)</code>. The following snippet will concatenate all images together to produce an animated gif of the running algorithm:</p>\n<pre class=\"brush: python; title: ; notranslate\">\nbasedir = '/my/output/directory/'\nos.chdir(basedir)\npngs = [pl for pl in os.listdir(basedir) if pl.endswith('png')]\nsortpngs = sorted(pngs, key=lambda a:int(a.split('it')[1][:-4]))\nbasepng = pngs[0][:-8]\n[sortpngs.append(sortpngs[-1]) for i in range(4)]\ncomm = (\"convert -delay 50 %s %s.gif\" % (' '.join(sortpngs), basepng)).split()\nproc = subprocess.Popen(comm, stdin = subprocess.PIPE, stdout = subprocess.PIPE)\n(out, err) = proc.communicate()\n</pre>\n<p>Below we can see how the algorithm tries successive values for the weight vector, leading to a succession of guesses for the linear separator, which are plotted in green. For as long as the green line misclassifies points (meaning that it does not separate the blue from the right points correctly), the perceptron keeps updating the weights in the manner described above. Eventually all points are on the correct side of the guessed line, the classification error in the training data set (<img src=\"https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"E_{in} = 0\" class=\"latex\" /> for the in-sample points) is thus zero and the algorithm converges and stops.</p>\n<p><a href=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif\"><img data-attachment-id=\"580\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/p_n20/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif\" data-orig-size=\"454,447\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"p_N20\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif?w=454\" class=\"size-full wp-image-580 alignnone\" alt=\"p_N20\" src=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif?w=830\"   /></a></p>\n<p>Clearly the final guessed green line after the 7 iterations does separate the training data points correctly but it does not completely agree with the target black line. An error in classifying not-seen data points is bound to exist (<img src=\"https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"E_{out} &#92;neq 0\" class=\"latex\" /> for out-of-sample points), and we can quantify it easily by evaluating the performance of the linear classifier on fresh, unseen data points:</p>\n<pre class=\"brush: python; title: ; notranslate\">\np.plot(p.generate_points(500), p.w, save=True)\n</pre>\n<p><a href=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.png\"><img loading=\"lazy\" data-attachment-id=\"581\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/p_n20-2/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.png\" data-orig-size=\"454,442\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"p_N20\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.png?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.png?w=454\" class=\"wp-image-581 alignleft\" alt=\"p_N20\" src=\"https://datasciencelab.files.wordpress.com/2014/01/p_n201.png?w=266&#038;h=264\" width=\"266\" height=\"264\" /></a>In this image we can observe how, even if <img src=\"https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E_%7Bin%7D+%3D+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"E_{in} = 0\" class=\"latex\" /> for the training points represented by the large dots, <img src=\"https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E_%7Bout%7D+%5Cneq+0&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"E_{out} &#92;neq 0\" class=\"latex\" />, as shown by the small red and blue dots that fall on one side of the black (target) line but are incorrectly classified by the green (guessed) one. The exact out-of-sample error is given by the area between both lines. This can be thus computed analytically and exactly. But it can also be estimated with a repeated Monte Carlo sampling:</p>\n<pre class=\"brush: python; title: ; notranslate\">\nerr = []\nfor i in range(100):\n    err.append(p.check_error(500, p.w))\nnp.mean(err)\n</pre>\n<p><code>0.0598200</code></p>\n<p>The perceptron algorithm has thus learned a linear binary classifier that correctly classifies data in 94% of the cases, having an out-of-sample error rate of 6%.</p>\n<h3>Table-top data experiment take-away message</h3>\n<p>The perceptron learning algorithm is a classical example of binary linear supervised classifier. Its implementation involves finding a linear boundary that completely separates points belonging to the two classes. If the data is linearly separable, then the procedure will converge to a weight vector that separates the data. (And if the data is inseparable, the PLA will never converge.) The perceptron is thus fundamentally limited in that its decision boundaries can only be linear. Eventually we will explore other methods that overcome this limitation, either combining multiple perceptrons in a single framework (neural networks) or by mapping features in an efficient way (kernels).</p>\n",
  "wfw:commentRss": "https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-perceptron/feed/",
  "slash:comments": 24,
  "media:content": [
    {
      "media:title": "datasciencelab"
    },
    {
      "media:title": "perceptron_update"
    },
    {
      "media:title": "p_N20_o"
    },
    {
      "media:title": "p_N20"
    },
    {
      "media:title": "p_N20"
    }
  ]
}