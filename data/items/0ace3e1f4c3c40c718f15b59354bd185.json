{
  "title": "A Convergence Theory for Federated Average: Beyond Smoothness. (arXiv:2211.01588v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01588",
  "description": "<p>Federated learning enables a large amount of edge computing devices to learn\na model without data sharing jointly. As a leading algorithm in this setting,\nFederated Average FedAvg, which runs Stochastic Gradient Descent (SGD) in\nparallel on local devices and averages the sequences only once in a while, have\nbeen widely used due to their simplicity and low communication cost. However,\ndespite recent research efforts, it lacks theoretical analysis under\nassumptions beyond smoothness. In this paper, we analyze the convergence of\nFedAvg. Different from the existing work, we relax the assumption of strong\nsmoothness. More specifically, we assume the semi-smoothness and semi-Lipschitz\nproperties for the loss function, which have an additional first-order term in\nassumption definitions. In addition, we also assume bound on the gradient,\nwhich is weaker than the commonly used bounded gradient assumption in the\nconvergence analysis scheme. As a solution, this paper provides a theoretical\nconvergence study on Federated Learning.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Runzhou Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>"
}