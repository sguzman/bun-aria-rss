{
  "title": "Neural Variational Inference: Blackbox Mode",
  "link": "http://artem.sobolev.name/posts/2016-07-05-neural-variational-inference-blackbox.html",
  "description": "<p>In the <a href=\"/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html\">previous post</a> we covered Stochastic VI: an efficient and scalable variational inference method for exponential family models. However, there’re many more distributions than those belonging to the exponential family. Inference in these cases requires significant amount of model analysis. In this post we consider <a href=\"https://arxiv.org/abs/1401.0118\">Black Box Variational Inference</a> by Ranganath et al. This work just as the previous one comes from <a href=\"http://www.cs.columbia.edu/~blei/\">David Blei</a> lab — one of the leading researchers in VI. And, just for the dessert, we’ll touch upon another paper, which will finally introduce some neural networks in VI.</p>\n<!--more-->\n<h3>\nBlackbox Variational Inference\n</h3>\n<p>As we have learned so far, the goal of VI is to maximize the ELBO <span class=\"math inline\">\\(\\mathcal{L}(\\Theta, \\Lambda)\\)</span>. When we maximize it by <span class=\"math inline\">\\(\\Lambda\\)</span>, we decrease the gap between the marginal likelihood of the model considered <span class=\"math inline\">\\(\\log p(x \\mid \\Theta)\\)</span>, and when we maximize it by <span class=\"math inline\">\\(\\Theta\\)</span> we acltually fit the model. So let’s concentrate on optimizing this objective:</p>\n<p><span class=\"math display\">\\[\n\\mathcal{L}(\\Theta, \\Lambda) = \\mathbb{E}_{q(z \\mid x, \\Lambda)} \\left[\\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right]\n\\]</span></p>\n<p>Let’s find gradients of this objective:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\nabla_{\\Lambda} \\mathcal{L}(\\Theta, \\Lambda)\n&= \\nabla_{\\Lambda} \\int q(z \\mid x, \\Lambda) \\left[\\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right] dz  \\\\\n&= \\int \\nabla_{\\Lambda} q(z \\mid x, \\Lambda) \\left[\\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right] dz - \\int q(z \\mid x, \\Lambda) \\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) dz  \\\\\n&= \\mathbb{E}_{q} \\left[\\frac{\\nabla_{\\Lambda} q(z \\mid x, \\Lambda)}{q(z \\mid x, \\Lambda)} \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)} \\right] - \\int q(z \\mid x, \\Lambda) \\frac{\\nabla_{\\Lambda} q(z \\mid x, \\Lambda)}{q(z \\mid x, \\Lambda)} dz \\\\\n&= \\mathbb{E}_{q} \\left[\\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)} \\right] - \\int \\nabla_{\\Lambda} q(z \\mid x, \\Lambda) dz \\\\\n&= \\mathbb{E}_{q} \\left[\\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)} \\right] - \\nabla_{\\Lambda} \\overbrace{\\int q(z \\mid x, \\Lambda) dz}^{=1} \\\\\n&= \\mathbb{E}_{q} \\left[\\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)} \\right]\n\\end{align}\n\\]</span></p>\n<p>In statistics <span class=\"math inline\">\\(\\nabla_\\Lambda \\log q(z \\mid x, \\Lambda)\\)</span> is known as <a href=\"https://en.wikipedia.org/wiki/Score_(statistics)\">score function</a>. For more on this “trick” see <a href=\"http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/\">a blogpost by Shakir Mohamed</a>. In many cases of practical interest <span class=\"math inline\">\\(\\log p(x, z, \\mid \\Theta)\\)</span> is too complicated to compute this expectation in closed form. Recall that we already used stochastic optimization successfully, so we can settle with just an estimate of true gradient. We get one by approximating the expectation using Monte-Carlo estimates using <span class=\"math inline\">\\(L\\)</span> samples <span class=\"math inline\">\\(z^{(l)} \\sim q(z \\mid x, \\Lambda)\\)</span> (in practice we sometimes use just <span class=\"math inline\">\\(L=1\\)</span> sample. We expect correct averaging to happen automagically due to use of minibatches):</p>\n<p><span class=\"math display\">\\[\n\\nabla_{\\Lambda} \\mathcal{L}(\\Theta, \\Lambda)\n\\approx \\frac{1}{L} \\sum_{l=1}^L \\nabla_{\\Lambda} \\log q(z^{(l)} \\mid x, \\Lambda) \\log \\frac{p(x, z^{(l)} \\mid \\Theta)}{q(z^{(l)} \\mid x, \\Lambda)}\n\\]</span></p>\n<p>For model parameters <span class=\"math inline\">\\(\\Theta\\)</span> gradients look even simpler, as we don’t need to differentiate w.r.t. expectation distribution’s parameters:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\nabla_{\\Theta} \\mathcal{L}(\\Theta, \\Lambda)\n&= \\mathbb{E}_{q} \\nabla_{\\Theta} \\log p(x, z \\mid \\Theta)\n\\approx \\frac{1}{L} \\sum_{l=1}^L \\nabla_{\\Theta} \\log p(x, z^{(l)} \\mid \\Theta)\n\\end{align}\n\\]</span></p>\n<p>We can even “naturalize” these gradients by premultiplying by the inverse Fisher Information Matrix <span class=\"math inline\">\\(\\mathcal{I}(\\Lambda)^{-1}\\)</span>. And that’s it! Much simpler than before, right? Of course, there’s no free lunch, so there must be a catch… And there is: performance of stochastic optimization methods crucially depends on the variance of gradient estimators. It makes perfect sense: the higher the variance — the less information about the step direction we get. And unfortunately, in practice the aforementioned estimator based on the score function has impractically high variance. Luckily, in Monte Carlo community there are many variance reductions techniques known, we now describe some of them.</p>\n<p>The first technique we’ll describe is <strong>Rao-Blackwellization</strong>. The idea is simple: if it’s possible to compute the expectation w.r.t. some of random variables, you should do it. If you think of it, it’s an obvious advice as you essentially reduce amount of randomness in your Monte Carlo estimates. But let’s put it more formally: we use chain rule to rewrite joint expectation as marginal expectation of conditional one:</p>\n<p><span class=\"math display\">\\[\n\\mathbb{E}_{X, Y} f(X, Y) = \\mathbb{E}_X \\left[ \\mathbb{E}_{Y \\mid X} f(X, Y) \\right]\n\\]</span></p>\n<p>Let’s see what happens with variance (in scalar case) when we estimate expectation of <span class=\"math inline\">\\(\\mathbb{E}_{Y \\mid X} f(X, Y)\\)</span> instead of expectation of <span class=\"math inline\">\\(f(X, Y)\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\text{Var}_X(\\mathbb{E}_{Y \\mid X} f(X, Y))\n&= \\mathbb{E} (\\mathbb{E}_{Y \\mid X} f(X, Y))^2 - (\\mathbb{E}_{X, Y} f(X, Y))^2 \\\\\n&= \\text{Var}_{X,Y}(f(X, Y)) - \\mathbb{E}_X \\left(\\mathbb{E}_{Y \\mid X} f(X, Y)^2 - (\\mathbb{E}_{Y \\mid X} f(X, Y))^2  \\right) \\\\\n&= \\text{Var}_{X,Y}(f(X, Y)) - \\mathbb{E}_X \\text{Var}_{Y\\mid X} (f(X, Y))\n\\end{align}\n\\]</span></p>\n<p>This formula says that Rao-Blackwellizing an estimator reduces its variance by <span class=\"math inline\">\\(\\mathbb{E}_X \\text{Var}_{Y\\mid X} (f(X, Y))\\)</span>. Indeed, you can think of this term as of a measure of how much information <span class=\"math inline\">\\(Y\\)</span> contains about <span class=\"math inline\">\\(X\\)</span> that’s relevant to computing <span class=\"math inline\">\\(f(X, Y)\\)</span>. Suppose <span class=\"math inline\">\\(Y = X\\)</span>: then you have <span class=\"math inline\">\\(\\mathbb{E}_X f(X, X)\\)</span>, and taking expectation w.r.t. <span class=\"math inline\">\\(Y\\)</span> does not reduce amount of randomness in the estimator. And this is what the formula tells us as <span class=\"math inline\">\\(\\text{Var}_{Y \\mid X} f(X, Y)\\)</span> would be 0 in this case. Here’s another example: suppose <span class=\"math inline\">\\(f\\)</span> does not use <span class=\"math inline\">\\(X\\)</span> at all: then only randomness in <span class=\"math inline\">\\(Y\\)</span> affects the estimate, and after Rao-Blackwellization we expect the variance to drop to 0. And the formula agrees with out expectations as <span class=\"math inline\">\\(\\mathbb{E}_X \\text{Var}_{Y \\mid X} f(X, Y) = \\text{Var}_Y f(X, Y)\\)</span> for any <span class=\"math inline\">\\(X\\)</span> since <span class=\"math inline\">\\(f(X, Y)\\)</span> does not depend on <span class=\"math inline\">\\(X\\)</span>.</p>\n<p>Next technique is <strong>Control Variates</strong>, which is slightly less intuitive. The idea is that we can add zero-mean function <span class=\"math inline\">\\(h(X)\\)</span> that’ll preserve the expectation, but reduce the variance. Again, for a scalar case</p>\n<p><span class=\"math display\">\\[\n\\text{Var}(f(X) - \\alpha h(X)) = \\text{Var}(f(X)) - 2 \\alpha \\text{Cov}(f(X), h(X)) + \\alpha^2 \\text{Var}(f(X))\n\\]</span></p>\n<p>Optimal <span class=\"math inline\">\\(\\alpha^* = \\frac{\\text{Cov}(f(X), h(X))}{\\text{Var}(f(X))}\\)</span>. This formula reflects an obvious fact: if we want to reduce the variance, <span class=\"math inline\">\\(h(X)\\)</span> must be correlated with <span class=\"math inline\">\\(f(X)\\)</span>. Sign of correlation does not matter, as <span class=\"math inline\">\\(\\alpha^*\\)</span> will adjust. BTW, in reinforcement learning <span class=\"math inline\">\\(\\alpha\\)</span> is called <strong>baseline</strong>.</p>\n<p>As we already have learned, <span class=\"math inline\">\\(\\mathbb{E}_{q(z \\mid x, \\Lambda)} \\nabla_\\Lambda \\log q(z \\mid x, \\Lambda) = 0\\)</span>, so the score function is a good candidate for <span class=\"math inline\">\\(h(x)\\)</span>. Therefore our estimates become</p>\n<p><span class=\"math display\">\\[\n\\nabla_{\\Lambda} \\mathcal{L}(\\Theta, \\Lambda)\n\\approx \\frac{1}{L} \\sum_{l=1}^L \\nabla_{\\Lambda} \\log q(z^{(l)} \\mid x, \\Lambda) \\circ \\left(\\log \\frac{p(x, z^{(l)} \\mid \\Theta)}{q(z^{(l)} \\mid x, \\Lambda)} - \\alpha^* \\right)\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\circ\\)</span> is pointwise multiplication and <span class=\"math inline\">\\(\\alpha\\)</span> is a vector of <span class=\"math inline\">\\(|\\Lambda|\\)</span> components with <span class=\"math inline\">\\(\\alpha_i\\)</span> being a baseline for variational parameter <span class=\"math inline\">\\(\\Lambda_i\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\alpha^*_i = \\frac{\\text{Cov}(\\nabla_{\\Lambda_i} \\log q(z \\mid x, \\Lambda)\\left( \\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right), \\nabla_{\\Lambda_i} \\log q(z \\mid x, \\Lambda))}{\\text{Var}(\\nabla_{\\Lambda_i} \\log q(z \\mid x, \\Lambda)\\left( \\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right))}\n\\]</span></p>\n<h3>\nNeural Variational Inference and Learning\n</h3>\n<p>Hoooray, neural networks! In this section I’ll briefly describe a variance reduction technique coined by A. Mnih and K. Gregor in <a href=\"https://arxiv.org/abs/1402.0030\">Neural Variational Inference and Learning in Belief Networks</a>. The idea is surprisingly simple: why not learn a baseline <span class=\"math inline\">\\(\\alpha\\)</span> using a neural network?</p>\n<p><span class=\"math display\">\\[\n\\nabla_{\\Lambda} \\mathcal{L}(\\Theta, \\Lambda)\n\\approx \\frac{1}{L} \\sum_{l=1}^L \\nabla_{\\Lambda} \\log q(z^{(l)} \\mid x, \\Lambda) \\circ \\left(\\log \\frac{p(x, z^{(l)} \\mid \\Theta)}{q(z^{(l)} \\mid x, \\Lambda)} - \\alpha^* - \\alpha(x) \\right)\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\alpha(x)\\)</span> is a neural network trained to minimize</p>\n<p><span class=\"math display\">\\[\n \\mathbb{E}_{q(z \\mid x, \\Lambda)} \\left( \\log \\frac{p(x, z^{(l)} \\mid \\Theta)}{q(z^{(l)} \\mid x, \\Lambda)} - \\alpha^* - \\alpha(x) \\right)^2\n\\]</span></p>\n<p>What’s the motivation of this objective? The gradient step of <span class=\"math inline\">\\(\\nabla_\\Lambda \\mathcal{L}(\\Theta, \\Lambda)\\)</span> can be seen as pushing <span class=\"math inline\">\\(q(z\\mid x, \\Lambda)\\)</span> towards <span class=\"math inline\">\\(p(x, z \\mid \\Theta)\\)</span>. Since <span class=\"math inline\">\\(q\\)</span> has to be normalized like any other proper distribution, it’s actually pushed towards the true posterior <span class=\"math inline\">\\(p(z \\mid x, \\Theta)\\)</span>. We can rewrite the gradient <span class=\"math inline\">\\(\\nabla_\\Lambda \\mathcal{L}(\\Theta, \\Lambda)\\)</span> as</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\nabla_{\\Lambda} \\mathcal{L}(\\Theta, \\Lambda)\n&= \\mathbb{E}_{q} \\left[\\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) \\left(\\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda) \\right) \\right] \\\\\n&= \\mathbb{E}_{q} \\left[\\nabla_{\\Lambda} \\log q(z \\mid x, \\Lambda) \\left(\\log p(z \\mid x, \\Theta) - \\log q(z \\mid x, \\Lambda) + \\log p(x \\mid \\Theta) \\right) \\right]\n\\end{align}\n\\]</span></p>\n<p>While this additional <span class=\"math inline\">\\(\\log p(x \\mid \\Theta)\\)</span> term does not contribute to the expectation, it affects the variance on the estimator. Therefore, <span class=\"math inline\">\\(\\alpha(x)\\)</span> is supposed to estimate the marginal log-likelihood <span class=\"math inline\">\\(\\log p(x \\mid \\Theta)\\)</span>.</p>\n<p>The paper also lists several other variance reduction techniques that can be used in combination with the neural network-based baseline:</p>\n<ul>\n<li>\n<strong>Constant baseline</strong> — analogue of <em>Control Variates</em>, uses running average of <span class=\"math inline\">\\(\\log p(x, z \\mid \\Theta) - \\log q(z \\mid x, \\Lambda)\\)</span> as a baseline\n</li>\n<li>\n<strong>Variance normalization</strong> — normalizes the learning signal to unit variance, equivalent to adaptive learning rate\n</li>\n<li>\n<strong>Local learning signals</strong> — falls out of the scope of this post as requires it model-specific analysis and alternations, and can’t be used in Blackbox regime\n</li>\n</ul>",
  "pubDate": "Tue, 05 Jul 2016 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2016-07-05-neural-variational-inference-blackbox.html",
  "dc:creator": "Artem"
}