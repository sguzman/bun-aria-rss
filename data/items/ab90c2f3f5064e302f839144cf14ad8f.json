{
  "title": "Denoising Dirty Documents: Part 11",
  "link": "https://colinpriest.com/2015/11/08/denoising-dirty-documents-part-11/",
  "comments": "https://colinpriest.com/2015/11/08/denoising-dirty-documents-part-11/#comments",
  "dc:creator": "Colin Priest",
  "pubDate": "Sun, 08 Nov 2015 09:45:18 +0000",
  "category": [
    "Adaptive Thresholding",
    "Background Removal",
    "Deep Learning",
    "Edge Detection",
    "h2o",
    "Image Processing",
    "Kaggle",
    "Machine Learning",
    "Median Filter",
    "Morphology",
    "R"
  ],
  "guid": "http://colinpriest.com/?p=578",
  "description": "In my last blog I showed how to use convolutional neural networks to build a model that removed stains from &#8230;<p><a href=\"https://colinpriest.com/2015/11/08/denoising-dirty-documents-part-11/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p>In my <a href=\"https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/\" target=\"_blank\">last blog</a> I showed how to use convolutional neural networks to build a model that removed stains from an image. While convolutional neural networks seem to be well suited for image processing, in this competition I found that deep neural networks performed better. In this blog I show how to build these models.</p>\n<p><a href=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png\"><img loading=\"lazy\" data-attachment-id=\"579\" data-permalink=\"https://colinpriest.com/2015/11/08/denoising-dirty-documents-part-11/warnh022-deep-water/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png\" data-orig-size=\"3046,2095\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"warnH022 &#8211; deep water\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=300\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=529\" class=\"alignnone size-medium wp-image-579\" src=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=300\" alt=\"warnH022 - deep water\" width=\"300\" height=\"206\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=300 300w, https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=600 600w, https://colinpriestdotcom.files.wordpress.com/2015/11/warnh022-deep-water.png?w=150 150w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<p>Since I wanted to use R, have limited RAM and I don&#8217;t have a powerful GPU, I chose to use <a href=\"http://h2o.ai/\" target=\"_blank\">h2o</a> to build the models. That way I could do the feature engineering in R, pass the data to h2o, let h2o build a model, then get the predicted values back in R. The memory management would be done in h2o, which uses deep learning algorithms that adjust the RAM constraints. So I guess this combination of deep learning and h2o could be called &#8220;deep water&#8221; <img src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png\" alt=\"ðŸ˜‰\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n<p>For my final competition submission I used an ensemble of models, including 3 deep learning models built with R and h2o. Each of the 3 deep learning models used different feature engineering:</p>\n<ul>\n<li>median based feature engineering</li>\n<li>edge based feature engineering</li>\n<li>threshold based feature engineering</li>\n</ul>\n<p>This blog shows the details of the median based model. I leave it to the reader to implement the edge based and threshold based models using the image processing scripts from my earlier blogs in this series.</p>\n<p>If you don&#8217;t already have h2o installed on your computer, then you can install it directly from R. At the time of writing this blog, you could install h2o using the following script:</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\n# The following two commands remove any previously installed H2O packages for R.\nif (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload=TRUE) }\nif (\"h2o\" %in% rownames(installed.packages())) { remove.packages(\"h2o\") }\n\n# Next, we download packages that H2O depends on.\nif (! (\"methods\" %in% rownames(installed.packages()))) { install.packages(\"methods\") }\nif (! (\"statmod\" %in% rownames(installed.packages()))) { install.packages(\"statmod\") }\nif (! (\"stats\" %in% rownames(installed.packages()))) { install.packages(\"stats\") }\nif (! (\"graphics\" %in% rownames(installed.packages()))) { install.packages(\"graphics\") }\nif (! (\"RCurl\" %in% rownames(installed.packages()))) { install.packages(\"RCurl\") }\nif (! (\"jsonlite\" %in% rownames(installed.packages()))) { install.packages(\"jsonlite\") }\nif (! (\"tools\" %in% rownames(installed.packages()))) { install.packages(\"tools\") }\nif (! (\"utils\" %in% rownames(installed.packages()))) { install.packages(\"utils\") }\n\n# Now we download, install and initialize the H2O package for R.\ninstall.packages(\"h2o\", type=\"source\", repos=(c(\"http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/3/R\")))\n</pre>\n<p>That script will need to be changed as new versions of h2o are released. So use the latest instructions shownÂ <a href=\"http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/3/index.html\" target=\"_blank\">here</a>.</p>\n<p>Once h2o is installed, you can interface with h2o from R using the CRAN package.</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\ninstall.packages(\"h2o\")\nlibrary(h2o)\n\n</pre>\n<p>Median based image processing is used for feature engineering in this example, but you could use any combination of image processing techniques for your feature engineering. I got better performance using separate deep learning models for different types of image processing, but that may be because I had limited computing resources. If you have more computing resources than me, then maybe you will be successful with a single large model that uses all of the image processing techniques to create features.</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\n# a function to turn a matrix image into a vector\nimg2vec = function(img)\n{\n return (matrix(img, nrow(img) * ncol(img), 1))\n}\n \nmedian_Filter = function(img, filterWidth)\n{\n pad = floor(filterWidth / 2)\n padded = matrix(NA, nrow(img) + 2 * pad, ncol(img) + 2 * pad)\n padded[pad + seq_len(nrow(img)), pad + seq_len(ncol(img))] = img\n \n tab = matrix(0, nrow(img) * ncol(img), filterWidth * filterWidth)\n k = 1\n for (i in seq_len(filterWidth))\n {\n for (j in seq_len(filterWidth))\n {\n tab[,k] = img2vec(padded[i - 1 + seq_len(nrow(img)), j - 1 + seq_len(ncol(img))])\n k = k + 1\n }\n }\n \n filtered = unlist(apply(tab, 1, function(x) median(x[!is.na(x)])))\n return (matrix(filtered, nrow(img), ncol(img)))\n}\n \n# a function that uses median filter to get the background then finds the dark foreground\nbackground_Removal = function(img)\n{\n w = 5\n p = 1.39\n th = 240\n \n # the background is found via a median filter\n background = median_Filter(img, w)\n \n # the foreground is darker than the background\n foreground = img / background\n foreground[foreground > 1] = 1\n \n foreground2 = foreground ^ p\n foreground2[foreground2 >= (th / 255)] = 1\n \n return (matrix(foreground2, nrow(img), ncol(img)))\n} \n\nimg2tab = function(imgX, f)\n{\n median5 = img2vec(median_Filter(imgX, 5))\n median17 = img2vec(median_Filter(imgX, 17))\n median25 = img2vec(median_Filter(imgX, 25))\n backgroundRemoval = img2vec(background_Removal(imgX))\n foreground = readPNG(file.path(foregroundFolder, f))\n \n # pad out imgX\n padded = matrix(0, nrow(imgX) + padding * 2, ncol(imgX) + padding * 2)\n offsets = expand.grid(seq_len(2*padding+1), seq_len(2*padding+1))\n \n # raw pixels window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = imgX\n x = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n \n # x2 window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median5\n x2 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n \n # x3 window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median17\n x3 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n \n # x4 window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median25\n x4 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n \n # x5 window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = backgroundRemoval\n x5 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n \n # x6 window\n padded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = foreground\n x6 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n dat = data.table(cbind(x, x2, x3, x4, x5, x6))\n setnames(dat,c(\n paste(\"x\", seq_len((2*padding+1)^2), sep=\"\"), \n paste(\"median5\", seq_len((2*padding+1)^2), sep=\"\"),\n paste(\"median17\", seq_len((2*padding+1)^2), sep=\"\"),\n paste(\"median25\", seq_len((2*padding+1)^2), sep=\"\"),\n paste(\"backgroundRemoval\", seq_len((2*padding+1)^2), sep=\"\"),\n paste(\"foreground\", seq_len((2*padding+1)^2), sep=\"\")\n ))\n \n return (dat)\n}\n</pre>\n<p>If you&#8217;ve been following my blog, then you will see that there&#8217;s nothing new in the two image processing functions shown above.</p>\n<p>To build the model you will need to startÂ h2o, import the data and tell h2o to create a deep learning model.</p>\n<pre class=\"brush: r; title: ; notranslate\">\nh2oServer = h2o.init(nthreads = 6, max_mem_size = \"10G\")\n\ntrainData = h2o.importFile(h2oServer, path = outPath)\ntestData = h2o.importFile(h2oServer, path = outPath2)\n\nmodel.dl.median <- h2o.deeplearning(x = 2:ncol(trainData), y = 1, training_frame = trainData, validation_frame = testData,\n score_training_samples = 0, \n overwrite_with_best_model = TRUE,\n activation = \"Rectifier\", seed = 1,\n hidden = c(200, 200,200), epochs = 15,\n adaptive_rate = TRUE, initial_weight_distribution = \"UniformAdaptive\", loss = \"MeanSquare\",\n fast_mode = T, diagnostics = T, ignore_const_cols = T,\n force_load_balance = T)\n\n\n</pre>\n<p>You should change the h2o.init parameters according to the hardware on your computer. I&#8217;m running my model on a PC with 8 CPUsÂ and 16GB of RAM, so I left a couple of CPUs free to do the user interface and core operating systemÂ functionality, plus some RAM for the operating system. Scale these parameters up or down if your PC specifications are more or less powerful than mine.</p>\n<p>The model may take a few hours to fit. During that time R will not do anything. So if you want to see how the model is progressing, then point your browser to your localhost (port 54321 on my PC, but maybe a different portÂ on yours) and use the h2o web interface to see what is happening.</p>\n<p>You can get the predicted values using the following script:</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\nfilenames = list.files(dirtyFolder)\nfor (f in filenames)\n{\n print(f)\n imgX = readPNG(file.path(dirtyFolder, f))\n\ndat = img2tab(imgX, f)\n\nx.h2o = as.h2o(h2oServer, dat)\n predict.dl = as.data.frame(h2o.predict(model.dl.median, newdata = x.h2o))\n imgOut = matrix(as.numeric(predict.dl$predict), nrow(imgX), ncol(imgX))\n \n # correct the pixel brightnesses that are out of bounds\n imgOut[imgOut > 1] = 1\n imgOut[imgOut < 0] = 0\n\nwritePNG(imgOut, file.path(outFolder, f))\n}\n\nh2o.shutdown()\n\n</pre>\n<p>Running predictions is as simple as creating a data file, importing it to h2o, and then asking h2o to give you the predicted values from your already fitted model. I found that some of the raw predicted values were out of the [0, 1] range, and improved my leaderboard score by limiting the predicted values to lie within this range.</p>\n<p>You do not need to shut down h2o after you finish running a model. In fact you may wish to leave it running so that you can do model diagnostics or run more predictions.</p>\n<p>If you wish to save a copy of your model, for later reuse, then you can use the following syntax:</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\nmodelPath = h2o.saveModel(model.dl.median, dir = \"./model\", name = \"model_dnn_median\", force = TRUE)\n\n</pre>\n<p>Just remember that h2o needs to be running when you save models or load previously saved models.</p>\n<p>In my next, and final, blog in this series, I will show how to take advantage of the second information leakage in the competition.</p>\n<p>For those who want the entire R script to try out for themselves, here it is:</p>\n<pre class=\"brush: r; title: ; notranslate\">\n\ninstall.packages(\"h2o\")\nlibrary(h2o)\nlibrary(png)\nlibrary(data.table)\n\n# a function to turn a matrix image into a vector\nimg2vec = function(img)\n{\nreturn (matrix(img, nrow(img) * ncol(img), 1))\n}\n\nmedian_Filter = function(img, filterWidth)\n{\npad = floor(filterWidth / 2)\npadded = matrix(NA, nrow(img) + 2 * pad, ncol(img) + 2 * pad)\npadded[pad + seq_len(nrow(img)), pad + seq_len(ncol(img))] = img\n\ntab = matrix(0, nrow(img) * ncol(img), filterWidth * filterWidth)\nk = 1\nfor (i in seq_len(filterWidth))\n{\nfor (j in seq_len(filterWidth))\n{\ntab[,k] = img2vec(padded[i - 1 + seq_len(nrow(img)), j - 1 + seq_len(ncol(img))])\nk = k + 1\n}\n}\n\nfiltered = unlist(apply(tab, 1, function(x) median(x[!is.na(x)])))\nreturn (matrix(filtered, nrow(img), ncol(img)))\n}\n\n# a function that uses median filter to get the background then finds the dark foreground\nbackground_Removal = function(img)\n{\nw = 5\np = 1.39\nth = 240\n\n# the background is found via a median filter\nbackground = median_Filter(img, w)\n\n# the foreground is darker than the background\nforeground = img / background\nforeground[foreground > 1] = 1\n\nforeground2 = foreground ^ p\nforeground2[foreground2 >= (th / 255)] = 1\n\nreturn (matrix(foreground2, nrow(img), ncol(img)))\n}\n\ndirtyFolder = \"./data/train\"\ncleanFolder = \"./data/train_cleaned\"\noutFolder = \"./model\"\nforegroundFolder = \"./foreground/train foreground\"\n\noutPath = file.path(outFolder, \"trainingdata.csv\")\noutPath2 = file.path(outFolder, \"testdata.csv\")\nfilenames = list.files(dirtyFolder)\npadding = 2\nset.seed(1)\nlibrary(h2o)\nh2oServer = h2o.init(nthreads = 15, max_mem_size = \"110G\")\n\ntrainData = h2o.importFile(h2oServer, path = outPath)\ntestData = h2o.importFile(h2oServer, path = outPath2)\n\nmodel.dl.median <- h2o.deeplearning(x = 2:ncol(trainData), y = 1, training_frame = trainData, validation_frame = testData,\nscore_training_samples = 0,\noverwrite_with_best_model = TRUE,\nactivation = \"Rectifier\", seed = 1,\nhidden = c(200, 200,200), epochs = 15,\nadaptive_rate = TRUE, initial_weight_distribution = \"UniformAdaptive\", loss = \"MeanSquare\",\nfast_mode = T, diagnostics = T, ignore_const_cols = T,\nforce_load_balance = T)\n\nsummary(model.dl)\n\nmodelPath = h2o.saveModel(model.dl.median, dir = \"./model\", name = \"model_dnn_median\", force = TRUE)\n\noutFolder = \"./model/training data\"\n\nimg2tab = function(imgX, f)\n{\nmedian5 = img2vec(median_Filter(imgX, 5))\nmedian17 = img2vec(median_Filter(imgX, 17))\nmedian25 = img2vec(median_Filter(imgX, 25))\nbackgroundRemoval = img2vec(background_Removal(imgX))\nforeground = readPNG(file.path(foregroundFolder, f))\n\n# pad out imgX\npadded = matrix(0, nrow(imgX) + padding * 2, ncol(imgX) + padding * 2)\noffsets = expand.grid(seq_len(2*padding+1), seq_len(2*padding+1))\n\n# raw pixels window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = imgX\nx = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n# x2 window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median5\nx2 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n# x3 window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median17\nx3 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n# x4 window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = median25\nx4 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n# x5 window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = backgroundRemoval\nx5 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\n# x6 window\npadded[padding + seq_len(nrow(imgX)), padding + seq_len(ncol(imgX))] = foreground\nx6 = sapply(seq_len((2*padding+1)^2), function(x) img2vec(padded[offsets[x, 2] - 1 + seq_len(nrow(imgX)), offsets[x, 1] - 1 + seq_len(ncol(imgX))]))\n\ndat = data.table(cbind(x, x2, x3, x4, x5, x6))\nsetnames(dat,c(\npaste(\"x\", seq_len((2*padding+1)^2), sep=\"\"),\npaste(\"median5\", seq_len((2*padding+1)^2), sep=\"\"),\npaste(\"median17\", seq_len((2*padding+1)^2), sep=\"\"),\npaste(\"median25\", seq_len((2*padding+1)^2), sep=\"\"),\npaste(\"backgroundRemoval\", seq_len((2*padding+1)^2), sep=\"\"),\npaste(\"foreground\", seq_len((2*padding+1)^2), sep=\"\")\n))\n\nreturn (dat)\n}\n\ndirtyFolder = \"./data/test\"\noutFolder = \"./model/test data\"\nforegroundFolder = \"./foreground/test foreground\"\nfilenames = list.files(dirtyFolder)\nfor (f in filenames)\n{\nprint(f)\nimgX = readPNG(file.path(dirtyFolder, f))\n\ndat = img2tab(imgX, f)\n\nx.h2o = as.h2o(h2oServer, dat)\npredict.dl = as.data.frame(h2o.predict(model.dl.median, newdata = x.h2o))\nimgOut = matrix(as.numeric(predict.dl$predict), nrow(imgX), ncol(imgX))\n\n# correct the pixel brightnesses that are out of bounds\nimgOut[imgOut > 1] = 1\nimgOut[imgOut < 0] = 0\n\nwritePNG(imgOut, file.path(outFolder, f))\n}\n\nh2o.shutdown()\n\n</pre>\n",
  "wfw:commentRss": "https://colinpriest.com/2015/11/08/denoising-dirty-documents-part-11/feed/",
  "slash:comments": 1,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "warnH022 - deep water"
    },
    {
      "media:title": "colinpriest1966"
    },
    {
      "media:title": "warnH022 - deep water"
    }
  ]
}