{
  "title": "Guest post by Julien Mairal: A Kernel Point of View on Convolutional Neural Networks, part I",
  "link": "https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/",
  "comments": "https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/#comments",
  "dc:creator": "Sebastien Bubeck",
  "pubDate": "Wed, 10 Jul 2019 15:20:34 +0000",
  "category": "Machine learning",
  "guid": "https://blogs.princeton.edu/imabandit/?p=1382",
  "description": "<p>&#160; &#160; I (n.b., Julien Mairal) have been interested in drawing links between neural networks and kernel methods for some time, and I am grateful to Sebastien for giving me the opportunity to say a few words about it on &#8230; <a href=\"https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>\n",
  "content:encoded": "<p>&nbsp;</p>\n<p><a href=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig.jpg\" class=\"liimagelink\"><img decoding=\"async\" loading=\"lazy\" class=\"alignnone wp-image-1393\" src=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig-300x171.jpg\" alt=\"\" width=\"646\" height=\"368\" srcset=\"https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig-300x171.jpg 300w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig-768x438.jpg 768w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig-1024x584.jpg 1024w, https://blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig.jpg 1707w\" sizes=\"(max-width: 646px) 100vw, 646px\" /></a></p>\n<p>&nbsp;</p>\n<p>I (<em>n.b., <a href=\"https://lear.inrialpes.fr/people/mairal/\" class=\"liinternal\">Julien Mairal</a></em>) have been interested in drawing links between neural networks and kernel methods for some time, and I am grateful to Sebastien for giving me the opportunity to say a few words about it on his blog. My initial motivation was not to provide another &#8220;why deep learning works&#8221; theory, but simply to encode into kernel methods a few successful principles from convolutional neural networks (CNNs), such as the ability to model the local stationarity of natural images at multiple scales&#8212;we may call that modeling receptive fields&#8212;along with feature compositions and invariant representations. There was also something challenging in trying to reconcile end-to-end deep neural networks and non-parametric methods based on kernels that typically decouple data representation from the learning task.</p>\n<p>The main goal of this blog post is then to discuss the construction of a particular multilayer kernel for images that encodes the previous principles, derive some invariance and stability properties for CNNs, and also present a simple mechanism to perform feature learning in reproducing kernel Hilbert spaces. In other words, we should not see any intrinsic contradiction between kernels and representation learning.</p>\n<p><strong>Preliminaries on kernel methods</strong></p>\n<p>Given data living in a set <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"15\" style=\"vertical-align: 0px;\"/>, a positive definite kernel <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f0d19a1401658006e20eb7aff7c20689_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#75;&#58;&#32;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;&#32;&#92;&#116;&#105;&#109;&#101;&#115;&#32;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;&#32;&#92;&#116;&#111;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"13\" width=\"124\" style=\"vertical-align: -1px;\"/> implicitly defines a Hilbert space <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d8c7ae0e5e08bd1b3f5ef053720bf142_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"15\" style=\"vertical-align: 0px;\"/> of functions from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"15\" style=\"vertical-align: 0px;\"/> to <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b2c3c459eddec9847f841b19a2274a3d_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"13\" style=\"vertical-align: 0px;\"/>, called reproducing kernel Hilbert space (RKHS), along with a mapping function <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-075eb9a40ac7f19fc1d24932d430cf57_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#58;&#32;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;&#32;&#92;&#116;&#111;&#32;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"84\" style=\"vertical-align: -4px;\"/>.</p>\n<p>A predictive model <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"10\" style=\"vertical-align: -4px;\"/> in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d8c7ae0e5e08bd1b3f5ef053720bf142_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"15\" style=\"vertical-align: 0px;\"/> associates to every point <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"10\" style=\"vertical-align: 0px;\"/> a label in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b2c3c459eddec9847f841b19a2274a3d_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"13\" style=\"vertical-align: 0px;\"/>, and admits a simple form <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-af2242f529038b9f66bdd803a7fcf32d_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;&#40;&#120;&#41;&#32;&#61;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#102;&#44;&#32;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;&#41;&#32;&#92;&#114;&#97;&#110;&#103;&#108;&#101;&#95;&#123;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"138\" style=\"vertical-align: -5px;\"/>. Then, Cauchy-Schwarz inequality gives us a first basic stability property</p>\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 21px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ba1f97e9889116f67e3caf7d27f6dca2_l3.png\" height=\"21\" width=\"418\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91; &#92;&#102;&#111;&#114;&#97;&#108;&#108;&#32;&#120;&#44;&#32;&#120;'&#92;&#105;&#110;&#32;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#88;&#125;&#44;&#126;&#126;&#126;&#126;&#126;&#32;&#124;&#102;&#40;&#120;&#41;&#45;&#102;&#40;&#120;'&#41;&#124;&#32;&#92;&#108;&#101;&#113;&#32;&#92;&#124;&#102;&#92;&#124;&#95;&#123;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#125;&#32;&#92;&#124;&#32;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;&#41;&#32;&#45;&#32;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;'&#41;&#92;&#124;&#95;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#46; &#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p>This relation exhibits a discrepancy between neural networks and kernel methods. Whereas neural networks optimize the data representation for a specific task, the term on the right involves the product of two quantities where data representation and learning are decoupled:</p>\n<p><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9b7eefc0051c0b86a82ee0265f44a085_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;&#41;&#45;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;'&#41;&#92;&#124;&#95;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"125\" style=\"vertical-align: -5px;\"/> is a distance between two data representations <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-41c6c60616e1acea2bdd02deee51011e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;&#41;&#44;&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;'&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"83\" style=\"vertical-align: -4px;\"/>, which are independent of the learning process, and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c755a8a9349d0895075e9494d1b11fc1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#124;&#102;&#92;&#124;&#95;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"38\" style=\"vertical-align: -5px;\"/> is a norm on the model <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"10\" style=\"vertical-align: -4px;\"/> (typically optimized over data) that acts as a measure of complexity.</p>\n<p>Thinking about neural networks in terms of kernel methods then requires defining the underlying representation <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eb419c2adecf84ed9a2d9693bc58d101_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#40;&#120;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"35\" style=\"vertical-align: -4px;\"/>, which can only depend on the network architecture, and the model <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"10\" style=\"vertical-align: -4px;\"/>, which will be parametrized by (learned) network&#8217;s weights.</p>\n<p><strong>Building a convolutional kernel for convolutional neural networks</strong></p>\n<p>Following <a href=\"http://jmlr.org/papers/volume20/18-190/18-190.pdf\" class=\"lipdf\">Alberto Bietti&#8217;s paper</a>, we now consider the direct construction of a multilayer convolutional kernel for images. Given a two-dimensional image <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-55b536a6647748d6c0c6b58015805c68_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/>, the main idea is to build a sequence of &#8220;feature maps&#8221; <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e504020251e8444e8047821206317fa_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#49;&#44;&#120;&#95;&#50;&#44;&#92;&#108;&#100;&#111;&#116;&#115;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"71\" style=\"vertical-align: -4px;\"/> that are two-dimensional spatial maps carrying information about image neighborhoods (a.k.a receptive fields) at every location. As we proceed in this sequence, the goal is to model larger neighborhoods with more &#8220;invariance&#8221;.</p>\n<p>Formally, an input image <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-55b536a6647748d6c0c6b58015805c68_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/> is represented as a square-integrable function in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b1cdcac953d52ed35e77925a243c3df7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#76;&#94;&#50;&#40;&#92;&#79;&#109;&#101;&#103;&#97;&#44;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#48;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"76\" style=\"vertical-align: -4px;\"/>, where <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec0c546b6596f336d8e1d41bb064b951_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#79;&#109;&#101;&#103;&#97;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"12\" style=\"vertical-align: 0px;\"/> is a set of pixel coordinates, and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c58a47e1230e20fa0f090bbe6e111ba7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/> is a Hilbert space. <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec0c546b6596f336d8e1d41bb064b951_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#79;&#109;&#101;&#103;&#97;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"12\" style=\"vertical-align: 0px;\"/> may be a discrete grid or a continuous domain such as <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d5abe0f29e8cc710ae26f4f0af5a0859_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#94;&#50;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"20\" style=\"vertical-align: 0px;\"/>, and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c58a47e1230e20fa0f090bbe6e111ba7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#48;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/> may simply be <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-97886402213f48c46e631e5331a34035_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#94;&#51;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"20\" style=\"vertical-align: 0px;\"/> for RGB images. Then, a feature map <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ad23c5c360c3f33031a5d000d37416f_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/> in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-66fdb69a62e8ec8647eac89f54998a71_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#76;&#94;&#50;&#40;&#92;&#79;&#109;&#101;&#103;&#97;&#44;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#107;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"77\" style=\"vertical-align: -4px;\"/> is obtained from a previous layer <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/> as follows:</p>\n<ul>\n<li><em> modeling larger neighborhoods than in the previous layer:</em> we map neighborhoods (patches) from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/> to a new Hilbert space <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/>. Concretely, we define a homogeneous dot-product kernel between patches <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ce80943b7f55934d998e09542933b73e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#122;&#44;&#32;&#122;'\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"30\" style=\"vertical-align: -4px;\"/> from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/>:\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 43px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e98e6c584e7aa34a129d04fa46a6981c_l3.png\" height=\"43\" width=\"304\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91;&#32;&#75;&#95;&#107;&#40;&#122;&#44;&#122;'&#41;&#32;&#61;&#32;&#92;&#124;&#122;&#92;&#124;&#32;&#92;&#124;&#122;'&#92;&#124;&#32;&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;&#32;&#92;&#108;&#101;&#102;&#116;&#40;&#32;&#92;&#108;&#101;&#102;&#116;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#122;&#125;&#123;&#92;&#124;&#122;&#92;&#124;&#125;&#44;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#122;'&#125;&#123;&#92;&#124;&#122;'&#92;&#124;&#125;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#92;&#114;&#97;&#110;&#103;&#108;&#101;&#32;&#92;&#114;&#105;&#103;&#104;&#116;&#41;&#44;&#32;&#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p> where <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b62527a227d32e3e2f43b8b9b2b31ad5_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#46;&#32;&#44;&#32;&#46;&#32;&#92;&#114;&#97;&#110;&#103;&#108;&#101;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"29\" style=\"vertical-align: -5px;\"/> is an inner-product derived from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec7eee8a3bac08b4c319cfce53408682_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"39\" style=\"vertical-align: -4px;\"/>, and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-684fcf23472c51919624049fb4e0129a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"11\" width=\"17\" style=\"vertical-align: -3px;\"/> is a non-linear function that ensures positive definiteness, <em>e.g.</em>, <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-46c64b76ccc9f508d30fec2fb80e244d_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#107;&#97;&#112;&#112;&#97;&#95;&#107;&#40;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#117;&#44;&#117;'&#92;&#114;&#97;&#110;&#103;&#108;&#101;&#32;&#41;&#32;&#61;&#32;&#101;&#94;&#123;&#92;&#97;&#108;&#112;&#104;&#97;&#32;&#40;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#117;&#44;&#117;'&#92;&#114;&#97;&#110;&#103;&#108;&#101;&#32;&#45;&#49;&#41;&#125;&#32;&#61;&#32;&#101;&#94;&#123;&#45;&#92;&#102;&#114;&#97;&#99;&#123;&#92;&#97;&#108;&#112;&#104;&#97;&#125;&#123;&#50;&#125;&#92;&#124;&#117;&#45;&#117;'&#92;&#124;&#94;&#50;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"23\" width=\"289\" style=\"vertical-align: -5px;\"/> for vectors <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0b88ce07daf9a52ba8a46659cff355fd_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#117;&#44;&#32;&#117;'\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"32\" style=\"vertical-align: -4px;\"/> with unit norm, see <a href=\"http://jmlr.org/papers/volume20/18-190/18-190.pdf\" class=\"lipdf\">this paper</a>. By doing so, we implicitly define a kernel mapping <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-243ed60e88d807834cd7cb1e1fbe0658_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#118;&#97;&#114;&#112;&#104;&#105;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"19\" style=\"vertical-align: -4px;\"/> that maps patches from <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/> to a new Hilbert space <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"22\" style=\"vertical-align: -3px;\"/>. This mechanism is illustrated in the picture at the beginning of the post, and produces a spatial map that carries these patch representations.</li>\n<li><em>increasing invariance:</em> to gain invariance to small deformations, we smooth~<img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/> with a linear filter, as shown in the picture at the beginning of the post, which may be interpreted as anti-aliasing (in terms of signal processing) or linear pooling (in terms of neural networks).</li>\n</ul>\n<p>Formally, the previous construction amounts to applying operators <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4726bbf70431cf284be54bbc6a04ad60_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#80;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"18\" style=\"vertical-align: -3px;\"/> (patch extraction), <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78f07026bc8c5150a11bf9e00756b7a7_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#77;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"24\" style=\"vertical-align: -3px;\"/> (kernel mapping), and <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-866de181a59a21d2ca2306a9adbd9bc1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#65;&#95;&#107;\" title=\"Rendered by QuickLaTeX.com\" height=\"15\" width=\"20\" style=\"vertical-align: -3px;\"/> (smoothing/pooling operator) to <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#120;&#95;&#123;&#107;&#45;&#49;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"12\" width=\"35\" style=\"vertical-align: -4px;\"/> such that the <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"11\" style=\"vertical-align: 0px;\"/>-th layer representation can be written as</p>\n<p class=\"ql-center-displayed-equation\" style=\"line-height: 21px;\"><span class=\"ql-right-eqno\"> &nbsp; </span><span class=\"ql-left-eqno\"> &nbsp; </span><img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e8c2d99cc679426d1af08e6d15510211_l3.png\" height=\"21\" width=\"437\" class=\"ql-img-displayed-equation \" alt=\"&#92;&#91; &#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#95;&#48;&#41;&#61;&#32;&#120;&#95;&#110;&#61; &#65;&#95;&#110;&#32;&#77;&#95;&#110;&#32;&#80;&#95;&#110;&#32;&#92;&#108;&#100;&#111;&#116;&#115;&#32;&#65;&#95;&#49;&#32;&#77;&#95;&#49;&#32;&#80;&#95;&#49;&#32;&#120;&#95;&#48;&#126;&#126;&#126;&#92;&#116;&#101;&#120;&#116;&#123;&#105;&#110;&#125;&#126;&#126;&#126;&#126;&#76;&#94;&#50;&#40;&#92;&#79;&#109;&#101;&#103;&#97;&#44;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#110;&#41;&#46; &#92;&#93;\" title=\"Rendered by QuickLaTeX.com\"/></p>\n<p>We may finally define a kernel for images as <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-16be084d5dd2ed3d7a18cdcf70c33fe2_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#75;&#125;&#95;&#110;&#40;&#120;&#95;&#48;&#44;&#120;&#95;&#48;'&#41;&#61;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#95;&#48;&#41;&#44;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#95;&#48;'&#41;&#32;&#92;&#114;&#97;&#110;&#103;&#108;&#101;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"231\" style=\"vertical-align: -5px;\"/>, whose RKHS contains the functions <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-95aa9a4388dd5f5da6292875abe6596a_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;&#95;&#119;&#40;&#120;&#95;&#48;&#41;&#32;&#61;&#32;&#92;&#108;&#97;&#110;&#103;&#108;&#101;&#32;&#119;&#32;&#44;&#32;&#92;&#80;&#104;&#105;&#95;&#110;&#40;&#120;&#95;&#48;&#41;&#32;&#92;&#114;&#97;&#110;&#103;&#108;&#101;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"162\" style=\"vertical-align: -5px;\"/> for <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#119;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"13\" style=\"vertical-align: 0px;\"/> in <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-502d385c60e5ecdb1a0f26ee770d30b1_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#76;&#94;&#50;&#40;&#92;&#79;&#109;&#101;&#103;&#97;&#44;&#92;&#109;&#97;&#116;&#104;&#99;&#97;&#108;&#123;&#72;&#125;&#95;&#110;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"19\" width=\"77\" style=\"vertical-align: -4px;\"/>. Note now that we have introduced a concept of image representation <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#92;&#80;&#104;&#105;&#95;&#110;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"21\" style=\"vertical-align: -3px;\"/>, which only depends on some network architecture (amounts of pooling, patch size), and predictive model <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fb636251e88ba51d909c76c1110eed5e_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#102;&#95;&#119;\" title=\"Rendered by QuickLaTeX.com\" height=\"16\" width=\"19\" style=\"vertical-align: -4px;\"/> parametrized by <img decoding=\"async\" loading=\"lazy\" src=\"https://blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png\" class=\"ql-img-inline-formula \" alt=\"&#119;\" title=\"Rendered by QuickLaTeX.com\" height=\"8\" width=\"13\" style=\"vertical-align: 0px;\"/>.</p>\n<p>From such a construction, we will now derive stability results for classical convolutional neural networks (CNNs) and then derive non-standard CNNs based on kernel approximations that we call convolutional kernel networks (CKNs).</p>\n<p>&nbsp;</p>\n<p>Next week, we will see how to perform feature (end-to-end) learning with the previous kernel representation, and also discuss other classical links between neural networks and kernel methods.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n",
  "wfw:commentRss": "https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/feed/",
  "slash:comments": 36
}