{
  "title": "Improved Seeding For Clustering With K-Means++",
  "link": "https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/",
  "comments": "https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/#comments",
  "dc:creator": "datasciencelab",
  "pubDate": "Wed, 15 Jan 2014 14:06:49 +0000",
  "category": [
    "Experiments",
    "clustering",
    "initialization",
    "k-means"
  ],
  "guid": "http://datasciencelab.wordpress.com/?p=598",
  "description": "Clustering data into subsets is an important task for many data science applications. At The Data Science Lab we have illustrated how Lloyd&#8217;s algorithm for k-means clustering works, including snapshots of python code to visualize the iterative clustering steps. One of the issues with the procedure is that this algorithm does not supply information as [&#8230;]",
  "content:encoded": "<p>Clustering data into subsets is an important task for many data science applications. At The Data Science Lab we have illustrated how <a title=\"Clustering With K-Means in&nbsp;Python\" href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">Lloyd&#8217;s algorithm for k-means clustering</a> works, including snapshots of python code to <a href=\"https://datasciencelab.files.wordpress.com/2013/12/p_n200_k3_g.gif\">visualize the iterative clustering steps</a>. One of the issues with the procedure is that</p>\n<blockquote><p>this algorithm does not supply information as to which K for the k-means is optimal; that has to be found out by alternative methods,</p></blockquote>\n<p>so that we went a <a href=\"https://twitter.com/DataScienceLab/status/416533810331656192\">step further</a> and coded up the <a title=\"Finding the K in K-Means&nbsp;Clustering\" href=\"https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/\">gap statistic to find the proper k for k-means clustering</a>. In combination with the clustering algorithm, the gap statistic allows to estimate the best value for k among those in a given range.<br />\n<a href=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k9_g.gif\"><img loading=\"lazy\" data-attachment-id=\"338\" data-permalink=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/p_n100_k9_g/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k9_g.gif\" data-orig-size=\"404,425\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"p_N100_K9_G\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k9_g.gif?w=285\" data-large-file=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k9_g.gif?w=404\" class=\"alignright  wp-image-338\" alt=\"p_N100_K9_G\" src=\"https://datasciencelab.files.wordpress.com/2013/12/p_n100_k9_g.gif?w=323&#038;h=340\" width=\"323\" height=\"340\" /></a><br />\nAn additional problem with the standard k-means procedure still remains though, as shown by the image on the right, where a poor random initialization of the centroids leads to suboptimal clustering:</p>\n<blockquote><p>If the target distribution is disjointedly clustered and only one instantiation of Lloydâ€™s algorithm is used, the danger exists that the local minimum reached is not the optimal solution.</p></blockquote>\n<p>The initialization problem for the k-means algorithm is an important practical one, and <a href=\"http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods\">has been discussed extensively</a>. It is desirable to augment the standard k-means clustering procedure with a robust initialization mechanism that guarantees convergence to the optimal solution.</p>\n<h3>k-means++: the advantages of careful seeding</h3>\n<p>A solution called <a href=\"http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf\">k-means++ was proposed in 2007 by Arthur and Vassilvitskii</a>. This algorithm comes with a theoretical guarantee to find a solution that is O(log k) competitive to the optimal k-means solution. It is also fairly simple to describe and implement. Starting with a dataset <em>X</em> of <em>N</em> points <img src=\"https://s0.wp.com/latex.php?latex=%28%5Cmathrm%7Bx%7D_1%2C+%5Cldots%2C+%5Cmathrm%7Bx%7D_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28%5Cmathrm%7Bx%7D_1%2C+%5Cldots%2C+%5Cmathrm%7Bx%7D_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Cmathrm%7Bx%7D_1%2C+%5Cldots%2C+%5Cmathrm%7Bx%7D_N%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(&#92;mathrm{x}_1, &#92;ldots, &#92;mathrm{x}_N)\" class=\"latex\" />,</p>\n<ul>\n<li>choose an initial center <img src=\"https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"c_1\" class=\"latex\" /> uniformly at random from <em>X</em>. Compute the vector containing the square distances between all points in the dataset and <img src=\"https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c_1&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"c_1\" class=\"latex\" />: <img src=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_i^2 = ||&#92;mathrm{x}_i - c_1 ||^2\" class=\"latex\" /></li>\n<li>choose a second center <img src=\"https://s0.wp.com/latex.php?latex=c_2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=c_2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c_2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"c_2\" class=\"latex\" /> from <em>X</em> randomly drawn from the probability distribution <img src=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_i^2 / &#92;sum_j D_j^2\" class=\"latex\" /></li>\n<li>recompute the distance vector as <img src=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_2+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_2+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_2+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_i^2 = &#92;mathrm{min} &#92;left(||&#92;mathrm{x}_i - c_1 ||^2, ||&#92;mathrm{x}_i - c_2 ||^2&#92;right)\" class=\"latex\" /></li>\n<li>choose a successive center <img src=\"https://s0.wp.com/latex.php?latex=c_l&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=c_l&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c_l&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"c_l\" class=\"latex\" /> and recompute the distance vector as <img src=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%5Cldots%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_l+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%5Cldots%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_l+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_i%5E2+%3D+%5Cmathrm%7Bmin%7D+%5Cleft%28%7C%7C%5Cmathrm%7Bx%7D_i+-+c_1+%7C%7C%5E2%2C+%5Cldots%2C+%7C%7C%5Cmathrm%7Bx%7D_i+-+c_l+%7C%7C%5E2%5Cright%29&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_i^2 = &#92;mathrm{min} &#92;left(||&#92;mathrm{x}_i - c_1 ||^2, &#92;ldots, ||&#92;mathrm{x}_i - c_l ||^2&#92;right)\" class=\"latex\" /></li>\n<li>when exactly <em>k</em> centers have been chosen, finalize the initialization phase and proceed with the standard k-means algorithm</li>\n</ul>\n<p>The interested reader can find a <a href=\"https://normaldeviate.wordpress.com/2012/09/30/the-remarkable-k-means/\">review of the k-means++ algorithm at normaldeviate</a>, a survey of <a href=\"http://rosettacode.org/wiki/K-means%2B%2B_clustering#Python\">implementations in several languages at rosettacode</a> and a <a href=\"https://github.com/jackmaney/k-means-plus-plus-pandas\">ready-to-use solution in pandas by Jack Maney in github</a>.</p>\n<h3>A python implementation of the k-means++ algorithm</h3>\n<p>Out python implementation of the k-means++ algorithm builds on the <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">code for standard k-means shown in the previous post</a>. The <code>KMeans</code> class defined below contains all necessary functions and methods to generate toy data and run the Lloyd&#8217;s clustering algorithm on it:</p>\n<pre class=\"brush: python; collapse: true; light: false; title: class KMeans(): (click to expand); toolbar: true; notranslate\">\nclass KMeans():\n    def __init__(self, K, X=None, N=0):\n        self.K = K\n        if X == None:\n            if N == 0:\n                raise Exception(\"If no data is provided, \\\n                                 a parameter N (number of points) is needed\")\n            else:\n                self.N = N\n                self.X = self._init_board_gauss(N, K)\n        else:\n            self.X = X\n            self.N = len(X)\n        self.mu = None\n        self.clusters = None\n        self.method = None\n\n    def _init_board_gauss(self, N, k):\n        n = float(N)/k\n        X = []\n        for i in range(k):\n            c = (random.uniform(-1,1), random.uniform(-1,1))\n            s = random.uniform(0.05,0.15)\n            x = []\n            while len(x) < n:\n                a,b = np.array([np.random.normal(c[0],s),np.random.normal(c[1],s)])\n                # Continue drawing points from the distribution in the range [-1,1]\n                if abs(a) and abs(b)<1:\n                    x.append([a,b])\n            X.extend(x)\n        X = np.array(X)[:N]\n        return X\n\n    def plot_board(self):\n        X = self.X\n        fig = plt.figure(figsize=(5,5))\n        plt.xlim(-1,1)\n        plt.ylim(-1,1)\n        if self.mu and self.clusters:\n            mu = self.mu\n            clus = self.clusters\n            K = self.K\n            for m, clu in clus.items():\n                cs = cm.spectral(1.*m/self.K)\n                plt.plot(mu[m][0], mu[m][1], 'o', marker='*', \\\n                         markersize=12, color=cs)\n                plt.plot(zip(*clus[m])[0], zip(*clus[m])[1], '.', \\\n                         markersize=8, color=cs, alpha=0.5)\n        else:\n            plt.plot(zip(*X)[0], zip(*X)[1], '.', alpha=0.5)\n        if self.method == '++':\n            tit = 'K-means++'\n        else:\n            tit = 'K-means with random initialization'\n        pars = 'N=%s, K=%s' % (str(self.N), str(self.K))\n        plt.title('\\n'.join([pars, tit]), fontsize=16)\n        plt.savefig('kpp_N%s_K%s.png' % (str(self.N), str(self.K)), \\\n                    bbox_inches='tight', dpi=200)\n\n    def _cluster_points(self):\n        mu = self.mu\n        clusters  = {}\n        for x in self.X:\n            bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]])) \\\n                             for i in enumerate(mu)], key=lambda t:t[1])[0]\n            try:\n                clusters[bestmukey].append(x)\n            except KeyError:\n                clusters[bestmukey] = [x]\n        self.clusters = clusters\n\n    def _reevaluate_centers(self):\n        clusters = self.clusters\n        newmu = []\n        keys = sorted(self.clusters.keys())\n        for k in keys:\n            newmu.append(np.mean(clusters[k], axis = 0))\n        self.mu = newmu\n\n    def _has_converged(self):\n        K = len(self.oldmu)\n        return(set([tuple(a) for a in self.mu]) == \\\n               set([tuple(a) for a in self.oldmu])\\\n               and len(set([tuple(a) for a in self.mu])) == K)\n\n    def find_centers(self, method='random'):\n        self.method = method\n        X = self.X\n        K = self.K\n        self.oldmu = random.sample(X, K)\n        if method != '++':\n            # Initialize to K random centers\n            self.mu = random.sample(X, K)\n        while not self._has_converged():\n            self.oldmu = self.mu\n            # Assign all points in X to clusters\n            self._cluster_points()\n            # Reevaluate centers\n            self._reevaluate_centers()\n</pre>\n<p>To initalize the board with n data points normally distributed around k centers, we call <code>kmeans = KMeans(k, N=n)</code>.</p>\n<pre class=\"brush: python; title: ; notranslate\">\nkmeans = KMeans(3, N=200)\nkmeans.find_centers()\nkmeans.plot_board()\n</pre>\n<p>The snippet above creates a board with 200 points around 3 clusters. The call to the <code>find_centers()</code> function runs the standard k-means algorithm initializing the centroids to 3 random points. Finally, the function <code>plot_board()</code> produces a plot of the data points as clustered by the algorithm, with the centroids marked as stars. In the image below we can see the results of running the algorithm twice. Due to the random initialization of the standard k-means, the correct solution is found some of the times (right panel) whereas in some cases a suboptimal end point is reached instead (left panel). <a href=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png\"><img loading=\"lazy\" data-attachment-id=\"628\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/kpp_n200_k3/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png\" data-orig-size=\"1327,670\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"kpp_N200_K3\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=830\" class=\"alignnone size-full wp-image-628\" alt=\"kpp_N200_K3\" src=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=830&#038;h=419\" width=\"830\" height=\"419\" srcset=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=830&h=419 830w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=150&h=76 150w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=300&h=151 300w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=768&h=388 768w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png?w=1024&h=517 1024w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k31.png 1327w\" sizes=\"(max-width: 830px) 100vw, 830px\" /></a></p>\n<p>Let us now implement the k-means++ algorithm in its own class, which inherits from the <code>class Kmeans</code> defined above.</p>\n<pre class=\"brush: python; title: ; notranslate\">\nclass KPlusPlus(KMeans):\n    def _dist_from_centers(self):\n        cent = self.mu\n        X = self.X\n        D2 = np.array([min([np.linalg.norm(x-c)**2 for c in cent]) for x in X])\n        self.D2 = D2\n\n    def _choose_next_center(self):\n        self.probs = self.D2/self.D2.sum()\n        self.cumprobs = self.probs.cumsum()\n        r = random.random()\n        ind = np.where(self.cumprobs >= r)[0][0]\n        return(self.X[ind])\n\n    def init_centers(self):\n        self.mu = random.sample(self.X, 1)\n        while len(self.mu) < self.K:\n            self._dist_from_centers()\n            self.mu.append(self._choose_next_center())\n\n    def plot_init_centers(self):\n        X = self.X\n        fig = plt.figure(figsize=(5,5))\n        plt.xlim(-1,1)\n        plt.ylim(-1,1)\n        plt.plot(zip(*X)[0], zip(*X)[1], '.', alpha=0.5)\n        plt.plot(zip(*self.mu)[0], zip(*self.mu)[1], 'ro')\n        plt.savefig('kpp_init_N%s_K%s.png' % (str(self.N),str(self.K)), \\\n                    bbox_inches='tight', dpi=200)\n</pre>\n<p>To run the k-means++ initialization stage using this class and visualize the centers found by the algorithm, we simply do:</p>\n<pre class=\"brush: python; title: ; notranslate\">\nkplusplus = KPlusPlus(5, N=200)\nkplusplus.init_centers()\nkplusplus.plot_init_centers()\n</pre>\n<p><a href=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png\"><img loading=\"lazy\" data-attachment-id=\"635\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/kpp_init_n200_k5/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png\" data-orig-size=\"908,857\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"kpp_init_N200_K5\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png?w=830\" class=\"alignright  wp-image-635\" alt=\"kpp_init_N200_K5\" src=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png?w=415&#038;h=390\" width=\"415\" height=\"390\" srcset=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png?w=150&h=142 150w, https://datasciencelab.files.wordpress.com/2014/01/kpp_init_n200_k5.png?w=300&h=283 300w\" sizes=\"(max-width: 415px) 100vw, 415px\" /></a> Let us explore what the function <code>init_centers()</code> is actually doing: to begin with, a random point is chosen as first center from the <em>X</em> data points as <code>random.sample(self.X, 1)</code>. Then, the successive centers are picked, stopping when we have <em>K</em>=5 of them. The procedure to choose the next most suitable center is coded up in the <code>_choose_next_center()</code> function. As we described above, the next center is drawn from a distribution given by the normalized distance vector <img src=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_i%5E2+%2F+%5Csum_j+D_j%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_i^2 / &#92;sum_j D_j^2\" class=\"latex\" />. To implement such a probability distribution, we compute the cumulative probabilities for choosing each of the <em>N</em> points in <em>X</em>. These cumulative probabilities are partitions in the interval [0,1] with length equal to the probability of the corresponding point being chosen as a center, as explained in <a href=\"http://stackoverflow.com/questions/5466323/how-exactly-does-k-means-work\">this stackoverflow thread</a>. Therefore, by picking a random value <img src=\"https://s0.wp.com/latex.php?latex=r+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=r+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=r+%5Cin+%5B0%2C1%5D&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"r &#92;in [0,1]\" class=\"latex\" /> and finding the point corresponding to the segment of the partition where that <img src=\"https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=r&#038;bg=ffffff&#038;fg=000000&#038;s=1&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"r\" class=\"latex\" /> value falls, we are effectively choosing a point drawn according to the desired probability distribution. On the right is a plot showing the results of the algorithm for 200 points and 5 clusters.</p>\n<p>Finally let us compare the results of k-means with random initialization and k-means++ with proper seeding, using the following code snippets:</p>\n<pre class=\"brush: python; title: ; notranslate\">\n# Random initialization\nkplusplus.find_centers()\nkplusplus.plot_board()\n# k-means++ initialization\nkplusplus.find_centers(method='++')\nkplusplus.plot_board()\n</pre>\n<p><a href=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png\"><img loading=\"lazy\" data-attachment-id=\"643\" data-permalink=\"https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/kpp_n200_k5_comparison/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png\" data-orig-size=\"1310,668\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"kpp_N200_K5_comparison\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=830\" src=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=830&#038;h=423\" alt=\"kpp_N200_K5_comparison\" width=\"830\" height=\"423\" class=\"alignnone size-full wp-image-643\" srcset=\"https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=830&h=423 830w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=150&h=76 150w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=300&h=153 300w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=768&h=392 768w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png?w=1024&h=522 1024w, https://datasciencelab.files.wordpress.com/2014/01/kpp_n200_k5_comparison.png 1310w\" sizes=\"(max-width: 830px) 100vw, 830px\" /></a></p>\n<p>The standard algorithm with random initialization in a particular instantiation (left panel) fails at identifying the 5 optimal centroids for the clustering, whereas the k-means++ initialization (right panel) succeeds in doing so. By picking up a specific and not random set of centroids to initiate the clustering process, the k-means++ algorithm also reaches convergence faster, guaranteed by the theorems proved in the <a href=\"http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf\">Arthur and Vassilvitskii article</a>. </p>\n<h3>Table-top data experiment take-away message</h3>\n<p>The k-means++ method for finding a proper seeding for the choice of initial centroids yields considerable improvement over the standard Lloyd&#8217;s implementation of the k-means algorithm. The initial selection in k-means++ takes extra time and involves choosing centers in a successive order and drawing them from a particular probability distribution that has to be recomputed at each step. However, by doing so, the k-means part of the algorithm converges very quickly after this seeding and thus the whole procedure actually runs in a shorter computation time. The combination of the k-means++ initialization stage with the <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\" title=\"Clustering With K-Means in&nbsp;Python\">standard Lloyd&#8217;s algorithm</a>, together with additional <a href=\"https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/\" title=\"Finding the K in K-Means&nbsp;Clustering\">various</a> <a href=\"https://datasciencelab.wordpress.com/2014/01/21/selection-of-k-in-k-means-clustering-reloaded/\" title=\"Selection of K in K-means Clustering,Â Reloaded\">techniques</a> to find out an optimal value for the ideal number of clusters, poses a robust way to solve the complete problem of clustering data points.</p>\n",
  "wfw:commentRss": "https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/feed/",
  "slash:comments": 12,
  "media:content": [
    {
      "media:title": "datasciencelab"
    },
    {
      "media:title": "p_N100_K9_G"
    },
    {
      "media:title": "kpp_N200_K3"
    },
    {
      "media:title": "kpp_init_N200_K5"
    },
    {
      "media:title": "kpp_N200_K5_comparison"
    }
  ]
}