{
  "title": "Recurrent Neural Networks in Tensorflow III - Variable Length Sequences",
  "link": "",
  "published": "2016-11-15T00:00:00-05:00",
  "updated": "2016-11-15T00:00:00-05:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2016-11-15:/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html",
  "summary": "This is the third in a series of posts about recurrent neural networks in Tensorflow. In this post, we'll use Tensorflow to construct an RNN that operates on input sequences of variable lengths.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<h2 id=\"task\">Task</h2>\n<p>In this post, we’ll use Tensorflow to construct an RNN that operates on input sequences of variable lengths. We’ll use this RNN to classify bloggers by age bracket and gender using sentence-long writing samples. One time step will represent a single word, with the complete input sequence representing a single sentence. The challenge is to build a model that can classify multiple sentences of different lengths at the same time.</p>\n<h3 id=\"other-tutorials-on-variable-length-sequences\">Other tutorials on variable length sequences</h3>\n<p>There are a couple other tutorials on this topic. For example, the <a href=\"https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\">official Tensorflow seq2seq tutorial model</a> accomodates variable length sequences. This official model, however, is a bit advanced for a first exposure and a little too specialized to be easily portable to other contexts. Danijar Hafner has written a more approachable guide <a href=\"http://danijar.com/variable-sequence-lengths-in-tensorflow/\">here</a>, which I recommend. In contrast to Danijar’s post, this post is written a linear ipython notebook-style to make it easy for you to follow along step-by-step. This post also includes a section on bucketing, a technique that can significantly improve your model’s training time.</p>\n<h2 id=\"data\">Data</h2>\n<p>The data for this post is sourced from the “Blog Authorship Corpus”, available <a href=\"http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm\">here</a>. The original dataset was tokenized and split into sentences using <a href=\"https://spacy.io/\">spacy</a>. Sentences with less than 5 tokens and sentences with more than 30 tokens were discarded. Number-like tokens were replaced by “&lt;#&gt;”. Tokens other than the 9999 most common tokens were replaced by “&lt; UNK &gt;”, for a 10000 token vocabulary. Sentences were tagged with the gender (0 for male, 1 for female) and age bracket (0 for teens, 1 for 20s, 2 for 30s) and placed into a pandas dataframe. The modified data and code to import can be found <a href=\"https://github.com/spitis/blogs_data\">here</a>.</p>\n<p>Below is the head of the dataframe (tokens in “string” are delimited by spaces):</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> pandas <span class=\"im\">as</span> pd, numpy <span class=\"im\">as</span> np, tensorflow <span class=\"im\">as</span> tf\n<span class=\"im\">import</span> blogs_data <span class=\"co\">#available at https://github.com/spitis/blogs_data</span></code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">df <span class=\"op\">=</span> blogs_data.loadBlogs().sample(frac<span class=\"op\">=</span><span class=\"dv\">1</span>).reset_index(drop<span class=\"op\">=</span><span class=\"va\">True</span>)\nvocab, reverse_vocab <span class=\"op\">=</span> blogs_data.loadVocab()\ntrain_len, test_len <span class=\"op\">=</span> np.floor(<span class=\"bu\">len</span>(df)<span class=\"op\">*</span><span class=\"fl\">0.8</span>), np.floor(<span class=\"bu\">len</span>(df)<span class=\"op\">*</span><span class=\"fl\">0.2</span>)\ntrain, test <span class=\"op\">=</span> df.ix[:train_len<span class=\"dv\">-1</span>], df.ix[train_len:train_len <span class=\"op\">+</span> test_len]\ndf <span class=\"op\">=</span> <span class=\"va\">None</span>\ntrain.head()</code></pre></div>\n<table>\n<colgroup>\n<col style=\"width: 2%\" />\n<col style=\"width: 6%\" />\n<col style=\"width: 6%\" />\n<col style=\"width: 9%\" />\n<col style=\"width: 34%\" />\n<col style=\"width: 34%\" />\n<col style=\"width: 6%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th></th>\n<th>post_id</th>\n<th>gender</th>\n<th>age</th>\n<th>string</th>\n<th>as_numbers</th>\n<th>length</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td>0</td>\n<td>118860</td>\n<td>0</td>\n<td>0</td>\n<td>the last time i checked the constitution of th…</td>\n<td>[4, 127, 63, 3, 1837, 4, 3871, 8, 4, 1236, 927…</td>\n<td>22</td>\n</tr>\n<tr class=\"even\">\n<td>1</td>\n<td>178031</td>\n<td>1</td>\n<td>1</td>\n<td>but i do wish more people were so <UNK> ( star…</td>\n<td>[20, 3, 31, 360, 77, 79, 88, 27, 0, 43, 631, 2…</td>\n<td>15</td>\n</tr>\n<tr class=\"odd\">\n<td>2</td>\n<td>182592</td>\n<td>1</td>\n<td>0</td>\n<td>it came back to me right away and i was off .</td>\n<td>[10, 209, 93, 5, 19, 136, 192, 6, 3, 17, 129, 2]</td>\n<td>12</td>\n</tr>\n<tr class=\"even\">\n<td>3</td>\n<td>144982</td>\n<td>0</td>\n<td>2</td>\n<td>day &lt;#&gt; - get to class &lt;#&gt; min . early .</td>\n<td>[94, 12, 33, 59, 5, 320, 12, 2703, 2, 457, 2]</td>\n<td>11</td>\n</tr>\n<tr class=\"odd\">\n<td>4</td>\n<td>43048</td>\n<td>0</td>\n<td>0</td>\n<td>cause you do n’t know how much i , i need you …</td>\n<td>[332, 15, 31, 28, 64, 86, 96, 3, 1, 3, 157, 15…</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p> </p>\n<p>We’re going to build an RNN that accepts batches of data from the “as_numbers” column and predicts the “gender” and “age_bracket” columns. The first step is to construct a simple iterator that returns batches of inputs along with their targets, and the length of each input:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">class</span> SimpleDataIterator():\n    <span class=\"kw\">def</span> <span class=\"fu\">__init__</span>(<span class=\"va\">self</span>, df):\n        <span class=\"va\">self</span>.df <span class=\"op\">=</span> df\n        <span class=\"va\">self</span>.size <span class=\"op\">=</span> <span class=\"bu\">len</span>(<span class=\"va\">self</span>.df)\n        <span class=\"va\">self</span>.epochs <span class=\"op\">=</span> <span class=\"dv\">0</span>\n        <span class=\"va\">self</span>.shuffle()\n\n    <span class=\"kw\">def</span> shuffle(<span class=\"va\">self</span>):\n        <span class=\"va\">self</span>.df <span class=\"op\">=</span> <span class=\"va\">self</span>.df.sample(frac<span class=\"op\">=</span><span class=\"dv\">1</span>).reset_index(drop<span class=\"op\">=</span><span class=\"va\">True</span>)\n        <span class=\"va\">self</span>.cursor <span class=\"op\">=</span> <span class=\"dv\">0</span>\n\n    <span class=\"kw\">def</span> next_batch(<span class=\"va\">self</span>, n):\n        <span class=\"cf\">if</span> <span class=\"va\">self</span>.cursor<span class=\"op\">+</span>n<span class=\"dv\">-1</span> <span class=\"op\">&gt;</span> <span class=\"va\">self</span>.size:\n            <span class=\"va\">self</span>.epochs <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n            <span class=\"va\">self</span>.shuffle()\n        res <span class=\"op\">=</span> <span class=\"va\">self</span>.df.ix[<span class=\"va\">self</span>.cursor:<span class=\"va\">self</span>.cursor<span class=\"op\">+</span>n<span class=\"dv\">-1</span>]\n        <span class=\"va\">self</span>.cursor <span class=\"op\">+=</span> n\n        <span class=\"cf\">return</span> res[<span class=\"st\">&#39;as_numbers&#39;</span>], res[<span class=\"st\">&#39;gender&#39;</span>]<span class=\"op\">*</span><span class=\"dv\">3</span> <span class=\"op\">+</span> res[<span class=\"st\">&#39;age_bracket&#39;</span>], res[<span class=\"st\">&#39;length&#39;</span>]</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">data <span class=\"op\">=</span> SimpleDataIterator(train)\nd <span class=\"op\">=</span> data.next_batch(<span class=\"dv\">3</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&#39;Input sequences</span><span class=\"ch\">\\n</span><span class=\"st\">&#39;</span>, d[<span class=\"dv\">0</span>], end<span class=\"op\">=</span><span class=\"st\">&#39;</span><span class=\"ch\">\\n\\n</span><span class=\"st\">&#39;</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&#39;Target values</span><span class=\"ch\">\\n</span><span class=\"st\">&#39;</span>, d[<span class=\"dv\">1</span>], end<span class=\"op\">=</span><span class=\"st\">&#39;</span><span class=\"ch\">\\n\\n</span><span class=\"st\">&#39;</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&#39;Sequence lengths</span><span class=\"ch\">\\n</span><span class=\"st\">&#39;</span>, d[<span class=\"dv\">2</span>])</code></pre></div>\n<pre><code>Input sequences\n0    [27, 3, 576, 146, 13, 204, 37, 150, 6, 804, 94...\n1    [10, 210, 30, 1554, 10, 22, 325, 6240, 11, 4, ...\n2    [2927, 78, 9324, 5, 2273, 4, 5937, 8, 1058, 4,...\n\nTarget values\n0    4\n1    4\n2    1\n\nSequence lengths\n0    13\n1    18\n2    22</code></pre>\n<p>We are immediately faced with a problem, in that our 3 sequences are of different lengths: we cannot feed them into a Tensorflow graph as is, unless we create a different tensor for each (inefficient, and hard!). To solve this, we <strong>pad shorter sequences so that all sequences are the same length</strong>. Then all sequences will fit into a single tensor.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">class</span> PaddedDataIterator(SimpleDataIterator):\n    <span class=\"kw\">def</span> next_batch(<span class=\"va\">self</span>, n):\n        <span class=\"cf\">if</span> <span class=\"va\">self</span>.cursor<span class=\"op\">+</span>n <span class=\"op\">&gt;</span> <span class=\"va\">self</span>.size:\n            <span class=\"va\">self</span>.epochs <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n            <span class=\"va\">self</span>.shuffle()\n        res <span class=\"op\">=</span> <span class=\"va\">self</span>.df.ix[<span class=\"va\">self</span>.cursor:<span class=\"va\">self</span>.cursor<span class=\"op\">+</span>n<span class=\"dv\">-1</span>]\n        <span class=\"va\">self</span>.cursor <span class=\"op\">+=</span> n\n\n        <span class=\"co\"># Pad sequences with 0s so they are all the same length</span>\n        maxlen <span class=\"op\">=</span> <span class=\"bu\">max</span>(res[<span class=\"st\">&#39;length&#39;</span>])\n        x <span class=\"op\">=</span> np.zeros([n, maxlen], dtype<span class=\"op\">=</span>np.int32)\n        <span class=\"cf\">for</span> i, x_i <span class=\"kw\">in</span> <span class=\"bu\">enumerate</span>(x):\n            x_i[:res[<span class=\"st\">&#39;length&#39;</span>].values[i]] <span class=\"op\">=</span> res[<span class=\"st\">&#39;as_numbers&#39;</span>].values[i]\n\n        <span class=\"cf\">return</span> x, res[<span class=\"st\">&#39;gender&#39;</span>]<span class=\"op\">*</span><span class=\"dv\">3</span> <span class=\"op\">+</span> res[<span class=\"st\">&#39;age_bracket&#39;</span>], res[<span class=\"st\">&#39;length&#39;</span>]</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">data <span class=\"op\">=</span> PaddedDataIterator(train)\nd <span class=\"op\">=</span> data.next_batch(<span class=\"dv\">3</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&#39;Input sequences</span><span class=\"ch\">\\n</span><span class=\"st\">&#39;</span>, d[<span class=\"dv\">0</span>], end<span class=\"op\">=</span><span class=\"st\">&#39;</span><span class=\"ch\">\\n\\n</span><span class=\"st\">&#39;</span>)</code></pre></div>\n<pre><code>Input sequences\n [[  34   90    5  470   16   19   16    7  159    2    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n [  82    1  109    7  377    8  421    8    0   33  124    3   69  180\n    17   90    5  133   16   19   33   34   12 3819   85  164  129   25]\n [1786 5570    1   13 7817  235   60 6168   19    2    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]</code></pre>\n<p>Our padded iterator now returns a single input matrix of dimension [batch_size, max_sequence_length], where shorter sequences have been padded with zeros.</p>\n<h4 id=\"a-note-on-pad-symbols\">A note on PAD symbols</h4>\n<p>For this model, the zero that we used to pad our input sequences with is the index of the “&lt; UNK &gt;” symbol (representing UNKnown words) in our vocabulary. In this case, what we pad with doesn’t affect the outcome, and so I chose to keep it simple, but there are cases in which we might want to introduce a special “PAD” symbol. For example, here we will be feeding in a length tensor that holds information about our input sequence lengths, but suppose instead that we want to have Tensorflow calculate our sequence lengths; in that case, using 0 to represent both &lt; UNK &gt; and PAD would make it impossible for the graph to disambiguate between a sentence ending in &lt; UNK &gt; and a padded sentence. If we were to add a special PAD symbol, it would likely want to represent it as zero, and so it would have to displace the &lt; UNK &gt; symbol, which would then need to be represented by a different index.</p>\n<p>The advantage of the approach shown here (zero-padding with no special PAD symbol) is that it generalizes better to sequences with multi-dimensional continuous input (e.g., stock price data). In such cases, it does not really make sense to have a separate PAD symbol.</p>\n<h2 id=\"a-basic-model-for-sequence-classification\">A basic model for sequence classification</h2>\n<p>We’ll now construct a <strong>sequence classification</strong> model using this data that assigns a single label to an entire input sequence. Later, we’ll look at how we can instead construct a sequence-to-sequence model that predicts the authors age and gender at each time step.</p>\n<p>Our model makes use of Tensorflow’s <code>dynamic_rnn</code>, specifying a <code>sequence_length</code> parameter, which is fed into the model along with the data. Calling <code>dynamic_rnn</code> with a <code>sequence_length</code> parameter returns padded outputs: e.g., if the maximum sequence length is 10, but a specific example in the batch has a sequence length of only 4 followed by 6 zero steps for padding, the output for that time step will also have a length of only 4, with 6 additional zero steps for padding.</p>\n<p>This introduces some added complexity, since for sequence classification we only care about the final output of each sequence. This could be solved in one line using <code>tf.gather_nd</code>, as commented below, however, the gradient for <code>tf.gather_nd</code> is not yet implemented as of this writing. It is expected to be implemented shortly (you can view the status on Github <a href=\"https://github.com/tensorflow/tensorflow/issues/5342\">here</a>). In the interim, I have adopted Danijar Hafner’s solution, which you can read more about in his <a href=\"http://danijar.com/variable-sequence-lengths-in-tensorflow/\">post</a> under the heading “Select the Last Relevant Output”.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> reset_graph():\n    <span class=\"cf\">if</span> <span class=\"st\">&#39;sess&#39;</span> <span class=\"kw\">in</span> <span class=\"bu\">globals</span>() <span class=\"kw\">and</span> sess:\n        sess.close()\n    tf.reset_default_graph()\n\n<span class=\"kw\">def</span> build_graph(\n    vocab_size <span class=\"op\">=</span> <span class=\"bu\">len</span>(vocab),\n    state_size <span class=\"op\">=</span> <span class=\"dv\">64</span>,\n    batch_size <span class=\"op\">=</span> <span class=\"dv\">256</span>,\n    num_classes <span class=\"op\">=</span> <span class=\"dv\">6</span>):\n\n    reset_graph()\n\n    <span class=\"co\"># Placeholders</span>\n    x <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, <span class=\"va\">None</span>]) <span class=\"co\"># [batch_size, num_steps]</span>\n    seqlen <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size])\n    y <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size])\n    keep_prob <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.0</span>)\n\n    <span class=\"co\"># Embedding layer</span>\n    embeddings <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;embedding_matrix&#39;</span>, [vocab_size, state_size])\n    rnn_inputs <span class=\"op\">=</span> tf.nn.embedding_lookup(embeddings, x)\n\n    <span class=\"co\"># RNN</span>\n    cell <span class=\"op\">=</span> tf.nn.rnn_cell.GRUCell(state_size)\n    init_state <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;init_state&#39;</span>, [<span class=\"dv\">1</span>, state_size],\n                                 initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n    init_state <span class=\"op\">=</span> tf.tile(init_state, [batch_size, <span class=\"dv\">1</span>])\n    rnn_outputs, final_state <span class=\"op\">=</span> tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length<span class=\"op\">=</span>seqlen,\n                                                 initial_state<span class=\"op\">=</span>init_state)\n\n    <span class=\"co\"># Add dropout, as the model otherwise quickly overfits</span>\n    rnn_outputs <span class=\"op\">=</span> tf.nn.dropout(rnn_outputs, keep_prob)\n\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Obtain the last relevant output. The best approach in the future will be to use:</span>\n\n<span class=\"co\">        last_rnn_output = tf.gather_nd(rnn_outputs, tf.pack([tf.range(batch_size), seqlen-1], axis=1))</span>\n\n<span class=\"co\">    which is the Tensorflow equivalent of numpy&#39;s rnn_outputs[range(30), seqlen-1, :], but the</span>\n<span class=\"co\">    gradient for this op has not been implemented as of this writing.</span>\n\n<span class=\"co\">    The below solution works, but throws a UserWarning re: the gradient.</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    idx <span class=\"op\">=</span> tf.<span class=\"bu\">range</span>(batch_size)<span class=\"op\">*</span>tf.shape(rnn_outputs)[<span class=\"dv\">1</span>] <span class=\"op\">+</span> (seqlen <span class=\"op\">-</span> <span class=\"dv\">1</span>)\n    last_rnn_output <span class=\"op\">=</span> tf.gather(tf.reshape(rnn_outputs, [<span class=\"op\">-</span><span class=\"dv\">1</span>, state_size]), idx)\n\n    <span class=\"co\"># Softmax layer</span>\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;softmax&#39;</span>):\n        W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W&#39;</span>, [state_size, num_classes])\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>, [num_classes], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n    logits <span class=\"op\">=</span> tf.matmul(last_rnn_output, W) <span class=\"op\">+</span> b\n    preds <span class=\"op\">=</span> tf.nn.softmax(logits)\n    correct <span class=\"op\">=</span> tf.equal(tf.cast(tf.argmax(preds,<span class=\"dv\">1</span>),tf.int32), y)\n    accuracy <span class=\"op\">=</span> tf.reduce_mean(tf.cast(correct, tf.float32))\n\n    loss <span class=\"op\">=</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n    train_step <span class=\"op\">=</span> tf.train.AdamOptimizer(<span class=\"fl\">1e-4</span>).minimize(loss)\n\n    <span class=\"cf\">return</span> {\n        <span class=\"st\">&#39;x&#39;</span>: x,\n        <span class=\"st\">&#39;seqlen&#39;</span>: seqlen,\n        <span class=\"st\">&#39;y&#39;</span>: y,\n        <span class=\"st\">&#39;dropout&#39;</span>: keep_prob,\n        <span class=\"st\">&#39;loss&#39;</span>: loss,\n        <span class=\"st\">&#39;ts&#39;</span>: train_step,\n        <span class=\"st\">&#39;preds&#39;</span>: preds,\n        <span class=\"st\">&#39;accuracy&#39;</span>: accuracy\n    }\n\n<span class=\"kw\">def</span> train_graph(graph, batch_size <span class=\"op\">=</span> <span class=\"dv\">256</span>, num_epochs <span class=\"op\">=</span> <span class=\"dv\">10</span>, iterator <span class=\"op\">=</span> PaddedDataIterator):\n    <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n        tr <span class=\"op\">=</span> iterator(train)\n        te <span class=\"op\">=</span> iterator(test)\n\n        step, accuracy <span class=\"op\">=</span> <span class=\"dv\">0</span>, <span class=\"dv\">0</span>\n        tr_losses, te_losses <span class=\"op\">=</span> [], []\n        current_epoch <span class=\"op\">=</span> <span class=\"dv\">0</span>\n        <span class=\"cf\">while</span> current_epoch <span class=\"op\">&lt;</span> num_epochs:\n            step <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n            batch <span class=\"op\">=</span> tr.next_batch(batch_size)\n            feed <span class=\"op\">=</span> {g[<span class=\"st\">&#39;x&#39;</span>]: batch[<span class=\"dv\">0</span>], g[<span class=\"st\">&#39;y&#39;</span>]: batch[<span class=\"dv\">1</span>], g[<span class=\"st\">&#39;seqlen&#39;</span>]: batch[<span class=\"dv\">2</span>], g[<span class=\"st\">&#39;dropout&#39;</span>]: <span class=\"fl\">0.6</span>}\n            accuracy_, _ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;accuracy&#39;</span>], g[<span class=\"st\">&#39;ts&#39;</span>]], feed_dict<span class=\"op\">=</span>feed)\n            accuracy <span class=\"op\">+=</span> accuracy_\n\n            <span class=\"cf\">if</span> tr.epochs <span class=\"op\">&gt;</span> current_epoch:\n                current_epoch <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n                tr_losses.append(accuracy <span class=\"op\">/</span> step)\n                step, accuracy <span class=\"op\">=</span> <span class=\"dv\">0</span>, <span class=\"dv\">0</span>\n\n                <span class=\"co\">#eval test set</span>\n                te_epoch <span class=\"op\">=</span> te.epochs\n                <span class=\"cf\">while</span> te.epochs <span class=\"op\">==</span> te_epoch:\n                    step <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n                    batch <span class=\"op\">=</span> te.next_batch(batch_size)\n                    feed <span class=\"op\">=</span> {g[<span class=\"st\">&#39;x&#39;</span>]: batch[<span class=\"dv\">0</span>], g[<span class=\"st\">&#39;y&#39;</span>]: batch[<span class=\"dv\">1</span>], g[<span class=\"st\">&#39;seqlen&#39;</span>]: batch[<span class=\"dv\">2</span>]}\n                    accuracy_ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;accuracy&#39;</span>]], feed_dict<span class=\"op\">=</span>feed)[<span class=\"dv\">0</span>]\n                    accuracy <span class=\"op\">+=</span> accuracy_\n\n                te_losses.append(accuracy <span class=\"op\">/</span> step)\n                step, accuracy <span class=\"op\">=</span> <span class=\"dv\">0</span>,<span class=\"dv\">0</span>\n                <span class=\"bu\">print</span>(<span class=\"st\">&quot;Accuracy after epoch&quot;</span>, current_epoch, <span class=\"st\">&quot; - tr:&quot;</span>, tr_losses[<span class=\"op\">-</span><span class=\"dv\">1</span>], <span class=\"st\">&quot;- te:&quot;</span>, te_losses[<span class=\"op\">-</span><span class=\"dv\">1</span>])\n\n    <span class=\"cf\">return</span> tr_losses, te_losses</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_graph()\ntr_losses, te_losses <span class=\"op\">=</span> train_graph(g)</code></pre></div>\n<pre><code>Accuracy after epoch 1  - tr: 0.319347791963 - te: 0.351068906904\nAccuracy after epoch 2  - tr: 0.355731238225 - te: 0.357366258375\nAccuracy after epoch 3  - tr: 0.361505161451 - te: 0.358625811348\nAccuracy after epoch 4  - tr: 0.363629598859 - te: 0.359358642169\nAccuracy after epoch 5  - tr: 0.365078599278 - te: 0.358609453518\nAccuracy after epoch 6  - tr: 0.365907767689 - te: 0.359358642169\nAccuracy after epoch 7  - tr: 0.367192406322 - te: 0.359833019263\nAccuracy after epoch 8  - tr: 0.368336397059 - te: 0.360304124791\nAccuracy after epoch 9  - tr: 0.369028188455 - te: 0.360434987437\nAccuracy after epoch 10  - tr: 0.37021715381 - te: 0.36041535804</code></pre>\n<p>After 10 epochs, our network has an accuracy of about 36%, about twice as good as chance–not bad for predicting age and gender from a single sentence!</p>\n<h2 id=\"improving-training-speed-using-bucketing\">Improving training speed using bucketing</h2>\n<p>For the network above, we used a batch_size of 256. But each example in the batch had a different length ranging from 5 to 30. As the maximum length for each batch is usually very close to 30, short sequences required a lot of padding (e.g., all sequences of length 5 in the batch are padded with up to 25 zeros). Given this dataset, each batch is padded with an average of over 3000 zeros, or over 10 padding symbols per sample:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">tr <span class=\"op\">=</span> PaddedDataIterator(train)\npadding <span class=\"op\">=</span> <span class=\"dv\">0</span>\n<span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">100</span>):\n    lengths <span class=\"op\">=</span> tr.next_batch(<span class=\"dv\">256</span>)[<span class=\"dv\">2</span>].values\n    max_len <span class=\"op\">=</span> <span class=\"bu\">max</span>(lengths)\n    padding <span class=\"op\">+=</span> np.<span class=\"bu\">sum</span>(max_len <span class=\"op\">-</span> lengths)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Average padding / batch:&quot;</span>, padding<span class=\"op\">/</span><span class=\"dv\">100</span>)</code></pre></div>\n<pre><code>Average padding / batch: 3279.9</code></pre>\n<p>This leads to a lot of excess computation, and we can improve upon it by “bucketing” our training samples. If we select our batches such that the lengths of the samples in each batch are within, say, 5 of each other, then the amount of padding in a batch of 256 is bounded by 256 * 5 = 1280. This would make our worst case outcome more than twice as good as previous average case outcome.</p>\n<p>To take advantage of bucketing, we simply modify our DataIterator. There are many ways one might implement this, but the key point to keep in mind is that we should not “bias” the order in which different sequence lengths are sampled any more than necessary to achieve bucketing. E.g., sorting our data by sequence length might seem like a good solution, but then each epoch would be trained on short sequences before longer sequences, which could harm results. Here is one solution, which uses a predetermined batch_size:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">class</span> BucketedDataIterator():\n    <span class=\"kw\">def</span> <span class=\"fu\">__init__</span>(<span class=\"va\">self</span>, df, num_buckets <span class=\"op\">=</span> <span class=\"dv\">5</span>):\n        df <span class=\"op\">=</span> df.sort_values(<span class=\"st\">&#39;length&#39;</span>).reset_index(drop<span class=\"op\">=</span><span class=\"va\">True</span>)\n        <span class=\"va\">self</span>.size <span class=\"op\">=</span> <span class=\"bu\">len</span>(df) <span class=\"op\">/</span> num_buckets\n        <span class=\"va\">self</span>.dfs <span class=\"op\">=</span> []\n        <span class=\"cf\">for</span> bucket <span class=\"kw\">in</span> <span class=\"bu\">range</span>(num_buckets):\n            <span class=\"va\">self</span>.dfs.append(df.ix[bucket<span class=\"op\">*</span><span class=\"va\">self</span>.size: (bucket<span class=\"op\">+</span><span class=\"dv\">1</span>)<span class=\"op\">*</span><span class=\"va\">self</span>.size <span class=\"op\">-</span> <span class=\"dv\">1</span>])\n        <span class=\"va\">self</span>.num_buckets <span class=\"op\">=</span> num_buckets\n\n        <span class=\"co\"># cursor[i] will be the cursor for the ith bucket</span>\n        <span class=\"va\">self</span>.cursor <span class=\"op\">=</span> np.array([<span class=\"dv\">0</span>] <span class=\"op\">*</span> num_buckets)\n        <span class=\"va\">self</span>.shuffle()\n\n        <span class=\"va\">self</span>.epochs <span class=\"op\">=</span> <span class=\"dv\">0</span>\n\n    <span class=\"kw\">def</span> shuffle(<span class=\"va\">self</span>):\n        <span class=\"co\">#sorts dataframe by sequence length, but keeps it random within the same length</span>\n        <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"va\">self</span>.num_buckets):\n            <span class=\"va\">self</span>.dfs[i] <span class=\"op\">=</span> <span class=\"va\">self</span>.dfs[i].sample(frac<span class=\"op\">=</span><span class=\"dv\">1</span>).reset_index(drop<span class=\"op\">=</span><span class=\"va\">True</span>)\n            <span class=\"va\">self</span>.cursor[i] <span class=\"op\">=</span> <span class=\"dv\">0</span>\n\n    <span class=\"kw\">def</span> next_batch(<span class=\"va\">self</span>, n):\n        <span class=\"cf\">if</span> np.<span class=\"bu\">any</span>(<span class=\"va\">self</span>.cursor<span class=\"op\">+</span>n<span class=\"op\">+</span><span class=\"dv\">1</span> <span class=\"op\">&gt;</span> <span class=\"va\">self</span>.size):\n            <span class=\"va\">self</span>.epochs <span class=\"op\">+=</span> <span class=\"dv\">1</span>\n            <span class=\"va\">self</span>.shuffle()\n\n        i <span class=\"op\">=</span> np.random.randint(<span class=\"dv\">0</span>,<span class=\"va\">self</span>.num_buckets)\n\n        res <span class=\"op\">=</span> <span class=\"va\">self</span>.dfs[i].ix[<span class=\"va\">self</span>.cursor[i]:<span class=\"va\">self</span>.cursor[i]<span class=\"op\">+</span>n<span class=\"dv\">-1</span>]\n        <span class=\"va\">self</span>.cursor[i] <span class=\"op\">+=</span> n\n\n        <span class=\"co\"># Pad sequences with 0s so they are all the same length</span>\n        maxlen <span class=\"op\">=</span> <span class=\"bu\">max</span>(res[<span class=\"st\">&#39;length&#39;</span>])\n        x <span class=\"op\">=</span> np.zeros([n, maxlen], dtype<span class=\"op\">=</span>np.int32)\n        <span class=\"cf\">for</span> i, x_i <span class=\"kw\">in</span> <span class=\"bu\">enumerate</span>(x):\n            x_i[:res[<span class=\"st\">&#39;length&#39;</span>].values[i]] <span class=\"op\">=</span> res[<span class=\"st\">&#39;as_numbers&#39;</span>].values[i]\n\n        <span class=\"cf\">return</span> x, res[<span class=\"st\">&#39;gender&#39;</span>]<span class=\"op\">*</span><span class=\"dv\">3</span> <span class=\"op\">+</span> res[<span class=\"st\">&#39;age_bracket&#39;</span>], res[<span class=\"st\">&#39;length&#39;</span>]</code></pre></div>\n<p>With this modified iterator, we improve the average padding / batch by a factor of about 6:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">tr <span class=\"op\">=</span> BucketedDataIterator(train, <span class=\"dv\">5</span>)\npadding <span class=\"op\">=</span> <span class=\"dv\">0</span>\n<span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">100</span>):\n    lengths <span class=\"op\">=</span> tr.next_batch(<span class=\"dv\">256</span>)[<span class=\"dv\">2</span>].values\n    max_len <span class=\"op\">=</span> <span class=\"bu\">max</span>(lengths)\n    padding <span class=\"op\">+=</span> np.<span class=\"bu\">sum</span>(max_len <span class=\"op\">-</span> lengths)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Average padding / batch:&quot;</span>, padding<span class=\"op\">/</span><span class=\"dv\">100</span>)</code></pre></div>\n<pre><code>Average padding / batch: 573.49</code></pre>\n<p>We can also compare the difference in training speed, and observe that this bucketing strategy speeds up training by about 30%:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">from</span> time <span class=\"im\">import</span> time\ng <span class=\"op\">=</span> build_graph()\nt <span class=\"op\">=</span> time()\ntr_losses, te_losses <span class=\"op\">=</span> train_graph(g, num_epochs<span class=\"op\">=</span><span class=\"dv\">1</span>, iterator<span class=\"op\">=</span>PaddedDataIterator)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Total time for 1 epoch with PaddedDataIterator:&quot;</span>, time() <span class=\"op\">-</span> t)</code></pre></div>\n<pre><code>Accuracy after epoch 1  - tr: 0.310819936427 - te: 0.341703713389\nTotal time for 1 epoch with PaddedDataIterator: 100.90330529212952</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_graph()\nt <span class=\"op\">=</span> time()\ntr_losses, te_losses <span class=\"op\">=</span> train_graph(g, num_epochs<span class=\"op\">=</span><span class=\"dv\">1</span>, iterator<span class=\"op\">=</span>BucketedDataIterator)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Total time for 1 epoch with BucketedDataIterator:&quot;</span>, time() <span class=\"op\">-</span> t)</code></pre></div>\n<pre><code>Accuracy after epoch 1  - tr: 0.31359524197 - te: 0.349176362254\nTotal time for 1 epoch with BucketedDataIterator: 71.45360088348389</code></pre>\n<p>Note how easy it was to move to a bucketed model–all we had to do was change our data generator. This was made possible by the use of a partially-known shape for our input placeholder, with the num_steps dimension unknown. Contrast this to the more complicated approach in Tensorflow’s <a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html\">seq2seq tutorial</a>, which builds a different graph for each of four buckets.</p>\n<h4 id=\"a-note-on-awkward-sequence-lengths\">A note on awkward sequence lengths</h4>\n<p>Suppose we had a dataset with awkward sequence lengths that made even a bucketed approach inefficient. For example, we might have lots of very short sequences of lengths 1, 2 and 3. Alternatively, we might have a few very long sequences among our shorter ones; we want to propagate the internal state forward through time for the long sequences, but don’t have enough of them to train efficiently in parallel. One solution in both of these scenarios is to combine short sequences into longer ones, but have the internal state of the RNN reset in between each such sequence. I believe this is not possible to do with Tensorflow’s default RNN functions (e.g., <code>dynamic_rnn</code>), so if you’re looking for a way to do this, I would look into writing a custom RNN method using <code>tf.scan</code>. I show how to use <code>tf.scan</code> to build a custom RNN in my post, <a href=\"http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\">Recurrent Neural Networks in Tensorflow II</a>. With the right accumulator function, you could program in the state resets dynamically based on either a special PAD symbol, or an auxiliary input sequence that indicates where the state should be reset.</p>\n<h2 id=\"a-basic-model-for-sequence-to-sequence-learning\">A basic model for sequence to sequence learning</h2>\n<p>Finally, we extend our sequence classification model to do <strong>sequence-to-sequence learning</strong>. We’ll use the same dataset, but instead of having our model guess the author’s age bracket and gender at the end of the sequence (i.e., only once), we’ll have it guess at every timestep.</p>\n<p>The added wrinkle when moving to a sequence-to-sequence model is that we need to make sure that time-steps with a PAD symbol do not contribute to our loss, since they are just there as filler. We do so by zeroing the loss at these time steps, which is known as applying a “mask” or “masking” the loss. This is achieved by pointwise multiplying the loss tensor (with each entry representing a time step), by a tensor of 1s and 0s, where 1s represent valid steps and 0s represent PAD steps. A similar modification is made to the “accuracy” calculation below, as noted in the comments.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_seq2seq_graph(\n    vocab_size <span class=\"op\">=</span> <span class=\"bu\">len</span>(vocab),\n    state_size <span class=\"op\">=</span> <span class=\"dv\">64</span>,\n    batch_size <span class=\"op\">=</span> <span class=\"dv\">256</span>,\n    num_classes <span class=\"op\">=</span> <span class=\"dv\">6</span>):\n\n    reset_graph()\n\n    <span class=\"co\"># Placeholders</span>\n    x <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, <span class=\"va\">None</span>]) <span class=\"co\"># [batch_size, num_steps]</span>\n    seqlen <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size])\n    y <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size])\n    keep_prob <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.0</span>)\n\n    <span class=\"co\"># Tile the target indices</span>\n    <span class=\"co\"># (in a regular seq2seq model, our targets placeholder might have this shape)</span>\n    y_ <span class=\"op\">=</span> tf.tile(tf.expand_dims(y, <span class=\"dv\">1</span>), [<span class=\"dv\">1</span>, tf.shape(x)[<span class=\"dv\">1</span>]]) <span class=\"co\"># [batch_size, num_steps]</span>\n\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Create a mask that we will use for the cost function</span>\n\n<span class=\"co\">    This mask is the same shape as x and y_, and is equal to 1 for all non-PAD time</span>\n<span class=\"co\">    steps (where a prediction is made), and 0 for all PAD time steps (no pred -&gt; no loss)</span>\n<span class=\"co\">    The number 30, used when creating the lower_triangle_ones matrix, is the maximum</span>\n<span class=\"co\">    sequence length in our dataset</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    lower_triangular_ones <span class=\"op\">=</span> tf.constant(np.tril(np.ones([<span class=\"dv\">30</span>,<span class=\"dv\">30</span>])),dtype<span class=\"op\">=</span>tf.float32)\n    seqlen_mask <span class=\"op\">=</span> tf.<span class=\"bu\">slice</span>(tf.gather(lower_triangular_ones, seqlen <span class=\"op\">-</span> <span class=\"dv\">1</span>),<span class=\"op\">\\</span>\n                           [<span class=\"dv\">0</span>, <span class=\"dv\">0</span>], [batch_size, tf.reduce_max(seqlen)])\n\n    <span class=\"co\"># Embedding layer</span>\n    embeddings <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;embedding_matrix&#39;</span>, [vocab_size, state_size])\n    rnn_inputs <span class=\"op\">=</span> tf.nn.embedding_lookup(embeddings, x)\n\n    <span class=\"co\"># RNN</span>\n    cell <span class=\"op\">=</span> tf.nn.rnn_cell.GRUCell(state_size)\n    init_state <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;init_state&#39;</span>, [<span class=\"dv\">1</span>, state_size],\n                                 initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n    init_state <span class=\"op\">=</span> tf.tile(init_state, [batch_size, <span class=\"dv\">1</span>])\n    rnn_outputs, final_state <span class=\"op\">=</span> tf.nn.dynamic_rnn(cell, rnn_inputs, sequence_length<span class=\"op\">=</span>seqlen,\n                                                 initial_state<span class=\"op\">=</span>init_state)\n\n    <span class=\"co\"># Add dropout, as the model otherwise quickly overfits</span>\n    rnn_outputs <span class=\"op\">=</span> tf.nn.dropout(rnn_outputs, keep_prob)\n\n    <span class=\"co\">#reshape rnn_outputs and y</span>\n    rnn_outputs <span class=\"op\">=</span> tf.reshape(rnn_outputs, [<span class=\"op\">-</span><span class=\"dv\">1</span>, state_size])\n    y_reshaped <span class=\"op\">=</span> tf.reshape(y_, [<span class=\"op\">-</span><span class=\"dv\">1</span>])\n\n    <span class=\"co\"># Softmax layer</span>\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;softmax&#39;</span>):\n        W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W&#39;</span>, [state_size, num_classes])\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>, [num_classes], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n    logits <span class=\"op\">=</span> tf.matmul(rnn_outputs, W) <span class=\"op\">+</span> b\n\n    preds <span class=\"op\">=</span> tf.nn.softmax(logits)\n\n    <span class=\"co\"># To calculate the number correct, we want to count padded steps as incorrect</span>\n    correct <span class=\"op\">=</span> tf.cast(tf.equal(tf.cast(tf.argmax(preds,<span class=\"dv\">1</span>),tf.int32), y_reshaped),tf.int32) <span class=\"op\">*\\</span>\n                tf.cast(tf.reshape(seqlen_mask, [<span class=\"op\">-</span><span class=\"dv\">1</span>]),tf.int32)\n\n    <span class=\"co\"># To calculate accuracy we want to divide by the number of non-padded time-steps,</span>\n    <span class=\"co\"># rather than taking the mean</span>\n    accuracy <span class=\"op\">=</span> tf.reduce_sum(tf.cast(correct, tf.float32)) <span class=\"op\">/</span> tf.reduce_sum(tf.cast(seqlen, tf.float32))\n\n    loss <span class=\"op\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped)\n    loss <span class=\"op\">=</span> loss <span class=\"op\">*</span> tf.reshape(seqlen_mask, [<span class=\"op\">-</span><span class=\"dv\">1</span>])\n\n    <span class=\"co\"># To calculate average loss, we need to divide by number of non-padded time-steps,</span>\n    <span class=\"co\"># rather than taking the mean</span>\n    loss <span class=\"op\">=</span> tf.reduce_sum(loss) <span class=\"op\">/</span> tf.reduce_sum(seqlen_mask)\n\n    train_step <span class=\"op\">=</span> tf.train.AdamOptimizer(<span class=\"fl\">1e-4</span>).minimize(loss)\n\n    <span class=\"cf\">return</span> {\n        <span class=\"st\">&#39;x&#39;</span>: x,\n        <span class=\"st\">&#39;seqlen&#39;</span>: seqlen,\n        <span class=\"st\">&#39;y&#39;</span>: y,\n        <span class=\"st\">&#39;dropout&#39;</span>: keep_prob,\n        <span class=\"st\">&#39;loss&#39;</span>: loss,\n        <span class=\"st\">&#39;ts&#39;</span>: train_step,\n        <span class=\"st\">&#39;preds&#39;</span>: preds,\n        <span class=\"st\">&#39;accuracy&#39;</span>: accuracy\n    }</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_seq2seq_graph()\ntr_losses, te_losses <span class=\"op\">=</span> train_graph(g, iterator<span class=\"op\">=</span>BucketedDataIterator)</code></pre></div>\n<pre><code>Accuracy after epoch 1  - tr: 0.292434578401 - te: 0.316306242085\nAccuracy after epoch 2  - tr: 0.320437548276 - te: 0.322733865921\nAccuracy after epoch 3  - tr: 0.325227848205 - te: 0.322927395211\nAccuracy after epoch 4  - tr: 0.327002136049 - te: 0.324078651696\nAccuracy after epoch 5  - tr: 0.327847927489 - te: 0.324469006651\nAccuracy after epoch 6  - tr: 0.328276157813 - te: 0.324198486081\nAccuracy after epoch 7  - tr: 0.329078430968 - te: 0.324715245167\nAccuracy after epoch 8  - tr: 0.330095707002 - te: 0.325317926384\nAccuracy after epoch 9  - tr: 0.330612316872 - te: 0.32550007953\nAccuracy after epoch 10  - tr: 0.331520609485 - te: 0.326069803531</code></pre>\n<p>As expected, our sequence-to-sequence model has slightly worse accuracy than our sequence classification model (because it’s early guesses are nearly random and reduce the accuracy).</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In this post, we learned four concepts, all related to building RNNs that work with variable length sequences. First, we learned how to pad input sequences so that we can feed in a single zero-padded input tensor. Second, we learned how to get the last relevant output in a sequence classification model. Third, we learned how to use bucketing to get a significantly boost in training time. Finally, we learned how to “mask” our loss function so that we can train sequence-to-sequence models with variable length sequences.</p>\n</body>\n</html>"
}