{
  "title": "Twin training: trick for better model comparisons",
  "description": "<p>Abstract: <em>Frequently comparing deep learning models?<br />\nA simple way to improve comparison is discussed here, \nthis trick becomes specially handy when comparing segmentation models.</em></p>\n\n<p>Reliable comparison of models is a question important for DL “theorists” (to evaluate new approaches) \nas well as for practitioners/engineers (to select an approach for a particular task in hand).\nComparison is time-consuming process, frequently with noisy results.</p>\n\n<p>Usual setting incorporates fixed dataset split into train/val/test and fixed metric of choice. \nNext, independent runs are conducted for all models under comparison and achieved quality is registered.</p>\n\n<p>As a result,</p>\n\n<ul>\n  <li>There is a significant noise in comparison (it is rare to rerun each model several times, specially in applications),</li>\n  <li>Validation can be done only using whole dataset</li>\n  <li>need to remember which version of code was used to generate a particular number, as you can \naccidentally compare things that are not ‘comparable’ because of e.g. changed augmentation or updates in the dataset\n    <ul>\n      <li>yes, practitioners have to deal with frequent updates in the dataset</li>\n    </ul>\n  </li>\n  <li>can’t use augmentations while testing, since it is hard to guarantee that exactly same augmentations were applied.\nSometimes it is handy to evaluate using several batches as a fast intermediate check. Augmentations in test allow ‘broader’ check.</li>\n</ul>\n\n<h2 id=\"what-is-suggested-twin-training\">What is suggested: twin training</h2>\n\n<p>Models can be trained <strong>side-by-side within the same process</strong>, with as high similarity in the training process as possible.\nSame batches, same augmentations, and of course the same datasets.</p>\n\n<ul>\n  <li>If models, say, have identical architecture, their initial weights should be identical (easy to achieve in any DL framework).\n    <ul>\n      <li>As we know, initial state influences optimization, in some cases drastically (that’s not desirable, but happens).</li>\n    </ul>\n  </li>\n  <li>During training, same exact batches with the same exact augmentation should be used to optimize models.\n    <ul>\n      <li>That’s right, you need to augment only once, thus CPU is not a bottleneck.</li>\n      <li>Similarly, one should always compare on the same batches.\nTo achieve smooth monitoring rather than ‘validate once on a while’, take one batch at a time and compute metrics on that batch.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>Pseudo-code may look like (fragment):</p>\n\n<!-- TODO fix display here -->\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">train_data</span><span class=\"p\">:</span>\n    <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">augment</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">model</span> <span class=\"ow\">in</span> <span class=\"n\">models</span><span class=\"p\">:</span>\n        <span class=\"c1\"># make an optimization step for each model using the same batch\n</span></code></pre></div></div>\n\n<p>Things usually tuned (architecture, loss, augmentations, parameters, optimizers, learning schedules, etc.) - \nall of them can be compared more efficiently this way.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<p><img src=\"/images/model_comparison/tensorboard1.png\" width=\"700\" /></p>\n\n<p>There are three models trained in parallel in this screenshot from tensorboard.\nOne can tell when one of models has lower loss and estimate level of ‘noise’. \nIt is also clear that most jumps and falls in learning curves are due to batches sampled, and are not model-specific behavior. \nIn other words, you can better see the difference between <strong>models</strong> not difference between <strong>runs</strong>.</p>\n\n<p>This demonstrates a typical comparison — things compared are extremely similar and there is little practical difference.\nModels’ response to the same training input is close to identical. \nIt’s not easy to get the same conclusion by looking at just final scores. \nThat’s a good argument towards including learning curves in the paper.</p>\n\n<h2 id=\"bonus-simpler-comparison-of-segmentation-models\">Bonus: simpler comparison of segmentation models</h2>\n\n<p>When training models for image segmentation (such as instance segmentation or class-segmentation),\nlack of memory becomes a critical factor. \nBatch sizes become very small, and it is almost impossible to train several segmentation models at once on a single GPU.</p>\n\n<p>During segmentation training each sample contributes a lot, since it provides a lot of labels (one per pixel!).<br />\nIt is also unlikely that you have thousands of well-labelled high-resolution segmentation images.</p>\n\n<p>However when you train several models inside a single script/notebook, there are no such problems, \n<em>because you never keep intermediate activations for more than one model at a time</em>. \nWeights of all models should still be kept in (GPU) memory, but that’s a small fraction of space taken by activations.</p>\n\n<h2 id=\"bonus-simple-organization-of-experiments-in-tensorboard\">Bonus: simple organization of experiments in tensorboard</h2>\n\n<p><img src=\"/images/model_comparison/folder_organization.png\" height=\"200\" /></p>\n\n<p>Tensorboard recursively scans subfolders for logs, so you can keep each ‘comparison’ in a separate folder, \nand each compared option saves its logs to a corresponding subfolder.</p>\n\n<h2 id=\"alternative-fix-random-seed\">Alternative: fix random seed?</h2>\n\n<p>I don’t think that fixed random seed is reliable enough to be considered as an alternative way to achieve similarity in training.</p>\n\n<p>THere are many different RNGs provided by different modules, and RNGs are used in too many places. \nAnd you need to precisely control RNG flow in your program.\nBecause if some of your functions use global RNGs like <code class=\"language-plaintext highlighter-rouge\">random</code> or <code class=\"language-plaintext highlighter-rouge\">np.random</code> directly, \nthis implies that <em>any</em> side call to those from anywhere in your program completely changes all following sampled numbers.\nAny ‘interruption’ in the sequence breaks it. \nRandom numbers on GPU is whole another story.</p>\n\n<p>So, you should look through all the augmentations, samplers, dropouts (basically, everything) to verify they don’t use global RNG’s \n(and find that some of them actually do).</p>\n\n<p>Long story short, if you <em>have</em> to rely on random seeds in DL, \nat least log some control sums to verify that sequence was not broken by an unexpected call from somewhere else.</p>\n\n<p>You can still use random seed to achieve reproducible training of the same model.</p>",
  "pubDate": "Tue, 01 Jan 2019 12:00:00 +0000",
  "link": "https://arogozhnikov.github.io/2019/01/01/trick-for-model-comparison.html",
  "guid": "https://arogozhnikov.github.io/2019/01/01/trick-for-model-comparison.html",
  "category": [
    "Machine Learning",
    "Engineering",
    "Code improvements"
  ]
}