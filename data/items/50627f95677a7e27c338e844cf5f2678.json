{
  "title": "Using PROC DEEPCAUSAL to optimize revenue through policy evaluation",
  "link": "https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/",
  "comments": "https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/#respond",
  "dc:creator": "Gunce E. Walton",
  "pubDate": "Mon, 12 Sep 2022 14:00:12 +0000",
  "category": [
    "Uncategorized",
    "Analytics R&D",
    "PROC DEEPCAUSAL",
    "SAS Econometrics"
  ],
  "guid": "https://blogs.sas.com/content/subconsciousmusings/?p=13536",
  "description": "<p>SAS' Gunce Walton introduces to you a new scoring capability, how it utilizes Deep Neural Networks (DNNs) and shares use cases with PROC DEEPCAUSAL.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/\">Using PROC DEEPCAUSAL to optimize revenue through policy evaluation</a> appeared first on <a rel=\"nofollow\" href=\"https://blogs.sas.com/content/subconsciousmusings\">The SAS Data Science Blog</a>.</p>\n",
  "content:encoded": "<p>When it comes to causal inference, scoring capability is particularly beneficial. It can be used in unique ways that result in an improved decision-making process, such as gaining optimal revenue using the least number of resources. In this post, I will introduce to you a new scoring capability and its use cases with <a href=\"https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_toc.htm\">PROC DEEPCAUSAL</a>. I will also show you how it utilizes Deep Neural Networks (DNNs) to perform causal inference as well as policy evaluation and comparison.</p>\n<h2>The powerful, state-of-the-art and easy-to-use PROC DEEPCAUSAL</h2>\n<p>Inference is not valid for the estimators when the estimates from machine learning methods are directly plugged into an econometric model. This way creates highly biased estimators, so econometrics methods need to correct for this bias. PROC DEEPCAUSAL does this by implementing the doubly robust estimation method (suggested by <a href=\"https://doi.org/10.3982/ECTA16901\">Max H. Farrell, Tengyuan Liang, and Sanjog Misra (2021</a>). This method applies DNNs, a powerful machine learning technique, and provides estimates for various causal effect parameters. As well, it performs policy evaluation and policy comparison. DNNs overcome several technical difficulties. These include big data problems, such as high-dimensional covariates; having a mix of discrete or continuous covariates: the unknown nonlinear relationships among the covariates, the outcome, and the treatment assignment. This makes PROC DEEPCAUSAL a very powerful tool for causal inference or analyzing a treatment effect.</p>\n<p>PROC DEEPCAUSAL has two main statements for specifying the causal model. First, there is the <a href=\"https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax07.htm\">PSMODEL statement</a> for specifying the propensity score model. Second is the <a href=\"https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax06.htm\">MODEL statement</a> for specifying the outcome model. The propensity score model estimates the treatment by assigning probability conditional on some covariates. The outcome model describes how the outcome depends on the treatment and some covariates. It also consists of two unknown functions, α( ⋅) and β(⋅), that are estimated by a DNN.</p>\n<p>If you are wondering what causal inference is, check out this <a href=\"https://blogs.sas.com/content/subconsciousmusings/2021/09/07/causal-inference-and-policy-evaluation-with-deep-neural-networks/\">post</a>. It provides preliminary knowledge that makes it easier to understand the concepts explained here.</p>\n<h2>The new SCORE statement in PROC DEEPCAUSAL</h2>\n<p>What exactly is scoring? Scoring is the act of producing predictions for a target variable from a predictive model. The benefit of scoring is to avoid the cost of refitting the model when new predictions are needed. Scoring new data to compute predictions for an existing model is a fundamental stage in the analytics life cycle. For example, in industrial process control or monitoring of atmospheric pollutant levels, a sequential data stream is monitored continuously to flag unexpected changes in the normal behavior of the process as soon as possible. Scoring in these situations can be quite useful.</p>\n<p>In the context of causal inference, computing predictions for new data or testing the precision of prediction on a test data set are only a part of the advantages of scoring. The scoring capability in causal inference can be useful in many different and important ways that I will be discussing in the next sections.</p>\n<p>You can access all the benefits of scoring capability in PROC DEEPCAUSAL by using the recently added <a href=\"https://documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_syntax08.htm\">SCORE statement</a>.</p>\n<p>Let’s look at an example that demonstrates the various advantages of PROC DEEPCAUSAL scoring capability. The causal model in this example is an online music subscription service estimating the effect of targeted customer discounts on the company’s revenue.</p>\n<p>The original data set is provided by the Microsoft research project ALICE. For more details, see <a href=\"https://go.documentation.sas.com/doc/en/pgmsascdc/v_030/casecon/casecon_deepcausal_examples02.htm\">Example 14.2</a> in the PROC DEEPCAUSAL documentation. The data are synthetically generated to protect the privacy of the company, but the model variable names and their meanings are preserved. The data have 10,000 observations and include customers’ personal characteristics, such as age and log-income. The data also includes online behavior history, such as previous purchases and previous online times per week. The treatment variable t is the binary variable of whether or not the discount is applied. The output variable is revenue. Table 1 shows the names of the variables that are used in the model, their types, and their definitions.</p>\n<table width=\"100%\">\n<tbody>\n<tr>\n<td width=\"25%\"><strong>Name</strong></td>\n<td width=\"25%\"><strong>Variable Type</strong></td>\n<td width=\"75%\"><strong>Details</strong></td>\n</tr>\n<tr>\n<td width=\"25%\">account_age</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">User’s account age</td>\n</tr>\n<tr>\n<td width=\"25%\">age</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">User’s age</td>\n</tr>\n<tr>\n<td width=\"25%\">avg_hours</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Average number of hours user was online per week in the past</td>\n</tr>\n<tr>\n<td width=\"25%\">days_visited</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Average number of days user visited website per week in the past</td>\n</tr>\n<tr>\n<td width=\"25%\">friend_count</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Number of friends user connected to in account</td>\n</tr>\n<tr>\n<td width=\"25%\">has_membership</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Whether user has membership</td>\n</tr>\n<tr>\n<td width=\"25%\">is_US</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Whether user accesses website from US</td>\n</tr>\n<tr>\n<td width=\"25%\">songs_purchased</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">Average number of songs user purchased per week in the past</td>\n</tr>\n<tr>\n<td width=\"25%\">income</td>\n<td width=\"25%\">covariate</td>\n<td width=\"75%\">User’s income</td>\n</tr>\n<tr>\n<td width=\"25%\">t</td>\n<td width=\"25%\">treatment</td>\n<td width=\"75%\">Whether a discount is applied</td>\n</tr>\n<tr>\n<td width=\"25%\">revenue</td>\n<td width=\"25%\">outcome</td>\n<td width=\"75%\">Number of songs purchased during discount season times price paid</td>\n</tr>\n</tbody>\n</table>\n<p><em>Table 1: Model Variables</em></p>\n<p>The data are split into three data tables - the training, testing, and “new” data sets. The “new” data observations for the treatment (discount) and the outcome variables (revenue) are set to missing. So, for these customers, you need to figure out the treatment assignment.</p>\n<p>The SAS code below demonstrates how to estimate the treatment effect, that is the effect of discount, and saves the estimation details. These details will be used as input for scoring.</p>\n\n<div class=\"wp_syntax\"><table><tr><td class=\"code\"><pre class=\"sas\" style=\"font-family:monospace;\">   <span style=\"color: #006400; font-style: italic;\">* estimate the treatment effect and save the estimation details;</span>\n   <span style=\"color: #000080; font-weight: bold;\">proc deepcausal</span> <span style=\"color: #000080; font-weight: bold;\">data</span>=mycas.pricing_sample_train;\n      id rowindex;\n      psmodel t = account_age age avg_hours days_visited friends_count\n                  has_membership is_US songs_purchased income /\n                  dnn=<span style=\"color: #66cc66;\">&#40;</span>nodes=<span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span><span style=\"color: #66cc66;\">&#41;</span>\n                  train=<span style=\"color: #66cc66;\">&#40;</span>optimizer=<span style=\"color: #66cc66;\">&#40;</span>miniBatchSize=<span style=\"color: #2e8b57; font-weight: bold;\">500</span> regL1=<span style=\"color: #2e8b57; font-weight: bold;\">0.0001</span> maxEpochs=<span style=\"color: #2e8b57; font-weight: bold;\">32000</span>\n                  algorithm=adam<span style=\"color: #66cc66;\">&#41;</span> nthreads=<span style=\"color: #2e8b57; font-weight: bold;\">20</span> seed=<span style=\"color: #2e8b57; font-weight: bold;\">12345</span> recordseed=<span style=\"color: #2e8b57; font-weight: bold;\">67890</span><span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span>;\n      model revenue = account_age age avg_hours days_visited friends_count\n                  has_membership is_US songs_purchased income /\n                  dnn=<span style=\"color: #66cc66;\">&#40;</span>nodes=<span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span> <span style=\"color: #2e8b57; font-weight: bold;\">32</span><span style=\"color: #66cc66;\">&#41;</span>\n                  train=<span style=\"color: #66cc66;\">&#40;</span>optimizer=<span style=\"color: #66cc66;\">&#40;</span>miniBatchSize=<span style=\"color: #2e8b57; font-weight: bold;\">500</span> regL1=<span style=\"color: #2e8b57; font-weight: bold;\">0.001</span> maxEpochs=<span style=\"color: #2e8b57; font-weight: bold;\">32000</span>\n                  algorithm=adam<span style=\"color: #66cc66;\">&#41;</span> nthreads=<span style=\"color: #2e8b57; font-weight: bold;\">20</span> seed=<span style=\"color: #2e8b57; font-weight: bold;\">12345</span> recordseed=<span style=\"color: #2e8b57; font-weight: bold;\">67890</span><span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span>;\n      infer out=mycas.oest outdetails=mycas.odetails;\n      score outps=mycas.outpsdata outa=mycas.outadata outb=mycas.outbdata;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;</pre></td></tr></table></div>\n\n<p>The output from this estimation is below.</p>\n<p><a href=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png\"><img loading=\"lazy\" class=\"aligncenter size-full wp-image-13569\" src=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png\" alt=\"PROC DEEPCAUSAL Output estimation\" width=\"687\" height=\"532\" srcset=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect.png 687w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-1-output-estimate-treatment-effect-300x232.png 300w\" sizes=\"(max-width: 687px) 100vw, 687px\" /></a></p>\n<p>The parameter average potential outcome for an untreated customer is about 14.32, indicating the average effect on the revenue from this customer type. Similarly, the average potential outcome for a treated customer is about 13.6, indicating the average effect on the revenue from this customer type. The parameter ATE (average treatment effect) is usually the main parameter of interest in causal inference. The value of ATE is about -0.72.</p>\n<p>This is statistically significant, suggesting that the discount, on average, causes revenue to decrease. However, the estimate for ATT (average treatment effect on the treated) reflects a positive effect of the discount on revenue. The value is small but significant. This could mean that the discount was given to some customers who didn’t care about it. Or the discount wasn’t given to some customers who would have used it. Identifying the characteristics of customers to whom the discount matters helps construct the optimum policy to use the least amount of resources to gain the most profit. The scoring ability can help with this quite efficiently.</p>\n<h2>Scoring on testing data</h2>\n<p>Testing the precision of prediction is important. However, in the causal inference context, two other points are also important to consider. The first would be making sure that the observations in the two data sets are coming from the same data generating process (DGP). This would help you understand whether the two policies are the same or not. The second would be to see if there are any outliers in the new data set. You can use the scoring capability of PROC DEEPCAUSAL to handle these three important points. This is how the SAS code might look.</p>\n\n<div class=\"wp_syntax\"><table><tr><td class=\"code\"><pre class=\"sas\" style=\"font-family:monospace;\"><span style=\"color: #006400; font-style: italic;\">* scoring 1: on testing data:\n   * (1) testing prediction precision (for outcome of regression or treatment of classification);</span>\n   <span style=\"color: #006400; font-style: italic;\">* (2) finding out whether the DGPs are the same (e.g., is there a policy difference?) and the outliers;</span>\n   <span style=\"color: #000080; font-weight: bold;\">proc deepcausal</span> <span style=\"color: #000080; font-weight: bold;\">data</span>=mycas.pricing_sample_test;\n      id rowindex;\n      infer out=mycas.oest2 outdetails=mycas.odetails2;\n      score inps=mycas.outpsdata ina=mycas.outadata inb=mycas.outbdata;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;</pre></td></tr></table></div>\n\n<p>Note that the MODEL and PSMODEL statements are not included. With the SCORE statement, the model estimation information is already included. So, there is no need to reestimate the model. This saves you a considerable amount of time.</p>\n<p><a href=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png\"><img loading=\"lazy\" class=\"aligncenter size-full wp-image-13572\" src=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png\" alt=\"PROC DEEPCAUSAL Output 2 testing data \" width=\"765\" height=\"533\" srcset=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data.png 765w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-2-output-scoring-testing-data-300x209.png 300w\" sizes=\"(max-width: 765px) 100vw, 765px\" /></a></p>\n<p>The results are interesting. The estimates for the ATE and many of the other parameters are completely different for the test data set as compared to the training data set. In fact, they are so “off” that it’s hard to make sense out of them. What is going on? There are two explanations for this. Either the two data sets are not generated from the same DGP or there are some outliers. As I mentioned before, this example uses simulated data, eliminating the first possibility. So, there must be some outliers. You can easily identify and eliminate them. Then you can confirm that scoring again produces more similar results to those obtained using the training data set.</p>\n<h2>Scoring for determining the treatment of new customers</h2>\n<p>So how would you deal with new customers? How would you determine whether to give them a discount or not given the characteristics that you observe about them?</p>\n<p>The third data set contains the information for these new customers. Note that the new observations are missing the outcome and the treatment variable values. Therefore, it is impossible to incorporate the new data into the existing one and reestimate the model. However, we can still use these data to determine who receives the discount and who doesn’t by using the score action of the aStore action set.</p>\n<p>One way of approaching this problem is to follow the existing policy we observed in the data set. For this, you can use the analytic store for the propensity score model, outpsdata, that is generated using the OUTPS= option in the SCORE statement during the first call to PROC DEEPCAUSAL. Then calculate the logit of the transformed propensity score and construct a policy based on this propensity score. The SAS code might look like this.</p>\n\n<div class=\"wp_syntax\"><table><tr><td class=\"code\"><pre class=\"sas\" style=\"font-family:monospace;\">   <span style=\"color: #006400; font-style: italic;\">* Scoring 2: Determine the treatment on new customers;</span>\n   <span style=\"color: #006400; font-style: italic;\">* 1) follow the existing policy;</span>\n   <span style=\"color: #000080; font-weight: bold;\">proc cas</span>;\n      aStore.score / <span style=\"color: #0000ff;\">table</span>=<span style=\"color: #a020f0;\">\"pricing_sample_new_customers\"</span> rstore=<span style=\"color: #a020f0;\">\"outpsdata\"</span> casout=<span style=\"color: #66cc66;\">&#123;</span>name=<span style=\"color: #a020f0;\">\"oldway\"</span>,<span style=\"color: #0000ff;\">replace</span>=true<span style=\"color: #66cc66;\">&#125;</span> copyvars=<span style=\"color: #66cc66;\">&#123;</span><span style=\"color: #a020f0;\">\"rowindex\"</span><span style=\"color: #66cc66;\">&#125;</span>;\n   <span style=\"color: #000080; font-weight: bold;\">quit</span>;\n&nbsp;\n   <span style=\"color: #000080; font-weight: bold;\">data</span> oldway;\n      <span style=\"color: #0000ff;\">set</span> mycas.oldway;\n      <span style=\"color: #0000ff;\">call</span> streaminit<span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #2e8b57; font-weight: bold;\">12345</span><span style=\"color: #66cc66;\">&#41;</span>;\n      u = rand<span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #a020f0;\">\"uniform\"</span><span style=\"color: #66cc66;\">&#41;</span>;\n      t_old = <span style=\"color: #66cc66;\">&#40;</span>u <= <span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #2e8b57; font-weight: bold;\">1</span>/<span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #2e8b57; font-weight: bold;\">1</span>+<span style=\"color: #0000ff;\">exp</span><span style=\"color: #66cc66;\">&#40;</span>-P_t<span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span>;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;</pre></td></tr></table></div>\n\n<p>However, this approach does not necessarily optimize the revenue from these new customers. The optimum policy, the one that maximizes the revenue, should always lead to a positive treatment effect. In our framework, this translates into treating everyone whose Individual Treatment Effect (ITE) is positive. The function β(x) measures ITE and this function is unknown. Therefore, in practice, an estimate of it is used. This, in effect, translates into a policy giving discounts to those whose predicted beta values are positive. For this, you can use the astore for the information for the beta function estimation, outbdata (similarly generated using the OUTB= option), and create a new policy variable that takes the value of 1 if the predicted revenue for these new customers is positive and the value 0 if it is negative.</p>\n\n<div class=\"wp_syntax\"><table><tr><td class=\"code\"><pre class=\"sas\" style=\"font-family:monospace;\"><span style=\"color: #006400; font-style: italic;\">* 2) optimize the policy;</span>\n   <span style=\"color: #000080; font-weight: bold;\">proc cas</span>;\n      aStore.score / <span style=\"color: #0000ff;\">table</span>=<span style=\"color: #a020f0;\">\"pricing_sample_new_customers\"</span> rstore=<span style=\"color: #a020f0;\">\"outbdata\"</span> casout=<span style=\"color: #66cc66;\">&#123;</span>name=<span style=\"color: #a020f0;\">\"newway\"</span>,<span style=\"color: #0000ff;\">replace</span>=true<span style=\"color: #66cc66;\">&#125;</span> copyvars=<span style=\"color: #66cc66;\">&#123;</span><span style=\"color: #a020f0;\">\"rowindex\"</span><span style=\"color: #66cc66;\">&#125;</span>;\n   <span style=\"color: #000080; font-weight: bold;\">quit</span>;\n&nbsp;\n   <span style=\"color: #000080; font-weight: bold;\">data</span> newway;\n      <span style=\"color: #0000ff;\">set</span> mycas.newway;\n      t_new = <span style=\"color: #66cc66;\">&#40;</span><span style=\"color: #66cc66;\">&#40;</span>P__revenue_beta_<span style=\"color: #66cc66;\">&#41;</span> > <span style=\"color: #2e8b57; font-weight: bold;\">0</span><span style=\"color: #66cc66;\">&#41;</span>;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;</pre></td></tr></table></div>\n\n<p>The last two columns of Table 2 show the resulting two policies, t_old and t_new, which is also the optimal policy, for the first 20 observations.</p>\n<div id=\"attachment_13575\" style=\"width: 435px\" class=\"wp-caption aligncenter\"><a href=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png\"><img aria-describedby=\"caption-attachment-13575\" loading=\"lazy\" class=\"size-full wp-image-13575\" src=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png\" alt=\"Table 1: Old and New (Optimized) Policies for the New Customers\" width=\"425\" height=\"522\" srcset=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers.png 425w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-3-table-old-new-policies-customers-244x300.png 244w\" sizes=\"(max-width: 425px) 100vw, 425px\" /></a><p id=\"caption-attachment-13575\" class=\"wp-caption-text\">Table 2: Old and New (Optimized) Policies for the New Customers</p></div>\n<p>The design of this optimal policy runs into the problem of interpretability. The function β(x) is unknown,  and a DNN is used to obtain an estimate for it. It’s not known how individual covariates affect the design of the policy. This interpretability issue might be a problem, particularly for policymakers at the government level. In practice, the optimized policy should be as simple as possible and easily explainable. It’s better to use just a small subset of the covariates and easy-to-follow rules. Of course, this raises the question of which subset. Fortunately, the scoring capability of PROC DEEPCAUSAL can help!</p>\n<h2>Scoring for policy evaluation/comparison/optimization</h2>\n<p>The optimal policy in application, that is giving discounts to customers for whom _BETA_ is positive, is hard to beat. But at least you can come up with a policy that can come close to the optimal policy and is interpretable. Consider a way of doing personalized discounts based on income. Let’s say you’d like to target customers with the log-income level of at least a threshold level that you pick, say, values starting from 0 to 6.25 by 0.25 increments. Here are the data steps that take care of creating this set of policies.</p>\n\n<div class=\"wp_syntax\"><table><tr><td class=\"code\"><pre class=\"sas\" style=\"font-family:monospace;\">   <span style=\"color: #006400; font-style: italic;\">* scoring 3: policy learning, the evaluation and comparison;</span>\n   <span style=\"color: #006400; font-style: italic;\">* create policies based on income;</span>\n   <span style=\"color: #000080; font-weight: bold;\">data</span> mycas.discountPolicy_income;\n      <span style=\"color: #0000ff;\">set</span> mycas.odetails;\n      <span style=\"color: #0000ff;\">array</span> pi<span style=\"color: #66cc66;\">&#91;</span><span style=\"color: #2e8b57; font-weight: bold;\">26</span><span style=\"color: #66cc66;\">&#93;</span> pi0 - pi25;\n      <span style=\"color: #0000ff;\">do</span> k = <span style=\"color: #2e8b57; font-weight: bold;\">1</span> to <span style=\"color: #2e8b57; font-weight: bold;\">26</span>;\n         pi<span style=\"color: #66cc66;\">&#91;</span>k<span style=\"color: #66cc66;\">&#93;</span> = <span style=\"color: #66cc66;\">&#40;</span>income <= <span style=\"color: #2e8b57; font-weight: bold;\">0.25</span><span style=\"color: #006400; font-style: italic;\">*(k-1));</span> \n         <span style=\"color: #0000ff;\">end</span>; \n         t_opt = <span style=\"color: #66cc66;\">&#40;</span>_beta_><span style=\"color: #2e8b57; font-weight: bold;\">0</span><span style=\"color: #66cc66;\">&#41;</span>;\n      <span style=\"color: #0000ff;\">keep</span> rowindex id t account_age age avg_hours days_visited\n           friends_count has_membership is_US \n           songs_purchased income revenue pi0-pi25 t_opt;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;\n&nbsp;\n   <span style=\"color: #000080; font-weight: bold;\">proc deepcausal</span> <span style=\"color: #000080; font-weight: bold;\">data</span>=mycas.discountPolicy_income;\n      id id;\n      infer policy=<span style=\"color: #66cc66;\">&#40;</span>t_opt pi0-pi25<span style=\"color: #66cc66;\">&#41;</span> policyComparison=<span style=\"color: #66cc66;\">&#40;</span>base=<span style=\"color: #66cc66;\">&#40;</span>t_opt<span style=\"color: #66cc66;\">&#41;</span> compare=<span style=\"color: #66cc66;\">&#40;</span>t pi0-pi25<span style=\"color: #66cc66;\">&#41;</span><span style=\"color: #66cc66;\">&#41;</span>\n            out=mycas.oest4 outdetails=mycas.odetails4;\n      score inps=mycas.outpsdata ina=mycas.outadata inb=mycas.outbdata;\n   <span style=\"color: #000080; font-weight: bold;\">run</span>;</pre></td></tr></table></div>\n\n<p>There are 26 policies, pi0 through pi25. Policies pi0 and pi25 have special meanings. Policy pi0 corresponds to the policy of giving no discount, as no one’s income is less than 0. Policy pi25 corresponds to giving a discount to everyone, as everyone in this data set has income less than 6.25. Now you can run PROC DEEPCAUSAL including this new set of policies in the POLICY= option in the INFER statement. Naturally, comparing this policy with the optimal policy is desired, therefore, t_opt, the optimal policy, is specified in the BASE= suboption of the POLICYCOMPARISON= option. For policy comparison, a negative value for a given policy indicates that it achieves a worse result than the base policy, and a value close to 0 indicates that it is very similar to the base policy. A plot of policy evaluation output is in Figure 1.</p>\n<div id=\"attachment_13578\" style=\"width: 658px\" class=\"wp-caption aligncenter\"><a href=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png\"><img aria-describedby=\"caption-attachment-13578\" loading=\"lazy\" class=\"size-full wp-image-13578\" src=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png\" alt=\"PROC DEEPCAUSAL Figure 1: Plot of the effect of new set of policies on revenue\" width=\"648\" height=\"489\" srcset=\"https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot.png 648w, https://blogs.sas.com/content/subconsciousmusings/files/2022/09/sept-walton-code-4-plot-300x226.png 300w\" sizes=\"(max-width: 648px) 100vw, 648px\" /></a><p id=\"caption-attachment-13578\" class=\"wp-caption-text\">Figure 1: Plot of the effect of a new set of policies on revenue</p></div>\n<p>The policy pi4 is almost as good as the optimal policy. Policy pi4 corresponds to threshold 1, that is the policy to provide discounts to customers whose log-income is less than or equal to 1. An important advantage of the policy pi4 is its easy interpretability, unlike the optimal policy.</p>\n<p>So, can we do even better? Can we find another set of policies that can outperform pi4 and come even closer to the optimal policy t_opt? This is where the scoring capability comes in handy (again). You can try various sets of policies and compare them with the optimal policy as the base policy by using the POLICYCOMPARISON= option. Thanks to the SCORE statement, each call to PROC DEEPCAUSAL takes only seconds!</p>\n<h2>Conclusion</h2>\n<p>Hopefully, you can now see how the scoring capability of PROC DEEPCAUSAL can contribute to making better business decisions. In the causal inference framework, the scoring extends beyond testing prediction. You can also benefit from the scoring capability in many ways that are unique to causal inference. As I demonstrated, you could use it for checking the data accuracy. You can determine if the observations from the two data sets come from the same DGP as well as uncover potential outliers.</p>\n<p>I also showed how you can use scoring to determine the treatment on completely new observations. As well, you can also use it for policy evaluation, comparison, and optimization. The latter advantage is particularly important for businesses. They are increasingly interested in understanding the different responses from customers so they can tailor their actions to the customer's needs. Understanding customer characteristics helps businesses to construct policies that use the least number of resources to optimize revenue. The scoring capability in PROC DEEPCAUSAL can most definitely help with that goal.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/\">Using PROC DEEPCAUSAL to optimize revenue through policy evaluation</a> appeared first on <a rel=\"nofollow\" href=\"https://blogs.sas.com/content/subconsciousmusings\">The SAS Data Science Blog</a>.</p>\n",
  "wfw:commentRss": "https://blogs.sas.com/content/subconsciousmusings/2022/09/12/using-proc-deepcausal-to-optimize-revenue-through-policy-evaluation/feed/",
  "slash:comments": 0,
  "enclosure": ""
}