{
  "title": "Why pandas users should be excited about Apache Arrow",
  "link": "",
  "published": "2016-02-22T10:00:00-08:00",
  "updated": "2016-02-22T10:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2016-02-22:/blog/pandas-and-apache-arrow/",
  "summary": "<p>I'm super excited to be involved in the new open source <a href=\"http://arrow.apache.org\">Apache Arrow</a>\ncommunity initiative. For Python (and R, too!), it will help enable</p>\n<ul>\n<li>Substantially improved data access speeds</li>\n<li>Closer to native performance Python extensions for big data systems like\n  Apache Spark</li>\n<li>New in-memory analytics functionality for nested / JSON-like data</li>\n</ul>\n<p>There's plenty of places you can learn more about Arrow, but this post is about\nhow it's specifically relevant to pandas users. See, for example:</p>\n<ul>\n<li><a href=\"http://vision.cloudera.com/python-and-hadoop-a-state-of-the-union/\">\"Python and Hadoop: A State of the Union\"</a></li>\n<li><a href=\"http://blog.cloudera.com/blog/2016/02/introducing-apache-arrow-a-fast-interoperable-in-memory-columnar-data-structure-standard/\">\"Introducing Apache Arrow: A Fast, Interoperable In-Memory Columnar Data Structure Standard\"</a></li>\n<li><a href=\"http://www.dremio.com/blog/apache-arrow/\">\"Introducing Apache Arrow: Columnar In-Memory Analytics\"</a></li>\n</ul>",
  "content": "<p>I'm super excited to be involved in the new open source <a href=\"http://arrow.apache.org\">Apache Arrow</a>\ncommunity initiative. For Python (and R, too!), it will help enable</p>\n<ul>\n<li>Substantially improved data access speeds</li>\n<li>Closer to native performance Python extensions for big data systems like\n  Apache Spark</li>\n<li>New in-memory analytics functionality for nested / JSON-like data</li>\n</ul>\n<p>There's plenty of places you can learn more about Arrow, but this post is about\nhow it's specifically relevant to pandas users. See, for example:</p>\n<ul>\n<li><a href=\"http://vision.cloudera.com/python-and-hadoop-a-state-of-the-union/\">\"Python and Hadoop: A State of the Union\"</a></li>\n<li><a href=\"http://blog.cloudera.com/blog/2016/02/introducing-apache-arrow-a-fast-interoperable-in-memory-columnar-data-structure-standard/\">\"Introducing Apache Arrow: A Fast, Interoperable In-Memory Columnar Data Structure Standard\"</a></li>\n<li><a href=\"http://www.dremio.com/blog/apache-arrow/\">\"Introducing Apache Arrow: Columnar In-Memory Analytics\"</a></li>\n</ul>\n\n\n<h2>Accelerating data access for pandas users on Hadoop clusters</h2>\n<p>For average pandas users, the gold standard for storing and retrieving data on\nlocal machines (or network file systems) is usually one of:</p>\n<ul>\n<li>CSV files, using <code>pandas.read_csv</code></li>\n<li>HDF5 data format files, using <code>pandas.HDFStore</code></li>\n<li>Another binary dataformat, like the <a href=\"https://github.com/Blosc/c-bloscx\">Blosc</a>-powered <a href=\"https://github.com/Blosc/bcolz\">bcolz</a></li>\n</ul>\n<p>But if your data is in a Hadoop cluster, it may not be as simple as reading a\nfile off disk. Here's some of the data-providing systems and storage formats\nyou can access from pandas:</p>\n<p><center>\n<img src=\"../../images/arrow_pandas_image1.png\" alt=\"Hadoop data access\" width=\"60%\"/>\n</center></p>\n<p>Unfortunately, the quality of these data connections for pandas are highly\nvariable. I did an <a href=\"https://gist.github.com/wesm/0cb5531b1c2e346a0007\">in-depth exploration</a> to compare the performance of\nretrieving a <code>pandas.DataFrame</code> with 1 million rows with a net footprint of\nabout 90 megabytes. Here is the performance summary:</p>\n<table>\n  <tr>\n  <th><strong>Method</strong></th>\n  <th><strong>Speed (sec)</strong></th>\n  <th><strong>vs HDF5</strong></th>\n  <th><strong>vs read_csv</strong></th>\n  <th><strong>Effective speed</strong></th>\n  </tr>\n  <tr>\n  <td><strong>pandas.HDF5Store (uncompressed)</strong></td>\n  <td>0.117</td>\n  <td>1.0x</td>\n  <td>0.05x</td>\n  <td>769.23 MB/s</td>\n  </tr>\n  <td><strong>pandas.read_csv</strong></td>\n  <td>2.43</td>\n  <td>20.8x</td>\n  <td>1.00x</td>\n  <td>37.04 MB/s</td>\n  </tr>\n  <tr>\n  <td><strong>Spark DataFrame.toPandas</strong></td>\n  <td>13.6</td>\n  <td>116.2x</td>\n  <td>5.60x</td>\n  <td>6.62 MB/s</td>\n  </tr>\n  <tr>\n  <td><strong>Impala SELECT (via impyla)</strong></td>\n  <td>15.3</td>\n  <td>130.8x</td>\n  <td>6.30x</td>\n  <td>5.88 MB/s</td>\n  </tr>\n</table>\n\n<p>As the original author of both <code>read_csv</code> and <code>HDFStore</code>, these benchmarks make\nme very proud, but it also shows what level of data access performance pandas\nusers should expect in general.</p>\n<p>The price of data serialization can be directly seen in the Impala query\nprofile (note, this is a DEBUG Impala build):</p>\n<div class=\"github\"><pre><span></span><code><span class=\"gh\">Operator       #Hosts  Avg Time  Max Time  #Rows</span>\n<span class=\"gh\">------------------------------------------------</span>\n01:EXCHANGE         1     149ms     149ms  1.00M\n00:SCAN HDFS        2     269ms     282ms  1.00M\n</code></pre></div>\n\n<p>So more than 90% of the execution time is data serialization.</p>\n<p>The reasons why the Spark and HiveServer2 data access speeds are slow boil down\nto a couple of factors:</p>\n<ul>\n<li>Data is transferred in a form that is expensive to deserialize.</li>\n<li>Data is passing through scalar Python objects (i.e. using\n  <code>DataFrame.from_records</code> on a list of tuples) rather than going directly into\n  pandas objects at the C API level.</li>\n</ul>\n<p>Apache Arrow helps mitigate both of these problems. In the reasonably near\nfuture, I expect things architecturally to look like this:</p>\n<p><center>\n<img src=\"../../images/arrow_pandas_image2.png\" alt=\"Arrow data access\" width=\"50%\"/>\n</center></p>\n<p>Realistically, the performance of ingesting data into pandas via Arrow should\nbe significantly faster than reading a CSV (being binary and columnar).</p>\n<p>It's important to note that using <code>pandas.read_csv</code> as a standard for data\naccess performance doesn't completely make sense. Parsing a CSV is fairly\nexpensive, which is why reading from HDF5 is 20x faster than parsing a CSV.</p>\n<h2>On Apache Parquet</h2>\n<p>The <a href=\"https://parquet.apache.org\">Apache Parquet</a> data format is a column-oriented binary storage format\nfor structured data optimized for IO throughput and fast analytics. Since it\nwas designed primarily for use in a MapReduce setting initially, most\ndevelopment energy was poured into the <a href=\"https://github.com/apache/parquet-mr\">parquet-mr</a> Java implementation.</p>\n<p>Last month I started getting involved in <a href=\"https://github.com/apache/parquet-cpp\">parquet-cpp</a>, a native C++11\nimplementation of Parquet. I'm pleased to report we've made great progress on\nthis in the last 6 weeks, and native read/write support for pandas users is\nreasonably near on the horizon. I'll report here when I get the whole thing\nworking end-to-end.</p>\n<h2>Summary and the road ahead</h2>\n<p>Data access performance is only one area where Arrow will help the Python\necosystem. High performance Python extensions and native in-memory handling for\nnested columnar data will also make a big impact. I look forward to sharing\nongoing progress updates.</p>",
  "category": ""
}