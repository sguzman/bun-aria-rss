{
  "title": "Deriving Gibbs distribution from stochastic gradients",
  "link": "https://mirror2image.wordpress.com/2013/11/13/deriving-gibbs-distribution-from-stochastic-gradients/",
  "dc:creator": "mirror2image",
  "pubDate": "Wed, 13 Nov 2013 09:49:19 +0000",
  "category": "Uncategorized",
  "guid": "http://mirror2image.wordpress.com/?p=1461",
  "description": "Stochastic gradients is one of the most important tools in optimization and machine learning (especially for Deep Learning &#8211; see for example ConvNet). One of it&#8217;s advantage is that it behavior is well understood in general case, by application of methods of statistical mechanics. In general form stochastic gradient descent could be written as where [&#8230;]",
  "content:encoded": "<p><a href=\"Stochastic gradients\">Stochastic gradients</a> is one of the most important tools in optimization and machine learning (especially for Deep Learning &#8211; see for example <a href=\"https://code.google.com/p/cuda-convnet/\">ConvNet</a>). One of it&#8217;s advantage is that it behavior is well understood in general case, by application of methods of <a href=\"http://en.wikipedia.org/wiki/Statistical_mechanics\">statistical mechanics</a>.</p>\n<p>In general form stochastic gradient descent could be written as<br />\n<img src=\"https://s0.wp.com/latex.php?latex=w_%7Bn%2B1%7D+%3D+w_n+-+%5Calpha+%5Cbigtriangledown+E%28w_n%29+%2B+F_n&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"w_{n+1} = w_n - &#92;alpha &#92;bigtriangledown E(w_n) + F_n\" class=\"latex\" /><br />\nwhere <img src=\"https://s0.wp.com/latex.php?latex=F_n&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"F_n\" class=\"latex\" /> is a random variable with expectation zero.<br />\nTo apply methods of statistical mechanics we can rewrite it in continuous form, as stochastic gradient flow<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cpartial+J%2F%5Cpartial+t+%3D+-+%5Cbigtriangledown_J+E%28J%29+%2BF%28t%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;partial J/&#92;partial t = - &#92;bigtriangledown_J E(J) +F(t)\" class=\"latex\" /><br />\nand random variable <em>F(t)</em> we assume to be white noise for simplicity.<br />\nIn that moment most of textbooks and papers refer to &#8220;methods of statistical mechanics&#8221; to show that<br />\nstochastic gradient flow has invariant probability distribution, which is called <a href=\"http://en.wikipedia.org/wiki/Gibbs_measure\">Gibbs distribution</a><br />\n<img src=\"https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cfrac+%7Bexp%28-%5Cbeta+E%28x%29%29%7D%7BZ%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p(x) = &#92;frac {exp(-&#92;beta E(x))}{Z}\" class=\"latex\" /><br />\nand from here derive some interesting things like temperature and free energy.<br />\nThe question is &#8211; how Gibbs distribution derived from stochastic gradient flow?</p>\n<p>First we have to understand what stochastic gradient flow really means.<br />\nIt&#8217;s not a partial differential equation (PDE), because it include random variable, which is not a function <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D+%5Cto+%5Cmathbb%7BR%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R} &#92;to &#92;mathbb{R}\" class=\"latex\" />. In fact it&#8217;s a <a href=\"http://en.wikipedia.org/wiki/Stochastic_differential_equation\">stochastic differential equation (SDE) </a>. SDE use some complex mathematical machinery and relate to partial differential equations, probability/statistics, measure theory and ergodic theory. However they used a lot in finance, so there is quite a number of textbooks on SDE for non-mathematicians. For the short, minimalistic and accessible book on stochastic differential equation I can&#8217;t recommend highly enough <a href=\"math.berkeley.edu/~evans/SDE.course.pdf‎\">introduction to SDE</a> by <a href=\"http://math.berkeley.edu/~evans/\">L.C. Evans</a></p>\n<p>SDE in question is called <a href=\"http://en.wikipedia.org/wiki/Ito_diffusion\">Ito diffusion</a>. Solution of that equation is a <a href=\"http://en.wikipedia.org/wiki/Stochastic_process\">stochastic process</a> &#8211; collection of random variables parametrized by time. Sample path of stochastic process in question as a function of time is nowhere differentiable &#8211; it&#8217;s difficult to talk about it in term of derivatives, so it is defined through it&#8217;s integral form.<br />\nFirst I&#8217;ll notice that integral of white noise is actually Brownian motion, or <a href=\"http://en.wikipedia.org/wiki/Wiener_process\">Wiener process</a>.<br />\nAssume that we have stochastic differential equation written in informal manner<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cpartial+X+%2F+%5Cpartial+t+%3D+b%28X%2C+t%29+%2B+g%28X%2C+t%29+F%28t%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;partial X / &#92;partial t = b(X, t) + g(X, t) F(t)\" class=\"latex\" /> with <em>X</em> -stochastic process and <em>F(t)</em> &#8211; white noise<br />\nIt&#8217;s integral form is<br />\n<img src=\"https://s0.wp.com/latex.php?latex=X%28T%29+-+X%280%29+%3D+%5Cint_%7B0%7D%5E%7BT%7D+b%28X%2C+t%29+dt+%2B+%5Cint_%7B0%7D%5E%7BT%7D+g%28X%2C+t%29+dW%28t%29+%5C+%5C+%281%29+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X(T) - X(0) = &#92;int_{0}^{T} b(X, t) dt + &#92;int_{0}^{T} g(X, t) dW(t) &#92; &#92; (1) \" class=\"latex\" /><br />\nwhere <em>W(t)</em> is a Wiener process<br />\n<img src=\"https://s0.wp.com/latex.php?latex=W%28t%29+%3D+%5Cint+F%28t%29+dt&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"W(t) = &#92;int F(t) dt\" class=\"latex\" /><br />\nThis equation is usually written in the form<br />\n<img src=\"https://s0.wp.com/latex.php?latex=dX+%3D+b%28X%2Ct%29dt+%2B+g%28X%2Ct%29dW+%5C+%5C+%282%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"dX = b(X,t)dt + g(X,t)dW &#92; &#92; (2)\" class=\"latex\" /><br />\nThis is only a notation for integral equation, <em>d</em> here is not a differential.<br />\nReturning to (1)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint_%7B0%7D%5E%7BT%7D+b%28X%2C+t%29+dt&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int_{0}^{T} b(X, t) dt\" class=\"latex\" /> is an integral along sample path, it&#8217;s meaning is obvious, or it can be defined as limit of <a href=\"http://en.wikipedia.org/wiki/Riemann_sum\">Riemann sums</a> with respect to time.</p>\n<p>The most notable thing here is<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint_%7B0%7D%5E%7BT%7D+g%28X%2C+t%29+dW%28t%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int_{0}^{T} g(X, t) dW(t)\" class=\"latex\" /> &#8211; integral with respect to Wiener process (3)<br />\nIt&#8217;s a <a href=\"http://en.wikipedia.org/wiki/Stochastic_calculus\">stochastic integral</a>, and it&#8217;s defined in the courses of stochastic differential equation as the limit of <a href=\"http://en.wikipedia.org/wiki/Riemann_sum\">Riemann sums</a> of random variables, in the manner similar to definition of ordinary integral.<br />\nCuriously, stochastic integral is not quite well defined. Depending on the form of the sum it produce different results, like <a href=\"http://en.wikipedia.org/wiki/It%C5%8D_integral\">Ito integral</a>:</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cint_0%5Et+g+%5C%2Cd+W+%3D%5Clim_%7Bn%5Crightarrow%5Cinfty%7D+%5Csum_%7B%5Bt_%7Bi-1%7D%2Ct_i%5D%5Cin%5Cpi_n%7Dg_%7Bt_%7Bi-1%7D%7D%28W_%7Bt_i%7D-W_%7Bt_%7Bi-1%7D%7D%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int_0^t g &#92;,d W =&#92;lim_{n&#92;rightarrow&#92;infty} &#92;sum_{[t_{i-1},t_i]&#92;in&#92;pi_n}g_{t_{i-1}}(W_{t_i}-W_{t_{i-1}})\" class=\"latex\" /><br />\nDifferent Riemann sums produce different integral &#8211; <a href=\"http://en.wikipedia.org/wiki/Stratonovich_integral\">Stratonovich integral</a>:<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint_0%5Et+g+%5C%2Cd+W+%3D%5Clim_%7Bn%5Crightarrow%5Cinfty%7D+%5Csum_%7B%5Bt_%7Bi-1%7D%2Ct_i%5D%5Cin%5Cpi_n%7D%28g_%7Bt_i%7D+%2B+g_%7Bt_%7Bi-1%7D%7D%29%2F2%28W_%7Bt_i%7D-W_%7Bt_%7Bi-1%7D%7D%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int_0^t g &#92;,d W =&#92;lim_{n&#92;rightarrow&#92;infty} &#92;sum_{[t_{i-1},t_i]&#92;in&#92;pi_n}(g_{t_i} + g_{t_{i-1}})/2(W_{t_i}-W_{t_{i-1}})\" class=\"latex\" /></p>\n<p>Ito integral used more often in statistics because it use <img src=\"https://s0.wp.com/latex.php?latex=g_%7Bt_%7Bi-1%7D%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"g_{t_{i-1}}\" class=\"latex\" /> &#8211; it don&#8217;t &#8220;look forward&#8221;, and Stratonovich more used in theoretical physics.</p>\n<p>Returning to Ito integral &#8211; Ito integral is stochastic process itself, and it has expectation zero for each <em>t</em>.<br />\nFrom definition of Ito integral follow one of the most important tools of stochastic calculus &#8211; <a href=\"http://en.wikipedia.org/wiki/It%C5%8D's_lemma\">Ito Lemma</a> (or Ito formula)<br />\nIto lemma states that for solution of SDE (2)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=dX+%3D+b%28X%2C+t%29dt+%2B+g%28x%2C+t%29dW&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"dX = b(X, t)dt + g(x, t)dW\" class=\"latex\" /> <em>X, b, W</em> &#8211; vectors, <em>g</em> &#8211; matrix<br />\nwere <em>W</em> is Wiener process (actually some more general process) and <em>b</em> and <em>g</em> are good enough</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=du%28%5Cmathbf%7BX%7D%2C+t%29+%3D+%5Cfrac%7B%5Cpartial+u%7D%7B%5Cpartial+t%7D+dt+%2B+%28%5Cnabla_%5Cmathbf%7BX%7D%5E%7B%5Cmathsf+T%7D+u%29+d%5Cmathbf%7BX%7D+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+%28gg%5E%5Cmathsf%7BT%7D%29_%7Bi%2Cj%7D%5Ctfrac%7B+%5Cpartial%5E2+u%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D+dt&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"du(&#92;mathbf{X}, t) = &#92;frac{&#92;partial u}{&#92;partial t} dt + (&#92;nabla_&#92;mathbf{X}^{&#92;mathsf T} u) d&#92;mathbf{X} + &#92;tfrac{1}{2} &#92;sum_{i,j} (gg^&#92;mathsf{T})_{i,j}&#92;tfrac{ &#92;partial^2 u}{&#92;partial x_i &#92;partial x_j} dt\" class=\"latex\" /></p>\n<p>where <img src=\"https://s0.wp.com/latex.php?latex=%5Cnabla_%5Cmathbf%7BX%7D+u&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;nabla_&#92;mathbf{X} u\" class=\"latex\" /> is the gradient.<br />\nFrom Ito lemma follow Ito product rule for scalar processes: applying Ito formula to process <img src=\"https://s0.wp.com/latex.php?latex=V+%3D+%7B+X+%5Cchoose+Y%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"V = { X &#92;choose Y}\" class=\"latex\" /> combined from two processes <em>X</em> and <em>Y</em> to function <em>u(V) = XY</em><br />\n<img src=\"https://s0.wp.com/latex.php?latex=d%28XY%29+%3D+XdY+%2B+YdX+%2B+g_x+%5Ccdot+g_y+dt&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d(XY) = XdY + YdX + g_x &#92;cdot g_y dt\" class=\"latex\" /><br />\nUsing Ito formula and Ito product rule it is possible to get <a href=\"http://en.wikipedia.org/wiki/Feynman%E2%80%93Kac_formula\">Feynman–Kac formula</a> (derivation could be found in the wikipedia, it use only Ito formula, Ito product rule and the fact that expectation of Ito integral (3) is zero):<br />\nfor partial differential equation (PDE)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+u%7D%7B%5Cpartial+t%7D+%2B+%28%5Cnabla_%5Cmathbf%7BX%7Du%29%5E%7B%5Cmathsf+T%7D+b+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+%28gg%5E%5Cmathsf%7BT%7D%29_%7Bi%2Cj%7D%5Ctfrac%7B+%5Cpartial%5E2+u%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D+-+vu+%3D+f&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;frac{&#92;partial u}{&#92;partial t} + (&#92;nabla_&#92;mathbf{X}u)^{&#92;mathsf T} b + &#92;tfrac{1}{2} &#92;sum_{i,j} (gg^&#92;mathsf{T})_{i,j}&#92;tfrac{ &#92;partial^2 u}{&#92;partial x_i &#92;partial x_j} - vu = f\" class=\"latex\" /><br />\nwith terminal condition<br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2C+T%29+%3D+%5Cpsi%28x%29+%5C+%5C+%284%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x, T) = &#92;psi(x) &#92; &#92; (4)\" class=\"latex\" /><br />\nsolution can be written as <a href=\"http://en.wikipedia.org/wiki/Conditional_expectation\">conditional expectation</a>:<br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2Ct%29+%3D+E%5Cleft%5B+%5Cint_t%5ET+e%5E%7B-+%5Cint_t%5Er+v%28X_%5Ctau%2C%5Ctau%29%5C%2C+d%5Ctau%7Df%28X_r%2Cr%29dr+%2B+e%5E%7B-%5Cint_t%5ET+v%28X_%5Ctau%2C%5Ctau%29%5C%2C+d%5Ctau%7D%5Cpsi%28X_T%29+%5CBigg%7C+X_t%3Dx+%5Cright%5D+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x,t) = E&#92;left[ &#92;int_t^T e^{- &#92;int_t^r v(X_&#92;tau,&#92;tau)&#92;, d&#92;tau}f(X_r,r)dr + e^{-&#92;int_t^T v(X_&#92;tau,&#92;tau)&#92;, d&#92;tau}&#92;psi(X_T) &#92;Bigg| X_t=x &#92;right] \" class=\"latex\" /><br />\nFeynman–Kac formula establish connection between PDE and stochastic process.<br />\nFrom Feynman–Kac formula taking <img src=\"https://s0.wp.com/latex.php?latex=v%5Cequiv+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"v&#92;equiv 0\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=f+%5Cequiv+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"f &#92;equiv 0\" class=\"latex\" /> we get <a href=\"http://en.wikipedia.org/wiki/Kolmogorov_backward_equations_(diffusion)\">Kolmogorov backward equation </a>:<br />\nfor<br />\n<img src=\"https://s0.wp.com/latex.php?latex=Lu+%3D+%28%5Cnabla_%5Cmathbf%7BX%7Du%29%5E%7B%5Cmathsf+T%7D+b+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+%28gg%5E%5Cmathsf%7BT%7D%29_%7Bi%2Cj%7D%5Ctfrac%7B+%5Cpartial%5E2+u%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"Lu = (&#92;nabla_&#92;mathbf{X}u)^{&#92;mathsf T} b + &#92;tfrac{1}{2} &#92;sum_{i,j} (gg^&#92;mathsf{T})_{i,j}&#92;tfrac{ &#92;partial^2 u}{&#92;partial x_i &#92;partial x_j}\" class=\"latex\" /><br />\nequation<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+u%7D%7B%5Cpartial+t%7D+%2B+Lu+%3D+0+%5C+%5C+%285%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;frac{&#92;partial u}{&#92;partial t} + Lu = 0 &#92; &#92; (5)\" class=\"latex\" /><br />\nwith terminal condition (4) have solution as conditional expectation<br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2C+t%29+%3D+E%28%5Cpsi%28X_T%29+%7C+X_t+%3D+x%29+%5C+%5C+%286%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x, t) = E(&#92;psi(X_T) | X_t = x) &#92; &#92; (6)\" class=\"latex\" /><br />\nFrom Kolmogorov backward equation we can obtain <a href=\"http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation\">Kolmogorov forward equation</a>, which describe evolution of probability density for random process <em>X</em> (2)<br />\nIn SDE courses it&#8217;s established that (2) is a <a href=\"http://en.wikipedia.org/wiki/Markov_process\">Markov process</a> and has transitional probability <em>P</em> and transitional density <em>p</em>:<br />\n<em>p(x, s, y, t) = </em>probability density at being at <em>y</em> in time <em>t</em>, on condition that it started at <em>x</em> in time <em>s</em><br />\ntaking <em>u</em> &#8211; solution of (5) with terminal condition (6)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2C+s%29+%3D+E%28%5Cpsi%28X_T%29+%7C+X_s+%3D+x%29+%3D+%5Cint+p%28x%2C+s%2C+z%2C+T%29+%5Cpsi%28z%29+dz&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x, s) = E(&#92;psi(X_T) | X_s = x) = &#92;int p(x, s, z, T) &#92;psi(z) dz\" class=\"latex\" /><br />\nFrom Markov property<br />\n<img src=\"https://s0.wp.com/latex.php?latex=p%28x%2C+s%2C+z%2C+T%29+%3D+%5Cint+p%28x%2C+s%2C+y%2C+t%29p%28y%2C+t%2C+z%2C+T%29+dz&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p(x, s, z, T) = &#92;int p(x, s, y, t)p(y, t, z, T) dz\" class=\"latex\" /><br />\nfrom here<br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2C+s%29+%3D+%5Cint+%5Cint+p%28x%2C+s%2C+y%2C+t%29+p%28y%2C+t%2C+z%2C+T%29+%5Cpsi%28z%29+dz+dy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x, s) = &#92;int &#92;int p(x, s, y, t) p(y, t, z, T) &#92;psi(z) dz dy\" class=\"latex\" /><br />\n<img src=\"https://s0.wp.com/latex.php?latex=u%28x%2C+s%29+%3D+%5Cint+p%28x%2C+s%2C+y%2C+t%29+u%28y%2C+t%29+dy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"u(x, s) = &#92;int p(x, s, y, t) u(y, t) dy\" class=\"latex\" /><br />\nform here<br />\n<img src=\"https://s0.wp.com/latex.php?latex=0+%3D+%5Cfrac%7B%5Cpartial+u%7D%7B%5Cpartial+t%7D+%3D+%5Cint+%5Cfrac+%7B%5Cpartial+p%28x%2C+s%2C+y%2C+t%29%7D%7B%5Cpartial+t%7Du%28y%2C+t%29+%2B+p%28x%2C+s%2C+y%2C+t%29+%5Cfrac+%7B%5Cpartial+u%28y%2C+t%29%7D%7B%5Cpartial+t%7D+dy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"0 = &#92;frac{&#92;partial u}{&#92;partial t} = &#92;int &#92;frac {&#92;partial p(x, s, y, t)}{&#92;partial t}u(y, t) + p(x, s, y, t) &#92;frac {&#92;partial u(y, t)}{&#92;partial t} dy\" class=\"latex\" /><br />\nfrom (5)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7B%5Cpartial+p%28x%2C+s%2C+y%2C+t%29%7D%7B%5Cpartial+t%7Du%28y%2C+t%29+-+%28Lu%29+p%28x%2C+s%2C+y%2C+t%29+dy+%3D+0+%5C+%5C+%287%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int &#92;frac{&#92;partial p(x, s, y, t)}{&#92;partial t}u(y, t) - (Lu) p(x, s, y, t) dy = 0 &#92; &#92; (7)\" class=\"latex\" /><br />\nNow we introduce dual operator <img src=\"https://s0.wp.com/latex.php?latex=L%5E%2A&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L^*\" class=\"latex\" /><br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint+p+%28Lu%29+dy+%3D+%5Cint+%28L%5E%7B%2A%7Dp%29+u+dy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int p (Lu) dy = &#92;int (L^{*}p) u dy\" class=\"latex\" /><br />\nBy integration by part we can get<br />\n<img src=\"https://s0.wp.com/latex.php?latex=L%5E%2Au+%3D+-%5Cnabla_%5Cmathbf%7BX%7D%5E%7B%5Cmathsf+T%7D%28ub%29+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+%5Ctfrac%7B+%5Cpartial%5E2+%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D%28gg%5E%5Cmathsf%7BT%7Du%29_%7Bi%2Cj%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L^*u = -&#92;nabla_&#92;mathbf{X}^{&#92;mathsf T}(ub) + &#92;tfrac{1}{2} &#92;sum_{i,j} &#92;tfrac{ &#92;partial^2 }{&#92;partial x_i &#92;partial x_j}(gg^&#92;mathsf{T}u)_{i,j}\" class=\"latex\" /><br />\nand from (7)<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint+%28%5Cfrac%7B%5Cpartial+p%28x%2C+s%2C+y%2C+t%29%7D%7B%5Cpartial+t%7D+-+L%5E%7B%2A%7Dp%29+u%28y%2C+t%29+dy+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int (&#92;frac{&#92;partial p(x, s, y, t)}{&#92;partial t} - L^{*}p) u(y, t) dy = 0\" class=\"latex\" /><br />\nfor <em>t=T</em><br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cint+%28%5Cfrac%7B%5Cpartial+p%28x%2C+s%2C+y%2C+T%29%7D%7B%5Cpartial+t%7D+-+L%5E%7B%2A%7Dp%28x%2C+s%2C+y%2C+T%29%29+%5Cpsi%28y%29+dy+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int (&#92;frac{&#92;partial p(x, s, y, T)}{&#92;partial t} - L^{*}p(x, s, y, T)) &#92;psi(y) dy = 0\" class=\"latex\" /><br />\nThis is true for any <img src=\"https://s0.wp.com/latex.php?latex=%5Cpsi&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;psi\" class=\"latex\" />, wich is independent from <em>p</em><br />\nAnd we get Kolmogorov forward equation for <em>p</em>. Integrating by <em>x</em> we get the same equation for probability density at any moment <em>T</em><br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+t%7D+-+L%5E%7B%2A%7Dp+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;frac{&#92;partial p}{&#92;partial t} - L^{*}p = 0\" class=\"latex\" /><br />\nNow we return to Gibbs invariant distribution for gradient flow<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+J%7D%7B%5Cpartial+t%7D+%3D+-+%5Cbigtriangledown_J+E%28J%29+%2B+%5Csqrt%7B%5Ctfrac%7B2%7D%7B%5Cbeta%7D%7DF%28t%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;tfrac{&#92;partial J}{&#92;partial t} = - &#92;bigtriangledown_J E(J) + &#92;sqrt{&#92;tfrac{2}{&#92;beta}}F(t)\" class=\"latex\" /><br />\nStochastic gradient flow in SDE notation<br />\n<img src=\"https://s0.wp.com/latex.php?latex=dJ+%3D+-+%5Cbigtriangledown_J+E%28J%29dt+%2B+%5Csqrt%7B%5Ctfrac%7B2%7D%7B%5Cbeta%7D%7D+dW&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"dJ = - &#92;bigtriangledown_J E(J)dt + &#92;sqrt{&#92;tfrac{2}{&#92;beta}} dW\" class=\"latex\" /> &#8211; integral of white noise is Wiener process<br />\nWe want to find invariant probability density <img src=\"https://s0.wp.com/latex.php?latex=p_G&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p_G\" class=\"latex\" />. Invariant &#8211; means it doesn&#8217;t change with time, <img src=\"https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+p_G%7D%7B%5Cpartial+t%7D+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;tfrac{&#92;partial p_G}{&#92;partial t} = 0\" class=\"latex\" /><br />\nso from Kolmogorov forward equation<br />\n<img src=\"https://s0.wp.com/latex.php?latex=L%5E%2Ap_G+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L^*p_G = 0\" class=\"latex\" /><br />\nor<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cnabla%5E%7B%5Cmathsf+T%7D%28p_G%5Cnabla+E%29%2B+%5Ctfrac%7B1%7D%7B%5Cbeta%7D+%5Cnabla%5E%7B%5Cmathsf+T%7D+%5Cnabla+p_G+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;nabla^{&#92;mathsf T}(p_G&#92;nabla E)+ &#92;tfrac{1}{&#92;beta} &#92;nabla^{&#92;mathsf T} &#92;nabla p_G = 0\" class=\"latex\" /><br />\nremoving gradient<br />\n<img src=\"https://s0.wp.com/latex.php?latex=p_G%5Cnabla+E+%2B+%5Ctfrac%7B1%7D%7B%5Cbeta%7D+%5Cnabla+p_G+%3D+C&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p_G&#92;nabla E + &#92;tfrac{1}{&#92;beta} &#92;nabla p_G = C\" class=\"latex\" /><br />\n<em>C = 0</em> because we want <img src=\"https://s0.wp.com/latex.php?latex=p_G&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p_G\" class=\"latex\" /> integrable<br />\n<img src=\"https://s0.wp.com/latex.php?latex=%5Cnabla%28E+%2B+%5Ctfrac%7B1%7D%7B%5Cbeta%7D+log%28p_G%29%29+%3D+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;nabla(E + &#92;tfrac{1}{&#92;beta} log(p_G)) = 0\" class=\"latex\" /><br />\nand at last we get Gibbs distribution<br />\n<img src=\"https://s0.wp.com/latex.php?latex=p_G+%3D+%5Cfrac%7Bexp%28-%5Cbeta+E%29%7D%7BZ%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"p_G = &#92;frac{exp(-&#92;beta E)}{Z}\" class=\"latex\" /><br />\nRecalling again the chain of reasoning:<br />\n<a href=\"http://en.wikipedia.org/wiki/Wiener_process\">Wiener process</a> →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Ito_integral\">Ito integral</a> →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Stochastic_differential_equation\">SDE</a> + <a href=\"http://en.wikipedia.org/wiki/Ito_lemma\">Ito Lemma</a> + Ito product rule + zero expecation of Ito integral →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Feynman-Kac_formula\">Feynman-Kac formula</a> →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Kolmogorov_backward_equations_(diffusion)\">Kolmogorov backward equation</a> + <a href=\"http://en.wikipedia.org/wiki/Markov_process\">Markov property</a> of SDE →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation\">Kolmogorov forward equation</a> for probability density  →</p>\n<p><a href=\"http://en.wikipedia.org/wiki/It%C5%8D_diffusion#Invariant_measures\">Gibbs invariant distribution</a></p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}