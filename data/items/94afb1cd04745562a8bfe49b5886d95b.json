{
  "title": "dithernet very slow movie player",
  "link": "http://matpalm.com/blog/dithernet_vsmp",
  "category": [
    "gan",
    "jax",
    "projects",
    "objax"
  ],
  "guid": "http://matpalm.com/blog/dithernet_vsmp",
  "description": "dithernet very slow movie player",
  "content:encoded": "<h1>very slow movie player</h1>\n<p>it's been about two years since i first saw the awesome\n   <a href=\"https://medium.com/s/story/very-slow-movie-player-499f76c48b62\">very slow movie player</a>\n   project by bryan boyer. i thought it was such an excellent idea but never got around\n   to buying the hardware to make one. more recently though i've seen a couple of references\n   to the project so i decided it was finally time to make one.\n</p>\n<p>one interesting concern about an eink very slow movie player is the screen refresh. simpler\n   eink screens refresh by doing a full cycle of a screen of white or black before displaying\n   the new image. i hated the idea of an ambient slow player doing this every few minutes\n   as it switched frames, so i wanted to make sure i got a piece of hardware that could do\n   incremental update.\n</p>\n<p>after a bit of shopping around i settled on a\n   <a href=\"https://www.waveshare.com/6inch-hd-e-paper-hat.htm\">6 inch HD screen from waveshare</a>\n</p>\n<p>it ticks all the boxes i wanted\n</p>\n<ul>\n <li>\n     6 inch\n </li>\n\n <li>\n     1448Ã—1072 high definition\n </li>\n\n <li>\n     comes with a raspberry pi HAT\n </li>\n\n <li>\n     and, most importantly, support partial refresh\n </li>\n</ul>\n<p>this screen also supports grey scale, but only with a flashy full cycle redraw,\n   so i'm going to stick to just black and white since it supports the partial redraw.\n</p>\n<p>note: even though the partial redraw is basically instant it does suffer from a ghosting problem;\n   when you draw a white pixel over a black one things are fine, but if you draw black over\n   white, in the partial redraw, you get a slight ghosting of gray that is present until a\n   full redraw :/\n</p>\n\n<h1>dithering</h1>\n<p>so how do you display an image when you can only show black and white?\n   dithering! here's an example of a 384x288 RGB image dithered using\n   <a href=\"https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert\">PILS implementation of the Floyd-Steinberg algorithm</a>\n</p>\n<table class='data'>\n<tr><td><img src=\"/blog/imgs/2020/dn/eg.dither.png\" /></td></tr>\n<tr><td>original RGB vs dithered version</td></tr>\n</table>\n\n<p>it makes intuitive sense that you could have small variations in the exact locations of the\n   dots as long as you get the densities generally right. s\n   so there's a reasonable question then; how do you dither in such a way that you get a\n   good result, but with minimal pixel changes from a previous frame? (since we're\n   motivated on these screens to change as little as possible)\n</p>\n<p>there are two approaches i see\n</p>\n<p>1) spend 30 minutes googling for a solution that no doubt someone came up with 20 years\n   ago that can be implemented in 10 lines of c running at 1000fps ...\n</p>\n<p>2) .... or train an\n   <a href=\"https://jax.readthedocs.io/\">jax</a>\n   based GAN to generate the dithers with a loss balancing a good dither vs no pixel change. :P\n</p>\n\n<h1>the data</h1>\n<p>when building a very slow movie player the most critical decision is...\n   what movie to play?\n   i really love the 1979 classic <a href=\"https://www.imdb.com/title/tt0078748/\">alien</a>,\n   it's such a great dark movie, so i thought i'd go with it.\n   the movie is 160,000 frames so at a play back rate of a frame every 200 seconds\n   it'll take just over a year to finish.\n</p>\n<p>note that in this type of problem there is no concern around overfitting.\n   we have access to all data going in and so it's fine to overfit as much as we like;\n   as long as we're minimising whatever our objective is we're good to go.\n</p>\n\n<h1>v1: the vanilla unet</h1>\n<p>i started with a\n   <a href=\"https://arxiv.org/abs/1505.04597\">unet</a>\n   that maps 3 channel RGB images to a single channel dither.\n</p>\n<table class='data'>\n<tr><td><img src=\"/blog/imgs/2020/dn/models.v1.png\" /></td></tr>\n<tr><td>v1 architecture</td></tr>\n</table>\n\n<p>i tinkered a bit with the architecture but didn't spend too much time tuning it.\n   for the final v3 result i ended with a pretty vanilla stack of encoders & decoders\n   (with skip connections connecting an encoder to the decoder at the same spatial resolution)\n   each encoder/decoder block uses a residual like shortcut around a couple of convolutions.\n   nearest neighbour upsampling gave a nicer result than deconvolutions in the decoder\n   for the v3 result.\n   also, <a href=\"https://arxiv.org/abs/1606.08415\">gelu</a> is my new favorite activation :)\n</p>\n<p>for v1 i used a binary cross entropy loss of P(white) per pixel\n   ( since it's what worked well for my\n   <a href=\"http://matpalm.com/blog/counting_bees/\">bee counting project</a> )\n</p>\n<p>as always i started by overfitting to a single example to get a baseline feel for capacity required.\n</p>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/overfit.png\" />\n</td></tr>\n<tr><td>\nv1 overfit result\n</td></tr>\n</table>\n\n<p>when scaling up to the full dataset i switched to training on half resolution images\n   against a patch size of 128. working on half resolution consistently gave a better\n   result than working with the full resolution.\n</p>\n<p>as expected though this model gave us the classic type of problem we see with\n   straight unet style image translation; we get a reasonable sense of the shapes, but no\n   fine details around the dithering.\n</p>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/v1.upsample.png\" />\n</td></tr>\n<tr><td>\nv1 vanilla unet with upsampling example\n</td></tr>\n</table>\n\n<p>side notes:\n</p>\n<ul>\n <li>\n     for this v1 version using deconvolutions in the decoder\n     (instead of nearest neighbour upsampling) actually looked pretty good!\n     nicely captured texture for a dither with a surprisingly small network.\n </li>\n\n <li>\n     i actually did some experiments using branches in the decoder for both upsampling\n     and deconvolutions but the deconvs always dominated too much. i thought that would\n     allow the upsampling to work as a kind of residual to the deconv but it never happened.\n </li>\n</ul>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/v1.deconv.png\" />\n</td></tr>\n<tr><td>\nv1 vanilla unet with deconvolution example\n</td></tr>\n</table>\n\n\n<h1>v2: to the GAN...</h1>\n<p>for v2 i added a GAN objective in an attempt to capture finer details\n</p>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/models.v2.png\" />\n</td></tr>\n<tr><td>\nv2 architecture\n</td></tr>\n</table>\n\n<p>i started with the original\n   <a href=\"https://arxiv.org/abs/1611.07004\">pix2pix</a>\n   objective but reasonably quickly moved to use a\n   <a href=\"https://arxiv.org/abs/1701.07875\">wasserstein</a>\n   critic style objective since i've always found it more stable.\n</p>\n<p>the generator (G) was the same as the unet above with the discriminator (D) running patch based.\n   at this point i also changed the reconstruction loss from a binary objective to just L1.\n   i ended up using batchnorm in D, but not G.\n   to be honest i only did a little did of manual tuning, i'm sure there's a better result\n   hidden in the hyperparameters somewhere.\n</p>\n<p>so, for this version, the loss for G has two components\n</p>\n<pre>\n1. D(G(rgb))             # fool D\n2. L1(G(rgb), dither)    # reconstruct the dither\n</pre>\n\n<p>very quickly (i.e. in < 10mins ) we get a reasonable result that is started to\n   show some more detail than just the blobby reconstruction.\n</p>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/v2.eg.png\" />\n</td></tr>\n<tr><td>\nv2 partial trained eg\n</td></tr>\n</table>\n\n<p>note: if the loss weight of 2) is 0 we degenerate to v1\n   (which proved a useful intermediate debugging step).\n   at this point i didn't want to tune to much since the final v3 is coming...\n</p>\n\n<h1>v3: a loss related to change from last frame</h1>\n<p>for v3 we finally introduce a loss relating the previous frame\n   (which was one of the main intentions of the project in the first place)\n</p>\n<p>now G takes not just the RGB image, but the dither of the previous frame.\n</p>\n<table class='data'>\n<tr><td>\n<img src=\"/blog/imgs/2020/dn/models.v3.png\" />\n</td></tr>\n<tr><td>\nv3 architecture\n</td></tr>\n</table>\n\n<p>the loss for G now has three parts\n</p>\n<pre>\n1. D(G(rgb_t1)) => real      # fool D\n2. L1(G(rgb_t1), dither_t1)  # reconstruct the dither\n3. L1(G(rgb_t1), dither_t0)  # don't change too much from the last frame\n</pre>\n\n<p>normally with a network that takes as input the same thing it's outputting\n   we have to be careful to include things like teacher forcing.\n   but since we don't intend to use this network for any kind of rollouts\n   we can just always feed the \"true\" dithers in where required.\n   having said that, rolling out the dithers from this network would be interesting :D\n</p>\n\n<h1>digression; the troublesome scene changes</h1>\n<p>the third loss objective, not changing too many pixels from the last frame,\n   works well for generally stationary shots\n   but is disastrous for scene changes :/\n</p>\n<p>consider the following graph for a sequence of frames showing the pixel difference\n   between frames.\n</p>\n<img src=\"/blog/imgs/2020/dn/pixel_diff_between_scenes.png\" />\n\n<p>when there is a scene change we observe a clear \"spike\" in pixel diff. my first thought\n   was to look for these and do a full redraw for them. it's very straightforward to\n   find them (using a simple z-score based anomaly detector on a sliding window) but\n   the problem is that it doesn't pick up the troublesome case of a panning shot where we don't\n   have a scene change exactly. in these cases there is no abrupt scene change, but there\n   are a lot of pixels changing so we end up seeing a lot of ghosting.\n</p>\n<p>i spent ages tinkering with the best way to approach this before deciding that a simple\n   approach of <code>num_pixels_changed_since_last_redraw > threshold</code> was good enough to decide\n   if a full redraw was required (with a cooldown to ensure we not redrawing all the time)\n</p>\n\n<h1>... and back to v3</h1>\n<p>the v3 network gets a very good result <em>very</em> quickly; unsurprisingly since the dither at time\n   t0 provided to G is a pretty good estimate of the dither at t1 :)\n   i.e. G can get a good result simply by copying it!\n</p>\n<p>the following scenario shows this effect...\n</p>\n<p>consider three sequential frames, the middle one being a scene change.\n</p>\n<img src=\"/blog/imgs/2020/dn/v3.scene_eg.1.png\" />\n\n<p>at the very start of training the reconstruction loss is dominant and\n   we get blobby outlines of the frame.\n</p>\n<img src=\"/blog/imgs/2020/dn/v3.scene_eg.2.png\" />\n\n<p>but as the contribution from the dither at time t0 kicks it things look good in general but\n   the frames at the scene change end up being a ghosted mix attempt to copy through the old\n   frame along with dithering the new one.\n   (depending on the relative strength of the loss terms of G).\n</p>\n<img src=\"/blog/imgs/2020/dn/v3.scene_eg.3.png\" />\n\n\n<h1>the final result</h1>\n<p>so the v3 version generally works and i'm sure with some more tuning i could get a better result\n   but, as luck would have it, i actually find the results from v2 more appealing when testing\n   on the actual eink screen. so even though the intention was do something like v3 i'm going to end\n   up running something more like v2 (as shown in these couple of examples (though the resolution\n   does it no justice (not to mention the fact the player will run about 5000 times slower than these\n   gifs)))\n</p>\n<table class='data'><tr>\n<td><img src=\"/blog/imgs/2020/dn/f_00048000.gif\"/></td>\n<td><img src=\"/blog/imgs/2020/dn/f_00067400.gif\"/></td>\n</tr><tr>\n<td><img src=\"/blog/imgs/2020/dn/f_00082400.gif\"/></td>\n<td><img src=\"/blog/imgs/2020/dn/f_00107045.gif\"/></td>\n</tr><tr>\n<td><img src=\"/blog/imgs/2020/dn/f_00120600.gif\"/></td>\n<td><img src=\"/blog/imgs/2020/dn/f_00145400.gif\"/></td>\n</tr></table>\n\n\n<h1>player case</h1>\n<p>i ran for a few weeks with a prototype that lived balanced precariously on a piece of foam\n   below it's younger sibling pi zero eink screen running game of life. eventually i cut up some\n   pieces of an old couch and made a simple wooden frame. a carpenter, i am not :/\n</p>\n<table class='data'>\n<tr>\n<td><img src=\"/blog/imgs/2020/dn/prototype.png\" /></td>\n<td><img src=\"/blog/imgs/2020/dn/prototype.2.png\" /></td>\n</tr>\n<tr>\n<td>prototype</td>\n<td>frame</td>\n</tr>\n</table>\n\n\n<h1>todos</h1>\n<ul>\n <li>\n     for reconstruction and frame change i used L1 loss, but that's not exactly what we\n     want. since we want to avoid the ghosting (white changing to black resulting in grey)\n     we should try to avoid white to black but ignore black to white.\n </li>\n\n <li>\n     we might be able to better handle scene changes by also including a\n     loss component around the <em>next</em> frame.\n </li>\n\n <li>\n     there's a padding issue where i train G on patches but when it's run on the full res\n     version we get an edge artefact the size of the original patch (see image below).\n     as a hacky fix i just padded the RGB image before passing it to G in the final run, but this problem could\n     be fixed by changing the padding schema during training.\n </li>\n</ul>\n<img src=\"/blog/imgs/2020/dn/todo_padding.png\"/>\n\n\n<h1>code</h1>\n<p><a href=\"https://github.com/matpalm/dither_net\">all on github</a>\n</p>"
}