{
  "title": "Data Science With Python: Part 1",
  "link": "",
  "updated": "2014-07-30T18:12:00-04:00",
  "id": "http://beckerfuffle.com/blog/2014/07/30/data-science-with-python-part-1",
  "content": "<p>This is the first post in a multi-part series wherein I will explain the details surrounding the language prediction model I presented in <a href=\"http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn\">my Pycon 2014 talk</a>. If you make it all the way through, you will learn how to create and deploy a language prediction model of your own.</p>\n\n<p><a href=\"http://pyvideo.org/video/2606/realtime-predictive-analytics-using-scikit-learn\"><img src=\"https://raw.githubusercontent.com/mdbecker/static_files/master/pycon/Becker.png\" alt=\"Realtime predictive analytics using scikit-learn & RabbitMQ\" /></a><br>\n<em>Realtime predictive analytics using scikit-learn & RabbitMQ</em></p>\n\n<h2>OSEMN</h2>\n\n<p>I&rsquo;m not sure if <a href=\"http://www.hilarymason.com/\">Hilary Mason</a> originally coined the term OSEMN, but I certainly learned it from her. OSEMN (pronounced <a href=\"http://www.dataists.com/2010/09/a-taxonomy-of-data-science/\">awesome</a>) is a typical data science process that is followed by many data scientists. OSEMN stands for <strong>Obtain</strong>, <strong>Scrub</strong>, <strong>Explore</strong>, <strong>Model</strong>, and <strong>iNterpret</strong>. As Hilary put it in <a href=\"http://www.dataists.com/2010/09/a-taxonomy-of-data-science/\">a blog post on the subject</a>: &ldquo;Different data scientists have different levels of expertise with each of these 5 areas, but ideally a data scientist should be at home with them all.&rdquo; As a common data science process, this is a great start, but sometimes this isn&rsquo;t enough. If you want to make your model a critical piece of your application, you must also make it accessible and performant. For this reason, I&rsquo;ll also discuss two more steps, <strong>Deploy</strong> and <strong>Scale</strong>.</p>\n\n<h2>Obtain & Scrub</h2>\n\n<p>In this post, I&rsquo;ll cover how I <strong>obtained</strong> and <strong>scrubbed</strong> the training data for the predictive algorithm in my talk. For those who didn&rsquo;t have a chance to watch my talk, I used data from Wikipedia to train a predictive algorithm to predict the language of some text. We use this algorithm at <a href=\"http://aweber.jobs/\">the company I work for</a> to partition user generated content for further processing and analysis.</p>\n\n<h3>Pagecounts</h3>\n\n<p>So step 1 is <strong>obtaining</strong> a dataset we can use to train a predictive model. <a href=\"https://www.cs.drexel.edu/~urlass/\">My friend Rob</a> recommended I use Wikipedia for this, so I decided to try it out. There are <a href=\"https://meta.wikimedia.org/wiki/Datasets\">a few datasets</a> extracted from Wikipedia obtainable online at the time of this writing. Otherwise you need to generate the dataset yourself, which is what I did. I grabbed hourly page views per article for the past 5 months from <a href=\"http://dumps.wikimedia.org/other/pagecounts-ez/\">dumps.wikimedia.org</a>. I wrote some Python scripts to aggregate these counts and dump the top 50,000 articles from each language.</p>\n\n<h3>Export bot</h3>\n\n<p>After this, I wrote an insanely simple bot to execute queries against the Wikipedia <a href=\"https://en.wikipedia.org/wiki/Special:Export\"><code>Special:Export</code></a> page. Originally, I was considering using <a href=\"http://scrapy.org/\">scrapy</a> for this since I&rsquo;ve been looking for an excuse to use it. A quick read through of the tutorial left me feeling like scrapy was overkill for my problem. I decided a simple bot would be more appropriate. I was inspecting the fields of the web-form for the <code>Special:Export</code> page using <a href=\"https://developer.chrome.com/devtools/index\">Chrome Developer Tools</a> when I stumbled upon a pretty cool trick. Under the &ldquo;Network&rdquo; tab, if you <code>ctrl</code> click on a request, you can use &ldquo;<a href=\"https://developer.chrome.com/devtools/docs/network#copying-requests-as-curl-commands\">Copy as cURL</a>&rdquo; to get a curl command that will reproduce the exact request made by the Chrome browser (headers, User-Agent and all). This makes it easy to write a simple bot that just interacts with a single web-form. The bot code looks a little something like this:</p>\n\n<figure class='code'><figcaption><span></span></figcaption><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\"><span class='line-number'>1</span>\n<span class='line-number'>2</span>\n<span class='line-number'>3</span>\n<span class='line-number'>4</span>\n<span class='line-number'>5</span>\n<span class='line-number'>6</span>\n<span class='line-number'>7</span>\n<span class='line-number'>8</span>\n<span class='line-number'>9</span>\n<span class='line-number'>10</span>\n<span class='line-number'>11</span>\n<span class='line-number'>12</span>\n<span class='line-number'>13</span>\n<span class='line-number'>14</span>\n</pre></td><td class='code'><pre><code class='python'><span class='line'><span class=\"kn\">from</span> <span class=\"nn\">subprocess</span> <span class=\"kn\">import</span> <span class=\"n\">call</span>\n</span><span class='line'><span class=\"kn\">from</span> <span class=\"nn\">urllib.parse</span> <span class=\"kn\">import</span> <span class=\"n\">urlencode</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">curl</span> <span class=\"o\">=</span> <span class=\"s\">\"\"\"curl 'https://{2}.wikipedia.org/w/index.php?title=Special:Export&action=submit' -H 'Origin: https://{2}.wikipedia.org' -H 'Accept-Encoding: gzip,deflate,sdch' -H 'User-Agent: Mozilla/5.0 Chrome/35.0' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' -H 'Cache-Control: max-age=0' -H 'Referer: https://{2}.wikipedia.org/wiki/Special:Export' -H 'Connection: keep-alive' -H 'DNT: 1' --compressed --data '{0}' > {1}\"\"\"</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n</span><span class='line'>    <span class=\"s\">'catname'</span><span class=\"p\">:</span> <span class=\"s\">''</span><span class=\"p\">,</span>\n</span><span class='line'>    <span class=\"s\">'pages'</span><span class=\"p\">:</span> <span class=\"s\">'Main_Page</span><span class=\"se\">\\n</span><span class=\"s\">Climatic_Research_Unit_email_controversy</span><span class=\"se\">\\n</span><span class=\"s\">Java</span><span class=\"se\">\\n</span><span class=\"s\">undefined'</span><span class=\"p\">,</span>\n</span><span class='line'>    <span class=\"s\">'curonly'</span><span class=\"p\">:</span> <span class=\"s\">'1'</span><span class=\"p\">,</span>\n</span><span class='line'>    <span class=\"s\">'wpDownload'</span><span class=\"p\">:</span> <span class=\"s\">'1'</span><span class=\"p\">,</span>\n</span><span class='line'><span class=\"p\">}</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">enc_data</span> <span class=\"o\">=</span> <span class=\"n\">urlencode</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n</span><span class='line'><span class=\"n\">call</span><span class=\"p\">(</span><span class=\"n\">curl</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">enc_data</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">),</span> <span class=\"n\">shell</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</span></code></pre></td></tr></table></div></figure>\n\n\n<p>The final version of my bot splits the list of articles into small chunks since the <code>Special:Export</code> page throws 503 errors when the requests are too large.</p>\n\n<h3>Convert to plain text</h3>\n\n<p>The <code>Special:Export</code> page on Wikipedia returns an XML file that contains the page contents and other pieces of information. The page contents include wiki markup, which for my purposes are not useful. I needed to <strong>scrub</strong> the Wikipedia markup to convert the pages to plain text. Fortunately, I found <a href=\"https://github.com/bwbaugh/wikipedia-extractor\">a tool that already does this</a>. There was one downside to this tool which is that it produces output in a format that looks strikingly similar to XML, but is not actually valid XML. To address this, I wrote a simple parser using <a href=\"https://docs.python.org/2/library/re.html#re.MatchObject.groupdict\">a regex</a> that looks something like this:</p>\n\n<figure class='code'><figcaption><span></span></figcaption><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\"><span class='line-number'>1</span>\n<span class='line-number'>2</span>\n<span class='line-number'>3</span>\n<span class='line-number'>4</span>\n<span class='line-number'>5</span>\n<span class='line-number'>6</span>\n<span class='line-number'>7</span>\n<span class='line-number'>8</span>\n<span class='line-number'>9</span>\n<span class='line-number'>10</span>\n<span class='line-number'>11</span>\n<span class='line-number'>12</span>\n<span class='line-number'>13</span>\n<span class='line-number'>14</span>\n<span class='line-number'>15</span>\n<span class='line-number'>16</span>\n</pre></td><td class='code'><pre><code class='python'><span class='line'><span class=\"kn\">import</span> <span class=\"nn\">bz2</span>\n</span><span class='line'><span class=\"kn\">import</span> <span class=\"nn\">re</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">article</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s\">r'<doc id=\"(?P<id>\\d+)\" url=\"(?P<url>[^\"]+)\" title=\"(?P<title>[^\"]+)\">\\n(?P<content>.+)\\n<\\/doc>'</span><span class=\"p\">,</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">S</span><span class=\"o\">|</span><span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">U</span><span class=\"p\">)</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"k\">def</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">):</span>\n</span><span class='line'>  <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"s\">\"\"</span>\n</span><span class='line'>  <span class=\"k\">with</span> <span class=\"n\">bz2</span><span class=\"o\">.</span><span class=\"n\">BZ2File</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">,</span> <span class=\"s\">'r'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n</span><span class='line'>    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n</span><span class='line'>      <span class=\"n\">line</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s\">'utf-8'</span><span class=\"p\">)</span>\n</span><span class='line'>      <span class=\"n\">data</span> <span class=\"o\">+=</span> <span class=\"n\">line</span>\n</span><span class='line'>      <span class=\"k\">if</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"s\">'</doc>'</span><span class=\"p\">):</span>\n</span><span class='line'>        <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">article</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n</span><span class='line'>        <span class=\"k\">if</span> <span class=\"n\">m</span><span class=\"p\">:</span>\n</span><span class='line'>          <span class=\"k\">yield</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">groupdict</span><span class=\"p\">()</span>\n</span><span class='line'>        <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"s\">\"\"</span>\n</span></code></pre></td></tr></table></div></figure>\n\n\n<p>This function will read every page in <code>filename</code> and return a dictionary with the <code>id</code> (an integer tracking the version of the page), the <code>url</code> (a permanent link to this version of the page), the <code>title</code>, and the plain text <code>content</code> of the page. Going through the file one article at a time, and using the <code>yield</code> keyword makes this function <a href=\"https://wiki.python.org/moin/Generators\">a generator</a> which means that it will be more memory efficient.</p>\n\n<h2>What&rsquo;s next?</h2>\n\n<p>In my next post I will cover the <strong>explore</strong> step using some of Python&rsquo;s state-of-the-art tools for data manipulation and visualization. You&rsquo;ll also get your first taste of <a href=\"http://scikit-learn.org/stable/\">scikit-learn</a>, my machine learning library of choice. If you have any questions or comments, please post them below!</p>\n"
}