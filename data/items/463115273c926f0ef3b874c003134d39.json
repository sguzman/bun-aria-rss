{
  "title": "On Deep Learning — A Tweeted Bibliography",
  "link": "https://medium.com/deep-learning-101/on-deep-learning-a-tweeted-bibliography-68ab095376e7?source=rss-b0cd2903e078------2",
  "guid": "https://medium.com/p/68ab095376e7",
  "category": [
    "fundamentals",
    "rds",
    "deep-learning"
  ],
  "dc:creator": "Christopher Nguyen",
  "pubDate": "Sat, 04 Apr 2015 05:32:34 GMT",
  "atom:updated": "2015-05-24T09:50:09.375Z",
  "content:encoded": "<h2>On Deep Learning</h2><p><em>A Tweeted Bibliography</em></p><p>Here’s a collection of my tweets on interesting/exciting developments in Deep Learning or Machine Learning in general.</p><p>It’s in no grand order, but does serve as a convenient reference & provides some context.</p><h3>Christopher Nguyen on Twitter</h3><p>Ever wonder what Stochastic in SGD really means? From 1951, Robbins & Monro http://bit.ly/1DIM0hq #MachineLearning pic.twitter.com/sJqDKsvFUt</p><h3>Adatao [ah-DAY-tao] on Twitter</h3><p>Here: our favorite reference on Discriminative vs. Generative classifiers, by @AndrewYNg http://buff.ly/1wA50bj pic.twitter.com/BbJoCIytYo</p><h3>Christopher Nguyen on Twitter</h3><p>Why do neural networks with more layers perform better than a single layer MLP with same # of params? http://bit.ly/1sGzYOj</p><h3>Christopher Nguyen on Twitter</h3><p>Universal function approximators: widely known, often misunderstood in #DeepLearning. Review http://bit.ly/1GTcWxz . pic.twitter.com/d2tJtn0eRM</p><h3>Christopher Nguyen on Twitter</h3><p>Geoff Hinton's Very Cool #DeepLearning demo. Run in reverse to see how Imagination might work http://bit.ly/1IWlcgE pic.twitter.com/DQJS8EjJGV</p><h3>Christopher Nguyen on Twitter</h3><p>Good 12/14 arXiv review of object recog w #DeepLearning. Amazingly it's also already outdated! http://bit.ly/1GRue08 pic.twitter.com/DaES8JFoPT</p><h3>Christopher Nguyen on Twitter</h3><p>A classic. Profoundly interesting in suggesting how we might encode concepts in our brains. http://bit.ly/1H0a0Be pic.twitter.com/SgIbdv7DBv</p><h3>Christopher Nguyen on Twitter</h3><p>Distributed-representation interpretation will prove largely correct #NeuralScience cf YBengio http://bit.ly/19147EK pic.twitter.com/cmZoB6hKOj</p><h3>Christopher Nguyen on Twitter</h3><p>Must-read: classic RHW Nature Letter, casually introducing backprop, \"that triggered a boom in neural net research\". pic.twitter.com/hhjGEaOMhG</p><h3>Christopher Nguyen on Twitter</h3><p>MSFT working on custom FPGA sys for #DeepLearning http://bit.ly/1LWs7XP . Should also look into @ylecun's NeuFlow. pic.twitter.com/qJECGBaS5Z</p><h3>Christopher Nguyen on Twitter</h3><p>MSFT Asia #DeepLearning team just demonstrated better-than-human visual recog. +1 @harryshum! http://bit.ly/1A03iFa pic.twitter.com/Xc28rPW596</p><h3>Christopher Nguyen on Twitter</h3><p>Microsoft team demonstrates shallow nets can rival #DeepLearning nets, suggesting alternative... http://bit.ly/1yEqdIA pic.twitter.com/0IWkwIz4mN</p><h3>Christopher Nguyen on Twitter</h3><p>New @MSFTResearch algorithm helps scale ad predictions & #DeepLearning to billions of vars http://bit.ly/1GGS1KF pic.twitter.com/VmiABaT7iB</p><h3>Andrew Ng on Twitter</h3><p>Deep Speech improves speech recognition; outperforms Bing/Google/Apple APIs in noisy environments! http://onforb.es/1x2BpOu</p><h3>Christopher Nguyen on Twitter</h3><p>DeepLearning UMich group has developed RL technique to best Google's $400MM DeepMind at Real-Time Atari Games http://bit.ly/1AgRjmx</p><h3>Christopher Nguyen on Twitter</h3><p>GOOG +1 over MSFT! Input norm >Param init, 1/10x training steps #DeepLearning HT @annodomini80 http://bit.ly/1E6dZXq pic.twitter.com/2QARCdZdoQ</p><h3>Christopher Nguyen on Twitter</h3><p>How do you find the best way to visualize a 784-dimensional dataset? http://bit.ly/1DOnE7Y pic.twitter.com/qVBUrUnv9V</p><h3>Christopher Nguyen on Twitter</h3><p>This is profound. Why wait for ASICs given our already very powerful #DeepLearning machine? #OneAlgorithm #Evolution http://bit.ly/1NFqEUK</p><h3>Christopher Nguyen on Twitter</h3><p>Google team's #DeepLearning sentence translator outperforms statistical machine translation http://bit.ly/15qdvjX pic.twitter.com/e6ZjdodL5s</p><h3>Adatao [ah-DAY-tao] on Twitter</h3><p>DeepLearning is figuring out how to tell stories from pictures #DataNarratives #DataViz http://buff.ly/1Dv9UxA pic.twitter.com/ayzu4yWl8k</p><h3>Christopher Nguyen on Twitter</h3><p>Neural Turing Machine creates its own algorithms (#NotAI, because I know exactly how it works) http://arxiv.org/pdf/1410.5401v2.pdf ... pic.twitter.com/nragAASyvg</p><h3>Christopher Nguyen on Twitter</h3><p>From Phil Colella's 7 dwarfs to Dave Patterson's 13: do get that MapReduce is just 1 of many http://bit.ly/1DgZ77Z pic.twitter.com/FTq3lbrh86</p><h3>Christopher Nguyen on Twitter</h3><p>This is the remarkable #DeepLearning Q&A work at @facebook demo'ed by @schrep at recent F8 http://buff.ly/1IQykEP pic.twitter.com/XkPEdMZPzn</p><h3>Christopher Nguyen on Twitter</h3><p>Introducing iRNNs, in a co-pub of Quoc Le and Geoff Hinton (and Navdeep Jaitly) http://bit.ly/1FediQX #DeepLearning pic.twitter.com/0KXgPz3xEK</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=68ab095376e7\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/deep-learning-101/on-deep-learning-a-tweeted-bibliography-68ab095376e7\">On Deep Learning — A Tweeted Bibliography</a> was originally published in <a href=\"https://medium.com/deep-learning-101\">Deep Learning 101</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}