{
  "id": "tag:blogger.com,1999:blog-19803222.post-3323848195507492414",
  "published": "2016-12-09T08:44:00.001-07:00",
  "updated": "2016-12-09T08:44:05.092-07:00",
  "title": "Whence your reward function?",
  "content": "I ran a grad seminar in <a href=\"http://ter.ps/rl16\">reinforcement learning this past semester</a>, which was a lot of fun and also gave me an opportunity to catch up on some stuff I'd been meaning to learn but haven't had a chance and old stuff I'd largely forgotten about. It's hard to believe, but my first RL paper was eleven years ago at a NIPS workshop where Daniel Marcu, John Langford and I had a <a href=\"http://www.umiacs.umd.edu/~hal/docs/daume05search.pdf\">first paper on reducing structured prediction to reinforcement learning</a>, essentially by running <a href=\"http://hal3.name/courses/2016F_RL/Kakade02.pdf\">Conservative Policy Iteration</a>. (This work eventually became <a href=\"http://hal3.name/docs/daume09searn.pdf\">Searn</a>.) Most of my own work in the RL space has focused on imitation learning/learning from demonstrations, but me and my students have recently been pushing more into straight up reinforcement learning <a href=\"http://hal3.name/docs/daume15lols.pdf\">algorithms</a> and <a href=\"http://www.umiacs.umd.edu/~hhe/papers/2016_icml_opponent.pdf\">applications</a> and <a href=\"http://arxiv.org/abs/1602.02181\">explanations</a> (also see <a href=\"https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf\">Tao Lei's awesome work in a similar explanations vein</a>, and Tim Vieira's really nice <a href=\"https://timvieira.github.io/doc/2016-tacl-pruning.pdf\">TACL paper</a> too).<br /><br />Reinforcement learning has undergone a bit of a renaissance recently, largely due to the efficacy of its combination with good function approximation via deep neural networks. Even more arguably this advance has been due to the increased availability and interest in \"interesting\" simulated environments, mostly video games and typified by the Atari game collection. In a very similar way that ImageNet made neural networks really work for computer vision (by being large, and capitalizing on the existence of GPUs), I think it's fair to say that these simulated environments have provided the same large data setting for RL that can also be combined with GPU power to build impressive solutions to many games.<br /><br />In a real sense, many parts of the RL community are going all-in on the notion that learning to play games is a path toward broader AI. The usual refrain that I hear arguing <i>against</i> that approach is based on the quantity of data. The argument is roughly: if you actually want to build a robot that acts in the real world, you're not going to be able to simulate 10 million frames (from the <a href=\"https://deepmind.com/research/dqn/\">Deepmind paper</a>, which is just under 8 days of real time experience).<br /><br />I think this is an issue, but I actually don't think it's the most substantial issue. I think the most substantial issue is the fact that game playing is a simulated environment and the reward function is generally crafted to make humans find the games fun, which usually means frequent small rewards that point you in the right direction. This is exactly where RL works well, and something that I'm not sure is a reasonable assumption in the real world.<br /><br />Delayed reward is one of the hardest issues in RL, because (a) it means you have to do a lot of exploration and (b) you have a significant credit assignment problem. For instance, if you imagine a variant of (pick your favorite video game) where you only get a +1/-1 reward at the end of the game that says whether you won or lost, it becomes much much harder to learn, even if you play 10 million frames or 10 billion frames.<br /><br />That's all to say: games are really nice settings for RL because there's a very well defined reward function and you typically get that reward very frequently. Neither of these things is going to be true in the real world, regardless of how much data you have.<br /><br />At the end of the day, playing video games, while impressive, is really not that different from doing classification on synthetic data. Somehow it's better because the people doing the research were not those who invented the synthetic data, but games---even recent games that you might play on your (insert whatever the current popular gaming system is) are still heavily designed---are built in such a way that they are fun for their human players, which typically means increasing difficulty/complexity and relatively regularly reward function.<br /><br />As we move toward systems that we expect to work in the real world (even if that is not embodied---I don't necessarily mean the difficulty of physical robots), it's less and less clear where the reward function comes from.<br /><br />One option is to design a reward function. For complex behavior, I don't think we have any idea how to do this. There is the joke example in the R+N AI textbook where you give a vacuum cleaner a reward function for number of pieces of gunk picked up; the vacuum learns to pick up gunk, then drop it, then pick it up again, ad infinitum. It's a silly example, but I don't think we have much of an understanding of how to design reward functions for truly complex behaviors without significant risk of \"unintended consequences.\" (To point a finger toward myself, we invented a reward function for simultaneous interpretation called <a href=\"http://hal3.name/docs/daume14simultaneousmt.pdf\">Latency-Bleu</a> a while ago, and six months later we realized there's a very simple way to game this metric. I was then disappointed that the models never learned that exploit.)<br /><br />This is one reason I've spent most of my RL effort on imitation learning (IL) like things, typically where you can simulate an oracle. I've rarely seen an NLP problem that's been solved with RL where I haven't thought that it would have been much better and easier to just do IL. Of course IL has it's own issues: it's not a panacea.<br /><br />One thing I've been thinking about a lot recently is forms of implicit feedback. One cool paper in this area I learned about when I visited GATech a few weeks ago is <a href=\"http://ieeexplore.ieee.org/abstract/document/7742965/?reload=true\">Learning from Explanations using Sentiment and Advice in RL</a> by Samantha Krening and colleagues. In this work they basically have a coach sitting on the side of an RL algorithm giving it advice, and used that to tailor things that I think of as more immediate reward. I generally think of this kind of like a baby. There's some built in reward signal (it can't be turtles all the way down), but what we think of as a reward signal (like a friend saying \"I really don't like that you did that\") only turn into this true reward through a learned model that tells me that that's negative feedback. I'd love to see more work in the area of trying to figure out how to transform sparse and imperfect \"true\" reward signals into something that we can actually learn to optimize.<br />",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "hal",
    "uri": "http://www.blogger.com/profile/02162908373916390369",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 4
}