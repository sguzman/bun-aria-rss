{
  "title": "Random forest interpretation with scikit-learn",
  "link": "http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/",
  "comments": "http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/#comments",
  "dc:creator": "ando",
  "pubDate": "Wed, 12 Aug 2015 20:48:38 +0000",
  "category": [
    "Machine learning",
    "Random forest"
  ],
  "guid": "http://blog.datadive.net/?p=3082",
  "description": "In one of my previous posts I discussed how random forests can be turned into a &#8220;white box&#8221;, such that each prediction is decomposed into a sum of contributions from each feature i.e. .I&#8217;ve a had quite a few requests &#8230; <a href=\"http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>In one of my <a href=\"/interpreting-random-forests/\">previous posts</a> I discussed how random forests can be turned into a &#8220;white box&#8221;, such that each prediction is decomposed into a sum of contributions from each feature i.e.  \\(prediction = bias + feature_1 contribution + &#8230; + feature_n contribution\\).</p><p>I&#8217;ve a had quite a few requests for code to do this. Unfortunately, most random forest libraries (including <a href=\"http://scikit-learn.org/\">scikit-learn</a>) don&#8217;t expose tree paths of predictions. The implementation for sklearn required a hacky patch for exposing the paths. Fortunately, since 0.17.dev, scikit-learn has two additions in the API that make this relatively straightforward: obtaining leaf node_ids for predictions, and storing all intermediate values in all nodes in decision trees, not only leaf nodes. Combining these, it is possible to extract the prediction paths for each individual prediction and decompose the predictions via inspecting the paths.</p><p>Without further ado, the code is available at <a href=\"https://github.com/andosa/treeinterpreter\">github</a>, and also via <code>pip install treeinterpreter</code></p><p><em>Note: this requires scikit-learn 0.17, which is still in development. You can check how to install it at http://scikit-learn.org/stable/install.html#install-bleeding-edge</em></p>\n<h2>Decomposing random forest predictions with treeinterpreter</h2><p>Let&#8217;s take a sample dataset, train a random forest model, predict some values on the test set and then decompose the predictions.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nfrom treeinterpreter import treeinterpreter as ti\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nrf = RandomForestRegressor()\nrf.fit(boston.data[:300], boston.target[:300])\n</pre><p>Lets pick two arbitrary data points that yield different price estimates from the model.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\ninstances = boston.data[[300, 309]]\nprint \"Instance 0 prediction:\", rf.predict(instances[0])\nprint \"Instance 1 prediction:\", rf.predict(instances[1])\n</pre>\n<blockquote><p>\n<code><br />\nInstance 0 prediction: [ 30.76]<br />\nInstance 1 prediction: [ 22.41]<br />\n</code>\n</p></blockquote><p>Predictions that the random forest model made for the two data points are quite different. But why? We can now decompose the predictions into the bias term (which is just the trainset mean) and individual feature contributions, so we see which features contributed to the difference and by how much.</p><p>We can simply call the treeinterpreter <code>predict </code> method with the model and the data.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nprediction, bias, contributions = ti.predict(rf, instances)\n</pre><p>Printint out the results:</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nfor i in range(len(instances)):\n    print \"Instance\", i\n    print \"Bias (trainset mean)\", biases[i]\n    print \"Feature contributions:\"\n    for c, feature in sorted(zip(contributions[i], \n                                 boston.feature_names), \n                             key=lambda x: -abs(x[0])):\n        print feature, round(c, 2)\n    print \"-\"*20  \n</pre>\n<blockquote><p><code><br />\nInstance 0<br />\nBias (trainset mean) 25.2849333333<br />\nFeature contributions:<br />\nRM 2.73<br />\nLSTAT 1.71<br />\nPTRATIO 1.27<br />\nZN 1.04<br />\nDIS -0.7<br />\nB -0.39<br />\nTAX -0.19<br />\nCRIM -0.13<br />\nRAD 0.11<br />\nINDUS 0.06<br />\nAGE -0.02<br />\nNOX -0.01<br />\nCHAS 0.0<br />\n--------------------<br />\nInstance 1<br />\nBias (trainset mean) 25.2849333333<br />\nFeature contributions:<br />\nRM -4.88<br />\nLSTAT 2.38<br />\nDIS 0.32<br />\nAGE -0.28<br />\nTAX -0.23<br />\nCRIM 0.16<br />\nPTRATIO 0.15<br />\nB -0.15<br />\nINDUS -0.14<br />\nCHAS -0.1<br />\nZN -0.05<br />\nNOX -0.05<br />\nRAD -0.02<br />\n</code></p></blockquote><p>The feature contributions are sorted by their absolute impact. We can see that in the first instance where the prediction was high, most of the positive contributions came from RM, LSTAT and PTRATIO feaures. On the second instance the predicted value is much lower, since RM feature actually has a very large negative impact that is not offset by the positive impact of other features, thus taking the prediction below the dataset mean.</p><p>But is the decomposition actually correct? This is easy to check: bias and contributions need to sum up to the predicted value:</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nprint prediction\nprint biases + np.sum(contributions, axis=1)\n</pre>\n<blockquote><p><code><br />\n[ 30.76  22.41]<br />\n[ 30.76  22.41]<br />\n</code></p></blockquote><p>Note that when summing up the contributions, we are dealing with floating point numbers so the values can slightly different due to rounding errors</p>\n<h2> Comparing too datasets</h2><p>One use case where this approach can be very useful is when comparing two datasets. For example</p>\n<ul>\n<li>Understanding the exact reasons why estimated values are different on two datasets, for example what contributes to estimated house prices being different in two neighborhoods.</li>\n<li>Debugging models and/or data, for example understanding why average predicted values on newer data do not match the results seen on older data.</li>\n</ul><p>For this example, let&#8217;s split the remaining housing price data into two test datasets and compute the average estimated prices for them.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nds1 = boston.data[300:400]\nds2 = boston.data[400:]\n\nprint np.mean(rf.predict(ds1))\nprint np.mean(rf.predict(ds2))\n</pre>\n<blockquote><p><code><br />\n22.1912<br />\n18.4773584906<br />\n</code></p></blockquote><p>We can see that the average predicted prices for the houses in the two datasets are quite different. We can now trivially break down the contributors to this difference: which features contribute to this different and by how much.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nprediction1, bias1, contributions1 = ti.predict(rf, ds1)\nprediction2, bias2, contributions2 = ti.predict(rf, ds2)\n</pre><p>We can now calculate the mean contribution of each feature to the difference.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\ntotalc1 = np.mean(contributions1, axis=0) \ntotalc2 = np.mean(contributions2, axis=0) \n</pre><p>Since biases are equal for both datasets (because the training data for the model was the same), the difference between the average predicted values has to come only from feature contributions. In other words, the sum of the feature contribution differences should be equal to the difference in average prediction, which we can trivially check to be the case</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nprint np.sum(totalc1 - totalc2)\nprint np.mean(prediction1) - np.mean(prediction2)\n</pre>\n<blockquote><p><code><br />\n3.71384150943<br />\n3.71384150943<br />\n</code></p></blockquote><p>Finally, we can just print out the differences of the contributions in the two datasets. The sum of these is exactly the difference between the average predictions.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nfor c, feature in sorted(zip(totalc1 - totalc2, \n                             boston.feature_names), reverse=True):\n    print feature, round(c, 2)\n</pre>\n<blockquote><p><code><br />\nLSTAT 2.8<br />\nCRIM 0.5<br />\nRM 0.5<br />\nPTRATIO 0.09<br />\nAGE 0.08<br />\nNOX 0.03<br />\nB 0.01<br />\nCHAS -0.01<br />\nZN -0.02<br />\nRAD -0.03<br />\nINDUS -0.03<br />\nTAX -0.08<br />\nDIS -0.14<br />\n</code></p></blockquote>\n<h2>Classification trees and forests</h2><p>Exactly the same method can be used for classification trees, where features contribute to the estimated probability of a given class.<br />\nWe can see this on the sample iris dataset.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nrf = RandomForestClassifier(max_depth = 4)\nidx = range(len(iris.target))\nnp.random.shuffle(idx)\n\nrf.fit(iris.data[idx][:100], iris.target[idx][:100])\n</pre><p>Let&#8217;s predict now for a single instance.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\ninstance = iris.data[idx][100:101]\nprint rf.predict_proba(instance)\n</pre><p>Breakdown of feature contributions:</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nprediction, bias, contributions = ti.predict(rf, instance)\nprint \"Prediction\", prediction\nprint \"Bias (trainset prior)\", bias\nprint \"Feature contributions:\"\nfor c, feature in zip(contributions[0], \n                             iris.feature_names):\n    print feature, c\n</pre>\n<blockquote><p><code><br />\nPrediction [[ 0.   0.9  0.1]]<br />\nBias (trainset prior) [[ 0.36   0.262  0.378]]<br />\nFeature contributions:<br />\nsepal length (cm) [-0.1228614   0.07971035  0.04315104]<br />\nsepal width (cm) [ 0.         -0.01352012  0.01352012]<br />\npetal length (cm) [-0.11716058  0.24709886 -0.12993828]<br />\npetal width (cm) [-0.11997802  0.32471091 -0.20473289]<br />\n</code></p></blockquote><p>We can see that the strongest contributors to predicting the second class were petal length and width, which had the larges impact on updating the prior.</p>\n<h2>Summary</h2><p>Making random forest predictions interpretable is actually pretty straightforward, and leading to similar level of interpretability as linear models. With <a href=\"https://github.com/andosa/treeinterpreter\">treeinterpreter</a> (<code>pip install treeinterpreter</code>), this can be done with just a couple of lines of code.</p>\n<div style=\"padding-top:0px;\t\npadding-right:0px;\npadding-bottom:0px;\npadding-left:0px;\nmargin-top:0px;\nmargin-right:0px;\nmargin-bottom:0px;\nmargin-left:0px;\"><a href=\"https://twitter.com/crossentropy\" class=\"twitter-follow-button\" \n\t\t\t\t\t\tdata-show-count=\"false\"\n\t\t\t\t\t\tdata-lang=\"autoLANGauto\"\n\t\t\t\t\t\tdata-width=\"250px\"\n\t\t\t\t\t\tdata-align=\"left\"\n\t\t\t\t\t\tdata-show-screen-name=\"true\"\n\t\t\t\t\t\tdata-size=\"medium\"\n\t\t\t\t\t\tdata-dnt=\"false\">\n\t\t\t\t\t\tFollow @crossentropy </a> </div>\n\t\t\t\t\t\t<script>\n\t\t\t\t\t\t!function(d,s,id) {\n\t\t\t\t\t\t  var js,fjs=d.getElementsByTagName(s)[0];\n\t\t\t\t\t\t  if(!d.getElementById(id)) {\n\t\t\t\t\t\t   js=d.createElement(s);\n\t\t\t\t\t\t   js.id=id;js.src=\"//platform.twitter.com/widgets.js\";\n\t\t\t\t\t\t   fjs.parentNode.insertBefore(js,fjs);\n\t\t\t\t\t\t  }\n\t\t\t\t\t\t}\n\t\t\t\t\t\t(document,\"script\",\"twitter-wjs\");\n\t\t\t\t\t\t</script><div style=\"padding-top:0px;\t\npadding-right:0px;\npadding-bottom:0px;\npadding-left:0px;\nmargin-top:0px;\nmargin-right:0px;\nmargin-bottom:0px;\nmargin-left:0px;\"><a href=\"https://twitter.com/share\" class=\"twitter-share-button\" \n\t\t\t\t        data-url=\"http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/\" \n\t\t\t\t        data-via=\"crossentropy\"\n\t\t\t\t\t    data-text=\"Random forest interpretation with scikit-learn\"\n\t\t\t\t\t    data-related=\"\"\n\t\t\t\t\t    data-count=\"horizontal\"\n\t\t\t\t\t    data-hashtags=\"\"\n\t\t\t\t\t    data-lang=\"autoLANGauto\"\n\t\t\t\t\t    data-counturl=\"\"\n\t\t\t\t\t    data-size=\"medium\"\n\t\t\t\t\t    data-dnt=\"false\"\t> Tweet </a> </div>\n\t\t                <script>\n\t\t\t\t\t    !function(d,s,id) {\n\t\t\t\t\t      var js,fjs=d.getElementsByTagName(s)[0];\n\t\t\t\t\t      if(!d.getElementById(id)) {\n\t\t\t\t\t       js=d.createElement(s);js.id=id;js.src=\"https://platform.twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);\n\t\t\t\t\t      }\n\t\t\t\t\t    }\n\t\t\t\t\t   (document,\"script\",\"twitter-wjs\");\n\t\t\t\t\t    </script>",
  "wfw:commentRss": "http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/feed/",
  "slash:comments": 50
}