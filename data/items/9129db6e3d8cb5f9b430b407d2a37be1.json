{
  "title": "Inverse scaling can become U-shaped. (arXiv:2211.02011v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.02011",
  "description": "<p>Although scaling language models improves performance on a range of tasks,\nthere are apparently some scenarios where scaling hurts performance. For\ninstance, the Inverse Scaling Prize Round 1 identified four ''inverse scaling''\ntasks, for which performance gets worse for larger models. These tasks were\nevaluated on models of up to 280B parameters, trained up to 500 zettaFLOPs of\ncompute.\n</p>\n<p>This paper takes a closer look at these four tasks. We evaluate models of up\nto 540B parameters, trained on five times more compute than those evaluated in\nthe Inverse Scaling Prize. With this increased range of model sizes and\ntraining compute, three out of the four tasks exhibit what we call ''U-shaped\nscaling'' -- performance decreases up to a certain model size, and then\nincreases again up to the largest model evaluated. One hypothesis is that\nU-shaped scaling occurs when a task comprises a ''true task'' and a\n''distractor task''. Medium-size models can do the distractor task, which hurts\nperformance, while only large-enough models can ignore the distractor task and\ndo the true task. The existence of U-shaped scaling implies that inverse\nscaling may not hold for larger models.\n</p>\n<p>Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT)\nprompting, in addition to basic prompting without CoT. With CoT prompting, all\nfour tasks show either U-shaped scaling or positive scaling, achieving perfect\nsolve rates on two tasks and several sub-tasks. This suggests that the term\n\"inverse scaling task\" is under-specified -- a given task may be inverse\nscaling for one prompt but positive or U-shaped scaling for a different prompt.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"
}