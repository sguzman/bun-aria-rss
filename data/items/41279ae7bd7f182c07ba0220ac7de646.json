{
  "title": "FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01572",
  "description": "<p>Federated learning is an emerging learning paradigm where multiple clients\ncollaboratively train a machine learning model in a privacy-preserving manner.\nPersonalized federated learning extends this paradigm to overcome heterogeneity\nacross clients by learning personalized models. Recently, there have been some\ninitial attempts to apply Transformers to federated learning. However, the\nimpacts of federated learning algorithms on self-attention have not yet been\nstudied. This paper investigates this relationship and reveals that federated\naveraging algorithms actually have a negative impact on self-attention where\nthere is data heterogeneity. These impacts limit the capabilities of the\nTransformer model in federated learning settings. Based on this, we propose\nFedTP, a novel Transformer-based federated learning framework that learns\npersonalized self-attention for each client while aggregating the other\nparameters among the clients. Instead of using a vanilla personalization\nmechanism that maintains personalized self-attention layers of each client\nlocally, we develop a learn-to-personalize mechanism to further encourage the\ncooperation among clients and to increase the scablability and generalization\nof FedTP. Specifically, the learn-to-personalize is realized by learning a\nhypernetwork on the server that outputs the personalized projection matrices of\nself-attention layers to generate client-wise queries, keys and values.\nFurthermore, we present the generalization bound for FedTP with the\nlearn-to-personalize mechanism. Notably, FedTP offers a convenient environment\nfor performing a range of image and language tasks using the same federated\nnetwork architecture - all of which benefit from Transformer personalization.\nExtensive experiments verify that FedTP with the learn-to-personalize mechanism\nyields state-of-the-art performance in non-IID scenarios. Our code is available\nonline.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiangnan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weiping Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Teng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Ye Shi</a>"
}