{
  "title": "Beer reviews with Recurrent Neural Networks (RNN)",
  "link": "https://datanice.wordpress.com/2015/10/08/beer-reviews-with-recurrent-neural-networks-rnn/",
  "comments": "https://datanice.wordpress.com/2015/10/08/beer-reviews-with-recurrent-neural-networks-rnn/#respond",
  "dc:creator": "aptissimum",
  "pubDate": "Thu, 08 Oct 2015 13:27:18 +0000",
  "category": [
    "Non classé",
    "beer",
    "deep learning",
    "neural networks",
    "rnn"
  ],
  "guid": "http://datanice.wordpress.com/?p=67",
  "description": "Since Andrej Karpathy conviced me of the The Unreasonable Effectiveness of Recurrent Neural Networks, I decided to give it a try as soon as possible. As I was wondering in the Wiesn in Munich for the Oktoberfest, the beer festival, I wondered how would a RNN write a beer review. The beautiful thing about recurrent neural [&#8230;]",
  "content:encoded": "<p>Since Andrej Karpathy conviced me of the <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a>, I decided to give it a try as soon as possible.</p>\n<p>As I was wondering in the Wiesn in Munich for the Oktoberfest, the beer festival, I wondered how would a RNN write a beer review.</p>\n<p><a href=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg\"><img data-attachment-id=\"70\" data-permalink=\"https://datanice.wordpress.com/2015/10/08/beer-reviews-with-recurrent-neural-networks-rnn/img_20150928_201119163/#main\" data-orig-file=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg\" data-orig-size=\"4160,2340\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"2\",\"credit\":\"\",\"camera\":\"MotoG3\",\"caption\":\"\",\"created_timestamp\":\"1443471078\",\"copyright\":\"\",\"focal_length\":\"3.64\",\"iso\":\"160\",\"shutter_speed\":\"0.03001\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"IMG_20150928_201119163\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=300\" data-large-file=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=525\" class=\"alignnone size-full wp-image-70\" src=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=525\" alt=\"IMG_20150928_201119163\" srcset=\"https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=525 525w, https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=1050 1050w, https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=150 150w, https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=300 300w, https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=768 768w, https://datanice.files.wordpress.com/2015/10/img_20150928_201119163.jpg?w=1024 1024w\" sizes=\"(max-width: 525px) 100vw, 525px\"   /></a></p>\n<p><span id=\"more-67\"></span>The beautiful thing about recurrent neural networks compared with regular neural networks is that they keep a history of all the past elements in a sequence in their hidden layers. They can be very efficient at generating sequences like text or music <a href=\"http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/\">Composing Music With Recurrent Neural Networks</a>.</p>\n<p><img src=\"https://i0.wp.com/karpathy.github.io/assets/rnn/charseq.jpeg\" alt=\"\" width=\"70%\" /></p>\n<p>In the diagram above (from Andrej Karpathy blog post) , we see how the different layers are linked to each other in the RNN. Note that to train a Neural Network we can use a regular backpropagation algorithm adjusting weights to increase the score of the desired output. For the test phase, we feed each output to the next input to generate a text sample.</p>\n<p>I used the <a href=\"https://github.com/gjreda/beer-snob-says/blob/master/sample.txt\">data</a> from Greg Reda repository, who created a beer review bot <a href=\"http://www.gregreda.com/2015/03/30/beer-review-markov-chains/\">using Markov Chains.</a>So what do RNN think about beer ?</p>\n<blockquote><p>The smell is creamy, malty and woody, not much presence. The taste is dark fruits, and floral hops before its a strong destroy from the mouth as it warms up</p></blockquote>\n<p>We see that the vocabulary is not perfect, Here I&#8217;m using a hign temperature which makes the RNN take greater risks but causes more mistakes.</p>\n<p>With a low temperature (0.3) we get the most frequent expressions, but not too much innovation</p>\n<blockquote><p>Pours a clear golden color with a thin head that dissipated quickly. The smell is very sweet and sweet with a bit of citrus and a hint of citrus. The taste is somewhat sweet and smokey with a light bitterness to it. The body is light and creamy with a somewhat smooth finish.</p></blockquote>\n<p>Notice that we find a complete review :look, smell ,taste and body. Sometimes I also encounter grades. The RNN has not only learnt to use words and expressions but it has also learnt the layout of a beer review !</p>\n<blockquote><p>The mouthfeel is good with a slightly sweet taste. The finish is sweet and refreshing, and it was pretty good. I smell a bit of bitterness and a crisp aftertaste.</p></blockquote>\n<p>We can also force the network to use some text at the beginning, we can get some nice reviews :</p>\n<blockquote><p>This one reminds me of a porter. Pours a moderate copper colour, with a malty straw color. Thick head that stays and left leaving no lacing.</p>\n<p>This one is pretty nice. The carbonation is in the mouth, creamy and pretty drinkable. The hops are the grainy and piney flavors even the hops right off</p>\n<p>One of my new large rum. I have got to sip a good brewery. It is by the 2007 barrelard for a consume. Just no hoppiness for the point.</p>\n<p>One of my favourite of all the weizen glass.Pours a bright straw color with beige gold colored head of good carbonation. Plenty of lace. Smell is caramel malt, and yeast.</p></blockquote>\n<p>I can go on for long with this&#8230; If you want to train your own RNN, just use <a href=\"https://github.com/karpathy/char-rnn\">Andrej&#8217;s Code on Github</a>, he also made a <a href=\"https://gist.github.com/karpathy/d4dee566867f8291f086\">simple version in numpy/python</a></p>\n",
  "wfw:commentRss": "https://datanice.wordpress.com/2015/10/08/beer-reviews-with-recurrent-neural-networks-rnn/feed/",
  "slash:comments": 0,
  "media:content": [
    {
      "media:title": "aptissimum"
    },
    {
      "media:title": "IMG_20150928_201119163"
    },
    ""
  ]
}