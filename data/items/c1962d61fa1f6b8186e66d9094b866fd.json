{
  "title": "Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01883",
  "description": "<p>Composition optimization recently appears in many machine learning\napplications such as meta learning and reinforcement learning. Recently many\ncomposition optimization algorithms have been proposed and studied, however,\nfew adaptive algorithm considers the composition optimization under the\ndistributed setting. Meanwhile, the existing distributed composition\noptimization methods still suffer from high sample and communication\ncomplexities. In the paper, thus, we develop a class of faster momentum-based\nfederated compositional gradient descent algorithms (i.e., MFCGD and AdaMFCGD)\nto solve the nonconvex distributed composition problems, which builds on the\nmomentum-based variance reduced and local-SGD techniques. In particular, our\nadaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly\nincorporate various adaptive learning rates. Moreover, we provide a solid\ntheoretical analysis for our algorithms under non-i.i.d. setting, and prove our\nalgorithms obtain a lower sample and communication complexities simultaneously\nthan the existing federated compositional algorithms. Specifically, our\nalgorithms obtain lower sample complexity of $\\tilde{O}(\\epsilon^{-3})$ with\nlower communication complexity of $\\tilde{O}(\\epsilon^{-2})$ in finding an\n$\\epsilon$-stationary point. We conduct the experiments on robust federated\nlearning and distributed meta learning tasks to demonstrate efficiency of our\nalgorithms.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>"
}