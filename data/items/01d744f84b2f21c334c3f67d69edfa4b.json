{
  "title": "Visualizing an actor critic algorithm in real time",
  "link": "",
  "published": "2020-05-07T12:00:00-07:00",
  "updated": "2020-05-07T12:00:00-07:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2020-05-07:/visualize-actor-critic",
  "summary": "<p>Deep reinforcement learning algorithms can be hard to debug, so it helps to visualize as much as possible in the absence of a stack trace [1].  How do we know if the learned policy and value functions make sense?  Seeing these quantities plotted in real time as an agent is …</p>",
  "content": "<p>Deep reinforcement learning algorithms can be hard to debug, so it helps to visualize as much as possible in the absence of a stack trace [1].  How do we know if the learned policy and value functions make sense?  Seeing these quantities plotted in real time as an agent is interacting with an environment can help us answer that&nbsp;question.</p>\n<p>Here’s an example of an agent wandering around a custom <a href=\"https://github.com/frangipane/gym-minigrid\">gridworld</a> environment.  When the agent executes the <code>toggle</code> action in front of an unopened red gift, it receives a reward of 1 point, and the gift turns&nbsp;grey/inactive.</p>\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/M3PMwPFRoc8\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n<p>The model is an actor critic, a type of policy gradient algorithm (for a nice introduction, see Jonathan’s <a href=\"https://efavdb.com/battleship\">battleship</a> post or [2]) that uses a neural network to parametrize its policy and value&nbsp;functions.</p>\n<p>This agent barely &#8220;meets expectations&#8221; &#8212; notably getting stuck at an opened gift between frames 5-35 &#8212; but the values and policy mostly make sense.  For example, we tend to see spikes in value when the agent is immediately in front of an unopened gift while the policy simultaneously outputs a much higher probability of taking the appropriate <code>toggle</code> action in front of the unopened gift.  (We&#8217;d achieve better performance by incorporating some memory into the model in the form of an <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\"><span class=\"caps\">LSTM</span></a>).</p>\n<p>We’re sharing a little helper code to generate the matplotlib plots of the value and policy functions that are shown in the&nbsp;video.</p>\n<script src=\"https://gist.github.com/frangipane/4adca6481bf55f2260ff215c5686851b.js\"></script>\n\n<p><strong>Comments</strong></p>\n<ul>\n<li>Training of the model is not included.  You&#8217;ll need to load a trained actor critic model, along with access to its policy and value functions for plotting.  Here, the trained model has been loaded into <code>agent</code> with a <code>get_action</code> method that returns the <code>action</code> to take, along with a numpy array of <code>policy</code> probabilities and a scalar <code>value</code> for the observation at the current time&nbsp;step.</li>\n<li>The minigridworld environment conforms to the OpenAI gym <span class=\"caps\">API</span>, and the <code>for</code> loop is a standard implementation for interacting with the&nbsp;environment.</li>\n<li>The gridworld environment already has a built in method for rendering the environment in iteractive mode <code>env.render('human')</code>.</li>\n<li>Matplotlib&#8217;s <code>autoscale_view</code> and <code>relim</code> functions are used to make updates to the figures at each step.  In particular, this allows us to show what appears to be a sliding window over time of the value function line plot.  When running the script, the plots pop up as three separate&nbsp;figures.</li>\n</ul>\n<h3>References</h3>\n<p>[1] Berkeley Deep <span class=\"caps\">RL</span> bootcamp - Core Lecture 6 Nuts and Bolts of Deep <span class=\"caps\">RL</span> Experimentation &#8212; John Schulman (<a href=\"https://youtu.be/8EcdaCk9KaQ\">video</a> | <a href=\"https://drive.google.com/open?id=0BxXI_RttTZAhc2ZsblNvUHhGZDA\">slides</a>) - great advice on the debugging process, things to&nbsp;plot</p>\n<p>[2] OpenAI Spinning Up: <a href=\"https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\">Intro to policy&nbsp;optimization</a></p>",
  "category": [
    "",
    "",
    "",
    "",
    ""
  ]
}