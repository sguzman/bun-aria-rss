{
  "title": "Why is Keras Running So Slow?",
  "description": "<p><a href=\"/wp-content/uploads/2015/12/meme.jpg\"><img class=\"aligncenter size-full wp-image-640\" src=\"/wp-content/uploads/2015/12/meme.jpg\" alt=\"meme\" width=\"512\" height=\"340\" /></a></p>\n\n<p>More notes for myselfâ€¦ so it may not be helpful for you who bumped into here. ðŸ˜‰</p>\n\n<h1 id=\"why-this-article\">Why This Article?</h1>\n\n<p><a href=\"/how-to-setup-theano-to-run-on-gpu-on-ubuntu-14-04-with-nvidia-geforce-gtx-780/\" target=\"_blank\">Setting Theano correctly</a>Â <strong>is not enough</strong> to ensureÂ you can run deep learning software correctly. In our case, it will be Keras, and it can slow to a crawl ifÂ not setup properly.</p>\n\n<p>Again, there could be many causes but I try to outline a clean step what I did, theÂ performance I run aÂ good setup, so you can compare. Hopefully you can glean some places where you did wrong.</p>\n\n<h1 id=\"specifications\">Specifications</h1>\n\n<p>MyÂ server hasÂ the following specifications finished running the steps outlined <a href=\"/how-to-setup-theano-to-run-on-gpu-on-ubuntu-14-04-with-nvidia-geforce-gtx-780/\" target=\"_blank\">here</a>.</p>\n\n<ul>\n  <li>OS: Ubuntu 14.04 LTS, X64</li>\n  <li>GPU:Â NvidiaÂ Geforce GTX 780</li>\n  <li>Ubuntu 14.04 LTS</li>\n  <li>CUDA 7.5</li>\n  <li>Theano 0.7.0</li>\n  <li>Numpy 1.8.2</li>\n  <li>Kera 0.2.0</li>\n  <li>Scipy 0.13.3</li>\n  <li>NVIDIA-SMI 352.39</li>\n  <li>Graphics Driver Version: 352.39</li>\n</ul>\n\n<h1 id=\"instructions\">Instructions</h1>\n\n<ol>\n  <li>Make sure your TheanoÂ configuration file, located at ~/.theanorc, is correct:</li>\n</ol>\n\n<pre>$ cat ~/.theanorc\n[global]\nfloatX = float32\ndevice = gpu\noptimizer = fast_run\n\n[lib]\ncnmem = 0.8\n\n[nvcc]\nfastmath = True\n\n[blas]\nldflags = -llapack -lblas</pre>\n\n<p>Use this to see your Theano settings in runtimeÂ and make sure it matches what you have above. Only some output is shown since it is very long:</p>\n\n<pre>$ python -c <span class=\"s1\">'import theano; print theano.config'\n</span>\nUsing gpu device 0: GeForce GTX 780 (CNMeM is enabled)\nfloatX (('float64', 'float32', 'float16')) \n Doc: Default floating-point precision for python casts.\n\nNote: float16 support is experimental, use at your own risk.\n Value: float32\n\nwarn_float64 (('ignore', 'warn', 'raise', 'pdb')) \n Doc: Do an action when a tensor variable with float64 dtype is created. They can't be run on the GPU with the current(old) gpu back-end and are slow with gamer GPUs.\n Value: ignore\n\n...\n...\n...\n\n</pre>\n\n<ol>\n  <li>Install Theano bleeding edge (0.7.0), sinceÂ the Keras examples needs â€˜reluâ€™.</li>\n</ol>\n\n<pre>pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git</pre>\n\n<p>Detailed instructions <a href=\"http://deeplearning.net/software/theano/install.html#bleeding-edge-install-instructions\">here</a>.</p>\n\n<p>3.Â Get Keras source and run the mnist_mlp.py to checkÂ the performance.</p>\n\n<pre>$ git cloneÂ https://github.com/fchollet/keras.git\nCloning into 'keras'...\nremote: Counting objects: 6572, done.\nremote: Total 6572 (delta 0), reused 0 (delta 0), pack-reused 6572\nReceiving objects: 100% (6572/6572), 1.29 MiB | 894.00 KiB/s, done.\nResolving deltas: 100% (4571/4571), done.\nChecking connectivity... done.</pre>\n\n<pre>$ cd keras/examples\n$ $ time python mnist_mlp.py \nUsing Theano backend.\nUsing gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n60000 train samples\n10000 test samples\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/20\n0s - loss: 0.2805 - acc: 0.9148 - val_loss: 0.1165 - val_acc: 0.9636\nEpoch 2/20\n0s - loss: 0.1151 - acc: 0.9651 - val_loss: 0.0960 - val_acc: 0.9685\nEpoch 3/20\n0s - loss: 0.0800 - acc: 0.9754 - val_loss: 0.0670 - val_acc: 0.9787\nEpoch 4/20\n0s - loss: 0.0624 - acc: 0.9806 - val_loss: 0.0703 - val_acc: 0.9775\nEpoch 5/20\n0s - loss: 0.0506 - acc: 0.9837 - val_loss: 0.0622 - val_acc: 0.9795\nEpoch 6/20\n0s - loss: 0.0414 - acc: 0.9867 - val_loss: 0.0641 - val_acc: 0.9803\nEpoch 7/20\n0s - loss: 0.0347 - acc: 0.9892 - val_loss: 0.0665 - val_acc: 0.9802\nEpoch 8/20\n0s - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0769 - val_acc: 0.9789\nEpoch 9/20\n0s - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0586 - val_acc: 0.9830\nEpoch 10/20\n0s - loss: 0.0215 - acc: 0.9928 - val_loss: 0.0577 - val_acc: 0.9841\nEpoch 11/20\n1s - loss: 0.0197 - acc: 0.9932 - val_loss: 0.0605 - val_acc: 0.9844\nEpoch 12/20\n0s - loss: 0.0180 - acc: 0.9940 - val_loss: 0.0560 - val_acc: 0.9863\nEpoch 13/20\n0s - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0630 - val_acc: 0.9838\nEpoch 14/20\n0s - loss: 0.0136 - acc: 0.9956 - val_loss: 0.0608 - val_acc: 0.9857\nEpoch 15/20\n0s - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0616 - val_acc: 0.9838\nEpoch 16/20\n0s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0584 - val_acc: 0.9854\nEpoch 17/20\n0s - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0672 - val_acc: 0.9849\nEpoch 18/20\n0s - loss: 0.0106 - acc: 0.9964 - val_loss: 0.0678 - val_acc: 0.9846\nEpoch 19/20\n0s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0749 - val_acc: 0.9835\nEpoch 20/20\n0s - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0685 - val_acc: 0.9843\nTest score: 0.0685058810504\nTest accuracy: 0.9843\n\n<span style=\"color: #00ff00;\"><strong>real 0m24.560s</strong></span>\nuser 0m23.216s\nsys 0m1.328s\n\n\n</pre>\n\n<p>As you can see,Â the whole run takes only 25 seconds, and itÂ may take even less or maybe 2 minutes for you. Anything longer than that looks strange and you should inspect.</p>\n\n<ol>\n  <li>Install <a href=\"https://developer.nvidia.com/cuDNN\" target=\"_blank\">CuDNN</a> if you are using ConvNet.Â The basic implementations of convolution in Theano are significantly slower.</li>\n</ol>\n\n<p>Downloading CuDNN is problematic, because you have to register an account on Nvidia and wait for hours or days for manual approval. Someone uploaded a version of CuDNN 6.5 for download on Google Drive <a href=\"http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html\" target=\"_blank\">here</a> if you donâ€™t want to wait.</p>\n\n<p>Once you have it, just unzip theÂ tgz file.</p>\n\n<pre>$ tar zxvf cudnn-6.5-linux-x64-v2.tgz\n$ cd cudnn-6.5-linux-x64-v2\n$ ls\ncudnn.h CUDNN_License.pdf INSTALL.txt libcudnn.so libcudnn.so.6.5 libcudnn.so.6.5.48 libcudnn_static.a\n$ pwd\n/home/echio/src/cudnn-6.5-linux-x64-v2</pre>\n\n<ol>\n  <li>Make sure your CUDA and CuDNN are bothÂ accessible to Theano.</li>\n</ol>\n\n<p>To check if your Theano is using CuDNN.Â Run this Python code below:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code># Run this python code\nfrom theano.sandbox.cuda.dnn import *\nprint(dnn_available())\nprint(dnn_available.msg)\n</code></pre>\n</div>\n\n<p>I alsoÂ captured the environment variables, replace echio with your username:</p>\n\n<pre>$ echo $CPATH\n\n$ echo $LD_LIBRARY_PATH\n/usr/local/cuda-7.5/lib64:\n$ echo $LIBRARY_PATH\n/usr/local/cuda-7.5/lib64:\n$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from theano.sandbox.cuda.dnn import *\nUsing gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n&gt;&gt;&gt; print(dnn_available())\nFalse\n&gt;&gt;&gt; print(dnn_available.msg)\n<strong>Theano can not compile with cuDNN. We got this error:</strong>\n<strong>/tmp/try_flags_sbkMKM.c:5:19: fatal error: cudnn.h: No such file or directory</strong>\n #include &lt;cudnn.h&gt;\n ^\ncompilation terminated.</pre>\n\n<p><a href=\"https://goo.gl/bTW22Q\" target=\"_blank\">Googling the error message</a>Â doesnâ€™t help too much.</p>\n\n<p>You need to add the location of the 3. into CPATH, LD_LIBRARY_PATH and LIBRARY_PATH. This is what myÂ .bashrc looks like this (replace echio with your username):</p>\n\n<pre># ~/.bashrc\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.5/lib64:/home/echio/src/cudnn-6.5-linux-x64-v2:\nexport LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.5/lib64:/home/echio/src/cudnn-6.5-linux-x64-v2:\nexport CPATH=$CPATH:/home/echio/src/cudnn-6.5-linux-x64-v2:\nexport PATH=$PATH:/usr/local/cuda-7.5/bin</pre>\n\n<p>If you instead see this error message:</p>\n\n<pre>ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: libcublas.so.7.5: cannot open shared object file: No such file or directory</pre>\n\n<p>You probably didnâ€™t have CUDA environment variables setup properly. See the above ~/.bashrc lines for correct setup.</p>\n\n<ol>\n  <li>\n    <p>Run this codeÂ again.</p>\n\n    <p># Run this python code\n from theano.sandbox.cuda.dnn import *\n print(dnn_available())\n print(dnn_available.msg)</p>\n  </li>\n</ol>\n\n<p>You should see below when executed in a Python REPL.</p>\n\n<pre>&gt;&gt;&gt; from theano.sandbox.cuda.dnn import *\nUsing gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n&gt;&gt;&gt; print(dnn_available())\nTrue\n&gt;&gt;&gt; print(dnn_available.msg)\nNone</pre>\n\n<p>This is good! Re-run your Keras code and hopefully it will be fast this timeâ€¦</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>ThisÂ may or may not solve your problem, but it certainly solved some of my problems. You will probably have to learn to debug things a bit to figure out how to get it to run well.</p>\n\n<h4 id=\"references\">References</h4>\n\n<ul>\n  <li><a href=\"http://deeplearning.net/software/theano/library/config.html\" target=\"_blank\">http://deeplearning.net/software/theano/library/config.html</a></li>\n  <li><a href=\"http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html\" target=\"_blank\">http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html</a></li>\n  <li><a href=\"https://groups.google.com/forum/#!topic/keras-users/EAbpVHJBvGQ\" target=\"_blank\">https://groups.google.com/forum/#!topic/keras-users/EAbpVHJBvGQ</a></li>\n</ul>\n\n<p>Â </p>",
  "pubDate": "Sat, 05 Dec 2015 16:31:15 +0000",
  "link": "http://www.chioka.in/why-is-keras-running-so-slow/",
  "guid": "http://www.chioka.in/why-is-keras-running-so-slow/",
  "category": [
    "Advice",
    "I Hate Linux",
    "Keras",
    "Linux",
    "Theano",
    "Tutorial",
    "In Practice",
    "Machine Learning"
  ]
}