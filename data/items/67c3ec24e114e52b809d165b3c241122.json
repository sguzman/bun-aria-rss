{
  "title": "If you did not already know",
  "link": "https://analytixon.com/2022/11/03/if-you-did-not-already-know-1876/",
  "comments": "https://analytixon.com/2022/11/03/if-you-did-not-already-know-1876/#respond",
  "dc:creator": "Michael Laux",
  "pubDate": "Thu, 03 Nov 2022 02:03:17 +0000",
  "category": "What is ...",
  "guid": "https://analytixon.com/?p=37445",
  "description": "Extended Fourier Amplitude Sensitivity Test Excluding irrelevant features in a pattern recognition task plays an important role in maintaining a &#8230;<p><a href=\"https://analytixon.com/2022/11/03/if-you-did-not-already-know-1876/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p><a href=\"http://arxiv.org/abs/1804.05092v1\" target=\"top\" rel=\"noopener\"><strong>Extended Fourier Amplitude Sensitivity Test</strong></a>  <a href=\"https://www.google.de/search?q=Extended Fourier Amplitude Sensitivity Test\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">Excluding irrelevant features in a pattern recognition task plays an important role in maintaining a simpler machine learning model and optimizing the computational efficiency. Nowadays with the rise of large scale datasets, feature selection is in great demand as it becomes a central issue when facing high-dimensional datasets. The present study provides a new measure of saliency for features by employing a Sensitivity Analysis (SA) technique called the extended Fourier amplitude sensitivity test, and a well-trained Feedforward Neural Network (FNN) model, which ultimately leads to the selection of a promising optimal feature subset. Ideas of the paper are mainly demonstrated based on adopting FNN model for feature selection in classification problems. But in the end, a generalization framework is discussed in order to give insights into the usage in regression problems as well as expressing how other function approximate models can be deployed. Effectiveness of the proposed method is verified by result analysis and data visualization for a series of experiments over several well-known datasets drawn from UCI machine learning repository. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1905.02266v1\" target=\"top\" rel=\"noopener\"><strong>Maximally Filtered Clique Forest (MFCF)</strong></a>  <a href=\"https://www.google.de/search?q=Maximally Filtered Clique Forest\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">We propose a topological learning algorithm for the estimation of the conditional dependency structure of large sets of random variables from sparse and noisy data. The algorithm, named Maximally Filtered Clique Forest (MFCF), produces a clique forest and an associated Markov Random Field (MRF) by generalising Prim&#8217;s minimum spanning tree algorithm. To the best of our knowledge, the MFCF presents three elements of novelty with respect to existing structure learning approaches. The first is the repeated application of a local topological move, the clique expansion, that preserves the decomposability of the underlying graph. Through this move the decomposability and calculation of scores is performed incrementally at the variable (rather than edge) level, and this provides better computational performance and an intuitive application of multivariate statistical tests. The second is the capability to accommodate a variety of score functions and, while this paper is focused on multivariate normal distributions, it can be directly generalised to different types of statistics. Finally, the third is the variable range of allowed clique sizes which is an adjustable topological constraint that acts as a topological penalizer providing a way to tackle sparsity at $l_0$ semi-norm level; this allows a clean decoupling of structure learning and parameter estimation. The MFCF produces a representation of the clique forest, together with a perfect ordering of the cliques and a perfect elimination ordering for the vertices. As an example we propose an application to covariance selection models and we show that the MCFC outperforms the Graphical Lasso for a number of classes of matrices. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1806.08198v1\" target=\"top\" rel=\"noopener\"><strong>DPP-Net</strong></a>  <a href=\"https://www.google.de/search?q=DPP-Net\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in applications such as image classification and language modeling. However, these techniques typically ignore device-related objectives such as inference time, memory usage, and power consumption. Optimizing neural architecture for device-related objectives is immensely crucial for deploying deep networks on portable devices with limited computing resources. We propose DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, optimizing for both device-related (e.g., inference time and memory usage) and device-agnostic (e.g., accuracy and model size) objectives. DPP-Net employs a compact search space inspired by current state-of-the-art mobile CNNs, and further improves search efficiency by adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10 are poised to demonstrate the effectiveness of Pareto-optimal networks found by DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2) NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53. Compared to CondenseNet and NASNet-A (Mobile), DPP-Net achieves better performances: higher accuracy and shorter inference time on various devices. Additional experimental results show that models found by DPP-Net also achieve considerably-good performance on ImageNet as well. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1905.11452v1\" target=\"top\" rel=\"noopener\"><strong>Differentiable Quantization</strong></a>  <a href=\"https://www.google.de/search?q=Differentiable Quantization\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">We propose differentiable quantization (DQ) for efficient deep neural network (DNN) inference where gradient descent is used to learn the quantizer&#8217;s step size, dynamic range and bitwidth. Training with differentiable quantizers brings two main benefits: first, DQ does not introduce hyperparameters; second, we can learn for each layer a different step size, dynamic range and bitwidth. Our experiments show that DNNs with heterogeneous and learned bitwidth yield better performance than DNNs with a homogeneous one. Further, we show that there is one natural DQ parametrization especially well suited for training. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain quantized DNNs with learned quantization parameters achieving state-of-the-art performance. &#8230; </span><BR/></p>\n",
  "wfw:commentRss": "https://analytixon.com/2022/11/03/if-you-did-not-already-know-1876/feed/",
  "slash:comments": 0,
  "post-id": 37445
}