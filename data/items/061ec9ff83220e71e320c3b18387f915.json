{
  "title": "Oracle Inequalities for Model Selection in Offline Reinforcement Learning. (arXiv:2211.02016v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.02016",
  "description": "<p>In offline reinforcement learning (RL), a learner leverages prior logged data\nto learn a good policy without interacting with the environment. A major\nchallenge in applying such methods in practice is the lack of both\ntheoretically principled and practical tools for model selection and\nevaluation. To address this, we study the problem of model selection in offline\nRL with value function approximation. The learner is given a nested sequence of\nmodel classes to minimize squared Bellman error and must select among these to\nachieve a balance between approximation and estimation error of the classes. We\npropose the first model selection algorithm for offline RL that achieves\nminimax rate-optimal oracle inequalities up to logarithmic factors. The\nalgorithm, ModBE, takes as input a collection of candidate model classes and a\ngeneric base offline RL algorithm. By successively eliminating model classes\nusing a novel one-sided generalization test, ModBE returns a policy with regret\nscaling with the complexity of the minimally complete model class. In addition\nto its theoretical guarantees, it is conceptually simple and computationally\nefficient, amounting to solving a series of square loss regression problems and\nthen comparing relative square loss between classes. We conclude with several\nnumerical simulations showing it is capable of reliably selecting a good model\nclass.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jonathan N. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1\">George Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1\">Ofir Nachum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>"
}