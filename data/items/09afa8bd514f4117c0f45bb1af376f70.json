{
  "title": "A Theory of PAC Learnability under Transformation Invariances. (arXiv:2202.07552v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2202.07552",
  "description": "<p>Transformation invariances are present in many real-world problems. For\nexample, image classification is usually invariant to rotation and color\ntransformation: a rotated car in a different color is still identified as a\ncar. Data augmentation, which adds the transformed data into the training set\nand trains a model on the augmented data, is one commonly used technique to\nbuild these invariances into the learning process. However, it is unclear how\ndata augmentation performs theoretically and what the optimal algorithm is in\npresence of transformation invariances. In this paper, we study PAC\nlearnability under transformation invariances in three settings according to\ndifferent levels of realizability: (i) A hypothesis fits the augmented data;\n(ii) A hypothesis fits only the original data and the transformed data lying in\nthe support of the data distribution; (iii) Agnostic case. One interesting\nobservation is that distinguishing between the original data and the\ntransformed data is necessary to achieve optimal accuracy in setting (ii) and\n(iii), which implies that any algorithm not differentiating between the\noriginal and transformed data (including data augmentation) is not optimal.\nFurthermore, this type of algorithms can even \"harm\" the accuracy. In setting\n(i), although it is unnecessary to distinguish between the two data sets, data\naugmentation still does not perform optimally. Due to such a difference, we\npropose two combinatorial measures characterizing the optimal sample complexity\nin setting (i) and (ii)(iii) and provide the optimal algorithms.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Han Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montasser_O/0/1/0/all/0/1\">Omar Montasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1\">Avrim Blum</a>"
}