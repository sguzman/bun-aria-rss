{
  "title": "Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2210.17406",
  "description": "<p>Large language models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this work, we propose a\nframework to evaluate the robustness of linguistic representations using\nprobing tasks. We leverage recent advances in extracting emergent linguistic\nconstructs from LLMs and apply syntax-preserving perturbations to test the\nstability of these constructs in order to better understand the representations\nlearned by LLMs. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures. We provide evidence that\ncontext-free representation (e.g., GloVe) are in some cases competitive with\ncontext-dependent representations from modern LLMs (e.g., BERT), yet equally\nbrittle to syntax-preserving manipulations. Emergent syntactic representations\nin neural networks are brittle, thus our work poses the attention on the risk\nof comparing such structures to those that are object of a long lasting debate\nin linguistics.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatkowska_M/0/1/0/all/0/1\">Marta Kiatkowska</a>"
}