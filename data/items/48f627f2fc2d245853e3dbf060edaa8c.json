{
  "title": "Deep learning for hackers with MXnet (2): Neural art",
  "link": "https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/",
  "comments": "https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/#comments",
  "dc:creator": "phunterlau",
  "pubDate": "Mon, 21 Dec 2015 04:47:01 +0000",
  "category": "Machine Learning",
  "guid": "http://no2147483647.wordpress.com/?p=170",
  "description": "Special thanks to Eric Xie for fixing the MXnet cuDNN problem. MXnet can fully utilize cuDNN for speeding up neural art. Have you clicked &#8220;star&#8221; and &#8220;fork&#8221; on MXnet github repo? If not yet, do it now! https://github.com/dmlc/mxnet Update: want an almost real time Neural Art? Let&#8217;s go to MXNet-GAN model: it is a¬†Generative Adversarial [&#8230;]",
  "content:encoded": "<p>Special thanks to <a href=\"https://www.cs.washington.edu/node/9554\">Eric Xie</a> for fixing the MXnet cuDNN problem. MXnet can fully utilize cuDNN for speeding up neural art.</p>\n<p>Have you clicked &#8220;star&#8221; and &#8220;fork&#8221; on MXnet github repo? If not yet, do it now! <a href=\"https://github.com/dmlc/mxnet\">https://github.com/dmlc/mxnet</a></p>\n<p>Update: want an almost real time Neural Art? Let&#8217;s go to MXNet-GAN model: it is a¬†Generative Adversarial Network pretrained model, please refer to¬†<a href=\"http://dmlc.ml/mxnet/2016/06/20/end-to-end-neural-style.html\">http://dmlc.ml/mxnet/2016/06/20/end-to-end-neural-style.html</a> for details.</p>\n<h2>Neural artÔºöpaint your cat like Van Gogh</h2>\n<p>Neural art is a deep learning algorithm which can learn the style from famous artwork and apply to a new image. For example, given a cat picture and a Van Gogh artwork, we can paint the cat in Van Gogh style, like this (Van Gogh Self-portrait in 1889 <a href=\"https://en.wikipedia.org/wiki/Portraits_of_Vincent_van_Gogh\">wikipedia</a>):</p>\n<p><img data-attachment-id=\"174\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/pogo-vangogh-mxnet_600px/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png\" data-orig-size=\"600,260\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"pogo-vangogh-mxnet_600px\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png?w=600\" class=\"alignnone size-full wp-image-174\" src=\"https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png?w=1008\" alt=\"pogo-vangogh-mxnet_600px\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png 600w, https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/12/pogo-vangogh-mxnet_600px.png?w=300 300w\" sizes=\"(max-width: 600px) 100vw, 600px\"   /></p>\n<p>Neural art comes from this paper ‚ÄúA Neural Algorithm of Artistic Style‚Äù by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge <a href=\"http://arxiv.org/abs/1508.06576\">http://arxiv.org/abs/1508.06576</a>. The basic idea is leveraging the power of Convolution Network (CNN) which can learn high level abstract features from the artwork and simulating the art style for generating a new image. These generative network is a popular research topic in deep learning. For example, you might know Google&#8217;s <a href=\"http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html\">Inception</a> which generates a goat-shape cloud image by given a goat image and a cloud image. Facebook also has their similar <a href=\"https://research.facebook.com/publications/980236685341417/deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks/\">Deep Generative Image Models</a>¬†,¬†inspired by this paper <a href=\"http://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a> where one of the authors, Bing Xu, is also the main author of the great <a href=\"https://github.com/dmlc/mxnet\">MXnet</a>.</p>\n<p>Neural art algorithm has many implementations and applications, for example these two Lua/Torch7 ones <a href=\"https://github.com/kaishengtai/neuralart\">LINK1</a> <a href=\"https://github.com/jcjohnson/neural-style\">LINK2</a>. The paper&#8217;s <a href=\"http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style\">gitxiv</a> also includes many interesting applications, for example, generating a neural art gif animation. All of these implemtations uses the VGG model of image classification as mentioned in the paper. With <a href=\"https://github.com/dmlc/mxnet/issues/664\">popular demand on github</a>, MXnet has just published its fast and memory efficient implementation. Let&#8217;s have fun with MXnet!</p>\n<p>Have you clicked the &#8220;star&#8221; and &#8220;fork&#8221; on MXnet github repo? If not yet, do it now! <a href=\"https://github.com/dmlc/mxnet\">https://github.com/dmlc/mxnet</a></p>\n<h3>MXnet&#8217;s Neural art example</h3>\n<p>The neural art example is under <code>mxnet/example/neural-style/</code>. Since this example needs much computing work, GPU is highly recommended, please refer to my previous blog <a href=\"https://no2147483647.wordpress.com/2015/12/07/deep-learning-for-hackers-with-mxnet-1/\">&#8220;Deep learning for hackers with MXnet (1) GPU installation and MNIST&#8221;</a> for detailed installation guide. For sure, one can use CPU anyway since mxnet supports seamless CPU/GPU switch, just a reminder, it may take about 40-50 minutes for generating an image with CPU.</p>\n<p><strong>Optional</strong>: MXnet can speed up with cuDNN! cuDNN v3 and v4 both work with mxnet, where v4 is¬†2-3 seconds faster than v3¬†on my GTX 960 4GB. Please go to <a href=\"https://developer.nvidia.com/cudnn\">https://developer.nvidia.com/cudnn</a> and apply for nVidia Developer program. If approved, one can install CuDNN with CUDA as simple as this (Reference: <a href=\"https://github.com/BVLC/caffe/wiki/Install-Caffe-on-EC2-from-scratch-(Ubuntu,-CUDA-7,-cuDNN\">Install Caffe on EC2 from scratch (Ubuntu, CUDA 7, cuDNN)</a>)) :</p>\n<pre><code>tar -zxf cudnn-7.0-linux-x64-v3.0-prod.tgz\ncd cuda\nsudo cp lib64/* /usr/local/cuda/lib64/\nsudo cp include/cudnn.h /usr/local/cuda/include/\n</code></pre>\n<p>And please turn on <code>USE_CUDNN = 0</code> in <code>make/config.mk</code> and re-compile MXnet for CuDNN support, if not previously compiled. Please also update the python installation if necessary.</p>\n<p><strong>For readers who don&#8217;t have an GPU-ready MXnet</strong>, the market has these free or paid services and apps for trying Neural Art. Since neural art needs a lot of computing, all these paid or free services need to upload the images to servers, and wait for a long time for finishing processing, usually from hours (if lucky) to weeks:</p>\n<ol>\n<li>Deepart <a href=\"https://deepart.io/\">https://deepart.io/</a> Free submission and the average waiting time is a week <img src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png\" alt=\"üò¶\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /> if one wants a faster processing, one can consider donation and speed up to&#8230;. 24 hours wait.</li>\n<li>Pikazo App: <a href=\"http://www.pikazoapp.com/\">http://www.pikazoapp.com/</a> It is a paid app (2.99$) as deepart.io, and you still need to wait in line for a long time.</li>\n<li>AI Painter: <a href=\"https://www.instapainting.com/ai-painter\">https://www.instapainting.com/ai-painter</a> It is a service from instapainting, free of charge, long waiting time too.</li>\n<li>DeepForger: <a href=\"https://twitter.com/DeepForger\">https://twitter.com/DeepForger</a>¬†(Thanks to¬†<a href=\"https://www.reddit.com/user/alexjc\">alexjc</a>¬†from reddit) a twitter account that people can submit images and get Neural Art results within hours. &#8220;It&#8217;s a new algorithm based on StyleNet that does context-sensitive style, and the implementation scales to HD.&#8221;</li>\n</ol>\n<p><strong>For my dear readers who are lucky to have a GPU-ready MXnet</strong>, let do it with MXnet! I am going to use an image from my sister&#8217;s cat &#8220;pogo&#8221; and show every single detail of generating an art image, from end to end.</p>\n<h2>Steps and parameters</h2>\n<p>MXnet needs <a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\">a VGG model</a>.¬†We need to download it for¬†the first time running using <code>download.sh</code>. MXnet version of this VGG model takes about several MB where the Lua version of the same model costs about 1 GB. After having the model ready, let&#8217;s put the style image and the content image into the <code>input</code> folder. For example, I give mxnet the cat image as content, and Van Gogh&#8217;s painting as style:</p>\n<pre><code>python run.py --content-image input/pogo.jpg --style-image input/vangogh.jpg\n</code></pre>\n<p>After 1-2 minutes, we can see the output in the <code>output</code> folder like this:</p>\n<p><img data-attachment-id=\"177\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/pogo-gogh-512/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg\" data-orig-size=\"384,512\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"pogo-gogh-512\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg?w=225\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg?w=384\" class=\"alignnone size-full wp-image-177\" src=\"https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg?w=1008\" alt=\"pogo-gogh-512.jpg\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg 384w, https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg?w=113 113w, https://no2147483647.files.wordpress.com/2015/12/pogo-gogh-512.jpg?w=225 225w\" sizes=\"(max-width: 384px) 100vw, 384px\"   /></p>\n<p>Let&#8217;s try painting the cat &#8220;Pogo&#8221; in a modern art style. By replacing Van Gogh with &#8216;Blue Horse&#8217; Modern Equine Art Contemporary Horse Daily Oil Painting by Texas Artist Laurie Pace (<a href=\"https://www.pinterest.com/pin/407223991276827181/\">https://www.pinterest.com/pin/407223991276827181/</a>) , pogo is painted like this:</p>\n<pre><code>python run.py --content-image input/pogo.jpg --style-image input/blue_horse.jpg\n</code></pre>\n<p><img data-attachment-id=\"179\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/pogo-horse-512/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg\" data-orig-size=\"384,512\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"pogo-horse-512\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg?w=225\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg?w=384\" class=\"alignnone size-full wp-image-179\" src=\"https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg?w=1008\" alt=\"pogo-horse-512\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg 384w, https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg?w=113 113w, https://no2147483647.files.wordpress.com/2015/12/pogo-horse-512.jpg?w=225 225w\" sizes=\"(max-width: 384px) 100vw, 384px\"   /></p>\n<p>Isn&#8217;t it cool?</p>\n<p>In the python script <code>run.py</code>, there are some fine tune parameters for better results, and each of them is explained as following:</p>\n<ul>\n<li><code>--model</code> The model name. In this example, we only have VGG model from image classification, so please let it as it is. In future, MXnet may provide multiple other models, like Google Inception, since they share the same framework.</li>\n<li><code>--content-image</code> Path to the content image, a.k.a the cat image.</li>\n<li><code>--style-image</code> Path to the style image, a.k.a the Van Gogh image.</li>\n<li><code>--stop-eps</code> The model use <code>eps</code> for evaluating the difference between the content and the style. One can see the <code>eps</code> value converging during the training. The less <code>eps</code>, the more similar style. <code>stop-eps</code> is the threshold for stopping the training. Usually a smaller <code>stop-eps</code> gives stronger style, but it needs longer training time. The default 0.005 value is good, and one can change to 0.004 for better results.</li>\n<li><code>--content-weight</code> <code>--style-weight</code> The weight of content and style. By default, it is 10:1. If one thinks the style is too strong, for example, the painting feels strange and harsh, please reduce it to 20:1 or 30:1.</li>\n<li><code>--max-num-epochs</code> The max number of epochs, by default it is 1000 epochs. Usually MXnet can converge to a good <code>eps</code> value around 200 epochs, and we can leave this parameter alone.</li>\n<li><code>--max-long-edge</code> The max length of the longer edge. MXnet adjust the content image to this size and keeps the aspect ratio. The runtime is almost proportional to the number of pixels (aread) of the image, because the convolution network input size is defined by the number of pixels, and each convolution is on each image block. In short, 700px image may double the memory cost and runtime to that in 500px image. In the following benchmark, one can see that, a 512 px image needs about 1.4GB memory, which is good for a 2014 Macbook Pro or other 2GB CUDA devices; a 850-900 px image is good for 4GB memory CUDA card; if one wants a 1080p HD image, one may need to get a 12GB memory Titan X. Meanwhile, the computing time is related to the number of CUDA cores: the more cores, the faster. I think my dear readers now understand why these free Neural Art services/apps needs hours to weeks of waiting time.</li>\n<li><code>--lr</code> logistic regression learning ratio <code>eta</code> for SGD. Mxnet uses SGD for finding a image which has similar content to cat and similar style to Van Gogh. As in other machine learning projects, larger <code>eta</code> converges faster, but it jumps around the minimum. The default value is 0.1, and 0.2 or 0.3 work too.</li>\n<li><code>--gpu</code> GPU ID. By default it is 0 for using the first GPU. For people who have multiple GPUs, please specify which ones would be used.<code>--gpu -1</code> means using CPU-only mxnet, which takes 40-50 minutes per image.</li>\n<li><code>--output</code> Filename and path for the output.</li>\n<li><code>--save-epochs</code> If save the tempory results. By default, it saves output for each 50 epochs.</li>\n<li><code>-remove-noise</code> Gaussian radius for reducing image noise. Neural art starts with white noise images for converging to the neural art from content + style, so it artificially introduces some unnecessary noise. Mxnet can simply smooth this noise. The default value is 0.2, and one can change it to 0.15 for less blur.</li>\n</ul>\n<h2>Troubleshooting</h2>\n<h3>Out of memory</h3>\n<p>Since the runtime memory cost is proportional to the size of the image. If <code>--max-long-edge</code> was set too large, MXnet may give this <code>out of memory</code> error:</p>\n<pre><code>terminate called after throwing an instance of 'dmlc::Error' what():  [18:23:33] src/engine/./threaded_engine.h:295: [18:23:33] src/storage/./gpu_device_storage.h:39: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: out of memory\n</code></pre>\n<p>To solve it, one needs to have smaller <code>--max-long-edge</code>: for 512px image, MXnet needs 1.4GB memory; for 850px image, MXnets needs 3.7GB. Please notices these two items:</p>\n<ol>\n<li>GTX 970 memory issue: GTX 970 can only support up to 3.5GB memory, otherwise it goes crazy. It is an known problem from nVidia, please refer to <a href=\"http://wccftech.com/nvidia-geforce-gtx-970-memory-issue-fully-explained/\">this link</a> for more details.</li>\n<li>The system GUI costs some memory too. In ubuntu, one can press <code>ctrl+alt+f1</code> for shutting down the system graphic interface, and save some about 40MB memory.</li>\n</ol>\n<h3>Out of workspace</h3>\n<p>If the image size is larger than 600 to 700 pixels, the default workspace parameter in <code>model_vgg19.py</code> may not be enough, and MXnet may give this error:</p>\n<pre><code>terminate called after throwing an instance of 'dmlc::Error' what():  [18:22:39] src/engine/./threaded_engine.h:295: [18:22:39] src/operator/./convolution-inl.h:256: Check failed: (param_.workspace) >= (required_size)\nMinimum workspace size: 1386112000 Bytes\nGiven: 1073741824 Bytes\n</code></pre>\n<p>The reason is MXnet needs a buffer space which is defined in <code>model_vgg19.py</code> as <code>workspace</code> for each CNN layer. Please replace all <code>workspace=1024</code> in <code>model_vgg19.py</code> with <code>workspace=2048</code>.</p>\n<h2>Benchmark</h2>\n<p>In this benchmark, we choose this Lua (Torch 7) implementation <a href=\"https://github.com/jcjohnson/neural-style\">https://github.com/jcjohnson/neural-style</a> and compare it with MXnet for learning Van Gogh style and painting pogo the cat. The hardware include a single GTX 960 4GB, a 4-core AMD CPU and 16GB memory.</p>\n<h3>512px</h3>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Memory</th>\n<th>Runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MXnet (w/o cuDNN)</td>\n<td><strong>1440MB</strong></td>\n<td><strong>117s</strong></td>\n</tr>\n<tr>\n<td>MXnet (w/ cuDNN)</td>\n<td><strong>1209MB</strong></td>\n<td><strong>89s</strong></td>\n</tr>\n<tr>\n<td>Lua Torch 7</td>\n<td>2809MB</td>\n<td>225s</td>\n</tr>\n</tbody>\n</table>\n<p>MXnet has efficient memory usage, and it costs only half of the memory as that in the Lua/Torch7 version.<br />\nÔøº<img data-attachment-id=\"181\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/mxnet-lua-512/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png\" data-orig-size=\"734,600\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"mxnet-lua-512\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png?w=734\" class=\"alignnone size-full wp-image-181\" src=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png?w=1008\" alt=\"mxnet-lua-512.png\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png 734w, https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/12/mxnet-lua-512.png?w=300 300w\" sizes=\"(max-width: 734px) 100vw, 734px\"   /></p>\n<h3>850px</h3>\n<p>Lua/Torch 7 is not able to run with 850px image because of no enough memory, while MXnet costs 3.7GB memory and finishes in 350 seconds.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Memory</th>\n<th>Runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MXnet (w/o cuDNN)</td>\n<td>3670MB</td>\n<td>350s</td>\n</tr>\n<tr>\n<td>MXnet (w/ cuDNN)</td>\n<td>2986MB</td>\n<td>320s</td>\n</tr>\n<tr>\n<td>Lua Torch 7</td>\n<td>Out of memory</td>\n<td>Out of memory</td>\n</tr>\n</tbody>\n</table>\n<p><img data-attachment-id=\"183\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/mxnet-850/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png\" data-orig-size=\"640,358\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"mxnet-850\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png?w=640\" class=\"alignnone size-full wp-image-183\" src=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png?w=1008\" alt=\"mxnet-850\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png 640w, https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/12/mxnet-850.png?w=300 300w\" sizes=\"(max-width: 640px) 100vw, 640px\"   /></p>\n<h3>MXnet magic to squeeze memory (12.21.2015 update)</h3>\n<p>With some <a href=\"https://www.reddit.com/r/MachineLearning/comments/3xqfwf/deep_learning_for_hackers_with_mxnet_2_neural_art/\">invaluable discussion</a> from reddit, and special thanks to¬†<a class=\"author may-blank id-t2_1cc8g\" href=\"https://www.reddit.com/user/alexjc\">alexjc</a>¬†(the author of DeepForger) and¬†<a class=\"author may-blank id-t2_khtfc\" href=\"https://www.reddit.com/user/jcjohnss\">jcjohnss</a>¬†(the author of Lua Neural-artstyle), I have this updated benchmark with MXnet&#8217;s new magic¬†MXNET_BACKWARD_DO_MIRROR to squeeze memory (<a href=\"https://github.com/dmlc/mxnet/pull/884\">github issue</a>). Please update to the latest MXnet github and re-compile. To add this magic, one can simply do:</p>\n<pre><code>MXNET_BACKWARD_DO_MIRROR=1 python run.py --content-image input/pogo.jpg --style-image input/vangogh.jpg\n</code></pre>\n<h4>512px</h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Memory</th>\n<th>Runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MXnet (w/o cuDNN)</td>\n<td>1440MB</td>\n<td>117s</td>\n</tr>\n<tr>\n<td>MXnet (w/ cuDNN)</td>\n<td>1209MB</td>\n<td><strong>89s</strong></td>\n</tr>\n<tr>\n<td>MXnet (w/o cuDNN + Mirror)</td>\n<td><strong>1116MB</strong></td>\n<td>92s</td>\n</tr>\n</tbody>\n</table>\n<h4>850px</h4>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Memory</th>\n<th>Runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MXnet (w/o cuDNN)</td>\n<td>3670MB</td>\n<td>350s</td>\n</tr>\n<tr>\n<td>MXnet (w/ cuDNN)</td>\n<td>2986MB</td>\n<td><strong>320s</strong></td>\n</tr>\n<tr>\n<td>MXnet (w/ cuDNN+Mirror)</td>\n<td><strong>2727MB</strong></td>\n<td>332s</td>\n</tr>\n</tbody>\n</table>\n<p>The mirror magic slows down a little bit and gains memory saving. With this Mirror magic, a 4GB GPU can process up to 1024px image with 3855MB memory!</p>\n<p><strong>Some comments about improving the memory efficiency</strong>: currently in the market, the¬†Lasagne version (with Theano) is the most memory efficient Neural Art generator (<a href=\"https://github.com/Lasagne/Recipes/tree/master/examples/styletransfer\">github link</a>, thanks to¬†<a class=\"author may-blank id-t2_1cc8g\" href=\"https://www.reddit.com/user/alexjc\">alexjc</a>) which can process 1440px images with a 4GB GPU.¬†<a class=\"author may-blank id-t2_6duif\" href=\"https://www.reddit.com/user/antinucleon\">antinucleon</a>, the author of MXnet, <a href=\"https://www.reddit.com/r/MachineLearning/comments/3xqfwf/deep_learning_for_hackers_with_mxnet_2_neural_art/cy7n3nf\">has mentioned</a> that, gram matrix uses¬†imperative mode while¬†symbolic mode should save more memory¬†by reusing it. I will update the benchmark when the symbolic version is available.</p>\n<p>In short, MXnet can save more memory than that in the Lua version, and has some speed up with CuDNN.¬†Considering the price difference between a Titan X (1000$) and a GTX 960 4GB (220$), MXnet is also eco-friendly.</p>\n<p><strong>A note about the speed comparision</strong>: Lua version uses <code>L-BFGS</code> for the optimal parameter search while MXnet uses <code>SGD</code>, which is faster but needs a little bit tune-ups for best results. To be honest, the comparision above doesn&#8217;t mean MXnet is always 2x faster.<br />\nÔøº<br />\nFor readers who want to know MXnet&#8217;s secret of efficient memory usage, please refer to MXnet&#8217;s design document where all dark magic happens. The link is <a href=\"http://mxnt.ml/en/latest/#open-source-design-notes\">http://mxnt.ml/en/latest/#open-source-design-notes</a></p>\n<p>Till now, my dear readers can play with Neural art in MXnet. Please share your creative artwork on twitter or instagram with <code>#mxnet</code> and I will check out your great art!</p>\n<h2>How machine learns the artwork style?</h2>\n<h3>Quantize the &#8220;style&#8221;</h3>\n<p>&#8220;Style&#8221; itself doesn&#8217;t have a clear definition, it might be &#8220;pattern&#8221; or &#8220;texture&#8221; or &#8220;method of painting&#8221; or something else. People believe it can be described by some higher order statistical variables. However, different art styles have different representations, and for a general approach of &#8220;learning the style&#8221;, it becomes very difficulty to extract these higher order variables and apply to some new images.</p>\n<p>Fortunately, Convolution Network (CNN) has proved its power of extracting high-level abstract features in the image classification, for example, computers can tell if a cat is in the image by using CNN. For more details, please refer to <a href=\"http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf\">Yann Lecun&#8217;s deep learning tutorial</a>. The power of &#8220;extracting high-level abstract features&#8221; is used in Neural Art: after couple of layers of convolution operations, the image has lost its pixel-level feature, and only keeps its high-level style. In the following figure from the paper, the author has defined a 5-layer CNN, where the staring night by Van Gogh keeps some content details in the 1st, 2nd and 3rd layer, and becomes &#8220;something looks like staring night&#8221; in the 4th and 5th layer:</p>\n<p><img data-attachment-id=\"186\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/paper-fig1/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png\" data-orig-size=\"640,467\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"paper-fig1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png?w=640\" class=\"alignnone size-full wp-image-186\" src=\"https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png?w=1008\" alt=\"paper-fig1.png\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png 640w, https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/12/paper-fig1.png?w=300 300w\" sizes=\"(max-width: 640px) 100vw, 640px\"   /><br />\nÔøº</p>\n<p>And the author has reached the &#8220;Aha!&#8221; moment: if we put a Van Gogh image and one more other image to the same CNN network, some clever adjustment may make the second image closer to Van Gogh, but keeps some content in the first 3 layers. It is the way to simulate Van Gogh painting style! Moreover, there is a VGG model for image classification in the market for it!</p>\n<h3>Learn style and generate a new image</h3>\n<p>Now the problem becomes an optimization problem: I want the generated picture looks like my cat (the content feature should be kept for the first 3 layers), and I want Van Gogh style (the style feature for the 4th and 5th layer), thus the solution is an intermediate result which has a similar content representation to the cat, and a similar style representation to Van Gogh. In the paper, the author uses a white noise image for generating a new image closer to the content using SGD, and the other white nose image for being closer to the style. The author has defined a magical <code>gram matrix</code> for describing the texture and has used this matrix to defind the loss function which is a weighted mixture of these two white noise image. Mxnet uses SGD for converging it into a image which meets both of the content and style requirement.</p>\n<p>For exmaple, in these 200+ steps of painting pogo the cat, the generated image changes like this:</p>\n<p><img data-attachment-id=\"187\" data-permalink=\"https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/steps/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/12/steps.png\" data-orig-size=\"640,210\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"steps\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/12/steps.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/12/steps.png?w=640\" class=\"alignnone size-full wp-image-187\" src=\"https://no2147483647.files.wordpress.com/2015/12/steps.png?w=1008\" alt=\"steps.png\" srcset=\"https://no2147483647.files.wordpress.com/2015/12/steps.png 640w, https://no2147483647.files.wordpress.com/2015/12/steps.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/12/steps.png?w=300 300w\" sizes=\"(max-width: 640px) 100vw, 640px\"   /></p>\n<p>where we can see, in the first 50 epoches, the generated image looks like a simple texture overlap in between the content and the style; with more epoches, the program gradually learns the color, the pattern etc, and becomes stable around 150th epoches, and finally paints pogo the cat in Van Gogh style.</p>\n<h3>Further reading: other methods of simulating art style</h3>\n<p>Neural art is not the only method of simulating artwork style and generating new images. There are many other computer vision and graph research papers, for example:</p>\n<ul>\n<li>&#8220;A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficient&#8221; <a href=\"http://www.cns.nyu.edu/pub/lcv/portilla99-reprint.pdf\">http://www.cns.nyu.edu/pub/lcv/portilla99-reprint.pdf</a> which uses wavelet transforming for the higher order texture feature.</li>\n<li>‚ÄúStyle Transfer for Headshot Portraits‚Äù <a href=\"https://people.csail.mit.edu/yichangshih/portrait_web/\">https://people.csail.mit.edu/yichangshih/portrait_web/</a> This work is specific for headshot portrait, which is a constraint problem and the method is much faster than Neural art.</li>\n</ul>\n<h2>Summary</h2>\n<p>Neural art is a nice demo for convolution network, and people can generate artwork from their own images. Let&#8217;s have fun with MXnet neural art. Please share your creative artwork on twitter or instagram and add hashtag <code>#mxnet</code>.</p>\n<p>A reminder about the style: if the content image is a portrait, please find a portrait artwork for learning the style instead of a landscape one. It is the same with landscape images, always landscape to landscape. Because the landscape artwork uses different paiting techniques and it doesn&#8217;t look good on portrait images.</p>\n<p>In the next blog, I will have detailed introduction to the convolution network for image classification, a.k.a, the dog vs the cat.</p>\n",
  "wfw:commentRss": "https://no2147483647.wordpress.com/2015/12/21/deep-learning-for-hackers-with-mxnet-2/feed/",
  "slash:comments": 14,
  "media:content": [
    {
      "media:title": "phunterlau"
    },
    {
      "media:title": "pogo-vangogh-mxnet_600px"
    },
    {
      "media:title": "pogo-gogh-512.jpg"
    },
    {
      "media:title": "pogo-horse-512"
    },
    {
      "media:title": "mxnet-lua-512.png"
    },
    {
      "media:title": "mxnet-850"
    },
    {
      "media:title": "paper-fig1.png"
    },
    {
      "media:title": "steps.png"
    }
  ]
}