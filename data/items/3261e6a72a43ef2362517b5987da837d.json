{
  "title": "A Posterior Sampling Framework for Interactive Decision Making. (arXiv:2211.01962v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01962",
  "description": "<p>We study sample efficient reinforcement learning (RL) under the general\nframework of interactive decision making, which includes Markov decision\nprocess (MDP), partially observable Markov decision process (POMDP), and\npredictive state representation (PSR) as special cases. Toward finding the\nminimum assumption that empowers sample efficient learning, we propose a novel\ncomplexity measure, generalized eluder coefficient (GEC), which characterizes\nthe fundamental tradeoff between exploration and exploitation in online\ninteractive decision making. In specific, GEC captures the hardness of\nexploration by comparing the error of predicting the performance of the updated\npolicy with the in-sample training error evaluated on the historical data. We\nshow that RL problems with low GEC form a remarkably rich class, which subsumes\nlow Bellman eluder dimension problems, bilinear class, low witness rank\nproblems, PO-bilinear class, and generalized regular PSR, where generalized\nregular PSR, a new tractable PSR class identified by us, includes nearly all\nknown tractable POMDPs. Furthermore, in terms of algorithm design, we propose a\ngeneric posterior sampling algorithm, which can be implemented in both\nmodel-free and model-based fashion, under both fully observable and partially\nobservable settings. The proposed algorithm modifies the standard posterior\nsampling algorithm in two aspects: (i) we use an optimistic prior distribution\nthat biases towards hypotheses with higher values and (ii) a loglikelihood\nfunction is set to be the empirical loss evaluated on the historical data,\nwhere the choice of loss function supports both model-free and model-based\nlearning. We prove that the proposed algorithm is sample efficient by\nestablishing a sublinear regret upper bound in terms of GEC. In summary, we\nprovide a new and unified understanding of both fully observable and partially\nobservable RL.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Han Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sirui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"
}