{
  "title": "The secular Bayesian: Using belief distributions without really believing",
  "description": "<h2 id=\"the-religious-bayesian\">The religious Bayesian</h2><p>My parents didn't raise me in a religious tradition. It all started to change when a great scientist took me under his wing and taught me the teachings of Bayes. I travelled the world and spent 4 years in a Bayesian monastery in Cambridge, UK.</p>",
  "link": "https://www.inference.vc/the-secular-bayesian-using-belief-distributions-without-really-believing/",
  "guid": "5dbac64b6c822a003820974f",
  "dc:creator": "Ferenc Huszar",
  "pubDate": "Thu, 31 Oct 2019 15:13:21 GMT",
  "media:content": "",
  "content:encoded": "<h2 id=\"the-religious-bayesian\">The religious Bayesian</h2><img src=\"https://www.inference.vc/content/images/2019/10/image-url-1.jpg\" alt=\"The secular Bayesian: Using belief distributions without really believing\"><p>My parents didn't raise me in a religious tradition. It all started to change when a great scientist took me under his wing and taught me the teachings of Bayes. I travelled the world and spent 4 years in a Bayesian monastery in Cambridge, UK. This particular place practiced the nonparametric Bayesian doctrine.</p><p>We were religious Bayesians. We looked at the world and all we saw the face of Bayes: if something worked, it did because it had a Bayesian interpretation. If an algorithm did not work, we shunned its creator for being unfaithful to Bayes. &#xA0;We scorned at point estimates, despised p-values. Bayes had the answer to everything. But above all, we believed in our models.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/10/Bayes_toast.jpg\" class=\"kg-image\" alt=\"The secular Bayesian: Using belief distributions without really believing\" loading=\"lazy\"></figure><h2 id=\"possessed-by-deamons\">Possessed by deamons</h2><p>At a convention dominated by Bayesian thinkers I was approached by a frequentist, let's call him Lucifer (in fact his real name is <a href=\"https://www.szit.bme.hu/~gyorfi/indexen.html\">Laci</a> so not that far off). \"Do you believe your data exists?\" - he asked. \"Yes\" I answered. \"Do you believe your model and its parameters exist?\" \"Well, not really, it's just a model I use to describe reality\" I said. Then he told me the following, poisoning my pure Bayesian heart forever: \"If you use Bayes' rule, you assume that a joint distribution between model parameters and data exist. This, however, only exists if your data and your parameters both exist, in the same $\\sigma$-algebra. You can't have it both ways. You have to think your model really exists somewhere.\" </p><p>I never forgot this encounter, but equally I didn't think much about it since then. Over the years, I started to doubt more and more aspects of my Bayesian faith. I realised the likelihood was important, but not the only thing that exists. There were scoring rules, loss functions which couldn't be written as a log-likelihood. I noticed nonparametric Bayesian models weren't automatically more useful than large parametric ones. &#xA0;I worked on weird stuff like <a href=\"http://proceedings.mlr.press/v15/lacoste_julien11a.html\">loss-calibrated Bayes</a>. I started having thoughts about model misspecification, kind of a taboo topic in the Bayesian church.</p><h2 id=\"the-secular-bayesian\">The secular Bayesian</h2><p>Over the years I came to terms with my Bayesian heritage, and I now live my life as a secular Bayesian. Certain elements of the Bayesian way are no doubt useful: Engineering inductuve biases explicitly into a prior distribution, using probabilities, divergences, information, variational bounds as tools for developing new algorithms. Posterior distributions can capture model uncertainty which can be exploited for active learning or exploration in interactive learning. Bayesian methods often - though not always - lead to increased robustness, better calibration, and so much more. At the same time, I can carry on living my life, use gradient descent to find local minima, use bootstrap to capture uncertainty. And first and foremost, I do not have to believe that my models really exist or perfectly describe reality anymore. I am free to think about model misspecification.</p><p>Lately, I have started to familiarize myself with a new body of work, which I call secular Bayesianism, that combines Bayesian inference with more frequentists ideas about learning from observation. In this body of work, people study model misspecification (see e.g. M-open Bayesian inference). And, I found a resolution to the \"you have to believe in your model, you can't have it both ways\" problem that bothered me all these years.</p><h2 id=\"a-generalized-framework-for-updating-belief-distributions\">A generalized framework for updating belief distributions</h2><p>After this rather long intro, let me present the paper this post is really about and which, as a secular Bayesian, I found very interesting:</p><ul><li>P.G. Bissiri, C.C. Holmes and S.G. Walker (2016) <a href=\"https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12158\">A General Framework for Updating Belief Distributions</a></li></ul><p>This paper basically asks: can we take the belief out of belief distributions? Let's say we want to estimate some parameter of interest $\\theta$ from data. Does it still make sense to specify a prior distribution over this parameter, and then update them in light of data using some kind of Bayes rule-like update mechanism to form posterior distributions, all without assuming that the parameter of interest $\\theta$ and the observations $x_i$ are linked to one another via a probabilistic model? And if it is meaningful, what form would that update rule take.</p><h3 id=\"the-setup\">The setup</h3><p>First of all, for simplicity, let's assume that data $x_i$ is sampled i.i.d from some distribution $P$. That's right, not exchangeable, actually i.i.d. like in frequentist settings. Let's also assume that we have some parameter of interest $\\theta$. Unlike in Bayesian analysis where $\\theta$ usually parametrises some kind of generative model for data $x_i$, we don't assume anything like that. All we assume is that there is a loss function $\\ell$ which connects the parameter to the observations: $\\ell(\\theta, x)$ measures how well the estimate $\\theta$ agrees with observation $x$.</p><p>Let's say that a priori, without seeing any datapoints we have a prior distribution $\\pi$ over $\\theta$. Now we observe a datapoint $x_1$. How should we make use of our observation $x_1$, the loss function $\\ell$ and the prior $\\pi$ to come up with some kind of posterior over this parameter? Let's denote this update rule $\\psi(\\ell(\\cdot, x_1), \\pi)$. There are many ways we could do this, but is there one which is better than the rest?</p><h3 id=\"desiderata\">Desiderata</h3><p>The paper lists a number of desiderata - desired properties the update rule $\\psi$ should satisfy. These are all meaningful assumptions to have. The main one is coherence, which is a property somewhat analogous to exchangeability: if we observe a sequence of observations, we would like the resulting posterior to be the same, irrespective of which order the observations are presented. The coherence property can be written as follows</p><p>$$<br>\\psi\\left(\\ell(\\cdot, x_2), \\psi\\left(\\ell(\\cdot, x_1), \\pi\\right)\\right) = \\psi\\left(\\ell(\\cdot, x_1), \\psi\\left(\\ell(\\cdot, x_2), \\pi \\right)\\right)<br>$$</p><p>As a desired property, this makes a lot of sense, and Bayes rule obviously satisfies it. However, this is not really how the authors actually define coherence. In Equation (3) they use a more restrictive definition of coherence, further restricting the set of acceptable update rules as follows:</p><p>$$<br>\\psi\\left(\\ell(\\cdot, x_2), \\psi\\left(\\ell(\\cdot, x_1), \\pi\\right)\\right) = \\psi\\left(\\ell(\\cdot, x_1) + \\ell(\\cdot, x_2), \\pi \\right)<br>$$</p><p>By combining losses from the two observations in an additive way, one can indeed ensure permuation invariance. However, the sum is not the only way to do this. Any pooling operation over observations would also have satisfied this. For example, one could replace the $\\ell(\\cdot, x_1) + \\ell(\\cdot, x_2)$ bit by $\\max(\\ell(\\cdot, x_1), \\ell(\\cdot, x_2))$ and still satisfy the general principle of coherence. The most general class of permutation invariant functions which would satisfy the general coherence desideratum are discussed in <a href=\"https://www.inference.vc/deepsets-modeling-permutation-invariance/\">DeepSets</a>. Overall, my hunch is that going with the sum is a design choice, rather than a general desideratum. This choice is the real reason why the resulting update rule will end up very Bayes-rule like, as we will see later.</p><p>The other desiderata the paper proposes are actually discussed separately in Section 1.2 of <a href=\"https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12158\">(Brissini et al, 2016)</a>, and called assumptions instead. These are much more basic requirements for the update function. Assumption 2 for example talks about how restricting the prior to a subset should result in a posterior which is also the restricted version of the original posterior. Assumption 3 requires that lower evidence (larger loss) for a parameter should yield smaller posterior probabilities - a monotonicity property.</p><h3 id=\"uniqueness-of-coherent-update-rule\">Uniqueness of coherent update rule</h3><p>One contribution of the paper is showing that all the desiderata mentioned above pinpoint a specific update rule $\\psi$ which satisfies all the desired properties. This update takes the following form:</p><p>$$<br>\\pi(\\theta\\vert x_{1:N}) = \\psi(\\ell(\\cdot, x), \\pi) \\alpha \\exp\\{-\\sum_{n=1}^N\\ell(\\theta, x_N)\\}\\pi(\\theta)<br>$$</p><p>Just like Bayes rule we have a normalized product of the prior with something that takes the role of the likelihood term. If the loss is the logarithmic loss of a probabilistic model, we recover the Bayes rule, but this update rule makes sense for arbitrary loss functions.</p><p>Again, this solution is unique under the very strong and specific desideratum that we'd like the losses from i.i.d. observations combine in an additive way, and I presume that, had we chosen a different permutation invariant function, we would end up with a similar generalization of Bayes rule with that permutation invariant function appearing in the exponent.</p><h3 id=\"rationality\">Rationality</h3><p>Now that we have an update rule which satisfies our desiderata, can we say if it's actually a good or useful update rule? It seems it is, in the following sense.</p><p>Let's think about a way to measure the usefulness of a posterior $\\nu$. Suppose we have data sampling distribution $P$, losses are still measured by $\\ell$, and our prior is $\\pi$. A good posterior does two things well: it allows us to make good decisions in some kind of downstream test scenario, and it is informed by our prior. It therefore makes sense to define a loss function over the posterior $\\nu$ as a sum of two terms:</p><p>$$<br>L(\\nu; \\ell, \\pi, P) = h_1(\\nu; \\ell, P) + h_2(\\nu; \\pi)<br>$$</p><p>The first term, $h_1$ measures the posterior's usefulness at test time, and $h_2$ measures how well it's influenced by the prior. The authors define $h_1$ to be as follows:</p><p>$h_1(\\nu; \\ell, P) = \\mathbb{E}_{x\\sim P} \\mathbb{E}_\\theta\\sim\\nu \\ell(x, \\theta)$</p><p>So basically, we will sample from the posterior, and then evaluate the random sample parameter $\\theta$ on a randomly chosen test datapoint $x$ using our loss $\\ell$. I would say this is a rather narrow view on what it means for a posterior to do well on a downstream task, more about it later in the criticism section. In any case it's one possible goal for a posterior to try to achieve.</p><p>Now we turn to choosing $h_2$, and the authors note something very interesting. If we want the resulting optimal posterior to possess the coherence property (as defined in their Eqn. (3)), it turns out the only choice for $h_2$ is the KL divergence between the prior and posterior. Any other choice would lead to incoherent updates. This, I believe is only true for the additive definition of coherence, not the more general definition I gave above.</p><p>Putting $h_1$ and $h_2$ together it turns out that the posterior that minimizes this loss function is precisely of the form $\\pi(\\theta\\vert x_{1:N}) \\alpha \\exp\\{-\\sum_{n=1}^N \\ell(\\theta, x_n)\\}$. So, not only is this update rule the only update rule that satisfies the desired properties, it is also optimal under this particular definition of optimality/rationality.</p><h2 id=\"why-is-this-significant\">Why is this significant?</h2><p>This work is interesting because it gives a new justification for Bayes rule-like updates to belief distributions, and as a result it also provides a different/new perspective on Bayesian inference. Crucially, never in this derivation did we have to reason about a joint distribution between $\\theta$ and the observations $x$ (or conditionals of one given the other). Even though I wrote $\\pi(\\theta \\vert x_{1:N})$ to denote a posterior, this is really just a shorthand notation, syntactic sugar. This is important. One of the main technical criticisms of the Bayesian terminology is that in order to reason about the joint distribution between two random variables ($x$ and $\\theta$), these variables have to live in the same probability space, so if you believe that your data exists, you have to believe in your model, and model parameters exist as well. This framework sidesteps that.</p><blockquote>It allows rational updates of belief distributions, without forcing you to believe in anything.</blockquote><p>From a practical viewpoint, this work also extends Bayesian inference in a meaningful way. While Bayesian inference only made sense if you inferred the whole set of parameters jointly, here you are allowed to specify any loss function, and really focus on the parameter of importance. For example, if you're only interested in estimating the median of a distribution in a Bayesian way, without assuming it follows a certain distribution, you can now do this by specifying your loss to be $\\vert x-\\theta\\vert$. This is explained a lot more clearly in the paper, so I encourage you to read it.</p><h2 id=\"criticism\">Criticism</h2><p>My main criticism of this work is that it made a number of assumptions that ultimately limited the range of acceptable solutions, and to my cynical eye it appears that these choices were specifically made so that Bayes rule-like update rules came out winning. So rather than really deriving Bayesian updates from first principles, we engineered principles under which Bayesian updates are optimal. In other words, the top-down analysis was rigged in favour of familiar Bayes-like updates. There are two specific assumptions which I would personally like to see relaxed:</p><p>The first one is the restrictive notion of coherence, which requires losses to combine additively from multiple observations. I think this very clearly gives rise to the convenient exponential, log-additive form in the end. It would be interesting to see whether other types of permutation invariant update rules also make sense in practice.</p><p>Secondly, the way the authors defined optimality, in terms of the loss $h_1$ above is very limiting. We rarely use posterior distributions in this way (take a random sample). Instead, we might be intersested integrating over the posterior, and evaluating the loss of that classifier. This is a loss that cannot be written in the bilinear form that is the formula for $h_1$ above. I wonder if. using more elaborate losses for the posterior, perhaps along the lines of general decision problems as in (<a href=\"http://proceedings.mlr.press/v15/lacoste_julien11a.html\">Lacoste-Julien et al, 2011)</a>, could lead to more interesting update rules which don't look at all like Bayes rule but are still rational.</p>"
}