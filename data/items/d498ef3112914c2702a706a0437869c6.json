{
  "title": "Dask for Institutions",
  "link": "",
  "updated": "2016-08-16T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/08/16/dask-for-institutions",
  "content": "<p><img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"20%\" /></p>\n\n<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a></em></p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Institutions use software differently than individuals.  Over the last few\nmonths I’ve had dozens of conversations about using Dask within larger\norganizations like universities, research labs, private companies, and\nnon-profit learning systems.  This post provides a very coarse summary of those\nconversations and extracts common questions.  I’ll then try to answer those\nquestions.</p>\n\n<p><em>Note: some of this post will be necessarily vague at points.  Some companies\nprefer privacy.  All details here are either in public Dask issues or have come\nup with enough institutions (say at least five) that I’m comfortable listing\nthe problem here.</em></p>\n\n<h3 id=\"common-story\">Common story</h3>\n\n<p>Institution X, a university/research lab/company/… has many\nscientists/analysts/modelers who develop models and analyze data with Python,\nthe PyData stack like NumPy/Pandas/SKLearn, and a large amount of custom code.\nThese models/data sometimes grow to be large enough to need a moderately large\namount of parallel computing.</p>\n\n<p>Fortunately, Institution X has an in-house cluster acquired for exactly this\npurpose of accelerating modeling and analysis of large computations and\ndatasets.  Users can submit jobs to the cluster using a job scheduler like\nSGE/LSF/Mesos/Other.</p>\n\n<p>However the cluster is still under-utilized and the users are still asking for\nhelp with parallel computing.  Either users aren’t comfortable using the\nSGE/LSF/Mesos/Other interface, it doesn’t support sufficiently complex/dynamic\nworkloads, or the interaction times aren’t good enough for the interactive use\nthat users appreciate.</p>\n\n<p>There was an internal effort to build a more complex/interactive/Pythonic\nsystem on top of SGE/LSF/Mesos/Other but it’s not particularly mature and\ndefinitely isn’t something that Institution X wants to pursue.  It turned out\nto be a harder problem than expected to design/build/maintain such a system\nin-house.  They’d love to find an open source solution that was well featured\nand maintained by a community.</p>\n\n<p>The Dask.distributed scheduler looks like it’s 90% of the system that\nInstitution X needs.  However there are a few open questions:</p>\n\n<ul>\n  <li>How do we integrate dask.distributed with the SGE/LSF/Mesos/Other job\nscheduler?</li>\n  <li>How can we grow and shrink the cluster dynamically based on use?</li>\n  <li>How do users manage software environments on the workers?</li>\n  <li>How secure is the distributed scheduler?</li>\n  <li>Dask is resilient to worker failure, how about scheduler failure?</li>\n  <li>What happens if <code class=\"language-plaintext highlighter-rouge\">dask-worker</code>s are in two different data centers?  Can we\nscale in an asymmetric way?</li>\n  <li>How do we handle multiple concurrent users and priorities?</li>\n  <li>How does this compare with Spark?</li>\n</ul>\n\n<p>So for the rest of this post I’m going to answer these questions.  As usual,\nfew of answers will be of the form “Yes Dask can solve all of your problems.”\nThese are open questions, not the questions that were easy to answer.  We’ll\nget into what’s possible today and how we might solve these problems in the\nfuture.</p>\n\n<h3 id=\"how-do-we-integrate-daskdistributed-with-sgelsfmesosother\">How do we integrate dask.distributed with SGE/LSF/Mesos/Other?</h3>\n\n<p>It’s not difficult to deploy dask.distributed at scale within an existing\ncluster using a tool like SGE/LSF/Mesos/Other.  In many cases there is already\na researcher within the institution doing this manually by running\n<code class=\"language-plaintext highlighter-rouge\">dask-scheduler</code> on some static node in the cluster and launching <code class=\"language-plaintext highlighter-rouge\">dask-worker</code>\na few hundred times with their job scheduler and a small job script.</p>\n\n<p>The goal now is how to formalize this process for the individual version of\nSGE/LSF/Mesos/Other used within the institution while also developing and\nmaintaining a standard Pythonic interface so that all of these tools can be\nmaintained cheaply by Dask developers into the foreseeable future.  In some\ncases Institution X is happy to pay for the development of a convenient “start\ndask on my job scheduler” tool, but they are less excited about paying to\nmaintain it forever.</p>\n\n<p>We want Python users to be able to say something like the following:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Executor</span><span class=\"p\">,</span> <span class=\"n\">SGECluster</span>\n\n<span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">SGECluster</span><span class=\"p\">(</span><span class=\"n\">nworkers</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">options</span><span class=\"p\">)</span>\n<span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">Executor</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>… and have this same interface be standardized across different job\nschedulers.</p>\n\n<h3 id=\"how-can-we-grow-and-shrink-the-cluster-dynamically-based-on-use\">How can we grow and shrink the cluster dynamically based on use?</h3>\n\n<p>Alternatively, we could have a single dask.distributed deployment running 24/7\nthat scales itself up and down dynamically based on current load.  Again, this\nis entirely possible today if you want to do it manually (you can add and\nremove workers on the fly) but we should add some signals to the scheduler like\nthe following:</p>\n\n<ul>\n  <li>“I’m under duress, please add workers”</li>\n  <li>“I’ve been idling for a while, please reclaim workers”</li>\n</ul>\n\n<p>and connect these signals to a manager that talks to the job scheduler.  This\nremoves an element of control from the users and places it in the hands of a\npolicy that IT can tune to play more nicely with their other services on the\nsame network.</p>\n\n<h3 id=\"how-do-users-manage-software-environments-on-the-workers\">How do users manage software environments on the workers?</h3>\n\n<p>Today Dask assumes that all users and workers share the exact same software\nenvironment.  There are some small tools to send updated <code class=\"language-plaintext highlighter-rouge\">.py</code> and <code class=\"language-plaintext highlighter-rouge\">.egg</code> files\nto the workers but that’s it.</p>\n\n<p>Generally Dask trusts that the full software environment will be handled by\nsomething else.  This might be a network file system (NFS) mount on traditional\ncluster setups, or it might be handled by moving docker or conda environments\naround by some other tool like <a href=\"http://knit.readthedocs.io/en/latest/\">knit</a>\nfor YARN deployments or something more custom.  For example Continuum <a href=\"https://docs.continuum.io/anaconda-cluster/\">sells\nproprietary software</a> that\ndoes this.</p>\n\n<p>Getting the standard software environment setup generally isn’t such a big deal\nfor institutions.  They typically have some system in place to handle this\nalready.  Where things become interesting is when users want to use\ndrastically different environments from the system environment, like using Python\n2 vs Python 3 or installing a bleeding-edge scikit-learn version.  They may\nalso want to change the software environment many times in a single session.</p>\n\n<p>The best solution I can think of here is to pass around fully downloaded conda\nenvironments using the dask.distributed network (it’s good at moving large\nbinary blobs throughout the network) and then teaching the <code class=\"language-plaintext highlighter-rouge\">dask-worker</code>s to\nbootstrap themselves within this environment.  We should be able to tear\neverything down and restart things within a small number of seconds.  This\nrequires some work; first to make relocatable conda binaries (which is usually\nfine but is not always fool-proof due to links) and then to help the\ndask-workers learn to bootstrap themselves.</p>\n\n<p>Somewhat related, Hussain Sultan of Capital One recently contributed a\n<code class=\"language-plaintext highlighter-rouge\">dask-submit</code> command to run scripts on the cluster:\n<a href=\"http://distributed.readthedocs.io/en/latest/submitting-applications.html\">http://distributed.readthedocs.io/en/latest/submitting-applications.html</a></p>\n\n<h3 id=\"how-secure-is-the-distributed-scheduler\">How secure is the distributed scheduler?</h3>\n\n<p>Dask.distributed is incredibly insecure.  It allows anyone with network access\nto the scheduler to execute arbitrary code in an unprotected environment.  Data\nis sent in the clear.  Any malicious actor can both steal your secrets and then\ncripple your cluster.</p>\n\n<p>This is entirely the norm however.  Security is usually handled by other\nservices that manage computational frameworks like Dask.</p>\n\n<p>For example we might rely on Docker to isolate workers from destroying their\nsurrounding environment and rely on network access controls to protect data\naccess.</p>\n\n<p>Because Dask runs on Tornado, a serious networking library and web framework,\nthere are some things we can do easily like enabling SSL, authentication, etc..\nHowever I hesitate to jump into providing “just a little bit of security”\nwithout going all the way for fear of providing a false sense of security.  In\nshort, I have no plans to work on this without a lot of encouragement.  Even\nthen I would strongly recommend that institutions couple Dask with tools\nintended for security.  I believe that is common practice for distributed\ncomputational systems generally.</p>\n\n<h3 id=\"dask-is-resilient-to-worker-failure-how-about-scheduler-failure\">Dask is resilient to worker failure, how about scheduler failure?</h3>\n\n<p>Workers can come and go.  Clients can come and go.  The state in the scheduler\nis currently irreplaceable and no attempt is made to back it up.  There are a\nfew things you could imagine here:</p>\n\n<ol>\n  <li>Backup state and recent events to some persistent storage so that state can\nbe recovered in case of catastrophic loss</li>\n  <li>Have a hot failover node that gets a copy of every action that the\nscheduler takes</li>\n  <li>Have multiple peer schedulers operate simultaneously in a way that they can\npick up slack from lost peers</li>\n  <li>Have clients remember what they have submitted and resubmit when a\nscheduler comes back online</li>\n</ol>\n\n<p>Currently option 4 is currently the most feasible and gets us most of the way\nthere.  However options 2 or 3 would probably be necessary if Dask were to ever\nrun as critical infrastructure in a giant institution.  We’re not there yet.</p>\n\n<p>As of <a href=\"https://github.com/dask/distributed/pull/413\">recent work</a> spurred on by\nStefan van der Walt at UC Berkeley/BIDS the scheduler can now die and come back\nand everyone will reconnect.  The state for computations in flight is entirely\nlost but the computational infrastructure remains intact so that people can\nresubmit jobs without significant loss of service.</p>\n\n<p>Dask has a bit of a harder time with this topic because it offers a persistent\nstateful interface.  This problem is much easier for distributed database\nprojects that run ephemeral queries off of persistent storage, return the\nresults, and then clear out state.</p>\n\n<h3 id=\"what-happens-if-dask-workers-are-in-two-different-data-centers--can-we-scale-in-an-asymmetric-way\">What happens if dask-workers are in two different data centers?  Can we scale in an asymmetric way?</h3>\n\n<p>The short answer is no.  Other than number of cores and available RAM all\nworkers are considered equal to each other (except when the user <a href=\"http://distributed.readthedocs.io/en/latest/locality.html#user-control\">explicitly\nspecifies\notherwise</a>).</p>\n\n<p>However this problem and problems like it have come up a lot lately.  Here are a\nfew examples of similar cases:</p>\n\n<ol>\n  <li>Multiple data centers geographically distributed around the country</li>\n  <li>Multiple racks within a single data center</li>\n  <li>Multiple workers that have GPUs that can move data between each other easily</li>\n  <li>Multiple processes on a single machine</li>\n</ol>\n\n<p>Having some notion of hierarchical worker group membership or inter-worker\npreferred relationships is probably inevitable long term.  As with all\ndistributed scheduling questions the hard part isn’t deciding that this is\nuseful, or even coming up with a sensible design, but rather figuring out how\nto make decisions on the sensible design that are foolproof and operate in\nconstant time.  I don’t personally see a good approach here yet but expect one\nto arise as more high priority use cases come in.</p>\n\n<h3 id=\"how-do-we-handle-multiple-concurrent-users-and-priorities\">How do we handle multiple concurrent users and priorities?</h3>\n\n<p>There are several sub-questions here:</p>\n\n<ul>\n  <li>Can multiple users use Dask on my cluster at the same time?</li>\n</ul>\n\n<p>Yes, either by spinning up separate scheduler/worker sets or by sharing the same\nset.</p>\n\n<ul>\n  <li>If they’re sharing the same workers then won’t they clobber each other’s\ndata?</li>\n</ul>\n\n<p>This is very unlikely.  Dask is careful about naming tasks, so it’s very\nunlikely that the two users will submit conflicting computations that compute to\ndifferent values but occupy the same key in memory.  However if they both submit\ncomputations that overlap somewhat then the scheduler will nicely avoid\nrecomputation.  This can be very nice when you have many people doing slightly\ndifferent computations on the same hardware.  This works in the same way that\nGit works.</p>\n\n<ul>\n  <li>If they’re sharing the same workers then won’t they clobber each other’s\nresources?</li>\n</ul>\n\n<p>Yes, this is definitely possible.  If you’re concerned about this then you\nshould give everyone their own scheduler/workers (which is easy and standard\npractice).  There is not currently much user management built into Dask.</p>\n\n<h3 id=\"how-does-this-compare-with-spark\">How does this compare with Spark?</h3>\n\n<p>At an institutional level Spark seems to primarily target ETL + Database-like\ncomputations.  While Dask modules like Dask.bag and Dask.dataframe can happily\nplay in this space this doesn’t seem to be the focus of recent conversations.</p>\n\n<p>Recent conversations are almost entirely around supporting interactive custom\nparallelism (lots of small tasks with complex dependencies between them) rather\nthan the big Map-&gt;Filter-&gt;Groupby-&gt;Join abstractions you often find in a\ndatabase or Spark.  That’s not to say that these operations aren’t hugely\nimportant; there is a lot of selection bias here.  The people I talk to are\npeople for whom Spark/Databases are clearly not an appropriate fit.  They are\ntackling problems that are way more complex, more heterogeneous, and with a\nbroader variety of users.</p>\n\n<p>I usually describe this situation with an analogy comparing “Big data” systems\nto human transportation mechanisms in a city.  Here we go:</p>\n\n<ul>\n  <li><em>A Database is like a train</em>: it goes between a set of well defined points\nwith great efficiency, speed, and predictability.  These are popular and\nprofitable routes that many people travel between (e.g. business analytics).\nYou do have to get from home to the train station on your own (ETL), but once\nyou’re in the database/train you’re quite comfortable.</li>\n  <li><em>Spark is like an automobile</em>: it takes you door-to-door from your home to\nyour destination with a single tool.  While this may not be as fast as the train for\nthe long-distance portion, it can be extremely convenient to do ETL, Database\nwork, and some machine learning all from the comfort of a single system.</li>\n  <li><em>Dask is like an all-terrain-vehicle</em>: it takes you out of town on rough\nground that hasn’t been properly explored before.  This is a good match for\nthe Python community, which typically does a lot of exploration into new\napproaches.  You can also drive your ATV around town and you’ll be just fine,\nbut if you want to do thousands of SQL queries then you should probably\ninvest in a proper database or in Spark.</li>\n</ul>\n\n<p>Again, there is a lot of selection bias here, if what you want is a database\nthen you should probably get a database.  Dask is not a database.</p>\n\n<p>This is also wildly over-simplifying things.  Databases like Oracle have lots\nof ETL and analytics tools, Spark is known to go off road, etc..  I obviously\nhave a bias towards Dask.  You really should never trust an author of a project\nto give a fair and unbiased view of the capabilities of the tools in the\nsurrounding landscape.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>That’s a rough sketch of current conversations and open problems for “How Dask\nmight evolve to support institutional use cases.”  It’s really quite surprising\njust how prevalent this story is among the full spectrum from universities to\nhedge funds.</p>\n\n<p>The problems listed above are by no means halting adoption.  I’m not listing\nthe 100 or so questions that are answered with “yes, that’s already supported\nquite well”.  Right now I’m seeing Dask being adopted by individuals and small\ngroups within various institutions.  Those individuals and small groups are\npushing that interest up the stack.  It’s still several months before any 1000+\nperson organization adopts Dask as infrastructure, but the speed at which\nmomentum is building is quite encouraging.</p>\n\n<p>I’d also like to thank the several nameless people who exercise Dask on various\ninfrastructures at various scales on interesting problems and have reported\nserious bugs.  These people don’t show up on the GitHub issue tracker but their\nutility in flushing out bugs is invaluable.</p>\n\n<p>As interest in Dask grows it’s interesting to see how it will evolve.\nCulturally Dask has managed to simultaneously cater to both the open science\ncrowd as well as the private-sector crowd.  The project gets both financial\nsupport and open source contributions from each side.  So far there hasn’t been\nany conflict of interest (everyone is pushing in roughly the same direction)\nwhich has been a really fruitful experience for all involved I think.</p>"
}