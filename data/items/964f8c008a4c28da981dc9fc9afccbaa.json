{
  "title": "Top gsutil command lines to get started on Google Cloud Storage",
  "description": "<p>Google storage is a file storage service available from Google Cloud. Quite similar to Amazon S3 it offers interesting functionalities such as signed-urls, bucket synchronization, collaboration bucket settings, parallel uploads and is S3 compatible. <code class=\"language-plaintext highlighter-rouge\">Gsutil</code>, the associated command line tool is part of the <code class=\"language-plaintext highlighter-rouge\">gcloud</code> command line interface.</p>\n\n<p>After a brief presentation of the <a href=\"https://cloud.google.com/storage/\">Google Cloud Storage</a> service, I will list the most important and useful <code class=\"language-plaintext highlighter-rouge\">gsutil</code> command lines and address a few of the service particularities.</p>\n\n<h1 id=\"google-storage\">Google storage</h1>\n\n<p><img src=\"/assets/gcp/google_cloud_storage.png\" alt=\"Google Cloud Storage Icon\" />\nThe google storage platform is Google’s Entreprise storage solution. Google Storage offers a classic bucket based file structure similarly to AWS S3 and Azure Storage. Google Storage was introduced in may 2010 as <a href=\"https://googlecode.blogspot.com/2010/05/google-storage-for-developers-preview.html\">Google Storage for Developers</a>, a RESTful cloud service limited at the time to a few hundreds developers. <code class=\"language-plaintext highlighter-rouge\">gsutil</code> the command line tool associated with Google Storage was released at the same time.</p>\n\n<p>Fast forward to 2018, Google Storage now offers <a href=\"https://cloud.google.com/storage-options/\">3 levels of storage</a> with different accessibility and pricing.</p>\n\n<ul>\n  <li><strong>standard storage</strong> is for fast access to large amounts of data. It offers high speed of response to requests.</li>\n  <li><strong>DRA</strong> is for long-term data storage and infrequent access and is priced lower than standard storage.</li>\n  <li><strong>Nearline</strong> storage is for even less frequent access and offers longer response times. It is the cheapest option.</li>\n</ul>\n\n<p>Google Storage price structure depends on location and storage class and evolves frequently. At time of writing prices are $0.026 per Gb-month for Standard , $0.01 for Nearline and as low as $0.007 for Coldline storage with a Multi-regional location. See the <a href=\"https://cloud.google.com/storage/pricing\">pricing page</a> for uptodate prices. See also <a href=\"https://medium.com/@duhroach/google-cloud-storage-on-a-shoestring-budget-55f054fad436?__s=veqfs39xxza6piky7ktj\">Google Cloud Storage on a shoestring budget</a>\nfor an interesting cost breakdown.</p>\n\n<p>A distinct trait of Google Storage structure is that folders and subfolders within a bucket are not associated with a “<em>physical</em>” structure as they would be on your local machine. On Google Storage, buckets have <em>virtual</em> folders. <strong>The full path to a file is interpreted as being the entire filename</strong>.\nConsider for instance, the file <code class=\"language-plaintext highlighter-rouge\">hello_world.txt</code> located in <code class=\"language-plaintext highlighter-rouge\">mybucket/myfolder/</code>. The file’s URL is: <code class=\"language-plaintext highlighter-rouge\">gs://mybucket/myfolder/hello_world.txt</code>. Google Storage interprets that file has having the filename <code class=\"language-plaintext highlighter-rouge\">myfolder/hello_world.txt</code>.</p>\n\n<p>The slash <code class=\"language-plaintext highlighter-rouge\">/</code> character is part of the object filename instead of being an indication of an existing folder. As Google calls it, this object naming scheme creates <em>” the illusion of a hierarchical file tree atop the “flat” name space”</em>.</p>\n\n<p>Although this is transparent most of the time, virtual paths may results in misplaced files when uploading a folder with multiple subfolders. If the upload fails and needs to be restarted, the copy command will have unexpected results since the folder did not exist in the first upload but does with the second try.</p>\n\n<p>In order to avoid these weird cases, the best practice, is to make sure to start by creating the expected folder structure and only then upload the files to their target folders.</p>\n\n<h2 id=\"gsutil\">gsutil</h2>\n\n<p><code class=\"language-plaintext highlighter-rouge\">Gsutil</code> is the command line tool used to manage buckets and objects on Google Storage. It is part of the <code class=\"language-plaintext highlighter-rouge\">gcloud</code> shell scripts. <code class=\"language-plaintext highlighter-rouge\">Gsutil</code> is fully <a href=\"https://github.com/GoogleCloudPlatform/gsutil\">open sourced on github</a>, and under active development.</p>\n\n<p>Gsutil goes well beyond simple file transfers with an impressive lists of advanced gsutil features, including:</p>\n\n<ul>\n  <li><strong>ACLs</strong>: setting access control via Access Control Lists</li>\n  <li><strong>rsync</strong>: synchronizing folders and buckets</li>\n  <li><strong>lifeline</strong>: defining lifecycle rules</li>\n  <li><strong>signed urls</strong>: setting time limited online access ()</li>\n  <li><strong>perfdiag</strong> for troubleshooting</li>\n  <li>logging, notifications and versioning</li>\n</ul>\n\n<p>Before diving in these powerful functionalities, let’s walk through a simple case of file transfer.\nIf you don’t have <code class=\"language-plaintext highlighter-rouge\">gsutil</code> installed on your local machine or cloud instance, follow the <a href=\"https://cloud.google.com/sdk/\">Google Cloud SDK install instructions</a> for your OS in order to get started. You may need to sign up for a <a href=\"https://cloud.google.com/free/\">free trial account</a>.</p>\n\n<h2 id=\"getting-around-with-gsutil\">Getting around with gsutil</h2>\n\n<p>In the following examples, I create a bucket, upload some files, get information on these files, move them around and change the bucket storage class.</p>\n\n<ul>\n  <li>First things first. In order to get <code class=\"language-plaintext highlighter-rouge\">help</code> on <code class=\"language-plaintext highlighter-rouge\">gsutil</code> or any <code class=\"language-plaintext highlighter-rouge\">gsutil</code> sub commands:</li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">help</span>\n<span class=\"nv\">$ </span>gsutil &lt;<span class=\"nb\">command</span><span class=\"o\">&gt;</span> <span class=\"nb\">help</span></code></pre></figure>\n\n<ul>\n  <li>Now create a bucket named <code class=\"language-plaintext highlighter-rouge\">&lt;bucketname&gt;</code></li>\n</ul>\n\n<p>All buckets names share a single global Google namespace and must not be already taken.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil mb gs://&lt;bucketname&gt;</code></pre></figure>\n\n<p>Note that there are certain <a href=\"https://cloud.google.com/storage/docs/naming\">restrictions</a> on bucket naming and creation beyond the uniqueness condition. For instance you cannot change the name of an existing bucket, and a bucket name cannot include the word <em>google</em>.</p>\n\n<ul>\n  <li>Upload and download a file with <code class=\"language-plaintext highlighter-rouge\">cp</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp</span> &lt;local_file&gt; gs://&lt;bucketname&gt;/\n<span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp  </span>gs://&lt;bucketname&gt;/&lt;remote_file&gt; ./</code></pre></figure>\n\n<ul>\n  <li>And transfer a file between buckets:</li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp  </span>gs://&lt;bucket_A&gt;/&lt;remote_file&gt; gs://&lt;bucket_B&gt;/</code></pre></figure>\n\n<ul>\n  <li>Create a folder in a bucket with <code class=\"language-plaintext highlighter-rouge\">cp</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp</span> &lt;new_folder&gt; gs://&lt;bucketname&gt;/</code></pre></figure>\n\n<ul>\n  <li>Upload a file to a <code class=\"language-plaintext highlighter-rouge\">&lt;new_folder&gt;</code> with <code class=\"language-plaintext highlighter-rouge\">cp</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp</span> &lt;local_file&gt; gs://&lt;bucketname&gt;/&lt;new_folder&gt;/</code></pre></figure>\n\n<p>This will create the folder <code class=\"language-plaintext highlighter-rouge\">&lt;new_folder&gt;</code> and at the same time upload the file <code class=\"language-plaintext highlighter-rouge\">&lt;local_file&gt;</code> to that folder. Note the trailing <code class=\"language-plaintext highlighter-rouge\">/</code> that tells <code class=\"language-plaintext highlighter-rouge\">gsutil</code> to actually interpret <code class=\"language-plaintext highlighter-rouge\">&lt;new_folder&gt;</code> as a new folder and not as the target filename. If you omit the trailing <code class=\"language-plaintext highlighter-rouge\">/</code> gsutil will rename the file with the filename <code class=\"language-plaintext highlighter-rouge\">&lt;new_folder&gt;</code> once uploaded and the new folder will not be created.</p>\n\n<ul>\n  <li>List the folder with <code class=\"language-plaintext highlighter-rouge\">ls</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">ls </span>gs://&lt;bucketname&gt;/</code></pre></figure>\n\n<ul>\n  <li>Check storage space with <code class=\"language-plaintext highlighter-rouge\">du</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">du</span> <span class=\"nt\">-h</span> gs://&lt;bucketname&gt;/</code></pre></figure>\n\n<p>where the <code class=\"language-plaintext highlighter-rouge\">-h</code> flag makes it human readable</p>\n\n<h2 id=\"the--r-and--m-flags\">The <code class=\"language-plaintext highlighter-rouge\">-r</code> and <code class=\"language-plaintext highlighter-rouge\">-m</code> flags</h2>\n\n<ul>\n  <li>Copy a local folder and its content to a bucket with <code class=\"language-plaintext highlighter-rouge\">cp -r</code></li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp</span> <span class=\"nt\">-r</span> ./&lt;local_folder&gt; gs://&lt;bucketname&gt;/</code></pre></figure>\n\n<p>Consider for instance a local <code class=\"language-plaintext highlighter-rouge\">./img</code> directory that contain several image files. We can copy that entire local directory and create the remote folder at the same time with the following command:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nb\">cp</span> <span class=\"nt\">-r</span> ./img gs://&lt;bucketname&gt;/</code></pre></figure>\n\n<p>The bucket now has the virtual folder <code class=\"language-plaintext highlighter-rouge\">/img</code>.</p>\n\n<ul>\n  <li>Improve performance with the <code class=\"language-plaintext highlighter-rouge\">-m</code> flag</li>\n</ul>\n\n<p>When moving large number of files, adding the <code class=\"language-plaintext highlighter-rouge\">-m</code> flag to <code class=\"language-plaintext highlighter-rouge\">cp</code> will run the transfers in parallel and significantly improve performance provided you are using a reasonably fast network connection.</p>\n\n<ul>\n  <li>Wildcards</li>\n</ul>\n\n<p>gsutil supports <code class=\"language-plaintext highlighter-rouge\">*</code> and <code class=\"language-plaintext highlighter-rouge\">?</code>  wildcards only for files. To include folders in the wildcard target you need to double the <code class=\"language-plaintext highlighter-rouge\">*</code> or <code class=\"language-plaintext highlighter-rouge\">?</code> sign. For instance, <code class=\"language-plaintext highlighter-rouge\">gsutil ls gs://&lt;bucketname&gt;/**.txt</code>  will list all the text files in all subdirectories. The <a href=\"https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames\">wildcard page</a> offers more details.</p>\n\n<h2 id=\"gsutil-full-configuration\">Gsutil full configuration</h2>\n<p>Gsutil full configuration is available in the <code class=\"language-plaintext highlighter-rouge\">~/.boto</code> file. You can edit that file directly or via the <code class=\"language-plaintext highlighter-rouge\">gsutil config</code> command. Some interesting parameters are:</p>\n\n<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">parallel_composite_upload_threshold</code>: to specify the maximum size of a file to be uploaded in a single stream. Files larger than this threshold will be uploaded in parallel. The <code class=\"language-plaintext highlighter-rouge\">parallel_composite_upload_threshold</code> parameter is disabled by default.</p>\n  </li>\n  <li><code class=\"language-plaintext highlighter-rouge\">check_hashes</code>: to enforce integrity checks when downloading data, always, never or conditionally.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">prefer_api</code>: to specify the API to use when interacting with cloud storage providers (S3, GCS, …)</li>\n  <li>and <code class=\"language-plaintext highlighter-rouge\">aws_access_key_id</code> and <code class=\"language-plaintext highlighter-rouge\">aws_secret_access_key</code> for interoperability with S3.</li>\n</ul>\n\n<p>Cloud storage compatibility is powerful. Not only can you migrate easily from AWS S3 to GCP or vice versa but you can also sync S3 buckets and GCP buckets with the rsync command.</p>\n\n<h2 id=\"acl\">ACL</h2>\n\n<p>As stated in the documentation, <em>Access Control Lists (ACLs) allow you to control who can read and write your data, and who can read and write the ACLs themselves</em>.\nACL are assigned to objects (files) or buckets. By default all files in a bucket have the same ACL as the bucket they’re in.</p>\n\n<p>ACL has 3 commands</p>\n\n<ul>\n  <li><strong>GET</strong>: lists the permissions on a given object. For instance <code class=\"language-plaintext highlighter-rouge\">gsutil acl get gs://&lt;bucketname&gt;/</code> outputs the access settings for the <code class=\"language-plaintext highlighter-rouge\">&lt;bucketname&gt;</code> bucket.</li>\n  <li><strong>SET</strong>: sets the permissions on a given object. The best way to set the permissions and avoid mistakes is by first exporting them to a file with <code class=\"language-plaintext highlighter-rouge\">gsutil acl get gs://&lt;bucketname&gt;/&lt;filename&gt; act.txt</code>, modify the acl.txt file and then set the new permissions with <code class=\"language-plaintext highlighter-rouge\">gsutil acl set acl.txt gs://bucket/&lt;filename&gt;</code></li>\n  <li><strong>CH</strong>: for change, modifies the current permissions on a given object. For instance to grant WRITE access to a user <code class=\"language-plaintext highlighter-rouge\">gsutil acl ch -u someone@gmail.com:WRITE gs://&lt;bucketname&gt;/</code></li>\n</ul>\n\n<p>The default settings for buckets are defined with the <code class=\"language-plaintext highlighter-rouge\">defacl</code> command which also responds to <code class=\"language-plaintext highlighter-rouge\">get</code>, <code class=\"language-plaintext highlighter-rouge\">set</code> and <code class=\"language-plaintext highlighter-rouge\">ch</code> subcommands. The command <code class=\"language-plaintext highlighter-rouge\">gsutil defacl get gs://&lt;bucketname&gt;/</code> will return the default settings for the bucket <code class=\"language-plaintext highlighter-rouge\">&lt;bucketname&gt;</code>.</p>\n\n<p>Several pre defined setings are available:</p>\n\n<ul>\n  <li><strong>project-private</strong>: is the default setting for new objects. It gives permission to the project team based on their roles. All team members have READ permission while editors and owners have OWNER permission.</li>\n  <li><strong>private</strong>: Gives the requester OWNER permission for a bucket or object</li>\n  <li><strong>public-read</strong>: Opens the objects to the whole internet as it gives all users read permission.</li>\n  <li><strong>public-read-write</strong>: The dangerous setting that allows anyone on the internet to upload files to your bucket.</li>\n</ul>\n\n<p>Further ACL details are available in the <a href=\"https://cloud.google.com/storage/docs/gsutil/commands/acl\">ACL page</a></p>\n\n<h2 id=\"rsync\">rsync</h2>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">gsutil rsync</code> makes the content of a target folder identical to the content of a source folder by copying, updating or deleting any file in the target folder that has changed in the source folder. This synchronization works across local and GCP folders as well as other gsutil cloud compatible storage solutions such as AWS S3. With the <code class=\"language-plaintext highlighter-rouge\">gsutil rsync</code> command you have everything you need to create an automatic backup of your data in the cloud. The rsync command follows:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil rsync &lt;<span class=\"nb\">source </span>folder&gt; &lt;target folder&gt;</code></pre></figure>\n\n<p>Consider a local folder <code class=\"language-plaintext highlighter-rouge\">./myfolder</code> and the <code class=\"language-plaintext highlighter-rouge\">&lt;bucketname&gt;</code> bucket, the following command synchronizes the content of the local folder with the storage bucket:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> <span class=\"nt\">-d</span> ./myfolder gs://&lt;bucketname&gt;</code></pre></figure>\n\n<p>The content of <code class=\"language-plaintext highlighter-rouge\">gs://&lt;bucketname&gt;</code> will match the content of your local <code class=\"language-plaintext highlighter-rouge\">./myfolder</code> directory, effectively backing up the local documents.</p>\n\n<ul>\n  <li>Note the presence of the <code class=\"language-plaintext highlighter-rouge\">-r</code> flag which ensures that all subfolders are matched.</li>\n  <li>The <code class=\"language-plaintext highlighter-rouge\">-d</code> flag is to be used with caution as it will delete the content in the target when deleted from the source. If you inadvertently make a mistake in your command, for instance inverting the source and target folders, you may end up deleting your content. A good way to ensure that does not happen is to enable bucket versioning.</li>\n</ul>\n\n<p>If you don’t want to have to run the <code class=\"language-plaintext highlighter-rouge\">gsutil</code> command every time you make a change in the source folder, you can set up a cron job on your local with <code class=\"language-plaintext highlighter-rouge\">crontab -e</code> or the equivalent for windows machines. For instance the following cron job will backup your local folder to Google Cloud every 15mn.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"se\">\\*</span>/15  <span class=\"k\">*</span> <span class=\"k\">*</span> <span class=\"k\">*</span> <span class=\"k\">*</span> gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> <span class=\"nt\">-d</span> &lt; full path to myfolder&gt; gs://mybucket <span class=\"o\">&gt;&gt;</span> &lt;full path to log file&gt;</code></pre></figure>\n\n<h2 id=\"bucket-versioning\">Bucket versioning</h2>\n\n<p>Bucket versioning is a powerful feature that prevents any file deletion by mistake. Enabling and disabling versioning is done at the bucket level with the command:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil versioning <span class=\"nb\">set </span>on gs://&lt;bucketname&gt;\n<span class=\"nv\">$ </span>gsutil versioning <span class=\"nb\">set </span>off gs://&lt;bucketname&gt;</code></pre></figure>\n\n<p>When versioning is enabled on a bucket, objects become accessible by specifying their version number. Listing the content of a bucket will show the version numbers of its objects as such:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">gs://&lt;bucketname&gt;/&lt;filename&gt;#&lt;version_number_1&gt;\ngs://&lt;bucketname&gt;/&lt;filename&gt;#&lt;version_number_2&gt;\ngs://&lt;bucketname&gt;/&lt;filename&gt;#&lt;version_number_3&gt;\n...</code></pre></figure>\n\n<p>To retrieve the correct version, simply append the version number to the object name in the cp command.</p>\n\n<p>The <a href=\"https://cloud.google.com/storage/docs/object-versioning\">object versioning</a> page offers more details on the subject.</p>\n\n<h2 id=\"signed-urls\">Signed URLs</h2>\n\n<p>Signed URLs is a mechanism for query string authentication for buckets and objects. In other words, Signed urls provide a way to give time-limited read or write access to anyone in possession of the URL, regardless of whether they have a Google account.\nTo create a signed url you first need to generate a generate a private key following these <a href=\"https://console.cloud.google.com/apis/credentials\">instructions</a>. Click on <code class=\"language-plaintext highlighter-rouge\">Create a service account key</code>, select  your project, and download the JSON file that contains your private key.</p>\n\n<p>You can now create a signed urls for one of your file with</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil signurl <span class=\"nt\">-d</span> 10m <span class=\"nt\">-m</span> GET &lt;path/private_key.json&gt;  gs://&lt;bucketname&gt;/&lt;filename&gt;</code></pre></figure>\n\n<p>Note that signed urls do not work on directories. If you want to give access to multiple files you can use wildcards. For instance the following command will give access for 10 minutes on all the png files in the <code class=\"language-plaintext highlighter-rouge\">gs://&lt;bucketname&gt;/img/</code> folder.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil signurl <span class=\"nt\">-d</span> 10m <span class=\"nt\">-m</span> GET &lt;path/private_key.json&gt; gs://&lt;bucketname&gt;/img/<span class=\"k\">*</span>.png<span class=\"k\">*</span></code></pre></figure>\n\n<p>Check the <a href=\"https://cloud.google.com/storage/docs/access-control/signed-urls\">signed urls</a> page for more info</p>\n\n<h3 id=\"service-accounts\">Service accounts</h3>\n\n<p>Service accounts are special accounts that represent software rather than people. They are the most common way applications authenticate with Google Cloud Storage. Every project has service accounts associated with it, which may be used for different authentication scenarios, as well as to enable advanced features such as Signed URLs and browser uploads using POST.</p>\n\n<p>When you use a service account to authenticate your application, you do not need a user to authenticate to get an access token. Instead, you obtain a private key from the Google Cloud Platform Console, which you then use to send a signed request for an access token. You can then use the access token like you normally would. For more information see the <a href=\"https://cloud.google.com/storage/docs/authentication\">Google Cloud Platform Auth Guide</a>.</p>\n\n<h3 id=\"lifecycle\">Lifecycle</h3>\n\n<p>Lifecycle configurations allows you to automatically delete or change the storage class of objects when some criterion is met.</p>\n\n<p>To enable lifecycle for a bucket with settings defined in the <code class=\"language-plaintext highlighter-rouge\">config_file.json</code> file, run:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">$ </span>gsutil lifecycle <span class=\"nb\">set</span> &lt;config_file.json&gt; gs://&lt;bucket_name&gt;</code></pre></figure>\n\n<p>For instance, in order to delete the content of the bucket after 30 days, the config file would be:\nExample: delete after 10 days</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span>\n    <span class=\"s2\">\"lifecycle\"</span>: <span class=\"o\">{</span>\n        <span class=\"s2\">\"rule\"</span>: <span class=\"o\">[</span>\n            <span class=\"o\">{</span>\n                <span class=\"s2\">\"action\"</span>: <span class=\"o\">{</span><span class=\"s2\">\"type\"</span>: <span class=\"s2\">\"Delete\"</span><span class=\"o\">}</span>,\n                <span class=\"s2\">\"condition\"</span>: <span class=\"o\">{</span>\n                    <span class=\"s2\">\"age\"</span>: 30,\n                    <span class=\"s2\">\"isLive\"</span>: <span class=\"nb\">true</span>\n                <span class=\"o\">}</span>\n            <span class=\"o\">}</span>\n        <span class=\"o\">]</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></figure>\n\n<p>While changing  storage class of a bucket to Nearline after a year would be:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span>\n    <span class=\"s2\">\"action\"</span>: <span class=\"o\">{</span>\n        <span class=\"s2\">\"type\"</span>: <span class=\"s2\">\"SetStorageClass\"</span>,\n        <span class=\"s2\">\"storageClass\"</span>: <span class=\"s2\">\"NEARLINE\"</span>\n    <span class=\"o\">}</span>,\n    <span class=\"s2\">\"condition\"</span>: <span class=\"o\">{</span>\n      <span class=\"s2\">\"age\"</span>: 365,\n      <span class=\"s2\">\"matchesStorageClass\"</span>: <span class=\"o\">[</span><span class=\"s2\">\"MULTI_REGIONAL\"</span>, <span class=\"s2\">\"STANDARD\"</span>, <span class=\"s2\">\"DURABLE_REDUCED_AVAILABILITY\"</span><span class=\"o\">]</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></figure>\n\n<p>Check the <a href=\"https://cloud.google.com/storage/docs/lifecycle\">lifecycle configurations page</a> for more info.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>Google Cloud Storage is a fully featured enterprise level service which offers a viable alternative to AWS S3. Prices, scalability, and reliability are key features of the service. I’ve been using Google Storage for awhile across different projects and find it very user friendly. Definitely worth testing if you need to store significant amount of data.</p>\n\n<h1 id=\"further-readings\">Further readings</h1>\n\n<ul>\n  <li><a href=\"https://cloud.google.com/solutions/transferring-big-data-sets-to-gcp\">Transferring Big Data Sets to Cloud Platform</a></li>\n  <li><a href=\"https://cloud.google.com/storage/docs/streaming\">Streaming data</a></li>\n  <li><a href=\"https://medium.com/@duhroach/google-cloud-storage-performance-4cfcec8bad72?__s=veqfs39xxza6piky7ktj\">Google Cloud Storage Performance</a></li>\n  <li><a href=\"https://medium.com/google-cloud/use-cases-and-a-few-different-ways-to-get-files-into-google-cloud-storage-c8dce8f4f25a\">Use Cases and Different Ways to get Files Into Google Cloud Storage</a></li>\n</ul>\n\n<p><br /></p>\n\n<blockquote>\n  <p>If you liked this post, please <a href=\"https://twitter.com/intent/tweet?url=https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html&amp;text=Top gsutil command lines to get started on Google Cloud Storage &amp;via=alexip\" target=\"_blank\"> share it on twitter</a>\nAnd leave me your feedback, questions, comments, suggestions below.\nMuch appreciated :)\n<br /></p>\n</blockquote>",
  "pubDate": "Mon, 01 Jan 2018 14:00:00 +0000",
  "link": "https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html",
  "guid": "https://alexisperrier.com/gcp/2018/01/01/google-storage-gsutil.html",
  "category": "gcp"
}