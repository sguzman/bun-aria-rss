{
  "title": "Caffe: Brew your first DNN",
  "description": "<h2 id=\"caffe\">Caffe</h2>\n\n<p>You can find some background for this post here: <a href=\"/2015/05/04/Deep-Learning-with-Python.html\">Deep Learning with Python</a>!</p>\n\n<p>Caffe has its strengths and its weaknesses. For example, there are some outstanding issues regarding using multiple GPUs in parallel during training. According to a wonderful write-up by <a href=\"https://plus.google.com/+TomaszMalisiewicz/posts\">Tomasz Malisiewicz</a> titled <a href=\"http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html\">Deep down the rabbit hole: CVPR 2015 and beyond</a>:</p>\n\n<blockquote>\n  <p>Caffe is much more popular that Torch, but when talking to some power users of Deep Learning (like <a href=\"https://plus.google.com/100209651993563042175\">+Andrej Karpathy</a> and other DeepMind scientists), a certain group of experts seems to be migrating from Caffe to Torch.</p>\n</blockquote>\n\n<p>I read somewhere else that Caffe : Torch :: Applications : Research. If you want to quickly iterate on datasets with the aim of building applications, Caffe gives you a flexible framework with a lot of built-in tools to do so; with Python bindings to boot. Additionally, one of its great features is that you can essentially specify all of the layers and layer parameters of a neural network with a simple config file. I know that there are some out there (you know who you are! :-P) who do not like this feature.</p>\n\n<h2 id=\"prerequisites\">Prerequisites</h2>\n\n<p>Caffe has some prerequisites, which, unless you’ve already got a CUDA driver installed, will prevent you from getting started in just minutes.</p>\n\n<p>Please go to the Caffe <a href=\"http://caffe.berkeleyvision.org/installation.html#prerequisites\">installation page</a> for more details. Be sure to also follow the <a href=\"http://caffe.berkeleyvision.org/install_osx.html\">OS X Installation</a> page very closely if you’re on a Mac like I am.</p>\n\n<p>For going against your GPU, you’ll need:</p>\n\n<ul>\n  <li><a href=\"http://brew.sh/\">Homebrew</a> if you don’t already have it.</li>\n  <li><a href=\"https://developer.nvidia.com/cuda-zone\">CUDA</a> (if you want to use CAFFE in GPU mode, which in itself requires an NVIDIA GPU and an NVIDIA Developer login)</li>\n  <li><a href=\"https://developer.nvidia.com/cuDNN\">cuDNN</a> (accelerated CUDA, in a nutshell)</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">brew install boost-python</code> (simply doing <code class=\"language-plaintext highlighter-rouge\">brew install boost</code> or <code class=\"language-plaintext highlighter-rouge\">brew install boost --with-python</code> didn’t do the trick)</li>\n</ul>\n\n<p>There are plenty of instructions on the caffe site for getting these prerequisites installed.</p>\n\n<p>Since we’re using Python, pay <strong>EXTRA SPECIAL ATTENTION</strong> to the Makefile.config instructions. Particularly, know where your Python and Numpy live.</p>\n\n<p>Mine’s a little complicated since I use Homebrew for lots of things, but here’s what my config info looks like. This will save you from pulling some hair out:</p>\n\n<p>Add these to your <code class=\"language-plaintext highlighter-rouge\">~/.bash_profile</code> or just <code class=\"language-plaintext highlighter-rouge\">export</code> them in your session.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nb\">export </span><span class=\"nv\">PATH</span><span class=\"o\">=</span>/usr/local/cuda/bin:<span class=\"nv\">$PATH</span>\n<span class=\"nb\">export </span><span class=\"nv\">DYLD_LIBRARY_PATH</span><span class=\"o\">=</span>/usr/local/cuda/lib:<span class=\"nv\">$DYLD_LIBRARY_PATH</span></code></pre></figure>\n\n<p>Cool. Now, in your <code class=\"language-plaintext highlighter-rouge\">Makefile.config</code> file:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>PYTHON_INCLUDE := /usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/include/python2.7 \\\n\t\t/usr/local/Cellar/numpy/1.9.2/lib/python2.7/site-packages/numpy/core/include\n\nPYTHON_LIB := /usr/local/lib /usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib\n</code></pre></div></div>\n\n<p>Moving on!\nIn your <code class=\"language-plaintext highlighter-rouge\">&lt;path&gt;/&lt;to&gt;/caffe/</code> directory:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>make all\n<span class=\"nv\">$ </span>make <span class=\"nb\">test</span>\n<span class=\"nv\">$ </span>make runtest\n...\n<span class=\"nv\">$ </span>make pycaffe</code></pre></figure>\n\n<p>When all of this is done, start Python in <code class=\"language-plaintext highlighter-rouge\">&lt;path&gt;/&lt;to&gt;/caffe/python/</code>.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">caffe</span></code></pre></figure>\n\n<p>Awesome! Finally, we can let the real fun begin.</p>\n\n<h2 id=\"model-zoo\">Model Zoo</h2>\n\n<p>Caffe, having the great ecosystem that it does, has a special place called the <a href=\"https://github.com/BVLC/caffe/wiki/Model-Zoo\">“Model Zoo”</a> where various reference models and variations thereof are curated in one place with code and citations. Be sure to check it out for Caffe implementations of some of the most recent cutting-edge research, such as GoogLeNet and models from the CVPR2015 DeepVision workshop, which occurred after this blog post began.</p>\n\n<h2 id=\"deep-dreaming\">Deep Dreaming</h2>\n\n<p>If you’re interested in some light background reading regarding GoogLeNet, the deep learning model that Google Deep Dream uses as its default, you should check out the following arXiv preprint: <a href=\"http://arxiv.org/abs/1409.4842\">Going Deeper with Convolutions</a><sup id=\"fnref:0\" role=\"doc-noteref\"><a href=\"#fn:0\" class=\"footnote\">1</a></sup>. Note that GoogLeNet is an homage to the pioneering work <a href=\"http://yann.lecun.com/exdb/lenet/\">LeNet</a><sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\">2</a></sup>, built by Yann LeCun in the 80’s for handwritten digit recognition.</p>\n\n<p>In fact, <a href=\"http://caffe.berkeleyvision.org/gathered/examples/mnist.html\">here</a> is a great Caffe tutorial building LeNet by hand and training it on MNIST. But I thought you’d find Google Deep Dream more interesting as a blog post :-).</p>\n\n<p>In <a href=\"http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html\">this</a> Google Research blog post, Google describes its notion of Inceptionism, and how it visualizes going deeper into neural networks. A couple of weeks later, they published <a href=\"http://googleresearch.blogspot.com/2015/07/deepdream-code-example-for-visualizing.html\">this</a> follow-up blog post, which links you to the DeepDream <a href=\"https://github.com/google/deepdream\">repo</a>, conveniently hosted on GitHub and available as an IPython Notebook <a href=\"https://github.com/google/deepdream/blob/master/dream.ipynb\">here</a>.</p>\n\n<h2 id=\"deep-dreaming-prerequisites\">Deep Dreaming Prerequisites</h2>\n\n<p>It’s simple enough to follow the IPython Notebook, but here are some instructions:</p>\n\n<ul>\n  <li>Make sure you have installed the following Python libraries: NumPy, SciPy, PIL, and IPython (just use <code class=\"language-plaintext highlighter-rouge\">pip</code>).</li>\n  <li>Make sure you have installed Caffe (i.e., that you have read the beginning of this blog post and didn’t skip ahead!).</li>\n  <li>Google’s <a href=\"https://developers.google.com/protocol-buffers/\">protobuf</a> library (I will go through this since it was a little tricky).</li>\n</ul>\n\n<h2 id=\"installing-protobuf\">Installing Protobuf</h2>\n\n<p>If you’re a nerd like me, you’re insane enough to go straight to the source: Google’s protobuf GitHub repo, found <a href=\"https://github.com/google/protobuf\">here</a>.</p>\n\n<p>The installation instructions are fairly simple. Unless you are using a Mac. You can follow their instructions using MacPorts, or you can join me in 2015 and use Homebrew :-) (or the package manager of your choice).</p>\n\n<p>Here’s the flow for a Mac user:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>brew <span class=\"nb\">install </span>autoconf\n<span class=\"nv\">$ </span>brew <span class=\"nb\">install </span>automake\n<span class=\"nv\">$ </span>brew <span class=\"nb\">install </span>libtool</code></pre></figure>\n\n<p>Nice. Now you can install protobuf. Go to a directory where you’d like to put the protobuf repo.</p>\n\n<p>Then do the following:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>git clone https://github.com/google/protobuf.git\n<span class=\"nv\">$ </span><span class=\"nb\">cd </span>protobuf/\n<span class=\"nv\">$ </span>./autogen.sh\n<span class=\"nv\">$ </span>./configure\n<span class=\"nv\">$ </span>make\n<span class=\"nv\">$ </span>make check\n<span class=\"nv\">$ </span>make <span class=\"nb\">install</span></code></pre></figure>\n\n<p>Assuming that works, you’re ready to deep dream!</p>\n\n<p>Go to a directory where you’d like to put the Deep Dream IPython notebook and <code class=\"language-plaintext highlighter-rouge\">git clone</code> it!</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>git clone https://github.com/google/deepdream.git\n<span class=\"nv\">$ </span><span class=\"nb\">cd </span>deepdream/\n<span class=\"nv\">$ </span>ipython notebook</code></pre></figure>\n\n<p>Click on <code class=\"language-plaintext highlighter-rouge\">dream.ipynb</code>, et voilà, you’re in.</p>\n\n<p>Suffice to say, I won’t copy the whole notebook here. That being said, let’s take a closer look.</p>\n\n<p>Did you run into an error when you tried to load in GoogLeNet? I did. Just because you’ve downloaded and installed Caffe doesn’t mean that you’re ready to brew!</p>\n\n<p>Go to <code class=\"language-plaintext highlighter-rouge\">&lt;path&gt;/&lt;to&gt;/caffe/</code>, and do the following:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>./scripts/download_model_binary.py models/bvlc_googlenet</code></pre></figure>\n\n<p>Once that has finished, you should be able to run the block of <code class=\"language-plaintext highlighter-rouge\">dream.ipynb</code> that loads in a pretrained GoogLeNet. Make sure you set your <code class=\"language-plaintext highlighter-rouge\">&lt;path&gt;/&lt;to&gt;/caffe/models/bvlc_googlenet</code> properly. Now you’re ready to deep dream.</p>\n\n<p>Get down a few lines to where you assign:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">(</span><span class=\"n\">PIL</span><span class=\"p\">.</span><span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"your_own_image.jpg\"</span><span class=\"p\">))</span></code></pre></figure>\n\n<h2 id=\"seurat\">Seurat</h2>\n\n<p>I’m picking my favorite work by pointillist George Seurat, his epic chef d’oeuvre, <em>Un dimanche après-midi à l’Île de la Grande Jatte</em>.</p>\n\n<p>You don’t need to be an art history buff to appreciate this work. Perhaps you’re a fan of <a href=\"http://www.imdb.com/title/tt0091042/\">Ferris Bueller’s Day Off</a>? Who can forget the moment when Cameron sees this work from afar and becomes fixated on its perfection? Here’s a clip from their visit to the Art Institute of Chicago (click <a href=\"https://www.youtube.com/watch?v=ubpRcZNJAnE\">here</a> if you want to see the whole thing):</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><a href=\"https://youtu.be/ubpRcZNJAnE?t=1m23s\"><img src=\"/assets/cameron-staring.png\" alt=\"Cameron staring at Seurat\" /></a></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Et Voilà:</p>\n\n<p><img src=\"/assets/seurat.jpg\" alt=\"Un dimanche après-midi à l'Île de la Grande Jatte\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<h2 id=\"prepare-for-your-mind-to-be-blown\">Prepare for your mind to be blown</h2>\n\n<p>Take a deep breath. This is it. This is the moment. It’s so intense.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">_</span><span class=\"o\">=</span><span class=\"n\">deepdream</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">,</span> <span class=\"n\">img</span><span class=\"p\">)</span></code></pre></figure>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><img src=\"/assets/seurat_dream.jpeg\" alt=\"Un deep dream dimanche après-midi à l'Île de la Grande Jatte\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>BOOM. Holy mackerel. What are all of these strange animals and pagodas and faces popping in and out of mes amis Parisiens?</p>\n\n<p>Let’s take a look at what else we can do:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">_</span><span class=\"o\">=</span><span class=\"n\">deepdream</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">,</span>\n            <span class=\"n\">img</span><span class=\"p\">,</span> \n            <span class=\"n\">octave_n</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">)</span></code></pre></figure>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><img src=\"/assets/seurat_dream_12octaves.jpeg\" alt=\"Un deep dream dimanche après-midi à l'Île de la Grande Jatte\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>OK, WOW, THIS IS DEFINITELY WEIRDER.</p>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">octave_n</code> parameter default is 4. When you run <code class=\"language-plaintext highlighter-rouge\">deepdream(net,img)</code> for the first time, you will see an output that ranges from <code class=\"language-plaintext highlighter-rouge\">0 0</code> to <code class=\"language-plaintext highlighter-rouge\">3 9</code>. The image is redrawn <code class=\"language-plaintext highlighter-rouge\">iter_n</code> times per octave, so <code class=\"language-plaintext highlighter-rouge\">3 9</code> here indicates (octave) 4 (interation) 10, since we’re counting from 0. Interesting. In music theory, octaves abstractly represent notes whose frequencies are double the previous octave, i.e., you have sung <em>do re me fa so la ti do</em> and both <em>do</em> notes are exactly one octave apart (and whose frequency ratio between the first and second <em>do</em> is 1:2). Here in the notebook you can follow the code to see that the definition of the octave is subject to your own experimentation!</p>\n\n<p>Let’s do one more:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">_</span><span class=\"o\">=</span><span class=\"n\">deepdream</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">,</span> \n            <span class=\"n\">img</span><span class=\"p\">,</span> \n            <span class=\"n\">octave_n</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> \n            <span class=\"n\">iter_n</span><span class=\"o\">=</span><span class=\"mi\">13</span><span class=\"p\">,</span> \n            <span class=\"n\">octave_scale</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">,</span> \n            <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s\">'inception_3a/output'</span><span class=\"p\">)</span></code></pre></figure>\n\n<p>What is the <code class=\"language-plaintext highlighter-rouge\">end='inception_3a/output'</code> parameter? Check it out: submit <code class=\"language-plaintext highlighter-rouge\">net.blobs.keys()</code> and pick a layer, any layer (except the splits, I think).</p>\n\n<p>Here’s what we get:</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><img src=\"/assets/seurat_dream_13o_13i_3a_output.jpeg\" alt=\"Un deep dream dimanche après-midi à l'Île de la Grande Jatte\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>How cool is that? Does it seem at all Kandinsky-esque, or is it just me?</p>\n\n<h2 id=\"what-to-do-next\">What to do next</h2>\n\n<p>You’ve got the power of Deep Dreaming in your hands. What do you want to do? There are a couple of interesting helper functions in the notebook that I am intentionally not covering so that you can explore them yourself!</p>\n\n<p>One loops over and over, so that the final output of calling <code class=\"language-plaintext highlighter-rouge\">deepdream(net, img)</code> is the input – i.e., <code class=\"language-plaintext highlighter-rouge\">img = deepdream(net, img)</code> – to another iteration of <code class=\"language-plaintext highlighter-rouge\">deepdream(net, img)</code>. This leads to a lot of interesting compositional weirdness. Definitely what I think of as <em>inception</em>: dreams within dreams.</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><img src=\"/assets/inception.jpg\" alt=\"We need to go deeper\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The second gives you the ability to use another image as the <em>objective guide</em> of your deep dreaming. Put another way, you can kind of think of this helper function as mashing up your deep dreaming with one image that tries to align with the other.</p>\n\n<p>Last, if you’re feeling particularly full of free time, try building a tree of such deep dreaming. Maybe you want to train an image on a hierarchy of different images, each one dreamed from a layer below. Moreover, don’t just follow my example and deep dream with oil on canvas. Try using natural images instead. Depending on your parameters, your deep dream results could be very far out indeed.</p>\n\n<p><strong>WARNING:</strong> Deep dreaming photos of people whom you love is definitely nighmare-inducing. Don’t say I didn’t warn you.</p>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:0\" role=\"doc-endnote\">\n      <p>Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. “Going deeper with convolutions.” arXiv preprint arXiv:1409.4842 (2014). <a href=\"#fnref:0\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:1\" role=\"doc-endnote\">\n      <p>Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541–551, December 1989. <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>",
  "pubDate": "Wed, 29 Jul 2015 11:49:40 +0000",
  "link": "http://korbonits.github.io/2015/07/29/Caffe-brew-your-first-DNN.html",
  "guid": "http://korbonits.github.io/2015/07/29/Caffe-brew-your-first-DNN.html"
}