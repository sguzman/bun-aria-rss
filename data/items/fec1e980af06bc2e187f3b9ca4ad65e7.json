{
  "title": "FUNCK: Information Funnels and Bottlenecks for Invariant Representation Learning. (arXiv:2211.01446v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01446",
  "description": "<p>Learning invariant representations that remain useful for a downstream task\nis still a key challenge in machine learning. We investigate a set of related\ninformation funnels and bottleneck problems that claim to learn invariant\nrepresentations from the data. We also propose a new element to this family of\ninformation-theoretic objectives: The Conditional Privacy Funnel with Side\nInformation, which we investigate in fully and semi-supervised settings. Given\nthe generally intractable objectives, we derive tractable approximations using\namortized variational inference parameterized by neural networks and study the\nintrinsic trade-offs of these objectives. We describe empirically the proposed\napproach and show that with a few labels it is possible to learn fair\nclassifiers and generate useful representations approximately invariant to\nunwanted sources of variation. Furthermore, we provide insights about the\napplicability of these methods in real-world scenarios with ordinary tabular\ndatasets when the data is scarce.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Freitas_J/0/1/0/all/0/1\">Jo&#xe3;o Machado de Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1\">Bernhard C. Geiger</a>"
}