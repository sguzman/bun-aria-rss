{
  "title": "If you did not already know",
  "link": "https://analytixon.com/2022/10/26/if-you-did-not-already-know-1868/",
  "comments": "https://analytixon.com/2022/10/26/if-you-did-not-already-know-1868/#respond",
  "dc:creator": "Michael Laux",
  "pubDate": "Wed, 26 Oct 2022 22:23:41 +0000",
  "category": "What is ...",
  "guid": "https://analytixon.com/?p=37413",
  "description": "Learnable ScatterNet In this paper we explore tying together the ideas from Scattering Transforms and Convolutional Neural Networks (CNN) for &#8230;<p><a href=\"https://analytixon.com/2022/10/26/if-you-did-not-already-know-1868/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p><a href=\"http://arxiv.org/abs/1903.03137v1\" target=\"top\" rel=\"noopener\"><strong>Learnable ScatterNet</strong></a>  <a href=\"https://www.google.de/search?q=Learnable ScatterNet\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">In this paper we explore tying together the ideas from Scattering Transforms and Convolutional Neural Networks (CNN) for Image Analysis by proposing a learnable ScatterNet. Previous attempts at tying them together in hybrid networks have tended to keep the two parts separate, with the ScatterNet forming a fixed front end and a CNN forming a learned backend. We instead look at adding learning between scattering orders, as well as adding learned layers before the ScatterNet. We do this by breaking down the scattering orders into single convolutional-like layers we call &#8216;locally invariant&#8217; layers, and adding a learned mixing term to this layer. Our experiments show that these locally invariant layers can improve accuracy when added to either a CNN or a ScatterNet. We also discover some surprising results in that the ScatterNet may be best positioned after one or more layers of learning rather than at the front of a neural network. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1901.00158v1\" target=\"top\" rel=\"noopener\"><strong>Text Infilling</strong></a>  <a href=\"https://www.google.de/search?q=Text Infilling\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">Recent years have seen remarkable progress of text generation in different contexts, such as the most common setting of generating text from scratch, and the emerging paradigm of retrieval-and-rewriting. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life, yet is under-explored. Previous work has focused on restricted settings by either assuming single word per missing portion or limiting to a single missing portion to the end of the text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. We study various approaches for the task, including a self-attention model with segment-aware position encoding and bidirectional context modeling. We create extensive supervised data by masking out text with varying strategies. Experiments show the self-attention model greatly outperforms others, creating a strong baseline for future research. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1810.08754v1\" target=\"top\" rel=\"noopener\"><strong>BCR-Net</strong></a>  <a href=\"https://www.google.de/search?q=BCR-Net\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">This paper proposes a novel neural network architecture inspired by the nonstandard form proposed by Beylkin, Coifman, and Rokhlin in [Communications on Pure and Applied Mathematics, 44(2), 141-183]. The nonstandard form is a highly effective wavelet-based compression scheme for linear integral operators. In this work, we first represent the matrix-vector product algorithm of the nonstandard form as a linear neural network where every scale of the multiresolution computation is carried out by a locally connected linear sub-network. In order to address nonlinear problems, we propose an extension, called BCR-Net, by replacing each linear sub-network with a deeper and more powerful nonlinear one. Numerical results demonstrate the efficiency of the new architecture by approximating nonlinear maps that arise in homogenization theory and stochastic computation. &#8230; </span><BR/><BR/><a href=\"http://arxiv.org/abs/1809.02404v1\" target=\"top\" rel=\"noopener\"><strong>Joint Spectrum</strong></a>  <a href=\"https://www.google.de/search?q=Joint Spectrum\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignright\" src=\"https://analytixon.files.wordpress.com/2015/01/google.png?w=529\" alt=\"google\" data-recalc-dims=\"1\"/></a><BR/><span style=\"font-size:12px;font-style:normal;text-align:justify;\">We introduce the notion of joint spectrum of a compact set of matrices $S \\subset$ GL$_{d}(\\mathbb{C})$, which is a multi-dimensional generalization of the joint spectral radius. In the irreducible case we describe its properties and examine how it relates to the set of eigenvalues of elements in the semi-group generated by $S$. We also make connections with the theory of random products of matrices. &#8230; </span><BR/></p>\n",
  "wfw:commentRss": "https://analytixon.com/2022/10/26/if-you-did-not-already-know-1868/feed/",
  "slash:comments": 0,
  "post-id": 37413
}