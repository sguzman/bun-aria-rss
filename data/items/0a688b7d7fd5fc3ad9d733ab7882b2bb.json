{
  "title": "User Tampering in Reinforcement Learning Recommender Systems. (arXiv:2109.04083v2 [cs.AI] UPDATED)",
  "link": "http://arxiv.org/abs/2109.04083",
  "description": "<p>This paper provides novel formal methods and empirical demonstrations of a\nparticular safety concern in reinforcement learning (RL)-based recommendation\nalgorithms. We call this safety concern `user tampering' -- a phenomenon\nwhereby an RL-based recommender system might manipulate a media user's opinions\nvia its recommendations as part of a policy to increase long-term user\nengagement. We then apply techniques from causal modelling to analyse the\nleading approaches in the literature for implementing scalable RL-based\nrecommenders, and we observe that the current approaches permit user tampering.\nAdditionally, we review the existing mitigation strategies for reward tampering\nproblems and show that they do not transfer well to the user tampering\nphenomenon found in the recommendation context. Furthermore, we provide a\nsimulation study of a media RL-based recommendation problem constrained to the\nrecommendation of political content. We show that a Q-learning algorithm\nconsistently learns to exploit its opportunities to polarise simulated users\nwith its early recommendations in order to have more consistent success with\nlater recommendations catering to that polarisation. This latter contribution\ncalls for urgency in designing safer RL-based recommenders; the former suggests\nthat creating such safe recommenders will require a fundamental shift in design\naway from the approaches we have seen in the recent literature.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Evans_C/0/1/0/all/0/1\">Charles Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasirzadeh_A/0/1/0/all/0/1\">Atoosa Kasirzadeh</a>"
}