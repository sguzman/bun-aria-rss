{
  "title": "Fantastic Measures of Generalization — That Actually Work",
  "link": "https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/",
  "comments": "https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/#respond",
  "dc:creator": "Charles H Martin, PhD",
  "pubDate": "Sun, 17 Oct 2021 20:28:14 +0000",
  "category": "Uncategorized",
  "guid": "http://calculatedcontent.com/?p=14250",
  "description": "In the next few posts, I am going to discuss how to use the generalization metrics included in the open-source &#8230; <a class=\"more-link\" href=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/\">More</a>",
  "content:encoded": "\n<p>In the next few posts, I am going to discuss how to use the generalization metrics included in the open-source weightwatcher tool.  The goal is to develop a general-purpose tool can that you can use, among other things, to predict (tends in) the test accuracy of a Deep Neural Network — without access to the test data &#8212; or even training data!  </p>\n\n\n\n<p>WeightWatcher is a work in progress, based on research into Why Deep Learning Works, and has been featured in venures like ICML, <a rel=\"noreferrer noopener\" href=\"https://dl.acm.org/doi/10.1145/3292500.3332294\" target=\"_blank\">KDD</a>, <a rel=\"noreferrer noopener\" href=\"https://jmlr.org/papers/v22/20-410.html\" target=\"_blank\">JMLR</a> and <a rel=\"noreferrer noopener\" href=\"https://www.nature.com/articles/s41467-021-24025-8\" target=\"_blank\">Nature Communications.</a> </p>\n\n\n\n<p>Before we start, let me just say that it is very difficult to develop general-purpose metrics that can work for any arbitrary DNN (DNN), and, also, maintains a tool that is also back comparable with all of our work to be  100% reproducible. I hope the tool is useful to you, and we rely upon your feedback (positive and negative )  to improve the tool. so if it works for you, please let me.  If not, feel free to post an issue on <a rel=\"noreferrer noopener\" href=\"https://github.com/CalculatedContent/WeightWatcher\" target=\"_blank\">the Github site</a>.  Thanks again for the interest!</p>\n\n\n\n<p>Given this, the first metrics we will look at will be the</p>\n\n\n\n<h3>Random Distance Metrics</h3>\n\n\n\n<p>How is it even possible to predict the generalization accuracy of a DNN without even needing the test data ?  Or at least trends ?   The basic idea is simple; for each later weight matrix, measure how non-random it is.  After all, the more information the layer learns, the less random it should be.</p>\n\n\n\n<p>What are some choices for such a layer capacity metric:  <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{C}(&#92;mathbf{W})\" class=\"latex\" /></p>\n\n\n\n<ul><li>Matrix Entropy: <img src=\"https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29%3D-%5Csum_%7Bi%7D%5Clambda_%7Bi%7D%5Clog%5Clambda_%7Bi%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29%3D-%5Csum_%7Bi%7D%5Clambda_%7Bi%7D%5Clog%5Clambda_%7Bi%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29%3D-%5Csum_%7Bi%7D%5Clambda_%7Bi%7D%5Clog%5Clambda_%7Bi%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"S(&#92;mathbf{W})=-&#92;sum_{i}&#92;lambda_{i}&#92;log&#92;lambda_{i} \" class=\"latex\" /></li><li>Distance from the initial weight matrix: <img src=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D-%5Cmathbf%7BW%7D_%7Binit%7D%5CVert_%7BF%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D-%5Cmathbf%7BW%7D_%7Binit%7D%5CVert_%7BF%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D-%5Cmathbf%7BW%7D_%7Binit%7D%5CVert_%7BF%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;Vert&#92;mathbf{W}-&#92;mathbf{W}_{init}&#92;Vert_{F} \" class=\"latex\" /> (and variants of this)</li><li>DIvergence from the ESD of the randomized weight matrix: <img src=\"https://s0.wp.com/latex.php?latex=div%5B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7BW%7D_%7Brand%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=div%5B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7BW%7D_%7Brand%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=div%5B%5Cmathbf%7BW%7D%2C+%5Cmathbf%7BW%7D_%7Brand%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"div[&#92;mathbf{W}, &#92;mathbf{W}_{rand}]\" class=\"latex\" /></li></ul>\n\n\n\n<h4>Matrix Entropy</h4>\n\n\n\n<p>We define <img src=\"https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S%28%5Cmathbf%7BW%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"S(&#92;mathbf{W})\" class=\"latex\" /> in terms of the eigenvalues <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{i}\" class=\"latex\" /> of the layer correlation matrix </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}=&#92;frac{1}{N}&#92;mathbf{W}^{T}&#92;mathbf{W}\" class=\"latex\" />.  </p>\n\n\n\n<p>So the matrix entropy i is both a measure of layer randomness and a measure of correlation.</p>\n\n\n\n<p>In <a rel=\"noreferrer noopener\" href=\"https://jmlr.org/papers/volume22/20-410/20-410.pdf\" target=\"_blank\">our JMLR paper</a>, however, we show that while the Matrix Entropy does decrease during training, it is not particularly informative.  Still, I mention it here for completeness.   (And in the next post, I will discuss the Stable Rank; stay tuned)</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png\"><img data-attachment-id=\"14271\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/screen-shot-2021-10-17-at-9-30-30-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png\" data-orig-size=\"2202,1086\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2021-10-17 at 9.30.30 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=840\" alt=\"\" class=\"wp-image-14271\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=840 840w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=1680 1680w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-9.30.30-pm.png?w=1024 1024w\" sizes=\"(max-width: 840px) 100vw, 840px\" /></a></figure>\n\n\n\n<h4>Distance from Init:</h4>\n\n\n\n<p>The next metric to consider is the Frobenius norm of the difference between the layer weight matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> and it&#8217;s specific, initialized, random value <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}_{init}\" class=\"latex\" />.  Note, however, that this metric requires that you actually have the initial layer matrices (which we did not for our <a href=\"https://www.nature.com/articles/s41467-021-24025-8\" target=\"_blank\" rel=\"noreferrer noopener\">Nature paper</a>)</p>\n\n\n\n<p>The weightwatcher tool supports this metric with the distance method:</p>\n\n\n\n<p> <pre class=\"brush: python; title: ; notranslate\"> import weightwatcher as ww watcher = ww.watcher(model=your_model) <strong>distance_from_init</strong> = watcher.distances(your_model, init_model) </pre></p>\n\n\n<p>\nwhere init_model is your model, with the original, actual, initial weight matrices.\n</p>\n\n\n<p>In <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/pdf/2106.00734.pdf\" target=\"_blank\">our most recent paper,</a> we evaluated the <strong>distance_from_init</strong> method as a generalization metric in great detal, however, in order to cut the paper down to submit (which I really hate doing), we had to remove most of this discussion, and only a table in the appendix remained.  I may redo this paper, and revert it to the long form, at some point.  For now, I will just present some results from that study here, that are unpublished. </p>\n\n\n\n<p>These are the raw results for the task1 and task2 sets of models, described in the paper.  Breifly we were given about 100 pretrained models, group into 2 tasks (corresponding to 2 different architectures), and then subgrouped again (by the number of layers in each set).  Here, we see how the <strong>distance_from_init</strong> metric correlates against the given test accuracies&#8211;and it&#8217;s pretty good most of the time.  But&#8217;s not the best metric in general.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png\"><img data-attachment-id=\"14286\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/draft_distancemetric_10_0/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png\" data-orig-size=\"1045,702\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"draft_DistanceMetric_10_0\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=1024\" alt=\"\" class=\"wp-image-14286\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_0.png 1045w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png\"><img loading=\"lazy\" data-attachment-id=\"14288\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/draft_distancemetric_10_1/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png\" data-orig-size=\"703,702\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"draft_DistanceMetric_10_1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=703\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=703\" alt=\"\" class=\"wp-image-14288\" width=\"586\" height=\"585\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=586 586w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/draft_distancemetric_10_1.png 703w\" sizes=\"(max-width: 586px) 100vw, 586px\" /></a></figure></div>\n\n\n<p>The are a few variants of this distance metric, depending on how one defines the distance.  These include:</p>\n\n\n\n<ol><li>Frobenius norm distance.  </li><li>Cosine distance</li><li>CKA distance</li></ol>\n\n\n\n<p>Currently, weightwatcher 0.5.5 only supports (1), but in the next minor release, we plan to include both (2) & (3). </p>\n\n\n\n<p>The problem with this simple approach is it is not going to be useful if your models are overfit because the distance from init increases over time anyway&#8211;and this is exactly what we think is happening in the <strong>task1</strong> models from this NeurIPS contest. But it is a good sanity check on your models during training, and can be used with other metrics as a diagnostic indicator.</p>\n\n\n\n<p>So the question becomes, can we somehow create a distance-from-random metric that compensates for overfitting.  And that leads to&#8230;</p>\n\n\n\n<h4>Distance from Random: (rand_distance)</h4>\n\n\n\n<p>With the new <strong>rand_distance</strong> metric, we mean something very different from the <strong>distance_from_init. </strong> Above, we used the original instantiation of the <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Binit%7D%3B&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}_{init};\" class=\"latex\" />, and constructed an <em>element-wise distance</em> metric.    The <strong>rand_distance</strong> metric, in contrast:</p>\n\n\n\n<ul><li>Does not require the original, initial weight matrics</li><li>Is not an element-wise metric, but, instead, is a <strong>distributional metric</strong></li></ul>\n\n\n\n<p>So this metric is defined in terms of a distance between the distributions of the eigenvalues (i..e the ESDs) of the layer matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> and its random counterpart <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Brand%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Brand%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_%7Brand%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}_{rand}\" class=\"latex\" />. </p>\n\n\n\n<p>For weightwatcher, we choose to use the <a rel=\"noreferrer noopener\" href=\"https://medium.com/@sourcedexter/how-to-find-the-similarity-between-two-probability-distributions-using-python-a7546e90a08d\" target=\"_blank\">Jensen-Shannon divergence </a>for this:</p>\n\n\n<p><!-- /wp:paragraph --></p>\n<pre><pre class=\"brush: python; title: ; notranslate\">rand_distance =  jensen_shannon_distance(esd, random_esd)</pre></pre>\n<p>where</p>\n<p><!-- /wp:paragraph --></p>\n<pre><pre class=\"brush: python; title: ; notranslate\">\ndef jensen_shannon_distance(p, q):\n    m = (p + q) / 2\n    divergence = (sp.stats.entropy(p, m) + sp.stats.entropy(q, m)) / 2\n    distance = np.sqrt(divergence)\n    return distance\n</pre></pre>\n<p><!-- wp:paragraph --></p>\n<p>Moreover, there are 2 ways to construct the random layer matrix $\\mathbf{W}_{rand}&bg=ffffff$ metric:</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:list {\"ordered\":true} --></p>\n<ol>\n<li>Take any (Gaussian) Random Matrix, with the same aspect ratio <img src=\"https://s0.wp.com/latex.php?latex=Q%3D%5Cfrac%7BN%7D%7BM%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Q%3D%5Cfrac%7BN%7D%7BM%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Q%3D%5Cfrac%7BN%7D%7BM%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"Q=&#92;frac{N}{M}\" class=\"latex\" /> as the layer weight matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /></li>\n<li>Permute (shuffle) the elements of <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /></li>\n</ol>\n<p><!-- /wp:list --></p>\n<p><!-- wp:paragraph --></p>\n<p>While at first glance, these may seem the same, in fact, in practice, they can be quite different. This is because while every (Normal) Random Matrix has the same ESD (up to finite size effects), the Marchenko-Pastur (MP) distribution, if the actual <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> contains any usually large elements <img src=\"https://s0.wp.com/latex.php?latex=W_%7BIj%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=W_%7BIj%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=W_%7BIj%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"W_{Ij}\" class=\"latex\" /> , then it&#8217;s ESD will behave like a Heavy-Tailed Random Matrix, and look very different from it&#8217;s random MP counterpart. For more details, see <a href=\"https://www.jmlr.org/papers/volume22/20-410/20-410.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">our JMLR paper.</a></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>Indeed, when <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}\" class=\"latex\" /> contains usually large elements, we call these <strong>Correlation Traps. </strong>Now, I have conjectured, in an earlier post, that such Correlation Traps may be a indication of a layer being overtrained. However, some research suggests the opposite, and, that, in-fact, such large elements are needed for large, modern NLP models. The jury is still out on this, however, the weightwatcher tool can be used to resolve this question since it can easily identify such Correlation Traps in every layer. I look forward to seeing the final conclusion.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>The take-a-way here is that, when a layer is well trained, we expect the ESD of the layer to be significantly different from the ESD its randomized, shuffled form. Let&#8217;s compare 2 cases:</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:image {\"id\":14298,\"sizeSlug\":\"large\",\"linkDestination\":\"media\"} --></p>\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png\"><img data-attachment-id=\"14298\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/screen-shot-2021-10-17-at-11-47-59-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png\" data-orig-size=\"2002,960\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2021-10-17 at 11.47.59 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=1024\" class=\"wp-image-14298\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=1024\" alt=\"\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-17-at-11.47.59-pm.png 2002w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"></a></figure>\n<p><!-- /wp:image --></p>\n<p><!-- wp:paragraph --></p>\n<p style=\"text-align:center;\"><em>The <strong>rand_distance</strong> metric measure the divergence between the <strong><span style=\"color:#008000;\">original</span></strong> and <span style=\"color:#ff0000;\"><strong>random</strong></span> ESDs</em></p>\n<p>In case (a), the <span style=\"color:#008000;\"><strong>original ESD</strong> (green)</span> looks significantly different from its <span style=\"color:#ff0000;\"><strong>randomized ESD</strong> (red).</span>&nbsp; In case (b), however, the <span style=\"color:#008000;\"><strong>original</strong></span> the <span style=\"color:#ff0000;\"><strong>randomized</strong></span> ESDs are much more similar.&nbsp;So we expect case (a) to be more well trained than case (b).&nbsp; &nbsp;</p>\n<p style=\"text-align:left;\"><em>And <strong>rand_distance</strong> works, presumably, at least in some cases, when the layer is overtrained, as in (b)</em>.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>To compute the <strong>rand_distance</strong> metric, simply specify the randomize=True option, and it will be available as a layer metric in the details dataframe</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<pre><pre class=\"brush: python; title: ; notranslate\">\nimport weightwatcher as ww\nwatcher = ww.watcher(model=your_model)\ndetails = watcher.analyze(randomize=True)\navg_rand_distance = details.rand_distance.mean()\n</pre></pre>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>Finally, let&#8217;s see how well the <strong>rand_distance</strong> metric actually works to predict trends in the test accuracy for a well known set of pretrained models, the VGG series. Similar to the analysis in our Nature paper, we consider how well the <strong>average rand_distance</strong> metric is correlated with the reported top1 errors of the series of VGG models: VGG11, VGG13, VGG16, and VGG19</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:image {\"align\":\"center\",\"id\":14301,\"width\":411,\"height\":387,\"sizeSlug\":\"large\",\"linkDestination\":\"media\"} --></p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png\"><img loading=\"lazy\" data-attachment-id=\"14301\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/screen-shot-2021-10-13-at-9-06-01-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png\" data-orig-size=\"1374,1292\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2021-10-13 at 9.06.01 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=1024\" class=\"wp-image-14301 aligncenter\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=1024\" alt=\"\" width=\"411\" height=\"387\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=411 411w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=822 822w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-13-at-9.06.01-pm.png?w=768 768w\" sizes=\"(max-width: 411px) 100vw, 411px\"></a></figure>\n</div>\n<p><!-- /wp:image --></p>\n<p>Actually this is pretty good, and comparable to the results for the weightwatcher powerlaw metric weighted-alpha (<img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" />).&nbsp; But for reasons I don&#8217;t yet understand, however, it does not work so well for the VGG_BN models (VGG with BatchNormalization).&nbsp; Never-the-less, I am hoping it may be useful to you, and I would love to hear if it is working for you or not.&nbsp; To help get started, the above results can be reproduced using weightwatcher 0.5.5 using the <a href=\"https://github.com/CalculatedContent/WeightWatcher/blob/master/WWVGG-TestRandDistance.ipynb\">WWVGG-TestRandDistance.ipynb</a> Jupyter Notebook in the WeightWatcher GitHub repo.</p>\n<p><!-- wp:paragraph --></p>\n<p>I&#8217;ll end part 1 of this series of blog posts with a comparison between the new <strong>rand_distance</strong> metric and the weighwatcher <strong>alpha</strong> metric, for all the layers VGG19.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>(Note, I am not using the ww2x option for this analysis, but if you want to reproduce the Nature results, use ww2x=True. If you don&#8217;t , you may get crazy-large alphas that are incorrect&#8211;I will show in the next post I will discuss the alpha powerlaw (PL) estimates in more detail.)</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:image {\"align\":\"center\",\"id\":14307,\"width\":485,\"height\":327,\"sizeSlug\":\"large\",\"linkDestination\":\"media\"} --></p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png\"><img loading=\"lazy\" data-attachment-id=\"14307\" data-permalink=\"https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/screen-shot-2021-10-12-at-12-59-20-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png\" data-orig-size=\"1316,888\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2021-10-12 at 12.59.20 AM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=1024\" class=\"wp-image-14307\" src=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=1024\" alt=\"\" width=\"485\" height=\"327\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=485 485w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=970 970w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/10/screen-shot-2021-10-12-at-12.59.20-am.png?w=768 768w\" sizes=\"(max-width: 485px) 100vw, 485px\"></a></figure>\n</div>\n<p><!-- /wp:image --></p>\n<p><!-- wp:paragraph --></p>\n<p>When the weightwatcher <strong>alpha</strong> < 5, this means that the layers are Heavy Tailed and therefore well trained, and, as expected, alpha is correlated with the <strong>rand_distance</strong> metric. As expected!</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>I hope this has been useful to you and that you will try out the weightwather tool</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<pre><pre class=\"brush: bash; title: ; notranslate\">\npip install weightwatcher\n</pre></pre>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>Give it a try. And please give me feedback if it is useful. And, if interested in getting involved or just learning more, ping me to join our Slack channel. And if you need help with AI, reach out. #t<strong>alkToChuck #theAIguy </strong></p>\n<p><!-- /wp:paragraph --></p>",
  "wfw:commentRss": "https://calculatedcontent.com/2021/10/17/fantastic-measures-of-generalization-that-actually-work-part-1/feed/",
  "slash:comments": 0,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "Screen Shot 2021-10-13 at 9.06.01 PM"
    },
    {
      "media:title": "charlesmartin14"
    },
    "",
    "",
    "",
    "",
    "",
    ""
  ]
}