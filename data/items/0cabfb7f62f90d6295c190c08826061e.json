{
  "title": "How to check hypotheses with bootstrap and Apache Spark?",
  "link": "https://fullstackml.com/how-to-check-hypotheses-with-bootstrap-and-apache-spark-cd750775286a?source=rss----46e065078cc1---4",
  "guid": "https://medium.com/p/cd750775286a",
  "category": [
    "apache-spark",
    "experimental-design",
    "analytics",
    "design-of-experiments"
  ],
  "dc:creator": "Dmitry Petrov",
  "pubDate": "Tue, 19 Jan 2016 18:19:32 GMT",
  "atom:updated": "2017-03-06T05:00:01.037Z",
  "content:encoded": "<p>There is a featureI really like in Apache Spark. <strong>Spark can process data out of memory in my local machine even without a cluster.</strong> Good news for those who process data sets bigger than the memory size that currently have. From time to time, I have this issue when I work with hypothesis testing.</p><p><strong>For hypothesis testing I usually use statistical bootstrapping techniques. This method does not require any statistical knowledge and is very easy to understand.</strong> Also, this method is very simple to implement. There are no normal distributions and student distributions from your statistical courses, only some basic coding skills. Good news for those who doesn’t like statistics. Spark and bootstrapping is a very powerful combination which can help you check hypotheses in a large scale.</p><h3>1. Bootstrap methods</h3><p>The most common application with bootstrapping is calculating confidence intervals and you can use these confidence intervals as a part of the hypotheses checking process. There is a very simple idea behind bootstrapping — sample your data set size N for hundreds or even thousands times with the replacement (this is important) and calculate the estimated metrics for each of the hundreds\\thousands subset. This process gives you a histogram which is an <strong>actual distribution for your data</strong>. Then, you can use this actual distribution for hypothesis testing.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/328/1*iji5ktU9zy4TvdXM6DR4Dw.jpeg\" /></figure><p>The beauty of this method is the actual distribution histogram. In a classical statistical approach, you need to approximate a distribution of your data by normal distribution and calculate z-scores or student-scores based on theoretical distributions. With the actual distribution from the first step it is easy to calculate 2.5% percentile and 97.5% percentiles and this would be your actual confidence interval. That’s it! <strong>Confident interval with almost no math.</strong></p><h3>2. Choosing the right hypothesis</h3><p>Choosing right hypotheses is only the tricky part in this analytical process. This is a question you ask the data and you cannot automate that. Hypotheses testing is a part of the analytical process and isn’t usual for machine learning experts. <strong>In machine learning you ask an algorithm to build a model\\structure which is sometimes called hypothesis and you are looking for the best hypotheses which correlates your data and labels.</strong></p><p><strong>In the analytics process, knowing the correlation is not enough</strong>, you should know the hypothesis from the get-go and the question is — if the hypothesis is correct and what is your level of confidence.</p><p>If you have a correct hypotheses it is easy to check the hypotheses based on the bootstrapping approach. For example let’s try to check the hypothesis in which we take an average for some feature in your dataset that is equal to 30.0. We should start with a null hypothesis H0 which we try to reject and an alternative hypothesis H1:</p><p>H0: mean(A) == 30.0</p><p>H1: meanA() != 30.0</p><p>If we fail to reject H0 we will take this hypothesis as ground truth. That’s what we need. If we don’t — then we should come up with a better hypothesis (mean(A) == 40).</p><h3>3. Checking hypotheses</h3><p>For the hypotheses checking we can simply calculate the confidence interval for dataset A by sampling and calculating 95% confidence interval. If the interval does not contain 30.0 then your hypotheses H0 was rejected.</p><p>Obviously, this confident interval starts with 2.5% and ends 97.5% which gives us 95% of the items between this interval. In the sorted array of our observations we should find 2.5% and 97.5% percentiles: p1 and p2. If p1 <= 30.0 <= p2, then we weren’t able to reject H0. So, we can suppose that H0 is the truth.</p><h3>4. Apache Spark code</h3><p>Implementation of bootstrapping in this particular case is straight forward.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/f33045ce9b926f4c9946f47539a84af8/href\">https://medium.com/media/f33045ce9b926f4c9946f47539a84af8/href</a></iframe><p>Because I did not find any good open datasets for the large scale hypotheses testing problem, let’s use skewdata.csv dataset from the book <a href=\"http://www.amazon.com/gp/product/1118941098\">“Statistics: An Introduction Using R”</a>. You can find this dataset <a href=\"http://www.bio.ic.ac.uk/research/crawley/statistics/data/zipped.zip\">in this archive</a>. It is not perfect but will work in a pinch.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/527ccc001a336740b4034a6db759c625/href\">https://medium.com/media/527ccc001a336740b4034a6db759c625/href</a></iframe><p><strong>We have to understand the difference between \"filed to reject H0\" and \"proof H0\".</strong> A failing to reject a hypothesis gives you a pretty strong level of evidence that the hypothesis is correct and you can use this information in your decision making process but this is not an actual proof.</p><h3>5. Equal means code example</h3><p>Another type of hypotheses - check if the means of the two datasets are different. This leads us to the usual design of experiment questions - if you apply some change in your web system (user interface change for example) would your click rate change in a positive direction?</p><p>Let's create a hypothesis:</p><p><strong>H0: mean(A) == mean(B)</strong></p><p><strong>H1: mean(A) > mean(B)</strong></p><p>It is not easy to find H1 for this hypothesis which we can prove. Let's change this hypothesis around a little bit:</p><p><strong>Ho': mean(A-B) == 0</strong></p><p><strong>H1': mean(A-B) > 0</strong></p><p>Now we can try to reject H0'.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/6f598aa994ab7ad474b4f30eeabc9cab/href\">https://medium.com/media/6f598aa994ab7ad474b4f30eeabc9cab/href</a></iframe><p>Now we can try to reject H0'.</p><p>We should change 2.5% and 97.5% percentiles in the interval to 5% percentile in the left side only because of one-side (one-tailed) hypothesis testing. And an actual code as an example:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/1e1337199076e2423a43b61f7780e848/href\">https://medium.com/media/1e1337199076e2423a43b61f7780e848/href</a></iframe><h3>Conclusion</h3><p>Bootstrapping methods are very simple for understanding and implementation. They are intuitively simple and you don't need any deep knowledge of statistics. Apache Spark can help you implement these methods in a large scale.</p><p>As I mentioned previously it is not easy to find a good open large dataset for hypotheses testing. <strong>Please share with our community if you have one or come across one.</strong></p><p>My code is shared in <a href=\"https://www.dropbox.com/s/epxih5r152rwsl5/hypotheses.scala?dl=0\">this Scala file</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd750775286a\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://fullstackml.com/how-to-check-hypotheses-with-bootstrap-and-apache-spark-cd750775286a\">How to check hypotheses with bootstrap and Apache Spark?</a> was originally published in <a href=\"https://fullstackml.com\">FullStackML</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}