{
  "title": "Scale out your Pandas DataFrame operations using Dask",
  "description": "<p>In Pandas, one can easily apply operations on all the data using the apply method. However, this method is quite slow and is not useful when scaling up your methods. Is there a way to speed up these operations? And if so, how? Yes, there is! This blog post will</p>",
  "link": "https://www.data-blogger.com/scale-out-your-pandas-dataframe-operations-using-dask/",
  "guid": "622cfda9092f120001a24de5",
  "category": [
    "Python",
    "Big Data"
  ],
  "dc:creator": "Kevin Jacobs",
  "pubDate": "Sun, 05 Aug 2018 00:00:00 GMT",
  "media:content": "",
  "content:encoded": "<img src=\"https://images.unsplash.com/photo-1617839625591-e5a789593135?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDR8fGNoaXB8ZW58MHx8fHwxNjQ3MTE1NzIw&ixlib=rb-1.2.1&q=80&w=2000\" alt=\"Scale out your Pandas DataFrame operations using Dask\"><p>In Pandas, one can easily apply operations on all the data using the apply method. However, this method is quite slow and is not useful when scaling up your methods. Is there a way to speed up these operations? And if so, how? Yes, there is! This blog post will explain how you can use Dask to maximize the power of parallelization and to scale out your DataFrame operations.</p><h2 id=\"an-example\">An example</h2><p>As an example, consider the following: suppose we generate a collection of numbers. We generate 1,000,000 integer between 0 and 9999. Then, we turn the integers into a Pandas DataFrame since this is a common operation which is done in most data-related projects:</p><pre><code class=\"language-python\">import pandas as pd\nimport random\n\nif __name__ == '__main__':\n    # The amount of numbers to generate\n    N = 1000000\n\n    # Now generate 1,000,000 integers between 0 and 9999\n    data = [random.randint(0, 9999) for _ in range(N)]\n    \n    # And finally, create the DataFrame\n    df = pd.DataFrame(data, columns=['number'])</code></pre><p>The task is then to filter out all the integers smaller than 1000 (so 0 &#x2026; 999).</p><h2 id=\"how-fast-can-we-process-all-the-numbers\">How fast can we process all the numbers?</h2><p>We can check whether the numbers are small (< 1000) using the following code:</p><pre><code class=\"language-python\">import pandas as pd\nimport random\n\n\ndef is_small_number(x):\n    # Returns True if x is less than 1000, False otherwise\n    return x < 1000\n\n\nif __name__ == '__main__':\n    # The amount of numbers to generate\n    N = 1000000\n\n    # Now generate 1,000,000 integers between 0 and 9999\n    data = [random.randint(0, 9999) for _ in range(N)]\n\n    # And finally, create the DataFrame\n    df = pd.DataFrame(data, columns=['number'])\n\n    # Now apply the \"is_small_number\" operation to othe data\n    small_number_booleans = df.apply(is_small_number)</code></pre><p>Here, the final variable contains booleans whether the number is small enough (less than 1000). We can now use timeit to figure out the speed of the apply operation. We run timeit for 100 times using the following code:</p><pre><code class=\"language-python\">import pandas as pd\nfrom timeit import timeit\nimport random\n\n\ndef is_small_number(x):\n    # Returns True if x is less than 1000, False otherwise\n    return x < 1000\n\n\nif __name__ == '__main__':\n    # The amount of numbers to generate\n    N = 1000000\n\n    # Now generate 1,000,000 integers between 0 and 9999\n    data = [random.randint(0, 9999) for _ in range(N)]\n\n    # And finally, create the DataFrame\n    df = pd.DataFrame(data, columns=['number'])\n\n    # Now we can figure out the speed of the apply method\n    print('pandas apply method', timeit(lambda: df.apply(is_small_number), number=100))</code></pre><p>This resulted into the following outcome:</p><p>pandas apply method 1.4612023359952082</p><p>Not bad, approximately 1.5 seconds to process 1,000,000 numbers (a 100 times)! However, if you are dealing with heavy operations (such as tokenization) this number would be way larger. Then, parallelization is needed. But even in this small example, there is room for improvement.</p><h2 id=\"parallelization-to-the-rescue\">Parallelization to the Rescue</h2><p>The main problem with the apply method is that it gets executed on a single core. How can we do better? We could &#x201C;chunk&#x201D; the work into smaller subtasks. Suppose that multiple machines work on the different subtasks. In this way, the problem is solved way faster. Here, Dask comes to the rescue. Dask is used for scaling out your method. Instead of running your problem-solver on only one machine, Dask can even scale out to a cluster of machines. If you have only one machine, then Dask can scale out from one thread to multiple threads. First, we need to convert our Pandas DataFrame to a Dask DataFrame. Here, you will loose some flexibility. The Dask DataFrame does not support all the operations of a Pandas DataFrame. Luckily for us, we can convert easily from a Pandas DataFrame to a Dask DataFrame and back. Consider the following code in which our Pandas DataFrame is converted to a Dask DataFrame:</p><pre><code class=\"language-python\">import pandas as pd\nimport dask.dataframe as dd\nfrom timeit import timeit\nimport random\n\n\ndef is_small_number(x):\n    # Returns True if x is less than 1000, False otherwise\n    return x < 1000\n\n\nif __name__ == '__main__':\n    # The amount of numbers to generate\n    N = 1000000\n\n    # Now generate 1,000,000 integers between 0 and 9999\n    data = [random.randint(0, 9999) for _ in range(N)]\n\n    # And finally, create the DataFrame\n    df = pd.DataFrame(data, columns=['number'])\n\n    # Now convert it to a Dask DataFrame and chunk it into 4 partitions\n    df = dd.from_pandas(df, npartitions=4)\n\n    # Now we can figure out the speed of the apply method\n    print('dask parallel map partitions method', timeit(lambda: df.map_partitions(is_small_number).compute(), number=100))</code></pre><p>This resulted into the following:</p><p>dask parallel map partitions method 0.7870494624626411</p><p>That is, 0.78 seconds which means a speed-up of 1.8x! A few words here: the more partitions you use, the more overhead for setting up the threads. Be careful with the number of partitions. For small tasks like this, a low number of partitions works better. For larger tasks, it might be the case that a larger number of partitions works better. Also note that the interface is similar to the Apache Spark interface. The map-reduce concepts are also available in Dask and Dask builds a computation graph in the background. When the compute() method is called, the computation graph is executed. This will be explained in a later post on Dask. For now, it is interesting that you can speed-up your Pandas DataFrame apply method calls!</p><h2 id=\"conclusions\">Conclusions</h2><ul><li>You now know how Dask can scale out operations on your Pandas DataFrames.</li><li>In the simple example, we achieved a speed-up of 1.8x. This speed-up is way larger for heavy tasks and datasets.</li><li>Parallelization is key to faster computations on large amounts of data.</li></ul>"
}