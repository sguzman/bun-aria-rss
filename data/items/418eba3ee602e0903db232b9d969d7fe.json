{
  "title": "Native Hadoop file system (HDFS) connectivity in Python",
  "link": "",
  "published": "2017-01-03T08:00:00-08:00",
  "updated": "2017-01-03T08:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2017-01-03:/blog/python-hdfs-interfaces/",
  "summary": "<p>There have been many Python libraries developed for interacting with the Hadoop\nFile System, HDFS, via its WebHDFS gateway as well as its native Protocol\nBuffers-based RPC interface. I'll give you an overview of what's out there and\nshow some engineering I've been doing to offer a high performance HDFS\ninterface within the developing Arrow ecosystem.</p>\n<p>This blog is a follow up to my <a href=\"https://wesmckinney.com/blog/outlook-for-2017/\">2017 Roadmap</a> post.</p>",
  "content": "<p>There have been many Python libraries developed for interacting with the Hadoop\nFile System, HDFS, via its WebHDFS gateway as well as its native Protocol\nBuffers-based RPC interface. I'll give you an overview of what's out there and\nshow some engineering I've been doing to offer a high performance HDFS\ninterface within the developing Arrow ecosystem.</p>\n<p>This blog is a follow up to my <a href=\"https://wesmckinney.com/blog/outlook-for-2017/\">2017 Roadmap</a> post.</p>\n\n\n<h2>Hadoop file system protocols</h2>\n<p>HDFS is a part of <a href=\"https://github.com/apache/hadoop\">Apache Hadoop</a>, and its design was originally based on\nthe Google File System described in the original MapReduce paper. Its native\nwire protocol uses's Google Protocol Buffers (or \"protobufs\" for short) for\nremote procedure calls, or RPCs.</p>\n<p>Traditionally, systems that talk to HDFS, like the main Java client library,\nwould implement the Protobuf messaging format and RPC protocol. To make it\neasier for light load applications to read and write files, WebHDFS was\ndeveloped to provide an HTTP or HTTPS gateway to make PUT and GET requests\ninstead of protobuf RPCs.</p>\n<p>For light load applications, WebHDFS and native protobuf RPCs provide\ncomparable data throughput, but native connectivity is generally considered to\nbe more scalable and suitable for production use.</p>\n<p>Python has two WebHDFS interfaces that I've used:</p>\n<ul>\n<li><a href=\"http://pywebhdfs.org/\">pywebhdfs</a></li>\n<li><a href=\"https://hdfscli.readthedocs.io/en/latest/\">hdfscli</a></li>\n</ul>\n<p>The rest of this article will focus instead on native RPC client interfaces.</p>\n<h2>Native RPC access in Python</h2>\n<p>The \"official\" way in Apache Hadoop to connect natively to HDFS from a\nC-friendly language like Python is to use <a href=\"https://wiki.apache.org/hadoop/LibHDFS\"><strong>libhdfs</strong></a>, a JNI-based C\nwrapper for the HDFS Java client. A primary benefit of libhdfs is that it is\ndistributed and supported by major Hadoop vendors, and it's a part of the\nApache Hadoop project. A downside is that it uses JNI (spawning a JVM within a\nPython process) and requires a complete Hadoop Java distribution on the client\nside. Some clients find this unpalatable and don't necessarily require the\nproduction-level support that other applications require. For example, Apache\nImpala (incubating), a C++ application, uses libhdfs to access data in HDFS.</p>\n<p>Due to the heavier-weight nature of libhdfs, alternate native interfaces to\nHDFS have been developed.</p>\n<ul>\n<li>\n<p><a href=\"https://github.com/Pivotal-Data-Attic/pivotalrd-libhdfs3\"><strong>libhdfs3</strong></a>, now part of Apache HAWQ (incubating), a pure C++ library\n  developed by Pivotal Labs for use in the HAWQ SQL-on-Hadoop\n  system. Conveniently, libhdfs3 is very nearly interchangeable for libhdfs at\n  the C API level. At one time it seemed that libhdfs3 might become a part of\n  Apache Hadoop officially, but that does not now seem likely (see\n  <a href=\"https://issues.apache.org/jira/browse/HDFS-8707\">HDFS-8707</a>, a new C++ client in development).</p>\n</li>\n<li>\n<p><a href=\"https://github.com/spotify/snakebite\"><strong>snakebite</strong></a>: a pure Python implementation of Hadoop's protobuf RPC\n  interface, created by Spotify.</p>\n</li>\n</ul>\n<p>Since snakebite does not offer a comprehensive client API (e.g. it cannot write\nfiles) and has worse performance (being implemented in pure Python), I'll focus\non libhdfs and libhdfs3 going forward.</p>\n<h2>Python interfaces to libhdfs and libhdfs3</h2>\n<p>There have been a number of prior efforts to build C-level interfaces to the\nlibhdfs JNI library. These includes <a href=\"https://pypi.python.org/pypi/cyhdfs\">cyhdfs</a> (using Cython), <a href=\"https://github.com/jdowner/libpyhdfs\">libpyhdfs</a>\n(plain Python C extension), and <a href=\"https://github.com/vbarter/pyhdfs\">pyhdfs</a> (using SWIG). One of the challenges\nwith building a C extension to libhdfs is that the <code>libhdfs.so</code> shared library\nis distributed with the Hadoop distribution, so you must properly configure the\n<code>$LD_LIBRARY_PATH</code> so that the shared library can be loaded. Additionally, the\nJVM's <code>libjvm.so</code> must also be loaded at import time. Combined, these lead to\nsome \"configuration hell\".</p>\n<p>When looking to build a C++ HDFS interface for use in Apache Arrow (and Python\nvia PyArrow), I discovered the <a href=\"https://github.com/turi-code/SFrame/blob/master/oss_src/fileio/libhdfs_shim.cpp\">libhdfs implementation</a> in Turi's SFrame\nproject. This takes the clever approach of discovering and loading both the JVM\nand libhdfs libraries at runtime. I adapted this approach for use in Arrow, and\nit has worked out nicely. This implementation provides very low-overhead IO to\nArrow data serialization tools (like Apache Parquet), and convenient Python\nfile interface.</p>\n<p>Because the libhdfs and libhdfs3 driver libraries have very nearly the same C\nAPI, we can switch between one driver and the other with a keyword argument in\nPython:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">pyarrow</span> <span class=\"kn\">import</span> <span class=\"n\">HdfsClient</span>\n\n<span class=\"c1\"># Using libhdfs</span>\n<span class=\"n\">hdfs</span> <span class=\"o\">=</span> <span class=\"n\">HdfsClient</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"p\">,</span> <span class=\"n\">username</span><span class=\"p\">,</span> <span class=\"n\">driver</span><span class=\"o\">=</span><span class=\"s1\">&#39;libhdfs&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Using libhdfs3</span>\n<span class=\"n\">hdfs_alt</span> <span class=\"o\">=</span> <span class=\"n\">HdfsClient</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"p\">,</span> <span class=\"n\">username</span><span class=\"p\">,</span> <span class=\"n\">driver</span><span class=\"o\">=</span><span class=\"s1\">&#39;libhdfs3&#39;</span><span class=\"p\">)</span>\n\n<span class=\"k\">with</span> <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;/path/to/file&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span>\n</code></pre></div>\n\n<p>In parallel, the <a href=\"https://github.com/dask/dask\">Dask</a> project developers created <a href=\"https://github.com/dask/hdfs3\"><strong>hdfs3</strong></a>, a pure\nPython interface to libhdfs3 that uses <a href=\"http://python.net/crew/theller/ctypes/\">ctypes</a> to avoid C extensions. It\nprovides a Python file interface and access to the rest of the libhdfs3\nfunctionality:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">from</span> <span class=\"nn\">hdfs3</span> <span class=\"kn\">import</span> <span class=\"n\">HDFileSystem</span>\n\n<span class=\"n\">hdfs</span> <span class=\"o\">=</span> <span class=\"n\">HDFileSystem</span><span class=\"p\">(</span><span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"p\">,</span> <span class=\"n\">user</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;/path/to/file&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rb&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"o\">...</span>\n</code></pre></div>\n\n<h2>pyarrow.HdfsClient and hdfs3 data access performance</h2>\n<p>Running against a local CDH 5.6.0 HDFS cluster, I computed ensemble average\nperformance in a set of file reads of various sizes from 4 KB to 100 MB under 3\nconfigurations:</p>\n<ul>\n<li><code>hdfs3</code> (which always uses libhdfs3)</li>\n<li><code>pyarrow.HdfsClient</code> using <code>driver='libhdfs'</code></li>\n<li><code>pyarrow.HdfsClient</code> using <code>driver='libhdfs3'</code></li>\n</ul>\n<p>You can obtain all of these packages by running:</p>\n<div class=\"github\"><pre><span></span><code>conda install pyarrow hdfs3 libhdfs3 -c conda-forge\n</code></pre></div>\n\n<blockquote>\n<p>Note: pyarrow conda-forge packages are currently only available for Linux; in\n  theory this will be resolved by January 20, 2017. If anyone would like to\n  help with Windows support, let me know.</p>\n</blockquote>\n<p>Performance numbers are in megabytes per second (\"throughput\"). Benchmarking\ncode is shown at the end of the post. I am very curious about results in more\ndiverse production environments and Hadoop configurations.</p>\n<p><center>\n<img src=\"../../images/libhdfs_perf_linear.png\" alt=\"HDFS RPC data perf\"/>\n</center></p>\n<p>Curiously, at least in my testing, I found these results:</p>\n<ul>\n<li>libhdfs, despite being Java and JNI-based, achieves the best throughput in\n  this test.</li>\n<li>libhdfs3 performs poorly in small size reads. This may be due to some RPC\n  latency or configuration issues that I am not aware of.</li>\n<li>Strictly comparing libhdfs3, pyarrow outperforms hdfs3 by a 10-15% margin; I\n  expect this is primarily due to memory handling / copying based on the ctypes\n  (hdfs3) vs. C++ (pyarrow) implementation.</li>\n</ul>\n<p>Here are the timings with a logarithmic axis:</p>\n<p><center>\n<img src=\"../../images/libhdfs_perf_log.png\" alt=\"HDFS RPC data perf\"/>\n</center></p>\n<h2>Native C++ IO in Apache Arrow</h2>\n<p>One of the reasons for bunding IO interfaces like HDFS in the pyarrow library\nis that they all utilize a common memory management layer that enables data to\nbe passed around with very low (and sometimes zero) copying overhead. By\ncomparison, libraries that expose only a Python file interface introduce some\namount of overhead because memory is being handled by bytes objects in the\nPython interpreter.</p>\n<p>Full details of the Arrow's C++ IO system are out of the scope of this article,\nbut I'll write a future blog post taking a deeper dive into the details.</p>\n<h3>Benchmarking code</h3>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">gc</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pyarrow</span> <span class=\"k\">as</span> <span class=\"nn\">pa</span>\n<span class=\"kn\">import</span> <span class=\"nn\">hdfs3</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"k\">as</span> <span class=\"nn\">sns</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"n\">DATA_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">200</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">&lt;&lt;</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;a&#39;</span> <span class=\"o\">*</span> <span class=\"n\">DATA_SIZE</span>\n\n<span class=\"n\">hdfs</span> <span class=\"o\">=</span> <span class=\"n\">pa</span><span class=\"o\">.</span><span class=\"n\">HdfsClient</span><span class=\"p\">(</span><span class=\"s1\">&#39;localhost&#39;</span><span class=\"p\">,</span> <span class=\"mi\">20500</span><span class=\"p\">,</span> <span class=\"s1\">&#39;wesm&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">hdfscpp</span> <span class=\"o\">=</span> <span class=\"n\">pa</span><span class=\"o\">.</span><span class=\"n\">HdfsClient</span><span class=\"p\">(</span><span class=\"s1\">&#39;localhost&#39;</span><span class=\"p\">,</span> <span class=\"mi\">20500</span><span class=\"p\">,</span> <span class=\"s1\">&#39;wesm&#39;</span><span class=\"p\">,</span> <span class=\"n\">driver</span><span class=\"o\">=</span><span class=\"s1\">&#39;libhdfs3&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">hdfs3_fs</span> <span class=\"o\">=</span> <span class=\"n\">hdfs3</span><span class=\"o\">.</span><span class=\"n\">HDFileSystem</span><span class=\"p\">(</span><span class=\"s1\">&#39;localhost&#39;</span><span class=\"p\">,</span> <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"mi\">20500</span><span class=\"p\">,</span> <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"s1\">&#39;wesm&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">delete</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n<span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;/tmp/test-data-file-1&#39;</span>\n<span class=\"k\">with</span> <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"s1\">&#39;wb&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">read_chunk</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"p\">):</span>\n    <span class=\"c1\"># do a random seek</span>\n    <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">ensemble_average</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"p\">,</span> <span class=\"n\">niter</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"n\">start</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">clock</span><span class=\"p\">()</span>\n    <span class=\"n\">gc</span><span class=\"o\">.</span><span class=\"n\">disable</span><span class=\"p\">()</span>\n    <span class=\"n\">data_chunks</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">niter</span><span class=\"p\">):</span>\n        <span class=\"n\">data_chunks</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"p\">())</span>\n    <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">clock</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">start</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">niter</span>\n    <span class=\"n\">gc</span><span class=\"o\">.</span><span class=\"n\">enable</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"n\">elapsed</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">make_test_func</span><span class=\"p\">(</span><span class=\"n\">fh</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">runner</span><span class=\"p\">():</span>\n        <span class=\"k\">return</span> <span class=\"n\">read_chunk</span><span class=\"p\">(</span><span class=\"n\">fh</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">runner</span>\n\n<span class=\"n\">KB</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>\n<span class=\"n\">MB</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span> <span class=\"o\">*</span> <span class=\"n\">KB</span>\n<span class=\"n\">chunksizes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">4</span> <span class=\"o\">*</span> <span class=\"n\">KB</span><span class=\"p\">,</span> <span class=\"n\">MB</span><span class=\"p\">,</span> <span class=\"mi\">10</span> <span class=\"o\">*</span> <span class=\"n\">MB</span><span class=\"p\">,</span> <span class=\"mi\">100</span> <span class=\"o\">*</span> <span class=\"n\">MB</span><span class=\"p\">]</span>\n<span class=\"n\">iterations</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n\n<span class=\"n\">handles</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"p\">(</span><span class=\"s1\">&#39;pyarrow&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;libhdfs&#39;</span><span class=\"p\">):</span> <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">&#39;pyarrow&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;libhdfs3&#39;</span><span class=\"p\">):</span> <span class=\"n\">hdfscpp</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s1\">&#39;hdfs3&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;libhdfs3&#39;</span><span class=\"p\">):</span> <span class=\"n\">hdfs3_fs</span><span class=\"o\">.</span><span class=\"n\">open</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rb&#39;</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">timings</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">,</span> <span class=\"n\">driver</span><span class=\"p\">),</span> <span class=\"n\">handle</span> <span class=\"ow\">in</span> <span class=\"n\">handles</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">chunksize</span><span class=\"p\">,</span> <span class=\"n\">niter</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">chunksizes</span><span class=\"p\">,</span> <span class=\"n\">iterations</span><span class=\"p\">):</span>\n        <span class=\"n\">tester</span> <span class=\"o\">=</span> <span class=\"n\">make_test_func</span><span class=\"p\">(</span><span class=\"n\">handle</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"p\">)</span>\n        <span class=\"n\">timing</span> <span class=\"o\">=</span> <span class=\"n\">ensemble_average</span><span class=\"p\">(</span><span class=\"n\">tester</span><span class=\"p\">,</span> <span class=\"n\">niter</span><span class=\"o\">=</span><span class=\"n\">niter</span><span class=\"p\">)</span>\n        <span class=\"n\">throughput</span> <span class=\"o\">=</span> <span class=\"n\">chunksize</span> <span class=\"o\">/</span> <span class=\"n\">timing</span>\n\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">library</span><span class=\"p\">,</span> <span class=\"n\">driver</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"p\">,</span> <span class=\"n\">timing</span><span class=\"p\">,</span> <span class=\"n\">throughput</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n        <span class=\"n\">timings</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"o\">.</span><span class=\"n\">from_records</span><span class=\"p\">(</span><span class=\"n\">timings</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;library&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;driver&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;read_size&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;timing&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;throughput&#39;</span><span class=\"p\">])</span>\n<span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">&#39;MB/s&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">&#39;throughput&#39;</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">MB</span>\n<span class=\"n\">results</span>\n<span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">&#39;type&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">&#39;library&#39;</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s1\">&#39;+&#39;</span> <span class=\"o\">+</span> <span class=\"n\">results</span><span class=\"p\">[</span><span class=\"s1\">&#39;driver&#39;</span><span class=\"p\">]</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))</span>\n<span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">factorplot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"s1\">&#39;read_size&#39;</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"s1\">&#39;MB/s&#39;</span><span class=\"p\">,</span> <span class=\"n\">hue</span><span class=\"o\">=</span><span class=\"s1\">&#39;type&#39;</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">results</span><span class=\"p\">,</span> <span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"s1\">&#39;bar&#39;</span><span class=\"p\">,</span> <span class=\"n\">orient</span><span class=\"o\">=</span><span class=\"s1\">&#39;h&#39;</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">g</span><span class=\"o\">.</span><span class=\"n\">despine</span><span class=\"p\">(</span><span class=\"n\">left</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"c1\">#g.fig.get_axes()[0].set_xscale(&#39;log&#39;, basex=2)</span>\n<span class=\"n\">g</span><span class=\"o\">.</span><span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">set_size_inches</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"s1\">&#39;results2.png&#39;</span><span class=\"p\">)</span>\n</code></pre></div>"
}