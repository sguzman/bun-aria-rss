{
  "title": "New York Times Article Search API to MongoDB",
  "link": "",
  "id": "http://brooksandrew.github.io/simpleblog/articles/new-york-times-api-to-mongodb",
  "published": "2015-01-06T00:00:00+00:00",
  "updated": "2015-01-06T00:00:00+00:00",
  "author": {
    "name": "andrew brooks",
    "uri": "http://brooksandrew.github.io/simpleblog",
    "email": "andrewbrooksct@gmail.com"
  },
  "content": "<ul id=\"markdown-toc\">\n  <li><a href=\"#motivation\" id=\"markdown-toc-motivation\">Motivation</a></li>\n  <li><a href=\"#accessing-nyt-api\" id=\"markdown-toc-accessing-nyt-api\">Accessing NYT API</a></li>\n  <li><a href=\"#extracting-and-parsing-the-article-body-text\" id=\"markdown-toc-extracting-and-parsing-the-article-body-text\">Extracting and parsing the article body text</a></li>\n  <li><a href=\"#writing-to-mongodb\" id=\"markdown-toc-writing-to-mongodb\">Writing to MongoDB</a></li>\n  <li><a href=\"#pipeline\" id=\"markdown-toc-pipeline\">Pipeline</a></li>\n  <li><a href=\"#results\" id=\"markdown-toc-results\">Results</a></li>\n</ul>\n\n<h2 id=\"motivation\">Motivation</h2>\n\n<p>I’ve learned a little about a lot of different corners of the text mining and NLP world over the last few years… which sometimes makes me feel like I know nothing for certain.  I’ve done a decent amount web scraping, processing HTML and parsing text recently, but never a full blown text mining project.  I decided to start with some topic modeling using Latent Dirichlet Allocation and document clustering.   Unsupervised learning techniques requiring minimal upfront work beyond the text pre-processing seemed like a good (and interesting) place to get started.</p>\n\n<p>After surveying APIs for a few news sources, the New York Times seemed to be the most robust.  I wanted the flexibility to acquire new documents in the future for testing models and from far enough in the past to build a hefty corpus.  I also wanted to have some fun and scrape the documents myself, experiment with a NoSQL database pipeline and process the HTML from the rawest form.</p>\n\n<h2 id=\"accessing-nyt-api\">Accessing NYT API</h2>\n\n<h5 id=\"api-documentation-and-keys\">API Documentation and keys</h5>\n<p>The <a href=\"http://developer.nytimes.com/docs/read/article_search_api_v2\">New York Times API</a> is well documented and user-friendly.  I didn’t experiment too much with targeted querying since I was pulling all articles over a period of time.  However the <code class=\"highlighter-rouge\">q</code> and <code class=\"highlighter-rouge\">fq</code> parameters seem to provide a lot of functionality for filtered searches.</p>\n\n<p>Requesting an a key for the Article Search API was easy and instantaneous.  I saved my key as a global parameter using R options.  I’m using <code class=\"highlighter-rouge\">sample-key</code> here simply for illustrative purposes, but I found that this key (provided by default in the <a href=\"http://developer.nytimes.com/io-docs\">NYT API Console</a>) actually works.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">options</span><span class=\"p\">(</span><span class=\"n\">nyt_as_key</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">'sample-key'</span><span class=\"p\">)</span><span class=\"w\">  </span><span class=\"c1\">## copy and paste your API key here.</span><span class=\"w\">\n</span><span class=\"n\">options</span><span class=\"p\">()</span><span class=\"o\">$</span><span class=\"n\">nyt_as_key</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## [1] \"sample-key\"</code></pre></figure>\n\n<p>The <a href=\"http://developer.nytimes.com/io-docs\">NYT API Console</a> is also a nifty way to kick the tires and understand the parameters at your disposal for constructing queries.  It’s basically a GUI around the API which gives you everything you need on one page to quickly formulate a query, submit it and inspect the response.  It also allowed me to quickly determine that this is a well developed and documented API worthy of pursuing further.  I wish all APIs were like this… sigh.</p>\n\n<h5 id=\"generate-url\">Generate URL</h5>\n\n<p>This is the easy part.  I did find an R package <a href=\"https://github.com/ropengov/rtimes\">rtimes</a> for accessing the API which worked for the few queries I tried.  However, I had already started building my own pipeline, so I stuck with it.</p>\n\n<p>The NYT API returns only 10 articles per request.  Which 10 are dictated by the <code class=\"highlighter-rouge\">page</code> parameter.  <code class=\"highlighter-rouge\">page=0</code> returns articles 1-10, <code class=\"highlighter-rouge\">page=1</code> returns articles 11-20, etc.  The tricky part is knowing how many pages to iterate through.  On my first pass, I failed to find a way to figure out the total number of articles matching my query to tell me how many pages and API requests I would need.  So instead, I simply started my requests with <code class=\"highlighter-rouge\">page=0</code> and incrementally added 1 to <code class=\"highlighter-rouge\">page</code> until the response stopped yielding articles.</p>\n\n<p>However, after I already maxed out my database, I found a way to do this using the <code class=\"highlighter-rouge\">facet_field</code> and <code class=\"highlighter-rouge\">facet_filter</code> parameters.  These can be used to count the number of articles matching a filtered query by simply adding <code class=\"highlighter-rouge\">&amp;facet_field=source&amp;facet_filter=true</code> to your query URL.</p>\n\n<p>And you get something like this:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"p\">{</span><span class=\"w\">  \n   </span><span class=\"s2\">\"facets\"</span><span class=\"o\">:</span><span class=\"p\">{</span><span class=\"w\">  \n      </span><span class=\"s2\">\"source\"</span><span class=\"o\">:</span><span class=\"p\">{</span><span class=\"w\">  \n         </span><span class=\"s2\">\"terms\"</span><span class=\"o\">:</span><span class=\"p\">[</span><span class=\"w\">  \n            </span><span class=\"p\">{</span><span class=\"w\">  \n               </span><span class=\"s2\">\"term\"</span><span class=\"o\">:</span><span class=\"s2\">\"The New York Times\"</span><span class=\"p\">,</span><span class=\"w\">\n               </span><span class=\"s2\">\"count\"</span><span class=\"o\">:</span><span class=\"m\">159</span><span class=\"w\">\n            </span><span class=\"p\">},</span><span class=\"w\">\n            </span><span class=\"p\">{</span><span class=\"w\">  \n               </span><span class=\"s2\">\"term\"</span><span class=\"o\">:</span><span class=\"s2\">\"International Herald Tribune\"</span><span class=\"p\">,</span><span class=\"w\">\n               </span><span class=\"s2\">\"count\"</span><span class=\"o\">:</span><span class=\"m\">7</span><span class=\"w\">\n            </span><span class=\"p\">},</span><span class=\"w\">\n            </span><span class=\"p\">{</span><span class=\"w\">  \n               </span><span class=\"s2\">\"term\"</span><span class=\"o\">:</span><span class=\"s2\">\"\"</span><span class=\"p\">,</span><span class=\"w\">\n               </span><span class=\"s2\">\"count\"</span><span class=\"o\">:</span><span class=\"m\">1</span><span class=\"w\">\n            </span><span class=\"p\">}</span><span class=\"w\">\n         </span><span class=\"p\">]</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n   </span><span class=\"p\">},</span><span class=\"w\">\n   </span><span class=\"s2\">\"status\"</span><span class=\"o\">:</span><span class=\"s2\">\"OK\"</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"s2\">\"copyright\"</span><span class=\"o\">:</span><span class=\"s2\">\"Copyright (c) 2013 The New York Times Company.  All Rights Reserved.\"</span><span class=\"w\">\n</span><span class=\"p\">}</span></code></pre></figure>\n\n<p>Here’s an example using R and httr to get the same result<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote\">1</a></sup>.  These counts almost perfectly match the counts I’ve calculated from my collection for the New York Times and International Herald Tribune.  However, I’ve collected a lot of Reuters and AP articles from the Article Search API that interestingly don’t appear here.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'httr'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">resp</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">GET</span><span class=\"p\">(</span><span class=\"s1\">'http://api.nytimes.com/svc/search/v2/articlesearch.json?facet_field=source&amp;facet_filter=true&amp;begin_date=20130105&amp;end_date=20130105&amp;api-key=sample-key'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"p\">(</span><span class=\"n\">resp</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'parsed'</span><span class=\"p\">)</span><span class=\"o\">$</span><span class=\"n\">response</span><span class=\"o\">$</span><span class=\"n\">facets</span><span class=\"p\">)</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## $source\n## $source$terms\n## $source$terms[[1]]\n## $source$terms[[1]]$term\n## [1] \"The New York Times\"\n## \n## $source$terms[[1]]$count\n## [1] 159\n## \n## \n## $source$terms[[2]]\n## $source$terms[[2]]$term\n## [1] \"International Herald Tribune\"\n## \n## $source$terms[[2]]$count\n## [1] 7\n## \n## \n## $source$terms[[3]]\n## $source$terms[[3]]$term\n## [1] \"\"\n## \n## $source$terms[[3]]$count\n## [1] 1</code></pre></figure>\n\n<p>Another rub is that “pagination beyond page 100 is not allowed at this time.” So if you’re extracting large quantities of articles, best do it chunks.  I chunked my queries into single days – usually returning about 700-800 articles.  Without using any facets or query terms, I ran the risk of only collecting 1000 articles for a day when there were more than 1000.  This occurred 12 days out of 120 for me.</p>\n\n<p><code class=\"highlighter-rouge\">makeURL</code> is a pretty trivial function that generates the URL to make the GET request from a collection of NYT API parameters.  However, I found encapsulating this step in a function which gets called in subsequent functions kept things clean.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">makeURL</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">fq</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">begin_date</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">end_date</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">getOption</span><span class=\"p\">(</span><span class=\"s2\">\"nyt_as_key\"</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">page</span><span class=\"o\">=</span><span class=\"m\">0</span><span class=\"p\">,</span><span class=\"w\"> \n                    </span><span class=\"n\">sort</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">fl</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">hl</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">facet_field</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">facet_filter</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">){</span><span class=\"w\">\n  </span><span class=\"n\">arglist</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">=</span><span class=\"n\">q</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">fq</span><span class=\"o\">=</span><span class=\"n\">fq</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">begin_date</span><span class=\"o\">=</span><span class=\"n\">begin_date</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">end_date</span><span class=\"o\">=</span><span class=\"n\">end_date</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'api-key'</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">page</span><span class=\"o\">=</span><span class=\"n\">page</span><span class=\"p\">,</span><span class=\"w\">\n                  </span><span class=\"n\">sort</span><span class=\"o\">=</span><span class=\"n\">sort</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">fl</span><span class=\"o\">=</span><span class=\"n\">fl</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">hl</span><span class=\"o\">=</span><span class=\"n\">hl</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">facet_field</span><span class=\"o\">=</span><span class=\"n\">facet_field</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">facet_filter</span><span class=\"o\">=</span><span class=\"n\">facet_filter</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">url</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"s1\">'http://api.nytimes.com/svc/search/v2/articlesearch.json?'</span><span class=\"w\">\n  </span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">arglist</span><span class=\"p\">)){</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">is.null</span><span class=\"p\">(</span><span class=\"n\">unlist</span><span class=\"p\">(</span><span class=\"n\">arglist</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span><span class=\"o\">==</span><span class=\"nb\">F</span><span class=\"p\">){</span><span class=\"w\">\n      </span><span class=\"n\">url</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'&amp;'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">names</span><span class=\"p\">(</span><span class=\"n\">arglist</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]),</span><span class=\"w\"> </span><span class=\"s1\">'='</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">arglist</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"nf\">return</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n\n</span><span class=\"c1\">## example: generate query which will return all articles for one day.</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'httr'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">url</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">makeURL</span><span class=\"p\">(</span><span class=\"n\">begin_date</span><span class=\"o\">=</span><span class=\"s1\">'20130101'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">end_date</span><span class=\"o\">=</span><span class=\"s1\">'20130102'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">key</span><span class=\"o\">=</span><span class=\"s1\">'sample-key'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">page</span><span class=\"o\">=</span><span class=\"m\">100</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## [1] \"http://api.nytimes.com/svc/search/v2/articlesearch.json?&amp;begin_date=20130101&amp;end_date=20130102&amp;api-key=sample-key&amp;page=100\"</code></pre></figure>\n\n<h5 id=\"scrape-nyt-metadata\">Scrape NYT metadata</h5>\n\n<p>Here in <code class=\"highlighter-rouge\">getMeta</code> we’re actually collecting the NYT article metadata and structuring it as a list object.  Like most things, the meat of it is pretty simple.  The rest is parsing, exception and error handling… which I find are worth it with web scraping, especially if you’re running a job overnight and want it to work by the time you wake up.</p>\n\n<p><code class=\"highlighter-rouge\">getMeta</code>:</p>\n\n<ul>\n  <li>DOES iterate through <code class=\"highlighter-rouge\">pages</code> until no new articles are returned.</li>\n  <li>DOES sleep for <code class=\"highlighter-rouge\">sleep</code> seconds after each request.  I had good luck with <code class=\"highlighter-rouge\">sleep=0.1</code>.</li>\n  <li>DOES re-attempt failed requests <code class=\"highlighter-rouge\">tryn</code> times.  Failed requests were almost always successful after the second try, so I set <code class=\"highlighter-rouge\">tryn=3</code> just to be safe.</li>\n  <li>DOES NOT cache responses to disc.  Everything is saved in memory and returned as a list in R memory after the function is complete.  I opted not to bother with caching with this step as it only takes ~20 seconds to run through 100 API calls (the max per query).</li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'httr'</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">getMeta</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">pages</span><span class=\"o\">=</span><span class=\"kc\">Inf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sleep</span><span class=\"o\">=</span><span class=\"m\">0.1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">tryn</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">art</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">()</span><span class=\"w\">\n  </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\">\n  </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">seq</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">tryn</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"n\">tryn</span><span class=\"o\">/</span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">length.out</span><span class=\"o\">=</span><span class=\"n\">tryn</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># initialize list of failed pages with arbitrary negative numbers</span><span class=\"w\">\n  </span><span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"o\">&lt;=</span><span class=\"n\">pages</span><span class=\"p\">){</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">[(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"n\">tryn</span><span class=\"m\">-1</span><span class=\"p\">))</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)]))</span><span class=\"o\">==</span><span class=\"m\">1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"m\">+1</span><span class=\"w\"> </span><span class=\"c1\">## attempt tryn times before moving on</span><span class=\"w\">\n    </span><span class=\"n\">tryget</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">try</span><span class=\"p\">({</span><span class=\"w\">\n      </span><span class=\"n\">urlp</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">gsub</span><span class=\"p\">(</span><span class=\"s1\">'page=\\\\d+'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"s1\">'page='</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">url</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## get the next page</span><span class=\"w\">\n      </span><span class=\"n\">p</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">GET</span><span class=\"p\">(</span><span class=\"n\">urlp</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## actually make GET request</span><span class=\"w\">\n      </span><span class=\"n\">pt</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'parsed'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## retrieve contents of response formatted in a nested R list</span><span class=\"w\">\n      </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">pt</span><span class=\"o\">$</span><span class=\"n\">response</span><span class=\"o\">$</span><span class=\"n\">docs</span><span class=\"p\">)</span><span class=\"o\">&gt;</span><span class=\"m\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">art</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">art</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">pt</span><span class=\"o\">$</span><span class=\"n\">response</span><span class=\"o\">$</span><span class=\"n\">docs</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## add articles to list</span><span class=\"w\">\n      </span><span class=\"k\">else</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' pages collected'</span><span class=\"p\">));</span><span class=\"w\"> </span><span class=\"k\">break</span><span class=\"p\">}</span><span class=\"w\">\n      </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">%%</span><span class=\"w\"> </span><span class=\"m\">10</span><span class=\"w\"> </span><span class=\"o\">==</span><span class=\"m\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' pages of metadata collected'</span><span class=\"p\">))</span><span class=\"w\">\n      </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"m\">+1</span><span class=\"w\">\n    </span><span class=\"p\">})</span><span class=\"w\">\n    \n    </span><span class=\"c1\">## if there was  a fail...</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">class</span><span class=\"p\">(</span><span class=\"n\">tryget</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"s1\">'try-error'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' error - metadata page not scraped'</span><span class=\"p\">))</span><span class=\"w\">\n      </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## add page i to failed list</span><span class=\"w\">\n      </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">e</span><span class=\"p\">[(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"n\">tryn</span><span class=\"m\">-1</span><span class=\"p\">))</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)]</span><span class=\"w\"> </span><span class=\"c1\">## keep the failed list to just tryn elements</span><span class=\"w\">\n      </span><span class=\"n\">Sys.sleep</span><span class=\"p\">(</span><span class=\"m\">0.5</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## probably scraping too fast -- slowing down</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"n\">Sys.sleep</span><span class=\"p\">(</span><span class=\"n\">sleep</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"nf\">return</span><span class=\"p\">(</span><span class=\"n\">art</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n\n</span><span class=\"c1\">## example: collect metadata for 3 pages (30 articles)</span><span class=\"w\">\n</span><span class=\"n\">meta</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">getMeta</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">pages</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">str</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">[[</span><span class=\"m\">1</span><span class=\"p\">]])</span><span class=\"w\"> </span><span class=\"c1\">## look at just one article </span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## List of 19\n##  $ web_url         : chr \"http://thecaucus.blogs.nytimes.com/2013/01/02/obama-returns-to-hawaii-to-continue-vacation/\"\n##  $ snippet         : chr \"President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation after a brief hiatus to wrangle with Co\"| __truncated__\n##  $ lead_paragraph  : NULL\n##  $ abstract        : chr \"President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation after a brief hiatus to wrangle with Co\"| __truncated__\n##  $ print_page      : NULL\n##  $ blog            : list()\n##  $ source          : chr \"The New York Times\"\n##  $ multimedia      : list()\n##  $ headline        :List of 2\n##   ..$ main  : chr \"Obama Returns to Hawaii to Continue Vacation\"\n##   ..$ kicker: chr \"The Caucus\"\n##  $ keywords        :List of 3\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"persons\"\n##   .. ..$ value: chr \"Obama, Barack\"\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"glocations\"\n##   .. ..$ value: chr \"Hawaii\"\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"subject\"\n##   .. ..$ value: chr \"United States Politics and Government\"\n##  $ pub_date        : chr \"2013-01-02T18:37:03Z\"\n##  $ document_type   : chr \"blogpost\"\n##  $ news_desk       : NULL\n##  $ section_name    : chr \"U.S.\"\n##  $ subsection_name : NULL\n##  $ byline          :List of 2\n##   ..$ person  :List of 1\n##   .. ..$ :List of 6\n##   .. .. ..$ firstname   : chr \"Jeremy\"\n##   .. .. ..$ middlename  : chr \"W.\"\n##   .. .. ..$ lastname    : chr \"PETERS\"\n##   .. .. ..$ rank        : int 1\n##   .. .. ..$ role        : chr \"reported\"\n##   .. .. ..$ organization: chr \"\"\n##   ..$ original: chr \"By JEREMY W. PETERS\"\n##  $ type_of_material: chr \"Blog\"\n##  $ _id             : chr \"50e4c5c700315214fbb821e4\"\n##  $ word_count      : int 366</code></pre></figure>\n\n<h2 id=\"extracting-and-parsing-the-article-body-text\">Extracting and parsing the article body text</h2>\n\n<p>So now we have a lot of information about NYT articles including URL, abstract, headline, publication date, section, author, type of material, etc.  However, notably absent is the article text itself.  The process for collecting article body text is much more manual.  We’ve used the API road as much as we can, but now we have to go off-road a little bit and collect each article’s body text from its individual URL provided from the metadata field <code class=\"highlighter-rouge\">web_url</code>.</p>\n\n<p>After a while of trial-and-error and guessing and checking, I developed a basic utility function <code class=\"highlighter-rouge\">parseArticleBody</code> to strip just the article body text from the raw HTML. At least one of three simple XPath queries seemed to work reasonably well for the normal plain vanilla articles.  Most of the video and photography pages contain little to no text, so these often come up empty.  Some of the more modern pages like this <a href=\"http://www.nytimes.com/roomfordebate/2013/01/02/should-social-security-cuts-be-considered\">one</a> which utilize multimedia and spread content over several pages usually fail too.</p>\n\n<p>However, there’s enough NYT articles in the world to sink a small ship<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote\">2</a></sup>, so I’m more concerned with precision than recall – I’m willing to let some articles slip through the cracks if it boosts the quality of text for the articles I am able to extract it for.  In most cases the gains from nailing the XPath query were marginal as the most common cause for missing article body text was out-of-date URLs provided by the NYT API.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'XML'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'httr'</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">parseArticleBody</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">artHTML</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">xpath2try</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s1\">'//div[@class=\"articleBody\"]//p'</span><span class=\"p\">,</span><span class=\"w\">\n                 </span><span class=\"s1\">'//p[@class=\"story-body-text story-content\"]'</span><span class=\"p\">,</span><span class=\"w\">\n                 </span><span class=\"s1\">'//p[@class=\"story-body-text\"]'</span><span class=\"w\">\n                 </span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"n\">xp</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">xpath2try</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"n\">bodyi</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">paste</span><span class=\"p\">(</span><span class=\"n\">xpathSApply</span><span class=\"p\">(</span><span class=\"n\">htmlParse</span><span class=\"p\">(</span><span class=\"n\">artHTML</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">xp</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">xmlValue</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">collapse</span><span class=\"o\">=</span><span class=\"s1\">''</span><span class=\"p\">)</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">nchar</span><span class=\"p\">(</span><span class=\"n\">bodyi</span><span class=\"p\">)</span><span class=\"o\">&gt;</span><span class=\"m\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"k\">break</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"nf\">return</span><span class=\"p\">(</span><span class=\"n\">bodyi</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n\n</span><span class=\"c1\">## Example: extract article text for one article</span><span class=\"w\">\n</span><span class=\"n\">p</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">GET</span><span class=\"p\">(</span><span class=\"s1\">'http://www.nytimes.com/2013/01/31/garden/faketv-can-fool-intruders.html'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">html</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'text'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">artBody</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">parseArticleBody</span><span class=\"p\">(</span><span class=\"n\">html</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">artBody</span><span class=\"p\">)</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## [1] \"\\nYou might not be home every evening watching television, but no burglar needs to know that. The coruscating light of a television set — signature feature of an occupied home — might be enough to scare prowlers off.        \\nThat’s the idea behind FakeTV ($35), a security gadget that replicates a television’s flickering light in all of its dynamic variety, from kinetic commercials to subdued news reports.        \\nThe lightweight wedge, smaller than a slice of cake, is the brainchild of Blaine Readler, an engineer, who intentionally left his TV on when going out one night. The fake version, which uses bright LEDs, saves wear and tear on a real TV, as well as the cost of electricity. Energy-wise, it’s equivalent to a 2-watt night light, said Rein Teder, president of the manufacturer, Hydreon Corporation.        \\nAnd unlike most TVs today, which turn on and off with buttons or remotes, FakeTV can be put on a timer. Information: faketv.com or (877) 532-5388.         \"</code></pre></figure>\n\n<h2 id=\"writing-to-mongodb\">Writing to MongoDB</h2>\n\n<h5 id=\"choosing-mongodb\">Choosing MongoDB</h5>\n\n<p>Github was a logical place to store code for this project – my blog is already hosted there and integrates well with my workflow allowing me to work from my home, office or villa on a remote island<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote\">3</a></sup>.  Where to store the data was less obvious.  I wanted something more robust than a directory full of text files… some sort of NoSQL database that I could access from Python and R.  I also wanted to make the data accessible via the web. And I wanted it all for free.</p>\n\n<p><a href=\"https://mongolab.com/\">mongolab</a> (MongoDB’s cloud database-as-a-service) fit the bill, for 500MB at least. I don’t have much experience with NoSQL databases and haven’t worked with Mongo in the past, but I had a lot of fun learning and working with Mongo.  My grasp of the Mongo querying language is still tenuous, but I was able make it do everything I needed it to do.</p>\n\n<p>I found the web interface intuitive and useful for testing queries or other Mongo command line operations when I was getting started.  From here you can also create database users with read-only or full access – useful for sharing your work with the world while holding the keys to the kingdom to yourself.</p>\n\n<h5 id=\"working-with-mongodb-in-python-and-r\">Working with MongoDB in Python and R</h5>\n\n<p>I used the <a href=\"http://cran.r-project.org/web/packages/RMongo/index.html\">RMongo</a> R package and the <a href=\"https://pypi.python.org/pypi/pymongo/\">pymongo</a> Python package for interacting with the database.  No complaints on either.  I like how the usage inside Python and R using these packages seems to mirror pretty closely usage at the command line.</p>\n\n<h5 id=\"when-the-cloud-can-bring-you-down\">When the cloud can bring you down</h5>\n\n<p>So high, yet so low.  I did experience a period of down-time where I was unable to access my database for an hour or so.  Although mongolab does a decent job of documenting issues on their <a href=\"http://status.mongolab.com/\">status page</a>, the list doesn’t appear to be short… As is life sometimes, I suppose, when you’re on the free-tier of a cloud service.</p>\n\n<h5 id=\"pulling-it-all-together\">Pulling it all together</h5>\n\n<p><code class=\"highlighter-rouge\">getArticles</code> does most of the heaviest lifting – extracts, parses and adds the article body text to the <code class=\"highlighter-rouge\">meta</code> object and inserts to MongoDB after scraping each article.</p>\n\n<p><code class=\"highlighter-rouge\">getArticles</code>:</p>\n\n<ul>\n  <li>DOES use the list of metadata as a starting point (the <code class=\"highlighter-rouge\">meta</code> argument).  Then scrapes, parses and adds the body as an attribute (text string) to the article’s metadata.  Only adds to the <code class=\"highlighter-rouge\">meta</code> object.</li>\n  <li>DOES cache – writes to MongoDB after extracting each article.  I didn’t bump into any rate-limits from MongoDB and I had to wait a decisecond or two after each article anyway, so insert speed was not an issue for me.  Had all the articles been pulled into memory first, I imagine a bulk insert would be more efficient.</li>\n  <li>DOES convert the R list of metadata for each article to JSON on the fly using the <code class=\"highlighter-rouge\">toJSON</code> function which is then conveniently inserted to MongoDB as-is.  MongoDB likes JSON.</li>\n  <li>DOES re-attempt failed article extraction, parsing and MongoDB inserts – If there was an error in one of these steps, I tried another 2 times before moving on to the next article.</li>\n  <li>DOES include parameters:\n    <ul>\n      <li><code class=\"highlighter-rouge\">meta</code> is created from the function <code class=\"highlighter-rouge\">getMeta</code>.</li>\n      <li><code class=\"highlighter-rouge\">n</code> is the number of articles to scrape.</li>\n      <li><code class=\"highlighter-rouge\">overwrite</code> decides whether to start scraping at the beginning or the first article that does not have a body attribute.</li>\n      <li><code class=\"highlighter-rouge\">sleep</code> is the time in seconds to pause after each article.  Defaults to 0.1 seconds.</li>\n      <li><code class=\"highlighter-rouge\">mongo</code> is list of credentials used to write to the database.  To return results in memory and not write to database, specify <code class=\"highlighter-rouge\">mongo=NULL</code>.</li>\n    </ul>\n  </li>\n</ul>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'XML'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'RMongo'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'rjson'</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">getArticles</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">=</span><span class=\"kc\">Inf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"nb\">F</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sleep</span><span class=\"o\">=</span><span class=\"m\">0.1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">mongo</span><span class=\"o\">=</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">dbName</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">collection</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'127.0.0.1'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">username</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">password</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">...</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">metaArt</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">meta</span><span class=\"w\">\n  </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">is.null</span><span class=\"p\">(</span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">dbName</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"nb\">F</span><span class=\"p\">){</span><span class=\"w\">\n    </span><span class=\"n\">con</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">mongoDbConnect</span><span class=\"p\">(</span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">dbName</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">host</span><span class=\"p\">)</span><span class=\"w\">\n    </span><span class=\"n\">try</span><span class=\"p\">(</span><span class=\"n\">authenticated</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"n\">dbAuthenticate</span><span class=\"p\">(</span><span class=\"n\">con</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">username</span><span class=\"o\">=</span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">username</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">password</span><span class=\"p\">))</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">overwrite</span><span class=\"o\">==</span><span class=\"nb\">T</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"n\">artIndex</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"p\">(</span><span class=\"nf\">min</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">)))</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"k\">else</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> \n    </span><span class=\"n\">ii</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\">  </span><span class=\"n\">which</span><span class=\"p\">(</span><span class=\"n\">sapply</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"nf\">is.null</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[[</span><span class=\"s1\">'bodyHTML'</span><span class=\"p\">]])))</span><span class=\"w\"> </span><span class=\"c1\">## get index of articles that have not been scraped yet</span><span class=\"w\">\n    </span><span class=\"n\">artIndex</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">ii</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"nf\">min</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">ii</span><span class=\"p\">))]</span><span class=\"w\"> </span><span class=\"c1\">## take first n articles</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"m\">-3</span><span class=\"p\">,</span><span class=\"m\">-2</span><span class=\"p\">,</span><span class=\"m\">-1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># initialize failed list of articles with arbitrary negative numbers</span><span class=\"w\">\n  </span><span class=\"n\">i</span><span class=\"o\">&lt;</span><span class=\"m\">-1</span><span class=\"w\">\n  </span><span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"o\">&lt;=</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">artIndex</span><span class=\"p\">)){</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">[(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"m\">-2</span><span class=\"p\">)</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)]))</span><span class=\"o\">==</span><span class=\"m\">1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"m\">+1</span><span class=\"w\"> </span><span class=\"c1\">## if we tried and failed 3 times, move on</span><span class=\"w\">\n    </span><span class=\"n\">tryget</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">try</span><span class=\"p\">({</span><span class=\"w\">\n      </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">overwrite</span><span class=\"o\">==</span><span class=\"nb\">T</span><span class=\"w\"> </span><span class=\"o\">|</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"nf\">is.null</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]</span><span class=\"o\">$</span><span class=\"n\">body</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"nb\">T</span><span class=\"w\"> </span><span class=\"o\">&amp;</span><span class=\"w\"> </span><span class=\"n\">overwrite</span><span class=\"o\">==</span><span class=\"nb\">F</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"n\">p</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">GET</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]</span><span class=\"o\">$</span><span class=\"n\">web_url</span><span class=\"p\">)</span><span class=\"w\">\n        </span><span class=\"n\">html</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">content</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">'text'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## metaArt[[i]]$bodyHTML &lt;- content(p, 'text')</span><span class=\"w\">\n        </span><span class=\"n\">metaArt</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]</span><span class=\"o\">$</span><span class=\"n\">body</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">parseArticleBody</span><span class=\"p\">(</span><span class=\"n\">html</span><span class=\"p\">)</span><span class=\"w\">\n        </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">is.null</span><span class=\"p\">(</span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">dbName</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"nb\">F</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">dbInsertDocument</span><span class=\"p\">(</span><span class=\"n\">con</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">mongo</span><span class=\"o\">$</span><span class=\"n\">collection</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">toJSON</span><span class=\"p\">(</span><span class=\"n\">metaArt</span><span class=\"p\">[[</span><span class=\"n\">i</span><span class=\"p\">]]))</span><span class=\"w\">\n        </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"w\"> </span><span class=\"o\">%%</span><span class=\"w\"> </span><span class=\"m\">10</span><span class=\"o\">==</span><span class=\"m\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' articles scraped'</span><span class=\"p\">))</span><span class=\"w\">\n        </span><span class=\"n\">i</span><span class=\"o\">&lt;-</span><span class=\"n\">i</span><span class=\"m\">+1</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">})</span><span class=\"w\">\n    </span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nf\">class</span><span class=\"p\">(</span><span class=\"n\">tryget</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"s1\">'try-error'</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' error - article not scraped'</span><span class=\"p\">))</span><span class=\"w\">\n      </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">i</span><span class=\"p\">)</span><span class=\"w\">\n      </span><span class=\"n\">e</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">e</span><span class=\"p\">[(</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"m\">-2</span><span class=\"p\">)</span><span class=\"o\">:</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)]</span><span class=\"w\">\n      </span><span class=\"n\">Sys.sleep</span><span class=\"p\">(</span><span class=\"m\">0.5</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## probably scraping too fast -- slowing down</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"n\">Sys.sleep</span><span class=\"p\">(</span><span class=\"n\">sleep</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"nf\">return</span><span class=\"p\">(</span><span class=\"n\">metaArt</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n\n</span><span class=\"c1\">## Example: get article bodies for first 10 articles.  </span><span class=\"w\">\n</span><span class=\"n\">art</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">getArticles</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">mongo</span><span class=\"o\">=</span><span class=\"kc\">NULL</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## scrape article body text for first 3 articles.  Do not write to MongoDB</span><span class=\"w\">\n</span><span class=\"nf\">length</span><span class=\"p\">(</span><span class=\"n\">art</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\">## we only add to meta.  Still returns articles for which we did not collect body text.</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## [1] 30</code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">str</span><span class=\"p\">(</span><span class=\"n\">art</span><span class=\"p\">[[</span><span class=\"m\">1</span><span class=\"p\">]])</span><span class=\"w\"> </span><span class=\"c1\">## we now have a \"body\" attribute.</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## List of 20\n##  $ web_url         : chr \"http://thecaucus.blogs.nytimes.com/2013/01/02/obama-returns-to-hawaii-to-continue-vacation/\"\n##  $ snippet         : chr \"President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation after a brief hiatus to wrangle with Co\"| __truncated__\n##  $ lead_paragraph  : NULL\n##  $ abstract        : chr \"President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation after a brief hiatus to wrangle with Co\"| __truncated__\n##  $ print_page      : NULL\n##  $ blog            : list()\n##  $ source          : chr \"The New York Times\"\n##  $ multimedia      : list()\n##  $ headline        :List of 2\n##   ..$ main  : chr \"Obama Returns to Hawaii to Continue Vacation\"\n##   ..$ kicker: chr \"The Caucus\"\n##  $ keywords        :List of 3\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"persons\"\n##   .. ..$ value: chr \"Obama, Barack\"\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"glocations\"\n##   .. ..$ value: chr \"Hawaii\"\n##   ..$ :List of 3\n##   .. ..$ rank : chr \"1\"\n##   .. ..$ name : chr \"subject\"\n##   .. ..$ value: chr \"United States Politics and Government\"\n##  $ pub_date        : chr \"2013-01-02T18:37:03Z\"\n##  $ document_type   : chr \"blogpost\"\n##  $ news_desk       : NULL\n##  $ section_name    : chr \"U.S.\"\n##  $ subsection_name : NULL\n##  $ byline          :List of 2\n##   ..$ person  :List of 1\n##   .. ..$ :List of 6\n##   .. .. ..$ firstname   : chr \"Jeremy\"\n##   .. .. ..$ middlename  : chr \"W.\"\n##   .. .. ..$ lastname    : chr \"PETERS\"\n##   .. .. ..$ rank        : int 1\n##   .. .. ..$ role        : chr \"reported\"\n##   .. .. ..$ organization: chr \"\"\n##   ..$ original: chr \"By JEREMY W. PETERS\"\n##  $ type_of_material: chr \"Blog\"\n##  $ _id             : chr \"50e4c5c700315214fbb821e4\"\n##  $ word_count      : int 366\n##  $ body            : chr \"\\tHONOLULU – President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation on Wednesday after a brief\"| __truncated__</code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">art</span><span class=\"p\">[[</span><span class=\"m\">1</span><span class=\"p\">]][</span><span class=\"s1\">'body'</span><span class=\"p\">])</span><span class=\"w\"> </span><span class=\"c1\">## full body text of the article.</span></code></pre></figure>\n\n<figure class=\"highlight\"><pre><code class=\"language-text\" data-lang=\"text\">## $body\n## [1] \"\\tHONOLULU – President Obama eased back into the relaxing groove of his Christmas in Hawaii vacation on Wednesday after a brief hiatus to wrangle with Congress over a deal to avert a fiscal crisis.\\tIn most respects, Wednesday on the island of Oahu was 5,000 miles away and a world apart from the political brinksmanship of the nation’s capital. The morning included a trip to the local Marine Corps base, where the president went to the gym.\\tAnd after speaking with Gov. Andrew M. Cuomo of New York and Gov. Chris Christie of New Jersey about the stalled efforts to provide tens of billions of dollars to the states to aid their recovery from Hurricane Sandy, Mr. Obama hit the golf course. \\tThe White House announced Mr. Obama’s golf partners, as is standard protocol, and they once again included a friend who always creates a bit of a stir whenever he is spotted with the president. Bobby Titcomb, an old friend of the president’s from their days at the Punahou School here, was arrested in 2011 on a charge of soliciting prostitution after being swept up in an undercover sting. He later entered a plea of no contest. \\tThe president’s group also included Marty Nesbitt, a Chicago friend. Mr. Nesbitt was an early backer of Mr. Obama, and he later served as treasurer for his 2008 presidential campaign. \\tThe third in their foursome was Allison Davis, a Chicago developer and lawyer who was one of the founders of the firm the president once worked for and who was another financial backer of his early political career. \\tThe White House also released a new taped message from the president on Wednesday in which he outlined his priorities for 2013 – listing them in order as winding down the war in Afghanistan, reforming immigration and gun control – and cautioned that many of the issues that made the tax-and-spending deal so difficult remain unresolved.\\t“Obviously there’s still more to do when it comes to reducing our debt,” he said. “And I’m willing to do more as long as it does it in a balanced way that doesn’t put all the burden on seniors or students or middle-class families.”\"</code></pre></figure>\n\n<h2 id=\"pipeline\">Pipeline</h2>\n\n<p>So putting it all together, the pipeline looks like this:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"c1\">## dependencies</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'rjson'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'httr'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'RMongo'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"s1\">'XML'</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\">## parameters</span><span class=\"w\">\n</span><span class=\"n\">days</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">gsub</span><span class=\"p\">(</span><span class=\"s1\">'-'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">''</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">seq</span><span class=\"p\">(</span><span class=\"n\">as.Date</span><span class=\"p\">(</span><span class=\"s1\">'2013-01-01'</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">Sys.Date</span><span class=\"p\">()</span><span class=\"m\">-1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">by</span><span class=\"o\">=</span><span class=\"m\">1</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">mongoCreds</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">dbName</span><span class=\"o\">=</span><span class=\"s1\">'nyt'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">collection</span><span class=\"o\">=</span><span class=\"s1\">'articles1'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">host</span><span class=\"o\">=</span><span class=\"s1\">'ds063240.mongolab.com:63240'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">username</span><span class=\"o\">=</span><span class=\"s1\">'myUsername'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">password</span><span class=\"o\">=</span><span class=\"s1\">'myPassword'</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">key</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"s1\">'myKey'</span><span class=\"w\"> </span><span class=\"c1\"># replace with your personal key</span><span class=\"w\">\n\n</span><span class=\"c1\">## Letting it rip ... </span><span class=\"w\">\n</span><span class=\"n\">all</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">list</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"c1\">## persist all results in memory in addition to MongoDB... just in case.</span><span class=\"w\">\n</span><span class=\"k\">for</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"n\">days</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">url</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">makeURL</span><span class=\"p\">(</span><span class=\"n\">begin_date</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">end_date</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># generate URL</span><span class=\"w\">\n  </span><span class=\"n\">meta</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">getMeta</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">pages</span><span class=\"o\">=</span><span class=\"kc\">Inf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sleep</span><span class=\"o\">=</span><span class=\"m\">0.1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># extract metadata from NYT API</span><span class=\"w\">\n  </span><span class=\"n\">artxt</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">getArticles</span><span class=\"p\">(</span><span class=\"n\">meta</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">n</span><span class=\"o\">=</span><span class=\"kc\">Inf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">sleep</span><span class=\"o\">=</span><span class=\"m\">0.1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">overwrite</span><span class=\"o\">=</span><span class=\"nb\">T</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">mongo</span><span class=\"o\">=</span><span class=\"n\">mongoCreds</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># extract article text and write to MongoDB</span><span class=\"w\">\n  </span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">paste0</span><span class=\"p\">(</span><span class=\"s1\">'day '</span><span class=\"p\">,</span><span class=\"w\">  </span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">' complete at '</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Sys.time</span><span class=\"p\">()))</span><span class=\"w\">\n  </span><span class=\"n\">all</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">all</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">artxt</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># persist results</span><span class=\"w\">\n</span><span class=\"p\">}</span></code></pre></figure>\n\n<h2 id=\"results\">Results</h2>\n<p>It took me about 1 full day to max out the 500MB in my free mongolab database.  I ended up with 85,000 articles covering roughly 4 months of NYT articles.  Since I’m specifically interested in text mining the article text, my next step will likely be to delete the documents where the article body could not be extracted.</p>\n\n<p>A lot of the URLs provided from the NYT API were stale – only ~35% of URLs contained a real article with text.  It wasn’t until I was doing some preliminary analysis poking around on the full corpus that I realized links to historic AP and Reuters articles (of which there are many) were included and almost universally not fruitful.  These could easily be filtered out using the <code class=\"highlighter-rouge\">fq</code> parameter in the NYT API: <code class=\"highlighter-rouge\">fq=source:(\"The New York Times\")</code>.</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-r\" data-lang=\"r\"><span class=\"c1\">## Crosstab of article source vs. whether article was successfully scraped </span><span class=\"w\">\n\n                        </span><span class=\"o\">**</span><span class=\"n\">body</span><span class=\"w\"> </span><span class=\"n\">scraped</span><span class=\"w\"> </span><span class=\"n\">successfully</span><span class=\"o\">**</span><span class=\"w\">\n  </span><span class=\"o\">**</span><span class=\"n\">Source</span><span class=\"o\">**</span><span class=\"w\">                   </span><span class=\"kc\">FALSE</span><span class=\"w\">  </span><span class=\"kc\">TRUE</span><span class=\"w\">\n  </span><span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">New</span><span class=\"w\"> </span><span class=\"n\">York</span><span class=\"w\"> </span><span class=\"n\">Times</span><span class=\"w\">            </span><span class=\"m\">6686</span><span class=\"w\"> </span><span class=\"m\">34322</span><span class=\"w\">\n  </span><span class=\"n\">AP</span><span class=\"w\">                           </span><span class=\"m\">28944</span><span class=\"w\">     </span><span class=\"m\">8</span><span class=\"w\">\n  </span><span class=\"n\">Reuters</span><span class=\"w\">                      </span><span class=\"m\">24001</span><span class=\"w\">     </span><span class=\"m\">0</span><span class=\"w\">\n  </span><span class=\"n\">International</span><span class=\"w\"> </span><span class=\"n\">Herald</span><span class=\"w\"> </span><span class=\"n\">Tribune</span><span class=\"w\">     </span><span class=\"m\">9</span><span class=\"w\">  </span><span class=\"m\">1221</span><span class=\"w\">\n                                </span><span class=\"m\">1007</span><span class=\"w\">     </span><span class=\"m\">4</span><span class=\"w\">\n  </span><span class=\"n\">du_recipe</span><span class=\"w\">                      </span><span class=\"m\">273</span><span class=\"w\">     </span><span class=\"m\">0</span><span class=\"w\">\n  </span><span class=\"n\">The</span><span class=\"w\"> </span><span class=\"n\">Broadway</span><span class=\"w\"> </span><span class=\"n\">Channel</span><span class=\"w\">             </span><span class=\"m\">8</span><span class=\"w\">     </span><span class=\"m\">0</span><span class=\"w\">\n  </span><span class=\"n\">CNBC</span><span class=\"w\">                             </span><span class=\"m\">6</span><span class=\"w\">     </span><span class=\"m\">0</span><span class=\"w\">\n  </span><span class=\"n\">ADAM</span><span class=\"w\">                             </span><span class=\"m\">1</span><span class=\"w\">     </span><span class=\"m\">0</span></code></pre></figure>\n\n<p>I decided to use <a href=\"https://radimrehurek.com/gensim/\">gensim</a> and <a href=\"http://www.nltk.org/\">nltk</a> in Python for the text pre-processing and topic modeling.  More to come on that analysis and visualization.</p>\n\n<h5 id=\"footnotes\">Footnotes</h5>\n\n<div class=\"footnotes\">\n  <ol>\n    <li id=\"fn:1\">\n      <p>Note the use of the <code class=\"highlighter-rouge\">content</code> function from the httr package with <code class=\"highlighter-rouge\">as='parsed'</code> is not recommended by the authors for use in other R packages or production systems.  It’s provided as a convenience function… which I found, well, convenient… so I used it. <a href=\"#fnref:1\" class=\"reversefootnote\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:2\">\n      <p>In fact there are over 1000 articles alone matching the query parameters “ship+sink” <a href=\"#fnref:2\" class=\"reversefootnote\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:3\">\n      <p>Still working on the villa … <a href=\"#fnref:3\" class=\"reversefootnote\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>\n\n  <p><a href=\"http://brooksandrew.github.io/simpleblog/articles/new-york-times-api-to-mongodb/\">New York Times Article Search API to MongoDB</a> was originally published by andrew brooks at <a href=\"http://brooksandrew.github.io/simpleblog\">andrew brooks</a> on January 06, 2015.</p>"
}