{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-7982911050269775769",
  "published": "2022-10-18T10:50:00.024-07:00",
  "updated": "2022-10-21T10:07:54.413-07:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "Table Tennis: A Research Platform for Agile Robotics",
  "content": "<span class=\"byline-author\">Posted by Avi Singh, Research Scientist, and Laura Graesser, Research Engineer, Robotics at Google</span><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh563V_fA2AfwFfnKgijiPz38oX40zeziDIAGd6chEHZiNJRLpo1N3IXjqYRhF9acNGgkwrrBvbDMKIeTxov5QJHtTx3FO36jdOVlvMpRWdjrASBWSrGLNWP8gIPoS_qEk4z5fdJiNhLKwYWiMQDsjiFwvE5iABgFNyoREW37VzHjJ102qWxpNP7aWBvg/s775/i-S2R.gif\" style=\"display: none;\" /><p>Robot learning has been applied to a wide range of challenging real world tasks, including <a href=\"https://arxiv.org/abs/1910.07113\">dexterous manipulation</a>, <a href=\"https://arxiv.org/abs/2004.00784\">legged locomotion</a>, and <a href=\"https://arxiv.org/abs/1806.10293\">grasping</a>. It is less common to see robot learning applied to dynamic, high-acceleration tasks requiring tight-loop human-robot interactions, such as table tennis. There are two complementary properties of the table tennis task that make it interesting for robotic learning research. First, the task requires both speed and precision, which puts significant demands on a learning algorithm. At the same time, the problem is highly-structured (with a fixed, predictable environment) and naturally multi-agent (the robot can play with humans or another robot), making it a desirable testbed to investigate questions about human-robot interaction and <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a>. These properties have led to several research groups developing table tennis research platforms [<a href=\"https://www.ias.informatik.tu-darmstadt.de/Research/LearningToPlayPing-pong\">1</a>, <a href=\"https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/kognitive-systeme/projects/table-tennis-robot/\">2</a>, <a href=\"https://www.omron.com/global/en/technology/omrontechnics/vol51/016.html\">3</a>, <a href=\"https://core-robotics.gatech.edu/\">4</a>].  </p><a name='more'></a><p>The <a href=\"https://research.google/teams/robotics/\">Robotics</a> team at Google has built such a platform to study problems that arise from robotic learning in a multi-player, dynamic and interactive setting. In the rest of this post we introduce two projects, <a href=\"https://sites.google.com/view/is2r\">Iterative-Sim2Real</a> (to be presented at <a href=\"https://corl2022.org/\">CoRL 2022</a>) and <a href=\"https://sites.google.com/view/goals-eye\">GoalsEye</a> (<a href=\"https://iros2022.org/\">IROS 2022</a>), which illustrate the problems we have been investigating so far. Iterative-Sim2Real enables a robot to hold rallies of over 300 hits with a human player, while GoalsEye enables learning goal-conditioned policies that match the precision of amateur humans. <p></p><p></p> <div class=\"separator\" style=\"clear: both; text-align: center;\"><video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"80%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/i-S2R_highlights.mp4?raw=true\" type=\"video/mp4\"></source></video> <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"80%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/goalseye-highlights-1x-4x-1x_compressed.mp4?raw=true\" type=\"video/mp4\"></source></video></div>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Iterative-Sim2Real policies playing cooperatively with humans (<b>top</b>) and a GoalsEye policy returning balls to different locations (<b>bottom</b>).</td></tr></tbody></table>  <h2>Iterative-Sim2Real: Leveraging a Simulator to Play Cooperatively with Humans</h2><p>In this project, the goal for the robot is <em>cooperative</em> in nature: to carry out a rally with a human for as long as possible. Since it would be tedious and time-consuming to train directly against a human player in the real world, we adopt a simulation-based (i.e., <a href=\"https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html\">sim-to-real</a>) approach. However, because it is difficult to simulate human behavior accurately, applying sim-to-real learning to tasks that require tight, close-loop interaction with a human participant is difficult. </p><p>In <a href=\"https://sites.google.com/view/is2r\">Iterative-Sim2Real</a>, (i.e., i-S2R), we present a method for learning human behavior models for human-robot interaction tasks, and instantiate it on our robotic table tennis platform. We have built a system that can achieve rallies of up to 340 hits with an amateur human player (shown below). </p>   <div class=\"separator\" style=\"clear: both; text-align: center;\"><iframe allowfullscreen=\"\" class=\"BLOG_video_class\" frameborder=\"0\" height=\"360\" src=\"https://www.youtube.com/embed/Fh7VK0WPvU4?rel=0&amp;\" width=\"640\" youtube-src-id=\"Fh7VK0WPvU4\"></iframe></div>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">A 340-hit rally lasting over 4 minutes.</td></tr></tbody></table>  <h2>Learning Human Behavior Models: a Chicken and Egg Problem</h2><p>The central problem in learning accurate human behavior models for robotics is the following: if we do not have a good-enough robot policy to begin with, then we cannot collect high-quality data on how a person might interact with the robot. But without a human behavior model, we cannot obtain robot policies in the first place. An alternative would be to train a robot policy directly in the real world, but this is often slow, cost-prohibitive, and poses safety-related challenges, which are further exacerbated when people are involved. i-S2R, visualized below, is a solution to this chicken and egg problem. It uses a simple model of human behavior as an approximate starting point and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s1360/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"785\" data-original-width=\"1360\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihM2ooQnHGqn_FavYojW0TKDGhT6fAVimaT8kYONHI-WZuNolM0Ve1zx9_kQQxdSSoW7_17aGyf8ylmELcFm1wZQ0hm0wa2W-WCVNkAJoyQeeoWsMH11Ferqgc5Ei34qq2QAPuAbsvLNs93CwqgdhhCcgxmkjPfIs-mYvMHdlDePfWMle-sMB6vPPoKQ/s16000/image4.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">i-S2R Methodology.</td></tr></tbody></table>   <h2>Results</h2><p>To evaluate i-S2R, we repeated the training process five times with five different human opponents and compared it with a baseline approach of ordinary sim-to-real plus fine-tuning (<a href=\"https://arxiv.org/abs/2110.05457\">S2R+FT</a>). When aggregated across all players, the i-S2R rally length is higher than S2R+FT by about 9% (below on the left). The histogram of rally lengths for i-S2R and S2R+FT (below on the right) shows that a large fraction of the rallies for S2R+FT are shorter (i.e., less than 5), while i-S2R achieves longer rallies more frequently.  </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s1999/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"935\" data-original-width=\"1999\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS_HGiUUqbp1nTJlfbMKmQWVRUA7T-pCNlZsprSNEvarV-MI1nATDjZ566oLzlApAGGa6mVsq2qlu4IN0KMsJBYaU4q6LPz8aap6sZqCO-fzkB1_eqa-aMrrx3yQuWyUtuCcFKeWZc0G8YfxQ8WbbFin3j_qpXg0Zxdce-BsNRBcoUAAqmzoedLMukNQ/s16000/image1.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Summary of i-S2R results. </b>Boxplot details: The white circle is the mean, the horizontal line is the median, box bounds are the 25th and 75th percentiles.</td></tr></tbody></table>       <p>We also break down the results based on player type: beginner (40% players), intermediate (40% of players) and advanced (20% players). We see that i-S2R significantly outperforms S2R+FT for both beginner and intermediate players (80% of players). </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s1999/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"645\" data-original-width=\"1999\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6i_dHv7JuJXFKJkEjmAgj0P90YdjWLXBe-Ah5OL5UrhlaV-4Uql2-snTz8VKfQenGZvQO-uAuIkOUvSYe9u4A6pYKkUCHX3ddhbqe67yMgNOvy9K1-gbFFIHU8I0f_xMTtkJc9sHa8X9rO-_OFBAHuKB7LVoXX2nfQnaRiR8-6fvePCV8aeVCDSzwlw/s16000/image2.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">i-S2R Results by player type.</td></tr></tbody></table>   <p>More details on i-S2R can be found on our <a href=\"https://arxiv.org/abs/2207.06572\">preprint</a>, <a href=\"https://sites.google.com/view/is2r\">website</a>, and also in the following summary video. </p>  <div class=\"separator\" style=\"clear: both; text-align: center;\"><iframe allowfullscreen=\"\" class=\"BLOG_video_class\" frameborder=\"0\" height=\"360\" src=\"https://www.youtube.com/embed/vtVUFXV9qR4?rel=0&amp;\" width=\"640\" youtube-src-id=\"vtVUFXV9qR4\"></iframe></div> <div style=\"line-height:120%;\">  <br>  <br></div> <h2>GoalsEye: Learning to Return Balls Precisely on a Physical Robot</h2><p>While we focused on sim-to-real learning in i-S2R, it is sometimes desirable to learn using only real-world data — closing the sim-to-real gap in this case is unnecessary. <a href=\"https://en.wikipedia.org/wiki/Imitative_learning\">Imitation learning</a> (IL) provides a simple and stable approach to learning in the real world, but it requires access to demonstrations and cannot exceed the performance of the teacher. Collecting expert human demonstrations of precise goal-targeting in high speed settings is challenging and sometimes impossible (due to limited precision in human movements). While reinforcement learning (RL) is well-suited to such high-speed, high-precision tasks, it faces a difficult exploration problem (especially at the start), and can be very sample inefficient. In <a href=\"https://sites.google.com/view/goals-eye\">GoalsEye</a>, we demonstrate an approach that combines recent behavior cloning techniques [<a href=\"https://arxiv.org/abs/1903.01973\">5</a>, <a href=\"https://arxiv.org/abs/1912.06088\">6</a>] to learn a precise goal-targeting policy, starting from a small, weakly-structured, non-targeting dataset. </p><p>Here we consider a different table tennis task with an emphasis on precision. We want the robot to return the ball to an arbitrary goal location on the table, e.g. “hit the back left corner\" or ''land the ball just over the net on the right side\" (see left video below). Further, we wanted to find a method that can be applied <em>directly</em> on our real world table tennis environment with no simulation involved. We found that the synthesis of two existing imitation learning techniques, <a href=\"https://arxiv.org/abs/1903.01973\">Learning from Play</a> (LFP) and <a href=\"https://arxiv.org/abs/1912.06088\">Goal-Conditioned Supervised Learning</a> (GCSL), scales to this setting. It is safe and <a href=\"https://en.wikipedia.org/wiki/Sample_complexity\">sample efficient</a> enough to train a policy on a physical robot which is as accurate as amateur humans at the task of returning balls to specific goals on the table. </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>     <td style=\"text-align: center;\">    <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"100%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-robot-5-goals-compressed.mp4?raw=true\" type=\"video/mp4\"></source></video>  </td> <td style=\"text-align: center;\"> &nbsp;   </td> <td style=\"text-align: center;\">    <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"100%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/goals-eye-narrow-incoming-ball-human-5-goals-compressed-unblurred.mp4?raw=true\" type=\"video/mp4\"></source></video>  </td></tr></tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">GoalsEye policy aiming at a 20cm diameter goal (<b>left</b>). Human player aiming at the same goal (<b>right</b>).</td></tr></tbody></table>     <p>The essential ingredients of success are:  </p><ol> <li><em>A minimal, but non-goal-directed “bootstrap” dataset<strong> </strong></em>of the robot hitting the ball to overcome an initial difficult exploration problem.   </li><li><em>Hindsight relabeled goal conditioned behavioral cloning</em> (GCBC) to train a goal-directed policy to reach any goal in the dataset.   </li><li><em>Iterative self-supervised goal reaching.</em> The agent improves continuously by setting random goals and attempting to reach them using the current policy. All attempts are relabeled and added into a continuously expanding training set. This <em>self-practice</em>, in which the robot expands the training data by setting and attempting to reach goals, is repeated iteratively. </li></ol>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s1600/image5.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"468\" data-original-width=\"1600\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfFWLsqP8TmC8sJpRao5WeWAjDB9iJrOSA3XSkvY5K8jzdCN4raHg7LkHziyvD0YJum9Jk8fVF8uDrUGdPPAsLLksX2n9drZPlylksvST40WDnwhGiONNPSBW6f6z8qE3v4-G_i-VlcjJso-kIE9EW1zvUsIgSnDjtJGbKeD3QCKtEDnJmDg-hMZ2Zxg/s16000/image5.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">GoalsEye methodology.</td></tr></tbody></table>   <h2>Demonstrations and Self-Improvement Through Practice Are Key</h2><p>The synthesis of techniques is crucial. The policy’s objective is to return a <em>variety </em>of incoming balls to <em>any </em>location on the opponent’s side of the table. A policy trained on the initial 2,480 demonstrations only accurately reaches within 30 cm of the goal 9% of the time. However, after a policy has self-practiced for ~13,500 attempts, goal-reaching accuracy rises to 43% (below on the right). This improvement is clearly visible as shown in the videos below. Yet if a policy only self-practices, training fails completely in this setting. Interestingly, the number of demonstrations improves the efficiency of subsequent self-practice, albeit with diminishing returns. This indicates that demonstration data and self-practice could be substituted depending on the relative time and cost to gather demonstration data compared with self-practice. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s1441/image7.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"541\" data-original-width=\"1441\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHQc683I9kCBQui1CUv4FI_NEDTFoGFR4OnKWHiVChfjvh9HE8ohQU1nhzq2_wKXhMJXu7Sb1WzP6Qh7tWgM9UA2F6_QZFdGVVGyj8eGANnM-jgbqTVRYcl9pXLCaNgirMhHEHaV5drnfzziNRY_3fsSPW2a_jEmfLvVpnB6ak4zxHtoTWMqnOYOKZmA/s16000/image7.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Self-practice substantially improves accuracy. <b>Left</b>: simulated training. <b>Right</b>: real robot training. The demonstration datasets contain ~2,500 episodes, both in simulation and the real world.</td></tr></tbody></table>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>     <td style=\"text-align: center;\">    <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"100%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt_5_goal_E_no_audio_4x_compressed.mp4?raw=true\" type=\"video/mp4\"></source></video>  </td> <td style=\"text-align: center;\"> &nbsp;   </td> <td style=\"text-align: center;\">    <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" width=\"100%\"> <source src=\"https://github.com/lauragraesser/videos/blob/main/goals-eye-ckpt-300-goal-E-no-audio-4x_compressed.mp4?raw=true\" type=\"video/mp4\"></source></video>  </td></tr></tbody></table>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Visualizing the benefits of self-practice. <b>Left</b>: policy trained on initial 2,480 demonstrations. <b>Right</b>: policy after an additional 13,500 self-practice attempts.</td></tr></tbody></table>   <p>More details on GoalsEye can be found in the <a href=\"https://arxiv.org/abs/2210.03662\">preprint</a> and on our <a href=\"https://sites.google.com/view/goals-eye\">website</a>. </p><div style=\"line-height:40%;\">    <br></div> <h2>Conclusion and Future Work</h2><p>We have presented two complementary projects using our robotic table tennis research platform. i-S2R learns RL policies that are able to interact with humans, while GoalsEye demonstrates that learning from real-world unstructured data combined with self-supervised practice is effective for learning goal-conditioned policies in a precise, dynamic setting. </p><p>One interesting research direction to pursue on the table tennis platform would be to build a robot “coach” that could adapt its play style according to the skill level of the human participant to keep things challenging and exciting.  </p><div style=\"line-height:40%;\">    <br></div> <h2>Acknowledgements</h2><p><em>We thank our co-authors, Saminda Abeyruwan, Alex Bewley, Krzysztof Choromanski, David B. D’Ambrosio, Tianli Ding, Deepali Jain, Corey Lynch, Pannag R. Sanketi, Pierre Sermanet and Anish Shankar. We are also grateful for the support of many members of the Robotics Team who are listed in the acknowledgement sections of the papers.</em></p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}