{
  "title": "Challenges of reproducing R-NET neural network using Keras",
  "link": "",
  "updated": "2017-08-25T00:00:00+00:00",
  "id": "http://yerevann.github.io//2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras",
  "content": "<p>By <a href=\"https://github.com/MartinXPN\">Martin Mirakyan</a>, <a href=\"https://github.com/mahnerak\">Karen Hambardzumyan</a> and\n <a href=\"https://github.com/Hrant-Khachatrian\">Hrant Khachatrian</a>.</p>\n\n<p>In this post we describe our attempt to re-implement a neural architecture for automated question answering called <a href=\"https://www.microsoft.com/en-us/research/publication/mrc/\">R-NET</a>, which is developed by the Natural Language Computing Group of Microsoft Research Asia. This architecture demonstrates the best performance among single models (not ensembles) on The Stanford Question Answering Dataset (as of August 25, 2017). MSR researchers released a <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">technical report</a> describing the model but did not release the code. We tried to implement the architecture in Keras framework and reproduce their results. This post describes the model and the challenges we faced while implementing it <a class=\"nav-link\" href=\"https://github.com/YerevaNN/R-NET-in-Keras\">[<span class=\"hidden-xs-down\">View on GitHub </span><svg version=\"1.1\" width=\"16\" height=\"16\" viewBox=\"0 0 16 16\" class=\"octicon octicon-mark-github\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" fill=\"#268bd2\" d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z\"></path></svg>]</a>.</p>\n\n<!--more-->\n\n<h2 class=\"no_toc\" id=\"contents\">Contents</h2>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#problem-statement\" id=\"markdown-toc-problem-statement\">Problem statement</a></li>\n  <li><a href=\"#the-architecture-of-r-net\" id=\"markdown-toc-the-architecture-of-r-net\">The architecture of R-NET</a>    <ul>\n      <li><a href=\"#drawing-complex-recurrent-networks\" id=\"markdown-toc-drawing-complex-recurrent-networks\">Drawing complex recurrent networks</a></li>\n      <li><a href=\"#1-question-and-passage-encoder\" id=\"markdown-toc-1-question-and-passage-encoder\">1. Question and passage encoder</a></li>\n      <li><a href=\"#2-obtain-question-aware-representation-for-the-passage\" id=\"markdown-toc-2-obtain-question-aware-representation-for-the-passage\">2. Obtain question aware representation for the passage</a></li>\n      <li><a href=\"#3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation\" id=\"markdown-toc-3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation\">3. Apply self-matching attention on the passage to get its final representation</a></li>\n      <li><a href=\"#4-predict-the-interval-which-contains-the-answer-of-a-question\" id=\"markdown-toc-4-predict-the-interval-which-contains-the-answer-of-a-question\">4. Predict the interval which contains the answer of a question</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#implementation-details\" id=\"markdown-toc-implementation-details\">Implementation details</a>    <ul>\n      <li><a href=\"#layers-with-masking-support\" id=\"markdown-toc-layers-with-masking-support\">Layers with masking support</a></li>\n      <li><a href=\"#slice-layer\" id=\"markdown-toc-slice-layer\">Slice layer</a></li>\n      <li><a href=\"#generators\" id=\"markdown-toc-generators\">Generators</a></li>\n      <li><a href=\"#bidirectional-grus\" id=\"markdown-toc-bidirectional-grus\">Bidirectional GRUs</a></li>\n      <li><a href=\"#dropout\" id=\"markdown-toc-dropout\">Dropout</a></li>\n      <li><a href=\"#weight-sharing\" id=\"markdown-toc-weight-sharing\">Weight sharing</a></li>\n      <li><a href=\"#hyperparameters\" id=\"markdown-toc-hyperparameters\">Hyperparameters</a></li>\n      <li><a href=\"#weight-initialization\" id=\"markdown-toc-weight-initialization\">Weight initialization</a></li>\n      <li><a href=\"#training\" id=\"markdown-toc-training\">Training</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#results-and-comparison-with-r-net-technical-report\" id=\"markdown-toc-results-and-comparison-with-r-net-technical-report\">Results and comparison with <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">R-NET technical report</a></a></li>\n  <li><a href=\"#challenges-of-reproducibility\" id=\"markdown-toc-challenges-of-reproducibility\">Challenges of reproducibility</a></li>\n</ul>\n\n<h2 id=\"problem-statement\">Problem statement</h2>\n\n<p>Given a passage and a question, the task is to predict an answer to the question based on the information found in the passage. The SQuAD dataset further constrains the answer to be a continuous sub-span of the provided passage. Answers usually include non-entities and can be long phrases. The neural network needs to “understand” both the passage and the question in order to be able to give a valid answer. Here is an example from the dataset.</p>\n\n<p><strong>Passage:</strong> Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused. Morgan was shocked by the reminder of his part in the stock market crash and by Tesla’s breach of contract by asking for more funds. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began.</p>\n\n<p><strong>Question:</strong> On what did Tesla blame for the loss of the initial money?\n<strong>Answer:</strong> Panic of 1901</p>\n\n<h2 id=\"the-architecture-of-r-net\">The architecture of R-NET</h2>\n\n<p>The <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py\">architecture</a> of R-NET network is designed to take the question and the passage as inputs and to output an interval on the passage that contains the answer. The process consists of several steps:</p>\n<ol>\n  <li>Encode the question and the passage</li>\n  <li>Obtain question aware representation for the passage</li>\n  <li>Apply self-matching attention on the passage to get its final representation.</li>\n  <li>Predict the interval which contains the answer of the question.</li>\n</ol>\n\n<p>Each of these steps is implemented as some sort of recurrent neural network. The model is trained end-to-end.</p>\n\n<h3 id=\"drawing-complex-recurrent-networks\">Drawing complex recurrent networks</h3>\n\n<p>We are using <a href=\"https://arxiv.org/abs/1412.3555\">GRU</a> cells (Gated Recurrent Unit) for all RNNs. The authors claim that their performance is similar to LSTM, but they are computationally cheaper.</p>\n\n<p><img src=\"https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/GRU.svg\" alt=\"GRU network\" title=\"GRU network\" /></p>\n\n<p>Most of the modules of R-NET are implemented as recurrent networks with  complex cells. We draw these cells using colorful charts. Here is a chart that corresponds to the original GRU cell.</p>\n\n<p><img src=\"https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/GRUcell.svg\" alt=\"GRU cell\" title=\"GRU cell\" /></p>\n\n<p>White rectangles represent operations on tensors (dot product, sum, etc.). Yellow rectangles are activations (tanh, softmax or sigmoid). Orange circles are the weights of the network. Compare this to the formula of GRU cell (taken from <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Olah’s famous blogpost</a>):</p>\n\n<script type=\"math/tex; mode=display\">% <![CDATA[\n\\begin{aligned}\n\\large\nz_t &=\\sigma(W_z \\cdot [h_{t-1}, x_t]) \\\\\nr_t &=\\sigma(W_r \\cdot [h_{t-1}, x_t]) \\\\\n\\tilde{h}_t &= tanh(W \\cdot [r_t \\circ h_{t-1}, x_t]) \\\\\nh_t &= (1 - z_t) \\circ h_{t-1} + z_t \\circ \\tilde{h}_t\n\\end{aligned} %]]></script>\n\n<p>Some parts of R-NET architecture require to use tensors that are neither part of a GRU state nor part of an input at time <script type=\"math/tex\">t</script>. These are “global” variables that are used in all timesteps. Following <a href=\"http://deeplearning.net/software/theano/library/scan.html\">Theano’s terminology</a>, we call these global variables <em>non-sequences</em>.</p>\n\n<p>To make it easier to create GRU cells with additional features and operations we’ve created a <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/WrappedGRU.py\">utility class called <strong>WrappedGRU</strong></a> which is a base class for all GRU modules. WrappedGRU supports operations with non-sequences and sharing weights between modules. Keras doesn’t directly support weight sharing, but instead it supports layer sharing and we use <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SharedWeight.py\">SharedWeight layer</a> to solve this problem (SharedWeight is a layer that has no inputs and returns tensor of weights). WrappedGRU supports taking SharedWeight as an input.</p>\n\n<h3 id=\"1-question-and-passage-encoder\">1. Question and passage encoder</h3>\n\n<p>This step consists of two parts: <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/preprocessing.py\">preprocessing</a> and text encoding. The preprocessing is done in a separate process and is not part of the neural network. First we preprocess the data by splitting it into parts, and then we convert all the words to corresponding vectors. Word-vectors are generated using <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/preprocessing.py#L35\">gensim</a>.</p>\n\n<p>The next steps are already part of the model. Each word is represented by a concatenation of two vectors: its GloVe vector and another vector that holds character level information. To obtain character level embeddings we use an Embedding layer followed by a Bidirectional GRU cell wrapped inside a TimeDistributed layer. Basically, each character is embedded in <script type=\"math/tex\">H</script> dimensional space, and a BiGRU runs over those embeddings to produce a vector for the word. The process is repeated for all the words using TimeDistributed layer.</p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L62\">Code on GitHub</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">TimeDistributed</span><span class=\"p\">(</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">InputLayer</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'int32'</span><span class=\"p\">),</span>\n    <span class=\"n\">Embedding</span><span class=\"p\">(</span><span class=\"n\">input_dim</span><span class=\"o\">=</span><span class=\"mi\">127</span><span class=\"p\">,</span> <span class=\"n\">output_dim</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">,</span> <span class=\"n\">mask_zero</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">),</span>\n    <span class=\"n\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">GRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">))</span>\n<span class=\"p\">]))</span>\n</code></pre></div></div>\n\n<p>When the word is missing from GloVe, we set its word vector to all zeros (as described in the technical report).</p>\n\n<p>Following the notation of the paper, we denote the vector representation of the question by <script type=\"math/tex\">u^Q</script> and the representation of the passage by <script type=\"math/tex\">u^P</script> (<script type=\"math/tex\">Q</script> corresponds to the question and <script type=\"math/tex\">P</script> corresponds to the passage).</p>\n\n<p>The network takes the preprocessed question <script type=\"math/tex\">Q</script> and the passage <script type=\"math/tex\">P</script>, applies masking on each one and then encodes them with 3 consecutive bidirectional GRU layers.</p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L81\">Code on GitHub</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Encode the passage P</span>\n<span class=\"n\">uP</span> <span class=\"o\">=</span> <span class=\"n\">Masking</span><span class=\"p\">()</span> <span class=\"p\">(</span><span class=\"n\">P</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"n\">uP</span> <span class=\"o\">=</span> <span class=\"n\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">GRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">,</span>\n                           <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                           <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">))</span> <span class=\"p\">(</span><span class=\"n\">uP</span><span class=\"p\">)</span>\n<span class=\"n\">uP</span> <span class=\"o\">=</span> <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"n\">rate</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'uP'</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">uP</span><span class=\"p\">)</span>\n\n<span class=\"c\"># Encode the question Q</span>\n<span class=\"n\">uQ</span> <span class=\"o\">=</span> <span class=\"n\">Masking</span><span class=\"p\">()</span> <span class=\"p\">(</span><span class=\"n\">Q</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span>\n    <span class=\"n\">uQ</span> <span class=\"o\">=</span> <span class=\"n\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">GRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">,</span>\n                           <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                           <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">))</span> <span class=\"p\">(</span><span class=\"n\">uQ</span><span class=\"p\">)</span>\n<span class=\"n\">uQ</span> <span class=\"o\">=</span> <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"n\">rate</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'uQ'</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">uQ</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>After encoding the passage and the question we finally have their vector representations <script type=\"math/tex\">u^P</script> and <script type=\"math/tex\">u^Q</script>. Now we can delve deeper in understanding the meaning of the passage having in mind the question.</p>\n\n<h3 id=\"2-obtain-question-aware-representation-for-the-passage\">2. Obtain question aware representation for the passage</h3>\n\n<p>The next module computes another representation for the passage by taking into account the words inside the question sentence. We implement it using the following code:</p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L97\">Code on GitHub</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">vP</span> <span class=\"o\">=</span> <span class=\"n\">QuestionAttnGRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">,</span>\n             <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span> <span class=\"p\">([</span>\n                 <span class=\"n\">uP</span><span class=\"p\">,</span> <span class=\"n\">uQ</span><span class=\"p\">,</span>\n                 <span class=\"n\">WQ_u</span><span class=\"p\">,</span> <span class=\"n\">WP_v</span><span class=\"p\">,</span> <span class=\"n\">WP_u</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">W_g1</span>\n             <span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/QuestionAttnGRU.py\">QuestionAttnGRU</a> is a complex extension of a recurrent layer (extends WrappedGRU and overrides the step method by adding additional operations before passing the input to the GRU cell).</p>\n\n<p><img src=\"https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/QuestionAttnGRU.svg\" alt=\"QuestionAttnGRU\" title=\"Question Attention GRU\" /></p>\n\n<p>The vectors of question aware representation of the passage are denoted by <script type=\"math/tex\">v^P</script>. As a reminder <script type=\"math/tex\">u^P_t</script> is the vector representation of the passage <script type=\"math/tex\">P</script>, <script type=\"math/tex\">u^Q</script> is the matrix representation of the question <script type=\"math/tex\">Q</script> (each row corresponds to a single word).</p>\n\n<p>In QuestionAttnGRU first we combine three things:</p>\n<ol>\n  <li>the previous state of the GRU (<script type=\"math/tex\">v^P_{t-1}</script>)</li>\n  <li>matrix representation of the question (<script type=\"math/tex\">u^Q</script>)</li>\n  <li>vector representation of the passage (<script type=\"math/tex\">u^P_{t}</script>) at the <script type=\"math/tex\">t</script>-th word.</li>\n</ol>\n\n<p>We compute the dot product of each input with the corresponding weights, then sum-up all together after broadcasting them into the same shape. The outputs of dot(<script type=\"math/tex\">u^P_{t}</script>, <script type=\"math/tex\">W^P_{u}</script>) and dot(<script type=\"math/tex\">v^P_{t-1}</script>, <script type=\"math/tex\">W^P_{v}</script>) are vectors, while the output of dot(<script type=\"math/tex\">u^Q</script>, <script type=\"math/tex\">W^Q_{u}</script>) is a matrix, therefore we broadcast (repeat several times) the vectors to match the shape of the matrix and then compute the sum of three matrices. Then we apply tanh activation on the result. The output of this operation is then multiplied (dot product) by a weight vector <script type=\"math/tex\">V</script>, after which <script type=\"math/tex\">softmax</script> activation is applied. The output of the <script type=\"math/tex\">softmax</script> is a vector of non-negative numbers that represent the “importance” of each word in the question. This kind of vectors are often called <em>attention vectors</em>. When computing the dot product of <script type=\"math/tex\">u^Q</script> (matrix representation of the question) and the attention vector, we obtain a single vector for the entire question which is a weighted average of question word vectors (weighted by the attention scores). The intuition behind this part is that we get a representation of the parts of the question that are relevant to the current word of the passage. This representation, denoted by <script type=\"math/tex\">c_{t}</script>, depends on the current word, the whole question and the previous state of the recurrent cell (formula 4 on page 3 of the <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">report</a>).</p>\n\n<p>These ideas seem to come from a paper by <a href=\"https://arxiv.org/abs/1509.06664\">Rocktäschel et al.</a> from Deepmind. The authors suggested to pass this <script type=\"math/tex\">c_{t}</script> vector as an input to the GRU cell. <a href=\"https://arxiv.org/abs/1512.08849\">Wang and Jiang</a> from Singapore Management University argued that passing <script type=\"math/tex\">c_{t}</script> is not enough, because we are losing information from the “original” input <script type=\"math/tex\">u^P_{t}</script>. So they suggested to concatenate <script type=\"math/tex\">c_{t}</script> and <script type=\"math/tex\">u^P_{t}</script> before passing it to the GRU cell.</p>\n\n<p>The authors of R-NET did one more step. They applied an additional gate to the concatenated vector <script type=\"math/tex\">[c_{t}, u^P_{t}]</script>. The gate is simply a dot product of some new weight matrix <script type=\"math/tex\">W_{g}</script> and the concatenated vector, passed through a sigmoid activation function. The output of the gate is a vector of non-negative numbers, which is then (element-wise) multiplied by the original concatenated vector (see formula 6 on page 4 of the <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">report</a>). The result of this multiplication is finally passed to the GRU cell as an input.</p>\n\n<h3 id=\"3-apply-self-matching-attention-on-the-passage-to-get-its-final-representation\">3. Apply self-matching attention on the passage to get its final representation</h3>\n\n<p>Next, the authors suggest to add a self attention mechanism on the passage itself.</p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L105\">Code on GitHub</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">hP</span> <span class=\"o\">=</span> <span class=\"n\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">SelfAttnGRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"n\">H</span><span class=\"p\">,</span>\n                               <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">))</span> <span class=\"p\">([</span>\n                       <span class=\"n\">vP</span><span class=\"p\">,</span> <span class=\"n\">vP</span><span class=\"p\">,</span>\n                       <span class=\"n\">WP_v</span><span class=\"p\">,</span> <span class=\"n\">WPP_v</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">W_g2</span>\n                   <span class=\"p\">])</span>\n<span class=\"n\">hP</span> <span class=\"o\">=</span> <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"n\">rate</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'hP'</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">hP</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The output of the previous step (Question attention) is denoted by <script type=\"math/tex\">v^P</script>. It represents the encoding of the passage while taking into account the question. <script type=\"math/tex\">v^P</script> is passed as an input to the self-matching attention module (top input, left input). The authors argue that the vectors <script type=\"math/tex\">v^P_{t}</script> have very limited information about the context. <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SelfAttnGRU.py\">Self-matching attention module</a> attempts to augment the passage vectors by information from other relevant parts of the passage.</p>\n\n<p>The output of the self-matching GRU cell at time <script type=\"math/tex\">t</script> is denoted by <script type=\"math/tex\">h^P_{t}</script>.</p>\n\n<p><img src=\"https://rawgit.com/YerevaNN/yerevann.github.io/master/public/2017-08-22/SelfAttnGRU.svg\" alt=\"SelfAttnGRU\" title=\"Self-matching Attention GRU\" /></p>\n\n<p>The implementation is very similar to the previous module. We compute dot products of weights <script type=\"math/tex\">W^PP_{u}</script> with the current word vector <script type=\"math/tex\">v^P_{t}</script>, and <script type=\"math/tex\">W^P_{v}</script> with the entire <script type=\"math/tex\">v^P</script> matrix, then add them up and apply <script type=\"math/tex\">\\tanh{}</script> activation. Next, the result is multiplied by a weight-vector <script type=\"math/tex\">V</script> and passed through <script type=\"math/tex\">softmax</script> activation, which produces an attention vector. The dot product of the attention vector and <script type=\"math/tex\">v^P</script> matrix, again denoted by <script type=\"math/tex\">c_{t}</script>, is the weighted average of all word vectors of the passage that are relevant to the current word <script type=\"math/tex\">v^P_{t}</script>. <script type=\"math/tex\">c_{t}</script> is then concatenated with <script type=\"math/tex\">v^P_{t}</script> itself. The concatenated vector is passed through a gate and is given to GRU cell as an input.</p>\n\n<p>The authors consider this step as their main contribution to the architecture.</p>\n\n<p>It is interesting to note that the authors write <code class=\"highlighter-rouge\">BiRNN</code> in Section 3.3 (Self-Matching Attention) and just <code class=\"highlighter-rouge\">RNN</code> in Section 3.2 (which describes question-aware passage representation). For that reason we used BiGRU in SelfAttnGRU and unidirectional GRU in QuestionAttnGRU. Later we discovered a sentence in Section 4.1 which suggests that we were not correct: <code class=\"highlighter-rouge\">the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment</code>.</p>\n\n<h3 id=\"4-predict-the-interval-which-contains-the-answer-of-a-question\">4. Predict the interval which contains the answer of a question</h3>\n\n<p>Finally we’re ready to predict the interval of the passage which contains the answer of the question. To do this we use <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/QuestionPooling.py\">QuestionPooling layer</a> followed by PointerGRU (<a href=\"https://arxiv.org/abs/1506.03134\">Vinyals et al., Pointer networks, 2015</a>).</p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L118\">Code on GitHub</a></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">rQ</span> <span class=\"o\">=</span> <span class=\"n\">QuestionPooling</span><span class=\"p\">()</span> <span class=\"p\">([</span><span class=\"n\">uQ</span><span class=\"p\">,</span> <span class=\"n\">WQ_u</span><span class=\"p\">,</span> <span class=\"n\">WQ_v</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">])</span>\n<span class=\"n\">rQ</span> <span class=\"o\">=</span> <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"n\">rate</span><span class=\"o\">=</span><span class=\"n\">dropout_rate</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'rQ'</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">rQ</span><span class=\"p\">)</span>\n\n<span class=\"o\">...</span>\n\n<span class=\"n\">ps</span> <span class=\"o\">=</span> <span class=\"n\">PointerGRU</span><span class=\"p\">(</span><span class=\"n\">units</span><span class=\"o\">=</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">H</span><span class=\"p\">,</span>\n                <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                <span class=\"n\">initial_state_provided</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'ps'</span><span class=\"p\">)</span> <span class=\"p\">([</span>\n            <span class=\"n\">fake_input</span><span class=\"p\">,</span> <span class=\"n\">hP</span><span class=\"p\">,</span>\n            <span class=\"n\">WP_h</span><span class=\"p\">,</span> <span class=\"n\">Wa_h</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">rQ</span>\n        <span class=\"p\">])</span>\n\n<span class=\"n\">answer_start</span> <span class=\"o\">=</span> <span class=\"n\">Slice</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'answer_start '</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">ps</span><span class=\"p\">)</span>\n<span class=\"n\">answer_end</span> <span class=\"o\">=</span> <span class=\"n\">Slice</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'answer_end'</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"n\">ps</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>QuestionPooling is the attention pooling of the whole question vector <script type=\"math/tex\">u^Q</script>. Its purpose is to create the first hidden state of PointerGRU. It is similar to the other attention-based modules, but has a strange description in the report. Formula 11 on page 5 includes a product of two tensors <script type=\"math/tex\">W_v^Q</script> and <script type=\"math/tex\">V_r^Q</script>. Both these tensors are trainable parameters (as confirmed by Furu Wei, one of the coauthors of the technical report), and it is not clear why this dot product is not replaced by a single trainable vector.</p>\n\n<p><script type=\"math/tex\">h^P</script> is the output of the previous module and it contains the final representation of the passage. It is passed to this module as an input to obtain the final answer.</p>\n\n<p>In Section 4.2 of the <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">technical report</a> the authors write that after submitting their paper to ACL they made one more modification. They have added <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L114-L116\">another bidirectional GRU</a> on top of <script type=\"math/tex\">h^P</script> before feeding it to PointerGRU.</p>\n\n<p><img src=\"https://rawgit.com/YerevaNN/yerevann.github.io/master//public/2017-08-22/PointerGRU.svg\" alt=\"PointerGRU\" title=\"Pointer GRU\" /></p>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/PointerGRU.py\">PointerGRU</a> is a recurrent network that works for just two steps. The first step predicts the first word of the answer span, and the second step predicts the last word. Here is how it works. Both <script type=\"math/tex\">h^P</script> and the previous state of the PointerGRU cell are multiplied by their corresponding weights <script type=\"math/tex\">W</script> and <script type=\"math/tex\">W^a_{v}</script>. Recall that the initial hidden state of the PointerGRU is the output of QuestionPooling. The products are then summed up and passed through <script type=\"math/tex\">tanh</script> activation. The result is multiplied by the weight vector <script type=\"math/tex\">V</script> and <script type=\"math/tex\">softmax</script> activation is applied which outputs scores over <script type=\"math/tex\">h^P</script>. These scores, denoted by <script type=\"math/tex\">a^t</script> are probabilities over the words of the passage. Argmax of <script type=\"math/tex\">a^1</script> vector is the predicted starting point, and argmax of <script type=\"math/tex\">a^2</script> is the predicted final point of the answer (formula 9 on page 4 of the <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">report</a>). The hidden state of PointerGRU is determined based on the dot product of <script type=\"math/tex\">h^P</script> and <script type=\"math/tex\">a^t</script>, which is passed as an input to a simple GRU cell (formula 10 on page 4 of the <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">report</a>). So, unlike all previous modules of R-NET, the <em>output</em> of PointerGRU (the red diamond at the top-right corner of the chart) is different from its hidden state.</p>\n\n<h2 id=\"implementation-details\">Implementation details</h2>\n\n<p>We use Theano backend for Keras. It was faster than TensorFlow in our experiments. Our experience shows that TensorFlow is usually faster for simple network architectures. Probably Theano’s optimization process is more efficient for complex extensions of recurrent networks.</p>\n\n<h4 id=\"layers-with-masking-support\">Layers with masking support</h4>\n\n<p>One of the most important challenges in training recurrent networks is to handle different lengths of data points in a single batch. Keras has a <a href=\"https://keras.io/layers/core/#masking\">Masking layer</a> that handles the basic cases. We use it in the <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L81\">encoding layer</a>. But R-NET has more complex scenarios for which we had to develop our own solutions. For example, in all attention pooling modules we use <script type=\"math/tex\">softmax</script> which is applied along “time” axis (e.g. over the words of the passage). We don’t want to have positive probabilities after the last word of the sentence. So we have implemented a <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/helpers.py#L7\">custom Softmax function</a> which supports masking:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">softmax</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"p\">,</span> <span class=\"n\">mask</span><span class=\"p\">):</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"n\">axis</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">e</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">mask</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"n\">axis</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">clip</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">K</span><span class=\"o\">.</span><span class=\"n\">floatx</span><span class=\"p\">(),</span> <span class=\"bp\">None</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">e</span> <span class=\"o\">/</span> <span class=\"n\">s</span>\n</code></pre></div></div>\n\n<p><code class=\"highlighter-rouge\">m</code> is used for numerical stability. To support masking we <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/helpers.py#L15\">multiply</a> <code class=\"highlighter-rouge\">e</code> by the mask. We also clip <code class=\"highlighter-rouge\">s</code> by a very small number, because in theory it is possible that all positive values of <code class=\"highlighter-rouge\">e</code> are outside the mask.</p>\n\n<p>Note that details like this are not described in the technical report. Probably these are considered as commonly known tricks. But sometimes the details of the masking process can have critical effects on the results (we know this from the work on <a href=\"https://arxiv.org/abs/1703.07771\">medical time series</a>).</p>\n\n<h4 id=\"slice-layer\">Slice layer</h4>\n\n<p><a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/Slice.py\">Slice layer</a> is supposed to slice and return the input tensor at the given indices. It also supports masking. The slice layer in R-NET model is needed to extract the final answer (i.e. the <code class=\"highlighter-rouge\">interval_start</code> and <code class=\"highlighter-rouge\">interval_end</code> numbers). The final output of the model is a tensor with shape <code class=\"highlighter-rouge\">(batch x 2 x passage_length)</code>. The first row contains probabilities for <code class=\"highlighter-rouge\">answer_start</code> and the second one for <code class=\"highlighter-rouge\">answer_end</code>, that’s why we need to <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L134-L135\">slice</a> the rows first and then extract the required information. Obviously we could accomplish the task without creating a new layer, yet it wouldn’t be a “Kerasic” solution.</p>\n\n<h4 id=\"generators\">Generators</h4>\n\n<p>Keras supports <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/data.py#L46\">batch generators</a> which are responsible for generating one batch per each iteration. One benefit of this approach is that the generator is working on a separate thread and is not waiting for the network to finish its training on the previous batch.</p>\n\n<h4 id=\"bidirectional-grus\">Bidirectional GRUs</h4>\n\n<p>R-NET uses multiple bidirectional GRUs. The common way of implementing BiRNN is to take two copies of the same network (without sharing the weights) and then concatenate the hidden states to produce the output. One can take the sum of the vectors instead of concatenating them, but concatenation seems to be more popular (that’s the default version of <a href=\"https://keras.io/layers/wrappers/\">Bidirectional layer</a> in Keras).</p>\n\n<h4 id=\"dropout\">Dropout</h4>\n\n<p>The report indicates that dropout is applied “between layers with a dropout rate of 0.2”. We have applied dropout <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L85\">before each of the three layers</a> of BiGRUs of both encoders, at the <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L87\">outputs of both encoders</a>, right <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L103\">after QuestionAttnGRU</a>, <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L112\">after SelfAttnGRU</a> and <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/model.py#L119\">after QuestionPooling</a> layer. We are not sure that this is exactly what the authors did.</p>\n\n<p>One more implementation detail is related to the way dropout is applied on the passage and question representation matrices. The rows of these matrices correspond to different words and the “vanilla” dropout will apply different masks on different words. These matrices are used as inputs to recurrent networks. But it is a common trick to apply the same mask at each “timestep”, i.e. each word. That’s how dropout is implemented in <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L15\">recurrent layers in Keras</a>. The report doesn’t discuss these details.</p>\n\n<h4 id=\"weight-sharing\">Weight sharing</h4>\n\n<p>The report doesn’t explicitly describe which weights are shared. We have decided to share those weights that are represented by the same symbol in the report. Note that the authors use the same symbol (e.g. <script type=\"math/tex\">c_{t}</script>) for different variables (not weights) that obviously cannot be shared. But we hope that our assumption is true for weights. In particular, we share:</p>\n<ul>\n  <li><script type=\"math/tex\">W^Q_{u}</script> matrix between <code class=\"highlighter-rouge\">QuestionAttnGRU</code> and <code class=\"highlighter-rouge\">QuestionPooling</code> layers,</li>\n  <li><script type=\"math/tex\">W^P_{v}</script> matrix between <code class=\"highlighter-rouge\">QuestionAttnGRU</code> and <code class=\"highlighter-rouge\">SelfAttnGRU</code> layers,</li>\n  <li><script type=\"math/tex\">V</script> vector between all four instances (it is used right before applying softmax).</li>\n</ul>\n\n<p>We didn’t share the weights of the “attention gates”: <script type=\"math/tex\">W_{g}</script>. The reason is that we have a mix of uni- and bidirectional GRUs that use this gate and require different dimensions.</p>\n\n<h4 id=\"hyperparameters\">Hyperparameters</h4>\n\n<p>The authors of the report tell many details about hyperparameters. Hidden vector lengths are 75 for all layers. As we concatenate the hidden states of two GRUs in bidirectional, we effectively get 150 dimensional vectors. 75 is not an even number so it could not refer to the length of the concatenated vector :) <a href=\"http://ruder.io/optimizing-gradient-descent/index.html#adadelta\">AdaDelta optimizer</a> is used to train the network with learning rate=1, <script type=\"math/tex\">\\rho=0.95</script> and <script type=\"math/tex\">\\varepsilon=1e^{-6}</script>. Nothing is written about the size of batches, or the way batches are sampled. We used <code class=\"highlighter-rouge\">batch_size=50</code> in our experiments to fit in 4GB GPU memory.</p>\n\n<p>We couldn’t get good performance with <code class=\"highlighter-rouge\">75</code> hidden units. The models were quickly overfitting. We got our best results using <code class=\"highlighter-rouge\">45</code> dimensional hidden states.</p>\n\n<h4 id=\"weight-initialization\">Weight initialization</h4>\n\n<p>The report doesn’t discuss weight initialization. We used default initialization schemes of Keras. In particular, Keras uses orthogonal initialization for recurrent connections of GRU, and uniform (<a href=\"http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\">Glorot, Bengio, 2010</a>) initialization for the connections that come from the inputs. We used Glorot initialization for <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/layers/SharedWeight.py#L12\">all shared weights</a>. It is not obvious that this was the best solution.</p>\n\n<h4 id=\"training\">Training</h4>\n\n<p>The <a href=\"https://github.com/YerevaNN/R-NET-in-Keras/blob/master/train.py\">training script</a> is very simple. First we create the model:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">RNet</span><span class=\"p\">(</span><span class=\"n\">hdim</span><span class=\"o\">=</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">hdim</span><span class=\"p\">,</span>                                            <span class=\"c\"># Defauls is 45</span>\n             <span class=\"n\">dropout_rate</span><span class=\"o\">=</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">dropout</span><span class=\"p\">,</span>                                 <span class=\"c\"># Default is 0 (0.2 in the report)</span>\n             <span class=\"n\">N</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>                                                    <span class=\"c\"># Size of passage</span>\n             <span class=\"n\">M</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>                                                    <span class=\"c\"># Size of question</span>\n             <span class=\"n\">char_level_embeddings</span><span class=\"o\">=</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">char_level_embeddings</span><span class=\"p\">)</span>          <span class=\"c\"># Default is false</span>\n</code></pre></div></div>\n\n<p>It is possible to slightly speed up computations by fixing <code class=\"highlighter-rouge\">M</code> and <code class=\"highlighter-rouge\">N</code>. It usually helps Theano’s compiler to further optimize the computational graph.</p>\n\n<p>We compile the model and fit it on the training set. Our training data is 90% of the original training set of SQuAD dataset. The other 10% is used as an internal validation dataset. We check the validation score after each epoch and save the current state of the model if it was better than the previous best one. The original <em>development set</em> of SQuAD is used as a test set, we don’t do model selection based on that.</p>\n\n<p>We had an idea to form the batches in a way that passages inside each batch have almost the same number of words. That would allow to train a little bit faster (as there would be many batches with short sequences), but we didn’t use this trick yet. We took maximum 300 words from passages and 30 words from questions to avoid very long sequences.</p>\n\n<p>Each epoch took around 100 minutes on a GTX980 GPU. We got our best results after 31 epochs.</p>\n\n<h2 id=\"results-and-comparison-with-r-net-technical-report\">Results and comparison with <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf\">R-NET technical report</a></h2>\n\n<p>R-NET is currently (August 2017) the <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">best model on Stanford QA</a> benchmark among single models. SQuAD dataset uses two performance metrics: exact match (EM) and F1-score (F1). Human performance is estimated to be EM=82.3% and F1=91.2% on the test set.</p>\n\n<p>The report by Microsoft Research describes two versions of R-NET. The first one is called <em>R-NET (Wang et al., 2017)</em> (which refers to a paper which is not yet available online) and reaches EM=71.3% and F1=79.7% on the test set. It is the model we described above without the additional biGRU between SelfAttnGRU and PointerGRU. The second version called <em>R-NET (March 2017)</em> has the additional BiGRU and reaches EM=72.3% and F1=80.7%. The current best single model on SQuAD leaderboard has a higher score, which means R-NET development continued since the technical report was released. Ensemble models reach even higher scores.</p>\n\n<p>The best performance we got so far with our implementation is EM=57.52% and F1=67.42% on the development set. These results would put R-NET at the bottom of the SQuAD leaderboard. The model is available on <a href=\"https://github.com/YerevaNN/R-NET-in-Keras\">GitHub</a>. We want to emphasize that R-NET’s technical report is pretty good in terms of the reported details of the architecture compared to many other papers. Probably we misunderstood several important details or have bugs in the code. Any feedback will be appreciated.</p>\n\n<h2 id=\"challenges-of-reproducibility\">Challenges of reproducibility</h2>\n\n<p>Recently, ICML 2017 hosted a special <a href=\"https://sites.google.com/view/icml-reproducibility-workshop/home\">workshop</a> devoted to the issues of reproducibility in machine learning. Hugo Larochelle shared the <a href=\"https://drive.google.com/file/d/0B8lLzpxgRHNQZ0paZWQ0cTcxMlNYYnc0TnpHekMxMjVBckVR/view\">slides of his presentation</a>, where he discussed many aspects of the problem. He argues that the research should be considered as reproducible if the code is open-sourced. On the other hand he suggests that the community should not require researchers to compare their new models with a related published result if the code for the latter is not available.</p>\n\n<p>As a radical solution he suggests to use platforms like <a href=\"http://ai-on.org/\">AI-ON</a>. AI-ON is open-sourcing not only the code, but the whole research process, including discussions and code experiments. We think about starting AI-ON projects just for reproducing the results of important papers that come without code.</p>\n\n<p>On the other hand, there are many simple tricks that can significantly improve reproducibility with little effort. For example, many papers report the number of parameters in the neural network. This number is a good checksum for other people. Another simple trick is to write the shapes of the tensors in the diagrams (just like we did in this post) or even in the text.</p>\n\n<p>The best open source model on SQuAD that we are aware of is the implementation of <a href=\"https://arxiv.org/abs/1704.00051\">DrQA architecture</a> released in Facebook’s <a href=\"https://github.com/facebookresearch/ParlAI\">ParlAI repository</a>. It <a href=\"https://github.com/facebookresearch/ParlAI/issues/109\">reaches</a> EM=66.4% and F1=76.5%. We will continue to play with our codebase and try to improve the results.</p>"
}