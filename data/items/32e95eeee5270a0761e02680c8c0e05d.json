{
  "title": "Deploy a machine learning inference data capture solution on AWS Lambda",
  "link": "https://aws.amazon.com/blogs/machine-learning/deploy-a-machine-learning-inference-data-capture-solution-on-aws-lambda/",
  "dc:creator": "Dan Fox",
  "pubDate": "Wed, 26 Oct 2022 15:54:07 +0000",
  "category": [
    "AWS Lambda",
    "Technical How-to"
  ],
  "guid": "62b0cfaa32e357ea2d152451463de2662afebd53",
  "description": "Monitoring machine learning (ML) predictions can help improve the quality of deployed models. Capturing the data from inferences made in production can enable you to monitor your deployed models and detect deviations in model quality. Early and proactive detection of these deviations enables you to take corrective actions, such as retraining models, auditing upstream systems, […]",
  "content:encoded": "<p>Monitoring machine learning (ML) predictions can help improve the quality of deployed models. Capturing the data from inferences made in production can enable you to monitor your deployed models and detect deviations in model quality. Early and proactive detection of these deviations enables you to take corrective actions, such as retraining models, auditing upstream systems, or fixing quality issues.</p> \n<p><a href=\"https://aws.amazon.com/lambda/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Lambda</a> is a serverless compute service that can provide real-time ML inference at scale. In this post, we demonstrate a sample data capture feature that can be deployed to a Lambda ML inference workload.</p> \n<p>In December 2020, Lambda introduced support for container images as a packaging format. This feature increased the deployment package size limit from 500 MB to 10 GB. Prior to this feature launch, the package size constraint made it difficult to deploy ML frameworks like TensorFlow or PyTorch to Lambda functions. After the launch, the increased package size limit made ML a viable and attractive workload to deploy to Lambda. In 2021, ML inference was one of the fastest growing workload types in the Lambda service.</p> \n<p><a href=\"https://aws.amazon.com/pm/sagemaker/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker</a>, Amazon’s fully managed ML service, contains its own model monitoring feature. However, the sample project in this post shows how to perform data capture for use in model monitoring for customers who use Lambda for ML inference. The project uses Lambda extensions to capture inference data in order to minimize the impact on the performance and latency of the inference function. Using Lambda extensions also minimizes the impact on function developers. By integrating via an extension, the monitoring feature can be applied to multiple functions and maintained by a centralized team.</p> \n<h2>Overview of solution</h2> \n<p>This project contains source code and supporting files for a serverless application that provides real-time inferencing using a distilbert-base, pretrained question answering model. The project uses the Hugging Face question and answer natural language processing (NLP) model with <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a> to perform natural language inference tasks. The project also contains a solution to perform inference data capture for the model predictions. The Lambda function writer can determine exactly which data from the inference request input and the prediction result to send to the extension. In this solution, we send the input and the answer from the model to the extension. The extension then periodically sends the data to an <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Simple Storage Service</a> (Amazon S3) bucket. We build the data capture extension as a container image using a <code>makefile</code>. We then build the Lambda inference function as a container image and add the extension container image as a container image layer. The following diagram shows an overview of the architecture.<a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/07/ML-8411-arch-diag.png\"><img loading=\"lazy\" class=\"alignnone size-full wp-image-43994\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/07/ML-8411-arch-diag.png\" alt=\"Solution overview\" width=\"979\" height=\"439\"></a></p> \n<p>Lambda extensions are a way to augment Lambda functions. In this project, we use an external Lambda extension to log the inference request and the prediction from the inference. The external extension runs as a separate process in the Lambda runtime environment, diminishing the impact on the inference function. However, the function shares resources such as CPU, memory, and storage with the Lambda function. We recommend allocating enough memory to the Lambda function to ensure optimal resource availability. (In our testing, we allocated 5 GB of memory to the inference Lambda function and saw optimal resource availability and inference latency). When an inference is complete, the Lambda service returns the response immediately and doesn’t wait for the extension to finish logging the request and response to the S3 bucket. With this pattern, the monitoring extension doesn’t affect the inference latency. To learn more about Lambda extensions <a href=\"https://serverlessland.com/learn/lambda-extensions\" target=\"_blank\" rel=\"noopener noreferrer\">check out these video series</a>.</p> \n<h2>Project contents</h2> \n<p>This project uses the <a href=\"https://aws.amazon.com/serverless/sam/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Serverless Application Model</a> (AWS SAM) command line interface (CLI). This command-line tool allows developers to initialize and configure applications; package, build, and test locally; and deploy to the AWS Cloud.</p> \n<p>You can download the source code for this project from <a href=\"https://github.com/aws-samples/aws-lambda-ml-monitoring\" target=\"_blank\" rel=\"noopener noreferrer\">the GitHub repository</a>.</p> \n<p>This project includes the following files and folders:</p> \n<ul> \n <li><strong>app/app.py</strong> – Code for the application’s Lambda function, including the code for ML inferencing.</li> \n <li><strong>app/Dockerfile</strong> – The Dockerfile to build the container image that packages the inference function, the model downloaded from Hugging Face, and the Lambda extension built as a layer. In contrast to .zip functions, layers can’t be attached to container-packaged Lambda functions at function create time. Instead, we build the layer and copy its contents into the container image.</li> \n <li><strong>Extensions</strong> – The model monitor extension files. This Lambda extension is used to log the input to the inference function and the corresponding prediction to an S3 bucket.</li> \n <li><strong>app/model</strong> – The model downloaded from Hugging Face.</li> \n <li><strong>app/requirements.txt</strong> – The Python dependencies to be installed into the container.</li> \n <li><strong>events</strong> – Invocation events that you can use to test the function.</li> \n <li><strong>template.yaml</strong> – A descriptor file that defines the application’s AWS resources.</li> \n</ul> \n<p>The application uses several AWS resources, including Lambda functions and an <a href=\"https://aws.amazon.com/api-gateway/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon API Gateway</a> API. These resources are defined in the <code>template.yaml</code> file in this project. You can update the template to add AWS resources through the same deployment process that updates your application code.</p> \n<h2>Prerequisites</h2> \n<p>For this walkthrough, you should have the following prerequisites:</p> \n<ul> \n <li>An <a href=\"https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&amp;client_id=signup\" target=\"_blank\" rel=\"noopener noreferrer\">AWS account</a></li> \n <li>The AWS SAM CLI <a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html\">installed</a></li> \n <li>Docker community edition <a href=\"https://hub.docker.com/search/?type=edition&amp;offering=community\">installed</a></li> \n <li><a href=\"https://www.python.org/downloads/\" target=\"_blank\" rel=\"noopener noreferrer\">Python 3.8.x</a></li> \n</ul> \n<h2>Deploy the sample application</h2> \n<p>To build your application for the first time, complete the following steps:</p> \n<ul> \n <li>Run the following code in your shell. (This will builds the extension as well):</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">sam build</code></pre> \n</div> \n<ul> \n <li>Build a Docker image of the model monitor application. The build contents reside in the <code>.aws-sam</code> directory</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">docker build -t serverless-ml-model-monitor:latest .</code></pre> \n</div> \n<ul> \n <li>Tag your container for deployment to <a href=\"https://aws.amazon.com/ecr/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Elastic Container Registry</a> (Amazon ECR)</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">docker tag serverless-ml-model-monitor:latest <span style=\"color: #ff0000\">&lt;aws-account-id&gt;</span>.dkr.ecr.us-east-1.amazonaws.com/serverless-ml-model-monitor:latest</code></pre> \n</div> \n<ul> \n <li>Login to Amazon ECR:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">aws ecr get-login-password --region us-east-1 docker login --username AWS --password-stdin <span style=\"color: #ff0000\">&lt;aws-account-id&gt;</span>.dkr.ecr.us-east-1.amazonaws.com</code></pre> \n</div> \n<ul> \n <li>Create a repository in Amazon ECR:</li> \n</ul> \n<p><code>aws ecr create-repository</code><code>repository-name serverless-ml-model-monitor</code><code>--image-scanning-configuration scanOnPush=true</code><code>--region us-east-1</code></p> \n<ul> \n <li>Push the container image to Amazon ECR:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">docker push <span style=\"color: #ff0000\">&lt;aws-account-id&gt;</span>.dkr.ecr.us-east-1.amazonaws.com/serverless-ml-model-monitor:latest</code></pre> \n</div> \n<ul> \n <li>Uncomment line #1 in <strong>app/Dockerfile</strong> and edit it to point to the correct ECR repository image, then uncomment lines #6 and #7 in <strong>app/Dockerfile:</strong></li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-yaml\">WORKDIR /opt\nCOPY --from=layer /opt/ .</code></pre> \n</div> \n<ul> \n <li>Build the application again:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">sam build</code></pre> \n</div> \n<p>We build again because Lambda doesn’t support Lambda layers directly for the container image packaging type. We need to first build the model monitoring component as a container image, upload it to Amazon ECR, and then use that image in the model monitoring application as a container layer.</p> \n<ul> \n <li>Finally, deploy the Lambda function, API Gateway, and extension:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">sam deploy --guided</code></pre> \n</div> \n<p>This command packages and deploys your application to AWS with a series of prompts:</p> \n<ul> \n <li><strong>Stack name </strong>: The name of the deployed <a href=\"https://aws.amazon.com/cloudformation/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS CloudFormation</a> stack. This should be unique to your account and Region, and a good starting point would be something matching your project name.</li> \n <li><strong>AWS Region </strong>: The AWS Region to which you deploy your application.</li> \n <li><strong>Confirm changes before deploy </strong>: If set to <code>yes</code>, any change sets are shown to you before running for manual review. If set to no, the AWS SAM CLI automatically deploys application changes.</li> \n <li><strong>Allow AWS SAM CLI IAM role creation </strong>: Many AWS SAM templates, including this example, create <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Identity and Access Management</a> (IAM) roles required for the Lambda function(s) included to access AWS services. By default, these are scoped down to the minimum required permissions. To deploy a CloudFormation stack that creates or modifies IAM roles, the <code>CAPABILITY_IAM</code> value for <code>capabilities</code> must be provided. If permission isn’t provided through this prompt, to deploy this example you must explicitly pass <code>--capabilities CAPABILITY_IAM</code> to the <code>sam deploy</code> command.</li> \n <li><strong>Save arguments to samconfig.toml </strong>: If set to <code>yes</code>, your choices are saved to a configuration file inside the project so that in the future, you can just run <code>sam deploy</code> without parameters to deploy changes to your application.</li> \n</ul> \n<p>You can find your API Gateway endpoint URL in the output values displayed after deployment.</p> \n<h2>Test the application</h2> \n<p>To test the application, use Postman or curl to send a request to the API Gateway endpoint. For example:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">curl -X POST -H \"Content-Type: text/plain\" https://<span style=\"color: #ff0000\">&lt;api-id&gt;</span>.execute-api.us-east-1.amazonaws.com/Prod/nlp-qa -d '{\"question\": \"Where do you live?\", \"context\": \"My name is Clara and I live in Berkeley.\"}'</code></pre> \n</div> \n<p>You should see output like the following code. The ML model inferred from the context and returned the answer for our question.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">{\n    \"Question\": \"Where do you live?\",\n    \"Answer\": \"Berkeley\",\n    \"score\": 0.9113729596138\n}</code></pre> \n</div> \n<p>After a few minutes, you should see a file in the S3 bucket <code>nlp-qamodel-model-monitoring-modelmonitorbucket-&lt;xxxxxx&gt;</code> with the input and the inference logged.</p> \n<h2>Clean up</h2> \n<p>To delete the sample application that you created, use the AWS CLI:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">aws cloudformation delete-stack --stack-name <span style=\"color: #ff0000\">&lt;stack-name&gt;</span></code></pre> \n</div> \n<h2>Conclusion</h2> \n<p>In this post, we implemented a model monitoring feature as a Lambda extension and deployed it to a Lambda ML inference workload. We showed how to build and deploy this solution to your own AWS account. Finally, we showed how to run a test to verify the functionality of the monitor.</p> \n<p>Please provide any thoughts or questions in the comments section. For more serverless learning resources, visit&nbsp;<a href=\"https://serverlessland.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Serverless Land</a>.</p> \n<hr> \n<h3>About the Authors</h3> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/Dan-Fox-150x150-1.png\"><img loading=\"lazy\" class=\"size-full wp-image-44058 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/Dan-Fox-150x150-1.png\" alt=\"\" width=\"100\" height=\"100\"></a>Dan Fox</strong> is a Principal Specialist Solutions Architect in the Worldwide Specialist Organization for Serverless. Dan works with customers to help them leverage serverless services to build scalable, fault-tolerant, high-performing, cost-effective applications. Dan is grateful to be able to live and work in lovely Boulder, Colorado.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/09/28/Jain-Newton.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-28633 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/09/28/Jain-Newton.jpg\" alt=\"\" width=\"100\" height=\"120\"></a>Newton Jain</strong>&nbsp;is a Senior Product Manager responsible for building new experiences for machine learning, high performance computing (HPC), and media processing customers on AWS Lambda. He leads the development of new capabilities to increase performance, reduce latency, improve scalability, enhance reliability, and reduce cost. He also assists AWS customers in defining an effective serverless strategy for their compute-intensive applications.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/DikshaSharma-150x150-1.png\"><img loading=\"lazy\" class=\"size-full wp-image-44057 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/DikshaSharma-150x150-1.png\" alt=\"\" width=\"100\" height=\"100\"></a>Diksha Sharma</strong> is a Solutions Architect and a Machine Learning Specialist at AWS. She helps customers accelerate their cloud adoption, particularly in the areas of machine learning and serverless technologies. Diksha deploys customized proofs of concept that show customers the value of AWS in meeting their business and IT challenges. She enables customers in their knowledge of AWS and works alongside customers to build out their desired solution.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/VedaRaman-150x150-1.png\"><img loading=\"lazy\" class=\"size-full wp-image-44056 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/VedaRaman-150x150-1.png\" alt=\"\" width=\"100\" height=\"100\"></a>Veda Raman </strong>is a Senior Specialist Solutions Architect for machine learning based in Maryland. Veda works with customers to help them architect efficient, secure and scalable machine learning applications. Veda is interested in helping customers leverage serverless technologies for Machine learning.</p> \n<p style=\"clear: both\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/JoshKahn-150x150-1.png\"><img loading=\"lazy\" class=\"size-full wp-image-44055 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/10/JoshKahn-150x150-1.png\" alt=\"\" width=\"100\" height=\"100\"></a> Josh Kahn</strong> is the Worldwide Tech Leader for Serverless and a Principal Solutions Architect. He leads a global community of serverless experts at AWS who help customers of all sizes, from start-ups to the world’s largest enterprises, to effectively use AWS serverless technologies.</p>"
}