{
  "title": "High level performance of Pandas, Dask, Spark, and Arrow",
  "link": "",
  "updated": "2018-08-28T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/08/28/dataframe-performance-high-level",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc</a></em></p>\n\n<h2 id=\"question\">Question</h2>\n\n<blockquote>\n  <p>How does Dask dataframe performance compare to Pandas?  Also, what about\nSpark dataframes and what about Arrow?  How do they compare?</p>\n</blockquote>\n\n<p>I get this question every few weeks.  This post is to avoid repetition.</p>\n\n<h2 id=\"caveats\">Caveats</h2>\n\n<ol>\n  <li>This answer is likely to change over time.  I’m writing this in August 2018</li>\n  <li>This question and answer are very high level.\nMore technical answers are possible, but not contained here.</li>\n</ol>\n\n<h2 id=\"answers\">Answers</h2>\n\n<h3 id=\"pandas\">Pandas</h3>\n\n<p>If you’re coming from Python and have smallish datasets then Pandas is the\nright choice.  It’s usable, widely understood, efficient, and well maintained.</p>\n\n<h3 id=\"benefits-of-parallelism\">Benefits of Parallelism</h3>\n\n<p>The performance benefit (or drawback) of using a parallel dataframe like Dask\ndataframes or Spark dataframes over Pandas will differ based on the kinds of\ncomputations you do:</p>\n\n<ol>\n  <li>\n    <p>If you’re doing small computations then Pandas is always the right choice.\nThe administrative costs of parallelizing will outweigh any benefit.\nYou should not parallelize if your computations are taking less than, say,\n100ms.</p>\n  </li>\n  <li>\n    <p>For simple operations like filtering, cleaning, and aggregating large data\nyou should expect linear speedup by using a parallel dataframes.</p>\n\n    <p>If you’re on a 20-core computer you might expect a 20x speedup.  If you’re\non a 1000-core cluster you might expect a 1000x speedup, assuming that you\nhave a problem big enough to spread across 1000 cores.  As you scale up\nadministrative overhead will increase, so you should expect the speedup to\ndecrease a bit.</p>\n  </li>\n  <li>\n    <p>For complex operations like distributed joins it’s more complicated.  You\nmight get linear speedups like above, or you might even get slowdowns.\nSomeone experienced in database-like computations and parallel computing\ncan probably predict pretty well which computations will do well.</p>\n  </li>\n</ol>\n\n<p>However, configuration may be required.  Often people find that parallel\nsolutions don’t meet expectations when they first try them out.  Unfortunately\nmost distributed systems require some configuration to perform optimally.</p>\n\n<h3 id=\"there-are-other-options-to-speed-up-pandas\">There are other options to speed up Pandas</h3>\n\n<p>Many people looking to speed up Pandas don’t need parallelism.  There are often\nseveral other tricks like encoding text data, using efficient file formats,\navoiding groupby.apply, and so on that are more effective at speeding up Pandas\nthan switching to parallelism.</p>\n\n<h3 id=\"comparing-apache-spark-and-dask\">Comparing Apache Spark and Dask</h3>\n\n<blockquote>\n  <p>Assuming that yes, I do want parallelism, should I choose Apache Spark, or Dask dataframes?</p>\n</blockquote>\n\n<p>This is often decided more by cultural preferences (JVM vs Python,\nall-in-one-tool vs integration with other tools) than performance differences,\nbut I’ll try to outline a few things here:</p>\n\n<ul>\n  <li>Spark dataframes will be much better when you have large SQL-style queries\n(think 100+ line queries) where their query optimizer can kick in.</li>\n  <li>Dask dataframes will be much better when queries go beyond typical database\nqueries.  This happens most often in time series, random access, and other\ncomplex computations.</li>\n  <li>Spark will integrate better with JVM and data engineering technology.\nSpark will also come with everything pre-packaged.  Spark is its own\necosystem.</li>\n  <li>Dask will integrate better with Python code.  Dask is designed to integrate\nwith other libraries and pre-existing systems.  If you’re coming from an\nexisting Pandas-based workflow then it’s usually much easier to evolve to\nDask.</li>\n</ul>\n\n<p>Generally speaking for most operations you’ll be fine using either one.  People\noften choose between Pandas/Dask and Spark based on cultural preference.\nEither they have people that really like the Python ecosystem, or they have\npeople that really like the Spark ecosystem.</p>\n\n<p>Dataframes are also only a small part of each project.  Spark and Dask both do\nmany other things that aren’t dataframes.  For example Spark has a graph\nanalysis library, Dask doesn’t.  Dask supports multi-dimensional arrays, Spark\ndoesn’t.  Spark is generally higher level and all-in-one while Dask is\nlower-level and focuses on integrating into other tools.</p>\n\n<p>For more information, see <a href=\"http://dask.pydata.org/en/latest/spark.html\">Dask’s “Comparison to Spark documentation”</a>.</p>\n\n<h3 id=\"apache-arrow\">Apache Arrow</h3>\n\n<blockquote>\n  <p>What about Arrow?  Is Arrow faster than Pandas?</p>\n</blockquote>\n\n<p>This question doesn’t quite make sense… <em>yet</em>.</p>\n\n<p>Arrow is not a replacement for Pandas.  Today Arrow is useful to people\nbuilding <em>systems</em> and not to analysts directly like Pandas.  Arrow is used to\nmove data between different computational systems and file formats.  Arrow does\nnot do computation today, but is commonly used as a component in other\nlibraries that do do computation.  For example, if you use Pandas or Spark or\nDask today you may be using Arrow without knowing it.  Today Arrow is more\nuseful for other libraries than it is to end-users.</p>\n\n<p>However, this is likely to change in the future.  Arrow developers plan\nto write computational code around Arrow that we would expect to be faster than\nthe code in either Pandas or Spark.  This is probably a year or two away\nthough.  There will probably be some effort to make this semi-compatible with\nPandas, but it’s much too early to tell.</p>"
}