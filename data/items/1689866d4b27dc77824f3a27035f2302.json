{
  "title": "How not to sort by average rating, revisited",
  "link": "",
  "published": "2015-07-11T20:30:00-07:00",
  "updated": "2015-07-11T20:30:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2015-07-11:/ranking-revisited",
  "summary": "<p>What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by <a href=\"http://www.evanmiller.org/\">Evan Miller</a> to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose …</p>",
  "content": "<p>What is the best method for ranking items that have positive and negative reviews? Some sites, including reddit, have adopted an algorithm suggested by <a href=\"http://www.evanmiller.org/\">Evan Miller</a> to generate their item rankings. However, this algorithm can sometimes be unfairly pessimistic about new, good items. This is especially true of items whose first few votes are negative &#8212; an issue that can be &#8220;gamed&#8221; by adversaries. In this post, we consider three alternative ranking methods that can enable high-quality items to more-easily bubble-up. The last is the simplest, but continues to give good results: One simply seeds each item&#8217;s vote count with a suitable fixed number of hidden &#8220;starter&#8221;&nbsp;votes.</p>\n<h3>Introduction &#8212; a review of Evan Miller&#8217;s&nbsp;post</h3>\n<p>In an <a href=\"http://www.evanmiller.org/how-not-to-sort-by-average-rating.html\">insightful prior post</a>, Evan Miller (<span class=\"caps\">EM</span>) considered the problem of ranking items that had been reviewed as positive or negative (up-voted or down-voted, represented by a 1 or a 0, respectively) by a sample of users. He began by illustrating that two of the more readily-arrived at solutions to this problem are highly flawed. To&nbsp;review:</p>\n<p><strong>Bad method 1:</strong> Rank item <span class=\"math\">\\(i\\)</span> by <span class=\"math\">\\(n_i(1) - n_i(0)\\)</span>, its up-vote count minus its down-vote&nbsp;count.</p>\n<p><em>Issue:</em> If one item has garnered 60 up-votes and 40 down-votes, it will get the same score as an item with only 20 votes, all positive. Yet, the latter has a 100% up-vote rate (20 for 20), suggesting that it is of very high quality. Despite this, the algorithm ranks the two&nbsp;equally.</p>\n<p><strong>Bad method 2:</strong> Rank item <span class=\"math\">\\(i\\)</span> by <span class=\"math\">\\(\\hat{p} \\equiv n_i(1)/[n_i(0) + n_i(1)]\\)</span>, its sample up-vote rate (average&nbsp;rating).</p>\n<p><em>Issue:</em> If any one item has only one vote, an up-vote, it will be given a perfect score by this algorithm. This means that it will be ranked above all other items, despite the fact that a single vote is not particularly informative/convincing. In general, this method can work well, but only once each item has a significant number of&nbsp;votes.</p>\n<p>To avoid the issues of these two bad methods (BMs), <span class=\"caps\">EM</span> suggests scoring and ranking each item by the <em>lower limit of its up-vote-rate confidence interval</em>. This is (<a href=\"https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval\"><span class=\"caps\">E.B.</span> Wilson, 1927</a>),\n</p>\n<div class=\"math\">$$\\tag{1} \\label{emsol}\np_{W} = \\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p}) + \\frac{z_{\\alpha/2}^2}{4n} }{n}}}{1 + \\frac{z_{\\alpha/2}^2}{n}},\n$$</div>\n<p>\nwhere <span class=\"math\">\\(\\hat{p}\\)</span> is again the sample up-vote rate, <span class=\"math\">\\(z_{\\alpha/2}\\)</span> is a positive constant that sets the size of the confidence interval used, and <span class=\"math\">\\(n\\)</span> is the total number of votes that have so far been recorded. The score <span class=\"math\">\\(p_{W}\\)</span> approaches <span class=\"math\">\\(\\hat{p}\\)</span> once an item has a significant number of votes &#8212; it consequently avoids the pitfall of <span class=\"caps\">BM1</span> above. By construction, it also avoids the pitfall of <span class=\"caps\">BM2</span>. With both of these pitfalls avoided, the <span class=\"caps\">EM</span> method can sometimes provide a reasonable, practical ranking&nbsp;system.</p>\n<h3>Potential issue with&nbsp;(\\ref{emsol})</h3>\n<p>Although (\\ref{emsol}) does a good job of avoiding the pitfall associated with <span class=\"caps\">BM2</span>, it can do a poor job of handling a related pitfall: If any new item has only a few votes, and these each happen to be down-votes, its sample up-vote rate will be <span class=\"math\">\\(\\hat{p} = 0\\)</span>. In this case, (\\ref{emsol})&nbsp;gives\n</p>\n<div class=\"math\">$$\\label{problem} \\tag{2}\np_{W} = \\left .\\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p}) + \\frac{z_{\\alpha/2}^2}{4n} }{n}}}{1 + \\frac{z_{\\alpha/2}^2}{n}}\\right \\vert_{\\hat{p} = 0} = 0.\n$$</div>\n<p>\nNow, <span class=\"math\">\\(p_W\\)</span> is always between <span class=\"math\">\\(0\\)</span> and <span class=\"math\">\\(1\\)</span>, so (\\ref{problem}) implies that any new, quickly-down-voted item will immediately be ranked below all others. This is extremely harsh and potentially unfair. For example, consider the case of a newly-opened restaurant: If an adversary were to quickly down-vote this restaurant on some ranking site &#8212; the day of its opening &#8212; the new restaurant would be ranked below all others, including the adversary. This would occur even if the new restaurant were of very high true quality. This could have potentially-damaging consequences, for both the restaurant and the ranking site &#8212; whose lists should provide only the best&nbsp;recommendations!</p>\n<p>An ideal ranking system should explicitly take into account the large uncertainty present when only a small number of votes have been recorded. The score (\\ref{emsol}) does a good job of this on the high <span class=\"math\">\\(\\hat{p}\\)</span> end, but a poor job on the low <span class=\"math\">\\(\\hat{p}\\)</span> end. This approach may be appropriate for cases where one is risk-averse on the high end only, but in general one should protect against both sorts of quick, strong judgements. Below we consider some alternative, <a href=\"https://en.wikipedia.org/wiki/Bayesian_statistics\">Bayesian</a> ranking solutions. The last is easy to understand and implement: One simply gives each item a hidden number of up- and down-votes to start with. These hidden &#8220;starter&#8221; votes can be chosen in various ways &#8212; they serve to simply bias new items towards an intermediate value early on, with the bias becoming less important as more votes come in. This approach avoids each of the pitfalls we have&nbsp;discussed.</p>\n<h3>Bayesian&nbsp;formulation</h3>\n<p>Note: This section and the next are both fairly mathematical. They can be skipped for those wishing to focus on application method&nbsp;only.</p>\n<p>To start our Bayesian analysis, we begin by positing a general beta distribution for the up-vote rate prior&nbsp;distribution,\n</p>\n<div class=\"math\">$$\\tag{3}\\label{beta}\nP(p) = \\tilde{\\mathcal{N}} p^a (1-p)^b.\n$$</div>\n<p>\nHere, <span class=\"math\">\\(\\tilde{\\mathcal{N}}\\)</span> is a normalization factor and <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span> are some constants (we suggest methods for choosing their values in the discussion section). The function <span class=\"math\">\\(P(p)\\)</span> specifies an initial guess &#8212; in the absence of any reviews for an item &#8212; for what we think the probability is that it will have up-vote rate <span class=\"math\">\\(p\\)</span>. If item <span class=\"math\">\\(i\\)</span> actually has been reviewed, we can update our guess for its distribution using <a href=\"https://en.wikipedia.org/wiki/Bayes'_theorem\">Bayes&#8217; rule</a>:\n</p>\n<div class=\"math\">$$\\begin{align} \\tag{4} \\label{bayes_rule}\nP(p \\vert n_i(1), n_i(0)) =\\frac{ P( n_i(1), n_i(0) \\vert p ) P(p)}{P(n_i(1), n_i(0))} = \\mathcal{N} p^{n_i(1)+a}(1-p)^{n_i(0)+b}.\n\\end{align}\n$$</div>\n<p>\nHere, we have evaluated <span class=\"math\">\\( P( n(1), n(0) \\vert p )\\)</span> using the <a href=\"https://en.wikipedia.org/wiki/Binomial_distribution\">binomial distribution</a>, we&#8217;ve plugged in (\\ref{beta}) for <span class=\"math\">\\(P(p)\\)</span>, and we&#8217;ve collected all <span class=\"math\">\\(p\\)</span>-independent factors into the new normalization factor <span class=\"math\">\\(\\mathcal{N}\\)</span>. The formula (\\ref{bayes_rule}) provides the basis for the three ranking methods discussed&nbsp;below.</p>\n<h3>Three Bayesian ranking&nbsp;systems</h3>\n<p>Let&#8217;s&nbsp;rank!</p>\n<p><strong>Bayesian method 1:</strong> Choose the ordering that is most&nbsp;likely.</p>\n<p>It is a simple matter to write down a formal expression for the probability of any ranking. For example, given two items we&nbsp;have\n</p>\n<div class=\"math\">$$\nP(p_1 &gt; p_2) = \\int_0^1 dp_1 \\int_0^{p_1} dp_2 P(p_1) P(p_2). \\tag{5} \\label{int}\n$$</div>\n<p>\nPlugging in (\\ref{bayes_rule}) for the <span class=\"math\">\\(P(p_i)\\)</span><span class=\"quo\">&#8216;</span>s, this can be evaluated numerically. Evaluating the probability for the opposite ordering, we can then choose that which is most likely to be&nbsp;correct.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Pros:</em> Approach directly optimizes for the object we&#8217;re interested in, the ranking &#8212; very&nbsp;appealing!</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Cons:</em> Given <span class=\"math\">\\(N\\)</span> items, one has <span class=\"math\">\\(N!\\)</span> integrals to carry out &#8212; untenable for large <span class=\"math\">\\(N\\)</span>.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Note:</em> See posssiblywrong&#8217;s post <a href=\"https://possiblywrong.wordpress.com/2014/05/31/reddits-comment-ranking-algorithm-revisited/\">here</a> for some related, interesting&nbsp;points.</p>\n<p><strong>Bayesian method 2:</strong> Rank item <span class=\"math\">\\(i\\)</span> by its median <span class=\"math\">\\(p\\)</span>-value.</p>\n<p>Sorting by an item score provides an approach that will scale well even at large <span class=\"math\">\\(N\\)</span>. A natural score to consider is an item&#8217;s median <span class=\"math\">\\(p\\)</span>-value: that which it has a <span class=\"math\">\\(50/50\\)</span> shot of being larger (or smaller) than. Using (\\ref{bayes_rule}), this&nbsp;satisfies\n</p>\n<div class=\"math\">$$\\tag{6}\\label{m2}\n\\frac{\\int_0^{p_{med}} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp}{\\int_0^{1} p^{n_i(1)+a}(1-p)^{n_i(0)+b} dp} = 1/2.\n$$</div>\n<p>\nThe integral at left actually has a name &#8212; it&#8217;s called the <a href=\"http://mathworld.wolfram.com/IncompleteBetaFunction.html\">incomplete beta function</a>. Using a statistics package, it can be inverted to give <span class=\"math\">\\(p_{med}\\)</span>. For example, if we set <span class=\"math\">\\(a = b = 1\\)</span>, an item with a single up-vote and no down-votes would get a score of <span class=\"math\">\\(0.614\\)</span>. In other words, we&#8217;d guess there&#8217;s a 50/50 shot that the item&#8217;s up-vote rate falls above this value, so we&#8217;d rank it higher than any other item whose <span class=\"math\">\\(p\\)</span> value is known to be smaller than&nbsp;this.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Pros:</em> Sorting is fast. Gives intuitive, meaningful score for each&nbsp;item.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Cons:</em> Inverting (\\ref{m2}) can be somewhat slow, e.g. <span class=\"math\">\\(\\sim 10^{-3}\\)</span> seconds in&nbsp;Mathematica.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Note</em>: <span class=\"caps\">EM</span> also derived this score function, in a follow-up to his original post. However, he motivated it in a slightly different way &#8212; see <a href=\"http://www.evanmiller.org/bayesian-average-ratings.html\">here</a>.</p>\n<p><strong>Bayesian method 3:</strong> Rank item <span class=\"math\">\\(i\\)</span> by its most likely (aka <a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\"><span class=\"caps\">MAP</span></a>) <span class=\"math\">\\(p\\)</span>-value.</p>\n<p>The most likely <span class=\"math\">\\(p\\)</span>-value for each item provides another natural score function. To find this, we simply set the derivative of (\\ref{bayes_rule}) to&nbsp;zero,\n</p>\n<div class=\"math\">$$\n\\begin{align}\n\\partial_p p^{n_i(1)+a}(1-p)^{n_i(0)+b} &amp;= \\left (\\frac{n_i(1)+a}{p} + \\frac{n_i(0)+b}{1-p} \\right ) p^{n_i(1)+a}(1-p)^{n_i(0)+b} = 0 \\\\\n\\to p = \\tilde{p} &amp;\\equiv \\frac{n_i(1)+a}{(n_i(1)+a) + (n_i(0)+b)}. \\tag{7} \\label{final}\n\\end{align}\n$$</div>\n<p>\nThis form <span class=\"math\">\\(\\tilde{p}\\)</span> is interesting because it resembles the sample mean <span class=\"math\">\\(\\hat{p}\\)</span> considered above. However, the actual number of up- and down-votes, <span class=\"math\">\\(n_i(1)\\)</span> and <span class=\"math\">\\(n_i(0)\\)</span>, are supplemented in (\\ref{final}) by <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span>, respectively. We can thus interpret these values as effective &#8220;starter votes&#8221;, given to each item before any real reviews are recorded. Their effect is to bias our guess for <span class=\"math\">\\(p\\)</span> towards the prior&#8217;s peak value, with the bias being most strong when <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span> are chosen large and/or when we have few actual votes present. For any non-zero choices, (\\ref{final}) avoids each of the pitfalls discussed above. Further, it approaches the true up-vote rate in the limit of large review sample sizes, as&nbsp;required.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Pros:</em> Sorting is fast. Simple method for avoiding the common&nbsp;pitfalls.</p>\n<p><em><span class=\"math\">\\(\\bullet\\)</span> Cons:</em> Have to pick <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span> &#8212; see below for suggested&nbsp;methods.</p>\n<h3>Discussion</h3>\n<p>We consider each of the four ranking methods we&#8217;ve discussed here to be interesting and useful &#8212; the three Bayesian ranking systems, as well as <a href=\"http://www.evanmiller.org/how-not-to-sort-by-average-rating.html\"><span class=\"caps\">EM</span>&#8217;s original system</a>, which works well when one only needs to protect against false positives (again, we note that Bayesian method 2 was also considered by <span class=\"caps\">EM</span> in a <a href=\"http://www.evanmiller.org/bayesian-average-ratings.html\">follow-up</a> to his original post). In practice, the three Bayesian approaches will each tend to return similar, but sometimes slightly different rankings. With regards to &#8220;correctness&#8221;, the essential point is that each method is well-motivated and avoids the common pitfalls. However, the final method is the easiest to apply, so it might be the most&nbsp;practical.</p>\n<p>To apply the Bayesian methods, one must specify the <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span> values defining the prior, (\\ref{bayes_rule}). We suggest three methods for choosing these: 1) Choose these values to provide a good approximation to your actual distribution, fitting only to items for which you have good statistics. 2) A/B test to get the ranking that optimizes some quantity you are interested in, e.g. clicks. 3) Heuristics: For example, if simplicity is key, choose <span class=\"math\">\\(a= b =1\\)</span>, which biases towards an up-vote rate of <span class=\"math\">\\(0.5\\)</span>. If a conservative estimate is desired for new items, one can set <span class=\"math\">\\(b\\)</span> larger than <span class=\"math\">\\(a\\)</span>. Finally, if you want to raise the number of actual votes required before the sample rates dominate, simply increase the values of <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span>&nbsp;accordingly.</p>\n<p>To conclude, we present some example output in the table below. We show values for the Wilson score <span class=\"math\">\\(p_W\\)</span>, with <span class=\"math\">\\(z_{\\alpha/2}\\)</span> set to <span class=\"math\">\\(1.281\\)</span> in (\\ref{emsol}) (the value <a href=\"https://github.com/reddit/reddit/blob/62db2373f2555df17ebeb13968e243fccfbeff5f/r2/r2/lib/db/_sorts.pyx\">reddit uses</a>), and the seed score <span class=\"math\">\\(\\tilde{p}\\)</span>, with <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(b\\)</span> set to <span class=\"math\">\\(1\\)</span> in (\\ref{final}). Notice that the two scores are in near-agreement for the last item shown, which has already accumulated a fair number of votes. However, <span class=\"math\">\\(p_W\\)</span> is significantly lower than <span class=\"math\">\\(\\tilde{p}\\)</span> for each of the first three items. For example, the third has an up-vote rate of <span class=\"math\">\\(66%\\)</span>, but is only given a Wilson score of <span class=\"math\">\\(0.32\\)</span>: This means that it would be ranked below any mature item having an up-vote rate at least this high &#8212; including fairly unpopular items liked by only one in three! This observation explains why it is nearly impossible to have new comments noticed on a reddit thread that has already hit the front page. Were reddit to move to a ranking system that were less pessimistic of new comments, its mature threads might remain&nbsp;dynamic.</p>\n<table>\n<thead>\n<tr>\n<th>up-votes</th>\n<th>down-votes</th>\n<th><span class=\"math\">\\(p_W\\)</span>, <span class=\"math\">\\(z_{\\alpha/2}= 1.281\\)</span></th>\n<th><span class=\"math\">\\(\\tilde{p}\\)</span>, <span class=\"math\">\\(a=b=1\\)</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>0.38</td>\n<td>0.67</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>0.16</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1</td>\n<td>0.32</td>\n<td>0.6</td>\n</tr>\n<tr>\n<td>40</td>\n<td>10</td>\n<td>0.72</td>\n<td>0.79</td>\n</tr>\n</tbody>\n</table>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}