{
  "title": "Synthetic Gradients with Tensorflow",
  "link": "",
  "published": "2018-04-08T00:00:00-04:00",
  "updated": "2018-04-08T00:00:00-04:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2018-04-08:/synthetic-gradients-with-tensorflow.html",
  "summary": "I stumbled upon Max Jaderberg's Synthetic Gradients paper while thinking about different forms of communication between neural modules. It's a simple idea: rather than compute gradients through backpropagation, we can train a model to predict what those gradients will be, and use our prediction to update our weights. I wanted to try using this in my own work and didn't find a Tensorflow implementation to my liking, so here is mine. I also take this opportunity to (attempt to) answer one of the questions I had while reading the paper: why not use synthetic loss instead of synthetic gradients?",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>I stumbled upon Max Jaderberg’s <a href=\"https://arxiv.org/abs/1703.00522\">Synthetic Gradients paper</a> while thinking about different forms of communication between neural modules. It’s a simple idea: rather than compute gradients through backpropagation, we can train a model to predict what those gradients will be, and use our prediction to update our weights. It’s dynamic programming for neural networks.</p>\n<p>This is the kind of idea I like because, if it works, it expands our modeling capabilities substantially. It would allow us to connect and train various neural modules asynchronously. Whether this turns out to be useful remains to be seen. I wanted to try using this in my own work and didn’t find a Tensorflow implementation to my liking, so here is mine. I also take this opportunity to (attempt to) answer one of the questions I had while reading the paper: why not use synthetic loss instead of synthetic gradients? Supposing we had multiple paths in a DAG architecture—then a synthetic loss (or better, advantage) would give us an interpretable measure of the “quality” of a part of the input, whereas synthetic gradients do not (without additional assumptions).</p>\n<p>Below, we use Tensorflow to implement the fully-connected MNIST experiment, as well as the convolutional CIFAR 10 experiment. The <a href=\"https://arxiv.org/abs/1703.00522\">Synthetic Gradients paper</a> itself is a non-technical and easy read, so I’m not going go into any detail about what exactly it is we’re doing. Jaderberg’s <a href=\"https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/\">blog post</a> may be helpful on this front. I also enjoyed Delip Rao’s <a href=\"http://deliprao.com/archives/187\">blog post</a> and <a href=\"http://deliprao.com/archives/191\">follow-up</a>.<br />\n### Implementation</p>\n<h4 id=\"imports-and-data\">Imports and data</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> tensorflow <span class=\"im\">as</span> tf, numpy <span class=\"im\">as</span> np, time\n<span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt, seaborn <span class=\"im\">as</span> sns\n<span class=\"im\">from</span> sklearn.utils <span class=\"im\">import</span> shuffle\n<span class=\"op\">%</span>matplotlib inline\nsns.<span class=\"bu\">set</span>(color_codes<span class=\"op\">=</span><span class=\"va\">True</span>)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">(xtr, ytr), (xte, yte) <span class=\"op\">=</span> tf.keras.datasets.mnist.load_data(path<span class=\"op\">=</span><span class=\"st\">&#39;mnist.npz&#39;</span>)\nxtr <span class=\"op\">=</span> xtr.reshape([<span class=\"op\">-</span><span class=\"dv\">1</span>,<span class=\"dv\">784</span>]).astype(np.float32) <span class=\"op\">/</span> <span class=\"fl\">255.</span>\nxte <span class=\"op\">=</span> xte.reshape([<span class=\"op\">-</span><span class=\"dv\">1</span>,<span class=\"dv\">784</span>]).astype(np.float32) <span class=\"op\">/</span> <span class=\"fl\">255.</span></code></pre></div>\n<h4 id=\"utility-functions\">Utility functions</h4>\n<p>Note that the layer and model functions below return their variables. This is so we can do selectively compute gradients for different variables as appropriate.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> reset_graph():\n    <span class=\"cf\">if</span> <span class=\"st\">&#39;sess&#39;</span> <span class=\"kw\">in</span> <span class=\"bu\">globals</span>() <span class=\"kw\">and</span> sess:\n        sess.close()\n    tf.reset_default_graph()</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> layer_dense_bn_relu(h, size, training<span class=\"op\">=</span><span class=\"va\">True</span>):\n  l <span class=\"op\">=</span> tf.layers.Dense(size)\n  h <span class=\"op\">=</span> tf.layers.batch_normalization(l(h), training<span class=\"op\">=</span>training)\n  \n  <span class=\"cf\">return</span> tf.nn.relu(h), l.trainable_variables\n\n<span class=\"kw\">def</span> model_linear(h, output_dim, output_activation<span class=\"op\">=</span><span class=\"va\">None</span>, \n                    kernel_initializer<span class=\"op\">=</span>tf.zeros_initializer, other_inputs<span class=\"op\">=</span><span class=\"va\">None</span>):\n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  h is input that gets mapped to output_dim dims</span>\n<span class=\"co\">  other_inputs is vector of other inputs</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  <span class=\"cf\">if</span> other_inputs <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n    h <span class=\"op\">=</span> tf.concat([h, other_inputs], axis<span class=\"op\">=</span><span class=\"bu\">len</span>(h.get_shape())<span class=\"op\">-</span><span class=\"dv\">1</span>)\n\n  l <span class=\"op\">=</span> tf.layers.Dense(output_dim, activation<span class=\"op\">=</span>output_activation, \n                      kernel_initializer<span class=\"op\">=</span>kernel_initializer)\n    \n  <span class=\"cf\">return</span> l(h), l.trainable_variables\n\n<span class=\"kw\">def</span> model_two_layer(h, output_dim, output_activation<span class=\"op\">=</span><span class=\"va\">None</span>, \n                    kernel_initializer<span class=\"op\">=</span>tf.zeros_initializer, other_inputs<span class=\"op\">=</span><span class=\"va\">None</span>):\n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  h is input that gets mapped to output_dim dims</span>\n<span class=\"co\">  other_inputs is vector of other inputs</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  <span class=\"cf\">if</span> other_inputs <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n    h <span class=\"op\">=</span> tf.concat([h, other_inputs], axis<span class=\"op\">=</span><span class=\"bu\">len</span>(h.get_shape())<span class=\"op\">-</span><span class=\"dv\">1</span>)\n  \n  h, v1 <span class=\"op\">=</span> layer_dense_bn_relu(h, <span class=\"dv\">1024</span>)\n  h, v2 <span class=\"op\">=</span> layer_dense_bn_relu(h, <span class=\"dv\">1024</span>)\n  \n  l <span class=\"op\">=</span> tf.layers.Dense(output_dim, activation<span class=\"op\">=</span>output_activation, \n                      kernel_initializer<span class=\"op\">=</span>kernel_initializer)\n  \n  <span class=\"cf\">return</span> l(h), v1 <span class=\"op\">+</span> v2 <span class=\"op\">+</span> l.trainable_variables</code></pre></div>\n<h4 id=\"synthetic-grad-loss-wrappers-and-more-utilities\">Synthetic grad / loss wrappers and more utilities</h4>\n<p>Synthetic loss is just like synthetic gradients except we are predicting a scalar loss and then computing the gradients with respect to that loss. I thought this work similarly to the synthetic gradients, but it doesn’t seem to work at all (discussed below).</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> sg_wrapper(x, h, hvs, model, other_inputs<span class=\"op\">=</span><span class=\"va\">None</span>): \n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  Predicts grads for x, h, and hvs (vars between x and h) using model.</span>\n<span class=\"co\">  Returns:</span>\n<span class=\"co\">  - synth grad for x</span>\n<span class=\"co\">  - synth grad for h</span>\n<span class=\"co\">  - synth grads &amp; vars for hvs</span>\n<span class=\"co\">  - sg model variables (so they can be trained)</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  sg, sg_vars <span class=\"op\">=</span> model(h, h.get_shape()[<span class=\"op\">-</span><span class=\"dv\">1</span>], other_inputs<span class=\"op\">=</span>other_inputs)\n  \n  xs <span class=\"op\">=</span> hvs <span class=\"op\">+</span> [x]\n  gvs <span class=\"op\">=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(h, xs, grad_ys<span class=\"op\">=</span>sg), xs))\n  \n  <span class=\"cf\">return</span> gvs[<span class=\"op\">-</span><span class=\"dv\">1</span>][<span class=\"dv\">0</span>], sg, gvs[:<span class=\"op\">-</span><span class=\"dv\">1</span>], sg_vars\n\n<span class=\"kw\">def</span> sl_wrapper(h, hvs, model, other_inputs<span class=\"op\">=</span><span class=\"va\">None</span>): \n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  Predicts loss given h, and produces grads_and_vars for hvs, using model.</span>\n<span class=\"co\">  Returns:</span>\n<span class=\"co\">  - synth loss for h</span>\n<span class=\"co\">  - synth grads &amp; vars for hvs</span>\n<span class=\"co\">  - model variables (so they can be trained)</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  sl, sl_vars <span class=\"op\">=</span> model(h, <span class=\"dv\">1</span>, tf.square, <span class=\"va\">None</span>, other_inputs)\n  \n  gvs <span class=\"op\">=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(sl, hvs), hvs))\n  \n  <span class=\"cf\">return</span> sl, gvs, sl_vars\n\n<span class=\"kw\">def</span> loss_grads_with_target(loss, vs, target):\n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  Returns grad and vars for vs and target with respect to loss. </span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  xs <span class=\"op\">=</span> vs <span class=\"op\">+</span> [target]\n  gvs <span class=\"op\">=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(loss, xs), xs))\n  <span class=\"cf\">return</span> gvs[<span class=\"op\">-</span><span class=\"dv\">1</span>][<span class=\"dv\">0</span>], gvs[:<span class=\"op\">-</span><span class=\"dv\">1</span>]\n\n<span class=\"kw\">def</span> model_grads(output_target_vars_tuple):\n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  Returns grads and vars for models given an iterable of tuples of</span>\n<span class=\"co\">    (model output, model target, model variables).</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  gvs <span class=\"op\">=</span> []\n  <span class=\"cf\">for</span> prediction, target, vs <span class=\"kw\">in</span> output_target_vars_tuple:\n    loss <span class=\"op\">=</span> tf.losses.mean_squared_error(prediction, target)\n    gvs <span class=\"op\">+=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(loss, vs), vs))\n  \n  <span class=\"cf\">return</span> gvs</code></pre></div>\n<h4 id=\"mnist-experiment\">MNIST Experiment</h4>\n<p>Note: the paper claims that the learning rate was not optimized, but I found that the results are quite sensitive to changes in the learning rate.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_graph_mnist_fcn(sg<span class=\"op\">=</span><span class=\"va\">False</span>, sl<span class=\"op\">=</span><span class=\"va\">False</span>, conditioned<span class=\"op\">=</span><span class=\"va\">False</span>, no_bprop<span class=\"op\">=</span><span class=\"va\">False</span>):\n  reset_graph()\n  g <span class=\"op\">=</span> {}\n  \n  g[<span class=\"st\">&#39;training&#39;</span>] <span class=\"op\">=</span> training <span class=\"op\">=</span> tf.placeholder_with_default(<span class=\"va\">True</span>, [])\n   \n  g[<span class=\"st\">&#39;x&#39;</span>] <span class=\"op\">=</span> x <span class=\"op\">=</span> tf.placeholder(tf.float32, [<span class=\"va\">None</span>, <span class=\"dv\">784</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;x_placeholder&#39;</span>)\n  g[<span class=\"st\">&#39;y&#39;</span>] <span class=\"op\">=</span> y <span class=\"op\">=</span> tf.placeholder(tf.int64, [<span class=\"va\">None</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;y_placeholder&#39;</span>)\n  other_inputs <span class=\"op\">=</span> <span class=\"va\">None</span>\n  <span class=\"cf\">if</span> conditioned:\n    other_inputs <span class=\"op\">=</span> tf.one_hot(y, <span class=\"dv\">10</span>)\n  \n  h1, h1vs <span class=\"op\">=</span> layer_dense_bn_relu(x, <span class=\"dv\">256</span>, training)\n  <span class=\"cf\">if</span> sg:\n    _, sg1, gvs1, svars1 <span class=\"op\">=</span> sg_wrapper(x, h1, h1vs, model_two_layer, other_inputs)\n  <span class=\"cf\">elif</span> sl:\n    sl1, gvs1, svars1 <span class=\"op\">=</span> sl_wrapper(h1, h1vs, model_two_layer, other_inputs)\n  \n  h2, h2vs <span class=\"op\">=</span> layer_dense_bn_relu(h1, <span class=\"dv\">256</span>, training)\n  <span class=\"cf\">if</span> sg:\n    sg1_target, sg2, gvs2, svars2 <span class=\"op\">=</span> sg_wrapper(h1, h2, h2vs, model_two_layer, other_inputs)\n  <span class=\"cf\">elif</span> sl:\n    sl2, gvs2, svars2 <span class=\"op\">=</span> sl_wrapper(h2, h2vs, model_two_layer, other_inputs)\n  \n  logit_layer <span class=\"op\">=</span> tf.layers.Dense(<span class=\"dv\">10</span>)\n  logits <span class=\"op\">=</span> logit_layer(h2)\n  logit_vs <span class=\"op\">=</span> logit_layer.trainable_variables\n  \n  g[<span class=\"st\">&#39;loss&#39;</span>] <span class=\"op\">=</span> loss <span class=\"op\">=\\</span>\n    tf.nn.sparse_softmax_cross_entropy_with_logits(logits<span class=\"op\">=</span>logits, labels<span class=\"op\">=</span>y)\n  \n  <span class=\"cf\">if</span> sg:\n    sg2_target, gvs3 <span class=\"op\">=</span> loss_grads_with_target(loss, logit_vs, h2)\n    gvs_sg <span class=\"op\">=</span> model_grads([(sg1, sg1_target, svars1), \n                          (sg2, sg2_target, svars2)])\n  <span class=\"cf\">elif</span> sl:\n    gvs3 <span class=\"op\">=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(loss, logit_vs), logit_vs))\n    gvs_sl <span class=\"op\">=</span> model_grads([(sl1, sl2, svars1),\n                          (sl2, tf.expand_dims(loss, <span class=\"dv\">1</span>), svars2)])\n  <span class=\"cf\">elif</span> no_bprop:\n    gvs3 <span class=\"op\">=</span> <span class=\"bu\">list</span>(<span class=\"bu\">zip</span>(tf.gradients(loss, logit_vs), logit_vs))\n    \n  <span class=\"cf\">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n    opt <span class=\"op\">=</span> tf.train.AdamOptimizer(<span class=\"fl\">3e-5</span>)\n    <span class=\"cf\">if</span> sg:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.apply_gradients(gvs1 <span class=\"op\">+</span> gvs2 <span class=\"op\">+</span> gvs3 <span class=\"op\">+</span> gvs_sg)\n    <span class=\"cf\">elif</span> sl:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.apply_gradients(gvs1 <span class=\"op\">+</span> gvs2 <span class=\"op\">+</span> gvs3 <span class=\"op\">+</span> gvs_sl)\n    <span class=\"cf\">elif</span> no_bprop:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.apply_gradients(gvs3)\n    <span class=\"cf\">else</span>:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.minimize(loss)\n    \n  g[<span class=\"st\">&#39;accuracy&#39;</span>] <span class=\"op\">=</span> tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, <span class=\"dv\">1</span>), y), tf.float32))\n  g[<span class=\"st\">&#39;init&#39;</span>] <span class=\"op\">=</span> tf.global_variables_initializer()\n  \n  <span class=\"cf\">return</span> g</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> train(graph, iters <span class=\"op\">=</span> <span class=\"dv\">25000</span>, batch_size <span class=\"op\">=</span> <span class=\"dv\">256</span>):\n  g <span class=\"op\">=</span> graph\n  res_tr <span class=\"op\">=</span> []\n  res_te <span class=\"op\">=</span> []\n  batches_per_epoch <span class=\"op\">=</span> <span class=\"bu\">len</span>(xtr)<span class=\"op\">//</span>batch_size\n  num_epochs <span class=\"op\">=</span> iters <span class=\"op\">//</span> batches_per_epoch\n  <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n    sess.run(g[<span class=\"st\">&#39;init&#39;</span>])\n    <span class=\"cf\">for</span> epoch <span class=\"kw\">in</span> <span class=\"bu\">range</span>(num_epochs):\n      x, y <span class=\"op\">=</span> shuffle(xtr, ytr)\n      acc <span class=\"op\">=</span> <span class=\"dv\">0</span>\n      <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(batches_per_epoch): \n        feed_dict <span class=\"op\">=</span> {g[<span class=\"st\">&#39;x&#39;</span>]: x[i<span class=\"op\">*</span>batch_size:(i<span class=\"op\">+</span><span class=\"dv\">1</span>)<span class=\"op\">*</span>batch_size],\n                     g[<span class=\"st\">&#39;y&#39;</span>]: y[i<span class=\"op\">*</span>batch_size:(i<span class=\"op\">+</span><span class=\"dv\">1</span>)<span class=\"op\">*</span>batch_size]}\n        acc_, _ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;accuracy&#39;</span>], g[<span class=\"st\">&#39;ts&#39;</span>]], feed_dict)\n        acc <span class=\"op\">+=</span> acc_\n        <span class=\"cf\">if</span> (i<span class=\"op\">+</span><span class=\"dv\">1</span>) <span class=\"op\">%</span> batches_per_epoch <span class=\"op\">==</span> <span class=\"dv\">0</span>:\n          res_tr.append(acc <span class=\"op\">/</span> batches_per_epoch)\n          \n          acc_te <span class=\"op\">=</span> <span class=\"dv\">0</span>\n          <span class=\"cf\">for</span> j <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">10</span>):\n            feed_dict <span class=\"op\">=</span> {g[<span class=\"st\">&#39;x&#39;</span>]: xte[j<span class=\"op\">*</span><span class=\"dv\">1000</span>:(j<span class=\"op\">+</span><span class=\"dv\">1</span>)<span class=\"op\">*</span><span class=\"dv\">1000</span>],\n                         g[<span class=\"st\">&#39;y&#39;</span>]: yte[j<span class=\"op\">*</span><span class=\"dv\">1000</span>:(j<span class=\"op\">+</span><span class=\"dv\">1</span>)<span class=\"op\">*</span><span class=\"dv\">1000</span>],\n                         g[<span class=\"st\">&#39;training&#39;</span>]: <span class=\"va\">False</span>}\n            acc_te <span class=\"op\">+=</span> sess.run(g[<span class=\"st\">&#39;accuracy&#39;</span>], feed_dict)\n          acc_te <span class=\"op\">/=</span> <span class=\"fl\">10.</span>\n          \n          res_te.append(acc_te)\n\n          <span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\r</span><span class=\"st\">Epoch </span><span class=\"sc\">{}</span><span class=\"st\">/</span><span class=\"sc\">{}</span><span class=\"st\">: </span><span class=\"sc\">{:4f}</span><span class=\"st\"> (TR) </span><span class=\"sc\">{:4f}</span><span class=\"st\"> (TE)&quot;</span>\\\n                .<span class=\"bu\">format</span>(epoch, num_epochs, acc<span class=\"op\">/</span>batches_per_epoch, acc_te), end<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>)\n          acc <span class=\"op\">=</span> <span class=\"dv\">0</span>\n\n  <span class=\"cf\">return</span> res_tr, res_te</code></pre></div>\n<h4 id=\"results\">Results</h4>\n<p>Below we are running only 25k iterations, which is enough to get the point (the 500k from the paper is quite excessive!).</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_mnist_fcn() <span class=\"co\"># baseline</span>\n_, res_baseline <span class=\"op\">=</span> train(g)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds!&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 105/106: 1.000000 (TR) 0.980100 (TE)\nTook 54.59836196899414 seconds!</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_mnist_fcn(no_bprop<span class=\"op\">=</span><span class=\"va\">True</span>)\n_, res_no_bprop <span class=\"op\">=</span> train(g)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds!&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 105/106: 0.881460 (TR) 0.889000 (TE)\nTook 33.66543793678284 seconds!</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_mnist_fcn(sl<span class=\"op\">=</span><span class=\"va\">True</span>)\n_, res_sl <span class=\"op\">=</span> train(g)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds!&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 105/106: 0.832816 (TR) 0.842900 (TE)\nTook 137.18904900550842 seconds!</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_mnist_fcn(sg<span class=\"op\">=</span><span class=\"va\">True</span>)\n_, res_sg <span class=\"op\">=</span> train(g)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds!&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 105/106: 0.997162 (TR) 0.977700 (TE)\nTook 115.9250328540802 seconds!</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_mnist_fcn(sg<span class=\"op\">=</span><span class=\"va\">True</span>, conditioned<span class=\"op\">=</span><span class=\"va\">True</span>)\n_, res_sgc <span class=\"op\">=</span> train(g)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds!&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 105/106: 0.999983 (TR) 0.980100 (TE)\nTook 117.7770209312439 seconds!</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plt.figure(figsize<span class=\"op\">=</span>(<span class=\"dv\">10</span>,<span class=\"dv\">6</span>))\nplt.plot(res_baseline, label<span class=\"op\">=</span><span class=\"st\">&quot;backprop&quot;</span>)\nplt.plot(res_no_bprop, label<span class=\"op\">=</span><span class=\"st\">&quot;no bprop&quot;</span>)\nplt.plot(res_sg, label<span class=\"op\">=</span><span class=\"st\">&quot;sg&quot;</span>)\nplt.plot(res_sgc, label<span class=\"op\">=</span><span class=\"st\">&quot;sg + c&quot;</span>)\nplt.plot(res_sl, label<span class=\"op\">=</span><span class=\"st\">&quot;sl&quot;</span>)\nplt.title(<span class=\"st\">&quot;Synthetic Gradients on MNIST&quot;</span>)\nplt.xlabel(<span class=\"st\">&quot;Epoch&quot;</span>)\nplt.ylabel(<span class=\"st\">&quot;Accuracy&quot;</span>)\nplt.ylim([<span class=\"fl\">0.5</span>,<span class=\"fl\">1.</span>])\nplt.legend()</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/synthetic_gradients/output_19_1.png\" alt=\"MNIST FCN Results\" /><figcaption>MNIST FCN Results</figcaption>\n</figure>\n<p>The results for synthetic gradients are similar to those in the paper over the first 100 epochs (25k mini-batches).</p>\n<p>We see that synthetic loss failed—doing worse than even the “no backpropagation” baseline (it is also the slowest approach!). This could be the result of a number of things (e.g., the loss distribution is bi-modal and hard to model, or perhaps I made a mistake in my implementation, as I did not debug extensively); I think, however, that there is something fundamentally wrong with doing gradient descent with respect to an approximated loss function. Though we might get a reasonable estimate of the loss, there is no guarantee that the gradient of our model will match the gradient of the actual loss. Imagine, for example, approximating a line with a zig-zag: one could get arbitrary good approximations but the gradient would always be wrong).</p>\n<h2 id=\"cifar-10-cnn-experiment\">CIFAR 10 CNN Experiment</h2>\n<p>This is just to show how the implementation works with a CNN architecture. Once again, results match the paper.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">(xtr, ytr), (xte, yte) <span class=\"op\">=</span> tf.keras.datasets.cifar10.load_data()\nxtr <span class=\"op\">=</span> xtr.astype(np.float32) <span class=\"op\">/</span> <span class=\"fl\">255.</span>\nytr <span class=\"op\">=</span> ytr.reshape([<span class=\"op\">-</span><span class=\"dv\">1</span>])\nxte <span class=\"op\">=</span> xte.astype(np.float32) <span class=\"op\">/</span> <span class=\"fl\">255.</span>\nyte <span class=\"op\">=</span> yte.reshape([<span class=\"op\">-</span><span class=\"dv\">1</span>])</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> layer_conv_bn_relu(h, num_filters, filter_dim, padding<span class=\"op\">=</span><span class=\"st\">&quot;same&quot;</span>, pooling<span class=\"op\">=</span><span class=\"va\">None</span>, training<span class=\"op\">=</span><span class=\"va\">True</span>):\n  l <span class=\"op\">=</span> tf.layers.Conv2D(num_filters, filter_dim, padding<span class=\"op\">=</span>padding)\n  h <span class=\"op\">=</span> tf.layers.batch_normalization(l(h), training<span class=\"op\">=</span>training)\n  h <span class=\"op\">=</span> tf.nn.relu(h)\n  \n  <span class=\"cf\">if</span> pooling <span class=\"op\">==</span> <span class=\"st\">&quot;max&quot;</span>:\n    h <span class=\"op\">=</span> tf.layers.max_pooling2d(h, <span class=\"dv\">3</span>, <span class=\"dv\">3</span>)\n  <span class=\"cf\">elif</span> pooling <span class=\"op\">==</span> <span class=\"st\">&quot;avg&quot;</span>:\n    h <span class=\"op\">=</span> tf.layers.average_pooling2d(h, <span class=\"dv\">3</span>, <span class=\"dv\">3</span>)\n    \n  <span class=\"cf\">return</span> h, l.trainable_variables\n\n<span class=\"kw\">def</span> model_two_layer_conv(h, output_dim, other_inputs<span class=\"op\">=</span><span class=\"va\">None</span>):\n  <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">  h is what we are computing the synth grads for, channels last data format</span>\n<span class=\"co\">  other_inputs is vector of other inputs, assumed to have same non-channel dims</span>\n<span class=\"co\">  &quot;&quot;&quot;</span>\n  <span class=\"cf\">if</span> other_inputs <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n    h <span class=\"op\">=</span> tf.concat([h, other_inputs], axis<span class=\"op\">=</span><span class=\"bu\">len</span>(h.get_shape())<span class=\"op\">-</span><span class=\"dv\">1</span>)\n  \n  h, v1 <span class=\"op\">=</span> layer_conv_bn_relu(h, <span class=\"dv\">128</span>, <span class=\"dv\">5</span>, padding<span class=\"op\">=</span><span class=\"st\">&quot;same&quot;</span>)\n  h, v2 <span class=\"op\">=</span> layer_conv_bn_relu(h, <span class=\"dv\">128</span>, <span class=\"dv\">5</span>, padding<span class=\"op\">=</span><span class=\"st\">&quot;same&quot;</span>)\n  \n  l <span class=\"op\">=</span> tf.layers.Conv2D(output_dim, <span class=\"dv\">5</span>, padding<span class=\"op\">=</span><span class=\"st\">&quot;same&quot;</span>, kernel_initializer<span class=\"op\">=</span>tf.zeros_initializer)\n    \n  <span class=\"cf\">return</span> l(h), v1 <span class=\"op\">+</span> v2 <span class=\"op\">+</span> l.trainable_variables</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_graph_cifar_cnn(sg<span class=\"op\">=</span><span class=\"va\">False</span>):\n  reset_graph()\n  g <span class=\"op\">=</span> {}\n  \n  g[<span class=\"st\">&#39;training&#39;</span>] <span class=\"op\">=</span> training <span class=\"op\">=</span> tf.placeholder_with_default(<span class=\"va\">True</span>, [])\n   \n  g[<span class=\"st\">&#39;x&#39;</span>] <span class=\"op\">=</span> x <span class=\"op\">=</span> tf.placeholder(tf.float32, [<span class=\"va\">None</span>, <span class=\"dv\">32</span>, <span class=\"dv\">32</span>, <span class=\"dv\">3</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;x_placeholder&#39;</span>)\n  g[<span class=\"st\">&#39;y&#39;</span>] <span class=\"op\">=</span> y <span class=\"op\">=</span> tf.placeholder(tf.int64, [<span class=\"va\">None</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;y_placeholder&#39;</span>)\n\n  h1, h1vs <span class=\"op\">=</span> layer_conv_bn_relu(x, <span class=\"dv\">128</span>, <span class=\"dv\">5</span>, <span class=\"st\">&#39;same&#39;</span>, <span class=\"st\">&#39;max&#39;</span>, training<span class=\"op\">=</span>training)\n  <span class=\"cf\">if</span> sg:\n    _, sg1, gvs1, svars1 <span class=\"op\">=</span> sg_wrapper(x, h1, h1vs, model_two_layer_conv)\n  \n  h2, h2vs <span class=\"op\">=</span> layer_conv_bn_relu(h1, <span class=\"dv\">128</span>, <span class=\"dv\">5</span>, <span class=\"st\">&#39;same&#39;</span>, <span class=\"st\">&#39;avg&#39;</span>, training<span class=\"op\">=</span>training)\n  <span class=\"cf\">if</span> sg:\n    sg1_target, sg2, gvs2, svars2 <span class=\"op\">=</span> sg_wrapper(h1, h2, h2vs, model_two_layer_conv)\n  \n  h <span class=\"op\">=</span> tf.reshape(h2, [<span class=\"op\">-</span><span class=\"dv\">1</span>, <span class=\"dv\">9</span><span class=\"op\">*</span><span class=\"dv\">128</span>])\n  \n  logit_layer <span class=\"op\">=</span> tf.layers.Dense(<span class=\"dv\">10</span>)\n  logits <span class=\"op\">=</span> logit_layer(h)\n  logit_vs <span class=\"op\">=</span> logit_layer.trainable_variables\n  \n  g[<span class=\"st\">&#39;loss&#39;</span>] <span class=\"op\">=</span> loss <span class=\"op\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(logits<span class=\"op\">=</span>logits, labels<span class=\"op\">=</span>y)\n  \n  <span class=\"cf\">if</span> sg:\n    sg2_target, gvs3 <span class=\"op\">=</span> loss_grads_with_target(loss, logit_vs, h2)\n    gvs_sg <span class=\"op\">=</span> model_grads([(sg1, sg1_target, svars1), \n                          (sg2, sg2_target, svars2)])\n  \n  <span class=\"cf\">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n    opt <span class=\"op\">=</span> tf.train.AdamOptimizer(<span class=\"fl\">3e-5</span>)\n    <span class=\"cf\">if</span> sg:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.apply_gradients(gvs1 <span class=\"op\">+</span> gvs2 <span class=\"op\">+</span> gvs3 <span class=\"op\">+</span> gvs_sg)\n    <span class=\"cf\">else</span>:\n      g[<span class=\"st\">&#39;ts&#39;</span>] <span class=\"op\">=\\</span>\n        opt.minimize(loss)\n    \n  g[<span class=\"st\">&#39;accuracy&#39;</span>] <span class=\"op\">=</span> tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, <span class=\"dv\">1</span>), y), tf.float32))\n  g[<span class=\"st\">&#39;init&#39;</span>] <span class=\"op\">=</span> tf.global_variables_initializer()\n  \n  <span class=\"cf\">return</span> g</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_cifar_cnn(sg<span class=\"op\">=</span><span class=\"va\">True</span>)\nres_tr, res_sg <span class=\"op\">=</span> train(g, iters<span class=\"op\">=</span><span class=\"dv\">25000</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 127/128: 0.774700 (TR) 0.648300 (TE)\nTook 943.7978417873383 seconds</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">t <span class=\"op\">=</span> time.time()\ng <span class=\"op\">=</span> build_graph_cifar_cnn() <span class=\"co\">#baseline</span>\nres_tr_backprop, res_backprop <span class=\"op\">=</span> train(g, iters<span class=\"op\">=</span><span class=\"dv\">25000</span>)\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Took </span><span class=\"sc\">{}</span><span class=\"st\"> seconds&quot;</span>.<span class=\"bu\">format</span>(time.time() <span class=\"op\">-</span> t))</code></pre></div>\n<pre><code>Epoch 127/128: 0.901683 (TR) 0.752400 (TE)\nTook 584.2685778141022 seconds</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plt.figure(figsize<span class=\"op\">=</span>(<span class=\"dv\">10</span>,<span class=\"dv\">6</span>))\nplt.plot(res_backprop, label<span class=\"op\">=</span><span class=\"st\">&quot;backprop&quot;</span>)\nplt.plot(res_sg, label<span class=\"op\">=</span><span class=\"st\">&quot;sg&quot;</span>)\nplt.title(<span class=\"st\">&quot;Synthetic Gradients on CIFAR (CNN)&quot;</span>)\nplt.xlabel(<span class=\"st\">&quot;Epoch&quot;</span>)\nplt.ylabel(<span class=\"st\">&quot;Accuracy&quot;</span>)\nplt.legend()</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/synthetic_gradients/output_27_1.png\" alt=\"CIFAR 10 CNN Results\" /><figcaption>CIFAR 10 CNN Results</figcaption>\n</figure>\n</body>\n</html>"
}