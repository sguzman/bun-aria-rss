{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2017-04-28T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/04/28/dask-dev-8",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a> and the\nData Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<p>To increase transparency I’m blogging weekly(ish) about the work done on Dask\nand related projects during the previous week.  This log covers work done\nbetween 2017-04-20 and 2017-04-28.  Nothing here is ready for production.  This\nblogpost is written in haste, so refined polish should not be expected.</p>\n\n<p>Development in Dask and Dask-related projects during the last week includes the\nfollowing notable changes:</p>\n\n<ol>\n  <li>Improved <strong>Joblib</strong> support, accelerating existing Scikit-Learn code</li>\n  <li>A <strong>dask-glm</strong> powered <strong>LogisticRegression</strong> estimator that is scikit-learn\ncompatible</li>\n  <li>Additional <strong>Parquet</strong> support by <strong>Arrow</strong></li>\n  <li><strong>Sparse arrays</strong></li>\n  <li>Better spill-to-disk behavior</li>\n  <li><strong>AsyncIO</strong> compatible Client</li>\n  <li><strong>TLS (SSL)</strong> support</li>\n  <li>NumPy <code class=\"language-plaintext highlighter-rouge\">__array_ufunc__</code> protocol</li>\n</ol>\n\n<h3 id=\"joblib\">Joblib</h3>\n\n<p>Scikit learn parallelizes most of their algorithms with\n<a href=\"https://pythonhosted.org/joblib/\">Joblib</a>, which provides a simple interface\nfor embarrassingly parallel computations.  Dask has been able to <a href=\"http://distributed.readthedocs.io/en/latest/joblib.html\">hijack\njoblib</a> code and\nserve as the backend for some time now, but it had some limitations,\nparticularly because we would repeatedly send data back and forth from a\nworker to client for every batch of computations.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">distributed.joblib</span>\n<span class=\"kn\">from</span> <span class=\"nn\">joblib</span> <span class=\"kn\">import</span> <span class=\"n\">Parallel</span><span class=\"p\">,</span> <span class=\"n\">parallel_backend</span>\n\n<span class=\"k\">with</span> <span class=\"n\">parallel_backend</span><span class=\"p\">(</span><span class=\"s\">'dask.distributed'</span><span class=\"p\">,</span> <span class=\"n\">scheduler_host</span><span class=\"o\">=</span><span class=\"s\">'HOST:PORT'</span><span class=\"p\">):</span>\n    <span class=\"c1\"># normal Joblib code\n</span></code></pre></div></div>\n\n<p>Now there is a <code class=\"language-plaintext highlighter-rouge\">scatter=</code> keyword, which allows you to pre-scatter select\nvariables out to all of the Dask workers.  This significantly cuts down on\noverhead, especially on machine learning workloads where most of the data\ndoesn’t change very much.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Send the training data only once to each worker\n</span><span class=\"k\">with</span> <span class=\"n\">parallel_backend</span><span class=\"p\">(</span><span class=\"s\">'dask.distributed'</span><span class=\"p\">,</span> <span class=\"n\">scheduler_host</span><span class=\"o\">=</span><span class=\"s\">'localhost:8786'</span><span class=\"p\">,</span>\n                      <span class=\"n\">scatter</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">digits</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">digits</span><span class=\"p\">.</span><span class=\"n\">target</span><span class=\"p\">]):</span>\n    <span class=\"n\">search</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">digits</span><span class=\"p\">.</span><span class=\"n\">target</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Early trials indicate that computations like scikit-learn’s RandomForest scale\nnicely on a cluster without any additional code.</p>\n\n<p>This is particularly nice because it allows Dask and Scikit-Learn to play well\ntogether without having to introduce Dask within the Scikit-Learn codebase at\nall.  From a maintenance perspective this combination is very attractive.</p>\n\n<p>Work done by <a href=\"http://jcrist.github.io/\">Jim Crist</a> in <a href=\"https://github.com/dask/distributed/pull/1022\">dask/distributed #1022</a></p>\n\n<h3 id=\"dask-glm-logistic-regression\">Dask-GLM Logistic Regression</h3>\n\n<p>The convex optimization solvers in the\n<a href=\"https://github.com/dask/dask-glm\">dask-glm</a> project allow us to solve common\nmachine learning and statistics problems in parallel and at scale.\nHistorically this young library has contained only optimization solvers and\nrelatively little in the way of user API.</p>\n\n<p>This week dask-glm grew new LogisticRegression and LinearRegression estimators\nthat expose the scalable convex optimization algorithms within dask-glm through\na Scikit-Learn compatible interface.  This can both speedup solutions on a\nsingle computer or provide solutions for datasets that were previously too\nlarge to fit in memory.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask_glm.estimators</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\n\n<span class=\"n\">est</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">()</span>\n<span class=\"n\">est</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">my_dask_array</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><a href=\"http://nbviewer.jupyter.org/gist/anonymous/15742155693794ddd31ea85b654cbc7e\">This notebook</a>\ncompares performance to the latest release of scikit-learn on a 5,000,000\ndataset running on a single machine.  Dask-glm beats scikit-learn by a factor\nof four, which is also roughly the number of cores on the development machine.\nHowever in response <a href=\"http://nbviewer.jupyter.org/gist/ogrisel/5f2d31bc5e7df852b4ca63f5f6049f42\">this\nnotebook</a>\nby <a href=\"http://ogrisel.com/\">Olivier Grisel</a> shows the development version of\nscikit-learn (with a new algorithm) beating out dask-glm by a factor of six.\nThis just goes to show you that being smarter about your algorithms is almost\nalways a better use of time than adopting parallelism.</p>\n\n<p>Work done by <a href=\"https://tomaugspurger.github.io/\">Tom Augspurger</a> and <a href=\"https://github.com/moody-marlin/\">Chris\nWhite</a> in\n<a href=\"https://github.com/dask/dask-glm/pull/40\">dask/dask-glm #40</a></p>\n\n<h3 id=\"parquet-with-arrow\">Parquet with Arrow</h3>\n\n<p>The Parquet format is quickly becoming a standard for parallel and distributed\ndataframes.  There are currently two Parquet reader/writers accessible from\nPython, <a href=\"http://fastparquet.readthedocs.io/en/latest/\">fastparquet</a> a\nNumPy/Numba solution, and <a href=\"https://github.com/apache/parquet-cpp\">Parquet-CPP</a> a\nC++ solution with wrappers provided by <a href=\"https://arrow.apache.org/\">Arrow</a>.\nDask.dataframe has supported parquet for a while now with fastparquet.</p>\n\n<p>However, users will now have an option to use Arrow instead by switching the\n<code class=\"language-plaintext highlighter-rouge\">engine=</code> keyword in the <code class=\"language-plaintext highlighter-rouge\">dd.read_parquet</code> function.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'/path/to/mydata.parquet'</span><span class=\"p\">,</span> <span class=\"n\">engine</span><span class=\"o\">=</span><span class=\"s\">'fastparquet'</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'/path/to/mydata.parquet'</span><span class=\"p\">,</span> <span class=\"n\">engine</span><span class=\"o\">=</span><span class=\"s\">'arrow'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Hopefully this capability increases the use of both projects and results in\ngreater feedback to those libraries so that they can continue to advance\nPython’s access to the Parquet format.  As a gentle reminder, you can typically\nget <em>much</em> faster query times by switching from CSV to Parquet.  This is often\nmuch more effective than parallel computing.</p>\n\n<p>Work by <a href=\"http://wesmckinney.com/\">Wes McKinney</a> in <a href=\"https://github.com/dask/dask/pull/2223\">dask/dask\n#2223</a>.</p>\n\n<h3 id=\"sparse-arrays\">Sparse Arrays</h3>\n\n<p>There is a small multi-dimensional sparse array library here:\n<a href=\"https://github.com/mrocklin/sparse\">https://github.com/mrocklin/sparse</a>.  It\nallows us to represent arrays compactly in memory when most entries are zero.\nThis differs from the standard solution in\n<a href=\"https://docs.scipy.org/doc/scipy-0.19.0/reference/sparse.html\">scipy.sparse</a>,\nwhich can only support arrays of dimension two (matrices) and not greater.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip install sparse\n</code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"o\">&lt;</span> <span class=\"mf\">0.9</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">nbytes</span>\n<span class=\"mi\">80000</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">sparse</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">sparse</span><span class=\"p\">.</span><span class=\"n\">COO</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s</span>\n<span class=\"o\">&lt;</span><span class=\"n\">COO</span><span class=\"p\">:</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float64</span><span class=\"p\">,</span> <span class=\"n\">nnz</span><span class=\"o\">=</span><span class=\"mi\">1074</span><span class=\"o\">&gt;</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s</span><span class=\"p\">.</span><span class=\"n\">nbytes</span>\n<span class=\"mi\">12888</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">sparse</span><span class=\"p\">.</span><span class=\"n\">tensordot</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">axes</span><span class=\"o\">=</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))).</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span> <span class=\"mf\">100.93868073</span><span class=\"p\">,</span>  <span class=\"mf\">128.72312323</span><span class=\"p\">,</span>  <span class=\"mf\">119.12997217</span><span class=\"p\">,</span>  <span class=\"mf\">118.56304153</span><span class=\"p\">,</span>\n        <span class=\"mf\">133.24522101</span><span class=\"p\">,</span>   <span class=\"mf\">98.33555365</span><span class=\"p\">,</span>   <span class=\"mf\">90.25304866</span><span class=\"p\">,</span>   <span class=\"mf\">98.99823973</span><span class=\"p\">,</span>\n        <span class=\"mf\">100.57555847</span><span class=\"p\">,</span>   <span class=\"mf\">78.27915528</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>Additionally, this <code class=\"language-plaintext highlighter-rouge\">sparse</code> library more faithfully follows the <code class=\"language-plaintext highlighter-rouge\">numpy.ndarray</code>\nAPI, which is exactly what <code class=\"language-plaintext highlighter-rouge\">dask.array</code> expects.  Because of this close API\nmatching dask.array is able to parallelize around sparse arrays just as easily\nas it parallelizes around dense numpy arrays.  This gives us a decent\ndistributed multidimensional sparse array library relatively cheaply.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">),</span>\n<span class=\"p\">...</span>                      <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">x</span> <span class=\"o\">&lt;</span> <span class=\"mf\">0.9</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">map_blocks</span><span class=\"p\">(</span><span class=\"n\">sparse</span><span class=\"p\">.</span><span class=\"n\">COO</span><span class=\"p\">)</span>  <span class=\"c1\"># parallel array of sparse arrays\n</span></code></pre></div></div>\n\n<p>Work on the <code class=\"language-plaintext highlighter-rouge\">sparse</code> library is so far by <a href=\"http://matthewrocklin.com/\">myself</a>\nand <a href=\"https://staff.washington.edu/jakevdp/\">Jake VanderPlas</a> and is available\n<a href=\"https://github.com/mrocklin/sparse\">here</a>.  Work connecting this up to\nDask.array is in <a href=\"http://matthewrocklin.com/\">dask/dask #2234</a>.</p>\n\n<h3 id=\"better-spill-to-disk-behavior\">Better spill to disk behavior</h3>\n\n<p>I’ve been playing with a 50GB sample of the 1TB <a href=\"http://labs.criteo.com/2013/12/download-terabyte-click-logs-2/\">Criteo\ndataset</a> on my\nlaptop (this is where I’m using sparse arrays).  To make computations flow a\nbit faster I’ve improved the performance of Dask’s spill-to-disk policies.</p>\n\n<p>Now, rather than depend on (cloud)pickle we use Dask’s network protocol, which\nhandles data more efficiently, compresses well, and has special handling for\ncommon and important types like NumPy arrays and things built out of NumPy\narrays (like sparse arrays).</p>\n\n<p>As a result reading and writing excess data to disk is significantly faster.\nWhen performing machine learning computations (which are fairly heavy-weight)\ndisk access is now fast enough that I don’t notice it in practice and running\nout of memory doesn’t significantly impact performance.</p>\n\n<p>This is only really relevant when using common types (like numpy arrays) and\nwhen your computation to disk access ratio is relatively high (such as is the\ncase for analytic workloads), but it was a simple fix and yielded a nice boost\nto my personal productivity.</p>\n\n<p>Work by myself in <a href=\"https://github.com/dask/distributed/pull/946\">dask/distributed #946</a>.</p>\n\n<h3 id=\"asyncio-compatible-client\">AsyncIO compatible Client</h3>\n\n<p>The Dask.distributed scheduler maintains a fully asynchronous API for use with\nnon-blocking systems like Tornado or AsyncIO.  Because Dask supports Python 2\nall of our internal code is written with Tornado.  While Tornado and AsyncIO\ncan work together, this generally requires a bit of excess book-keeping, like\nturning Tornado futures into AsyncIO futures, etc..</p>\n\n<p>Now there is an AsyncIO specific Client that only includes non-blocking methods\nthat are AsyncIO native.  This allows for more idiomatic asynchronous code in\nPython 3.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">AioClient</span><span class=\"p\">(</span><span class=\"s\">'scheduler-address:8786'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n    <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">future</span>\n</code></pre></div></div>\n\n<p>Work by <a href=\"https://github.com/kszucs\">Krisztián Szűcs</a> in <a href=\"https://github.com/dask/distributed/pull/1029\">dask/distributed\n#1029</a>.</p>\n\n<h3 id=\"tls-ssl-support\">TLS (SSL) support</h3>\n\n<p>TLS (previously called SSL) is a common and trusted solution to authentication\nand encryption.  It is a commonly requested feature by companies of\ninstitutions where intra-network security is important.  This is currently\nbeing worked on now at <a href=\"https://github.com/dask/distributed/pull/1034\">dask/distributed\n#1034</a>.  I encourage anyone who\nthis may affect to engage on that pull request.</p>\n\n<p>Work by <a href=\"https://github.com/pitrou\">Antoine Pitrou</a> in <a href=\"https://github.com/dask/distributed/pull/1034\">dask/distributed\n#1034</a> and previously by <a href=\"https://github.com/mariusvniekerk\">Marius\nvan Niekerk</a> in <a href=\"https://github.com/dask/distributed/pull/866\">dask/distributed\n#866</a>.</p>\n\n<h3 id=\"numpy-__array_ufunc__\">NumPy <code class=\"language-plaintext highlighter-rouge\">__array_ufunc__</code></h3>\n\n<p>This recent change in NumPy (literally merged as I was typing this blogpost)\nallows other array libraries to take control of the the existing NumPy ufuncs,\nso if you call something like <code class=\"language-plaintext highlighter-rouge\">np.exp(my_dask_array)</code> this will no longer\nconvert to a NumPy array, but will rather call the appropriate\n<code class=\"language-plaintext highlighter-rouge\">dask.array.exp</code> function.  This is a big step towards writing generic array\ncode that works both on NumPy arrays as well as other array projects like\ndask.array, xarray, bcolz, sparse, etc..</p>\n\n<p>As with all large changes in NumPy this was accomplished through a\ncollaboration of many people.  PR in <a href=\"https://github.com/numpy/numpy/pull/8247\">numpy/numpy #8247</a>.</p>"
}