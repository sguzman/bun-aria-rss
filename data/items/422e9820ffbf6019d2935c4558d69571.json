{
  "title": "NBA learner: 2013-14 warmup",
  "link": "",
  "published": "2014-10-30T22:07:00-07:00",
  "updated": "2014-10-30T22:07:00-07:00",
  "author": {
    "name": "Dustin McIntosh"
  },
  "id": "tag:efavdb.com,2014-10-30:/nba-learner-2013-14-warmup",
  "summary": "<p>We’ve spent the last couple of evenings training some preliminary algorithms on the <span class=\"caps\">NBA</span> 2013-14, regular season data, which we grabbed from <a href=\"http://www.basketball-reference.com/\">basketball-reference.com</a>.  Each of the 30 <span class=\"caps\">NBA</span> teams play 82 times a season, summing to 1230 games total — a sizable number that we can comfortably attempt to …</p>",
  "content": "<p>We’ve spent the last couple of evenings training some preliminary algorithms on the <span class=\"caps\">NBA</span> 2013-14, regular season data, which we grabbed from <a href=\"http://www.basketball-reference.com/\">basketball-reference.com</a>.  Each of the 30 <span class=\"caps\">NBA</span> teams play 82 times a season, summing to 1230 games total — a sizable number that we can comfortably attempt to model.  Here, we cover our first pass at the prediction problem, what we’ve learned so far, and challenges we’re looking forward to tackling&nbsp;soon.</p>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2014/10/NBAlearnV0Results.png\"><img alt=\"NBAlearnV0Results\" src=\"https://efavdb.com/wp-content/uploads/2014/10/NBAlearnV0Results.png\"></a></p>\n<p>The first algorithm:  As mentioned in the prior post, we decided to initially train only on historical win-loss data triples of the form (home team, away team, y), where the Boolean y equals one if the home team won, zero otherwise.  For prediction, we use logistic classification:  We attempt to identify which teams team <span class=\"math\">\\(\\alpha\\)</span> would likely beat, were they to play them at home.  In order to accomplish this task, our logistic model has at its disposal a set of variable features characterizing each team:  a home feature vector <span class=\"math\">\\(\\textbf{H}_{\\alpha}\\)</span> and an away feature vector <span class=\"math\">\\(\\textbf{A}_{\\alpha}\\)</span>, each of length 10.  The model predicts a home team <span class=\"math\">\\(\\alpha\\)</span> win over away team <span class=\"math\">\\(\\beta\\)</span> probability of <span class=\"math\">\\(h =  1/[1 + \\exp(- \\textbf{H}_{\\alpha} \\cdot \\textbf{A}_{\\beta})]\\)</span> <span class=\"math\">\\( \\in [0,1]\\)</span>.  In training, the model is initially fed random feature vectors, which are then relaxed to minimize the logistic cost function, <span class=\"math\">\\(J \\equiv<div class=\"math\">$$ - \\sum_{i = 1}^{m} y_i \\log h_i$$</div>+ (1-y_i) \\log (1 - h_i) $, where the sum is over all training examples.  The cost function $J\\)</span> heavily penalizes large mismatch between the actual outcome <span class=\"math\">\\(y\\)</span> and the predicted outcome <span class=\"math\">\\(h\\)</span> for any training example — we also added to this a suppression term that prevents&nbsp;over-fitting.</p>\n<p>Results:  We trained the above model on the first 800 games of the 2013-14 season, and then tested the accuracy of the model on the remaining 430 games it did not train on.  Sample output is shown in the figure.  As you can see in the last line, the algorithm correctly predicted the outcome of 64% of these games.  As a first pass, this compares favorably to, for example, the accuracy of the predictions provided by <a href=\"http://www.teamrankings.com/nba/betting-models/detailed-splits/\">teamrankings.com</a> (about 68% for the 2013-14 season). Further, after implementing a quick improvement to the first model above, basing predictions on prior score-differentials, rather than simply win-loss results, we managed to pop our accuracy up to 69% on the same data&nbsp;set.</p>\n<p>Caveats <span class=\"amp\">&amp;</span> future directions:  Our comparison to teamrankings.com (<span class=\"caps\">TR</span>) above isn’t really a fair one.  The reason is that our analysis was only carried out on the final 2/3rds of the last season, whereas <span class=\"caps\">TR</span>’s average covered its entirety.  Early-season prediction is necessarily less accurate for all bettors, given the paucity of relevant data available at that time.  Nevertheless, we’re encouraged by our first attempts here.  To improve, we aim next to incorporate the information provided by prior seasons.  A closely related challenge will be to figure out how to intelligently weight data according to its age:  We want to be able to capture timely effects, like current momentum and injuries, while retaining all relevant long-term&nbsp;trends.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    "",
    ""
  ]
}