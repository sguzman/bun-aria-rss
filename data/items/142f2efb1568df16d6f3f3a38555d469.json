{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2017-02-20T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/02/20/dask-dev-7",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<p>To increase transparency I’m blogging weekly(ish) about the work done on Dask\nand related projects during the previous week.  This log covers work done\nbetween 2017-02-01 and 2017-02-20.  Nothing here is ready for production.  This\nblogpost is written in haste, so refined polish should not be expected.</p>\n\n<p>Themes of the last couple of weeks:</p>\n\n<ol>\n  <li>Profiling experiments with Dask-GLM</li>\n  <li>Subsequent graph optimizations, both non-linear fusion and avoiding\nrepeatedly creating new graphs</li>\n  <li>Tensorflow and Keras experiments</li>\n  <li>XGBoost experiments</li>\n  <li>Dask tutorial refactor</li>\n  <li>Google Cloud Storage support</li>\n  <li>Cleanup of Dask + SKLearn project</li>\n</ol>\n\n<h3 id=\"dask-glm-and-iterative-algorithms\">Dask-GLM and iterative algorithms</h3>\n\n<p>Dask-GLM is currently just a bunch of solvers like Newton, Gradient Descent,\nBFGS, Proximal Gradient Descent, and ADMM.  These are useful in solving\nproblems like logistic regression, but also several others.  The mathematical\nside of this work is mostly done by <a href=\"https://github.com/moody-marlin/\">Chris White</a>\nand <a href=\"https://github.com/hussainsultan\">Hussain Sultan</a> at Capital One.</p>\n\n<p>We’ve been using this project also to see how Dask can scale out machine\nlearning algorithms.  To this end we ran a few benchmarks here:\nhttps://github.com/dask/dask-glm/issues/26 .  This just generates and solves\nsome random problems, but at larger scales.</p>\n\n<p>What we found is that some algorithms, like ADMM perform beautifully, while\nfor others, like gradient descent, scheduler overhead can become a substantial\nbottleneck at scale.  This is mostly just because the actual in-memory\nNumPy operations are so fast; any sluggishness on Dask’s part becomes very\napparent.  Here is a profile of gradient descent:</p>\n\n<iframe src=\"https://cdn.rawgit.com/mrocklin/e7bcb979e147102bf9ac428ed9074000/raw/d38234f3e9072816bea98d032f1e4f9e618242c3/task-stream-glm-gradient-descent.html\" width=\"800\" height=\"400\"></iframe>\n\n<p>Notice all the white space.  This is Dask figuring out what to do during\ndifferent iterations.  We’re now working to bring this down to make all of the\ncolored parts of this graph squeeze together better.  This will result in\ngeneral overhead improvements throughout the project.</p>\n\n<h3 id=\"graph-optimizations---aggressive-fusion\">Graph Optimizations - Aggressive Fusion</h3>\n\n<p>We’re approaching this in two ways:</p>\n\n<ol>\n  <li>More aggressively fuse tasks together so that there are fewer blocks for\nthe scheduler to think about</li>\n  <li>Avoid repeated work when generating very similar graphs</li>\n</ol>\n\n<p>In the first case, Dask already does standard task fusion.  For example, if you\nhave the following to tasks:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">g</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">h</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Dask (along with every other compiler-like project since the 1980’s)  already\nturns this into the following:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">h</span><span class=\"p\">(</span><span class=\"n\">g</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)))</span>\n</code></pre></div></div>\n\n<p>What’s tricky with a lot of these mathematical or optimization algorithms\nthough is that they are mostly, but not entirely linear.  Consider the\nfollowing example:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">x</span>\n</code></pre></div></div>\n\n<p>Visualized as a node-link diagram, this graph looks like a diamond like the following:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>         o  exp(x) - 1/x\n        / \\\nexp(x) o   o   1/x\n        \\ /\n         o  x\n</code></pre></div></div>\n\n<p>Graphs like this generally don’t get fused together because we <em>could</em> compute\nboth <code class=\"language-plaintext highlighter-rouge\">exp(x)</code> and <code class=\"language-plaintext highlighter-rouge\">1/x</code> in parallel.  However when we’re bound by scheduling\noverhead and when we have plenty of parallel work to do, we’d prefer to fuse\nthese into a single task, even though we lose some potential parallelism.\nThere is a tradeoff here and we’d like to be able to exchange some parallelism\n(of which we have a lot) for less overhead.</p>\n\n<p>PR here <a href=\"https://github.com/dask/dask/pull/1979\">dask/dask #1979</a> by <a href=\"https://github.com/eriknw\">Erik\nWelch</a> (Erik has written and maintained most of\nDask’s graph optimizations).</p>\n\n<h3 id=\"graph-optimizations---structural-sharing\">Graph Optimizations - Structural Sharing</h3>\n\n<p>Additionally, we no longer make copies of graphs in dask.array.  Every\ncollection like a dask.array or dask.dataframe holds onto a Python dictionary\nholding all of the tasks that are needed to construct that array.  When we\nperform an operation on a dask.array we get a new dask.array with a new\ndictionary pointing to a new graph.  The new graph generally has all of the\ntasks of the old graph, plus a few more.  As a result, we frequently make\ncopies of the underlying task graph.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"k\">assert</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">.</span><span class=\"n\">dask</span><span class=\"p\">).</span><span class=\"n\">issuperset</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">dask</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Normally this doesn’t matter (copying graphs is usually cheap) but it can\nbecome very expensive for large arrays when you’re doing many mathematical\noperations.</p>\n\n<p>Now we keep dask graphs in a custom mapping (dict-like object) that shares\nsubgraphs with other arrays.  As a result, we rarely make unnecessary copies\nand some algorithms incur far less overhead.  Work done in\n<a href=\"https://github.com/dask/dask/pull/1985\">dask/dask #1985</a>.</p>\n\n<h3 id=\"tensorflow-and-keras-experiments\">TensorFlow and Keras experiments</h3>\n\n<p>Two weeks ago I gave a talk with <a href=\"https://github.com/seibert\">Stan Seibert</a>\n(Numba developer) on Deep Learning (Stan’s bit) and Dask (my bit).  As part of\nthat talk I decided to launch tensorflow from Dask and feed Tensorflow from a\ndistributed Dask array. See <a href=\"http://matthewrocklin.com/blog/work/2017/02/11/dask-tensorflow\">this\nblogpost</a> for\nmore information.</p>\n\n<p>That experiment was nice in that it showed how easy it is to deploy and\ninteract with other distributed servies from Dask.  However from a deep\nlearning perspective it was immature.  Fortunately, it succeeded in attracting\nthe attention of other potential developers (the true goal of all blogposts)\nand now <a href=\"https://github.com/bnaul\">Brett Naul</a> is using Dask to manage his GPU\nworkloads with Keras.  Brett <a href=\"https://github.com/dask/distributed/pull/878\">contributed\ncode</a> to help Dask move around\nKeras models.  He seems to particularly value <a href=\"http://distributed.readthedocs.io/en/latest/resources.html\">Dask’s ability to manage\nresources</a> to help\nhim fully saturate the GPUs on his workstation.</p>\n\n<h3 id=\"xgboost-experiments\">XGBoost experiments</h3>\n\n<p>After deploying Tensorflow we asked what would it take to do the same for\nXGBoost, another very popular (though very different) machine learning library.\nThe conversation for that is here: <a href=\"https://github.com/dmlc/xgboost/issues/2032\">dmlc/xgboost\n#2032</a> with prototype code here\n<a href=\"https://github.com/mrocklin/dask-xgboost\">mrocklin/dask-xgboost</a>.  As with\nTensorFlow, the integration is relatively straightforward (if perhaps a bit\nsimpler in this case).  The challenge for me is that I have little concrete\nexperience with the applications that these libraries were designed to solve.\nFeedback and collaboration from open source developers who use these libraries\nin production is welcome.</p>\n\n<h3 id=\"dask-tutorial-refactor\">Dask tutorial refactor</h3>\n\n<p>The <a href=\"https://github.com/dask/dask-tutorial\">dask/dask-tutorial</a> project on\nGithub was originally written or PyData Seattle in July 2015 (roughly 19 months\nago).  Dask has evolved substantially since then but this is still our only\neducational material.  Fortunately <a href=\"http://martindurant.github.io/\">Martin\nDurant</a> is doing a <a href=\"https://github.com/dask/dask-tutorial/pull/29\">pretty\nserious rewrite</a>, both correcting parts that are no longer modern API, and also\nadding in new material around distributed computing and debugging.</p>\n\n<h3 id=\"google-cloud-storage\">Google Cloud Storage</h3>\n\n<p>Dask developers (mostly Martin) maintain libraries to help Python users connect\nto distributed file systems like HDFS (with\n<a href=\"http://hdfs3.readthedocs.io/en/latest/\">hdfs3</a>, S3 (with\n<a href=\"http://s3fs.readthedocs.io/en/latest/\">s3fs</a>, and Azure Data Lake (with\n<a href=\"https://github.com/Azure/azure-data-lake-store-python\">adlfs</a>), which\nsubsequently become usable from Dask.  Martin has been working on support for\nGoogle Cloud Storage (with <a href=\"https://github.com/martindurant/gcsfs\">gcsfs</a>) with\nanother small project that uses the same API.</p>\n\n<h3 id=\"cleanup-of-dasksklearn-project\">Cleanup of Dask+SKLearn project</h3>\n\n<p>Last year Jim Crist published\n<a href=\"http://jcrist.github.io/dask-sklearn-part-1.html\">three</a>\n<a href=\"http://jcrist.github.io/dask-sklearn-part-2.html\">great</a>\n<a href=\"http://jcrist.github.io/dask-sklearn-part-3.html\">blogposts</a> about using Dask\nwith SKLearn.  The result was a small library\n<a href=\"https://github.com/dask/dask-learn\">dask-learn</a> that had a variety of\nfeatures, some incredibly useful, like a cluster-ready Pipeline and\nGridSearchCV, other less so.  Because of the experimental nature of this work\nwe had labeled the library “not ready for use”, which drew some curious\nresponses from potential users.</p>\n\n<p>Jim is now busy dusting off the project, removing less-useful parts and\ngenerally reducing scope to strictly model-parallel algorithms.</p>"
}