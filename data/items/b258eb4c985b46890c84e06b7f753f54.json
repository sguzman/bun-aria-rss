{
  "title": "Importance Weighted Hierarchical Variational Inference",
  "link": "http://artem.sobolev.name/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html",
  "description": "<p>This post finishes the discussion on <a href=\"/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html\">Neural Samplers for Variational Inference</a> by introducing some recent results (including mine).</p>\n<p>Also, there’s <a href=\"https://youtu.be/pdSu7XfGhHw\">a talk recording</a> of me presenting this post’s content, so if you like videos more than texts, check it out.</p>\n<!--more-->\n<h2 id=\"quick-recap\">Quick Recap</h2>\n<p>It all started with an aspiration for a more expressive variational approximation <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span> since it restricts expressivity of our hierarhical model <span class=\"math inline\">\\(p_\\theta(x)\\)</span>. We could use the multisample bound, which can lighten the restriction to an arbitrary extent, but the price is more computation and multiple evaluations of high-dimensional decoder <span class=\"math inline\">\\(p_\\theta(x|z)\\)</span> are especially frustrating.</p>\n<p>Instead, we hope to leverage Neural Net’s universal approximation properties and introduce a hierarchical variational approximation <span class=\"math inline\">\\(q_\\phi(z|x) = \\int q_\\phi(z, \\psi|x) d\\psi\\)</span> which should be much more expressive and we can sample from it by passing some simple noise <span class=\"math inline\">\\(\\psi\\)</span> through a neural network that generates<a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a> <span class=\"math inline\">\\(q_\\phi(z|\\psi, x)\\)</span> distribution. However, we lost access to the marginal log-density <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span>, required by the KL term of the ELBO.</p>\n<p>A theoretically sound way then is to give an upper bound on the log-density (to obtain a lower bound on the ELBO), but this bound regularizes the <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span> and alleviating this regularization requires more expressive auxiliary variational distribution <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z)\\)</span>. Full circle, full stop. At this point, an efficiently computable multisample variational upper bound on the <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span> would be handy, but our naive attempt to obtain one was unsuccessful. Moreover, it might well be that there are no good bounds at all.</p>\n<h2 id=\"new-semi-implicit-hope\"><del>New</del> Semi-Implicit Hope</h2>\n<p>A year ago Mingzhang Yin and Mingyuan Zhou published a paper <a href=\"https://arxiv.org/abs/1805.11183\">Semi-Implicit Variational Inference</a> (SIVI) where they essentially proposed the following multisample surrogate ELBO for our model:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\mathcal{L}}_K^\\text{SIVI}\n:=\n\\E_{q_\\phi(z, \\psi_0|x)}\n\\E_{q_\\phi(\\psi_{1:K}|x)}\n\\log \\frac{p_\\theta(x, z)}{ \\frac{1}{K+1} \\sum_{k=0}^K q_\\phi(z|\\psi_k, x) }\n\\]</span></p>\n<p>However, the original paper did not prove that this surrogate is a lower bound for all finite <span class=\"math inline\">\\(K\\)</span>, only that it converges to the ELBO <span class=\"math inline\">\\(\\mathcal{L}\\)</span> in the limit of infinite <span class=\"math inline\">\\(K\\)</span>. This fact was later <a href=\"https://arxiv.org/abs/1810.02789\">shown by Molchanov et al.</a>: this surrogate objective is indeed a lower bound for all finite <span class=\"math inline\">\\(K\\)</span>. Moreover, since this is a lower bound on ELBO,</p>\n<p><span class=\"math display\">\\[\n\\E_{q_\\phi(z, \\psi_0|x)}\n\\E_{q_\\phi(\\psi_{1:K}|x)}\n\\left[\n\\log \\frac{p_\\theta(x, z)}{ q_\\phi(z|x) }\n-\n\\log \\frac{p_\\theta(x, z)}{ \\frac{1}{K+1} \\sum_{k=0}^K q_\\phi(z|\\psi_k, x) }\n\\right]\n\\ge 0\n\\]</span> We can recover an upper bound on the marginal log-density (at least in expectation) <span class=\"math display\">\\[\n\\E_{q_\\phi(z|x)}\n\\log q_\\phi(z|x)\n\\le\n\\E_{q_\\phi(z|x)}\n\\E_{q_\\phi(\\psi_0|z, x)}\n\\E_{q_\\phi(\\psi_{1:K}|x)}\n\\log \\frac{1}{K+1} \\sum_{k=0}^K q_\\phi(z|\\psi_k, x)\n\\]</span></p>\n<p>Which does indeed give us a multisample upper bound (not variational, though). Unfortunately, this particular bound has a severe weakness: the samples <span class=\"math inline\">\\(\\psi_{1:K}\\)</span> are <em>uninformed</em> about the <span class=\"math inline\">\\(z\\)</span> they’re supposed to describe in the <span class=\"math inline\">\\(q(z|x,\\psi_k)\\)</span> terms, so they are likely to do a poor job of reconstructing a particular <span class=\"math inline\">\\(z\\)</span>.</p>\n<p>Interestingly, this bound looks similar to the multisample variational <em>lower</em> bound <span class=\"math inline\">\\(\\mathcal{L}_K\\)</span>… <span class=\"math display\">\\[\n\\log q(z|x)\n\\ge\n\\E_{\\tau_\\eta(\\psi_{1:K}|z, x)}\n\\log \\frac{1}{K} \\sum_{k=1}^K \\frac{q_\\phi(z, \\psi_k|x)}{\\tau_\\eta(\\psi_k|x,z)}\n\\]</span> … when <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z)\\)</span> is taken to be <span class=\"math inline\">\\(q_\\phi(\\psi|x)\\)</span> – the “variational prior”: <span class=\"math display\">\\[\n\\log q(z|x)\n\\ge\n\\E_{q_\\phi(\\psi_{1:K}|x)}\n\\log \\frac{1}{K} \\sum_{k=1}^K q_\\phi(z|\\psi_k, x)\n\\]</span></p>\n<p>The only difference between this lower bound and the SIVI upper bound is that the later adds one (free, see previous post for the discussion on free posterior samples) sample from the true inverse model <span class=\"math inline\">\\(q_\\phi(\\psi|x,z)\\)</span>.</p>\n<h2 id=\"importance-weighted-hierarchical-variational-inference\">Importance Weighted Hierarchical Variational Inference</h2>\n<p>The natural question to ask then is… could maybe the following be an upper bound on <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span>? <span class=\"math display\">\\[\n\\mathcal{U}_K\n:=\n\\E_{q_\\phi(\\psi_0|z, x)}\n\\E_{\\tau_\\eta(\\psi_{1:K}|z, x)}\n\\log \\frac{1}{K+1} \\sum_{k=0}^K \\frac{q_\\phi(z, \\psi_k|x)}{\\tau_\\eta(\\psi_k|x,z)}\n\\]</span> The formula is very bizarre, yet several special cases do give upper bounds:</p>\n<ul>\n<li>Setting <span class=\"math inline\">\\(K=0\\)</span> gives the Hierarchical Variational Models (HVM) bound (from the previous post) for arbitrary <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z)\\)</span>,</li>\n<li>Setting <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z) = q_\\phi(\\psi|x)\\)</span> gives the SIVI bound for arbitrary <span class=\"math inline\">\\(K\\)</span>,</li>\n<li>Setting <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z) = q_\\phi(\\psi|{\\color{red} z}, x)\\)</span> recovers the <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span> exactly.</li>\n</ul>\n<p>The <a href=\"https://arxiv.org/abs/1905.03290\">Importance Weighted Hierarchical Variational Inference</a> paper gives an affirmative answer. <span class=\"math inline\">\\(\\mathcal{U}_K\\)</span> is indeed an upper bound (Multisample Variational Upper Bound) for any <span class=\"math inline\">\\(K\\)</span> and any <span class=\"math inline\">\\(\\tau(\\psi|x,z)\\)</span>. Moreover, it enjoys same guarantees as the IWAE bound (Multisample Variational Lower Bound):</p>\n<ol style=\"list-style-type: decimal\">\n<li><span class=\"math inline\">\\(\\mathcal{U}_K \\ge \\log q_\\phi(z|x)\\)</span></li>\n<li><span class=\"math inline\">\\(\\mathcal{U}_K \\ge \\mathcal{U}_{K+1}\\)</span></li>\n<li><span class=\"math inline\">\\(\\lim_{K \\to \\infty} \\mathcal{U}_K = \\log q_\\phi(z|x)\\)</span></li>\n</ol>\n<p>Combining this bound with the (intractable) ELBO, we obtain the following lower bound on <span class=\"math inline\">\\(\\log p_\\theta(x)\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\hat{\\mathcal{L}}_K^\\text{IWHVI}\n:=\n\\E_{q_\\phi(z, \\psi_0|x)}\n\\E_{\\tau_\\eta(\\psi_{1:K}|z, x)}\n\\log \\frac{p_\\theta(x, z)}{ \\frac{1}{K+1} \\sum_{k=0}^K \\frac{q_\\phi(z, \\psi_k|x)}{\\tau_\\eta(\\psi_k|x,z)} }\n\\]</span></p>\n<p>To test the bound we used a simple toy task of upper-bounding the negative differential entropy <span class=\"math inline\">\\(\\mathbb{E}_{q(z)} \\log q(z)\\)</span> of the standard 50-dimensional Laplace distribution represented as a <a href=\"https://statisticaloddsandends.wordpress.com/2018/12/21/laplace-distribution-as-a-mixture-of-normals/\">Gaussian compound</a>: <span class=\"math display\">\\[\n\\prod_{d=1}^{50} \\text{Laplace}(z_d | 0, 1) = \\int \\prod_{d=1}^{50} \\mathcal{N}(z_d | 0, \\psi_d) \\text{Exp}(\\psi_d | \\tfrac{1}{2}) d\\psi_{1:50}\n\\]</span></p>\n<p>The results look good</p>\n<div class=\"post-image\">\n<p><img src=\"/files/iwhvi-plot.png\" style=\"width: 500px\" /> Comparison of IWHVI bounds for different number of optimization steps over <span class=\"math inline\">\\(\\eta\\)</span>.</p>\n</div>\n<p>Moreover, multisample bounds have been extensively studied and some results translate to our bound as well.</p>\n<h3 id=\"estimating-the-marginal-log-likelihood-log-p_thetax\">Estimating the marginal log-likelihood <span class=\"math inline\">\\(\\log p_\\theta(x)\\)</span></h3>\n<p>Increasing <span class=\"math inline\">\\(K\\)</span> will lead to the bound <span class=\"math inline\">\\(\\hat{\\mathcal{L}}_K^\\text{IWHVI}\\)</span> approaching the ELBO <span class=\"math inline\">\\(\\mathcal{L}\\)</span>, but the gap between the ELBO and the marginal log-likelihood <span class=\"math inline\">\\(\\log p_\\theta(x)\\)</span> is not negligible. Even by employing more powerful variational distribution we might not be able to overcome the <a href=\"https://arxiv.org/abs/1802.02550\">gap introduced by amortization</a>. The standard approach to evaluate Variational Autoencoders it to use the Multisample Variational Lower Bound (IWAE bound) with large <span class=\"math inline\">\\(M\\)</span>. Can we tighten our bound in such a way?</p>\n<p>It turns out, the answer is yes and the tighter bound is simply</p>\n<p><span class=\"math display\">\\[\n\\hat{\\mathcal{L}}_K^\\text{$M$-IWHVI}\n:=\n\\E_{\\substack{q_\\phi(z_{1:M}, \\psi_{1:M, 0}|x) \\\\ \\tau_\\eta(\\psi_{1:M, 1:K}|z_{1:M}, x)}}\n\\log \\frac{1}{M} \\sum_{m=1}^M \\frac{p_\\theta(x, z_m)}{ \\frac{1}{K+1} \\sum_{k=0}^K \\frac{q_\\phi(z_m, \\psi_{m,k}|x)}{\\tau_\\eta(\\psi_{m,k}|x,z_m)} }\n\\le\n\\log p_\\theta(x)\n\\]</span></p>\n<p>Essentially we just sampled the original <span class=\"math inline\">\\(\\hat{\\mathcal{L}}_K^\\text{IWHVI}\\)</span> bound <span class=\"math inline\">\\(M\\)</span> times (independently) and averaged them all inside the <span class=\"math inline\">\\(\\log\\)</span>.</p>\n<h3 id=\"but-tighter-variational-bounds-are-not-necessarily-better\">But Tighter Variational Bounds are Not Necessarily Better</h3>\n<p>It <a href=\"https://arxiv.org/abs/1802.04537\">was observed</a> that training IWAE with large <span class=\"math inline\">\\(K\\)</span> leads to inference networks gradients deterioration. Namely, the signal-to-noise ratio of <span class=\"math inline\">\\(\\nabla_\\phi \\mathcal{L}_K\\)</span> estimates decrease with <span class=\"math inline\">\\(K\\)</span>, while the signal-to-noise ratio of <span class=\"math inline\">\\(\\nabla_\\theta \\mathcal{L}_K\\)</span> estimates increase with <span class=\"math inline\">\\(K\\)</span>. Luckily, the <a href=\"https://arxiv.org/abs/1810.04152\">Doubly Reparameterized Gradients paper</a> resolved this problem. The same derivations apply to our case except for having an additional term corresponding to a sample from <span class=\"math inline\">\\(q_\\phi(\\psi|z, x)\\)</span>, which prevents SNR from increasing, leaving it approximately constant.</p>\n<h3 id=\"debiasing-and-jackknife\">Debiasing and Jackknife</h3>\n<p><a href=\"https://openreview.net/forum?id=HyZoi-WRb\">Nowozin has shown</a> that Multisample Variational Lower Bound <span class=\"math inline\">\\(\\mathcal{L}_K\\)</span> (the IWAE bound) can be seen as a biased evidence estimate with the bias of order <span class=\"math inline\">\\(1/K\\)</span>, which can be reduced with <a href=\"https://en.wikipedia.org/wiki/Jackknife_resampling\">Jackknife</a>. This procedure results in an improved estimator with the bias of order <span class=\"math inline\">\\(1/K^2\\)</span>. By repeating the procedure over and over again <span class=\"math inline\">\\(d\\)</span> times we obtain an estimator with the bias of order <span class=\"math inline\">\\(1/K^{d+1}\\)</span>. The price for that is increased variance, computational complexity and loss of bound guarantees.</p>\n<p>It can be shown that the Multisample Variational Upper Bound <span class=\"math inline\">\\(\\mathcal{U}_K\\)</span> also has the bias of order <span class=\"math inline\">\\(1/(K+1)\\)</span> and thus allows the jackknife. We tested the debiased estimator on a toy task but did not use it in more serious experiments due to loss of guarantees.</p>\n<h2 id=\"is-sivi-obsolete\">Is SIVI obsolete?</h2>\n<p>It depends. In the case of Neural Samplers IWHVI does give a much tighter bound with little extra overhead. However, in some cases the general formulation of IWHVI might be challenging to work with, for example, in the case of <a href=\"https://arxiv.org/abs/1705.07120\">VampPrior</a>-like distributions: <span class=\"math display\">\\[\nq_\\phi(z)\n:= \\frac{1}{N} \\sum_{n=1}^N q_\\phi(z|x_n)\n= \\sum_{n=1}^N q_\\phi(z|n) q(\\psi = n)\n\\]</span> Here <span class=\"math inline\">\\(\\psi\\)</span> is essentially a number from 1 to N and the prior <span class=\"math inline\">\\(q(\\psi)\\)</span> is a uniform distribution. The IWHVI bound would involve <span class=\"math inline\">\\(\\tau(\\psi|x,z)\\)</span> as a categorical distribution over <span class=\"math inline\">\\(N\\)</span> outcomes. Learning <span class=\"math inline\">\\(\\tau\\)</span> not only would require <a href=\"/tags/stochastic%20computation%20graphs%20series.html\">advandced gradient estimates</a><a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a> to deal with discrete random variables, but also an efficient <a href=\"http://ruder.io/word-embeddings-softmax/index.html\">softmax estimators</a> to scale favorably to large datasets. In this setting SIVI presents a much simpler alternative as it frees us from all these hurdles. SIVI only requires sampling from <span class=\"math inline\">\\(U\\{1, \\dots, N\\}\\)</span>, which is easy.</p>\n<p>In many cases though, IWHVI only adds one extra pass of each <span class=\"math inline\">\\(z\\)</span> through a network that generates <span class=\"math inline\">\\(\\tau_\\eta(\\psi|z,x)\\)</span> distribution, which is dominated by <span class=\"math inline\">\\(K+1\\)</span> passes of <span class=\"math inline\">\\(\\psi_{0:K}\\)</span> through a network that generates <span class=\"math inline\">\\(q_\\phi(z|x, \\psi_k)\\)</span> distributions, so it’s added cost is negligible.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In this work we identified a generalized bound that bridges prior work on HVM and SIVI. Such generalized bounds are shown to be much tighter. A particularly nice property is that such multisample bound breaks the vicious cycle we stumbled upon in the last post: increasing the number of samples allows us to tighten the bound without complicating the auxiliary variational distribution <span class=\"math inline\">\\(\\tau_\\eta(\\psi|x,z)\\)</span> and thus reduce the amount of regularuzation simple it imposes on the true inverse model <span class=\"math inline\">\\(q_\\phi(\\psi|x,z)\\)</span>, which lets us learn expressive Neural Samplers. Although multiple samples are still more computationally expensive than just one sample (HVM), <span class=\"math inline\">\\(z\\)</span> typically has much lower dimension than <span class=\"math inline\">\\(x\\)</span>, so this bound is cheaper to evaluate than the IWAE’s one.</p>\n<p>For more details check out the <a href=\"https://arxiv.org/abs/1905.03290\">preprint</a>.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>In the standard VAE the encoder network takes in the observation <span class=\"math inline\">\\(x\\)</span> and generates <span class=\"math inline\">\\(q(z|x)\\)</span> by outputting mean and variance of a normal distribution.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>Although one can attempt avoiding this particular issue by fitting <span class=\"math inline\">\\(\\tau(n|x,z)\\)</span> using the bound with <span class=\"math inline\">\\(K=0\\)</span><a href=\"#fnref2\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Fri, 10 May 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html",
  "dc:creator": "Artem"
}