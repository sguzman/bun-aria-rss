{
  "title": "Some Technical Notes on Kullback-Leibler Divergence",
  "link": "https://alexanderetz.com/2018/09/05/some-technical-notes-on-kullback-leibler-divergence/",
  "comments": "https://alexanderetz.com/2018/09/05/some-technical-notes-on-kullback-leibler-divergence/#comments",
  "dc:creator": "Alex Etz",
  "pubDate": "Wed, 05 Sep 2018 23:00:48 +0000",
  "category": [
    "Statistics",
    "KL",
    "KL divergence",
    "Kullback-Leibler",
    "Technical Notes"
  ],
  "guid": "http://alexanderetz.com/?p=4471",
  "description": "TLDR: I typed up some of my technical notes where I derive the Kullback-Leibler divergence for some common distributions. Find them here on PsyArXiv. The Kullback-Leibler (KL) divergence is a concept that arises pretty frequently across many different areas of statistics. I recently found myself needing to use the KL divergence for a particular Bayesian &#8230; <a href=\"https://alexanderetz.com/2018/09/05/some-technical-notes-on-kullback-leibler-divergence/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Some Technical Notes on Kullback-Leibler Divergence</span></a>",
  "wfw:commentRss": "https://alexanderetz.com/2018/09/05/some-technical-notes-on-kullback-leibler-divergence/feed/",
  "slash:comments": 3,
  "media:content": {
    "media:title": "alexanderetz"
  }
}