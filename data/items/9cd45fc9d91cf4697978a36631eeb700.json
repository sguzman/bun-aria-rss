{
  "title": "Dask Cluster Deployments",
  "link": "",
  "updated": "2016-09-22T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/09/22/cluster-deployments",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><em>All code in this post is experimental.  It should not be relied upon.  For\npeople looking to deploy dask.distributed on a cluster please refer instead to\nthe <a href=\"https://distributed.readthedocs.org\">documentation</a> instead.</em></p>\n\n<p>Dask is deployed today on the following systems in the wild:</p>\n\n<ul>\n  <li>SGE</li>\n  <li>SLURM,</li>\n  <li>Torque</li>\n  <li>Condor</li>\n  <li>LSF</li>\n  <li>Mesos</li>\n  <li>Marathon</li>\n  <li>Kubernetes</li>\n  <li>SSH and custom scripts</li>\n  <li>… there may be more. This is what I know of first-hand.</li>\n</ul>\n\n<p>These systems provide users access to cluster resources and ensure that\nmany distributed services / users play nicely together.  They’re essential for\nany modern cluster deployment.</p>\n\n<p>The people deploying Dask on these cluster resource managers are power-users;\nthey know how their resource managers work and they read the <a href=\"http://distributed.readthedocs.io/en/latest/setup.html\">documentation on\nhow to setup Dask\nclusters</a>.  Generally\nthese users are pretty happy; however we should reduce this barrier so that\nnon-power-users with access to a cluster resource manager can use Dask on their\ncluster just as easily.</p>\n\n<p>Unfortunately, there are a few challenges:</p>\n\n<ol>\n  <li>Several cluster resource managers exist, each with significant adoption.\nFinite developer time stops us from supporting all of them.</li>\n  <li>Policies for scaling out vary widely.\nFor example we might want a fixed number of workers, or we might want\nworkers that scale out based on current use.  Different groups will want\ndifferent solutions.</li>\n  <li>Individual cluster deployments are highly configurable.  Dask needs to get\nout of the way quickly and let existing technologies configure themselves.</li>\n</ol>\n\n<p>This post talks about some of these issues.  It does not contain a definitive\nsolution.</p>\n\n<h2 id=\"example-kubernetes\">Example: Kubernetes</h2>\n\n<p>For example, both <a href=\"http://ogrisel.com/\">Olivier Griesl</a> (INRIA, scikit-learn)\nand <a href=\"https://github.com/timodonnell\">Tim O’Donnell</a> (Mount Sinai, Hammer lab)\npublish instructions on how to deploy Dask.distributed on\n<a href=\"http://kubernetes.io/\">Kubernetes</a>.</p>\n\n<ul>\n  <li><a href=\"https://github.com/ogrisel/docker-distributed\">Olivier’s repository</a></li>\n  <li><a href=\"https://github.com/hammerlab/dask-distributed-on-kubernetes/\">Tim’s repository</a></li>\n</ul>\n\n<p>These instructions are well organized.  They include Dockerfiles, published\nimages, Kubernetes config files, and instructions on how to interact with cloud\nproviders’ infrastructure.  Olivier and Tim both obviously know what they’re\ndoing and care about helping others to do the same.</p>\n\n<p>Tim (who came second) wasn’t aware of Olivier’s solution and wrote up his own.\nTim was capable of doing this but many beginners wouldn’t be.</p>\n\n<p><strong>One solution</strong> would be to include a prominent registry of solutions like\nthese within Dask documentation so that people can find quality references to\nuse as starting points.  I’ve started a list of resources here:\n<a href=\"https://github.com/dask/distributed/pull/547\">dask/distributed #547</a> comments\npointing to other resources would be most welcome..</p>\n\n<p>However, even if Tim did find Olivier’s solution I suspect he would still need\nto change it.  Tim has different software and scalability needs than Olivier.\nThis raises the question of <em>“What should Dask provide and what should it leave\nto administrators?”</em>  It may be that the <em>best</em> we can do is to support\ncopy-paste-edit workflows.</p>\n\n<p>What is Dask-specific, resource-manager specific, and what needs to be\nconfigured by hand each time?</p>\n\n<h2 id=\"adaptive-deployments\">Adaptive Deployments</h2>\n\n<p>In order to explore this topic of separable solutions I built a small adaptive\ndeployment system for Dask.distributed on\n<a href=\"https://mesosphere.github.io/marathon/\">Marathon</a>, an orchestration platform\non top of Mesos.</p>\n\n<p>This solution does two things:</p>\n\n<ol>\n  <li>It scales a Dask cluster dynamically based on the current use.  If there\nare more tasks in the scheduler then it asks for more workers.</li>\n  <li>It deploys those workers using Marathon.</li>\n</ol>\n\n<p>To encourage replication, these two different aspects are solved in two different pieces of code with a clean API boundary.</p>\n\n<ol>\n  <li>A backend-agnostic piece for adaptivity that says when to scale workers up\nand how to scale them down safely</li>\n  <li>A Marathon-specific piece that deploys or destroys dask-workers using the\nMarathon HTTP API</li>\n</ol>\n\n<p>This combines a policy, <em>adaptive scaling</em>, with a backend, <em>Marathon</em> such\nthat either can be replaced easily.  For example we could replace the adaptive\npolicy with a fixed one to always keep N workers online, or we could replace\nMarathon with Kubernetes or Yarn.</p>\n\n<p>My hope is that this demonstration encourages others to develop third party\npackages.  The rest of this post will be about diving into this particular\nsolution.</p>\n\n<h2 id=\"adaptivity\">Adaptivity</h2>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">distributed.deploy.Adaptive</code> class wraps around a <code class=\"language-plaintext highlighter-rouge\">Scheduler</code> and\ndetermines when we should scale up and by how many nodes, and when we should\nscale down specifying which idle workers to release.</p>\n\n<p>The current policy is fairly straightforward:</p>\n\n<ol>\n  <li>If there are unassigned tasks or any stealable tasks and no idle workers,\nor if the average memory use is over 50%, then increase the number of\nworkers by a fixed factor (defaults to two).</li>\n  <li>If there are idle workers and the average memory use is below 50% then\nreclaim the idle workers with the least data on them (after moving data to\nnearby workers) until we’re near 50%</li>\n</ol>\n\n<p>Think this policy could be improved or have other thoughts?  Great.  It was\neasy to implement and entirely separable from the main code so you should be\nable to edit it easily or create your own.  The current implementation is about\n80 lines\n(<a href=\"https://github.com/dask/distributed/blob/master/distributed/deploy/adaptive.py\">source</a>).</p>\n\n<p>However, this <code class=\"language-plaintext highlighter-rouge\">Adaptive</code> class doesn’t actually know how to perform the\nscaling.  Instead it depends on being handed a separate  object, with two\nmethods, <code class=\"language-plaintext highlighter-rouge\">scale_up</code> and <code class=\"language-plaintext highlighter-rouge\">scale_down</code>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">MyCluster</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">scale_up</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n        <span class=\"s\">\"\"\"\n        Bring the total count of workers up to ``n``\n\n        This function/coroutine should bring the total number of workers up to\n        the number ``n``.\n        \"\"\"</span>\n        <span class=\"k\">raise</span> <span class=\"nb\">NotImplementedError</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">scale_down</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"p\">):</span>\n        <span class=\"s\">\"\"\"\n        Remove ``workers`` from the cluster\n\n        Given a list of worker addresses this function should remove those\n        workers from the cluster.\n        \"\"\"</span>\n        <span class=\"k\">raise</span> <span class=\"nb\">NotImplementedError</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>This cluster object contains the backend-specific bits of <em>how</em> to scale up and\ndown, but none of the adaptive logic of <em>when</em> to scale up and down.  The\nsingle-machine\n<a href=\"http://distributed.readthedocs.io/en/latest/local-cluster.html\">LocalCluster</a>\nobject serves as reference implementation.</p>\n\n<p>So we combine this adaptive scheme with a deployment scheme.  We’ll use a tiny\nDask-Marathon deployment library available\n<a href=\"https://github.com/mrocklin/dask-marathon\">here</a></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask_marathon</span> <span class=\"kn\">import</span> <span class=\"n\">MarathonCluster</span>\n<span class=\"kn\">from</span> <span class=\"nn\">distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Scheduler</span>\n<span class=\"kn\">from</span> <span class=\"nn\">distributed.deploy</span> <span class=\"kn\">import</span> <span class=\"n\">Adaptive</span>\n\n<span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">Scheduler</span><span class=\"p\">()</span>\n<span class=\"n\">mc</span> <span class=\"o\">=</span> <span class=\"n\">MarathonCluster</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">cpus</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">mem</span><span class=\"o\">=</span><span class=\"mi\">4000</span><span class=\"p\">,</span>\n                     <span class=\"n\">docker_image</span><span class=\"o\">=</span><span class=\"s\">'mrocklin/dask-distributed'</span><span class=\"p\">)</span>\n<span class=\"n\">ac</span> <span class=\"o\">=</span> <span class=\"n\">Adaptive</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">mc</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This combines a policy, Adaptive, with a deployment scheme, Marathon in a\ncomposable way.  The Adaptive cluster watches the scheduler and calls the\n<code class=\"language-plaintext highlighter-rouge\">scale_up/down</code> methods on the MarathonCluster as necessary.</p>\n\n<h2 id=\"marathon-code\">Marathon code</h2>\n\n<p>Because we’ve isolated all of the “when” logic to the Adaptive code, the\nMarathon specific code is blissfully short and specific.  We include a slightly\nsimplified version below.  There is a fair amount of Marathon-specific setup in\nthe constructor and then simple scale_up/down methods below:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">marathon</span> <span class=\"kn\">import</span> <span class=\"n\">MarathonClient</span><span class=\"p\">,</span> <span class=\"n\">MarathonApp</span>\n<span class=\"kn\">from</span> <span class=\"nn\">marathon.models.container</span> <span class=\"kn\">import</span> <span class=\"n\">MarathonContainer</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">MarathonCluster</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">scheduler</span><span class=\"p\">,</span>\n                 <span class=\"n\">executable</span><span class=\"o\">=</span><span class=\"s\">'dask-worker'</span><span class=\"p\">,</span>\n                 <span class=\"n\">docker_image</span><span class=\"o\">=</span><span class=\"s\">'mrocklin/dask-distributed'</span><span class=\"p\">,</span>\n                 <span class=\"n\">marathon_address</span><span class=\"o\">=</span><span class=\"s\">'http://localhost:8080'</span><span class=\"p\">,</span>\n                 <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">cpus</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">mem</span><span class=\"o\">=</span><span class=\"mi\">4000</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">scheduler</span>\n\n        <span class=\"c1\"># Create Marathon App to run dask-worker\n</span>        <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"n\">executable</span><span class=\"p\">,</span>\n            <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">address</span><span class=\"p\">,</span>\n            <span class=\"s\">'--nthreads'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">cpus</span><span class=\"p\">),</span>\n            <span class=\"s\">'--name'</span><span class=\"p\">,</span> <span class=\"s\">'$MESOS_TASK_ID'</span><span class=\"p\">,</span>  <span class=\"c1\"># use Mesos task ID as worker name\n</span>            <span class=\"s\">'--worker-port'</span><span class=\"p\">,</span> <span class=\"s\">'$PORT_WORKER'</span><span class=\"p\">,</span>\n            <span class=\"s\">'--nanny-port'</span><span class=\"p\">,</span> <span class=\"s\">'$PORT_NANNY'</span><span class=\"p\">,</span>\n            <span class=\"s\">'--http-port'</span><span class=\"p\">,</span> <span class=\"s\">'$PORT_HTTP'</span>\n        <span class=\"p\">]</span>\n\n        <span class=\"n\">ports</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s\">'port'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n                  <span class=\"s\">'protocol'</span><span class=\"p\">:</span> <span class=\"s\">'tcp'</span><span class=\"p\">,</span>\n                  <span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"n\">name</span><span class=\"p\">}</span>\n                 <span class=\"k\">for</span> <span class=\"n\">name</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">,</span> <span class=\"s\">'nanny'</span><span class=\"p\">,</span> <span class=\"s\">'http'</span><span class=\"p\">]]</span>\n\n        <span class=\"n\">args</span><span class=\"p\">.</span><span class=\"n\">extend</span><span class=\"p\">([</span><span class=\"s\">'--memory-limit'</span><span class=\"p\">,</span>\n                     <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">mem</span> <span class=\"o\">*</span> <span class=\"mf\">0.6</span> <span class=\"o\">*</span> <span class=\"mf\">1e6</span><span class=\"p\">))])</span>\n\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s\">'cmd'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s\">' '</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">)</span>\n        <span class=\"n\">container</span> <span class=\"o\">=</span> <span class=\"n\">MarathonContainer</span><span class=\"p\">({</span><span class=\"s\">'image'</span><span class=\"p\">:</span> <span class=\"n\">docker_image</span><span class=\"p\">})</span>\n\n        <span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"n\">MarathonApp</span><span class=\"p\">(</span><span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n                          <span class=\"n\">container</span><span class=\"o\">=</span><span class=\"n\">container</span><span class=\"p\">,</span>\n                          <span class=\"n\">port_definitions</span><span class=\"o\">=</span><span class=\"n\">ports</span><span class=\"p\">,</span>\n                          <span class=\"n\">cpus</span><span class=\"o\">=</span><span class=\"n\">cpus</span><span class=\"p\">,</span> <span class=\"n\">mem</span><span class=\"o\">=</span><span class=\"n\">mem</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Connect and register app\n</span>        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">MarathonClient</span><span class=\"p\">(</span><span class=\"n\">marathon_address</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">create_app</span><span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"ow\">or</span> <span class=\"s\">'dask-%s'</span> <span class=\"o\">%</span> <span class=\"n\">uuid</span><span class=\"p\">.</span><span class=\"n\">uuid4</span><span class=\"p\">(),</span> <span class=\"n\">app</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">scale_up</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">instances</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">scale_app</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">,</span> <span class=\"n\">instances</span><span class=\"o\">=</span><span class=\"n\">instances</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">scale_down</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">workers</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">kill_task</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">app</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">,</span>\n                                  <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">worker_info</span><span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">][</span><span class=\"s\">'name'</span><span class=\"p\">],</span>\n                                  <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This isn’t trivial, you need to know about Marathon for this to make sense, but\nfortunately you don’t need to know much else.  My hope is that people familiar\nwith other cluster resource managers will be able to write similar objects and\nwill publish them as third party libraries as I have with this Marathon\nsolution here:\n<a href=\"https://github.com/mrocklin/dask-marathon\">https://github.com/mrocklin/dask-marathon</a>\n(thanks goes to Ben Zaitlen for setting up a great testing harness for this and\ngetting everything started.)</p>\n\n<h2 id=\"adaptive-policies\">Adaptive Policies</h2>\n\n<p>Similarly, we can design new policies for deployment.  You can read more about\nthe policies for the <code class=\"language-plaintext highlighter-rouge\">Adaptive</code> class in the\n<a href=\"http://distributed.readthedocs.io/en/latest/adaptive.html\">documentation</a> or\nthe\n<a href=\"https://github.com/dask/distributed/blob/master/distributed/deploy/adaptive.py\">source</a>\n(about eighty lines long).  I encourage people to implement and use other\npolicies and contribute back those policies that are useful in practice.</p>\n\n<h2 id=\"final-thoughts\">Final thoughts</h2>\n\n<p>We laid out a problem</p>\n\n<ul>\n  <li><em>How does a distributed system support a variety of cluster resource managers\nand a variety of scheduling policies while remaining sensible?</em></li>\n</ul>\n\n<p>We proposed two solutions:</p>\n\n<ol>\n  <li><em>Maintain a registry of links to solutions, supporting copy-paste-edit practices</em></li>\n  <li><em>Develop an API boundary that encourages separable development of third party libraries.</em></li>\n</ol>\n\n<p>It’s not clear that either solution is sufficient, or that the current\nimplementation of either solution is any good.  This is is an important problem\nthough as Dask.distributed is, today, still mostly used by super-users.  I\nwould like to engage community creativity here as we search for a good\nsolution.</p>"
}