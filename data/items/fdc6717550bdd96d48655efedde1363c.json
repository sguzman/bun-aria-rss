{
  "title": "Machine learning with wearable sensors",
  "link": "",
  "published": "2015-01-09T12:34:00-08:00",
  "updated": "2015-01-09T12:34:00-08:00",
  "author": {
    "name": "Damien RJ"
  },
  "id": "tag:efavdb.com,2015-01-09:/machine-learning-with-wearable-sensors",
  "summary": "<p>A guest post, contributed by Damien Ramunno-Johnson (<a href=\"https://www.linkedin.com/profile/view?id=60223336&amp;authType=NAME_SEARCH&amp;authToken=LOV_&amp;locale=en_US&amp;trk=tyah2&amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1\">LinkedIn</a>, <a href=\"http://www.efavdb.com/about\">bio-sketch</a>)</p>\n<p><strong>Introduction</strong><br>\nWearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples …</p>",
  "content": "<p>A guest post, contributed by Damien Ramunno-Johnson (<a href=\"https://www.linkedin.com/profile/view?id=60223336&amp;authType=NAME_SEARCH&amp;authToken=LOV_&amp;locale=en_US&amp;trk=tyah2&amp;trkInfo=tarId%3A1420748440448%2Ctas%3Adamien%2Cidx%3A1-1-1\">LinkedIn</a>, <a href=\"http://www.efavdb.com/about\">bio-sketch</a>)</p>\n<p><strong>Introduction</strong><br>\nWearable sensors have become increasingly popular over the last few years with the success of smartphones, fitness trackers, and smart watches. All of these devices create a large amount of data that is ideal for machine learning. Two early examples are the FitBit and Jawbone&#8217;s up band, both of which analyze sensor input to determine how many steps the user has taken, a metric which is helpful for measuring physical activity. There is no reason to stop there: With all of this data available it is also possible to extract more information. For example, fitness trackers coming out now can also analyze your&nbsp;sleep.</p>\n<p>In that spirit, I&#8217;m going to show here that it is pretty straightforward to make an algorithm that can differentiate between 6 different&nbsp;states.</p>\n<ol>\n<li>Walking</li>\n<li>Walking&nbsp;Upstairs</li>\n<li>Walking&nbsp;Downstairs</li>\n<li>Sitting</li>\n<li>Standing</li>\n<li>Laying</li>\n</ol>\n<p>To do this I am going to use <a href=\"https://www.python.org/\">Python</a>, <a href=\"http://scikit-learn.org/\">Sklearn</a> and <a href=\"https://plot.ly\">Plot.ly</a>. Plot.ly is a wonderful plotting package that makes interactive graphs you can share. The first step is to import all of the relevant&nbsp;packages.</p>\n<p><strong>Load packages and source data</strong><br>\nFor this example, I used one of the datasets available from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones\"><span class=\"caps\">UCI</span> Machine Learning Repository</a>. For this data set 30 subjects were recorded performing activities of daily living (<span class=\"caps\">ADL</span>) while carrying a waist-mounted smartphone (Samsung Galaxy <span class=\"caps\">II</span>) with embedded inertial sensors. A testing dataset and training dataset are provided. The dataset has 561 features which were created from the sensor data: <span class=\"caps\">XYZ</span> acceleration,&nbsp;etc.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">numpy</span> <span class=\"kn\">import</span> <span class=\"kp\">loadtxt</span>  \n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>  \n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">svm</span><span class=\"p\">,</span> <span class=\"n\">grid_search</span>  \n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">accuracy_score</span><span class=\"p\">,</span> <span class=\"n\">f1_score</span>  \n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_selection</span> <span class=\"kn\">import</span> <span class=\"n\">SelectPercentile</span><span class=\"p\">,</span> <span class=\"n\">f_classif</span>  \n<span class=\"kn\">import</span> <span class=\"nn\">plotly.plotly</span> <span class=\"k\">as</span> <span class=\"nn\">py</span>  \n<span class=\"kn\">from</span> <span class=\"nn\">plotly.graph_objs</span> <span class=\"kn\">import</span> <span class=\"o\">*</span>  \n</pre></div>\n\n\n<p>Now that we have loaded all of our packages, it is time to import the data into memory. This data set is not large enough to cause any memory issues, so go ahead and load the whole&nbsp;thing.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">data_test = loadtxt(&quot;./Wearable/UCI_HAR_Dataset/test/X_test.txt&quot;)  </span>\n<span class=\"err\">label_test=loadtxt(&quot;./Wearable/UCI_HAR_Dataset/test/y_test.txt&quot;)  </span>\n<span class=\"err\">data_train = loadtxt(&quot;./Wearable/UCI_HAR_Dataset/train/X_train.txt&quot;)  </span>\n<span class=\"err\">label_train = loadtxt(&quot;./Wearable/UCI_HAR_Dataset/train/y_train.txt&quot;)  </span>\n</pre></div>\n\n\n<p><strong>Feature selection</strong><br>\nGiven that this data set has training and testing data with labels, it makes sense to do supervised machine learning. We have over 500 potential features to use, which is a lot. Let&#8217;s see if we can get by with fewer features. To do that, we will use <span class=\"caps\">SK</span>-learn&#8217;€™s SelectKBest to keep the top 20 percent of the features, and then transform the&nbsp;data.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"err\">selector = SelectPercentile(f_classif, 20)  </span>\n<span class=\"err\">selector.fit(data_train, label_train)  </span>\n<span class=\"err\">data_train_transformed = selector.transform(data_train)  </span>\n<span class=\"err\">data_test_transformed = selector.transform(data_test)  </span>\n</pre></div>\n\n\n<p><strong>Machine learning</strong><br>\nAt this point you need to decide which algorithm you want to use. I tried a few of them and got the best results using a <a href=\"http://scikit-learn.org/stable/modules/svm.html\">Support Vector Machine</a> (<span class=\"caps\">SVM</span>). SVMs attempt to determine the decision boundary between two classes that is as far away from the data of both classes as possible. In general they work pretty&nbsp;well.</p>\n<p>Let&#8217;s try some parameters and see how good our results&nbsp;are.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"p\">.</span><span class=\"n\">SVC</span><span class=\"p\">(</span><span class=\"n\">kernel</span><span class=\"o\">=</span><span class=\"ss\">&quot;rbf&quot;</span><span class=\"p\">,</span> <span class=\"k\">C</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>  \n<span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">data_train_transformed</span><span class=\"p\">,</span> <span class=\"n\">label_train</span><span class=\"p\">)</span>  \n<span class=\"n\">pred</span><span class=\"o\">=</span><span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">data_test_transformed</span><span class=\"p\">)</span>\n\n<span class=\"n\">print</span> <span class=\"ss\">&quot;Accuracy is %.4f and the f1-score is %.4f &quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>  \n<span class=\"n\">accuracy_score</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">label_test</span><span class=\"p\">),</span> <span class=\"n\">f1_score</span><span class=\"p\">(</span><span class=\"n\">label_test</span><span class=\"p\">,</span> <span class=\"n\">pred</span><span class=\"p\">))</span>  \n</pre></div>\n\n\n<div class=\"highlight\"><pre><span></span><span class=\"err\">&gt;&gt;Accuracy is 0.8812 and the f1-score is 0.8788  </span>\n</pre></div>\n\n\n<p><strong>Optimization</strong><br>\nThat&#8217;s not too bad, but I think we can still optimize our results some more. We could change the parameters manually, or we can automate the task using a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\">grid search</a>. This is a handy module that allows you to do a parameter sweep. Below, I set up a sweep using two different kernels and various penalty term values (C) to see if we can raise our&nbsp;accuracy.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">parameters</span> <span class=\"o\">=</span> <span class=\"err\">{</span><span class=\"s1\">&#39;kernel&#39;</span><span class=\"p\">:(</span><span class=\"s1\">&#39;linear&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;rbf&#39;</span><span class=\"p\">),</span> <span class=\"s1\">&#39;C&#39;</span><span class=\"p\">:[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">]</span><span class=\"err\">}</span>  \n<span class=\"n\">svr</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"p\">.</span><span class=\"n\">SVC</span><span class=\"p\">()</span>  \n<span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">grid_search</span><span class=\"p\">.</span><span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">svr</span><span class=\"p\">,</span> <span class=\"k\">parameters</span><span class=\"p\">)</span>  \n<span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">data_train_transformed</span><span class=\"p\">,</span> <span class=\"n\">label_train</span><span class=\"p\">)</span>  \n<span class=\"n\">pred</span><span class=\"o\">=</span><span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">data_test_transformed</span><span class=\"p\">)</span>\n\n<span class=\"n\">print</span> <span class=\"ss\">&quot;Accuracy is %.4f and the f1-score is %.4f &quot;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>  \n<span class=\"n\">accuracy_score</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">label_test</span><span class=\"p\">),</span> <span class=\"n\">f1_score</span><span class=\"p\">(</span><span class=\"n\">label_test</span><span class=\"p\">,</span> <span class=\"n\">pred</span><span class=\"p\">))</span>  \n</pre></div>\n\n\n<div class=\"highlight\"><pre><span></span><span class=\"err\">&gt;&gt;Accuracy is 0.9430 and the f1-score is 0.9430  </span>\n</pre></div>\n\n\n<p><strong>Visualization</strong><br>\nLooks like we are getting pretty good accuracy for using only 20% of the features available to us. You may have also noticed that I am outputting the <a href=\"http://en.wikipedia.org/wiki/F1_score\">F1-Score</a> which is another measure of the accuracy which takes into account the precision and the&nbsp;recall.</p>\n<p>Now let&#8217;s plot some of these data points to see if we can visualize why this is all working. Here, I am using Plot.ly to make the plot. You can make the plots many different ways including converting matplotlib plots into these online plots. If you click on the &#8220;play with this data&#8221; link at the bottom of the figure (or click <a href=\"https://plot.ly/~Damien RJ/104\">here</a>) you can see the code used to make the&nbsp;plot.</p>\n<p>[iframe src=&#8221;https://plot.ly/~Damien <span class=\"caps\">RJ</span>/104&#8221; width=&#8221;100%&#8221;&nbsp;height=&#8221;680&#8221;]</p>\n<p>I picked two of the features to plot, the z acceleration average, and the z acceleration standard deviation. Note, the gravity component of the acceleration was removed and placed into its own feature. Only 3/6 labels are being plotted to make it a little easier to see what is going on. For example, it is easy to see that the walking profile in the top graph differs significantly from those of standing and laying in the bottom&nbsp;two.</p>\n<p><strong>Discussion</strong><br>\nFrom the graphs above alone, it would be difficult to differentiate between laying and standing. We might be able to comb through different combinations of features to find a set that is more easily distinguishable, but we are limited by the simple fact that it is hard to visualize data in more than 3 dimensions. If it turns out that more than a handful of features need to be considered simultaneously to separate the different classes, this approach will fail. In contrast, we have seen in our <span class=\"caps\">SVM</span> analysis above that it is actually pretty easy to use machine learning to pick out, with high accuracy, a variety of motions from the sensor data. This is a neat application that is currently being applied widely in industry. It illustrates why machine learning is so interesting in general: It allows us to automate data analysis, and apply it to problems where a by-hand, visual analysis is not&nbsp;possible.</p>",
  "category": [
    "",
    ""
  ]
}