{
  "title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively. (arXiv:2211.01642v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01642",
  "description": "<p>Large-scale pre-trained language models have achieved impressive results on a\nwide range of downstream tasks recently. However, fine-tuning an extremely\nlarge-scale pre-trained language model on limited target datasets is often\nplagued by overfitting and representation degradation. In this paper, we\npropose a Dynamic Parameter Selection (DPS) algorithm for the large-scale\npre-trained models during fine-tuning, which adaptively selects a more\npromising subnetwork to perform staging updates based on gradients of\nback-propagation. Experiments on the GLUE benchmark show that DPS outperforms\nprevious fine-tuning methods in terms of overall performance and stability, and\nconsistently achieves better results with variable pre-trained language models.\nIn addition, DPS brings a large magnitude of improvement in out-of-domain\ntransferring experiments and low-resource scenarios, which shows that it can\nmaintain stable general contextual features and reduce the representation\ncollapse. We release our code at https://github.com/ZhangHaojie077/DPS\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"
}