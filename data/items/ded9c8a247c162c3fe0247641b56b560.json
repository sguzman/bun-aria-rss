{
  "title": "December finds in #arxiv",
  "link": "https://mirror2image.wordpress.com/2014/01/05/december-finds-in-arxiv/",
  "dc:creator": "mirror2image",
  "pubDate": "Sun, 05 Jan 2014 12:26:46 +0000",
  "category": [
    "arxiv",
    "computer vision",
    "Deep Learning",
    "Math",
    "optimization"
  ],
  "guid": "http://mirror2image.wordpress.com/2014/01/05/december-finds-in-arxiv/",
  "description": "Repost from my googleplus stream Computer Vision Non-Local means is a local image denoising algorithm Paper shows that non-local mean weights are not identify patches globally point in the images, but are susceptible to aperture problem: http://en.wikipedia.org/wiki/Optical_flow#Estimation That&#8217;s why short radius NLM could be better then large radius NLM. Small radius cutoff play the role of [&#8230;]",
  "content:encoded": "<p>Repost from my <a title=\"G+\" href=\"https://plus.google.com/u/0/+SergeyTen/posts\">googleplus </a>stream</p>\n<p><b>Computer Vision</b><br />\n<i>Non-Local means is a local image denoising algorithm</i><br />\nPaper shows that non-local mean weights are not identify patches globally point in the images, but are susceptible to aperture problem:<br />\n<a href=\"http://en.wikipedia.org/wiki/Optical_flow#Estimation\" target=\"_blank\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Optical_flow#Estimation</a> That&#8217;s why short radius NLM could be better then large radius NLM. Small radius cutoff play the role of regularizer, similar to the Total Variation in Horn-Shunk Optical flow.<br />\n<a href=\"http://en.wikipedia.org/wiki/Horn%E2%80%93Schunck_method\" target=\"_blank\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Horn%E2%80%93Schunck_method</a> (my comment &#8211; TV-L1 is generally better than TV-L2 in Horn-Schunk)<br />\n<a href=\"http://arxiv.org/abs/1311.3768\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1311.3768</a></p>\n<p><b>Deep Learning</b><br />\n<i>Do Deep Nets Really Need to be Deep?</i><br />\nAuthors state that shallow neural nets can in fact achieve similar performance to deep convolutional nets. The problem though is, that they had to be initialized or preconditioned &#8211; they can not be trained using existing algorithms.<br />\nAnd for that initialization they need deep nets. Authors hypothesize that there should be algorithms that allow training of those shallow nets to reach the same performance as deep nets.<br />\n<a href=\"http://arxiv.org/abs/1312.6184\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.6184</a></p>\n<p><i>Intriguing properties of neural networks</i><br />\nThe linear combination of deep-level nodes produce the same results as the the original nodes. That  suggest that nodes the spaces itself rather it&#8217;s representation keep information for deep levels.<br />\nThe input-output mapping also discontinuous &#8211; small perturbations cause misclassification. Those perturbation are not dependent on the training, only on input of classification. (My comment &#8211; sparse coding is generally not smooth on input, another argument that sparse coding is part of internal mechanics of deep learning)<br />\n<a href=\"http://arxiv.org/abs/1312.6199\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.6199</a></p>\n<p><i>From Maxout to Channel-Out: Encoding Information on Sparse Pathways</i><br />\nThis paper start with observation that max-out is a form of sparse coding: only one of the input pathway is chosen for father processing. From this inferred development of that principle:<br />\nremove &#8220;middle&#8221; layer which &#8220;choose&#8221; maximum input, and transfer maximal input at once into next level &#8211; make choice function index-aware. Some other choice function beside the max is considered, but max still seems the best<br />\nPiecewise-constant choice function make interesting reference to previous paper (discontinuity of input-output mapping)<br />\n<a href=\"http://arxiv.org/abs/1312.1909\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.1909</a></p>\n<p><i>Unsupervised Feature Learning by Deep Sparse Coding</i><br />\nThis, for a difference is not about convolutional network.<br />\nInstead SIFT(or similar) descriptors are used to produce bag-of-words, sparse coding is used with max-out, and manifold learning applied to it. (<a href=\"http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction\" target=\"_blank\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction</a>)<br />\n<a href=\"http://arxiv.org/abs/1312.5783\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.5783</a></p>\n<p><i>Generative NeuroEvolution for Deep Learning</i><br />\nI&#8217;m generally wary of evolutionary methods, but this looks kind of interesting &#8211;  it&#8217;s based on <i>compositional pattern producing network</i> (CPPN)- encoding geometric pattern as composition of simple functions.<br />\nThis CPPN is used to encode connectivity pattern of ANN (Convolutional newtwork most used). Thus complete process is the combination of ANN training and evolutionary CPPN training<br />\n<a href=\"http://arxiv.org/abs/1312.5355\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.5355</a></p>\n<p><i>Some Improvements on Deep Convolutional Neural Network Based Image Classification</i>, <i>Unsupervised feature learning by augmenting single images</i><br />\nBotht papers seems about the same subject &#8211; squeeze more out of labeled images by applying a lot of transformation to them(Some of those transformations are implemented in cuda-convnet BTW)<br />\n<a href=\"http://arxiv.org/abs/1312.5402\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.5402</a>, <a href=\"http://arxiv.org/abs/1312.5242\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.5242</a></p>\n<p><i>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</i><br />\nAnalytical exploration of toy 3-layer model *without_ actual non-linear neurons. Model completely linear to input (polynomial to weights). Nevertheless it show some interesting properties, like step in learning curve<br />\n<a href=\"http://arxiv.org/abs/1312.6120\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.6120</a></p>\n<p><b>Optimization</b><br />\n<i>Distributed Interior-point Method for Loosely Coupled Problems</i><br />\nMixing together all my favorite methods: Interior point, Newton, ADMM(Split-Bregman) into one algorithm  and make a distribute implementation of it.<br />\nMixing Newton and ADMM, ADMM and Interior point looks risky to me, though with a lot of subiterations it may work(that&#8217;s why it&#8217;s distributed &#8211; require a lot of calculating power)<br />\nAlso I&#8217;m not sure about convergene of the combined algorithm &#8211; each step&#8217;s convergence is proven, but I&#8217;m not sure the same could be applyed to the combination.<br />\nNewton and ADMM have kind of contradicting optimal conditions &#8211; smoothness vs piecewise linearity. Would like to see more research on this&#8230;<br />\n<a href=\"http://arxiv.org/abs/1312.5440\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.5440</a></p>\n<p><i>Total variation regularization for manifold-valued data</i><br />\nProximal mapping and soft thresholding for manifolds &#8211; analog of ADMM for manifolds.<br />\n<a href=\"http://arxiv.org/abs/1312.7710\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.7710</a></p>\n<p><b>just interesting stuff</b><br />\n<i>Coping with Physical Attacks on Random Network Structures</i><br />\nInclude finding vulnerable spots and results of random attacks<br />\n(My comment &#8211; shouldn&#8217;t it be connected to precolation theory?)<br />\n<a href=\"http://arxiv.org/abs/1312.6189\" target=\"_blank\" rel=\"nofollow\">http://arxiv.org/abs/1312.6189</a></p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}