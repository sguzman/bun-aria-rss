{
  "title": "Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions. (arXiv:2211.01542v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01542",
  "description": "<p>This paper considers continual learning of large-scale pretrained neural\nmachine translation model without accessing the previous training data or\nintroducing model separation. We argue that the widely used\nregularization-based methods, which perform multi-objective learning with an\nauxiliary loss, suffer from the misestimate problem and cannot always achieve a\ngood balance between the previous and new tasks. To solve the problem, we\npropose a two-stage training method based on the local features of the real\nloss. We first search low forgetting risk regions, where the model can retain\nthe performance on the previous task as the parameters are updated, to avoid\nthe catastrophic forgetting problem. Then we can continually train the model\nwithin this region only with the new training data to fit the new task.\nSpecifically, we propose two methods to search the low forgetting risk regions,\nwhich are based on the curvature of loss and the impacts of the parameters on\nthe model output, respectively. We conduct experiments on domain adaptation and\nmore challenging language adaptation tasks, and the experimental results show\nthat our method can achieve significant improvements compared with several\nstrong baselines.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bojie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"
}