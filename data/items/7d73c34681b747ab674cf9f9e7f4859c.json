{
  "title": "How to Install Presto or Trino on a Cluster and Query Distributed Data on Apache Hive and HDFS",
  "link": "",
  "published": "2020-10-17T00:00:00-05:00",
  "updated": "2020-10-17T00:00:00-05:00",
  "id": "http://janakiev.com/blog/presto-cluster",
  "content": "<p><a href=\"https://prestodb.io/\">Presto</a> is an open source distibruted query engine built for Big Data enabling high performance SQL access to a large variety of data sources including HDFS, PostgreSQL, MySQL, Cassandra, MongoDB, Elasticsearch and Kafka among others.</p>\n\n<p><strong>Update 6 Feb 2021:</strong> PrestoSQL is now rebranded as Trino. Read more about it <a href=\"https://trino.io/blog/2020/12/27/announcing-trino.html\">here</a>. If you installed PrestoSQL before, have a look at the <a href=\"https://trino.io/blog/2021/01/04/migrating-from-prestosql-to-trino.html\">migration guide</a>. This tutorial was done using PrestoDB 0.242 and PrestoSQL 344.</p>\n\n<p>To start off with a bit of history: Presto started 2012 in Facebook and was later released in 2013 as an open source project under the Apache Licence. It is most comparable to <a href=\"https://spark.apache.org/\">Apache Spark</a> in the Big Data space as it also offers query optimization with the <a href=\"https://databricks.com/de/glossary/catalyst-optimizer\">Catalyst Optimizer</a> and an SQL interface to its data sources. Presto and Apache Spark have its own resource manager, but Apache Spark is generally run on top of Hadoops’ <a href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\">YARN</a> resource manager. Presto on the other hand uses its own coordinator within the cluster to schedule queries among its workers.</p>\n\n<p>Presto itself does not offer a database and should be only used for large analytical queries that fall into <a href=\"https://en.wikipedia.org/wiki/Online_analytical_processing\">Online Analytical Processing (OLAP)</a>. Therefore <a href=\"https://en.wikipedia.org/wiki/Online_transaction_processing\">Online transaction processing (OLTP)</a> workloads should be avoided. Presto offers a large variety of <a href=\"https://prestodb.io/docs/current/connector.html\">connectors</a> like for example MySQL, PostgreSQL, HDFS with Hive, Cassandra, Redis, Kafka, ElasticSearch, MongoDB among others. Further, Presto enables federated queries which means that you can query different databases with different schemas in the same SQL statement at the same time.</p>\n\n<p>To read further into the inner workings and architecture behind Presto, check out the 2019 paper <a href=\"https://prestosql.io/Presto_SQL_on_Everything.pdf\">Presto: SQL on Everything</a>.</p>\n\n<h1 id=\"installation\">Installation</h1>\n\n<p>Prerequesite for this tutorial is having a running Hadoop and Hive installation, you can follow the instructions in the tutorial <a href=\"https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/\">How to Install and Set Up a 3-Node Hadoop Cluster</a> and this <a href=\"https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-InstallationandConfiguration\">Hive Tutorial</a>. The configuration and setup scripts used for this tutorial including further configurations of the HDFS cluster can be found in this <a href=\"https://github.com/njanakiev/scalable-geospatial-data-science\">repository</a>. This installation also requires Java version &gt;= 11. To install Java you can type:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>apt-get update\n<span class=\"nb\">sudo </span>apt-get <span class=\"nb\">install </span>openjdk-11-jdk-headless <span class=\"se\">\\</span>\n                     openjdk-11-jre-headless <span class=\"se\">\\</span>\n                     openjdk-11-jre\n</code></pre></div></div>\n\n<p>Note, that there are two active projects of Presto, <a href=\"https://trino.io/\">Trino</a> and <a href=\"https://prestodb.io/\">PrestoDB</a>. To clarify the difference between both, have a read into <a href=\"https://github.com/prestosql/presto/issues/380\">What is the relationship of prestosql and prestodb?</a>. All of this article including the configuration runs on both with the releases of presto-server 0.242 and trino-server 352. In this article we will focus on Trino. First, <a href=\"https://trino.io/download.html\">download Trino</a> and unpack it to a desired location. In this case it will be located in <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino</code>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://repo1.maven.org/maven2/io/trino/trino-server/352/trino-server-352.tar.gz\"</span>\n<span class=\"nb\">tar</span> <span class=\"nt\">-xzvf</span> trino-server-352.tar.gz\n<span class=\"nb\">sudo mv </span>trino-server-352 /usr/local/trino\n<span class=\"nb\">sudo chown</span> <span class=\"nv\">$$</span>USER:<span class=\"nv\">$$</span>USER /usr/local/trino\n</code></pre></div></div>\n\n<p>Next, add <code class=\"language-plaintext highlighter-rouge\">TRINO_HOME</code> environment variable and add the <code class=\"language-plaintext highlighter-rouge\">TRINO_HOME/bin</code> directory to the <code class=\"language-plaintext highlighter-rouge\">PATH</code> environment variable in <code class=\"language-plaintext highlighter-rouge\">~/.bashrc</code>:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">TRINO_HOME</span><span class=\"o\">=</span>/usr/local/trino\n<span class=\"nb\">export </span><span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$$</span>PATH:<span class=\"nv\">$$</span>TRINO_HOME/bin\n</code></pre></div></div>\n\n<p>If you aim to run multiple servers, this needs to be done for all servers.</p>\n\n<h1 id=\"configuration\">Configuration</h1>\n\n<p>Before being able to start Presto, you need to to configure Presto on your system. For this you will need to add the following files:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">etc/node.properties</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">etc/jvm.config</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">etc/config.properties</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">etc/log.properties</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">etc/catalog/hive.properties</code></li>\n</ul>\n\n<p>Let’s start by creating the <code class=\"language-plaintext highlighter-rouge\">etc</code> folder where all these files will be located:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">mkdir</span> /usr/local/presto/etc\n</code></pre></div></div>\n\n<p>Now, create a node properties file <code class=\"language-plaintext highlighter-rouge\">etc/node.properties</code> with the contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>node.environment=production\nnode.id=ffffffff-ffff-ffff-ffff-ffffffffffff\nnode.data-dir=/usr/local/presto/data\n</code></pre></div></div>\n\n<p>Each server needs a unique <code class=\"language-plaintext highlighter-rouge\">node.id</code>. For this, you can generate a <a href=\"https://en.wikipedia.org/wiki/Universally_unique_identifier\">UUID</a> with Python, by typing <code class=\"language-plaintext highlighter-rouge\">python -c \"import uuid; print(uuid.uuid1())\"</code> Another way is to install the uuid package with <code class=\"language-plaintext highlighter-rouge\">sudo apt install uuid</code> and then typing <code class=\"language-plaintext highlighter-rouge\">uuid</code>. Next, create JVM config file <code class=\"language-plaintext highlighter-rouge\">etc/jvm.config</code> with the contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>-server\n-Xmx16G\n-XX:+UseG1GC\n-XX:G1HeapRegionSize=32M\n-XX:+UseGCOverheadLimit\n-XX:+ExplicitGCInvokesConcurrent\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:+ExitOnOutOfMemoryError\n-Djdk.attach.allowAttachSelf=true\n</code></pre></div></div>\n\n<p>It is important to set the flag <code class=\"language-plaintext highlighter-rouge\">-Xmx16G</code> to the available RAM on the nodes (in this case 16 GB). A rule of thumb is to allocate around 80% of the available RAM to leave some for the operating system and other processes. The flag <code class=\"language-plaintext highlighter-rouge\">-Djdk.attach.allowAttachSelf=true</code> needs to be added in order to avoid the error <a href=\"https://github.com/prestosql/presto/issues/752\">Error injecting constructor, java.io.IOException: Can not attach to current VM</a>. These two files (except the <code class=\"language-plaintext highlighter-rouge\">node.id</code>) are the same for all servers.</p>\n\n<h2 id=\"single-node\">Single Node</h2>\n\n<p>Create config properties file <code class=\"language-plaintext highlighter-rouge\">etc/config.properties</code> with the contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\nquery.max-memory=5GB\nquery.max-memory-per-node=1GB\nquery.max-total-memory-per-node=2GB\ndiscovery-server.enabled=true\ndiscovery.uri=http://master-node:8080\n</code></pre></div></div>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">discovery.uri</code> specifies the URI of the discovery server. This is generally the same server where the coordinator is located, so take the host and port of this server.</p>\n\n<h2 id=\"multiple-nodes\">Multiple Nodes</h2>\n\n<p>Configuration for the coordinator is the following:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>coordinator=true\nnode-scheduler.include-coordinator=false\nhttp-server.http.port=8080\nquery.max-memory=50GB\nquery.max-memory-per-node=1GB\nquery.max-total-memory-per-node=2GB\ndiscovery-server.enabled=true\ndiscovery.uri=http://master-node.net:8080\n</code></pre></div></div>\n\n<p>If the coordinator should be also used to compute queries set <code class=\"language-plaintext highlighter-rouge\">node-scheduler.include-coordinator=true</code>.</p>\n\n<p>Configuration for the workers:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>coordinator=false\nhttp-server.http.port=8080\nquery.max-memory=50GB\nquery.max-memory-per-node=1GB\nquery.max-total-memory-per-node=2GB\ndiscovery.uri=http://master-node.net:8080\n</code></pre></div></div>\n\n<p>Some more points and the various properties are covered in the documentation on <a href=\"https://prestodb.io/docs/current/installation/deployment.html\">Deploying Presto</a></p>\n\n<h1 id=\"logging-in-trino\">Logging in Trino</h1>\n\n<p>Create log configuration file <code class=\"language-plaintext highlighter-rouge\">etc/log.properties</code> with the following contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>io.trino=DEBUG\n</code></pre></div></div>\n\n<p>The log levels are <code class=\"language-plaintext highlighter-rouge\">DEBUG</code>, <code class=\"language-plaintext highlighter-rouge\">INFO</code>, <code class=\"language-plaintext highlighter-rouge\">WARN</code> and <code class=\"language-plaintext highlighter-rouge\">ERROR</code>, default is <code class=\"language-plaintext highlighter-rouge\">INFO</code>. By default, the log files are located in <code class=\"language-plaintext highlighter-rouge\">data/var/log</code> and are generally helpful when searching for issues with failed Presto queries or crashed Presto nodes. Additionally, Presto offers a <a href=\"https://prestosql.io/docs/current/connector/jmx.html\">JMX Connector</a> to monitor and debug <a href=\"https://en.wikipedia.org/wiki/Java_Management_Extensions\">Java Management Extensions (JMX)</a> information from all nodes.</p>\n\n<h1 id=\"hive-connector\">Hive Connector</h1>\n\n<p>In order to connect to HDFS, we will use <a href=\"https://hive.apache.org/\">Apache Hive</a>, which is commonly used together with Hadoop and HDFS to provide an SQL-like interface. Apache Hive was open sourced 2008, again by Facebook. Presto was later designed to further scale operations and reduce query time. Presto and the Hive connector do not use the Hive runtime, but rather act as a replacement in order to run interactive queries.</p>\n\n<p>Add the Hive connector by adding the configuring the connection with <code class=\"language-plaintext highlighter-rouge\">etc/catalog/hive.properties</code> with the following contents (port 9083 by default):</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>connector.name=hive-hadoop2\nhive.metastore.uri=thrift://localhost:9083\n</code></pre></div></div>\n\n<p>For more information have a look at the documentation for the <a href=\"https://prestosql.io/docs/current/connector/hive.html\">Hive Connector</a>. Start the <a href=\"https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Overview\">HiveServer2</a> if it is not already running with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>hive <span class=\"se\">\\</span>\n  <span class=\"nt\">--service</span> hiveserver2 <span class=\"se\">\\</span>\n  <span class=\"nt\">--hiveconf</span> hive.server2.thrift.port<span class=\"o\">=</span>9083\n</code></pre></div></div>\n\n<p>Start the command-line interface of Hive with <a href=\"https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients\">Beeline</a> and create a schema that we use for our data with:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">SCHEMA</span> <span class=\"n\">tutorial</span><span class=\"p\">;</span>\n<span class=\"n\">USE</span> <span class=\"n\">tutorial</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Here, we will use the often used and abused <a href=\"https://archive.ics.uci.edu/ml/datasets/Iris\">Iris Data Set</a>. After downloading the data set, create a table for our data with:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">tutorial</span><span class=\"p\">.</span><span class=\"n\">iris</span><span class=\"p\">;</span>\n\n<span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">tutorial</span><span class=\"p\">.</span><span class=\"n\">iris</span> <span class=\"p\">(</span>\n  <span class=\"n\">sepal_length</span> <span class=\"nb\">FLOAT</span><span class=\"p\">,</span>\n  <span class=\"n\">sepal_width</span>  <span class=\"nb\">FLOAT</span><span class=\"p\">,</span>\n  <span class=\"n\">petal_length</span> <span class=\"nb\">FLOAT</span><span class=\"p\">,</span>\n  <span class=\"n\">petal_width</span>  <span class=\"nb\">FLOAT</span><span class=\"p\">,</span>\n  <span class=\"k\">class</span>        <span class=\"n\">STRING</span>\n<span class=\"p\">)</span> <span class=\"k\">ROW</span> <span class=\"n\">FORMAT</span> <span class=\"n\">DELIMITED</span> <span class=\"n\">FIELDS</span> <span class=\"n\">TERMINATED</span> <span class=\"k\">BY</span> <span class=\"s1\">','</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Then, insert the downloaded data to Hive with:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">LOAD</span> <span class=\"k\">DATA</span> <span class=\"k\">LOCAL</span> <span class=\"n\">INPATH</span> <span class=\"s1\">'/path/to/data/iris.data'</span> \n<span class=\"n\">OVERWRITE</span> <span class=\"k\">INTO</span> <span class=\"k\">TABLE</span> <span class=\"n\">tutorial</span><span class=\"p\">.</span><span class=\"n\">iris</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>To see the freshly create table type <code class=\"language-plaintext highlighter-rouge\">SHOW TABLES tutorial;</code>. To show metadata about a table such as column names and their data types, you can type <code class=\"language-plaintext highlighter-rouge\">DESCRIBE tutorial.iris;</code> which should return the following output:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>+---------------+------------+----------+--+\n|   col_name    | data_type  | comment  |\n+---------------+------------+----------+--+\n| sepal_length  | float      |          |\n| sepal_width   | float      |          |\n| petal_length  | float      |          |\n| petal_width   | float      |          |\n| class         | string     |          |\n+---------------+------------+----------+--+\n5 rows selected (2 seconds)\n</code></pre></div></div>\n\n<p>For even more information you can use <code class=\"language-plaintext highlighter-rouge\">DESCRIBE FORMATTED tutorial.iris;</code>.</p>\n\n<p>The Hive connector is also used with the various cloud-based object stores like S3, GCS, Azure Blob Storage, Minio and others. To read more on this have a read in this <a href=\"https://trino.io/blog/2020/10/20/intro-to-hive-connector.html\">explainer</a>.</p>\n\n<h1 id=\"start-trino\">Start Trino</h1>\n\n<p>Now, everything is set to start Presto. To start the Presto daemon simply run on each node:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>launcher start\n</code></pre></div></div>\n\n<p>The status of the daemon and its PID can be checked with:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>launcher status\n</code></pre></div></div>\n\n<p>The deamon can be stopped with:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>launcher stop\n</code></pre></div></div>\n\n<p>The server can be accessed at <code class=\"language-plaintext highlighter-rouge\">http://localhost:8080</code>, which was previously configured in <code class=\"language-plaintext highlighter-rouge\">etc/config.properties</code>. This would give you an overview of the cluster and statistics on the queries that have been run:</p>\n\n<p><img src=\"/assets/presto_cluster_files/presto_ui_screenshot.png\" alt=\"Presto UI Screenshot\" /></p>\n\n<h1 id=\"command-line-interface\">Command Line Interface</h1>\n\n<p>Presto does not install the command-line by default, therefore we need to download the command-line interface separately:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://repo1.maven.org/maven2/io/prestosql/presto-cli/344/</span><span class=\"se\">\\</span><span class=\"s2\">\n      presto-cli-344-executable.jar\"</span>\n<span class=\"nb\">mv </span>presto-cli-344-executable.jar /usr/local/presto/bin/presto\n<span class=\"nb\">chmod</span> +x /usr/local/presto/bin/presto\n</code></pre></div></div>\n\n<p>Start the Presto CLI for Hive catalog with the previously created <code class=\"language-plaintext highlighter-rouge\">tutorial</code> schema:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>presto <span class=\"se\">\\</span>\n  <span class=\"nt\">--catalog</span> hive <span class=\"se\">\\</span>\n  <span class=\"nt\">--schema</span> tutorial\n</code></pre></div></div>\n\n<p>Here is the documentation on the <a href=\"https://prestosql.io/docs/current/installation/cli.html\">Command Line Interface</a>. Instead of using the Presto command-line interface, you can also use <a href=\"https://dbeaver.com/\">DBeaver</a> which offers a Presto connection via <a href=\"https://en.wikipedia.org/wiki/Java_Database_Connectivity\">Java Database Connectivity (JDBC)</a>. Somewhat similar to Hive you can list all available Hive schemas with:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>SHOW SCHEMAS FROM hive;\n</code></pre></div></div>\n\n<p>To list all the available tables from a Hive schema, type:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>SHOW TABLES FROM hive.tutorials;\n</code></pre></div></div>\n\n<p>Now, let’s try querying the data we previously added. Here we answer the question: <em>What is the average sepal length and width per iris class</em>:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">SELECT</span> \n  <span class=\"k\">class</span><span class=\"p\">,</span> \n  <span class=\"k\">AVG</span><span class=\"p\">(</span><span class=\"n\">sepal_length</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"n\">avg_sepal_length</span><span class=\"p\">,</span>\n  <span class=\"k\">AVG</span><span class=\"p\">(</span><span class=\"n\">sepal_width</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"n\">avg_sepal_width</span>\n<span class=\"k\">FROM</span> <span class=\"n\">tutorial</span><span class=\"p\">.</span><span class=\"n\">iris</span>\n<span class=\"k\">GROUP</span> <span class=\"k\">BY</span> <span class=\"k\">class</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Which should return the following output:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>      class      | avg_sepal_length | avg_sepal_width \n-----------------+------------------+-----------------\n Iris-versicolor |            5.936 |            2.77 \n Iris-setosa     |            5.006 |           3.418 \n Iris-virginica  |            6.588 |           2.974 \n(3 rows)\n</code></pre></div></div>\n\n<p>That’s nice, we can already see clear differences here between the flowers without resorting to deep learning. Also note that Presto uses the ANSI SQL Dialect. To read more about the SQL Syntax in Presto have a look at the documentation on <a href=\"https://prestosql.io/docs/current/sql.html\">SQL Statement Syntax</a> and to analyze the execution plan, you can use <a href=\"https://prestosql.io/docs/current/sql/explain.html\">EXPLAIN</a> or <a href=\"https://prestosql.io/docs/current/sql/explain-analyze.html\">EXPLAIN ANALZE</a> in front of a statement or explore the Live Plan for a query in the Presto UI.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>For more information have a look at the paper <a href=\"https://prestosql.io/Presto_SQL_on_Everything.pdf\">Presto: SQL on Everything</a>, which explains the inner workings of Presto in much more technical detail and also explains some of the challenges that Presto tries to solve. Additionally there is <a href=\"https://prestosql.io/blog/2020/04/11/the-definitive-guide.html\">Presto: The Definitive Guide</a>, a great book that goes into much more detail on how to use and configure Presto in an optimal way.</p>\n\n<p>Further resources and links are listed in the homepage of the <a href=\"https://prestosql.io/foundation.html\">Presto Software Foundation</a>. It is also helpful to directly check the issues and pull requests in the <a href=\"https://github.com/prestodb/presto\">prestodb/presto</a> and <a href=\"https://github.com/prestosql/presto\">prestosql/presto</a> Github pages as they often include detailed descriptions of some of the more advanced features. Finally, Mark Litwintischik did a great performance comparision of <a href=\"https://tech.marksblogg.com/billion-nyc-taxi-rides-spark-2-4-versus-presto-214.html\">Spark 2.4.0 versus Presto 0.214</a>.</p>",
  "author": {
    "name": "Nikolai Janakiev"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "Presto is an open source distibruted query engine built for Big Data enabling high performance SQL access to a large variety of data sources including HDFS, PostgreSQL, MySQL, Cassandra, MongoDB, Elasticsearch and Kafka among others.",
  "media:thumbnail": "",
  "media:content": ""
}