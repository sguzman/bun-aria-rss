{
  "title": "A decade of fun and learning",
  "link": "https://blogs.princeton.edu/imabandit/2019/12/30/a-decade-of-fun-and-learning/",
  "comments": "https://blogs.princeton.edu/imabandit/2019/12/30/a-decade-of-fun-and-learning/#comments",
  "dc:creator": "Sebastien Bubeck",
  "pubDate": "Mon, 30 Dec 2019 07:22:52 +0000",
  "category": "Uncategorized",
  "guid": "https://blogs.princeton.edu/imabandit/?p=1416",
  "description": "<p>I started out this decade with the project of writing a survey of the multi-armed bandit literature, which I had read thoroughly during the graduate studies that I was about to finish. At the time we resisted the temptation to &#8230; <a href=\"https://blogs.princeton.edu/imabandit/2019/12/30/a-decade-of-fun-and-learning/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>\n",
  "content:encoded": "<p>I started out this decade with the project of writing a survey of the multi-armed bandit literature, which I had read thoroughly during the graduate studies that I was about to finish. At the time we resisted the temptation to name the survey &#8220;modern banditology&#8221;, which was indeed the right call given how much this &#8220;modern&#8221; picture has evolved over the decade! It is truly wonderful to now end the decade with two new iterations on the work we did in that survey: </p>\n<ol>\n<li><a href=\"https://tor-lattimore.com/downloads/book/book.pdf\" class=\"lipdf\">Bandit algorithms</a> by Tor Lattimore and Csaba Szepesvari</li>\n<li><a href=\"https://arxiv.org/abs/1904.07272\" class=\"liinternal\">Introduction to bandits</a> by Alex Slivkins</li>\n</ol>\n<p>These new references very significantly expand the 2012 survey, and they are wonderful starting points for anyone who wants to enter the field.</p>\n<p>Here are some of the discoveries in the world of bandits that stood out for me this decade:</p>\n<ol>\n<li>We now understand very precisely Thompson Sampling, the first bandit strategy that was proposed back in 1933. The most beautiful reference here is the one by Dan Russo and Ben Van Roy: <a href=\"http://www.jmlr.org/papers/volume17/14-087/14-087.pdf\" class=\"lipdf\">An Information-Theoretic Analysis of Thompson Sampling</a>, JMLR 2016. Another one that stands out is <a href=\"http://www.jmlr.org/proceedings/papers/v23/agrawal12/agrawal12.pdf\" class=\"lipdf\">Analysis of thompson sampling for the multi-armed bandit problem</a> by S. Agrawal and N. Goyal at COLT 2012.</li>\n<li>T^{2/3} lower bound for  *non-stochastic* bandit with switching cost by Dekel, Ding, Koren and Peres at <u><font color=\"#000121\"><a href=\"https://arxiv.org/abs/1310.2997\" class=\"liinternal\">STOC 201</a></font></u><a href=\"https://arxiv.org/abs/1310.2997\" class=\"liinternal\">4</a>. This is a striking result for several reasons. In particular the proof has to be based on a non-trivial stochastic process, since for the classical stochastic i.i.d. model one can obtain \\sqrt{T} (very easily in fact). </li>\n<li>We now know that bandit convex optimization is &#8220;easy&#8221;, in the sense that it is a \\sqrt{T}-regret type problem. What&#8217;s more is that in our <a href=\"https://arxiv.org/abs/1607.03084\" class=\"liinternal\">STOC 2017 paper</a> with Y.T. Lee and R. Eldan we introduced a new way to do function estimation based on bandit feedback, using kernels (I have written at length about this on <a href=\"https://blogs.princeton.edu/imabandit/2016/08/06/kernel-based-methods-for-bandit-convex-optimization-part-1/\" class=\"liinternal\">this blog</a>).</li>\n<li>A very intriguing model of computation for contextual bandit was proposed, where one can access the policy space only through an offline optimization oracle. With such access, the classical Exp4 algorithm cannot be simulated, and thus one needs new strategies. We now have a reasonable understanding that \\sqrt{T} is doable with mild assumptions (see e.g. this ICML 2014 paper on &#8220;<u><font color=\"#000117\"><a href=\"http://arxiv.org/abs/1402.0555\" class=\"liexternal\">Taming the Monster</a></font>&#8220;</u> by Agarwal, Hsu, Kale, Langford, L. Li and Schapire) and that it is impossible with no assumptions (work of Hazan and Koren at <a href=\"https://arxiv.org/abs/1504.02089\" class=\"liinternal\">STOC 2016</a>).</li>\n<li>Honorable mentions also go to the work of Wei and Luo showing that very strong variation bounds are possible in bandits (see this <a href=\"https://arxiv.org/abs/1801.03265\" class=\"liinternal\">COLT 2018 paper</a>), and Zimmert and Seldin who made striking progress on the best of both worlds phenomenon that we discovered with Slivkins at the beginning of the decade (I blogged about it <a href=\"https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/\" class=\"liinternal\">here</a> already).</li>\n</ol>\n<h2>Life beyond bandits</h2>\n<p>In addition to starting the decade with the bandit survey, I also started it with being bored with the bandit topic altogether. I thought that many (if not most) of the fundamental results were now known, and it was a good idea to move on to something else. Obviously I was totally wrong, as you can see with all the works cited above (and many many more for stochastic bandits, including much deeper understanding of best arm identification, a topic very close to my heart, see e.g., <a href=\"http://www.jmlr.org/papers/volume17/kaufman16a/kaufman16a.pdf\" class=\"lipdf\">[Kaufmann, Cappe, Garivier, JMLR 16]</a>). In fact I am now optimistic that there is probably another decade-worth of exploration left for the bandit problem(s). Nevertheless I ventured outside, and explored the world of optimization (out of which first came <a href=\"http://sbubeck.com/Bubeck15.pdf\" class=\"lipdf\">a survey</a>, and more recently <a href=\"https://www.youtube.com/playlist?list=PLAPSKVSdi0oZPbS-UD_kwT4ePZQx_CiME\" class=\"liinternal\">video lectures</a>) and briefly networks (another <a href=\"https://arxiv.org/abs/1609.03511\" class=\"liinternal\">modest survey</a> came out of this too). </p>\n<p>Here are some of the landmark optimization results of this decade in my view:</p>\n<ol>\n<li>Perhaps the most striking result of the decade in optimization is the observation that for finite sum problems, one can reduce the variance in stochastic gradient descent by somehow centering the estimates (e.g., using a slowly moving sequence on which we can afford to compute full gradients; but this is not the only way to perform such variance reduction). This idea, while very simple, has a lot of implications, both in practice and in theory! The origin of the idea are in the SAG algorithm of <a href=\"http://papers.nips.cc/paper/4633-a-stochastic-gradient-method-with-an-exponential-convergence-_ra\" class=\"liexternal\">[Schmidt, Le Roux, Bach, NIPS 2012]</a> and SDCA <a href=\"http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf\" class=\"lipdf\">[Shalev-Shwartz and Zhang, JMLR 2013]</a>. A simpler instantiation of the idea, called SVRG appeared shortly after in <a href=\"http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf\" class=\"lipdf\">[Johnson and Zhang, NIPS 2013]</a> (and also independently at the same NeurIPS, in <a href=\"https://papers.nips.cc/paper/4941-mixed-optimization-for-smooth-functions\" class=\"liinternal\">[M. Madhavi, L. Zhang, R. Li, NIPS 2013]</a>).</li>\n<li>An intriguing direction that I pursued fervently is the use of convex optimization for problems that have a priori nothing to do with convex optimization. A big inspiration for me was the COLT 2008 paper by Abernethy, Hazan and Rakhlin, who showed how mirror descent naturally solves bandit problems. In this decade, we (this we includes myself and co-authors, but also various other teams) explored how to use mirror descent for other online decision making problems, and made progress on some long-standing problems (k-server and MTS), see for example this set of <a href=\"https://www.youtube.com/playlist?list=PLAPSKVSdi0obG1b3w4k41JMLFbyBJS5AQ\" class=\"liinternal\">video lectures</a> on the &#8220;Five miracles of mirror descent&#8221;.</li>\n<li>Arnak Dalalyan showed how to use ideas inspired from convex optimization to <a href=\"https://arxiv.org/abs/1412.7392\" class=\"liinternal\">analyze the Langevin Monte Carlo algorithm</a>. This was absolutely beautiful work, that led to many many follow-ups.</li>\n<li>There has been a lot of rewriting of Nesterov&#8217;s acceleration, to try to demystify it. Overall the enterprise is not yet a resounding success in my opinion, but certainly a lot of progress has been made (again I have written a lot about it on this blog already). We now even have optimal acceleration for higher order of smoothness (see this 15 authors <u><font color=\"#000117\"></font><font><a href=\"http://proceedings.mlr.press/v99/gasnikov19b/gasnikov19b.pdf\" class=\"lipdf\">paper at COLT 2019</a></font></u>), but these techniques are clouded with the same shroud of mystery as was Nesterov&#8217;s original method.</li>\n<li>Yin Tat Lee and Aaron Sidford obtained an <a href=\"https://arxiv.org/abs/1910.08033\" class=\"liinternal\">efficient construction of a universal barrier</a>.</li>\n<li>We now know that certain problems cannot be efficiently represented by SDPs (the so-called &#8220;extension complexity), see e.g. this work by <a href=\"https://arxiv.org/abs/1411.6317\" class=\"liinternal\">Lee-Raghavendra-Steurer</a>.</li>\n<li>We now know how to chase convex bodies, and we can even do so very elegantly with <a href=\"https://blogs.princeton.edu/imabandit/2019/11/05/convex-body-chasing-steiner-point-sellke-point-and-soda-2020-best-papers/\" class=\"liinternal\">the Steiner/Sellke point</a>.</li>\n</ol>\n<h2>Some other things that captivated me</h2>\n<p>The papers above are mostly topics on which I tried to work at some point. Here are some questions that I didn&#8217;t work on but followed closely and was fascinated by the progress:</p>\n<ol>\n<li>The stochastic block model was essentially solved during this decade, see for example <a href=\"http://www.princeton.edu/~eabbe/publications/abbe_FNT_2.pdf\" class=\"lipdf\">this survey</a> by Emmanuel Abbe.</li>\n<li>The computational/statistical tradeoffs were extensively explored, yet they remain mysterious. A nice impulse to the field was given by this <a href=\"https://projecteuclid.org/euclid.aos/1378386239\" class=\"liinternal\">COLT 2013 paper</a> by Berthet and Rigollet relating sparse PCA and planted clique. In a similar spirit I also enjoyed the more recent work by Moitra, Jerry Li, and many co-authors on computationally efficient robust estimation (see e.g., <a href=\"https://arxiv.org/abs/1906.11366\" class=\"liinternal\">this recent paper</a>)</li>\n<li>Adaptive data analysis strikes me as both very important in practice, and quite deep theoretically, see e.g. <a href=\"http://nematilab.info/bmijc/assets/091218_paper.pdf\" class=\"lipdf\">the reusable holdout by Dwork et al</a>. A related paper that I liked a lot is this <a href=\"https://arxiv.org/abs/1502.04585\" class=\"liinternal\">ICML 2015 paper</a> by Blum and Hardt, which essentially explores the regularization effect of publishing only models that beat the state of the art significantly (more generally this is an extremely interesting question, of why we can keep using the same datasets to evaluate progress in machine learning, see this provokingly titled paper &#8220;<a href=\"https://arxiv.org/abs/1902.10811\" class=\"liinternal\">Do ImageNet Classifiers Generalize to ImageNet?</a>&#8220;).</li>\n<li>A general trend has been in finding very fast (nearly linear time) method for many classical problems. Sometimes these investigations even led to actually practical algorithm, as with this now classical paper by Marco Cuturi at NIPS 2013 titled &#8220;<a href=\"http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport\" class=\"liexternal\">Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a>&#8220;.</li>\n</ol>\n<h2>Oh, one last thing</h2>\n<p>I also heard that, surprisingly, gradient descent can work to optimize highly non-convex functions such as training loss for neural networks. Not sure what this is about, it&#8217;s a pretty obscure topic, maybe it will catch up in the decade 2020-2029&#8230;</p>\n<h2>Share some more in the comments!</h2>\n<p>The above is only a tiny sample, there were many many more interesting directions being explored (tensor methods for latent variable models <a href=\"http://www.jmlr.org/papers/volume15/anandkumar14b/anandkumar14b.pdf\" class=\"lipdf\">[Anandkumar, Ge, Hsu, Kakade, Telgarsky, JMLR 14]</a>; phenomenon of &#8220;all local minima are good&#8221; for various non-convex learning problems, see e.g., <a href=\"http://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum\" class=\"liexternal\">[Ge, Lee, Ma, NIPS 2016]</a>; etc etc). Feel free to share your favorite ML theory paper in the comments!</p>\n",
  "wfw:commentRss": "https://blogs.princeton.edu/imabandit/2019/12/30/a-decade-of-fun-and-learning/feed/",
  "slash:comments": 54
}