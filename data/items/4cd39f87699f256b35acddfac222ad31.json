{
  "title": "Physically Adversarial Attacks and Defenses in Computer Vision: A Survey. (arXiv:2211.01671v1 [cs.CV])",
  "link": "http://arxiv.org/abs/2211.01671",
  "description": "<p>Although Deep Neural Networks (DNNs) have been widely applied in various\nreal-world scenarios, they are vulnerable to adversarial examples. The current\nadversarial attacks in computer vision can be divided into digital attacks and\nphysical attacks according to their different attack forms. Compared with\ndigital attacks, which generate perturbations in the digital pixels, physical\nattacks are more practical in the real world. Owing to the serious security\nproblem caused by physically adversarial examples, many works have been\nproposed to evaluate the physically adversarial robustness of DNNs in the past\nyears. In this paper, we summarize a survey versus the current physically\nadversarial attacks and physically adversarial defenses in computer vision. To\nestablish a taxonomy, we organize the current physical attacks from attack\ntasks, attack forms, and attack methods, respectively. Thus, readers can have a\nsystematic knowledge about this topic from different aspects. For the physical\ndefenses, we establish the taxonomy from pre-processing, in-processing, and\npost-processing for the DNN models to achieve a full coverage of the\nadversarial defenses. Based on the above survey, we finally discuss the\nchallenges of this research field and further outlook the future direction.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_B/0/1/0/all/0/1\">Bangzheng Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiefan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>"
}