{
  "id": "tag:blogger.com,1999:blog-9149402429183581490.post-3689344049108158170",
  "published": "2015-07-11T19:54:00.000-07:00",
  "updated": "2015-07-11T20:08:13.781-07:00",
  "title": "Power Rankings: Looking at a Very Simple Method",
  "content": "One of the simplest and most common power ranking models is known as the <a href=\"https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model\" target=\"_blank\">Bradley-Terry-Luce model</a>, which is <a href=\"http://angrystatistician.blogspot.com/2013/03/baseball-chess-psychology-and.html\" target=\"_blank\">equivalent to other famous models</a> such the <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\" target=\"_blank\">logistic model</a> and the <a href=\"https://en.wikipedia.org/wiki/Elo_rating_system\" target=\"_blank\">Elo rating system</a>. I'll be referring to \"teams\" here, but of course the same ideas apply to any two-participant game.<br /><br />Let me clarify what I mean when I use the term \"power ranking\". A power ranking supplies not only a ranking of teams, but also provides numbers that may be used to estimate the probabilities of various outcomes were two particular teams to play a match.<br /><br />In the BTL power ranking system we assume the teams have some latent (hidden/unknown) \"strength\" \\(R_i\\), and that the probability of \\(i\\) beating \\(j\\) is \\( \\frac{R_i}{R_i+R_j} \\). Note that each \\(R_i\\) is assumed to be strictly positive. Where does this model structure come from?<br /><br />Here are three reasonable constraints for a power ranking model:<br /><ol><li>&nbsp;If \\(R_i\\) and \\(R_j\\) have equal strength, the probability of one beating the other should be \\( \\frac{1}{2}\\).</li><li>As the strength of one team strictly approaches 0 (infinitely weak) with the other team fixed, the probability of the other team winning strictly increases to 1.</li><li>As the strength of one team strictly approaches 1 (infinitely strong) with the other team fixed, the probability of the other team winning strictly decreases to 0.</li></ol><div>Note that our model structure satisfies all three constraints. Can you think of other simple model structures that satisfy all three constraints?</div><div><br /></div><div>Given this model and a set of teams and match results, how can we estimate the \\(R_i\\). The <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood\" target=\"_blank\">maximum-likelihood estimators</a>&nbsp;are the set of \\( R_i \\) that maximizes the probability of the observed outcomes actually happening. For any given match this probability of team \\( i \\) beating team \\( j \\) is&nbsp;\\( \\frac{R_i}{R_i+R_j} \\), so the overall probability of the observed outcomes of the matches \\( M \\) occurring is \\[ \\mathcal{L} = \\prod_{m\\in M} \\frac{R_{w(m)}}{R_{w(m)}+R_{l(m)}},\\] where \\( w(m) \\) is then winner and \\( l(m) \\) the loser of match \\( m \\). We can transform this into a sum by taking logarithms; \\[ \\log\\left( \\mathcal{L} \\right) = \\log\\left(R_{w(m)}\\right) - \\log\\left(R_{w(m)}+R_{l(m)}\\right).\\] Before going further, let's make a useful reparameterization by setting \\( e^{r_i} = R_i \\); this makes sense as we're requiring the \\( R_i \\) to be strictly positive. We then get \\[&nbsp;\\log\\left( \\mathcal{L} \\right) = r_{w(m)} - \\log\\left(e^{r_{w(m)}}+e^{r_{l(m)}}\\right).\\] Taking partial derivatives we get \\begin{eqnarray*}<br />\\frac{\\partial \\log\\left( \\mathcal{L} \\right)}{\\partial r_i} &amp;=&amp; \\sum_{w(m)=i} 1 - \\frac{e^{r_{w(m)}}}{e^{r_{w(m)}}+e^{r_{l(m)}}} + \\sum_{l(m)=i} - \\frac{e^{r_{l(m)}}}{e^{r_{w(m)}}+e^{r_{l(m)}}}\\\\<br />&amp;=&amp; \\sum_{w(m)=i} 1 - \\frac{e^{r_i}}{e^{r_i}+e^{r_{l(m)}}} + \\sum_{l(m)=i} - \\frac{e^{r_i}}{e^{r_{w(m)}}+e^{r_i}}\\\\<br />&amp;=&amp;0.<br />\\end{eqnarray*} But this is just the number of actual wins minus the expected wins! Thus, the maximum likelihood estimators for the \\( r_i \\) satisfy \\( O_i = E_i \\) for all teams \\( i \\), where \\( O_i \\) is the actual (observed) number of wins for team \\( i \\), and \\( E_i \\) is the expected number of wins for team \\( i \\) based on our model. That's a nice property!<br /><br />If you'd like to experiment with some actual data, and to see that the resulting fit does indeed satisfy this property, here's an <a href=\"https://github.com/octonion/hockey/tree/master/lunchtime\" target=\"_blank\">example BTL model using NCAA men's ice hockey scores</a>. You can, of course, actually use this property to iteratively solve for the MLE estimators \\( R_i \\). Note that you'll have to fix one of the \\( R_i \\) to be a particular value (or add some other constraint), as the model probabilities are invariant with respect to multiplication of the \\( R_i \\) by the same positive scalar.</div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Christopher D. Long",
    "uri": "http://www.blogger.com/profile/13687149457345266350",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 0
}