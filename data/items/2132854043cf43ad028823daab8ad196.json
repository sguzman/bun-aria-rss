{
  "title": "Apache Arrow and the \"10 Things I Hate About pandas\"",
  "link": "",
  "published": "2017-09-21T00:00:00-07:00",
  "updated": "2017-09-21T00:00:00-07:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2017-09-21:/blog/apache-arrow-pandas-internals/",
  "summary": "<blockquote>\n<p>This post is the first of many to come on Apache Arrow, pandas, pandas2, and\n  the general trajectory of my work in recent times and into the foreseeable\n  future. This is a bit of a read and overall fairly technical, but if\n  interested I  encourage you to take the time â€¦</p></blockquote>",
  "content": "<blockquote>\n<p>This post is the first of many to come on Apache Arrow, pandas, pandas2, and\n  the general trajectory of my work in recent times and into the foreseeable\n  future. This is a bit of a read and overall fairly technical, but if\n  interested I  encourage you to take the time to work through it.</p>\n</blockquote>\n<p>In this post I hope to explain as concisely as I can some of the key problems\nwith pandas's internals and how I've been steadily planning and building\npragmatic, working solutions for them. To the outside eye, the projects I've\ninvested in may seem only tangentially-related: e.g. pandas, Badger, Ibis,\nArrow, Feather, Parquet. Quite the contrary, they are all closely-interrelated\ncomponents of a continuous arc of work I started almost 10 years ago.</p>\n<blockquote>\n<p>Side note: consider making a <a href=\"http://pandas.pydata.org/donate.html\">tax-deductible donation</a> to support pandas\n  development</p>\n</blockquote>\n<h2>Some background</h2>\n<p>I started building pandas in April, 2008. It started out as a skunkworks that I\ndeveloped mostly on my nights and weekends. I didn't know much about software\nengineering or even how to use Python's scientific computing stack well back\nthen. My code was ugly and slow. I figured things out as I went and learned as\nmuch from others as I could. I didn't start doing serious C development until\n2013 and C++ development until 2015. I appreciate C++ a lot more now than I\nwould have 9 years ago.</p>\n<p>Python was a comparatively more inhospitable place for what we might now call\ndata science development. The problems that pandas solves for people in 2017\nwere not problems that people generally solved with Python at all. They\ngenerally used R, SAS, SPSS, Stata, or MATLAB, in no particular order of\npreference.</p>\n<p>So maybe it's not a surprise that pandas's internal architecture has some\nwarts. In Summer 2011, I devised a contraption known as the <code>BlockManager</code>, a\nmemory management object that uses NumPy arrays internally, for managing the\ninternal columns of data inside a <code>pandas.DataFrame</code>. You can see me <a href=\"http://wesmckinney.com/blog/a-roadmap-for-rich-scientific-data-structures-in-python/\">writing\nabout it</a> all the way back in July 2011.</p>\n<p>While the BlockManager and pandas's overall tight internal coupling to NumPy\nhas served the project well historically, these things are some of the root\ncauses of problems that plague pandas users working with larger datasets in\nmodern times.</p>\n<p>To put it simply, we weren't thinking about analyzing 100 GB or 1 TB datasets\nin 2011. Nowadays, <strong>my rule of thumb for pandas is that you should have 5 to\n10 times as much RAM as the size of your dataset</strong>. So if you have a 10 GB\ndataset, you should really have about 64, preferably 128 GB of RAM if you want\nto avoid memory management problems. This comes as a shock to users who expect\nto be able to analyze datasets that are within a factor of 2 or 3 the size of\ntheir computer's RAM.</p>\n<blockquote>\n<p>pandas rule of thumb: have 5 to 10 times as much RAM as the size of your\n  dataset</p>\n</blockquote>\n<p>There are additional, hidden memory killers in the project, like the way that\nwe use Python objects (like strings) for many internal details, so it's not\nunusual to see a dataset that is 5GB on disk take up 20GB or more in\nmemory. It's an overall bad situation for large datasets.</p>\n<h2>DataPad, Badger, and my time at Cloudera</h2>\n<p>I started DataPad in 2013 with <a href=\"https://github.com/changhiskhan\">Chang She</a>, my longtime friend and pandas\ncollaborator. We wanted to use the nascent PyData stack to power the visual\nanalytics application we were building, but we ran into some serious\nperformance issues, especially in the cloud. The responsiveness of analytics\nqueries from the DataPad application weren't great with pandas out of the\nbox.</p>\n<p>So I pared down the pandas feature set to the bare essentials and created a\nsmall new implementation which we called Badger. I found that through using\ncontiguous, immutable columnar data structures optimized for data locality,\nthat I could get 2-20x better performance in a wide variety of operations. The\nbiggest wins were in string processing, but there were huge gains across the\nboard. You can see a <a href=\"https://www.youtube.com/watch?v=wPEmoT018s8\">demo of DataPad here</a>.</p>\n<p>Badger was definitely \"startup code\". When we were acquired by Cloudera in\n2014, I contemplated open sourcing Badger, but felt that it would be a lot of\nwork to clean up the code (mostly written in C, with far too many macros) for\nhuman consumption and I wanted to build a more future-proof implementation that\nwould still be useful 10 years down the road. Releasing it as-is would have\nbeen distracting for pandas users, and I didn't want to keep developing that\ncodebase. It's not a good idea to release codebases only to abandon them. In\nlight of the fact that basically a rewrite was needed, I left Badger on the\nshelf.</p>\n<p>I gave a <a href=\"https://www.slideshare.net/wesm/practical-medium-data-analytics-with-python\">talk in November 2013</a> with the subtitle <em>10 Things I Hate About\nPandas</em>, which has had almost 100,000 slide views 4 years later. It's a summary\nof the things that I'd learned throughout 2013 and battle scars from the first\n5 years of pandas development.</p>\n<p>The 10 (really 11) things are (paraphrasing my own words):</p>\n<ol>\n<li><strong>Internals too far from \"the metal\"</strong></li>\n<li><strong>No support for memory-mapped datasets</strong></li>\n<li><strong>Poor performance in database and file ingest / export</strong></li>\n<li><strong>Warty missing data support</strong></li>\n<li><strong>Lack of transparency into memory use, RAM management</strong></li>\n<li><strong>Weak support for categorical data</strong></li>\n<li><strong>Complex groupby operations awkward and slow</strong></li>\n<li><strong>Appending data to a DataFrame tedious and very costly</strong></li>\n<li><strong>Limited, non-extensible type metadata</strong></li>\n<li><strong>Eager evaluation model, no query planning</strong></li>\n<li><strong>\"Slow\", limited multicore algorithms for large datasets</strong></li>\n</ol>\n<p>I had begun to solve some of these problems in Badger, but the solutions were\nnarrow in scope to the problems we were solving at DataPad. Luckily, I moved to\nCloudera where there were a lot of database and big data system developers for\nme to learn from.</p>\n<p>At Cloudera, I started looking at Impala, Kudu, Spark, Parquet, and other such\nbig data storage and analysis systems. Since Python and pandas had never been\ninvolved with any of these projects, building integrations with them was\ndifficult. The single biggest problem was data interchange, particularly\n<strong>moving large tabular datasets from one process's memory space to\nanother's</strong>. It was extremely expensive, and there was no standard solution for\ndoing it. RPC-oriented serialization protocols like Thrift and Protocol Buffers\nwere too slow and too general purpose.</p>\n<p>As I dug through the different points of contact between different systems, I\nsaw a lot of commonality with the problems I'd been working on above in\nBadger. <strong>Zero-copy data access</strong> was the biggest thing; you need to be able to\nmemory map complex tables to make accessing 1 terabyte of data on disk as fast\nand easy as 1 megabyte.</p>\n<p>By early 2015, I was yearning for what I was then calling a <strong>\"columnar data\nmiddleware\"</strong> which provided zero-copy access, with rich enough support for\nstrings, nested types, and all the other hairy JSON-like data found in the\nwild. Like the prototype Badger runtime, this format needed to be optimized for\ndata locality so that we could evaluate queries at maximum speeds.</p>\n<p>I was lucky to bump into a collection of like-minded people across many big\ndata projects, especially folks from Apache Drill, Impala, Kudu, Spark, and\nothers. In late 2015, to create a neutral \"safe space\" free from software\nvendor affiliation (which can make industry collaborations more complex), we\nworked with the Apache Software Foundation to establish <a href=\"http://arrow.apache.org/\">Apache Arrow</a>.</p>\n<p>On paper, Apache Arrow was everything I had been wanting for years. But, in\nlate 2015, all I had (as far as Python is concerned) were some Markdown\nspecification documents. These specifications weren't even final; we set up the\nApache project to create a venue for the broader community to have a dialogue\nabout the specs and the problems that Arrow solves. We had to buckle down and\nbuild real software to make the vision real and useful. Now that I've been\nworking on the project for almost 2 years, we've made huge progress in\nrealizing the things that we set out to accomplish.</p>\n<p><strong>I strongly feel that Arrow is a key technology for the next generation of\ndata science tools</strong>. I laid out my vision for this recently in my <a href=\"https://www.youtube.com/watch?v=wdmf1msbtVs\">JupyterCon\nkeynote</a>.</p>\n<p>Also in late 2015, I wrote a <a href=\"https://pandas-dev.github.io/pandas2/\">long set of design documents</a> to start\ndiscussions about building a faster, cleaner core pandas implementation, which\nwe may call <strong>pandas2</strong>. pandas is a community project that <a href=\"https://github.com/pandas-dev/pandas-governance\">governs itself\nbased on consensus</a> (with me as the BDFL to break impasses). I wanted to\nsee if the rest of the core developers agreed with my assessment of what is\nwrong with pandas's internals. It's been 2 years since then, and by and large\nthere has been general agreement on the problems, but how to solve them all\nwithout disrupting the existing pandas user community is an open question. Over\nthis time I have focused on building computational infrastructure that will\nlargely go unseen by pandas users.</p>\n<h2>Does Arrow solve the \"10 Things\"?</h2>\n<p>Arrow doesn't solve all of the 10 things quite yet, but it's made huge strides\ntoward doing so.</p>\n<p>Arrow's C++ implementation provides essential in-memory analytics\ninfrastructure for projects like pandas:</p>\n<ul>\n<li>A runtime column-oriented memory format optimized for analytical processing\n  performance</li>\n<li>A zero-copy, streaming / chunk-oriented data layer designed for <a href=\"http://wesmckinney.com/blog/arrow-streaming-columnar/\">moving and\n  accessing large datasets at maximum speeds</a></li>\n<li>Extensible type metadata for describing a wide variety of flat and nested\n  data types occurring in real-world systems, with support for user-defined\n  types</li>\n</ul>\n<p>What's missing from the Arrow C++ project at the moment (but not for too much\nlonger) is:</p>\n<ul>\n<li>A comprehensive analytical function \"kernel\" library</li>\n<li>Logical operator graphs for graph dataflow-style execution (think TensorFlow\n  or PyTorch, but for data frames)</li>\n<li>A multicore schedular for parallel evaluation of operator graphs</li>\n</ul>\n<p>I'll write more about the roadmap for building an analytics engine for Arrow\nmemory (that we can use in projects like pandas) in a follow up post.</p>\n<p>In the rest of this post, I'm going to go deeper into the \"10 Things\" and how\nthey're addressed by the Arrow project.</p>\n<h3>1. Getting closer to the metal</h3>\n<p>All memory in Arrow on a per column basis, whether strings, numbers, or nested\ntypes, is arranged in contiguous memory buffers optimized for random access\n(single values) and scan (multiple values next to each other) performance. The\nidea is that you want to minimize CPU or GPU cache misses when looping over the\ndata in a table column, even with strings or other non-numeric types.</p>\n<p>In pandas, an array of strings is an array of <code>PyObject</code> pointers, and the\nactual string data lives inside <code>PyBytes</code> or <code>PyUnicode</code> structs that live all\nover the process heap. As developers, we are hamstrung by the bloated,\nmemory-bound nature of processing these objects. In Python, the simple string\n<code>'wes'</code> occupies 52 bytes of memory. <code>''</code> occupies 49 bytes. For a great\ndiscussion of issues around this, see <a href=\"https://github.com/jakevdp\">Jake Vanderplas's</a> epic exposÃ© on\n<a href=\"http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/\">Why Python is Slow</a>.</p>\n<p>In Arrow, each string is right next to the previous one in memory, so you can\nscan all of the data in a column of strings without any cache\nmisses. Processing contiguous bytes right against the metal, guaranteed.</p>\n<p>Arrow's C/C++ API means that applications which know nothing about Python can\nconsume or produce pristine Arrow tables and share them either in-process or\nvia shared memory / memory maps. pandas's lack of a C or Cython API for data\nframes has been another big problem over time.</p>\n<h3>2. Memory mapping huge datasets</h3>\n<p>Perhaps the single biggest memory management problem with pandas is the\nrequirement that data must be loaded completely into RAM to be\nprocessed. pandas's internal BlockManager is far too complicated to be usable\nin any practical memory-mapping setting, so you are performing an unavoidable\nconversion-and-copy anytime you create a <code>pandas.DataFrame</code>.</p>\n<p>Arrow serialization design provides a <a href=\"https://github.com/apache/arrow/blob/master/format/Message.fbs#L44\">\"data header\"</a> which describes the\nexact locations and sizes of all the memory buffers for all the columns in a\ntable. This means you can memory map huge, bigger-than-RAM datasets and\nevaluate pandas-style algorithms on them in-place without loading them into\nmemory like you have to with pandas now. You could read 1 megabyte from the\nmiddle of a 1 terabyte table, and you only pay the cost of performing those\nrandom reads totalling 1 megabyte. With modern solid state drives, this is\ngenerally a good strategy.</p>\n<p>Arrow's memory-mapping capability also allows multiple processes to work with\nthe same large dataset without moving it or copying it in any way. We've seen\nthis applied to great effect in the <a href=\"http://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/\">Plasma Object Store</a> (now part of\nArrow) used in the <a href=\"https://github.com/ray-project/ray\">Ray project</a> at UC Berkeley.</p>\n<h3>3. High speed data ingest and export (databases and file formats)</h3>\n<p>Arrow's efficient memory layout and rich type metadata make it an ideal\ncontainer for inbound data from databases and columnar storage formats like\nApache Parquet.</p>\n<p>One of Arrow's primitive constructs is the concept of a <a href=\"http://wesmckinney.com/blog/arrow-streaming-columnar/\">\"record batch\nstream\"</a>, a sequence of atomic tables together comprising a large\ndataset. This stream processing data model is an idea for databases which serve\nstreams of records from a database cursor.</p>\n<p>We have been developing <a href=\"http://wesmckinney.com/blog/python-parquet-update/\">a high-speed connector with Parquet format</a>. We've\nalso seen the optimized <a href=\"http://arrow.apache.org/blog/2017/06/16/turbodbc-arrow/\">turbodbc</a> project for ODBC-based database\nconnections.</p>\n<p>I aspire to build Arrow-native connectors for many other file formats and\ndatabases, such as:</p>\n<ul>\n<li>SQLite</li>\n<li>PostgreSQL</li>\n<li>Apache Avro</li>\n<li>Apache ORC</li>\n<li>CSV (a better version of <code>pandas.read_csv</code>)</li>\n<li>JSON</li>\n</ul>\n<h3>4. Doing missing data right</h3>\n<p>All missing data in Arrow is represented as a packed bit array, separate from\nthe rest of the data. This makes missing data handling simple and consistent\nacross all data types. You can also do analytics on the null bits (AND-ing\nbitmaps, or counting set bits) using fast bit-wise built-in hardware operators\nand SIMD.</p>\n<p>The null count in an array is also explicitly stored in its metadata, so if\ndata does not have nulls, we can choose faster code paths that skip null\nchecking. With pandas, we cannot assume that arrays do not have null sentinel\nvalues and so most analytics has extra null checking which hurts\nperformance. If you have no nulls, you don't even need to allocate the bit\narray.</p>\n<p>Because missing data is not natively supported in NumPy, over time we have had\nto implement our own null-friendly versions of most key performance-critical\nalgorithms. It would be better to have null-handling built into all algorithms\nand memory management from the ground up.</p>\n<h3>5. Keeping memory allocations in check</h3>\n<p>In pandas, all memory is owned either by NumPy or the Python interpreter, and\nit can be difficult to measure exactly how much memory is used by a given\n<code>pandas.DataFrame</code>. It's not unusual for a line of code to double or triple the\nmemory footprint of a process due to temporary allocations, sometimes causing a\n<code>MemoryError.</code></p>\n<p>In Arrow's C++ implementation, all memory allocations are carefully tracked in\na central \"memory pool\", so you know exactly how much Arrow memory is in RAM at\nany given time. By using \"subpools\" with parent-child relationships, you can\nprecisely measure the \"high water mark\" in algorithms to understand the peak\nmemory usage of analytical operations. This technique is common in databases to\nmonitor or limit memory usage in operator evaluation. If you know that you are\ngoing to exceed available RAM, you can apply mitigation strategies like\nspilling to disk (where the ability to memory-map on-disk datasets is of course\nkey).</p>\n<p>In Arrow memory is either immutable or copy-on-write. At any given time, you\nknow if another array references a buffer that you can see. This enables us to\navoid defensive copying.</p>\n<h3>6. Supporting categorical data well</h3>\n<p>When I gave my talk in 2013, pandas did not have the <code>pandas.Categorical</code> type;\nthat was implemented afterwards. But pandas's workarounds for data types not in\nNumPy has always been a bit warty. If you step outside pandas, you can't work\nwith pandas Categoricals. The way that extension dtypes are implemented works,\nbut is a bit bolted-on due to pandas's tight coupling to NumPy.</p>\n<p>In Arrow, categorical data is a first-class citizen, and we have prioritized\nhaving an efficient and consistent representation both in-memory and on the\nwire or in shared memory. We support sharing categories (called <em>dictionaries</em>\nin Arrow) between multiple arrays.</p>\n<p>pandas has other user-defined types: datetime with time zone and periods. We\nintend to be able to support logical data types (having a particular physical\nmemory representation) in Arrow gracefully so that a particular system can\nfaithfully transport its data using Arrow without having to make changes to the\nArrow format documents.</p>\n<h3>7. Better groupby(...).apply operations</h3>\n<p>The way that Arrow helps is by enabling easier parallelization of <code>groupby</code>\noperations; due to other problems listed here, it is difficult or impossible to\nfully parallelize a <code>df.groupby(...).apply(f)</code> operation.</p>\n<p>At some point, we will also want to improve the API for complex apply\noperations in pandas.</p>\n<h3>8. Appending to data frames</h3>\n<p>In pandas, all of the data in a column in a DataFrame must reside in the same\nNumPy array. This is a restrictive requirement, and frequently results in\nmemory-doubling and additional computation to concatenate Series and DataFrame\nobjects.</p>\n<p>Table columns in Arrow C++ can be chunked, so that appending to a table is a\nzero copy operation, requiring no non-trivial computation or memory\nallocation. By designing up front for streaming, chunked tables, appending to\nexisting in-memory tabler is computationally inexpensive relative to pandas\nnow. Designing for chunked or streaming data is also essential for implementing\nout-of-core algorithms, so we are also laying the foundation for processing\nlarger-than-memory datasets.</p>\n<h3>9. Adding new data types</h3>\n<p>There are multiple layers of complexity to adding new data types:</p>\n<ul>\n<li>Adding new metadata</li>\n<li>Creating dynamic dispatch rules to operator implementations in analytics</li>\n<li>Preserving metadata through operations</li>\n</ul>\n<p>For example, a \"currency\" type could have a currently type a string, with the\ndata physically represented as a <code>float64</code> or <code>decimal</code>. So you could treat the\ncurrency computationally like its numeric representation, but then carry\nthrough the currency metadata in numeric operations.</p>\n<p>The rules about preserving metadata may be operator-dependent, so it can get\ncomplicated.</p>\n<p>In Arrow we have decoupled the metadata representation from the details of\ncomputation and metadata nannying. In the C++ implementation, we have been\nplanning ahead for user-defined types, so when we are focusing more on building\nan analytics engine it is a goal to enable the creation of user-defined\noperator dispatch and metadata promotion rules.</p>\n<h3>10/11. Query planning, multicore execution</h3>\n<p>When you write <code>df[df.c &lt; 0].d.sum()</code>, pandas creates a temporary DataFrame\n<code>df[df.c &lt; 0]</code> then sums the <code>d</code> column of that temporary object. If <code>df</code>\ncontains a lot of columns, this is ridiculously wasteful. Of course you can\nwrite <code>df.d[df.c &lt; 0].sum()</code>, but even that produces a temporary Series, which\nis then summed!</p>\n<p>Clearly, if you know the whole expression you are evaluating you can do better\nand avoid these temporary allocations altogether. Additionally, many algorithms\n(including this example) can be parallelized amongst all the processors cores on\nyour computer.</p>\n<p>As part of building an analytics engine for Arrow, we also plan to build a\nlightweight physical \"query planner\" with a multicore in-process scheduler to\nenable many kinds of algorithms to be parallelized and evaluated\nefficiently. There is substantial prior art in the domain of graph data flow\nexecution (particularly in the ML world lately, like TensorFlow and PyTorch),\nso this amounts to creating a graph data flow engine whose primitive unit of\ndata is an Arrow table.</p>\n<p>To plan ahead for this use case, in 2015, I started the <a href=\"https://github.com/ibis-project/ibis\">Ibis project</a>\n(still under active development) to create a pandas-friendly deferred\nexpression system for static analysis and compilation these types of\noperations. Since an efficient multithreaded in-memory engine for pandas was\nnot available when I started Ibis, I instead focused on building compilers for\nSQL engines (Impala, PostgreSQL, SQLite), similar to the R <a href=\"https://github.com/tidyverse/dplyr\">dplyr\npackage</a>. <a href=\"https://github.com/cpcloud\">Phillip Cloud</a> from the pandas core team has been actively\nworking on Ibis with me for quite a long time.</p>\n<h2>What's next?</h2>\n<p>In an upcoming blog post, I will go into some more detail about the roadmap for\nbuilding an Arrow-native multithreaded in-memory execution engine and how\nthat's relevant to the architecture of pandas2.</p>\n<h2>Addendum: On Dask</h2>\n<p>Nowadays, a lot of people ask me about <a href=\"http://dask.pydata.org\">Dask</a> (and Spark, and other such\nprojects) and how it is helping with pandas performance and scalability. It is\ndefinitely helping in various ways, such as:</p>\n<ul>\n<li>Splitting up large datasets into pieces and working with them in separate\n  threads or separate processes</li>\n<li>Evicting pandas data from RAM that is no longer needed</li>\n</ul>\n<p>Dask makes it easy to read a directory of CSV files by running\n<code>pandas.read_csv</code> in parallel and then running a groupby operation on the\nentire dataset. Truly, what <a href=\"https://github.com/mrocklin\">Matt Rocklin</a> and team have built is an\nexcellent piece of kit.</p>\n<p>One issue with the Dask model is that it's using pandas as a black\nbox. <code>dask.dataframe</code> does not solve pandas's inherent performance and memory\nuse problems, but it spreads them out across multiple processes and helps\nmitigate them by being careful to not work with too large pieces of data all at\nonce, which can result in an unpleasant <code>MemoryError</code>.</p>\n<p>Some problems don't fit the Dask partition-parallel distributed task execution\nmodel. Also, pandas's memory management and IO challenges make Dask jobs a lot\nslower than they could be with a more efficient in-memory runtime.</p>"
}