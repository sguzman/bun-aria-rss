{
  "title": "74 Summaries of Machine Learning and NLP Research",
  "link": "https://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/",
  "comments": "https://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/#comments",
  "dc:creator": "Marek",
  "pubDate": "Tue, 12 Nov 2019 18:53:53 +0000",
  "category": "Uncategorized",
  "guid": "http://www.marekrei.com/blog/?p=1430",
  "description": "<p>My previous post on summarising 57 research papers turned out to be quite useful for people working in this field, so it is about time&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/\">74 Summaries of Machine Learning and NLP Research</a> appeared first on <a rel=\"nofollow\" href=\"https://www.marekrei.com/blog\">Marek Rei</a>.</p>\n",
  "content:encoded": "<p>My previous post on <a href=\"http://www.marekrei.com/blog/paper-summaries/\">summarising 57 research papers</a> turned out to be quite useful for people working in this field, so it is about time for a sequel.</p>\n<p>Below you will find short summaries of a number of different research papers published in the areas of Machine Learning and Natural Language Processing in the past couple of years (2017-2019). They cover a wide range of different topics, authors and venues. These are not meant to be reviews showing my subjective opinion, but instead I aim to provide a blunt and concise overview of the core contribution of each publication. </p>\n<p>Given how many papers are published in our area every year, it is getting more and more difficult to keep track of all of them. The goal of this post is to save some time for both new and experienced readers in the field and allow them to get a quick overview of 74 research papers in about 30 minutes reading time.</p>\n<p>I set out to post 60 summaries (up from 50 compared to last time). At the end, I also include the summaries for my own published papers since the last iteration (papers 61-74).</p>\n<p>Here we go.</p>\n<h4>1. Improving Language Understanding by Generative Pre-Training</h4>\n<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. OpenAI. 2018.<br />\n<a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p>\n<p>A transformer architecture that is trained as a language model on a large corpus, then fine-tuned for individual text classification and similarity tasks. Multiple sentences are combined together into a single sequence using delimiters in order to work with the same model. Reporting high results on entailment, question answering and semantic similarity tasks. </p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt-1024x507.png\" alt=\"\" width=\"843\" height=\"417\" class=\"aligncenter size-large wp-image-1546\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt-1024x507.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt-150x74.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt-300x149.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt-768x381.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gpt.png 1326w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h4>\n<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. Google. NAACL 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/N19-1423.pdf\">https://www.aclweb.org/anthology/N19-1423.pdf</a></p>\n<p>A bidirectional transformer architecture for pre-training language representations. The model is optimized on unlabaled data by 1) predicting masked words in the input sequence, and 2) predicting whether the input sequences occur together. The parameters can then be fine-tuned for a specific task, such as classifying sentences, sentence pairs, or tokens.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/bert.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/bert-1024x434.png\" alt=\"\" width=\"843\" height=\"357\" class=\"aligncenter size-large wp-image-1526\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/bert-1024x434.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bert-150x64.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bert-300x127.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bert-768x325.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bert.png 1514w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>3. LXMERT: Learning Cross-Modality Encoder Representations from Transformers</h4>\n<p>Hao Tan, Mohit Bansal. UNC. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1908.07490.pdf\">https://arxiv.org/pdf/1908.07490.pdf</a></p>\n<p>Building a cross-modal pre-trained model for both vision and language. Both images and text are encoded and attended over jointly with a cross-modal encoder, the model is then optimized with both unimodal and multimodal tasks (masked LM, image classification, image-caption matching, visual QA).<br />\nThe model achieves new state-of-the-art on several VQA datasets.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert-1024x413.png\" alt=\"\" width=\"843\" height=\"340\" class=\"aligncenter size-large wp-image-1557\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert-1024x413.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert-150x61.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert-300x121.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert-768x310.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lxmert.png 1501w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<p><span id=\"more-1430\"></span></p>\n<h4>4. Language GANs Falling Short</h4>\n<p>Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin.  Montréal, McGill, Facebook, Google. ArXiv 2018.<br />\n<a href=\"https://arxiv.org/pdf/1811.02549.pdf\">https://arxiv.org/pdf/1811.02549.pdf</a></p>\n<p>Compares different text-based GANs on language generation and finds that MLE language models still outperform all of them. The tuning of a temperature parameter on the output distribution is proposed for controlling the balance between quality and diversity of the generation. The models are evaluated at different values of this parameter space, revealing the different operating areas of each architecture.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gans.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gans-1024x511.png\" alt=\"\" width=\"843\" height=\"421\" class=\"aligncenter size-large wp-image-1528\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/gans-1024x511.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gans-150x75.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gans-300x150.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gans-768x383.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gans.png 1152w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>5. Unsupervised Recurrent Neural Network Grammars</h4>\n<p>Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, Gábor Melis. Harvard, Oxford, DeepMind. NAACL 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/N19-1114.pdf\">https://www.aclweb.org/anthology/N19-1114.pdf</a></p>\n<p>Extending recurrent neural network grammars to the unsupervised setting, discovering constituency parses only from plain text. They train jointly 1) a generative network that sequentially generates both a binary tree structure and the words in the leaves, and 2) an inference network that generates a tree conditioned on the whole sentence. The generative part is then evaluated as a language model, while the inference network is evaluated as an unsupervised unlabeled constituency parser.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/rnng.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/rnng-300x241.png\" alt=\"\" width=\"300\" height=\"241\" class=\"aligncenter size-medium wp-image-1553\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/rnng-300x241.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/rnng-150x120.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/rnng.png 695w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>6. InferLite: Simple Universal Sentence Representations from Natural Language Inference Data</h4>\n<p>Jamie Kiros, William Chan. Toronto. EMNLP 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/D18-1524.pdf\">https://www.aclweb.org/anthology/D18-1524.pdf</a></p>\n<p>The paper describes a method for learning general-purpose sentence representations. Word embeddings from different sources are mapped to a new space and combined with attention. An optional convolution module is also used. A sentence representation is calculated using a max-pooling operation over word representations. The model is optimised using NLI data. Evaluation is performed on a number of sentence classification and similarity datasets.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/inferlite.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/inferlite-275x300.png\" alt=\"\" width=\"275\" height=\"300\" class=\"aligncenter size-medium wp-image-1467\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/inferlite-275x300.png 275w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/inferlite-137x150.png 137w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/inferlite.png 684w\" sizes=\"(max-width: 275px) 100vw, 275px\" /></a></p>\n<h4>7. Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task</h4>\n<p>Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, Kenneth Heafield. Microsoft, Edinburgh. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-1055.pdf\">https://www.aclweb.org/anthology/N18-1055.pdf</a></p>\n<p>The paper describes a series of data processing and model architecture choices that improve the results of a neural MT model for error correction.<br />\nThese include separating words into sub-tokens, sampling strategies, a custom version of dropout, tied embeddings, weighting edits higher during optimisation, pre-training the model and the embeddings, and combination with a language model.<br />\nEvaluations on CoNLL 2014 and JFLEG show a considerable improvement over previous best results of neural models, making this work comparable to state-of-the art on error correction.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gec.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gec.png\" alt=\"\" width=\"982\" height=\"649\" class=\"aligncenter size-full wp-image-1440\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/gec.png 982w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gec-150x99.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gec-300x198.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gec-768x508.png 768w\" sizes=\"(max-width: 982px) 100vw, 982px\" /></a></p>\n<h4>8. Attention Is All You Need</h4>\n<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Google, Toronto. NIPS 2017.<br />\n<a href=\"https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>\n<p>Replacing RNNs in a seq2seq model with multi-layer attention mechanisms. Also using multi-head attention, where 8 different attention modules are used in parallel and the result is concatenated. They achieve good results on English-French and English-German translation.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/attention.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/attention.png\" alt=\"\" width=\"755\" height=\"734\" class=\"aligncenter size-full wp-image-1442\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/attention.png 755w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/attention-150x146.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/attention-300x292.png 300w\" sizes=\"(max-width: 755px) 100vw, 755px\" /></a></p>\n<h4>9. Deep Contextualized Word Representations</h4>\n<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Allen Institute, Washington. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-1202.pdf\">https://www.aclweb.org/anthology/N18-1202.pdf</a></p>\n<p>A bidirectional LSTM is trained as a language model and its hidden layers are used to augment word embeddings for other models. Representations from different layers of the fixed LM are combined together, then concatenated with a regular word embedding and fed into a downstream model. They show improvements on a range of tasks, including QA; NER and SNLI.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1-1024x422.png\" alt=\"\" width=\"843\" height=\"347\" class=\"aligncenter size-large wp-image-1451\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1-1024x422.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1-150x62.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1-300x124.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1-768x317.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/elmo-1.png 1382w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>10. Long Short-Term Memory with Dynamic Skip Connections</h4>\n<p>Tao Gui, Qi Zhang, Lujun Zhao, Yaosong Lin, Minlong Peng, Jingjing Gong, Xuanjing Huang. Fudan. AAAI 2019.<br />\n<a href=\"https://aaai.org/ojs/index.php/AAAI/article/view/4613/4491\">https://aaai.org/ojs/index.php/AAAI/article/view/4613/4491</a></p>\n<p>The paper describes an extension to LSTMs for sequence labeling, where the model is able to incorporate the state from one previous step directly into the current step.<br />\nThis is achieved by a separate module that predicts one of K possible steps that will be chosen for inclusion. As the discrete steps are non-differentiable, the model is trained through reinforcement learning, optimising the token-level loss.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/connections.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/connections-1024x375.png\" alt=\"\" width=\"843\" height=\"309\" class=\"aligncenter size-large wp-image-1444\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/connections-1024x375.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/connections-150x55.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/connections-300x110.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/connections-768x282.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/connections.png 1271w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>11. Learning Distributed Event Representations with a Multi-Task Approach</h4>\n<p>Xudong Hong, Asad Sayeed, Vera Demberg. Saarland, Gothenburg. *SEM 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/S18-2002.pdf\">https://www.aclweb.org/anthology/S18-2002.pdf</a></p>\n<p>The paper describes a neural model for predicting arguments and relation types, based on other arguments and relations describing the same event.<br />\nThe input words and relations are encoded into a vector representation, then used to predict both the missing word (conditioned on the relation) and the missing relation type (conditioned on the word).<br />\nSeveral different configurations are explored, using different composition methods and making use of residual connections.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/events.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/events-1024x455.png\" alt=\"\" width=\"843\" height=\"375\" class=\"aligncenter size-large wp-image-1445\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/events-1024x455.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/events-150x67.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/events-300x133.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/events-768x341.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/events.png 1227w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>12. Combining Abstractness and Language-specific Theoretical Indicators for Detecting Non-Literal Usage of Estonian Particle Verbs</h4>\n<p>Eleri Aedmaa, Maximilian Köper, Sabine Schulte im Walde. Tartu, Stuttgart. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-4002.pdf\">https://www.aclweb.org/anthology/N18-4002.pdf</a></p>\n<p>The paper describes an approach for detecting non-literal usage of particle verbs in Estonian. The authors construct a new dataset of 1490 sentences and annotate it for metaphorical or literal meaning. They then construct a range of features and train a random forest classifier to detect these metaphorical uses. In the process, an automatically generated dataset of abstractness scores for 243,675 Estonian lemmas is created.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian-1024x601.png\" alt=\"\" width=\"843\" height=\"495\" class=\"aligncenter size-large wp-image-1446\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian-1024x601.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian-150x88.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian-300x176.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian-768x451.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/estonian.png 1135w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>13. Generating Token-Level Explanations for Natural Language Inference</h4>\n<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal. Cambridge, Amazon. NAACL 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/N19-1101.pdf\">https://www.aclweb.org/anthology/N19-1101.pdf</a></p>\n<p>Constructing a system for NLI that explains its decisions by pointing to the most relevant parts of the input. The architecture is based on self-attention, using the attention weights as token-level decisions. While the result is not as accurate as LIME, it is about 6000 times faster.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel-1024x444.png\" alt=\"\" width=\"843\" height=\"366\" class=\"aligncenter size-large wp-image-1566\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel-1024x444.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel-150x65.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel-300x130.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel-768x333.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/tokenlevel.png 1505w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>14. Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</h4>\n<p>Liyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu, Jian Peng, Jiawei Han. Illinois, SoCal. EMNLP 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/D18-1153.pdf\">https://www.aclweb.org/anthology/D18-1153.pdf</a></p>\n<p>The paper describes a method for training and pruning a language model, which can be integrated with a sequence labeler. The model is a multi-layer LSTM, with direct connections between all possible layers. The pruning process involves removing some LSTM layers from the model.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/contextualized.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/contextualized-273x300.png\" alt=\"\" width=\"273\" height=\"300\" class=\"aligncenter size-medium wp-image-1449\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/contextualized-273x300.png 273w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/contextualized-136x150.png 136w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/contextualized.png 568w\" sizes=\"(max-width: 273px) 100vw, 273px\" /></a></p>\n<h4>15. Meaningless yet meaningful: Morphology grounded subword-level NMT</h4>\n<p>Tamali Banerjee, Pushpak Bhattacharyya. Bombay. SCLEM 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-1207.pdf\">https://www.aclweb.org/anthology/W18-1207.pdf</a></p>\n<p>The paper proposes using two subword creation systems in a pipeline, instead of either of them separately. Morphessor break words into linguistically-motivated morphs, whereas Byte Pair Encoding uses co-occurrence statistics to combine smaller units into larger subwords. The paper proposes processing the text first with Morphessor, then with BPE to get the benefit of both approaches. The subword creation process is evaluated on the task of translation.</p>\n<h4>16. Who is Killed by Police: Introducing Supervised Attention for Hierarchical LSTMs</h4>\n<p>Minh Nguyen, Thien Huu Nguyen. Hanoi, Montreal, Oregon. COLING 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/C18-1193.pdf\">https://www.aclweb.org/anthology/C18-1193.pdf</a></p>\n<p>The paper describes a model for classifying collections of sentences based on whether they refer to a person who has been killed by police. The system uses a two-level LSTM architecture for composing representations of sentences and full texts. The authors investigate two extensions for directly applying supervision to the attention weights. </p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/killed.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/killed-1024x382.png\" alt=\"\" width=\"843\" height=\"314\" class=\"aligncenter size-large wp-image-1456\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/killed-1024x382.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/killed-150x56.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/killed-300x112.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/killed-768x287.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/killed.png 1344w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>17. Corpus Specificity in LSA and Word2vec: The Role of Out-of-Domain Documents</h4>\n<p>Edgar Altszyler, Mariano Sigman, Diego Fernández Slezak. Conicet. RepL4NLP 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-3001.pdf\">https://www.aclweb.org/anthology/W18-3001.pdf</a></p>\n<p>The paper investigates the impact of corpus size on the word embedding quality, when using LSA and word2vec. It concludes that word2vec consistently benefits from more data, whereas LDA benefits from data that is more topic-relevant.</p>\n<h4>18. Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification</h4>\n<p>Mingkuan Liu, Musen Wen, Selcuk Kopru, Xianjing Liu, Alan Lu. eBay. DeepLo 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-3409.pdf\">https://www.aclweb.org/anthology/W18-3409.pdf</a></p>\n<p>The paper describes a semi-supervised setup for text classification, assuming a small amount of annotated data and a large amount of unlabeled data. Two models are trained in tandem: a regular text classifier and a model that predicts whether the first model has made a mistake. The classifier is then used to predict labels for the unannotated examples and the second model is used to select examples that get included into the training data.</p>\n<h4>19. Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus</h4>\n<p>Fariz Ikhwantri, Samuel Louvan, Kemal Kurniawan, Bagas Abisena, Valdi Rachman, Alfan Farizki Wicaksono, Rahmad Mahendra. Jakarta, Trento, Depok. DeepLo 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-3406.pdf\">https://www.aclweb.org/anthology/W18-3406.pdf</a></p>\n<p>The paper describes a sequence tagging model for semantic role labeling. The model uses multi-task learning, through the task of entity recognition, also combined with active learning. It is evaluated on a new SRL dataset in Indonesian which will be made available.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lowsrl.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lowsrl-300x277.png\" alt=\"\" width=\"300\" height=\"277\" class=\"aligncenter size-medium wp-image-1461\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/lowsrl-300x277.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lowsrl-150x138.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lowsrl.png 696w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>20. Learning with Latent Language</h4>\n<p>Jacob Andreas, Dan Klein, Sergey Levine. Berkeley. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-1197.pdf\">https://www.aclweb.org/anthology/N18-1197.pdf</a></p>\n<p>The paper aims to show the benefit of using natural language strings as a parameter space. During training time, the model is trained to generate natural language descriptions of the task. During testing, these descriptions are geenrated automatically based on a small number of examples and the end performance on solving the task is evaluated.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/latentlang.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/latentlang-279x300.png\" alt=\"\" width=\"279\" height=\"300\" class=\"aligncenter size-medium wp-image-1463\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/latentlang-279x300.png 279w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/latentlang-139x150.png 139w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/latentlang.png 556w\" sizes=\"(max-width: 279px) 100vw, 279px\" /></a></p>\n<h4>21. Robust Incremental Neural Semantic Graph Parsing</h4>\n<p>Jan Buys, Phil Blunsom. Oxford, Deepmind. ACL 2017.<br />\n<a href=\"https://www.aclweb.org/anthology/P17-1112.pdf\">https://www.aclweb.org/anthology/P17-1112.pdf</a></p>\n<p>The paper describes a neural semantic graph parser. They combine an encoder-decoder architecture and a transition pased parser, with modifications to cover graphs as opposed to trees. Evaluation is done on Minimal Recursion Semantics (MRS) and Abstract Meaning Representation (AMR).</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/graphparsing.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/graphparsing-242x300.png\" alt=\"\" width=\"242\" height=\"300\" class=\"aligncenter size-medium wp-image-1465\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/graphparsing-242x300.png 242w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/graphparsing-121x150.png 121w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/graphparsing.png 572w\" sizes=\"(max-width: 242px) 100vw, 242px\" /></a></p>\n<h4>22. Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge</h4>\n<p>Pasquale Minervini, Sebastian Riedel. UCL. CoNLL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/K18-1007.pdf\">https://www.aclweb.org/anthology/K18-1007.pdf</a></p>\n<p>The paper describes a method for introducing adversarial examples into the process of training an NLI system. Five logical rules are listed, based on the definition of entailment. For example, a sentence should entail itself and contradiction should be symmetrical. Artificial adversarial sentences are then generated, by making changes to existing sentences in the dataset and making sure that the model gets a low score on these examples.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli-1024x433.png\" alt=\"\" width=\"843\" height=\"356\" class=\"aligncenter size-large wp-image-1470\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli-1024x433.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli-150x63.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli-300x127.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli-768x325.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/advnli.png 1483w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>23. CAM: A Combined Attention Model for Natural Language Inference</h4>\n<p>Amit Gajbhiye, Sardar Jaf, Noura Al Moubayed, Steven Bradley, A. Stephen McGough. Durham, Newcastle. Big Data 2018.<br />\n<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622057\">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622057</a></p>\n<p>The paper describes an architecture for the natural language inference (NLI) task where two levels of attention are combined. First, intra-attention where each sentence itself is used to attend to each of the words, then inter-attention where the attention weights are calculated based on the other sentence (premise vs hypothesis). Evaluation is performed on the SNLI and SciTail datasets.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/cam.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/cam-1024x488.png\" alt=\"\" width=\"843\" height=\"402\" class=\"aligncenter size-large wp-image-1472\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/cam-1024x488.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cam-150x71.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cam-300x143.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cam-768x366.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cam.png 1365w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>24. Modeling Student Response Times: Towards Efficient One-on-one Tutoring Dialogues</h4>\n<p>Luciana Benotti, Jayadev Bhaskaran, Sigtryggur Kjartansson, David Lang. Cordoba, Stanford. W-NUT 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-6117.pdf\">https://www.aclweb.org/anthology/W18-6117.pdf</a></p>\n<p>Predicting how long a student will take to answer a question from a tutor. They construct a new dataset of 18K tutoring session dialogues, covering math, chemistry and physics. Several baselines are reported, starting with a basic RNN and adding task-specific features.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/responsetimes-1.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/responsetimes-1-268x300.png\" alt=\"\" width=\"268\" height=\"300\" class=\"aligncenter size-medium wp-image-1476\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/responsetimes-1-268x300.png 268w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/responsetimes-1-134x150.png 134w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/responsetimes-1.png 627w\" sizes=\"(max-width: 268px) 100vw, 268px\" /></a></p>\n<h4>25. Retrieve and Re-rank: A Simple and Effective IR Approach to Simple Question Answering over Knowledge Graphs</h4>\n<p>Vishal Gupta, Manoj Chinnakotla, Manish Shrivastava. Hyderabad, Microsoft. FEVER 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-5504.pdf\">https://www.aclweb.org/anthology/W18-5504.pdf</a></p>\n<p>The paper presents a neural model for question answering over knowledge graphs. The question and a tuple of the possible answer relation are mapped to a score and used to rerank the output of a simple term-based IR system. State-of-the-art results on the SimpleQuestions dataset are reported.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve-1024x560.png\" alt=\"\" width=\"843\" height=\"461\" class=\"aligncenter size-large wp-image-1478\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve-1024x560.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve-150x82.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve-300x164.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve-768x420.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/retrieve.png 1163w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>26. Dynamic Meta-Embeddings for Improved Sentence Representations</h4>\n<p>Douwe Kiela, Changhan Wang, Kyunghyun Cho. Facebook, NYU, CIFAR. EMNLP 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/D18-1176.pdf\">https://www.aclweb.org/anthology/D18-1176.pdf</a></p>\n<p>An architecture for using multiple types of pre-trained word embeddings in the same model. The different embeddings are mapped to a joint space and combined using attention, conditioned on the context. Evaluation is performed on NLI, sentiment detection adn image caption retrieval.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings-1024x387.png\" alt=\"\" width=\"843\" height=\"319\" class=\"aligncenter size-large wp-image-1480\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings-1024x387.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings-150x57.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings-300x113.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings-768x290.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/metaembeddings.png 1336w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>27. Regularizing and Optimizing LSTM Language Models</h4>\n<p>Stephen Merity, Nitish Shirish Keskar, Richard Socher. Salesforce. ICLR 2018.<br />\n<a href=\"https://openreview.net/pdf?id=SyyGPP0TZ\">https://openreview.net/pdf?id=SyyGPP0TZ</a></p>\n<p>A series of modifications to a basic LSTM language model to achieve state-of-the-art performance. Dropout on hidden connection weights, optimizing with averaged SGD, variable-length backpropagation, variational dropout, dropping out whole words, tying embedding and output weights, regularization of hidden state activations, and a cache model. Evaluation on PTB and WikiText-2 achieves very low perplexities.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising.png\" alt=\"\" width=\"818\" height=\"609\" class=\"aligncenter size-full wp-image-1481\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising.png 818w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising-150x112.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising-300x223.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/regularising-768x572.png 768w\" sizes=\"(max-width: 818px) 100vw, 818px\" /></a></p>\n<h4>28. Universal Language Model Fine-tuning for Text Classification</h4>\n<p>Jeremy Howard, Sebastian Ruder. fast.ai, San Francisco, Insight, Aylien. ACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/P18-1031.pdf\">https://www.aclweb.org/anthology/P18-1031.pdf</a></p>\n<p>Pre-training a language model on large amounts of plain text, then fine-tuning it to work as a text classifier for a specific dataset. Strategies for controlling the learning rate and unfreezing different layers are introduced in order to prevent catastrophic forgetting. Achieves state-of-the art results on several datasets for sentiment analysis, question classification and topic classification.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit-1024x590.png\" alt=\"\" width=\"843\" height=\"486\" class=\"aligncenter size-large wp-image-1483\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit-1024x590.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit-150x86.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit-300x173.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit-768x443.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ulmfit.png 1253w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>29. Improving Neural Language Models with a Continuous Cache</h4>\n<p>Edouard Grave, Armand Joulin, Nicolas Usunier. Facebook. ICLR 2017.<br />\n<a href=\"https://openreview.net/pdf?id=B184E5qee\">https://openreview.net/pdf?id=B184E5qee</a></p>\n<p>A method for augmenting trained neural language models with a continuous cache system, biasing the model towards words that it has recently seen in the text. The hidden states from a number of previous timesteps are recorded, along with the actual next word after it is observed. At test time, a dot product between the current hidden state and all the recorded ones is used to calculate a second probability distribution for next word prediction, which is then interpolated with the regular language model.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache-1024x415.png\" alt=\"\" width=\"843\" height=\"342\" class=\"aligncenter size-large wp-image-1485\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache-1024x415.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache-150x61.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache-300x122.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache-768x311.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/ccache.png 1293w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>30. Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks</h4>\n<p>Awni Y. Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey H. Tison, Codie Bourn, Mintu P. Turakhia, Andrew Y. Ng. Stanford, IRhythm. ArXiv 2017.<br />\n<a href=\"https://arxiv.org/pdf/1707.01836.pdf\">https://arxiv.org/pdf/1707.01836.pdf</a></p>\n<p>The paper describes a deep convolutional model for classifying cardiac arrhythmias based on ECG signals. They collect a large dataset of 30-second ECG recordings annotated for 12 different arrhythmias; unfortunately the data is not made public.<br />\nThe model achieves better F1 score on most of the label, compared to individual human annotators.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/cardio.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/cardio-300x291.png\" alt=\"\" width=\"300\" height=\"291\" class=\"aligncenter size-medium wp-image-1488\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/cardio-300x291.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cardio-150x145.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/cardio.png 659w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>31. Explainable Prediction of Medical Codes from Clinical Text</h4>\n<p>James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein. Georgia Tech. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-1100.pdf\">https://www.aclweb.org/anthology/N18-1100.pdf</a></p>\n<p>A neural model for classifying the text in patient discharge summaries with one of 8,922 medical codes. The architecture uses a convolutional network over words, followed by an attention layer, where the attention weights are also used for providing an interpretaion of the classification. While training, the model is regularised to have similar model parameters for medical codes that have similar descriptions.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes-1024x632.png\" alt=\"\" width=\"843\" height=\"520\" class=\"aligncenter size-large wp-image-1492\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes-1024x632.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes-150x93.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes-300x185.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes-768x474.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/medicalcodes.png 1230w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>32. Learning Patient Representations from Text</h4>\n<p>Dmitriy Dligach, Timothy Miller. Loyola, Boston, Harvard. *SEM 2018.<br />\nhttps://www.aclweb.org/anthology/S18-2014.pdf</p>\n<p>A model for learning to encode patient information into a useful vector representation. First, a text classifier is trained to take patient records as input and predict the diagnostic billing codes. This encoder is then applied on a different dataset for classifying discharge records for obesity and related conditions.</p>\n<h4>33. On the Limitations of Unsupervised Bilingual Dictionary Induction</h4>\n<p>Anders Søgaard, Sebastian Ruder, Ivan Vulić. Copenhagen, Insight, Aylien, Cambridge. ACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/P18-1072.pdf\">https://www.aclweb.org/anthology/P18-1072.pdf</a></p>\n<p>Investigating under which conditions bilingual dictionary induction actually works. They find that it requires 1) data from a comparable domain, 2) with embeddings trained using the same hyper-parameters, 3) using large monolingual corpora, and 4) only works on languages that use similar marking and typing. They also investigate using some supervision in the form of identically spelled words, which provides much more robust results.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/limitations.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/limitations-227x300.png\" alt=\"\" width=\"227\" height=\"300\" class=\"aligncenter size-medium wp-image-1495\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/limitations-227x300.png 227w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/limitations-113x150.png 113w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/limitations.png 535w\" sizes=\"(max-width: 227px) 100vw, 227px\" /></a></p>\n<h4>34. Phrase-Based &#038; Neural Unsupervised Machine Translation</h4>\n<p>Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc’Aurelio Ranzato. Facebook, Le Mans, Sorbonne. EMNLP 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/D18-1549.pdf\">https://www.aclweb.org/anthology/D18-1549.pdf</a></p>\n<p>A more streamlined version of previous unsupervised machine translation work. The framework starts with cross-lingual embeddings (which are inferred without parallel data) and uses language modeling with iterative back-translation in order to train and improve the translation model. Both SMT and NMT are investigated, with SMT actually giving better results.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/umt.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/umt-1024x532.png\" alt=\"\" width=\"843\" height=\"438\" class=\"aligncenter size-large wp-image-1497\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/umt-1024x532.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/umt-150x78.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/umt-300x156.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/umt-768x399.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/umt.png 1212w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>35. Deep Image Prior</h4>\n<p>Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky. Yandex, Oxford, Skoltech. CVPR 2018.<br />\n<a href=\"https://arxiv.org/pdf/1711.10925.pdf\">https://arxiv.org/pdf/1711.10925.pdf</a></p>\n<p>A network is trained to take a random vector as input and produce one specific image as output. As it learns to reconstruct this image, it can produce an &#8220;improved&#8221; version of that image, provided that it does not overtrain. The method is evaluated on tasks such as denoising, image superv-resolution and inpainting.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior-1024x235.png\" alt=\"\" width=\"843\" height=\"193\" class=\"aligncenter size-large wp-image-1499\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior-1024x235.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior-150x34.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior-300x69.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior-768x177.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/imageprior.png 1448w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>36. Enriching Word Vectors with Subword Information</h4>\n<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Facebook. TACL 2017.<br />\n<a href=\"https://www.aclweb.org/anthology/Q17-1010.pdf\">https://www.aclweb.org/anthology/Q17-1010.pdf</a></p>\n<p>The skip-gram model for word embeddings is extended with character-level information. Individual word embeddings are constructed by adding together embeddings for individual character n-grams, along with the main word embedding. They show good performance on word similarity and language modeling, particularly for morphologically rich languages or rare words.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching-1024x456.png\" alt=\"\" width=\"843\" height=\"375\" class=\"aligncenter size-large wp-image-1504\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching-1024x456.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching-150x67.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching-300x134.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching-768x342.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/enriching.png 1281w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>37. Distributional Modeling on a Diet: One-shot Word Learning from Text Only</h4>\n<p>Su Wang, Stephen Roller, Katrin Erk. UTexas. IJCNLP 2017.<br />\n<a href=\"https://www.aclweb.org/anthology/I17-1021.pdf\">https://www.aclweb.org/anthology/I17-1021.pdf</a></p>\n<p>Bayesian models for learning to predict properties of new words, after having observed them only once. The model is trained using concrete nouns and their lists of known properties, and then learns latent topics that can generate both word contexts (eg dependency relations) and the properties. They evaluate as a ranking problem on datasets with annotated properties.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/diet.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/diet-300x279.png\" alt=\"\" width=\"300\" height=\"279\" class=\"aligncenter size-medium wp-image-1506\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/diet-300x279.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/diet-150x139.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/diet.png 719w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>38. Differentiable plasticity: training plastic neural networks with backpropagation</h4>\n<p>Thomas Miconi, Jeff Clune, Kenneth O. Stanley. Uber. ICML 2018.<br />\n<a href=\"http://proceedings.mlr.press/v80/miconi18a/miconi18a.pdf\">http://proceedings.mlr.press/v80/miconi18a/miconi18a.pdf</a></p>\n<p>Proposes a neural connection that in addition to regular learnable weights includes plasticity. The activation of a neuron depends on the Hebbian trace of the previous activations, while the internal weight for this component is also learned. Improves performance at tasks that test memorization of patterns and images.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/plasticity.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/plasticity-300x269.png\" alt=\"\" width=\"300\" height=\"269\" class=\"aligncenter size-medium wp-image-1508\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/plasticity-300x269.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/plasticity-150x135.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/plasticity.png 738w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>39. Neural Character-based Composition Models for Abuse Detection</h4>\n<p>Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova. Cambridge, Amsterdam. ALW2 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-5101.pdf\">https://www.aclweb.org/anthology/W18-5101.pdf</a></p>\n<p>Investigation of character-based neural architectures for detecting abusive language. They evaluate 10 different model variants on Twitter and Wikipedia datasets. The proposed model predicts word embeddings for unseen and obfuscated words based on character-level input from the word and its context.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse-1024x592.png\" alt=\"\" width=\"843\" height=\"487\" class=\"aligncenter size-large wp-image-1510\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse-1024x592.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse-150x87.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse-300x173.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse-768x444.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/abuse.png 1164w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>40. Breaking the Activation Function Bottleneck through Adaptive Parameterization</h4>\n<p>Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot. Manchester, Turing. NeurIPS 2018.<br />\n<a href=\"https://papers.nips.cc/paper/8000-breaking-the-activation-function-bottleneck-through-adaptive-parameterization.pdf\">https://papers.nips.cc/paper/8000-breaking-the-activation-function-bottleneck-through-adaptive-parameterization.pdf</a></p>\n<p>Proposing an adaptive feed-forward layer module. Regular activation functions are replaced with parameterized versions that are calculated based on the input. Demonstrating improvement on MNIST and language modeling.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck-1024x485.png\" alt=\"\" width=\"843\" height=\"399\" class=\"aligncenter size-large wp-image-1512\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck-1024x485.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck-150x71.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck-300x142.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck-768x364.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/bottleneck.png 1336w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>41. Towards Robust Interpretability with Self-Explaining Neural Networks</h4>\n<p>David Alvarez-Melis, Tommi S. Jaakkola. MIT. NeurIPS 2018.<br />\n<a href=\"https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf\">https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf</a></p>\n<p>Defines three desiderata for interpretable machine learning models: easy to understand, faithful to model decision, and locally stable around a given datapoint. Based on these properties, a model class is proposed that takes them into account. An explicit regularization term is introduced, training the model to have stable outputs around a specific input.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining-1024x645.png\" alt=\"\" width=\"843\" height=\"531\" class=\"aligncenter size-large wp-image-1514\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining-1024x645.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining-150x94.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining-300x189.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining-768x484.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/selfexplaining.png 1070w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>42. Axiomatic attribution for deep networks</h4>\n<p>Mukund Sundararajan, Ankur Taly, Qiqi Yan,<br />\n<a href=\"https://arxiv.org/pdf/1703.01365.pdf\">https://arxiv.org/pdf/1703.01365.pdf</a></p>\n<p>Proposes a novel method for attributing network predictions to individual input features, called Integrated Gradients. The gradient values are measured at various points along a path, starting with a neutral input (e.g. a black image) and moving towards a real input vector. These are then summed and multiplied by the difference in feature values, giving the attribution weight for a each specific feature position.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic-300x213.png\" alt=\"\" width=\"300\" height=\"213\" class=\"aligncenter size-medium wp-image-1516\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic-300x213.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic-150x106.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic-768x544.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/axiomatic.png 779w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>43. Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning</h4>\n<p>Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, Jeff Clune. Uber. Deep RL 2018.<br />\n<a href=\"https://arxiv.org/pdf/1712.06567.pdf\">https://arxiv.org/pdf/1712.06567.pdf</a></p>\n<p>Experiments with a genetic algorithm for training neural networks to play Atari games. A deep network with 4M+ parameters is mutated by adding random noise to the parameters, the resulting networks are evaluated and 1000 best configurations are kept for the next iteration.<br />\nThe genetic algorithm achieves higher or comparable results to Q-learning and policy gradients on several games.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/evolution.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/evolution-231x300.png\" alt=\"\" width=\"231\" height=\"300\" class=\"aligncenter size-medium wp-image-1518\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/evolution-231x300.png 231w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/evolution-115x150.png 115w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/evolution.png 478w\" sizes=\"(max-width: 231px) 100vw, 231px\" /></a></p>\n<h4>44. Multi-Task Learning as Multi-Objective Optimization</h4>\n<p>Ozan Sener, Vladlen Koltun. Intel. NeurIPS 2018.<br />\n<a href=\"https://arxiv.org/pdf/1810.04650.pdf\">https://arxiv.org/pdf/1810.04650.pdf</a></p>\n<p>A method for using adaptive weights for different loss functions when optimizing multiple objectives. After calculating the gradients, a separate function is minimized in order to find better weights. The method is shown to give a Pareto optimal solution and improves performance on several multi-task vision datasets.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto-1024x598.png\" alt=\"\" width=\"843\" height=\"492\" class=\"aligncenter size-large wp-image-1522\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto-1024x598.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto-150x88.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto-300x175.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto-768x448.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/pareto.png 1095w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>45. Predictive Uncertainty Estimation via Prior Networks</h4>\n<p>Andrey Malinin, Mark Gales. Cambridge. NeurIPS 2018.<br />\n<a href=\"https://arxiv.org/pdf/1802.10501.pdf\">https://arxiv.org/pdf/1802.10501.pdf</a></p>\n<p>A network that is able to quantify how uncertain it is when classifying specific datapoints. The model is constructed to parameterize a Dirichlet distribution, and it is directly optimized to predict a flat distribution for out-of-distribution examples. Evaluation is performed on the tasks of detecting misclassifications and out-of-domain examples.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty-1024x414.png\" alt=\"\" width=\"843\" height=\"341\" class=\"aligncenter size-large wp-image-1524\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty-1024x414.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty-150x61.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty-300x121.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty-768x310.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/uncertainty.png 1181w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>46. Co-Attention Based Neural Network for Source-Dependent Essay Scoring</h4>\n<p>Haoran Zhang, Diane Litman. Pittsburgh. BEA 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/W18-0549v2.pdf\">https://www.aclweb.org/anthology/W18-0549v2.pdf</a></p>\n<p>The paper presents a neural architecture for scoring essays that are written based on a source article. Both the article and the resulting essay are encoded into sentence representations, an attention mechanism is applied to these sentences in both directions, and a final essay score is predicted for an article-essay pair. Evaluation is performed on the subsets of datasets that have reference articles available.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention-1024x579.png\" alt=\"\" width=\"843\" height=\"477\" class=\"aligncenter size-large wp-image-1447\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention-1024x579.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention-150x85.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention-300x170.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention-768x434.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/coattention.png 1198w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>47. Relational recurrent neural networks</h4>\n<p>Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap. DeepMind, CoMPLEX. NeurIPS 2018.<br />\n<a href=\"https://arxiv.org/pdf/1806.01822.pdf\">https://arxiv.org/pdf/1806.01822.pdf</a></p>\n<p>Introduces the Relational Memory Core, an architecture for storing and modifying distributed memory cells in RNNs. At each step, multi-head self-attention is performed over all the memory cells, along with the input at the current time step, producing an updated versions of the full memory. The model is applied to Pacman, language modeling, program evaluation and a synthetic task for finding the Nth farthest input vector.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/relational-1.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/relational-1.png\" alt=\"\" width=\"738\" height=\"402\" class=\"aligncenter size-full wp-image-1532\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/relational-1.png 738w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/relational-1-150x82.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/relational-1-300x163.png 300w\" sizes=\"(max-width: 738px) 100vw, 738px\" /></a></p>\n<h4>48. Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging</h4>\n<p>Lifu Huang, Heng Ji, Jonathan May. Rensselaer, USC. NAACL 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/N19-1383.pdf\">https://www.aclweb.org/anthology/N19-1383.pdf</a></p>\n<p>The paper describes a model for multilingual named entity recognition, where language amounts of annotated data in a source language is used to improve performance in the target language. First, monolingual word embeddings are mapped into the same space using an adversarial objective. These are then given to a Bi-LSTM that predicts NER labels, while also being trained with the adversarial language detection objective. </p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual-1024x461.png\" alt=\"\" width=\"843\" height=\"380\" class=\"aligncenter size-large wp-image-1533\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual-1024x461.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual-150x68.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual-300x135.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual-768x346.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/crosslingual.png 1361w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>49. Understanding Black-box Predictions via Influence Functions</h4>\n<p>Pang Wei Koh, Percy Liang. Stanford. ICML 2017.<br />\n<a href=\"http://proceedings.mlr.press/v70/koh17a/koh17a.pdf\">http://proceedings.mlr.press/v70/koh17a/koh17a.pdf</a></p>\n<p>Proposing the use of influence functions for interpreting the predictions of deep neural networks. The method calculates the gradient with respect to specific training images, thereby pointing out which training points are most responsible for a prediction on a particular test instance. They also describe a method of using this gradient to create imperceptible adversarial modifications in the training set, such that the model makes a mistake on a particular test image.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/blackbox.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/blackbox-266x300.png\" alt=\"\" width=\"266\" height=\"300\" class=\"aligncenter size-medium wp-image-1535\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/blackbox-266x300.png 266w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/blackbox-133x150.png 133w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/blackbox.png 587w\" sizes=\"(max-width: 266px) 100vw, 266px\" /></a></p>\n<h4>50. Sentence Simplification with Deep Reinforcement Learning</h4>\n<p>Xingxing Zhang, Mirella Lapata. Edinburgh. EMNLP 2017.<br />\n<a href=\"https://www.aclweb.org/anthology/D17-1062.pdf\">https://www.aclweb.org/anthology/D17-1062.pdf</a></p>\n<p>Trains a sequence-to-sequence model for sentence simplification using reinforcement learning. The loss combines metrics for simplicity, relevance and fluency, and is non-differentiable, which is why the REINFORCE algorithm is used for training. They evaluate on several simplification datasets and make their code available, along with the system output.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification-1024x452.png\" alt=\"\" width=\"843\" height=\"372\" class=\"aligncenter size-large wp-image-1534\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification-1024x452.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification-150x66.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification-300x133.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification-768x339.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/simplification.png 1211w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>51. Insertion Transformer: Flexible Sequence Generation via Insertion Operations</h4>\n<p>Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit. Google, Berkeley. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1902.03249.pdf\">https://arxiv.org/pdf/1902.03249.pdf</a></p>\n<p>The paper describes a modification of the transformer architecture for generation, where tokens can be generated out of order.<br />\nAt each step, the transformer takes a partial sequence as input and predicts words that should be inserted into that sequence, along with the positions of where they should be inserted.<br />\nThis process is iteratively repeated until end tokens are generated, either for each slot or for the whole sequence.<br />\nThe model is evaluated on the WMT 2014 English-German dataset, with the proposed architecture performing comparably to the left-to-right transformer and the blockwise parallel model.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion-1024x315.png\" alt=\"\" width=\"843\" height=\"259\" class=\"aligncenter size-large wp-image-1538\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion-1024x315.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion-150x46.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion-300x92.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion-768x237.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/insertion.png 1224w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>52. Area Attention</h4>\n<p>Yang Li, Lukasz Kaiser, Samy Bengio, Si Si. Google. ICML 2019.<br />\n<a href=\"http://proceedings.mlr.press/v97/li19e/li19e.pdf\">http://proceedings.mlr.press/v97/li19e/li19e.pdf</a></p>\n<p>The paper describes a modification to attention where the model is able to attend to collections of items in addition to individual items.<br />\nThe keys and values of nearby items are combined in a grid, in order to create these new collections, which are then included as choices in the regular attention mechanism.<br />\nEvaluation is performed on machine translation and image captioning datasets, by modifying the existing transformer or LSTM architectures with this type of attention.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/areaattention.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/areaattention-247x300.png\" alt=\"\" width=\"247\" height=\"300\" class=\"aligncenter size-medium wp-image-1540\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/areaattention-247x300.png 247w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/areaattention-124x150.png 124w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/areaattention.png 472w\" sizes=\"(max-width: 247px) 100vw, 247px\" /></a></p>\n<h4>53. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement</h4>\n<p>Nina Poerner, Hinrich Schütze, Benjamin Roth. Munich. ACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/P18-1032.pdf\">https://www.aclweb.org/anthology/P18-1032.pdf</a></p>\n<p>Proposes two artificial tasks in order to evaluate different methods of explaining the decisions of neural networks in text classification. Also proposes a new explanation method based on LIME that is more suited to text classification that requires context. Performs an evaluation across a number of different classifier architectures and explanation methods.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation-1024x287.png\" alt=\"\" width=\"843\" height=\"236\" class=\"aligncenter size-large wp-image-1542\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation-1024x287.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation-150x42.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation-300x84.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation-768x215.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/explanation.png 1425w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>54. Corpora Generation for Grammatical Error Correction</h4>\n<p>Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki Parmar, Simon Tong. Google. NAACL 2019.<br />\n<a href=\"https://arxiv.org/pdf/1904.05780.pdf\">https://arxiv.org/pdf/1904.05780.pdf</a></p>\n<p>The paper investigates methods for generating parallel data for grammatical error correction systems. The methods include extracting examples from Wikipedia edit histories, backtranslation through a different language, randomly inserting spelling errors and inserting frequently occurring errors. Iterative correction, repeatedly passing the output through the correction system, is also shown to help.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora-1024x413.png\" alt=\"\" width=\"843\" height=\"340\" class=\"aligncenter size-large wp-image-1545\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora-1024x413.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora-150x61.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora-300x121.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora-768x310.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/geccorpora.png 1507w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>55. A single-layer RNN can approximate stacked and bidirectional RNNs, and topologies in between</h4>\n<p>Javier S. Turek, Shailee Jain, Mihai Capota, Alexander G. Huth, Theodore L. Willke. Intel, UTexas. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1909.00021.pdf\">https://arxiv.org/pdf/1909.00021.pdf</a></p>\n<p>The paper investigates how a single-layer RNN or LSTM can approximate multi-layer and bi-directional RNN/LSTM architectures.<br />\nThis is achieved through delaying the output from the RNN, which allows the model to replace the depth in layers with depth in time steps.<br />\nThe bidirectionality aspect can also be approximated, if the delay is large enough and the RNN is exposed to the necessary number of input steps before starting to produce output.<br />\nEvaluation is performed on two synthetic datasets and on the task of part-of-speech tagging.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate-1024x499.png\" alt=\"\" width=\"843\" height=\"411\" class=\"aligncenter size-large wp-image-1550\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate-1024x499.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate-150x73.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate-300x146.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate-768x375.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/approximate.png 1345w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>56. Language Models as Knowledge Bases?</h4>\n<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel. Facebook, UCL. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1909.01066.pdf\">https://arxiv.org/pdf/1909.01066.pdf</a></p>\n<p>Investigates the use of language models for question answering. The questions are formulated as fill-in-the-blank cloze sentences and the language model has to fill in the answer.<br />\nThey find that BERT-large is surprisingly competitive against supervised knowledge bases and relation extractors, although the performance does depend on the type of question.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa-300x235.png\" alt=\"\" width=\"300\" height=\"235\" class=\"aligncenter size-medium wp-image-1555\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa-300x235.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa-150x117.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa-768x600.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lmqa.png 784w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>57. Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks</h4>\n<p>Brenden Lake, Marco Baroni. Facebook. ArXiv 2017.<br />\n<a href=\"https://openreview.net/pdf?id=H18WqugAb\">https://openreview.net/pdf?id=H18WqugAb</a></p>\n<p>Experiments on synthetic data on the learning abilities of seq2seq models. The results show that the RNN/LSTM models have difficulty in learning to compose, for example based on shorter sequences. Also, their composition skills can be unintuitive, sometimes failing on a simpler case and succeeding on a more difficult one.</p>\n<h4>58. HellaSwag: Can a Machine Really Finish Your Sentence?</h4>\n<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi. Allen School, Allen Institute. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1905.07830.pdf\">https://arxiv.org/pdf/1905.07830.pdf</a></p>\n<p>Creates a dataset for choosing the right last sentence in a text, specifically designed to be difficult for BERT. Performance on the existing SWAG dataset is high even if BERT does not see the context, or when the words in the candidate sentences are shuffled. To make a task where BERT does not perform as well, they take existing paragraphs from ActivityNet and WikiHow, generate candidates from GPT and explicitly exclude those that BERT guesses easily.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag-1024x567.png\" alt=\"\" width=\"843\" height=\"467\" class=\"aligncenter size-large wp-image-1559\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag-1024x567.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag-150x83.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag-300x166.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag-768x425.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/hellaswag.png 1217w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>59. Topic Modeling in Embedding Spaces</h4>\n<p>Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei. Columbia, Cambridge. ArXiv 2019.<br />\n<a href=\"https://arxiv.org/pdf/1907.04907.pdf\">https://arxiv.org/pdf/1907.04907.pdf</a></p>\n<p>Extending Latent Dirichlet Allocation with distributed embeddings of words and topics. The probability of a word belonging to a particular topic is calculated as the dot product of their embeddings, resulting in words and topics being in the same space. Evaluation is performed by measuring topic coherence and topic diversity, finding that the proposed model outperforms LDA and NVDM.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lda.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/lda-1024x408.png\" alt=\"\" width=\"843\" height=\"336\" class=\"aligncenter size-large wp-image-1562\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/lda-1024x408.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lda-150x60.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lda-300x120.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lda-768x306.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/lda.png 1498w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>60. Fluency Boost Learning and Inference for Neural Grammatical Error Correction</h4>\n<p>Tao Ge, Furu Wei, Ming Zhou. Microsoft. ACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/P18-1097.pdf\">https://www.aclweb.org/anthology/P18-1097.pdf</a></p>\n<p>The paper describes a pipeline for grammatical error correction. Artificial error examples are generated through machine translation and filtered using a language model. At inference time, each sentence will be passed through the same system multiple times until the fluency score from a language model no longer improves.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency-1024x570.png\" alt=\"\" width=\"843\" height=\"469\" class=\"aligncenter size-large wp-image-1448\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency-1024x570.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency-150x83.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency-300x167.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency-768x427.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fluency.png 1165w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>61. Neural Multi-task Learning in Automated Assessment</h4>\n<p>Ronan Cummins, Marek Rei. Cambridge. ArXiv 2018.<br />\n<a href=\"https://arxiv.org/pdf/1801.06830.pdf\">https://arxiv.org/pdf/1801.06830.pdf</a></p>\n<p>Investigating a multi-task architecture for essay scoring. While the model learns to predict a score for the overall essay, it is also optimized to perform error detection on the token level. Evaluation on the FCE dataset shows that provides small improvements for the error detection task and substantial improvements for the essay scoring task.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring-300x205.png\" alt=\"\" width=\"300\" height=\"205\" class=\"aligncenter size-medium wp-image-1574\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring-300x205.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring-150x102.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring-768x524.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/multitaskscoring.png 965w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>62. Variable Typing: Assigning Meaning to Variables in Mathematical Text</h4>\n<p>Yiannos Stathopoulos, Simon Baker, Marek Rei, Simone Teufel. Cambridge. NAACL 2018.<br />\n<a href=\"https://www.aclweb.org/anthology/N18-1028.pdf\">https://www.aclweb.org/anthology/N18-1028.pdf</a></p>\n<p>Learning to assign type relations to variables in mathematical text. The target variable is designated in the input and a neural sequence labeling model is used to identify the tokens that refer to its type. Evaluation on a novel dataset shows that the tagging model outperforms a traditional SVM approach and a convnet architecture.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping-1024x385.png\" alt=\"\" width=\"843\" height=\"317\" class=\"aligncenter size-large wp-image-1576\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping-1024x385.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping-150x56.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping-300x113.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping-768x289.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vartyping.png 1335w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>63. Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens</h4>\n<p>Marek Rei, Anders Søgaard. Cambridge, Copenhagen. NAACL 2018.<br />\nhttps://www.aclweb.org/anthology/N18-1027.pdf</p>\n<p>Proposing a sentence classification model that learns to assign labels to individual tokens without any token-level supervision. A modified version of attention and a tied loss function is used to encourage the model to return distrete token labels. Evaluation is performed on hedge detection, error detection and sentiment detection, with the performance being surprisingly competitive to a fully-supervised model.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot-1024x477.png\" alt=\"\" width=\"843\" height=\"393\" class=\"aligncenter size-large wp-image-1578\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot-1024x477.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot-150x70.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot-300x140.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot-768x358.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/zeroshot.png 1487w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>64. Scoring Lexical Entailment with a Supervised Directional Similarity Network</h4>\n<p>Marek Rei, Daniela Gerz, Ivan Vulić. Cambridge. ACL 2018.<br />\nhttps://www.aclweb.org/anthology/P18-2101.pdf</p>\n<p>A supervised network for classifying directional relations between word pairs. The input word embeddings are mutually gated, mapped to a new space, then compared through a dot product in order to predict a score. Achieves state-of-the-art performance on the HyperLex and HypeNet datasets for hyponym detection.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/sdsn.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/sdsn-235x300.png\" alt=\"\" width=\"235\" height=\"300\" class=\"aligncenter size-medium wp-image-1581\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/sdsn-235x300.png 235w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sdsn-117x150.png 117w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sdsn.png 477w\" sizes=\"(max-width: 235px) 100vw, 235px\" /></a></p>\n<h4>65. Sequence Classification with Human Attention</h4>\n<p>Maria Barrett, Joachim Bingel, Nora Hollenstein, Marek Rei, Anders Søgaard. Copenhagen, Zurich, Cambridge. CoNLL 2018.<br />\nhttps://www.aclweb.org/anthology/K18-1030.pdf</p>\n<p>Improving text classification by teaching the model to behave more like a human. An internal attention component is explicitly trained to assign attention weights proportional to the attention of human subjects, based on a gaze recording dataset. Experiments show general improvement on detecting sentiment, grammatical errors and hatespeech.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze-1024x449.png\" alt=\"\" width=\"843\" height=\"370\" class=\"aligncenter size-large wp-image-1583\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze-1024x449.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze-150x66.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze-300x131.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze-768x337.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/gaze.png 1499w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>66. Advance Prediction of Ventricular Tachyarrhythmias using Patient Metadata and Multi-Task Networks</h4>\n<p>Marek Rei, Joshua Oppenheimer, Marek Sirendi. Transformative, Cambridge, Washington. ML4H 2018.<br />\n<a href=\"https://arxiv.org/pdf/1811.12938.pdf\">https://arxiv.org/pdf/1811.12938.pdf</a></p>\n<p>Model architecture for predicting ventricular tachyarrhythmias in advance, based on heartbeat intervals. A number of time series features are given as input to the network, along with patient-level metadata, and the model is optimized to predict VT along with some patient properties. Evaluation on a publicly available dataset shows 77% precision and 71% recall.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/vta.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/vta-1024x401.png\" alt=\"\" width=\"843\" height=\"330\" class=\"aligncenter size-large wp-image-1585\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/vta-1024x401.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vta-150x59.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vta-300x117.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vta-768x301.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/vta.png 1272w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>67. Jointly Learning to Label Sentences and Tokens</h4>\n<p>Marek Rei and Anders Søgaard. Cambridge, Copenhagen. AAAI 2019.<br />\n<a href=\"https://arxiv.org/pdf/1811.05949.pdf\">https://arxiv.org/pdf/1811.05949.pdf</a></p>\n<p>Investigating a model for joint supervision between text classification and sequence labeling. An attention function is used to construct sentence representations from individual words, which is then explicitly supervised with information from token-level labels. Evaluation on hedge detection, error detection and sentiment detection shows substantial improvements for both sentences and tokens.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/jointly.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/jointly-287x300.png\" alt=\"\" width=\"287\" height=\"300\" class=\"aligncenter size-medium wp-image-1587\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/jointly-287x300.png 287w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/jointly-143x150.png 143w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/jointly.png 668w\" sizes=\"(max-width: 287px) 100vw, 287px\" /></a></p>\n<h4>68. A Simple and Robust Approach to Detecting Subject-Verb Agreement Errors</h4>\n<p>Simon Flachs, Ophélie Lacroix, Marek Rei, Helen Yannakoudakis, Anders Søgaard. Siteimprove, Copenhagen, Cambridge. NAACL 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/N19-1251.pdf\">https://www.aclweb.org/anthology/N19-1251.pdf</a></p>\n<p>Investigating systems for detecting subject-verb agreement errors, through the use of artificial data. Additional training examples are generated using a rule-based system, POS-tagging the text and replacing verbs with the incorrect inflections. Different models are compared and a neural sequence labeler trained on the artificial data is found to have the best performance.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/sva.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/sva-1024x341.png\" alt=\"\" width=\"843\" height=\"281\" class=\"aligncenter size-large wp-image-1589\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/sva-1024x341.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sva-150x50.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sva-300x100.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sva-768x255.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/sva.png 1506w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>69. CAMsterdam at SemEval-2019 Task 6: Neural and graph-based feature extraction for the identification of offensive tweets</h4>\n<p>Guy Aglionby, Chris Davis, Pushkar Mishra, Andrew Caines, Helen Yannakoudakis, Marek Rei, Ekaterina Shutova, Paula Buttery. Cambridge, Facebook, Amsterdam. SemEval 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/S19-2100.pdf\">https://www.aclweb.org/anthology/S19-2100.pdf</a></p>\n<p>A hybrid system for detecting offensive messages on Twitter. Neural text representations are given to a GBDT for classification, along with hierarchical labels and graph-based representations. Evaluation on the SemEval 2019 dataset achieves 84.7% accuracy. </p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive-300x192.png\" alt=\"\" width=\"300\" height=\"192\" class=\"aligncenter size-medium wp-image-1590\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive-300x192.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive-150x96.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive-768x491.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/offensive.png 920w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>70. Context is Key: Grammatical Error Detection with Contextual Word Representations</h4>\n<p>Samuel Bell, Helen Yannakoudakis, Marek Rei. Cambridge. BEA 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/W19-4410.pdf\">https://www.aclweb.org/anthology/W19-4410.pdf</a></p>\n<p>Investigating different contextual word representations for grammatical error detection. Flair, ELMo and BERT all provide an improvement, but combining BERT-large with the existing best error detection system gives a new state-of-the-art performance. Analysis shows large improvements on nouns, particles and preposition errors, but weaker performance on error types that are less likely to occur in general-purpose corpora, such as conjugation and orthography.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/context.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/context-1024x488.png\" alt=\"\" width=\"843\" height=\"402\" class=\"aligncenter size-large wp-image-1592\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/context-1024x488.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/context-150x72.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/context-300x143.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/context-768x366.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/context.png 1200w\" sizes=\"(max-width: 843px) 100vw, 843px\" /></a></p>\n<h4>71. Neural and FST-based approaches to grammatical error correction</h4>\n<p>Zheng Yuan, Felix Stahlberg, Marek Rei, Bill Byrne, Helen Yannakoudakis. Cambridge. BEA 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/W19-4424.pdf\">https://www.aclweb.org/anthology/W19-4424.pdf</a></p>\n<p>A pipeline system for grammatical error correction. The output of two machine traslation systems is combined using a finite state transducer and a language model, then reranked based on the output of an error detection system. The model achieves 66.75% F0.5 and ranks 2nd on error detection and 4th on error correction in the shared task.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec-300x158.png\" alt=\"\" width=\"300\" height=\"158\" class=\"aligncenter size-medium wp-image-1594\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec-300x158.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec-150x79.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec-768x404.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec-1024x538.png 1024w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/fstgec.png 1033w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>72. Bad Form: Comparing Context-Based and Form-Based Few-Shot Learning in Distributional Semantic Models</h4>\n<p>Jeroen Van Hautte, Guy Emerson, Marek Rei. Cambridge, Imperial, TechWolf. DeepLo 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/D19-6104.pdf\">https://www.aclweb.org/anthology/D19-6104.pdf</a></p>\n<p>Investigating embedding methods for rare words. Experiments demonstrate that existing baselines can achieve very competitive performance with some small modifications. Also showing the issues with common benchmarks when evaluating models that use word-form information and proposing 3 new variations of these tasks that address this. </p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/badform.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/badform-300x258.png\" alt=\"\" width=\"300\" height=\"258\" class=\"aligncenter size-medium wp-image-1597\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/badform-300x258.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/badform-150x129.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/badform.png 645w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>73. Modelling the interplay of metaphor and emotion through multitask learning</h4>\n<p>Verna Dankers, Marek Rei, Martha Lewis, Ekaterina Shutova. Amsterdam, Imperial, Cambridge. EMNLP 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/D19-1227.pdf\">https://www.aclweb.org/anthology/D19-1227.pdf</a></p>\n<p>Proposing a joint architecture for the detection of metaphorical language and emotion in text. Comparing hard parameter sharing, a cross-stitch network and a gated network where models predict either token-level labels or sentence-level scores. Evaluation on different metaphor and emotion datasets shows consistent improvements from multi-task learning, with the emotion<br />\ndimension of dominance contributing and benefiting most from metaphors.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/emotion.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/emotion-300x287.png\" alt=\"\" width=\"300\" height=\"287\" class=\"aligncenter size-medium wp-image-1599\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/emotion-300x287.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/emotion-150x143.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/emotion.png 621w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<h4>74. Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling</h4>\n<p>Bo-Hsiang Tseng, Marek Rei, Paweł Budzianowski, Richard Turner, Bill Byrne, Anna Korhonen. Cambridge, Imperial, PolyAI. EMNLP 2019.<br />\n<a href=\"https://www.aclweb.org/anthology/D19-1125.pdf\">https://www.aclweb.org/anthology/D19-1125.pdf</a></p>\n<p>Methods for reducing the amount of required annotation in a task-oriented dialogue system. The first approach is based on pseudo-labelling, training on automatically labeled examples if the model confidence is high enough. The second approach perturbs the inputs and optimizes the model to predict the same belief states compared to the original input.</p>\n<p><a href=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/dst.png\"><img decoding=\"async\" loading=\"lazy\" src=\"http://www.marekrei.com/blog/wp-content/uploads/2019/11/dst-300x185.png\" alt=\"\" width=\"300\" height=\"185\" class=\"aligncenter size-medium wp-image-1601\" srcset=\"https://www.marekrei.com/blog/wp-content/uploads/2019/11/dst-300x185.png 300w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/dst-150x93.png 150w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/dst-768x474.png 768w, https://www.marekrei.com/blog/wp-content/uploads/2019/11/dst.png 977w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/\">74 Summaries of Machine Learning and NLP Research</a> appeared first on <a rel=\"nofollow\" href=\"https://www.marekrei.com/blog\">Marek Rei</a>.</p>\n",
  "wfw:commentRss": "https://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/feed/",
  "slash:comments": 17
}