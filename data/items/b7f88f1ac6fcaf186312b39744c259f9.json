{
  "title": "Profiling Data Throughput",
  "link": "",
  "updated": "2015-04-21T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/04/21/Profiling-Bag",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><em>Disclaimer: This post is on experimental/buggy code.</em></p>\n\n<p><strong>tl;dr</strong> We measure the costs of processing semi-structured data like JSON\nblobs.</p>\n\n<h2 id=\"semi-structured-data\">Semi-structured Data</h2>\n\n<p>Semi-structured data is ubiquitous and computationally painful.  Consider\nthe following JSON blobs:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{'name': 'Alice',   'payments': [1, 2, 3]}\n{'name': 'Bob',     'payments': [4, 5]}\n{'name': 'Charlie', 'payments': None}\n</code></pre></div></div>\n\n<p>This data doesn’t fit nicely into NumPy or Pandas and so we fall back to\ndynamic pure-Python data structures like dicts and lists.  Python’s core data\nstructures are surprisingly good, about as good as compiled languages like\nJava, but dynamic data structures present some challenges for efficient\nparallel computation.</p>\n\n<h2 id=\"volume\">Volume</h2>\n\n<p>Semi-structured data is often at the beginning of our data pipeline and so\noften has the greatest size.  We may start with 100GB of raw data, reduce to\n10GB to load into a database, and finally aggregate down to 1GB for analysis,\nmachine learning, etc., 1kB of which becomes a plot or table.</p>\n\n<table align=\"right\">\n  <thead>\n  <tr>\n    <th></th>\n    <th>Data Bandwidth (MB/s)</th>\n    <th>In Parallel (MB/s)</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr><th>Disk I/O</th><td>500</td><td>500</td></tr>\n    <tr><th>Decompression</th><td>100</td><td>500</td></tr>\n    <tr><th>Deserialization</th><td>50</td><td>250</td></tr>\n    <tr><th>In-memory computation</th><td>2000</td><td>oo</td></tr>\n    <tr><th>Shuffle</th><td>9</td><td>30</td></tr>\n  </tbody>\n</table>\n\n<p>Common solutions for large semi-structured data include Python iterators,\nmultiprocessing, Hadoop, and Spark as well as proper databases like MongoDB and\nElasticSearch.  <a href=\"http://matthewrocklin.com/blog/work/2015/02/17/Towards-OOC-Bag/\">Two months\nago</a> we built\n<code class=\"language-plaintext highlighter-rouge\">dask.bag</code>, a toy dask experiment for semi-structured data.  Today we’ll\nstrengthen the <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> project and look more deeply at performance in this\nspace.</p>\n\n<p>We measure performance with data bandwidth, usually in megabytes per\nsecond (MB/s).  We’ll build intuition for why dealing with this data is costly.</p>\n\n<h2 id=\"dataset\">Dataset</h2>\n\n<p>As a test dataset we play with a dump of GitHub data from\n<a href=\"https://www.githubarchive.org/\">https://www.githubarchive.org/</a>.  This data\nrecords every public github event (commit, comment, pull request, etc.) in the\nform of a JSON blob.  This data is representative fairly representative of a\nbroader class of problems.  Often people want to do fairly simple analytics,\nlike find the top ten committers to a particular repository, or clean the\ndata before they load it into a database.</p>\n\n<p>We’ll play around with this data using <code class=\"language-plaintext highlighter-rouge\">dask.bag</code>.  This is both to get a feel\nfor what is expensive and to provide a cohesive set of examples.  In truth we\nwon’t do any real analytics on the github dataset, we’ll find that the\nexpensive parts come well before analytic computation.</p>\n\n<p>Items in our data look like this:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.bag</span> <span class=\"k\">as</span> <span class=\"n\">db</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s\">'/home/mrocklin/data/github/2013-05-0*.json.gz'</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">).</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">json</span><span class=\"p\">.</span><span class=\"n\">loads</span><span class=\"p\">).</span><span class=\"n\">take</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">({</span><span class=\"sa\">u</span><span class=\"s\">'actor'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'mjcramer'</span><span class=\"p\">,</span>\n  <span class=\"sa\">u</span><span class=\"s\">'actor_attributes'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"sa\">u</span><span class=\"s\">'gravatar_id'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'603762b7a39807503a2ee7fe4966acd1'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'login'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'mjcramer'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'User'</span><span class=\"p\">},</span>\n  <span class=\"sa\">u</span><span class=\"s\">'created_at'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'2013-05-01T00:01:28-07:00'</span><span class=\"p\">,</span>\n  <span class=\"sa\">u</span><span class=\"s\">'payload'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"sa\">u</span><span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">''</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'master_branch'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'master'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'ref'</span><span class=\"p\">:</span> <span class=\"bp\">None</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'ref_type'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'repository'</span><span class=\"p\">},</span>\n  <span class=\"sa\">u</span><span class=\"s\">'public'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n  <span class=\"sa\">u</span><span class=\"s\">'repository'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"sa\">u</span><span class=\"s\">'created_at'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'2013-05-01T00:01:28-07:00'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">''</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'fork'</span><span class=\"p\">:</span> <span class=\"bp\">False</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'forks'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'has_downloads'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'has_issues'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'has_wiki'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'id'</span><span class=\"p\">:</span> <span class=\"mi\">9787210</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'master_branch'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'master'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'settings'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'open_issues'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'owner'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'mjcramer'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'private'</span><span class=\"p\">:</span> <span class=\"bp\">False</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'pushed_at'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'2013-05-01T00:01:28-07:00'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'size'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'stargazers'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'url'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'https://github.com/mjcramer/settings'</span><span class=\"p\">,</span>\n   <span class=\"sa\">u</span><span class=\"s\">'watchers'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">},</span>\n  <span class=\"sa\">u</span><span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'CreateEvent'</span><span class=\"p\">,</span>\n  <span class=\"sa\">u</span><span class=\"s\">'url'</span><span class=\"p\">:</span> <span class=\"sa\">u</span><span class=\"s\">'https://github.com/mjcramer/settings'</span><span class=\"p\">},)</span></code></pre>\n</figure>\n\n<h2 id=\"disk-io-and-decompression--100-500-mbs\">Disk I/O and Decompression – 100-500 MB/s</h2>\n\n<table align=\"right\">\n  <thead>\n  <tr>\n    <th></th>\n    <th>Data Bandwidth (MB/s)</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr><th>Read from disk with open</th><td>500</td></tr>\n    <tr><th>Read from disk with gzip.open</th><td>100</td></tr>\n    <tr><th>Parallel Read from disk with gzip.open</th><td>500</td></tr>\n  </tbody>\n</table>\n\n<p>A modern laptop hard drive can theoretically read data from disk to memory at\n800 MB/s.  So we could burn through a 10GB dataset in fifteen seconds on our\nlaptop.  Workstations with RAID arrays can do a couple GB/s.  In practice I\nget around 500 MB/s on my personal laptop.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.bag</span> <span class=\"k\">as</span> <span class=\"n\">db</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"kn\">from</span> <span class=\"nn\">glob</span> <span class=\"kn\">import</span> <span class=\"n\">glob</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s\">'/home/mrocklin/data/github/2013-05-0*.json.gz'</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">compressed</span> <span class=\"o\">=</span> <span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">).</span><span class=\"n\">read</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">fn</span> <span class=\"ow\">in</span> <span class=\"n\">glob</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">75.1</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">1.07</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">1.14</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">1.14</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">0.194</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>  <span class=\"c1\"># MB/s\n</span><span class=\"mf\">508.5912175438597</span></code></pre>\n</figure>\n\n<p>To reduce storage and transfer costs we often compress data.  This requires CPU\neffort whenever we want to operate on the stored values.  This can limit\ndata bandwidth.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">7</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">gzip</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">gzip</span><span class=\"p\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">).</span><span class=\"n\">read</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">fn</span> <span class=\"ow\">in</span> <span class=\"n\">glob</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">12.2</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">18.7</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">30.9</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">30.9</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">30.9</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>         <span class=\"c1\"># MB/s  total bandwidth\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">]:</span> <span class=\"mf\">102.16563844660195</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">30.9</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>   <span class=\"c1\"># MB/s  compressed bandwidth\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">]:</span> <span class=\"mf\">18.763559482200648</span></code></pre>\n</figure>\n\n<p>So we lose some data bandwidth through compression.  Where we could previously\nprocess 500 MB/s we’re now down to only 100 MB/s.  If we count bytes in terms\nof the amount stored on disk then we’re only hitting 18 MB/s.  We’ll get around\nthis with multiprocessing.</p>\n\n<h2 id=\"decompression-and-parallel-processing--500-mbs\">Decompression and Parallel processing – 500 MB/s</h2>\n\n<p>Fortunately we often have more cores than we know what to do with.\nParallelizing reads can hide much of the decompression cost.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">dask.bag</span> <span class=\"k\">as</span> <span class=\"n\">db</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">13</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">nbytes</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">).</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">).</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">130</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">402</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">532</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">5.5</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">]:</span> <span class=\"n\">nbytes</span> <span class=\"o\">/</span> <span class=\"mf\">5.5</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">14</span><span class=\"p\">]:</span> <span class=\"mf\">573.9850932727272</span></code></pre>\n</figure>\n\n<p>Dask.bag infers that we need to use gzip from the filename.  Dask.bag currently\nuses <code class=\"language-plaintext highlighter-rouge\">multiprocessing</code> to distribute work, allowing us to reclaim our 500 MB/s\nthroughput on compressed data.  We also could have done this with\nmultiprocessing, straight Python, and a little elbow-grease.</p>\n\n<h2 id=\"deserialization--30-mbs\">Deserialization – 30 MB/s</h2>\n\n<table align=\"right\">\n  <thead>\n  <tr>\n    <th></th>\n    <th>Data Bandwidth (MB/s)</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr><th>json.loads</th><td>30</td></tr>\n    <tr><th>ujson.loads</th><td>50</td></tr>\n    <tr><th>Parallel ujson.loads</th><td>150</td></tr>\n  </tbody>\n</table>\n\n<p>Once we decompress our data we still need to turn bytes into meaningful data\nstructures (dicts, lists, etc..)  Our GitHub data comes to us as JSON.  This\nJSON contains various encodings and bad characters so, just for today, we’re\ngoing to punt on bad lines.  Converting JSON text to Python objects\nexplodes out in memory a bit, so we’ll consider a smaller subset for this part,\na single day.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">20</span><span class=\"p\">]:</span> <span class=\"k\">def</span> <span class=\"nf\">loads</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">):</span>\n<span class=\"p\">...</span>          <span class=\"k\">try</span><span class=\"p\">:</span> <span class=\"k\">return</span> <span class=\"n\">json</span><span class=\"p\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>          <span class=\"k\">except</span><span class=\"p\">:</span> <span class=\"k\">return</span> <span class=\"bp\">None</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">21</span><span class=\"p\">]:</span> <span class=\"n\">path</span> <span class=\"o\">=</span> <span class=\"s\">'/home/mrocklin/data/github/2013-05-01-*.json.gz'</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">22</span><span class=\"p\">]:</span> <span class=\"n\">lines</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">))</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">23</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">blobs</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">,</span> <span class=\"n\">lines</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">10.7</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">760</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">11.5</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">11.3</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">24</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">11.3</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">24</span><span class=\"p\">]:</span> <span class=\"mf\">33.9486321238938</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">11.3</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">]:</span> <span class=\"mf\">6.2989179646017694</span></code></pre>\n</figure>\n\n<p>So in terms of actual bytes of JSON we can only convert about 30MB per second.\nIf we count in terms of the compressed data we store on disk then this looks\nmore bleak at only 6 MB/s.</p>\n\n<h3 id=\"this-can-be-improved-by-using-faster-libraries--50-mbs\">This can be improved by using faster libraries – 50 MB/s</h3>\n\n<p>The <a href=\"https://github.com/esnme/ultrajson\">ultrajson</a> library, <code class=\"language-plaintext highlighter-rouge\">ujson</code>, is pretty\nslick and can improve our performance a bit.  This is what Pandas uses under\nthe hood.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">28</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">ujson</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">29</span><span class=\"p\">]:</span> <span class=\"k\">def</span> <span class=\"nf\">loads</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">):</span>\n<span class=\"p\">...</span>          <span class=\"k\">try</span><span class=\"p\">:</span> <span class=\"k\">return</span> <span class=\"n\">ujson</span><span class=\"p\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>          <span class=\"k\">except</span><span class=\"p\">:</span> <span class=\"k\">return</span> <span class=\"bp\">None</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">30</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">blobs</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">,</span> <span class=\"n\">lines</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">6.37</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">1.17</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">7.53</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">7.37</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">31</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">7.37</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">31</span><span class=\"p\">]:</span> <span class=\"mf\">52.05149837177748</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">32</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">7.37</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">32</span><span class=\"p\">]:</span> <span class=\"mf\">9.657771099050203</span></code></pre>\n</figure>\n\n<h3 id=\"or-through-parallelism---150-mbs\">Or through Parallelism  – 150 MB/s</h3>\n\n<p>This can also be accelerated through parallelism, just like decompression.\nIt’s a bit cumbersome to show parallel deserializaiton in isolation.\nInstead we’ll show all of them together.  This will under-estimate\nperformance but is much easier to code up.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">33</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">).</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">).</span><span class=\"n\">count</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">32.3</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">822</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">854</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">2.8</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">38</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">2.8</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">38</span><span class=\"p\">]:</span> <span class=\"mf\">137.00697964285717</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">39</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">compressed</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">2.8</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">39</span><span class=\"p\">]:</span> <span class=\"mf\">25.420633214285715</span></code></pre>\n</figure>\n\n<h2 id=\"mapping-and-grouping---2000-mbs\">Mapping and Grouping - 2000 MB/s</h2>\n\n<table align=\"right\">\n  <thead>\n  <tr>\n    <th></th>\n    <th>Data Bandwidth (MB/s)</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr><th>Simple Python operations</th><td>1400</td></tr>\n    <tr><th>Complex CyToolz operations</th><td>2600</td></tr>\n  </tbody>\n</table>\n\n<p>Once we have data in memory, Pure Python is relatively fast.  Cytoolz moreso.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">55</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s\">'type'</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">blobs</span><span class=\"p\">)</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">162</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">123</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">285</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">268</span> <span class=\"n\">ms</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">55</span><span class=\"p\">]:</span>\n<span class=\"p\">{</span><span class=\"sa\">u</span><span class=\"s\">'CommitCommentEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'CreateEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'DeleteEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'DownloadEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'FollowEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'ForkEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'GistEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'GollumEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'IssueCommentEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'IssuesEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'MemberEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'PublicEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'PullRequestEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'PullRequestReviewCommentEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'PushEvent'</span><span class=\"p\">,</span>\n <span class=\"sa\">u</span><span class=\"s\">'WatchEvent'</span><span class=\"p\">}</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">56</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">0.268</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">56</span><span class=\"p\">]:</span> <span class=\"mf\">1431.4162052238805</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">57</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">cytoolz</span>\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">58</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">cytoolz</span><span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s\">'type'</span><span class=\"p\">,</span> <span class=\"n\">blobs</span><span class=\"p\">)</span>  <span class=\"c1\"># CyToolz FTW\n</span><span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">144</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">0</span> <span class=\"n\">ns</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">144</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">144</span> <span class=\"n\">ms</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">0.144</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">59</span><span class=\"p\">]:</span> <span class=\"mf\">2664.024604166667</span></code></pre>\n</figure>\n\n<p>So slicing and logic are essentially free.  The cost of compression and\ndeserialization dominates actual computation time.  Don’t bother optimizing\nfast per-record code, especially if CyToolz has already done so for you.  Of\ncourse, you might be doing something expensive per record.  If so then most of\nthis post isn’t relevant for you.</p>\n\n<h2 id=\"shuffling---5-50-mbs\">Shuffling - 5-50 MB/s</h2>\n\n<table align=\"right\">\n  <thead>\n  <tr>\n    <th></th>\n    <th>Data Bandwidth (MB/s)</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr><th>Naive groupby with on-disk Shuffle</th><td>25</td></tr>\n    <tr><th>Clever foldby without Shuffle</th><td>250</td></tr>\n  </tbody>\n</table>\n\n<p>For complex logic, like full groupbys and joins, we need to communicate large\namounts of data between workers.  This communication forces us to go through\nanother full serialization/write/deserialization/read cycle.  This hurts.  And\nso, the single most important message from this post:</p>\n\n<p><strong>Avoid communication-heavy operations on semi-structured data.  Structure your\ndata and load into a database instead.</strong></p>\n\n<p>That being said, people will inevitably ignore this advice so we need to have a\nnot-terrible fallback.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">62</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>                   <span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>                   <span class=\"p\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s\">'type'</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>                   <span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">):</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">))))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">46.3</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">6.57</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">52.8</span> <span class=\"n\">s</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"nb\">min</span> <span class=\"mi\">14</span><span class=\"n\">s</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">62</span><span class=\"p\">]:</span>\n<span class=\"p\">{</span><span class=\"s\">'CommitCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">17889</span><span class=\"p\">,</span>\n <span class=\"s\">'CreateEvent'</span><span class=\"p\">:</span> <span class=\"mi\">210516</span><span class=\"p\">,</span>\n <span class=\"s\">'DeleteEvent'</span><span class=\"p\">:</span> <span class=\"mi\">14534</span><span class=\"p\">,</span>\n <span class=\"s\">'DownloadEvent'</span><span class=\"p\">:</span> <span class=\"mi\">440</span><span class=\"p\">,</span>\n <span class=\"s\">'FollowEvent'</span><span class=\"p\">:</span> <span class=\"mi\">35910</span><span class=\"p\">,</span>\n <span class=\"s\">'ForkEvent'</span><span class=\"p\">:</span> <span class=\"mi\">67939</span><span class=\"p\">,</span>\n <span class=\"s\">'GistEvent'</span><span class=\"p\">:</span> <span class=\"mi\">7344</span><span class=\"p\">,</span>\n <span class=\"s\">'GollumEvent'</span><span class=\"p\">:</span> <span class=\"mi\">31688</span><span class=\"p\">,</span>\n <span class=\"s\">'IssueCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">163798</span><span class=\"p\">,</span>\n <span class=\"s\">'IssuesEvent'</span><span class=\"p\">:</span> <span class=\"mi\">102680</span><span class=\"p\">,</span>\n <span class=\"s\">'MemberEvent'</span><span class=\"p\">:</span> <span class=\"mi\">11664</span><span class=\"p\">,</span>\n <span class=\"s\">'PublicEvent'</span><span class=\"p\">:</span> <span class=\"mi\">1867</span><span class=\"p\">,</span>\n <span class=\"s\">'PullRequestEvent'</span><span class=\"p\">:</span> <span class=\"mi\">69080</span><span class=\"p\">,</span>\n <span class=\"s\">'PullRequestReviewCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">17056</span><span class=\"p\">,</span>\n <span class=\"s\">'PushEvent'</span><span class=\"p\">:</span> <span class=\"mi\">960137</span><span class=\"p\">,</span>\n <span class=\"s\">'WatchEvent'</span><span class=\"p\">:</span> <span class=\"mi\">173631</span><span class=\"p\">}</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">63</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mi\">134</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>  <span class=\"c1\"># MB/s\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">63</span><span class=\"p\">]:</span> <span class=\"mf\">23.559091</span></code></pre>\n</figure>\n\n<p>This groupby operation goes through the following steps:</p>\n\n<ol>\n  <li>Read from disk</li>\n  <li>Decompress GZip</li>\n  <li>Deserialize with <code class=\"language-plaintext highlighter-rouge\">ujson</code></li>\n  <li>Do in-memory groupbys on chunks of the data</li>\n  <li>Reserialize with <code class=\"language-plaintext highlighter-rouge\">msgpack</code> (a bit faster)</li>\n  <li>Append group parts to disk</li>\n  <li>Read in new full groups from disk</li>\n  <li>Deserialize <code class=\"language-plaintext highlighter-rouge\">msgpack</code> back to Python objects</li>\n  <li>Apply length function per group</li>\n</ol>\n\n<p>Some of these steps have great data bandwidths, some less-so.\nWhen we compound many steps together our bandwidth suffers.\nWe get about 25 MB/s total.  This is about what pyspark gets (although today\n<code class=\"language-plaintext highlighter-rouge\">pyspark</code> can parallelize across multiple machines while <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> can not.)</p>\n\n<p>Disclaimer, the numbers above are for <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> and could very easily be\ndue to implementation flaws, rather than due to inherent challenges.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">pyspark</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">sc</span> <span class=\"o\">=</span> <span class=\"n\">pyspark</span><span class=\"p\">.</span><span class=\"n\">SparkContext</span><span class=\"p\">(</span><span class=\"s\">'local[8]'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">rdd</span> <span class=\"o\">=</span> <span class=\"n\">sc</span><span class=\"p\">.</span><span class=\"n\">textFile</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">rdd</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">keyBy</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">d</span><span class=\"p\">:</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s\">'type'</span><span class=\"p\">])</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">groupByKey</span><span class=\"p\">()</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">):</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)))</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">collect</span><span class=\"p\">())</span></code></pre>\n</figure>\n\n<p>I would be interested in hearing from people who use full groupby on BigData.\nI’m quite curious to hear how this is used in practice and how it performs.</p>\n\n<h2 id=\"creative-groupbys---250-mbs\">Creative Groupbys - 250 MB/s</h2>\n\n<p>Don’t use groupby.  You can often work around it with cleverness.  Our example\nabove can be handled with streaming grouping reductions (see <a href=\"http://toolz.readthedocs.org/en/latest/streaming-analytics.html#split-apply-combine-with-groupby-and-reduceby\">toolz\ndocs.</a>)\nThis requires more thinking from the programmer but avoids the costly shuffle\nprocess.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">66</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">db</span><span class=\"p\">.</span><span class=\"n\">from_filenames</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>                   <span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>                   <span class=\"p\">.</span><span class=\"n\">foldby</span><span class=\"p\">(</span><span class=\"s\">'type'</span><span class=\"p\">,</span> <span class=\"k\">lambda</span> <span class=\"n\">total</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">:</span> <span class=\"n\">total</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"k\">lambda</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"p\">))</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">66</span><span class=\"p\">]:</span>\n<span class=\"p\">{</span><span class=\"s\">'CommitCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">17889</span><span class=\"p\">,</span>\n <span class=\"s\">'CreateEvent'</span><span class=\"p\">:</span> <span class=\"mi\">210516</span><span class=\"p\">,</span>\n <span class=\"s\">'DeleteEvent'</span><span class=\"p\">:</span> <span class=\"mi\">14534</span><span class=\"p\">,</span>\n <span class=\"s\">'DownloadEvent'</span><span class=\"p\">:</span> <span class=\"mi\">440</span><span class=\"p\">,</span>\n <span class=\"s\">'FollowEvent'</span><span class=\"p\">:</span> <span class=\"mi\">35910</span><span class=\"p\">,</span>\n <span class=\"s\">'ForkEvent'</span><span class=\"p\">:</span> <span class=\"mi\">67939</span><span class=\"p\">,</span>\n <span class=\"s\">'GistEvent'</span><span class=\"p\">:</span> <span class=\"mi\">7344</span><span class=\"p\">,</span>\n <span class=\"s\">'GollumEvent'</span><span class=\"p\">:</span> <span class=\"mi\">31688</span><span class=\"p\">,</span>\n <span class=\"s\">'IssueCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">163798</span><span class=\"p\">,</span>\n <span class=\"s\">'IssuesEvent'</span><span class=\"p\">:</span> <span class=\"mi\">102680</span><span class=\"p\">,</span>\n <span class=\"s\">'MemberEvent'</span><span class=\"p\">:</span> <span class=\"mi\">11664</span><span class=\"p\">,</span>\n <span class=\"s\">'PublicEvent'</span><span class=\"p\">:</span> <span class=\"mi\">1867</span><span class=\"p\">,</span>\n <span class=\"s\">'PullRequestEvent'</span><span class=\"p\">:</span> <span class=\"mi\">69080</span><span class=\"p\">,</span>\n <span class=\"s\">'PullRequestReviewCommentEvent'</span><span class=\"p\">:</span> <span class=\"mi\">17056</span><span class=\"p\">,</span>\n <span class=\"s\">'PushEvent'</span><span class=\"p\">:</span> <span class=\"mi\">960137</span><span class=\"p\">,</span>\n <span class=\"s\">'WatchEvent'</span><span class=\"p\">:</span> <span class=\"mi\">173631</span><span class=\"p\">}</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">322</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mi\">604</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">926</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">13.2</span> <span class=\"n\">s</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">67</span><span class=\"p\">]:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">total</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"mf\">13.2</span> <span class=\"o\">/</span> <span class=\"mf\">1e6</span>  <span class=\"c1\"># MB/s\n</span><span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">67</span><span class=\"p\">]:</span> <span class=\"mf\">239.16047181818183</span></code></pre>\n</figure>\n\n<p>We can also spell this with PySpark which performs about the same.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">rdd</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">loads</span><span class=\"p\">)</span>  <span class=\"c1\"># PySpark equivalent\n</span><span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">keyBy</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">d</span><span class=\"p\">:</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s\">'type'</span><span class=\"p\">])</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">combineByKey</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">d</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"k\">lambda</span> <span class=\"n\">total</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">:</span> <span class=\"n\">total</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"k\">lambda</span> <span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n<span class=\"p\">...</span>         <span class=\"p\">.</span><span class=\"n\">collect</span><span class=\"p\">())</span></code></pre>\n</figure>\n\n<h2 id=\"use-a-database\">Use a Database</h2>\n\n<p>By the time you’re grouping or joining datasets you probably have structured\ndata that could fit into a dataframe or database.  You should transition from\ndynamic data structures (dicts/lists) to dataframes or databases as early as\npossible.  DataFrames and databases compactly represent data in formats that\ndon’t require serialization; this improves performance.  Databases are also\nvery clever about reducing communication.</p>\n\n<p>Tools like <code class=\"language-plaintext highlighter-rouge\">pyspark</code>, <code class=\"language-plaintext highlighter-rouge\">toolz</code>, and <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> are great for initial cleanings\nof semi-structured data into a structured format but they’re relatively\ninefficient at complex analytics.  For inconveniently large data you should\nconsider a database as soon as possible.  That could be some big-data-solution\nor often just Postgres.</p>\n\n<h2 id=\"better-data-structures-for-semi-structured-data\">Better data structures for semi-structured data?</h2>\n\n<p>Dynamic data structures (dicts, lists) are overkill for semi-structured data.\nWe don’t need or use their full power but we inherit all of their limitations\n(e.g.  serialization costs.)  Could we build something NumPy/Pandas-like that\ncould handle the blob-of-JSON use-case?  Probably.</p>\n\n<p>DyND is one such project.  DyND is a C++ project with Python bindings written\nby Mark Wiebe and Irwin Zaid and historically funded largely by Continuum and\nXData under the same banner as Blaze/Dask.  It could probably handle the\nsemi-structured data problem case if given a bit of love.  It handles variable\nlength arrays, text data, and missing values all with numpy-like semantics:</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">dynd</span> <span class=\"kn\">import</span> <span class=\"n\">nd</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"s\">'Alice'</span><span class=\"p\">,</span>                       <span class=\"c1\"># Semi-structured data\n</span><span class=\"p\">...</span>          <span class=\"s\">'location'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">'city'</span><span class=\"p\">:</span> <span class=\"s\">'LA'</span><span class=\"p\">,</span> <span class=\"s\">'state'</span><span class=\"p\">:</span> <span class=\"s\">'CA'</span><span class=\"p\">},</span>\n<span class=\"p\">...</span>          <span class=\"s\">'credits'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]},</span>\n<span class=\"p\">...</span>         <span class=\"p\">{</span><span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"s\">'Bob'</span><span class=\"p\">,</span>\n<span class=\"p\">...</span>          <span class=\"s\">'credits'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],</span>\n<span class=\"p\">...</span>          <span class=\"s\">'location'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">'city'</span><span class=\"p\">:</span> <span class=\"s\">'NYC'</span><span class=\"p\">,</span> <span class=\"s\">'state'</span><span class=\"p\">:</span> <span class=\"s\">'NY'</span><span class=\"p\">}}]</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">dtype</span> <span class=\"o\">=</span> <span class=\"s\">'''var * {name: string,\n...                   location: {city: string,\n...                              state: string[2]},\n...                   credits: var * int}'''</span>        <span class=\"c1\"># Shape of our data\n</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">nd</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"n\">dtype</span><span class=\"p\">)</span>                  <span class=\"c1\"># Create DyND array\n</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span>                                               <span class=\"c1\"># Store compactly in memory\n</span><span class=\"n\">nd</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"s\">\"Alice\"</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s\">\"LA\"</span><span class=\"p\">,</span> <span class=\"s\">\"CA\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]],</span>\n          <span class=\"p\">[</span><span class=\"s\">\"Bob\"</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s\">\"NYC\"</span><span class=\"p\">,</span> <span class=\"s\">\"NY\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]]])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">location</span><span class=\"p\">.</span><span class=\"n\">city</span>                                 <span class=\"c1\"># Nested indexing\n</span><span class=\"n\">nd</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([</span> <span class=\"s\">\"LA\"</span><span class=\"p\">,</span> <span class=\"s\">\"NYC\"</span><span class=\"p\">],</span>\n         <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s\">\"strided * string\"</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">credits</span>                                       <span class=\"c1\"># Variable length data\n</span><span class=\"n\">nd</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],</span>    <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]],</span>\n         <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s\">\"strided * var * int32\"</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">credits</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>                                  <span class=\"c1\"># And computation\n</span><span class=\"n\">nd</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">],</span>     <span class=\"p\">[</span><span class=\"mi\">40</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">]],</span>\n         <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"s\">\"strided * var * int32\"</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p>Sadly DyND has functionality gaps which limit usability.</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">credits</span>                                      <span class=\"c1\"># Sadly incomplete :(\n</span><span class=\"nb\">TypeError</span><span class=\"p\">:</span> <span class=\"n\">bad</span> <span class=\"n\">operand</span> <span class=\"nb\">type</span> <span class=\"k\">for</span> <span class=\"n\">unary</span> <span class=\"o\">-</span></code></pre>\n</figure>\n\n<p>I would like to see DyND mature to the point where it could robustly handle\nsemi-structured data.  I think that this would be a big win for productivity\nthat would make projects like <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> and <code class=\"language-plaintext highlighter-rouge\">pyspark</code> obsolete for a large\nclass of use-cases.  If you know Python, C++, and would like to help DyND grow\nI’m sure that Mark and Irwin would love the help</p>\n\n<ul>\n  <li><a href=\"https://groups.google.com/forum/#!forum/libdynd-dev\">DyND Mailing list</a></li>\n  <li><a href=\"https://github.com/libdynd/dynd-python\">DyND GitHub repository</a></li>\n</ul>\n\n<h2 id=\"comparison-with-pyspark\">Comparison with PySpark</h2>\n\n<p>Dask.bag pros:</p>\n\n<ol>\n  <li>Doesn’t engage the JVM, no heap errors or fiddly flags to set</li>\n  <li>Conda/pip installable.  You could have it in less than twenty seconds from now.</li>\n  <li>Slightly faster in-memory implementations thanks to <code class=\"language-plaintext highlighter-rouge\">cytoolz</code>; this isn’t\nimportant though</li>\n  <li>Good handling of lazy results per-partition</li>\n  <li>Faster / lighter weight start-up times</li>\n  <li>(Subjective) I find the API marginally cleaner</li>\n</ol>\n\n<p>PySpark pros:</p>\n\n<ol>\n  <li>Supports distributed computation (this is obviously huge)</li>\n  <li>More mature, more filled out API</li>\n  <li>HDFS integration</li>\n</ol>\n\n<p>Dask.bag reinvents a wheel; why bother?</p>\n\n<ol>\n  <li>Given the machinery inherited from <code class=\"language-plaintext highlighter-rouge\">dask.array</code> and <code class=\"language-plaintext highlighter-rouge\">toolz</code>, <code class=\"language-plaintext highlighter-rouge\">dask.bag</code> is\nvery cheap to build and maintain.  It’s around 500 significant lines of code.</li>\n  <li>PySpark throws Python processes inside a JVM ecosystem which can cause some\nconfusion among users and a performance hit.  A task scheduling\nsystem in the native code ecosystem would be valuable.</li>\n  <li>Comparison and competition is healthy</li>\n  <li>I’ve been asked to make a distributed array.  I suspect that distributed\nbag is a good first step.</li>\n</ol>"
}