{
  "title": "History of Monte Carlo Methods - Part 3",
  "link": "",
  "published": "2015-11-13T21:30:00+00:00",
  "updated": "2015-11-13T21:30:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-11-13:/sebastian/blog/history-of-monte-carlo-methods-part-3.html",
  "summary": "<p>This is the third part of a three part post.\nThe <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-1.html\">first part</a> covered the early history of Monte\nCarlo and the rejection sampling method, the <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-2.html\">second\npart</a> covered sequential Monte Carlo.</p>\n<h1>Part 3</h1>\n<p>In this part we are going â€¦</p>",
  "content": "<p>This is the third part of a three part post.\nThe <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-1.html\">first part</a> covered the early history of Monte\nCarlo and the rejection sampling method, the <a href=\"http://www.nowozin.net/sebastian/blog/history-of-monte-carlo-methods-part-2.html\">second\npart</a> covered sequential Monte Carlo.</p>\n<h1>Part 3</h1>\n<p>In this part we are going to look at Markov chain Monte Carlo.</p>\n<p>The video files <a href=\"https://1drv.ms/u/s!AniEhJbTwIdrkuMvpGcjY3NR12xbuw?e=YvUIAn\">are also available for offline\nviewing</a> in\nMP4/H.264, WebM/VP8, and WebM/VP9 formats.</p>\n<video width=\"639\" controls>\n<source src=\"https://onedrive.live.com/download?cid=6B87C0D396848478&resid=6B87C0D396848478%21307640&authkey=ALXIUk3FhLZCBFU\"\n    type=\"video/mp4\">\nYour browser does not support the video tag.\n</video>\n\n<p><iframe\nsrc=\"https://onedrive.live.com/embed?cid=6B87C0D396848478&resid=6B87C0D396848478%21108438&authkey=AD-c_aaKMW9BSFI&em=2\"\nwidth=\"639\" height=\"360\" frameborder=\"0\" scrolling=\"no\"></iframe>\n</p>\n<p>(Click on the slide to advance, or use the previous/next buttons.)</p>\n<p>(Also note there are three additional video visualization below.)</p>\n<h2>Transcript</h2>\n<p>(This is a slightly edited and link-annotated transcript of the audio in the\nabove video.  As this is spoken text, it is not as polished as my writing.)</p>\n<p><strong>Speaker</strong>: So this was one of family of Monte Carlo methods. I have too few\ntime remaining but a little bit of time to talk about a completely different\nfamily of Monte Carlo methods and you may have heard this abbreviation before.\nIt is called MCMC.</p>\n<p>MCMC stands for Markov chain Monte Carlo and it is completely different from\n<a href=\"http://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf\">importance sampling</a>.\nThe basic difference is instead of growing a configuration or weighting\nconfigurations, I always have a certain state and I manipulate that state\niteratively, and if I do this long enough then I have obtained a sample that\nis uniformly distributed. I will get into the details in a minute. I first\nwant to talk briefly about the history.</p>\n<p>It was invented by <a href=\"https://en.wikipedia.org/wiki/Marshall_Rosenbluth\">Marshall\nRosenbluth</a> but it is\ncalled the Metropolis algorithm. Why is that? Well, there were five authors on\nthe paper. The first of which was Nicholas Metropolis. And I roughly sized the\npictures according to contributions to the paper.</p>\n<p>There are two different historical accounts of how the method came about and\nthey agree on that <a href=\"https://en.wikipedia.org/wiki/Edward_Teller\">Edward\nTeller</a> posed the mathematical\nproblem, and Marshall Rosenbluth solved it, and Arianna Rosenbluth implemented\nit, and the other two authors did not do anything.  They ordered the author\nnames alphabetically and <a href=\"https://en.wikipedia.org/wiki/Nicholas_Metropolis\">Nicholas\nMetropolis</a> happened to be\nhead of the group at that time. In any case, two interesting things about it,\nall of these authors did not use the method in their following research. Also\nthe method is now very, very popular.  Actually, Marshall Rosenbluth\nafterwards founded the field of plasma physics, so he completely went into a\ndifferent direction. At the turn of the 21st century, Jack Dongarra and\nFrancis Sullivan, two researchers in scientific computing, were asking to\ncompile a <a href=\"https://www.siam.org/pdf/news/637.pdf\">list of the top 10 algorithms of the 20th\ncentury</a> and this was one\nof them in their list. And quicksort was another one.  So it is really an\nimportant algorithm. First, the intuition, then a little bit formalization and\nhow it applies to our problem.</p>\n<p>So, here is the intuition. What the method does: it constructs a directed\ngraph where each possible state is a node, and there are simple modifications\nyou can make indicated by these directed arcs to transform one state into\nanother state. For example, in our chain case, we could bend the chain at a\ncertain node or we could maybe change the last state a little bit or\nsomething. Some simple transformations you can perform to transform one state\ninto another state. Only a few for each state, only a few arcs that leave each\nstate. And then it performs a random walk on this graph in such a way that if\nyou perform the random walk long enough and then stop it, you are uniformly\ndistributed across the whole graph, or according to some target probability\ndistribution.</p>\n<p>The graph is much too large to explicit construct, right? It has exponentially\nmany configurations on our case, but it still it is able to perform a random\nwalk on this graph. It is called Markov chain Monte Carlo because the basic\nconcept is a <a href=\"https://en.wikipedia.org/wiki/Markov_chain\">Markov chain</a> and I\nwant to quickly introduce that concept to you.  So here is a simple graph with\njust three states, <em>A</em>, <em>B</em>, and <em>C</em>. And imagine that you are standing at\nState <em>B</em> and you follow the very simple rule of sequentially walking along\nthat graph.  You see all the edges that leave your current state. They have\nnumbers associated to them and these numbers sum up to one. So if you stand on\nState <em>B</em>, you have a 40% probability to move to State <em>A</em>, a 50% probability\nto move to State <em>C</em>, and a 10% probability to stay in State <em>B</em>. So you just\nfollow that rule in each time step and you arrive at a new state.</p>\n<p>(This is the video I showed, which is not visible in the slides above.  The\nvideo file is also available as <a href=\"/sebastian/videos/MarkovChain1.mp4\">MP4/H.264 file</a> (1MB).)</p>\n<video id=\"markovchain1\" class=\"video-js vjs-default-skin\"\n    controls preload=\"auto\"\n    width=\"600\" height=\"450\" data-setup=\"{}\">\n<source src=\"/sebastian/videos/MarkovChain1.mp4\" type='video/mp4'>\n<p class=\"vjs-no-js\">\nTo view this video please enable JavaScript, and\nconsider upgrading to a web browser that\n<a href=\"http://videojs.com/html5-video-support/\" target=\"_blank\">supports HTML5 video</a>\n</p>\n</video>\n\n<p>And when you do that and you record how often you reach a certain\nstate, then this is a histogram you would obtain. So it seems to converge to\nsome values after just a few hundred steps. And in fact, the limit\ndistribution that you obtain ultimately is given by these numbers here. And it\ndoes not depend where you start, but it is completely independent of where you\nstart.</p>\n<p>The Metropolis algorithm solves the inverse problem. The inverse problem is\nthe following. So here we had the rules how to walk and we just recalled that\nthe limit distribution. What if you have a graph structure and you have some\ntarget probabilities that you want to realize? How should you put numbers at\nthe edges to reach that limiting distribution? That was the problem that was\nposed by Edward Teller in essence.</p>\n<p>Here we have that target distribution and the Metropolis algorithm is a\nconstructive way to choose its transition probabilities. It assumes that you\nhave a base Markov chain, so some basic random walk on the graph. So let us\nsay, just uniformly go over all outgoing edges on that graph. And we could\nfollow that random walk, right? It would not be the same limit distribution\nthat we are interested in but we could follow that random walk and we would\nget a different limit distribution. And what it now does is whenever we\ntransition from one state to the other, in addition, modulates that decision\nand has the option to reject that step. It can only accept or reject steps\nproposed by the base Markov chain. The final Markov chain of the Metropolis\nalgorithm is this chain <span class=\"math\">\\(T\\)</span> which is the base Markov chain multiplied with its\nacceptance rate. And the acceptance rate is calculated according to that\nformula which has a quite simple interpretation.</p>\n<p>The acceptance probability is high when the target probability, <span class=\"math\">\\(\\pi_j\\)</span>, is\nhigh but your current probability is low, when it's more likely to be in a\ntarget state or vice versa, if it is unlikely, you are more likely to stay at\nthe current configuration. Or if the base chain pushes you to some other\nstate, you divide by that probability to compensate for that bias of the base\nchain. So that is sort of the numerator and denominator have the two effects.\nAnd the remaining probability, everything that is sort of modulated down, that\nis the reject probability to stay in the current state.</p>\n<p>(This is the video I showed, which is not visible in the slides above.  The\nvideo file is also available as <a href=\"/sebastian/videos/MarkovChain2.mp4\">MP4/H.264 file</a> (1MB).)</p>\n<video id=\"markovchain2\" class=\"video-js vjs-default-skin\"\n    controls preload=\"auto\"\n    width=\"600\" height=\"450\" data-setup=\"{}\">\n<source src=\"/sebastian/videos/MarkovChain2.mp4\" type='video/mp4'>\n<p class=\"vjs-no-js\">\nTo view this video please enable JavaScript, and\nconsider upgrading to a web browser that\n<a href=\"http://videojs.com/html5-video-support/\" target=\"_blank\">supports HTML5 video</a>\n</p>\n</video>\n\n<p>So let us do that simple calculation for our example here. For that limit\ndistribution, we would obtain that values. And let us take a look at whether\nwe converge to that limiting distribution. So the limiting distribution was\n0.7, 0.1, 0.2, it converges slower than in the example before, but ultimately,\nwe are guaranteed to converge to the limit distribution that we setup. It is\nnot a unique way to construct such a Markov chain but it is a constructive way\nto do so.</p>\n<p>In our (self-avoiding random walk) example, we want to walk on this graph. And\nwhat we are going to do is we just allow simple transformations, we pick a\nrandom element on that chain and bend it 90 degrees in a random fashion. So\nthat gives us the arcs on that graph and you can imagine for a chain of a\ncertain length, there are only so many ways which you can enumerate to bending\nthe elements by 90 degrees. And if we happen to bend it in such a way that it\nis actually no longer self-avoiding, we can remove that and add that back to\nthe probability of staying in a certain state. And remember we wanted to\nsample uniformly overall the states on the graph. So we just plug that into\nthe acceptance rate calculation of the Metropolis algorithm and the <span class=\"math\">\\(\\pi\\)</span>,\nbecause it is uniformly distributed, just cancels out of that rate. So it is a\nvery simple calculation.</p>\n<p>And in practice, it is even simpler to implement that. We have a certain\nstate. We just initialize it with any state we want. We propose a random\nmodification and we accept or reject that, and then we have a new state, and\nwe iterate, we accept or reject that. So we keep doing that and after a\ncertain amount of time, we just keep the number of samples that we have\ngenerated and we can compute our expectations with that sample that we have\ngenerated. They are no longer independent samples because we have always\nmodified them a little bit but they are a set of samples that we have\ngenerated. And this is the estimate, again, same curve that we had before,\nthis time, with the MCMC approach. And I am almost out time but I want to take\nthe last three slides here to show you our last method, the final method of\nthe talk. Any questions on MCMC so far?</p>\n<p><strong>Attendee</strong>: If we said we accept or reject. We make a bend and we accept\nthem. How do you decide whether to accept or reject, just whether it crosses\nitself.</p>\n<p><strong>Speaker</strong>: If it crosses itself, it is no longer a valid state, and we\nimmediately reject. If it does not cross itself, you compute the acceptance\nprobability by that formula and the acceptance probability maybe 0.8, and then\nyou roll a random number between zero and one, uniformly distributed, and if\nthat number is below the acceptance rate, then you <em>accept</em>. If it is above the\nacceptance rate that you have calculated, you <em>reject</em>. So if the acceptance\nrate is one, you always accept. If the acceptance rate is 0.5, you flip a coin\nuniformly to accept or reject.</p>\n<p><strong>Attendee</strong>: How would I calculate the acceptance rate in the self-avoiding\nrandom walk problem?</p>\n<p><strong>Speaker</strong>: That depends on this graph structure here. So every state in the\ngraph has a certain set of allowed changes, right? For some state which is\nvery compact in number of allowed changes becomes smaller. For some longer\nchain, you can basically bend it at any element, in any direction and it would\nstill be a self-avoiding work.</p>\n<p><strong>Attendee</strong>: We have to check them all to count how many were\nself-intersecting?</p>\n<p><strong>Speaker</strong>: Yes, in some case you can remove it, I mean, not in this case. In\nsome cases, you always have a probability mass everywhere and then not like a\nhard constraint like self-avoiding, then it cancels out as well. But in this\ncase, we have to enumerate all of them.</p>\n<p>(A warning in this edit: I cannot recommend learning about the simulated\nannealing by browsing the web as there is lots of misinformation around or\nspecial cases are described as simulated annealing, or the base Markov chain\nis not a reversible Markov chain, etc.  See the references at the end of this\ntranscript for good links if you want to learn about the method.)</p>\n<p>The final method is called <em>Simulated Annealing</em>.\nIt is a method to convert a Markov chain into an optimization method. As\nsimple as that, a Markov chain into an optimization method. It was proposed in\n1983 by <a href=\"http://www.cs.huji.ac.il/~kirk/\">Scott Kirkpatrick</a> and co-workers\nand it is a very simple and often quite effective optimization method. And\nsimple to implement. It can optimize over complex state spaces. And for that\nreason, it is very popular. So this\n<a href=\"http://www.csb.pitt.edu/BBSI/2006/jc_talk/cheng.pdf\">Science paper that they published in\n1983</a> has 28,000\ncitations (now 35,000, October 2015). And interestingly, Scott Kirkpatrick\nlater in the '90s worked at the IBM T.J.  Watson Research Center and there, at\nleast he writes this, he invented the first pen-based tablet computer. So it\nis nice to see that he is innovative on quite different levels.</p>\n<p>So how does it work? Say we have a function that we want to optimize. A very\nsimple function here. There are only 40 possible inputs to that function. So\nin that case, we could simply enumerate all the 40 possible states and pick\nthe one that is maximal, so we want to maximize that function. But imagine you\nhave a different problem with exponentially many states so we can no longer\nlist them and this is just for illustration. But imagine instead of 40, you\nwould have 2 to the power of 40 or something. What we are going to do is we\nconvert that function into a probability distribution and we do that by what\nis called a Gibbs distribution. So just a simple formula, where <span class=\"math\">\\(Z\\)</span> is a\nnormalizing constant and the formula depends on the parameter <span class=\"math\">\\(T\\)</span>, the so called\ntemperature parameter. If the temperature parameter is very high, you divide\nthat function value by a very large value and the argument almost does not\nmatter. So the function value does not matter. In this case, the temperature\nis 100 and you see that the resulting distribution is almost uniform. Maybe\nhard to see but it is almost uniform because the temperature is quite high\ncompared to the function value.</p>\n<p><strong>Attendee</strong>: What is <span class=\"math\">\\(Z\\)</span>?</p>\n<p><strong>Speaker</strong>: <span class=\"math\">\\(Z\\)</span> is a normalizing constant. So it is just a sum over all the\npossible configurations. But it is a constant and it just depends on <span class=\"math\">\\(T\\)</span>. And\ninterestingly, if we apply this Metropolis algorithm, the <span class=\"math\">\\(Z\\)</span> constant cancels\nout, it is not really important. You do not even need to write it down. You\ncan just write <span class=\"math\">\\(P\\)</span> is proportional to <span class=\"math\">\\(X\\)</span>, (<span class=\"math\">\\(P \\propto X\\)</span>) or something, the\nconstant is just a normalizing constant. If I decrease the temperature, you\nsee, so this is temperature 10, temperature 4, 1, and now I decrease it even\nfurther, 0.1, the distribution puts more and more mass on the function values\nthat are higher; and basically, what the simulated annealing does it runs a\nMarkov chain, a Metropolis chain, for example. But, while it runs it, it\nmodifies it by decreasing the temperature.</p>\n<p>So it tries to shift all the probability mass in our current state as well to\nthe states that have high function value. And how it does it, well, it chooses\nits schedule, a temperature schedule so on the x-axis here I have the steps\nthat I take with the Markov chain and on the y-axis I have the temperature\nthat I use, and I just decrease the temperature here, a geometric schedule so\nI just modify the temperature on each step with 0.99 or something.</p>\n<p>For very high temperatures, the Markov chain is basically just a purely random\nwalk; it does not even look at the function value. For very low temperatures,\nit basically is a local search algorithm; it only accepts improvements in the\nfunction value. But, for intermediate temperature values, it is something in\nbetween so it tries to optimize but it can still escape local minima.</p>\n<p>So that is intuition. There's some theory to it actually, in another famous\npaper by <a href=\"http://www.dam.brown.edu/people/geman/\">Stuart</a> and <a href=\"http://cis.jhu.edu/people/faculty/geman/\">Don Geman</a>, a <a href=\"http://www.csee.wvu.edu/~xinl/library/papers/infor/Geman_Geman.pdf\">1984 paper</a> which is actually famous for a\ndifferent reason because they proposed another famous Monte Carlo method they\n<a href=\"https://en.wikipedia.org/wiki/Gibbs_sampling\">Gibbs sampler</a> in that paper\nbut in that very paper they also have some theory of simulated annealing and\nthey prove that if you decrease the temperature slow enough the probability is\none to obtain the optimal state.  But that optimal schedule is too slow in\npractice so you cannot use it and that is why we are still stuck with the\ngeometric schedule.</p>\n<p>Last minute, let us do simulated annealing and I go back to the more\ncomplicated model where we actually have this two types of elements: the black\nones that attract each other and the white ones which are neutral. And you see\nhere I plotted whenever two black elements are close to each other on this 2-D\ngrid; I plotted a red line, and I am going to try to optimize the number of\nred lines so I try to get as many red connections as possible.</p>\n<p>(This is the video I showed, which is not visible in the slides above.  The\nvideo file is also available as <a href=\"/sebastian/videos/SimulatedAnnealing-HP2D-48.mp4\">MP4/H.264 file</a> (5MB).)</p>\n<video id=\"simulatedannealing\" class=\"video-js vjs-default-skin\"\n    controls preload=\"auto\"\n    width=\"600\" height=\"450\" data-setup=\"{}\">\n<source src=\"/sebastian/videos/SimulatedAnnealing-HP2D-48.mp4\" type='video/mp4'>\n<p class=\"vjs-no-js\">\nTo view this video please enable JavaScript, and\nconsider upgrading to a web browser that\n<a href=\"http://videojs.com/html5-video-support/\" target=\"_blank\">supports HTML5 video</a>\n</p>\n</video>\n\n<p>And that is really a model for protein folding, folding in such a way that\nthere are many black to black noncovalent bonds. So here, is an animation of\nperforming simulated annealing exactly with that proposal that I had, bending\nit at 90 degrees left or right at a random position and, I do 100,000 steps\nand I show you every 100th step. At high temperatures, this is quite high\ntemperatures still you see it is very stretched out, there are not very many\ncompact structures but as the optimization proceeds the temperature decreases\nand it favors more and more compact configuration. So I think already at a\nstep of, like now, you would see already quite some compact structures\nappearing.</p>\n<p>So this is a purely random walk. I think it goes until 1,000. I can skip it in\nthe interest of time but, I can show you the result. This is the configuration\nthat we have obtained with 100,000 steps in our Markov chain in simulated\nannealing and for that model problem there is a paper that analyzes different\nmodel problems and the optimal configuration is known, the ground state is\nknown and the ground state is slightly better. It has 23 connections. We only\nhave 21 but actually, with such a simple method we have obtained a quite good\nsolution and that is really the essence of why simulated annealing is popular.</p>\n<p>We can often get quite far with allegedly fewer effort both in implementation\nand runtime. Although, there may be a better method in a specific domain, that\nis optimized for that. And let us reflect a little bit on what we did. We have\nsolved a rather complicated problem like folding this 2D protein with a very,\nvery simple method with just a random walk that accepts or rejects simple\nmodifications.</p>\n<p>And that is basically it. Before I have my last slide, I just want to say a\nlittle bit about the literature if you are interested in the Monte Carlo\nMethods I can highly recommend the first book and if you are interested in the\nblack and white pictures I showed of the people that are relevant to the\ninvention of the Monte Carlo method I can recommend the last book which is the\nautobiography of Stanislaw Ulam, it is very interesting.</p>\n<p>So, thank you very much for your attention.</p>\n<h2>References</h2>\n<p>Here are some of the introductionary book references mentioned in the talk.</p>\n<p>The historical context and anecdotes are mostly from the autobiography of Stan\nUlam,\n<a href=\"http://www.ucpress.edu/book.php?isbn=9780520071544\">Adventures of a\nMathematician</a>.\nThe book is accessible to anyone with a basic high school math background.\nSee also this <a href=\"http://projecteuclid.org/euclid.bams/1183540384\">kind 1978 review of the\nbook</a>.</p>\n<p>A great, now somewhat dated introduction to Monte Carlo methods is <a href=\"http://www.people.fas.harvard.edu/~junliu/\">Jun\nLiu</a>'s <a href=\"http://www.springer.com/us/book/9780387763699\">Monte Carlo Strategies in\nScientific Computing</a>.  I\nlearned Monte Carlo through this book and it has a spot in my bookshelf that\nis at arm-length from my desk.</p>\n<p>The Liu book is somewhat dated and it covers a lot of ground; a slightly more\nformal but up-to-date Monte Carlo book is\n<a href=\"http://statweb.stanford.edu/~owen/\">Art Owen</a>'s upcoming book <a href=\"http://statweb.stanford.edu/~owen/mc/\">Monte Carlo\ntheory, methods and examples</a>, which is\nexcellent.</p>\n<p>A highly accessible and very well written introduction to Markov chains and\nsimple Monte Carlo methods is\n<a href=\"http://www.math.chalmers.se/~olleh/\">Olle H&auml;ggstr&ouml;m</a>'s\n<a href=\"http://www.cambridge.org/catalogue/catalogue.asp?isbn=0521813573\">Finite Markov Chains and Algorithmic Applications</a>.\nI recommend it if you want a most gentle introduction to the theory behind\nMCMC.\nStill the most authoritative reference on MCMC is the\n<a href=\"http://www.mcmchandbook.net/HandbookTableofContents.html\">Handbook of Markov Chain Monte Carlo</a>.\nIn particular, the first twelve chapters cover are on general methodology and\ncontain a wealth of information not found in other textbooks.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}