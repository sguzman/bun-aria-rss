{
  "title": "Detecting Pedestrians using PyTorch â€“ A Helpful Guide",
  "description": "<p>In this blog post, you will learn how to implement a Pedestrian Detection algorithm using PyTorch.</p><p>Computer vision is a field of computer science that applies artificial intelligence models to understand, reason with as well as synthesize visual information. This visual information is usually in the form of a two-dimensional</p>",
  "link": "https://www.data-blogger.com/pedestrian-detection-using-pytorch/",
  "guid": "622cffd8092f120001a24e20",
  "category": [
    "Machine Learning",
    "Artificial Intelligence",
    "Computer Vision",
    "Python"
  ],
  "dc:creator": "Kevin Jacobs",
  "pubDate": "Fri, 09 Jul 2021 00:00:00 GMT",
  "media:content": "",
  "content:encoded": "<img src=\"https://images.unsplash.com/photo-1519922639192-e73293ca430e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDV8fHBlZGVzdHJpYW5zfGVufDB8fHx8MTY0NzExNjMxMg&ixlib=rb-1.2.1&q=80&w=2000\" alt=\"Detecting Pedestrians using PyTorch &#x2013; A Helpful Guide\"><p>In this blog post, you will learn how to implement a Pedestrian Detection algorithm using PyTorch.</p><p>Computer vision is a field of computer science that applies artificial intelligence models to understand, reason with as well as synthesize visual information. This visual information is usually in the form of a two-dimensional image but can also be in other forms like videos, 3-D meshes, and polyhedrons, etc.</p><p>The most common problem addressed by computer vision is that of image classification i.e. taking an image as input and returning the type of the image. As trivial as it sounds, it was not an easy problem to solve for computers as recently as the beginning of the 21<em>st </em>century. Take the simple example of classifying whether an image is a cat or not. Cats come in various shapes (intra-class variations) and sizes (scale variation), are often found in front of cluttered indoor scenes (background clutter) or partially hidden (occlusion), among a host of other different variations.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2021/07/cats.jpg.webp\" class=\"kg-image\" alt=\"Detecting Pedestrians using PyTorch &#x2013; A Helpful Guide\" loading=\"lazy\"><figcaption>Stock image showing variations in cats: appearance, shape, size and pose</figcaption></figure><p>Classifying images is just the tip of the iceberg when it comes to computer vision as the type of image is often the most basic information we can extract from it. Some other use cases of computer vision are as listed below:</p><ul><li><strong>Object detection: </strong>Determining the position and type of an object (or multiple objects) in an image</li><li><strong>Semantic/Instance segmentation: </strong>Labeling all the pixels corresponding to an object type (semantic) or each occurrence of the object (instance)</li><li><strong>Action recognition: </strong>Given a video, understanding the activity being performed by it</li><li><strong>Pose detection: </strong>Identifying the pose of a human subject in an image/video</li><li><strong>Visual question answering: </strong>Answering questions based on information provided by an image</li><li><strong>Image generation: </strong>Generating novel images based on data/criterion provided to a computer vision model</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2021/07/index.jpg.webp\" class=\"kg-image\" alt=\"Detecting Pedestrians using PyTorch &#x2013; A Helpful Guide\" loading=\"lazy\"><figcaption>Computer vision applications (clockwise from top left): Object detection, Action recognition, Relationship detection among visual entities, 3D mesh generation, Image generation, Semantic segmentation (Image source: [1])</figcaption></figure><p>In the past decade or so there has been tremendous progress in computer vision research and its real-world applications, enabled mostly by the success of deep learning models as well as software libraries which make it easy to implement and deploy these models.</p><p>In particular, Convolution Neural Networks (CNNs) have been extremely successful for computer vision applications. If you are unfamiliar with CNNs, there are many excellent resources on the internet to get started with. In particular, I&#x2019;d recommend the reader to go through the course notes for CS231n (https://cs231n.github.io/) which build up from the fundamentals and provide excellent visualization of convolutions, the fundamental mathematical operations used in CNNs.</p><p>PyTorch is a python library released by Facebook for building and training neural networks. Pytorch is particularly helpful for computer vision since it comes paired with the Torchvision library which provides common CNN architectures, pre-trained models, datasets for easily loading images, as well as many other helpful features to train your computer vision model. Since we will be using the PyTorch framework I&#x2019;d recommend familiarizing yourself with the basics of building a neural network as well as a convolution neural network in PyTorch at https://pytorch.org/tutorials (see references [3], [4]).</p><h2 id=\"an-example-object-detection-using-pytorch\">An example: Object Detection using PyTorch</h2><p>Object detection is the problem of detecting the pixels corresponding to an object among all the pixels that constitute an image. The object detector returns a bounding box which is a rectangle surrounding all the object pixels. Object detection is a fundamental problem in computer vision and finds applications in almost all fields from robotics to autonomous driving to medical imaging.</p><p>Next, I will describe (with code) the steps involved in taking a pre-trained detection model in PyTorch and then finetuning it for your own object detection problem. This tutorial very closely follows and borrows the official PyTorch tutorial [5].</p><p>But unlike the official tutorial, which is focused on instance segmentation, I will address the more accessible problem of object detection.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2021/07/index-1.jpg\" class=\"kg-image\" alt=\"Detecting Pedestrians using PyTorch &#x2013; A Helpful Guide\" loading=\"lazy\"><figcaption>An example of object (pedestrians) detection. Image source: [2]&#xA0;</figcaption></figure><p>For this tutorial, I am going to use images from the Penn-Fudan dataset [2] which you can download at <a href=\"https://www.cis.upenn.edu/~jshi/ped_html\" rel=\"noreferrer noopener nofollow\">https://www.cis.upenn.edu/~jshi/ped_html</a>. This dataset has just two types of objects: (a) Pedestrians (b) Background objects. Since we are only concerned with pedestrians I will disregard background objects for the sake of this tutorial. Let&#x2019;s load an example image with the bounding boxes from the dataset for visualization:</p><pre><code class=\"language-python\">from PIL import Image, ImageDraw\n\n source_img = Image.open('PennFudanPed/PNGImages/FudanPed00001.png')\n draw = ImageDraw.Draw(source_img)\n draw.rectangle(((160, 182), (302, 431)), outline=\"red\")\n draw.rectangle(((420, 171), (535, 486)), outline=\"red\")\n display(source_img)</code></pre><p>In order to facilitate reading in data as well as to load it into the neural network models, PyTorch provides two very helpful classes in the torch.utils.data module: <em>Dataset</em> and <em>Dataloader</em>. The dataset class can be modified to read in the data from our own dataset. The two main member functions in this class are <em>__getitem__()</em> and <em>__len__(). </em>The former tells PyTorch how to load one item from the dataset and the second tells PyTorch how many data points are in the dataset. I have implemented a PyTorch dataset class for the Penn-Fudan dataset:</p><pre><code class=\"language-python\">class PennFudanDataset(object):\n\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n\n        # convert everything into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n\n        return img, target\n\n\n    def __len__(self):\n        return len(self.imgs)\n\n</code></pre><p>Next, I implemented some pre-processing steps which transform the input training images in order to provide the model more data to train on. In particular, I flip half the training images horizontally during training. The Torchvision library provides the <em>transforms</em> module (https://pytorch.org/docs/stable/torchvision/transforms.html) which has functions for this as well as several other pre-processing formulations.</p><pre><code class=\"language-python\">from engine import train_one_epoch, evaluate import utils\nimport transforms as T\n\n\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)</code></pre><p>Now that we have our dataset, we split it into train and test sets and implement the <em>Dataloaders</em> to easily load the data into our object detection model for training and testing:</p><pre><code class=\"language-python\">dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)</code></pre><p>Now that we are done with processing the dataset, I&#x2019;ll move onto the actual model used for object detection. I will be using the Faster-RCNN [6] model which is available in the <em>torchvision.models.detection</em> module.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2021/07/index-2.jpg.webp\" class=\"kg-image\" alt=\"Detecting Pedestrians using PyTorch &#x2013; A Helpful Guide\" loading=\"lazy\"><figcaption>The Faster R-CNN architecture. Image source: [6]</figcaption></figure><p>The exact configuration of Faster R-CNN and its components is beyond the scope of this tutorial (as well as not necessary for getting started). However, the important thing to note is that unlike a classifier which has one final output &#x2018;head&#x2019;, a detector has two output heads: (a) the detector, which returns the four vertices of the bounding boxes of the detected objects, and (b) the classifier, which returns the output class of the objects. &#xA0;Since we only have two classes in the Penn-Fudan dataset I&#x2019;ve replaced the classifier (<em>torchvision.models.detection.faster_rcnn.box_predictor</em>) with a new classifier with 2 output classes using <em>torchvision.models.detection.faster_rcnn.FastRCNNPredictor</em> class.</p><pre><code class=\"language-python\">import torchvision from torchvision.models.detection.faster_rcnnfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\n      \ndef get_object_detection_model(num_classes):\n    # load a Faster-RCNN object detection model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    \n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\n    return model</code></pre><p>Now that I have my dataset and model ready, I&#x2019;m going to use some of the training utities available at (<em><a href=\"https://github.com/pytorch/vision/tree/master/references\" rel=\"noreferrer noopener nofollow\">https://github.com/pytorch/vision/tree/master/references</a>) </em>to train our model for 25 epochs:</p><pre><code class=\"language-python\">num_epochs = 25\n\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)</code></pre><p>Once trained, we can visualize the results of our model using it in evaluation mode with PyTorch&#x2019;s <em>model.eval() </em>functionality. You&#x2019;ll notice that just after 25 epochs on a small dataset like Penn-Fudan our model is able to get good object detection results.</p><pre><code class=\"language-python\"># pick one image from the test setimg, _ = dataset_test[0]\n\n\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])</code></pre><pre><code class=\"language-python\">print(prediction)\n\nbbox_1 = prediction[0]['boxes'].cpu().numpy()[0]\nbbox_2 = prediction[0]['boxes'].cpu().numpy()[1]\n\n\n# convert the image, which has been rescaled to 0-1 and had the channels flipped\npred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n\n\ndraw = ImageDraw.Draw(pred_img)\ndraw.rectangle(((bbox_1[0], bbox_1[1]), (bbox_1[2], bbox_1[3])), outline=\"red\")\ndraw.rectangle(((bbox_2[0], bbox_2[1]), (bbox_2[2], bbox_2[3])), outline=\"red\")\ndisplay(pred_img)</code></pre><p>And there you have it folks: now you know how to take your own dataset, finetune an object detection model on it and then perform object detection using PyTorch and Torchvision.</p><p>This blog post is written by Shashank Shekhar.</p><h2 id=\"references\">References</h2><p>[1] CS231n: Convolutional Neural Networks for Visual Recognition (<a href=\"http://cs231n.stanford.edu/index.html\" rel=\"noreferrer noopener nofollow\">http://cs231n.stanford.edu/index.html</a>)<br>[2] The Penn-Fudan Database (<a href=\"https://www.cis.upenn.edu/~jshi/ped_html/\" rel=\"noreferrer noopener nofollow\">https://www.cis.upenn.edu/~jshi/ped_html/</a>)<br>[3] PyTorch tutorial on Neural Networks (<a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\" rel=\"noreferrer noopener nofollow\">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py</a>)<br>[4] PyTorch tutorial on Training a classifier (<a href=\"https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\" rel=\"noreferrer noopener nofollow\">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py</a>)<br>[5] Torchvision Object Detection Finetuning Tutorial (<a href=\"https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\" rel=\"noreferrer noopener nofollow\">https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html</a>)<br>[6] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (<a href=\"https://arxiv.org/pdf/1506.01497.pdf\" rel=\"noreferrer noopener nofollow\">https://arxiv.org/pdf/1506.01497.pdf</a>)</p>"
}