{
  "title": "Data Bandwidth",
  "link": "",
  "updated": "2015-12-29T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/12/29/data-bandwidth",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr: We list and combine common bandwidths relevant in data science</strong></p>\n\n<p>Understanding data bandwidths helps us to identify bottlenecks and write\nefficient code.  Both hardware and software can be characterized by how\nquickly they churn through data.  We present a rough list of relevant data\nbandwidths and discuss how to use this list when optimizing a data pipeline.</p>\n\n<table>\n  <thead>\n    <tr> <th>Name</th> <th>Bandwidth MB/s</th> </tr>\n  </thead>\n<tbody>\n  <tr> <td> Memory copy </td> <td> 3000 </td> </tr>\n  <tr> <td> Basic filtering in C/NumPy/Pandas </td> <td> 3000 </td> </tr>\n  <tr> <td> Fast decompression </td> <td> 1000 </td> </tr>\n  <tr> <td> SSD Large Sequential Read</td> <td> 500 </td> </tr>\n  <tr> <td> Interprocess communication (IPC) </td> <td> 300 </td> </tr>\n  <tr> <td> msgpack deserialization </td> <td> 125 </td> </tr>\n  <tr> <td> Gigabit Ethernet </td> <td> 100 </td> </tr>\n  <tr> <td> Pandas read_csv </td> <td> 100 </td> </tr>\n  <tr> <td> JSON Deserialization </td> <td> 50 </td> </tr>\n  <tr> <td> Slow decompression (e.g. gzip/bz2) </td> <td> 50 </td> </tr>\n  <tr> <td> SSD Small Random Read </td> <td> 20 </td> </tr>\n  <tr> <td> Wireless network </td> <td> 1 </td> </tr>\n</tbody>\n</table>\n\n<p><em>Disclaimer: all numbers in this post are rule-of-thumb and vary by situation</em></p>\n\n<p>Understanding these scales can help you to identify how to speed up your\nprogram.  For example, there is no need to use a faster network or disk\nif you store your data as JSON.</p>\n\n<h2 id=\"combining-bandwidths\">Combining bandwidths</h2>\n\n<p>Complex data pipelines involve many stages.  The rule to combine bandwidths is\nto add up the inverses of the bandwidths, then take the inverse again:</p>\n\n\\[\\textrm{total bandwidth} = \\left(\\sum_i \\frac{1}{x_i}\\right)^{-1}\\]\n\n<p>This is the same principle behind adding conductances in serial within\nelectrical circuits.   One quickly learns to optimize the slowest link in the\nchain first.</p>\n\n<h3 id=\"example\">Example</h3>\n\n<p>When we read data from disk (500 MB/s) and then deserialize it from JSON (50 MB/s)\nour full bandwidth is 45 MB/s:</p>\n\n\\[\\left(\\frac{1}{500} + \\frac{1}{50}\\right)^{-1} = 45.4\\]\n\n<p>If we invest in a faster hard drive system that has 2GB of read\nbandwidth then we get only marginal performance improvement:</p>\n\n\\[\\left(\\frac{1}{2000} + \\frac{1}{50}\\right)^{-1} = 48.8\\]\n\n<p>However if we invest in a faster serialization technology, like msgpack (125\nMB/s), then we double our effective bandwidth.</p>\n\n\\[\\left(\\frac{1}{500} + \\frac{1}{125}\\right)^{-1} = 100\\]\n\n<p>This example demonstrates that we should focus on the weakest bandwidth first.\nCheap changes like switching from JSON to msgpack can be more effective than\nexpensive changes, like purchasing expensive hardware for fast storage.</p>\n\n<h3 id=\"overlapping-bandwidths\">Overlapping Bandwidths</h3>\n\n<p>We can overlap certain classes of bandwidths.  In particular we can often\noverlap communication bandwidths with computation bandwidths.  In our disk+JSON\nexample above we can probably hide the disk reading time completely.  The same\nwould go for network applications <em>if</em> we handle sockets correctly.</p>\n\n<h3 id=\"parallel-bandwidths\">Parallel Bandwidths</h3>\n\n<p>We can parallelize some computational bandwidths.  For example we can\nparallelize JSON deserialization by our number of cores to quadruple the\neffective bandwidth <code class=\"language-plaintext highlighter-rouge\">50 MB/s * 4 = 200 MB/s</code>.  Typically communication\nbandwidths are not parallelizable per core.</p>"
}