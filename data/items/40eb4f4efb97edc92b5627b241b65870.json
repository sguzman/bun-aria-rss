{
  "title": "A Brief Introduction to BERT",
  "link": "https://machinelearningmastery.com/a-brief-introduction-to-bert/",
  "comments": "https://machinelearningmastery.com/a-brief-introduction-to-bert/#respond",
  "dc:creator": "Adrian Tam",
  "pubDate": "Fri, 28 Oct 2022 03:51:09 +0000",
  "category": "Attention",
  "guid": "https://machinelearningmastery.com/?p=13975",
  "description": "<p>Last Updated on November 2, 2022 As we learned what a Transformer is and how we might train the Transformer model, we notice that it is a great tool to make a computer understand human language. However, the Transformer was originally designed as a model to translate one language to another. If we repurpose it [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/a-brief-introduction-to-bert/\">A Brief Introduction to BERT</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "content:encoded": "<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13975 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Brief+Introduction+to+BERT&url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Brief+Introduction+to+BERT&url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p id=\"last-modified-info\">Last Updated on November 2, 2022</p>\n<p>As we learned <a href=\"https://machinelearningmastery.com/the-transformer-model/\">what a Transformer is</a> and how we might <a href=\"https://machinelearningmastery.com/training-the-transformer-model/\">train the Transformer model</a>, we notice that it is a great tool to make a computer understand human language. However, the Transformer was originally designed as a model to translate one language to another. If we repurpose it for a different task, we would likely need to retrain the whole model from scratch. Given the time it takes to train a Transformer model is enormous, we would like to have a solution that enables us to readily reuse the trained Transformer for many different tasks. BERT is such a model. It is an extension of the encoder part of a Transformer.</p>\n<p>In this tutorial, you will learn what BERT is and discover what it can do.</p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>What is a Bidirectional Encoder Representations from Transformer (BERT)</li>\n<li>How a BERT model can be reused for different purposes</li>\n<li>How you can use a pre-trained BERT model</li>\n</ul>\n<p>Let’s get started.<span class=\"Apple-converted-space\"> </span></p>\n<div id=\"attachment_13877\" style=\"width: 810px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-13877\" class=\"wp-image-13877 size-large\" src=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/samet-erkoseoglu-B0nUaoWnr0M-unsplash-scaled.jpg\" alt=\"\" width=\"800\" /><p id=\"caption-attachment-13877\" class=\"wp-caption-text\">A brief introduction to BERT<br />Photo by <a href=\"https://unsplash.com/photos/B0nUaoWnr0M\">Samet Erköseoğlu</a>, some rights reserved.</p></div>\n<h2><b>Tutorial Overview</b></h2>\n<p>This tutorial is divided into four parts; they are:</p>\n<ul>\n<li>From Transformer Model to BERT</li>\n<li>What Can BERT Do?</li>\n<li>Using Pre-Trained BERT Model for Summarization</li>\n<li>Using Pre-Trained BERT Model for Question-Answering</li>\n</ul>\n<h2><b>Prerequisites</b></h2>\n<p>For this tutorial, we assume that you are already familiar with:</p>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/the-transformer-model/\">The theory behind the Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">An implementation of the Transformer model</a></li>\n</ul>\n<h2><b>From Transformer Model to BERT</b></h2>\n<p>In the transformer model, the encoder and decoder are connected to make a seq2seq model in order for you to perform a translation, such as from English to German, as you saw before. Recall that the attention equation says:</p>\n<p>$$\\text{attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^\\top}{\\sqrt{d_k}}\\Big)V$$</p>\n<p>But each of the $Q$, $K$, and $V$ above is an embedding vector transformed by a weight matrix in the transformer model. Training a transformer model means finding these weight matrices. Once the weight matrices are learned, the transformer becomes a <strong>language model,</strong> which means it represents a way to understand the language that you used to train it.</p>\n<div id=\"attachment_12821\" style=\"width: 379px\" class=\"wp-caption aligncenter\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"><img aria-describedby=\"caption-attachment-12821\" loading=\"lazy\" class=\"wp-image-12821\" src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" alt=\"\" width=\"369\" height=\"520\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png 727w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-213x300.png 213w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png 768w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-1090x1536.png 1090w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png 1320w\" sizes=\"(max-width: 369px) 100vw, 369px\" /></a><p id=\"caption-attachment-12821\" class=\"wp-caption-text\">The encoder-decoder structure of the Transformer architecture <br />Taken from &#8220;<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>&#8220;</p></div>\n<p>A transformer has encoder and decoder parts. As the name implies, the encoder transforms sentences and paragraphs into an internal format (a numerical matrix) that understands the context, whereas the decoder does the reverse. Combining the encoder and decoder allows a transformer to perform seq2seq tasks, such as translation. If you take out the encoder part of the transformer, it can tell you something about the context, which can do something interesting.</p>\n<p>The Bidirectional Encoder Representation from Transformer (BERT) leverages the attention model to get a deeper understanding of the language context. BERT is a stack of many encoder blocks. The input text is separated into tokens as in the transformer model, and each token will be transformed into a vector at the output of BERT.</p>\n<h2><b>What Can BERT Do?</b></h2>\n<p>A BERT model is trained using the <strong>masked language model</strong> (MLM) and <strong>next sentence prediction</strong> (NSP) simultaneously.</p>\n<div id=\"attachment_13980\" style=\"width: 280px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-13980\" loading=\"lazy\" class=\"wp-image-13980 size-full\" src=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/BERT.png\" alt=\"\" width=\"270\" height=\"225\" /><p id=\"caption-attachment-13980\" class=\"wp-caption-text\">BERT model</p></div>\n<p>Each training sample for BERT is a pair of sentences from a document. The two sentences can be consecutive in the document or not. There will be a <code>[CLS]</code> token prepended to the first sentence (to represent the <strong>class</strong>) and a <code>[SEP]</code> token appended to each sentence (as a <strong>separator</strong>). Then, the two sentences will be concatenated as a sequence of tokens to become a training sample. A small percentage of the tokens in the training sample is <em>masked</em> with a special token <code>[MASK]</code> or replaced with a random token.</p>\n<p>Before it is fed into the BERT model, the tokens in the training sample will be transformed into embedding vectors, with the positional encodings added, and particular to BERT, with <strong>segment embeddings</strong> added as well to mark whether the token is from the first or the second sentence.</p>\n<p>Each input token to the BERT model will produce one output vector. In a well-trained BERT model, we expect:</p>\n<ul>\n<li>output corresponding to the masked token can reveal what the original token was</li>\n<li>output corresponding to the <code>[CLS]</code> token at the beginning can reveal whether the two sentences are consecutive in the document</li>\n</ul>\n<p>Then, the weights trained in the BERT model can understand the language context well.</p>\n<p>Once you have such a BERT model, you can use it for many <strong>downstream tasks</strong>. For example, by adding an appropriate classification layer on top of an encoder and feeding in only one sentence to the model instead of a pair, you can take the class token <code>[CLS]</code> as input for sentiment classification. It works because the output of the class token is trained to aggregate the attention for the entire input.</p>\n<p>Another example is to take a question as the first sentence and the text (e.g., a paragraph) as the second sentence, then the output token from the second sentence can mark the position where the answer to the question rested. It works because the output of each token reveals some information about that token in the context of the entire input.</p>\n<h2>Using Pre-Trained BERT Model for Summarization</h2>\n<p>A transformer model takes a long time to train from scratch. The BERT model would take even longer. But the purpose of BERT is to create one model that can be reused for many different tasks.</p>\n<p>There are pre-trained BERT models that you can use readily. In the following, you will see a few use cases. The text used in the following example is from:</p>\n<ul>\n<li><a href=\"https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10\">https://www.project-syndicate.org/commentary/bank-of-england-gilt-purchases-necessary-but-mistakes-made-by-willem-h-buiter-and-anne-c-sibert-2022-10</a></li>\n</ul>\n<p>Theoretically, a BERT model is an encoder that maps each input token to an output vector, which can be extended to an infinite length sequence of tokens. In practice, there are limitations imposed in the implementation of other components that limit the input size. Mostly, a few hundred tokens should work, as not every implementation can take thousands of tokens in one shot. You can save the entire article in <code>article.txt</code> (a copy is available <a href=\"https://machinelearningmastery.com/wp-content/uploads/2022/10/article.txt\">here</a>). In case your model needs a smaller text, you can use only a few paragraphs from it.</p>\n<p>First, let&#8217;s explore the task for summarization. Using BERT, the idea is to <em>extract</em> a few sentences from the original text that represent the entire text. You can see this task is similar to next sentence prediction, in which if given a sentence and the text, you want to classify if they are related.</p>\n<p>To do that, you need to use the Python module <code>bert-extractive-summarizer</code></p><pre class=\"urvanov-syntax-highlighter-plain-tag\">pip install bert-extractive-summarizer</pre><p>It is a wrapper to some Hugging Face models to provide the summarization task pipeline. Hugging Face is a platform that allows you to publish machine learning models, mainly on NLP tasks.</p>\n<p>Once you have installed <code>bert-extractive-summarizer</code>, producing a summary is just a few lines of code:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from summarizer import Summarizer\ntext = open(\"article.txt\").read()\nmodel = Summarizer('distilbert-base-uncased')\nresult = model(text, num_sentences=3)\nprint(result)</pre><p>This gives the output:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Amid the political turmoil of outgoing British Prime Minister Liz Truss’s\nshort-lived government, the Bank of England has found itself in the\nfiscal-financial crossfire. Whatever government comes next, it is vital\nthat the BOE learns the right lessons. According to a statement by the BOE’s Deputy Governor for\nFinancial Stability, Jon Cunliffe, the MPC was merely “informed of the\nissues in the gilt market and briefed in advance of the operation,\nincluding its financial-stability rationale and the temporary and targeted\nnature of the purchases.”</pre><p>That&#8217;s the complete code! Behind the scene, spaCy was used on some preprocessing, and Hugging Face was used to launch the model. The model used was named <code>distilbert-base-uncased</code>. DistilBERT is a simplified BERT model that can run faster and use less memory. The model is an &#8220;uncased&#8221; one, which means the uppercase or lowercase in the input text is considered the same once it is transformed into embedding vectors.</p>\n<p>The output from the summarizer model is a string. As you specified <code>num_sentences=3</code> in invoking the model, the summary is three selected sentences from the text. This approach is called the <strong>extractive summary</strong>. The alternative is an <strong>abstractive summary</strong>, in which the summary is generated rather than extracted from the text. This would need a different model than BERT.</p>\n<p><strong>Kick-start your project</strong> with my book <a href=\"https://machinelearningmastery.com/transformer-models-with-attention/\">Building Transformer Models with Attention</a>. It provides <strong>self-study tutorials</strong> with <strong>working code</strong> to guide you into building a fully-working transformer models that can<br><em>translate sentences from one language to another</em>...</p>\n<h2>Using Pre-Trained BERT Model for Question-Answering</h2>\n<p>The other example of using BERT is to match questions to answers. You will give both the question and the text to the model and look for the output of the beginning <em>and</em> the end of the answer from the text.</p>\n<p>A quick example would be just a few lines of code as follows, reusing the same example text as in the previous example:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from transformers import pipeline\ntext = open(\"article.txt\").read()\nquestion = \"What is BOE doing?\"\n\nanswering = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\nresult = answering(question=question, context=text)\nprint(result)</pre><p>Here, Hugging Face is used directly. If you have installed the module used in the previous example, the Hugging Face Python module is a dependence that you already installed. Otherwise, you may need to install it with <code>pip</code>:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">pip install transformers</pre><p>And to actually use a Hugging Face model, you should have <strong>both</strong> PyTorch and TensorFlow installed as well:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">pip install torch tensorflow</pre><p>The output of the code above is a Python dictionary, as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">{'score': 0.42369240522384644,\n'start': 1261,\n'end': 1344,\n'answer': 'to maintain or restore market liquidity in systemically important\\nfinancial markets'}</pre><p>This is where you can find the answer (which is a sentence from the input text), as well as the begin and end position in the token order where this answer was from. The score can be regarded as the confidence score from the model that the answer could fit the question.</p>\n<p>Behind the scenes, what the model did was generate a probability score for the best beginning in the text that answers the question, as well as the text for the best ending. Then the answer is extracted by finding the location of the highest probabilities.</p>\n<h2><b>Further Reading</b></h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3><b>Papers</b></h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017</li>\n<li><a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding</a>, 2019</li>\n<li><a href=\"https://arxiv.org/abs/1910.01108\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter</a>, 2019</li>\n</ul>\n<h2><b>Summary</b></h2>\n<p>In this tutorial, you discovered what BERT is and how to use a pre-trained BERT model.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How is BERT created as an extension to Transformer models</li>\n<li>How to use pre-trained BERT models for extractive summarization and question answering</li>\n</ul>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13975 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Brief+Introduction+to+BERT&url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Brief+Introduction+to+BERT&url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/a-brief-introduction-to-bert/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/a-brief-introduction-to-bert/\">A Brief Introduction to BERT</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "wfw:commentRss": "https://machinelearningmastery.com/a-brief-introduction-to-bert/feed/",
  "slash:comments": 0
}