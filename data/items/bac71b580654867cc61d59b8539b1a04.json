{
  "title": "The Softmax Function Derivative (Part 3)",
  "link": "https://aimatters.wordpress.com/2020/07/01/the-softmax-function-derivative-part-3/",
  "comments": "https://aimatters.wordpress.com/2020/07/01/the-softmax-function-derivative-part-3/#respond",
  "dc:creator": "Stephen Oman",
  "pubDate": "Wed, 01 Jul 2020 21:00:34 +0000",
  "category": [
    "Artificial Intelligence",
    "Examples",
    "Machine Intelligence",
    "Machine Learning",
    "machine learning algorithms",
    "neural networks"
  ],
  "guid": "http://aimatters.wordpress.com/?p=1292",
  "description": "Previously I&#8217;ve shown how to work out the derivative of the Softmax Function combined with the summation function, typical in artificial neural networks. In this final part, we&#8217;ll look at how the weights in a Softmax layer change in respect to a Loss Function. The Loss Function is a measure of how &#8220;bad&#8221; the estimate [&#8230;]",
  "content:encoded": "\n<p>Previously I&#8217;ve shown how to work out the <a href=\"https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\">derivative of the Softmax Function</a> combined with <a href=\"https://aimatters.wordpress.com/2020/06/14/derivative-of-softmax-layer/\">the summation function</a>, typical in artificial neural networks.</p>\n\n\n\n<p>In this final part, we&#8217;ll look at how the weights in a Softmax layer change in respect to a Loss Function.  The Loss Function is a measure of how &#8220;bad&#8221; the estimate from the network is.  We&#8217;ll then be modifying the weights in the network in order to improve the &#8220;Loss&#8221;, i.e. make it less bad.</p>\n\n\n\n<p>The Python code is based on the excellent article by Eli Bendersky which can be found <a rel=\"noreferrer noopener\" href=\"https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\" target=\"_blank\">here</a>.</p>\n\n\n\n<h2>Cross Entropy Loss Function</h2>\n\n\n\n<p>There are different kinds Cross Entropy functions depending on what kind of classification that you want your network to estimate.  In this example, we&#8217;re going to use the Categorical Cross Entropy.  This function is typically used when the network is required to estimate which class something belongs to, when there are many classes.  The output of the Softmax Function is a vector of probabilities, each element represents the network&#8217;s estimate that the input is in that class. For example:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\n&#91;0.19091352 0.20353145 0.21698333 0.23132428 0.15724743]\n</pre></div>\n\n\n<p>The first element, 0.19091352, represents the network&#8217;s estimate that the input is in the first class, and so on.</p>\n\n\n\n<p>Usually, the input is in one class, and we can represent the correct class for an input as a one-hot vector.  In other words, the class vector is all zeros, except for a 1 in the index corresponding to the class. </p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\n&#91;0 0 1 0 0]\n</pre></div>\n\n\n<p>In this example, the input is in class 3, represented by a 1 in the third element.</p>\n\n\n\n<p>The multi-class Cross Entropy Function is defined as follows:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=-%5Csum_%7Bc%3D1%7D%5EM%3Dy_%7Bo%2Cc%7D+%5Ctextup%7B+log%7D%28S_%7Bo%2Cc%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=-%5Csum_%7Bc%3D1%7D%5EM%3Dy_%7Bo%2Cc%7D+%5Ctextup%7B+log%7D%28S_%7Bo%2Cc%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=-%5Csum_%7Bc%3D1%7D%5EM%3Dy_%7Bo%2Cc%7D+%5Ctextup%7B+log%7D%28S_%7Bo%2Cc%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"-&#92;sum_{c=1}^M=y_{o,c} &#92;textup{ log}(S_{o,c})\" class=\"latex\" /></p>\n\n\n\n<p>where M is the number of classes, y is the one-hot vector representing the correct classification c for the observation o (i.e. the input). S is the Softmax output for the class c for the observation o.  Here is some code to calculate that (which continues from my previous posts on this topic):</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ndef x_entropy(y, S):\n    return np.sum(-1 * y * np.log(S))\n\ny = np.zeros(5)\ny&#91;2] = 1   # picking the third class for example purposes\nxe = x_entropy(y, S)\nprint(xe)\n\n1.5279347484961026\n</pre></div>\n\n\n<h2>Cross Entropy Derivative</h2>\n\n\n\n<p>Just like the other derivatives we&#8217;ve looked at before, the Cross-Entropy derivative is a vector of partial derivatives with respect to it&#8217;s input:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B++%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B1%7D%7D+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B2%7D%7D+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D++%5Cright%5D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B++%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B1%7D%7D+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B2%7D%7D+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D++%5Cright%5D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B++%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B1%7D%7D+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7B2%7D%7D+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D++%5Cright%5D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S} = &#92;left[  &#92;frac{&#92;delta XE}{&#92;delta S_{1}} &#92;frac{&#92;delta XE}{&#92;delta S_{2}} &#92;ldots &#92;frac{&#92;delta XE}{&#92;delta S_{t}}  &#92;right] \" class=\"latex\" /></p>\n\n\n\n<p>We can make this a little simpler by observing that since Y (i.e. the ground truth classification vector) is zeros, except for the target class, c, then the Cross Entropy derivative vector is also going to be zeros, except for the class c.</p>\n\n\n\n<p>To see why this is the case, let&#8217;s examine the Cross Entropy function itself. We calculate it by summing up a product.  Each product is the value from Y multiplied by the log of the corresponding value from S. Since all the elements in Y are actually 0 (except for the target class, c), then the corresponding derivative will also be 0. No matter how much we change the values in S, the result will still be 0.</p>\n\n\n\n<p>Therefore:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+XE%7D%7B%5Cdelta+S_%7Bt%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S} = &#92;left[ &#92;ldots &#92;frac{&#92;delta XE}{&#92;delta S_{t}} &#92;ldots &#92;right]\" class=\"latex\" /></p>\n\n\n\n<p>We can rewrite this a little, expanding out the XE function:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%28Y_%7Bc%7D%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%28Y_%7Bc%7D%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%28Y_%7Bc%7D%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S} = &#92;left[ &#92;ldots &#92;frac{&#92;delta -(Y_{c}&#92;textup{log}(S_{c}))}{&#92;delta S_{c}} &#92;ldots &#92;right]\" class=\"latex\" /></p>\n\n\n\n<p>We already know that <img src=\"https://s0.wp.com/latex.php?latex=Y_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Y_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Y_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"Y_{c}\" class=\"latex\" /> is 1, so we are left with:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+%5Cfrac%7B%5Cdelta+-%5Ctextup%7Blog%7D%28S_%7Bc%7D%29%7D%7B%5Cdelta+S_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S} = &#92;left[ &#92;ldots &#92;frac{&#92;delta -&#92;textup{log}(S_{c})}{&#92;delta S_{c}} &#92;ldots &#92;right]\" class=\"latex\" /></p>\n\n\n\n<p>So we are just looking for the derivative of the log of <img src=\"https://s0.wp.com/latex.php?latex=S_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=S_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S_%7Bc%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"S_{c}\" class=\"latex\" />:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%3D+%5Cleft%5B+%5Cldots+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Cldots+%5Cright%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S} = &#92;left[ &#92;ldots -&#92;frac{1}{S_{c}} &#92;ldots &#92;right]\" class=\"latex\" /></p>\n\n\n\n<p>The rest of the elements in the vector will be 0. Here is the code that works that out:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ndef xe_dir(y, S):\n    return (-1 / S) * y\n\nDXE = xe_dir(y, S)\nprint(DXE)\n\n&#91;-0.      -0.      -4.60864 -0.      -0.     ]\n</pre></div>\n\n\n<h2>Bringing it all together</h2>\n\n\n\n<p>When we have a neural network layer, we want to change the weights in order to make the loss as small as possible. So we are trying to calculate:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta W}\" class=\"latex\" /></p>\n\n\n\n<p>for each of the input instances X. Since XE is a function that depends on the Softmax function, which itself depends on the summation function in the neurons, we can use the calculus chain rule as follows:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D+%3D+%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%5Ccdot+%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D+%5Ccdot+%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D+%3D+%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%5Ccdot+%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D+%5Ccdot+%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+W%7D+%3D+%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D+%5Ccdot+%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D+%5Ccdot+%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta W} = &#92;frac{&#92;Delta XE}{&#92;Delta S} &#92;cdot &#92;frac{&#92;Delta S}{&#92;Delta Z} &#92;cdot &#92;frac{&#92;Delta Z}{&#92;Delta W}\" class=\"latex\" /></p>\n\n\n\n<p>In this post, we&#8217;ve calculated <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+XE%7D%7B%5CDelta+S%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta XE}{&#92;Delta S}\" class=\"latex\" /> and in the previous posts, we calculated <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+Z%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta S}{&#92;Delta Z}\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+Z%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta Z}{&#92;Delta W}\" class=\"latex\" />. To calculate the overall changes to the weights, we simply carry out a dot product of all those matrices:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nprint(np.dot(DXE, DL_shortcut).reshape(W.shape))\n\n&#91;&#91; 0.01909135  0.09545676  0.07636541  0.02035314  0.10176572]\n &#91; 0.08141258 -0.07830167 -0.39150833 -0.31320667  0.02313243]\n &#91; 0.11566214  0.09252971  0.01572474  0.07862371  0.06289897]]\n</pre></div>\n\n\n<h2>Shortcut</h2>\n\n\n\n<p>Now that we&#8217;ve seen how to calculate the individual parts of the derivative, we can now look to see if there is a shortcut that avoids all that matrix multiplication, especially since there are lots of zeros in the elements.</p>\n\n\n\n<p>Previously, we had established that the elements in the matrix <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5CDelta+S%7D%7B%5CDelta+W%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;Delta S}{&#92;Delta W}\" class=\"latex\" /> can be calculated using:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{S_{t}}}{&#92;delta{W_{ij}}} = S_{t}(1-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>where the input and output indices are the same, and</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS_%7Bt%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{S_{t}}}{&#92;delta{W_{ij}}} = S_{t}(0-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>where they are different. </p>\n\n\n\n<p>Using this result, we can see that an element in the derivative of the Cross Entropy function XE, with respect to the weights W is (swapping c for t):</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BS_%7Bc%7D%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BS_%7Bc%7D%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BS_%7Bc%7D%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{XE_{c}}}{&#92;delta{W_{ij}}} = &#92;frac{&#92;delta{XE_{c}}}{&#92;delta{S_{c}}} &#92;cdot S_{c}(1-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>We&#8217;ve shown above that the derivative of XE with respect to S is just <img src=\"https://s0.wp.com/latex.php?latex=-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"-&#92;frac{1}{S_{c}}\" class=\"latex\" />. So each element in the derivative where i = c becomes:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+-%5Cfrac%7B1%7D%7BS_%7Bc%7D%7D+%5Ccdot+S_%7Bc%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{XE_{c}}}{&#92;delta{W_{ij}}} = -&#92;frac{1}{S_{c}} &#92;cdot S_{c}(1-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>This simplifies to:</p>\n\n\n\n<p> <img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D-1%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D-1%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D-1%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{XE_{c}}}{&#92;delta{W_{ij}}} = (S_{i}-1)x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>Similarly, where i <> c:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BXE_%7Bc%7D%7D%7D%7B%5Cdelta%7BW_%7Bij%7D%7D%7D+%3D+%28S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{XE_{c}}}{&#92;delta{W_{ij}}} = (S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>Here is the corresponding Python code for that:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ndef xe_dir_shortcut(W, S, x, y):\n    dir_matrix = np.zeros((W.shape&#91;0] * W.shape&#91;1]))\n    \n    for i in range(0, W.shape&#91;1]):\n        for j in range(0, W.shape&#91;0]):\n            dir_matrix&#91;(i*W.shape&#91;0]) + j] = (S&#91;i] - y&#91;i]) * x&#91;j]\n                \n    return dir_matrix\n\ndelta_w = xe_dir_shortcut(W, h, x, y)\n</pre></div>\n\n\n<p>Let&#8217;s verify that this gives us the same results as the longer matrix multiplication above:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nprint(delta_w.reshape(W.shape))\n\n&#91;&#91; 0.01909135  0.09545676  0.07636541  0.02035314  0.10176572]\n &#91; 0.08141258 -0.07830167 -0.39150833 -0.31320667  0.02313243]\n &#91; 0.11566214  0.09252971  0.01572474  0.07862371  0.06289897]]\n</pre></div>\n\n\n<p>Now we have a simple function that will calculate the changes to the weights for a seemingly complicated single-layer of a neural network.</p>\n",
  "wfw:commentRss": "https://aimatters.wordpress.com/2020/07/01/the-softmax-function-derivative-part-3/feed/",
  "slash:comments": 0,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "A single neuron"
    },
    {
      "media:title": "stephenoman"
    }
  ]
}