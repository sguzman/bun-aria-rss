{
  "title": "Learners' Languages. (arXiv:2103.01189v2 [math.CT] UPDATED)",
  "link": "http://arxiv.org/abs/2103.01189",
  "description": "<p>In \"Backprop as functor\", the authors show that the fundamental elements of\ndeep learning -- gradient descent and backpropagation -- can be conceptualized\nas a strong monoidal functor Para(Euc)$\\to$Learn from the category of\nparameterized Euclidean spaces to that of learners, a category developed\nexplicitly to capture parameter update and backpropagation. It was soon\nrealized that there is an isomorphism Learn$\\cong$Para(Slens), where Slens is\nthe symmetric monoidal category of simple lenses as used in functional\nprogramming.\n</p>\n<p>In this note, we observe that Slens is a full subcategory of Poly, the\ncategory of polynomial functors in one variable, via the functor $A\\mapsto\nAy^A$. Using the fact that (Poly,$\\otimes$) is monoidal closed, we show that a\nmap $A\\to B$ in Para(Slens) has a natural interpretation in terms of dynamical\nsystems (more precisely, generalized Moore machines) whose interface is the\ninternal-hom type $[Ay^A,By^B]$.\n</p>\n<p>Finally, we review the fact that the category p-Coalg of dynamical systems on\nany $p \\in$ Poly forms a topos, and consider the logical propositions that can\nbe stated in its internal language. We give gradient descent as an example, and\nwe conclude by discussing some directions for future work.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Spivak_D/0/1/0/all/0/1\">David I. Spivak</a> (Topos Institute)"
}