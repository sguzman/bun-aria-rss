{
  "title": "How to tell if you have trained your Model with enough data ?",
  "link": "https://calculatedcontent.com/2021/07/09/how-to-tell-if-you-have-trained-your-model-with-enough-data/",
  "comments": "https://calculatedcontent.com/2021/07/09/how-to-tell-if-you-have-trained-your-model-with-enough-data/#respond",
  "dc:creator": "Charles H Martin, PhD",
  "pubDate": "Sat, 10 Jul 2021 02:12:21 +0000",
  "category": "Uncategorized",
  "guid": "http://calculatedcontent.com/?p=14103",
  "description": "Deep Neural Networks (DNN) require a lot of training data. Even fine-tuning a model can require a lot. A LOT. &#8230; <a class=\"more-link\" href=\"https://calculatedcontent.com/2021/07/09/how-to-tell-if-you-have-trained-your-model-with-enough-data/\">More</a>",
  "content:encoded": "\n<p>Deep Neural Networks (DNN) require a lot of training data.  Even fine-tuning a model can require a lot.  A LOT.  So how can you know if you have used enough?  For Computer Vision (CV) models, you can always look at the test error.  But what about fine-tuning large, transformer models like BERT or GPT ?</p>\n\n\n\n<ul>\n<li>What is the best metric to evaluate your model ? </li>\n\n\n\n<li>How can you be sure you trained it with enough data ?  </li>\n\n\n\n<li>And how can your customers be sure ?</li>\n</ul>\n\n\n\n<p class=\"has-text-align-left\"><a href=\"https://github.com/CalculatedContent/WeightWatcher\" target=\"_blank\" rel=\"noreferrer noopener\">WeightWatcher</a> can help.</p>\n\n\n\n<p class=\"has-text-align-center\"><code>pip install weightwatcher  </code></p>\n\n\n\n<p>WeightWatcher is an open-source, diagnostic tool for evaluating the performance of (pre)-trained and fine-tuned Deep Neural Networks.  It is based on state-of-the-art research into <em>Why Deep Learning Works</em>.  Recently,<a href=\"https://www.nature.com/articles/s41467-021-24025-8\"> </a><a rel=\"noreferrer noopener\" href=\"https://www.nature.com/articles/s41467-021-24025-8\" target=\"_blank\">it has been featured in Nature:</a></p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png\"><img data-attachment-id=\"14145\" data-permalink=\"https://calculatedcontent.com/screen-shot-2021-07-08-at-10-53-10-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png\" data-orig-size=\"2914,316\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2021-07-08-at-10.53.10-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=1024\" alt=\"\" class=\"wp-image-14145\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=2048 2048w, https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2021/07/screen-shot-2021-07-08-at-10.53.10-pm.png?w=768 768w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<p>Here, we show you how to use WeightWatcher to determine if your DNN model has been trained with enough data.  </p>\n\n\n\n<p>In the paper, we consider the example of GPT vs GPT2.  GPT is a NLP Transformer model, developed by <a href=\"https://openai.com/blog/better-language-models/\">OpenAI,</a> to generate fake text.   When it was first developed, OpenAI released the GPT model, which had specifically been trained with a small data set, making it unusable to generate fake text. Later, they realized fake text is good business, and they released GPT2, which is just like GPT. but trained with enough data to make it useful.  </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><img src=\"https://charlesmartin14.files.wordpress.com/2021/07/e06a6-gpt2.png\" alt=\"TechViz - The Data Science Guy: Data Augmentation in NLP using GPT2\" width=\"529\" height=\"175\" /></figure></div>\n\n\n<p>We can apply WeightWatcher to GPT and GPT2 and compare the results; we will see that the WeightWatcher <em>log</em> <em>spectral norm </em>and <em>alpha (power law)</em> metrics can immediately tell us that something is wrong with the GPT model.  <a rel=\"noreferrer noopener\" href=\"https://www.nature.com/articles/s41467-021-24025-8/figures/6\" target=\"_blank\">This is shown in Figure 6 of the paper;</a> </p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-021-24025-8/MediaObjects/41467_2021_24025_Fig6_HTML.png\" alt=\"Fig. 6\" /></figure>\n\n\n\n<p>Here we will walk through exactly how to do this yourself for the WeightWatcher Power Law (PL) alpha metric <img src=\"https://s0.wp.com/latex.php?latex=%28%5Calpha%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28%5Calpha%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28%5Calpha%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(&#92;alpha)\" class=\"latex\" />, and explain how to interpret these plots.</p>\n\n\n\n<p>It is recommended to run these calculations in a Jupiter notebook, or Google Colab. (For reference, <a rel=\"noreferrer noopener\" href=\"https://github.com/CalculatedContent/ww-trends-2020/blob/master/WeightWatcher-OpenAI-GPT.ipynb\" target=\"_blank\">you can also view the actual notebook</a> used to create the plots in the paper, however, this uses an older version of weightwatcher)</p>\n\n\n\n<p>For this post, <a href=\"https://github.com/CalculatedContent/WeightWatcher/blob/master/WeightWatcher-GPT.ipynb\" target=\"_blank\" rel=\"noreferrer noopener\">we provide a working notebook</a> in the<a href=\"https://github.com/CalculatedContent/WeightWatcher\" target=\"_blank\" rel=\"noreferrer noopener\"> WeightWatcher github repo.</a></p>\n\n\n\n<p> WeightWatcher understands the basic Huggingface models. Indeed, WeightWatcher supports:</p>\n\n\n\n<ul>\n<li>TF2.0 / Keras</li>\n\n\n\n<li>pyTorch 1.x</li>\n\n\n\n<li>HuggingFace</li>\n</ul>\n\n\n\n<p>and (soon)</p>\n\n\n\n<p>ONNX (in the current trunk)</p>\n\n\n\n<p>Currently, we support Dense and Conv2D layers.  Support for more layers is coming.  For our NLP Transformer models, we only need support for the Dense layers.</p>\n\n\n\n<p>First, we need the GPT and GPT2 pyTorch models. We will use the popular HuggingFace  transformers package.</p>\n\n\n\n<div class=\"is-layout-constrained wp-block-group\"><div class=\"wp-block-group__inner-container\"><div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\n!pip install transformers<\n</pre></div></div></div>\n\n\n\n<p>Second, we need to import pyTorch and weightwatcher</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 2; title: ; notranslate\">\nimport torch\nimport weightwatcher as ww<\n</pre></div>\n\n\n<p>We will also want the pandas and matplotlib libraries to help us interpret the weightwatcher metrics.  In Jupyter notebooks, this looks like</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 4; title: ; notranslate\">\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre></div>\n\n\n<p>We now import the transformers package and the 2 model classes</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 9; title: ; notranslate\">\nimport transformers\nfrom transformers import OpenAIGPTModel,GPT2Model\n</pre></div>\n\n\n<p>We have to get the 2 pretrained models, and run <em>model.eval() </em></p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; first-line: 11; title: ; notranslate\">\ngpt_model = OpenAIGPTModel.from_pretrained('openai-gpt')\ngpt_model.eval();\n\ngpt2_model = GPT2Model.from_pretrained('gpt2')\ngpt2_model.eval();\n</pre></div>\n\n\n<p>To analyze our GPT models with WeightWatcher , simply create a watcher instance, and run <em>watcher.analyze(</em>).  This will return a pandas dataframe with the metrics for each layer</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 16; title: ; notranslate\">\nwatcher = ww.WeightWatcher(model=gpt_model)\ngpt_details = watcher.analyze()\n</pre></div>\n\n\n<p>The details dataframes reports quality metrics that can be used to analyze the model performance&#8211;without needing access to test or training data. The most important metric is our Power Law metric <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />.  WeightWatcher reports <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> for every layer.  The GPT model has nearly 50 layers, so it is convenient to examine all the layer alphas at once as a histogram (using the pandas API).</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 18; title: ; notranslate\">\ngpt_details.alpha.plot.hist(bins=100, color='red', alpha=0.5, density=True, label='gpt')\nplt.xlabel(r\"alpha $(\\alpha)$ PL exponent\")\nplt.legend()\n</pre></div>\n\n\n<p>This plots the density of the <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> values for all layers in the GPT model.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png\"><img data-attachment-id=\"14171\" data-permalink=\"https://calculatedcontent.com/image-1/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png\" data-orig-size=\"457,333\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"image-1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png?w=457\" src=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png?w=457\" alt=\"\" class=\"wp-image-14171\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/07/image-1.png 457w, https://charlesmartin14.files.wordpress.com/2021/07/image-1.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/07/image-1.png?w=300 300w\" sizes=\"(max-width: 457px) 100vw, 457px\" /></a></figure>\n\n\n\n<p>From this histogram, we can immediately see 2 problems with the model</p>\n\n\n\n<ul>\n<li>The peak <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Csim+4&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Csim+4&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5Csim+4&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;sim 4\" class=\"latex\" />. which is higher than optimal for a well trained model.</li>\n\n\n\n<li>There are several outliers with <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%3E6&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%3E6&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%3E6&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha>6\" class=\"latex\" />, indicating several poorly trained layers.</li>\n\n\n\n<li>There are no <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 2 \" class=\"latex\" />; when alpha is too small, the layer may be overtrained.</li>\n</ul>\n\n\n\n<p>So knowing nothing about GPT, and having never seen the test or training data, WeightWatcher tells us that this model should never go into production.</p>\n\n\n\n<p>Now let&#8217;s look GPT2, which has the same architecture, but trained with more and better data. Again, we make a <em>watche</em>r instance with the model specified, and just run <em>watcher.analyze()</em></p>\n\n\n\n<pre class=\"wp-block-preformatted\">watcher = ww.WeightWatcher(model=gpt2_model)\ngpt2_details = watcher.analyze()</pre>\n\n\n\n<p>Now let&#8217;s compare the Power Law alpha metrics for GPT and GPT2.  We just create 2 histograms, 1 for each model, and overlay them.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; first-line: 21; title: ; notranslate\">\ngpt_details.alpha.plot.hist(bins=100, color='red', alpha=0.5, density=True, label='gpt')\ngpt2_details.alpha.plot.hist(bins=100, color='green', density=True, label='gpt2')\nplt.xlabel(r\"alpha $(\\alpha)$ PL exponent\")\nplt.legend()\n</pre></div>\n\n\n<p>The layer alphas for GPT are shown in <font color=\"red\"><strong>red</strong></font>, and for GPT2 in <font color=\"green\"><strong>green</strong></font>, and the histograms differ significantly.  For the GPT2, the peak <span style=\"font-size:revert;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%5Csim+3.5+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%5Csim+3.5+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%5Csim+3.5+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha &#92;sim 3.5 \" class=\"latex\" /></span>, and, more importantly, there are no outlier <span style=\"font-size:revert;\"><span style=\"font-size:revert;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3E+6++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3E+6++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3E+6++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha > 6  \" class=\"latex\" /></span></span>. Smaller alphas are better, and the GPT2 model is much better than GPT because it is trained with significantly more and better data.<br><br>The only caveat here is if <span style=\"font-size:revert;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 2 \" class=\"latex\" />;</span> in these cases, the layer is overtrained or overfit in some way.  In GPT and GPT2, we have no alphas that are too small.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png\"><img data-attachment-id=\"14174\" data-permalink=\"https://calculatedcontent.com/image-2/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png\" data-orig-size=\"457,333\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"image-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png?w=457\" src=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png?w=457\" alt=\"\" class=\"wp-image-14174\" srcset=\"https://charlesmartin14.files.wordpress.com/2021/07/image-2.png 457w, https://charlesmartin14.files.wordpress.com/2021/07/image-2.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2021/07/image-2.png?w=300 300w\" sizes=\"(max-width: 457px) 100vw, 457px\" /></a></figure>\n\n\n\n<p>WeightWatcher has many features to help you evaluate your models.  It can do things like</p>\n\n\n\n<ul>\n<li>Help you decide if you have trained it with enough data (as shown here)</li>\n\n\n\n<li>Detect potential layers that are overtrained (as shown<a rel=\"noreferrer noopener\" href=\"https://calculatedcontent.com/2021/04/04/are-your-models-overtrained/\" target=\"_blank\"> in a previous blog</a>)</li>\n\n\n\n<li>Be used to get early stopping criteria (when you can&#8217;t peek at the test data)</li>\n\n\n\n<li>Predict trends in the test accuracies across models and hyperparameters (see <a rel=\"noreferrer noopener\" href=\"https://www.nature.com/articles/s41467-021-24025-8\" target=\"_blank\">our Nature paper</a>, and <a href=\"https://arxiv.org/abs/2106.00734\" target=\"_blank\" rel=\"noreferrer noopener\">our most recent submission</a>).</li>\n</ul>\n\n\n\n<p>and many other things.</p>\n\n\n\n<p>Please give it a try. And if it is useful to you, let me know.</p>\n\n\n\n<p><strong>And if your company needs help with AI, reach out. I provide strategy consulting, mentorship, and hands-on development.&nbsp;#talkToChuck,&nbsp;#theAIguy</strong>.</p>\n",
  "wfw:commentRss": "https://calculatedcontent.com/2021/07/09/how-to-tell-if-you-have-trained-your-model-with-enough-data/feed/",
  "slash:comments": 0,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "41467_2021_24025_fig6_html"
    },
    {
      "media:title": "charlesmartin14"
    },
    "",
    {
      "media:title": "TechViz - The Data Science Guy: Data Augmentation in NLP using GPT2"
    },
    {
      "media:title": "Fig. 6"
    },
    "",
    ""
  ]
}