{
  "title": "Parallel, Disk-Efficient .zip to .gz Conversion",
  "description": "<p>Similar to my last post about needing to <a href=\"https://randyzwitch.com/bulk-loading-postgis/\">merge shapefiles using Postgis</a>, I recently downloaded a bunch of energy data from the federal government. 13,370 files to be exact. While the data size itself isn’t that large (~8GB, compressed), an open-source tool I was looking to evaluate only supports <em>gzip</em> compression instead of the <em>zip</em> compressed files I actually had.</p>",
  "pubDate": "Mon, 18 Jun 2018 00:00:00 +0000",
  "link": "http://randyzwitch.com/zip-to-gzip-conversion-parallel/",
  "guid": "http://randyzwitch.com/zip-to-gzip-conversion-parallel/",
  "content": "<p>Similar to my last post about needing to <a href=\"https://randyzwitch.com/bulk-loading-postgis/\">merge shapefiles using Postgis</a>, I recently downloaded a bunch of energy data from the federal government. 13,370 files to be exact. While the data size itself isn’t that large (~8GB, compressed), an open-source tool I was looking to evaluate only supports <em>gzip</em> compression instead of the <em>zip</em> compressed files I actually had.</p>\n\n<p>While I could’ve used this opportunity to merge the files together into one and do all the data cleaning, I became obsessed with figuring out how to just switch the compression scheme. Here’s the one-liner that emerged:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\"><span class=\"nv\">$ </span>find <span class=\"nb\">.</span> <span class=\"nt\">-type</span> f <span class=\"nt\">-name</span> <span class=\"s1\">'*.zip'</span> | parallel <span class=\"s2\">\"unzip -p -q {} | gzip &gt; {.}.gz &amp;&amp; rm {}\"</span>\n\nfind <span class=\"nb\">.</span> <span class=\"nt\">-type</span> f <span class=\"nt\">-name</span> <span class=\"s1\">'*.zip'</span>\n  - find all zip files <span class=\"k\">in </span>the current directory, including subdirectories\n\n| parallel\n  - take input list passed by find <span class=\"nb\">command</span>, run some <span class=\"nb\">command </span>against each argument <span class=\"k\">in </span>parallel\n\n<span class=\"s2\">\"unzip -p -q {} | gzip &gt; {.}.gz &amp;&amp; rm {}\"</span>\n  - unzip a file, with flags <span class=\"nt\">-p</span> to pass data to STDOUT and <span class=\"nt\">-q</span> <span class=\"k\">for </span>quiet mode\n  - <span class=\"o\">{}</span> represents the input file, which comes from list passed by find\n  - <span class=\"nb\">gzip </span>takes STDOUT as its input, writes to a file whose name is determined by <span class=\"o\">{</span>.<span class=\"o\">}</span>\n    <span class=\"o\">(</span>the input file name going into parallel, where the <span class=\"nb\">.</span> removes the file extension<span class=\"o\">)</span>\n  - <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">rm</span> <span class=\"o\">{}</span> is run after the <span class=\"nb\">gzip </span>process finishes, removing the original .zip file</code></pre></figure>\n\n<p>As a one-liner, it’s not the hardest to comprehend what’s going on, but it’s also not the most intuitive. The key idea here is that once we find all of the zip files, we can unzip/gzip the files in parallel. Note that this works because each process is independent from the other; a single file itself is being unzipped and then gzipped, we’re not unzipping and gzipping a single file in parallel. Just that multiple single-threaded processes are being kicked off at once instead of leaving the other cores in the CPU idle.</p>\n\n<p>Once the unzip-to-gzip process has occurred, then I delete the original zip file. So for the most part, this process can be considered to take constant disk space (if you ignore that 4-8 files are being processed at one time).</p>\n\n<p>Like many one-liners, this took longer to figure out than was actually worth the time savings. But such is life, and now it’s available in the wild for the next person who wonders how to do something like this!</p>"
}