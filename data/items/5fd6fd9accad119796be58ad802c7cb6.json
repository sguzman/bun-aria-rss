{
  "title": "Feedback is Good, Active Feedback is Better: Block Attention Active Feedback Codes. (arXiv:2211.01730v1 [cs.IT])",
  "link": "http://arxiv.org/abs/2211.01730",
  "description": "<p>Deep neural network (DNN)-assisted channel coding designs, such as\nlow-complexity neural decoders for existing codes, or end-to-end\nneural-network-based auto-encoder designs are gaining interest recently due to\ntheir improved performance and flexibility; particularly for communication\nscenarios in which high-performing structured code designs do not exist.\nCommunication in the presence of feedback is one such communication scenario,\nand practical code design for feedback channels has remained an open challenge\nin coding theory for many decades. Recently, DNN-based designs have shown\nimpressive results in exploiting feedback. In particular, generalized block\nattention feedback (GBAF) codes, which utilizes the popular transformer\narchitecture, achieved significant improvement in terms of the block error rate\n(BLER) performance. However, previous works have focused mainly on passive\nfeedback, where the transmitter observes a noisy version of the signal at the\nreceiver. In this work, we show that GBAF codes can also be used for channels\nwith active feedback. We implement a pair of transformer architectures, at the\ntransmitter and the receiver, which interact with each other sequentially, and\nachieve a new state-of-the-art BLER performance, especially in the low SNR\nregime.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1\">Emre Ozfatura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazanfari_A/0/1/0/all/0/1\">Amin Ghazanfari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1\">Alberto Perotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_B/0/1/0/all/0/1\">Branislav Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>"
}