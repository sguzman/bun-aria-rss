{
  "title": "Gradient-based Weight Density Balancing for Robust Dynamic Sparse Training. (arXiv:2210.14012v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2210.14012",
  "description": "<p>Training a sparse neural network from scratch requires optimizing connections\nat the same time as the weights themselves. Typically, the weights are\nredistributed after a predefined number of weight updates, removing a fraction\nof the parameters of each layer and inserting them at different locations in\nthe same layers. The density of each layer is determined using heuristics,\noften purely based on the size of the parameter tensor. While the connections\nper layer are optimized multiple times during training, the density of each\nlayer remains constant. This leaves great unrealized potential, especially in\nscenarios with a high sparsity of 90% and more. We propose Global\nGradient-based Redistribution, a technique which distributes weights across all\nlayers - adding more weights to the layers that need them most. Our evaluation\nshows that our approach is less prone to unbalanced weight distribution at\ninitialization than previous work and that it is able to find better performing\nsparse subnetworks at very high sparsity levels.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Parger_M/0/1/0/all/0/1\">Mathias Parger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_A/0/1/0/all/0/1\">Alexander Ertl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eibensteiner_P/0/1/0/all/0/1\">Paul Eibensteiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Joerg H. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winter_M/0/1/0/all/0/1\">Martin Winter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_M/0/1/0/all/0/1\">Markus Steinberger</a>"
}