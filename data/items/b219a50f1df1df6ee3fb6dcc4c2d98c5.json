{
  "title": "Reciprocal Convexity to reverse the Jensen Inequality",
  "link": "http://artem.sobolev.name/posts/2021-05-02-reciprocal-convexity-to-reverse-the-jensen-inequality.html",
  "description": "<p><a href=\"https://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen’s inequality</a> is a powerful tool often used in mathematical derivations and analyses. It states that for a convex function <span class=\"math inline\">\\(f(x)\\)</span> and an arbitrary random variable <span class=\"math inline\">\\(X\\)</span> we have the following <em>upper</em> bound: <span class=\"math display\">\\[\nf\\left(\\E X\\right)\n\\le\n\\E f\\left(X\\right)\n\\]</span></p>\n<p>However, oftentimes we want the inequality to work in the other direction, to give a <em>lower</em> bound. In this post I’ll outline one possible approach to this.</p>\n<!--more-->\n<h2 id=\"the-trick\">The Trick</h2>\n<p>The basic idea is very simple: let’s turn our convex function into a concave function. First, define</p>\n<p><span class=\"math display\">\\[\n\\hat{f}(x) = f\\left(\\tfrac{1}{x}\\right)\n\\]</span></p>\n<p>As <a href=\"https://core.ac.uk/download/pdf/82634388.pdf\">defined by Merkle</a>, a function <span class=\"math inline\">\\(h(x)\\)</span> is called <strong>reciprocally convex</strong> if <span class=\"math inline\">\\(h(x)\\)</span> is concave and <span class=\"math inline\">\\(\\hat{h}(x) = h(1/x)\\)</span> is convex. For the sake of this discussion we’ll assume <span class=\"math inline\">\\(f(x)\\)</span> is <strong>reciprocally concave</strong>, that is, of course, <span class=\"math inline\">\\(-f(x)\\)</span> is reciprocally convex.</p>\n<p>Next, we’ll need an unbiased estimator <span class=\"math inline\">\\(Y\\)</span> of the reciprocal of the mean <span class=\"math inline\">\\(\\E X\\)</span>, that is, <span class=\"math inline\">\\(Y\\)</span> should satisfy the following:</p>\n<p><span class=\"math display\">\\[\n\\E Y = \\frac{1}{\\E X}\n\\]</span></p>\n<p>Now, the rest is simple algebra and the standard Jensen’s inequality (remember <span class=\"math inline\">\\(\\hat{f}\\)</span> is concave by definition): <span class=\"math display\">\\[\nf\\left(\\E X\\right)\n= \\hat{f}\\left(\\frac{1}{\\E X}\\right)\n= \\hat{f}\\left(\\E Y\\right)\n\\ge \\E \\hat{f}\\left(Y\\right)\n= \\E f\\left(\\frac{1}{Y}\\right)\n\\tag{1}\n\\]</span></p>\n<h3 id=\"example\">Example</h3>\n<p>This trick is actually the reason why we can have both <a href=\"/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html\">upper</a> and lower bounds on the log marginal likelihood in latent variable models. Indeed, consider the following example:</p>\n<p><span class=\"math display\">\\[\nf(x) := -\\log(x),\n\\quad X := p(x \\mid Z),\n\\quad Z \\sim p(z)\n\\]</span></p>\n<p>This is the standard Variational Inference setup. Putting it all together, we’d like to give bounds on</p>\n<p><span class=\"math display\">\\[\n-\\log \\left( \\E_{Z \\sim p(z)} p(x|Z) \\right)\n= -\\log p(x)\n\\]</span></p>\n<p>Normally, in VI we use the standard Jensen’s Inequality to obtain an upper bound on this negative log-likelihood, and all is good. However, sometimes we need lower bounds on the same quantity. This is where the framework above comes to the rescue.</p>\n<p>First, it’s easy to see that we’re very lucky – <span class=\"math inline\">\\(f(x)\\)</span> is indeed reciprocally concave: <span class=\"math inline\">\\(-\\log(x)\\)</span> is convex, and <span class=\"math inline\">\\(-\\log\\tfrac{1}{x} = \\log(x)\\)</span> is concave.</p>\n<p>Next, we need an unbiased estimator <span class=\"math inline\">\\(Y\\)</span> of the inverse mean of <span class=\"math inline\">\\(X\\)</span>, that is, an unbiased estimator of <span class=\"math inline\">\\(1/p(x)\\)</span>. Such estimator can be given this way:</p>\n<p><span class=\"math display\">\\[\n\\frac{1}{p(x)}\n= \\int \\frac{q(z)}{p(x)} dz\n= \\int \\frac{q(z) p(z|x)}{p(x) p(z | x)} dz\n= \\E_{p(z|x)} \\frac{q(z)}{p(x, z)}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(q(z)\\)</span> is an arbitrary distribution. Thus, the estimator is <span class=\"math inline\">\\(Y\\)</span> generated by r.v. <span class=\"math inline\">\\(Z\\)</span>: <span class=\"math display\">\\[\nY := \\frac{q(Z)}{p(x, Z)},\n\\quad Z \\sim p(z|x)\n\\]</span></p>\n<p>Now, putting these into (1) we obtain: <span class=\"math display\">\\[\n-\\log p(x)\n\\ge -\\E_{p(z|x)} \\log \\frac{p(x, z)}{q(z)}\n\\]</span> Or, equivalently, <span class=\"math display\">\\[\n\\log p(x)\n\\le \\E_{p(z|x)} \\log \\frac{p(x, z)}{q(z)}\n\\]</span> By the way, for comparison, here’s the classical lower bound obtained through the standard Jensen’s Inequality. Curiously, the only difference is where the random variables <span class=\"math inline\">\\(z\\)</span> are coming from: <span class=\"math display\">\\[\n\\log p(x)\n\\ge \\E_{q(z)} \\log \\frac{p(x, z)}{q(z)}\n\\]</span></p>\n<h3 id=\"generalization\">Generalization</h3>\n<p>Why limit ourselves to a particular <span class=\"math inline\">\\(\\hat{f} = f \\circ (1/x)\\)</span>? One can consider other invertible functions <span class=\"math inline\">\\(g(x)\\)</span> instead of the <span class=\"math inline\">\\(1/x\\)</span>. Here’s the recipe:</p>\n<ul>\n<li>Define <span class=\"math inline\">\\(f^{[g]}(x) = f(g(x))\\)</span></li>\n<li>First, we need <span class=\"math inline\">\\(f^{[g]}(x)\\)</span> to be concave</li>\n<li>Second, we need an unbiased estimator <span class=\"math inline\">\\(Y\\)</span> of <span class=\"math inline\">\\(g^{-1}(\\E X)\\)</span></li>\n</ul>\n<p>This leads to a generalization of (1): <span class=\"math display\">\\[\nf\\left(\\E X\\right)\n= f^{[g]}\\left( g^{-1}(\\E X) \\right)\n= f^{[g]}\\left( \\E Y \\right)\n\\ge \\E f^{[g]}\\left( Y \\right)\n= \\E f\\left( g(Y) \\right)\n\\]</span></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>This trick is simple, and perhaps obvious even without any fancy words such as reciprocal convexity. Moreover, it has its limitations: you either need to get lucky with <span class=\"math inline\">\\(f(x)\\)</span> being reciprocally concave, or need to find an invertible <span class=\"math inline\">\\(g(x)\\)</span> such that <span class=\"math inline\">\\(f \\circ g\\)</span> is concave. But even that’s not enough, as you also need to construct an unbiased estimator <span class=\"math inline\">\\(Y\\)</span>, and if you fancy practical applications, efficiency of the resulting bound will heavily depend on the quality of this estimator.</p>\n<p>Nevertheless, I believe this is an interesting idea and it might prove itself useful in various analyses and derivations.</p>",
  "pubDate": "Sun, 02 May 2021 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2021-05-02-reciprocal-convexity-to-reverse-the-jensen-inequality.html",
  "dc:creator": "Artem"
}