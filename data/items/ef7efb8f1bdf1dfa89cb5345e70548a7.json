{
  "title": "Multi-Armed Bandits and Conjugate Models — Bayesian Reinforcement Learning (Part 1)",
  "link": "",
  "id": "https://www.georgeho.org/bayesian-bandits/",
  "updated": "2018-08-31T00:00:00Z",
  "published": "2018-08-31T00:00:00Z",
  "content": "<blockquote>\n<p>This is the first of a two-part series about Bayesian bandit algorithms. Check\nout the second post <a href=\"https://www.georgeho.org/bayesian-bandits-2/\">here</a>.</p>\n</blockquote>\n<p>Let&rsquo;s talk about Bayesianism. It&rsquo;s developed a reputation (not entirely\njustified, but not entirely unjustified either) for being too mathematically\nsophisticated or too computationally intensive to work at scale. For instance,\ninferring from a Gaussian mixture model is fraught with computational problems\n(hierarchical funnels, multimodal posteriors, etc.), and may take a seasoned\nBayesian anywhere between a day and a month to do well. On the other hand, other\nblunt hammers of estimation are as easy as a maximum likelihood estimate:\nsomething you could easily get a SQL query to do if you wanted to.</p>\n<p>In this blog post I hope to show that there is more to Bayesianism than just\nMCMC sampling and suffering, by demonstrating a Bayesian approach to a classic\nreinforcement learning problem: the <em>multi-armed bandit</em>.</p>\n<p>The problem is this: imagine a gambler at a row of slot machines (each machine\nbeing a “one-armed bandit”), who must devise a strategy so as to maximize\nrewards. This strategy includes which machines to play, how many times to play\neach machine, in which order to play them, and whether to continue with the\ncurrent machine or try a different machine.</p>\n<p>This problem is a central problem in decision theory and reinforcement learning:\nthe agent (our gambler) starts out in a state of ignorance, but learns through\ninteracting with its environment (playing slots). For more details, Cam\nDavidson-Pilon has a great introduction to multi-armed bandits in Chapter 6 of\nhis book <a href=\"https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb\"><em>Bayesian Methods for\nHackers</em></a>,\nand Tor Lattimore and Csaba Szepesvári cover a breathtaking amount of the\nunderlying theory in their book <a href=\"http://banditalgs.com/\"><em>Bandit Algorithms</em></a>.</p>\n<p>So let&rsquo;s get started! I assume that you are familiar with:</p>\n<ul>\n<li>some basic probability, at least enough to know some distributions: normal,\nBernoulli, binomial&hellip;</li>\n<li>some basic Bayesian statistics, at least enough to understand what a\n<a href=\"https://en.wikipedia.org/wiki/Conjugate_prior\">conjugate prior</a> (and\nconjugate model) is, and why one might like them.</li>\n<li><a href=\"https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/\">Python generators and the <code>yield</code>\nkeyword</a>,\nto understand some of the code I&rsquo;ve written<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>.</li>\n</ul>\n<p>Dive in!</p>\n<h2 id=\"the-algorithm\">The Algorithm</h2>\n<p>The algorithm is straightforward. The description below is taken from Cam\nDavidson-Pilon over at Data Origami<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup>.</p>\n<p>For each round,</p>\n<ol>\n<li>Sample a random variable $X_b$ from the prior of bandit $b$, for all\n$b$.</li>\n<li>Select the bandit with largest sample, i.e. select bandit $B =\n\\text{argmax}(X_b)$.</li>\n<li>Observe the result of pulling bandit $B$, and update your prior on bandit\n$B$ using the conjugate model update rule.</li>\n<li>Repeat!</li>\n</ol>\n<p>What I find remarkable about this is how dumbfoundingly simple it is! No MCMC\nsampling, no $\\hat{R}$s to diagnose, no pesky divergences&hellip; all it requires is\na conjugate model, and the rest is literally just counting.</p>\n<p><strong>NB:</strong> This algorithm is technically known as <em>Thompson sampling</em>, and is only\none of many algorithms out there. The main difference is that there are other\nways to go from our current priors to a decision on which bandit to play\nnext. E.g. instead of simply sampling from our priors, we could use the\nupper bound of the 90% credible region, or some dynamic quantile of the\nposterior (as in Bayes UCB). See Data Origami<sup id=\"fnref1:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup> for more information.</p>\n<h3 id=\"stochastic-aka-stationary-bandits\">Stochastic (a.k.a. stationary) bandits</h3>\n<p>Let&rsquo;s take this algorithm for a spin! Assume we have rewards which are Bernoulli\ndistributed (this would be the situation we face when e.g. modelling\nclick-through rates). The conjugate prior for the Bernoulli distribution is the\nBeta distribution (this is a special case of the Beta-Binomial model).</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">make_bandits</span>(params):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">pull</span>(arm, size<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">None</span>):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Bernoulli distributed rewards</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>binomial(n<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span>, p<span style=\"color:#f92672\">=</span>params[arm], size<span style=\"color:#f92672\">=</span>size)\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> reward\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">return</span> pull, len(params)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">bayesian_strategy</span>(pull, num_bandits):\n</span></span><span style=\"display:flex;\"><span> num_rewards <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>zeros(num_bandits)\n</span></span><span style=\"display:flex;\"><span> num_trials <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>zeros(num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample from the bandits&#39; priors, and choose largest</span>\n</span></span><span style=\"display:flex;\"><span> choice <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>argmax(\n</span></span><span style=\"display:flex;\"><span> np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>beta(a<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">2</span> <span style=\"color:#f92672\">+</span> num_rewards, b<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">2</span> <span style=\"color:#f92672\">+</span> num_trials <span style=\"color:#f92672\">-</span> num_rewards)\n</span></span><span style=\"display:flex;\"><span> )\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample the chosen bandit</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> next(pull(choice))\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Update</span>\n</span></span><span style=\"display:flex;\"><span> num_rewards[choice] <span style=\"color:#f92672\">+=</span> reward\n</span></span><span style=\"display:flex;\"><span> num_trials[choice] <span style=\"color:#f92672\">+=</span> <span style=\"color:#ae81ff\">1</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> choice, reward, num_rewards, num_trials\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> __name__ <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;__main__&#34;</span>:\n</span></span><span style=\"display:flex;\"><span> pull, num_bandits <span style=\"color:#f92672\">=</span> make_bandits([<span style=\"color:#ae81ff\">0.2</span>, <span style=\"color:#ae81ff\">0.5</span>, <span style=\"color:#ae81ff\">0.7</span>])\n</span></span><span style=\"display:flex;\"><span> play <span style=\"color:#f92672\">=</span> bayesian_strategy(pull, num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">for</span> _ <span style=\"color:#f92672\">in</span> range(<span style=\"color:#ae81ff\">100</span>):\n</span></span><span style=\"display:flex;\"><span> choice, reward, num_rewards, num_trials <span style=\"color:#f92672\">=</span> next(play)\n</span></span></code></pre></div><p>Here, <code>pull</code> returns the result of pulling on the <code>arm</code>&lsquo;th bandit, and\n<code>make_bandits</code> is just a factory function for <code>pull</code>.</p>\n<p>The <code>bayesian_strategy</code> function actually implements the algorithm. We only need\nto keep track of the number of times we win and the number of times we played\n(<code>num_rewards</code> and <code>num_trials</code>, respectively). It samples from all current\n<code>np.random.beta</code> priors (where the original prior was a $\\text{Beta}(2,\n2)$, which is symmetrix about 0.5 and explains the odd-looking <code>a=2+</code> and\n<code>b=2+</code> there), picks the <code>np.argmax</code>, <code>pull</code>s that specific bandit, and updates\n<code>num_rewards</code> and <code>num_trials</code>.</p>\n<p>I&rsquo;ve omitted the data visualization code here, but if you want to see it, check\nout the <a href=\"https://github.com/eigenfoo/wanderings/blob/afcf37a8c6c2a2ac38f6708c1f3dd50db2ebe71f/bayes/bayesian-bandits.ipynb\">Jupyter notebook on my\nGitHub</a></p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/beta-binomial.png\"><img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/beta-binomial.png\" alt=\"Posterior distribution after several pulls for the Beta-Binomial model\"></a>\n</figure>\n<h3 id=\"generalizing-to-conjugate-models\">Generalizing to conjugate models</h3>\n<p>In fact, this algorithm isn&rsquo;t just limited to Bernoulli-distributed rewards: it\nwill work for any <a href=\"https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions\">conjugate\nmodel</a>!\nHere I implement the Gamma-Poisson model (that is, Poisson distributed rewards,\nwith a Gamma conjugate prior) to illustrate how extensible this framework is.\n(Who cares about Poisson distributed rewards, you ask? Anyone who worries about\nreturning customers, for one!)</p>\n<p>Here&rsquo;s what we need to change:</p>\n<ul>\n<li>The rewards distribution in the <code>pull</code> function (in practice, you don&rsquo;t get\nto pick this, so <em>technically</em> there&rsquo;s nothing to change if you&rsquo;re doing this\nin production!)</li>\n<li>The sampling from the prior in <code>bayesian_strategy</code></li>\n<li>The variables you need to keep track of and the update rule in <code>bayesian_strategy</code></li>\n</ul>\n<p>Without further ado:</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">make_bandits</span>(params):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">pull</span>(arm, size<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">None</span>):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Poisson distributed rewards</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>poisson(lam<span style=\"color:#f92672\">=</span>params[arm], size<span style=\"color:#f92672\">=</span>size)\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> reward\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">return</span> pull, len(params)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">bayesian_strategy</span>(pull, num_bandits):\n</span></span><span style=\"display:flex;\"><span> num_rewards <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>ones(num_bandits)\n</span></span><span style=\"display:flex;\"><span> num_trials <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>ones(num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample from the bandits&#39; priors, and choose largest</span>\n</span></span><span style=\"display:flex;\"><span> choice <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>argmax(np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>gamma(num_rewards, scale<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span> <span style=\"color:#f92672\">/</span> num_trials))\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample the chosen bandit</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> next(pull(choice))\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Update</span>\n</span></span><span style=\"display:flex;\"><span> num_rewards[choice] <span style=\"color:#f92672\">+=</span> reward\n</span></span><span style=\"display:flex;\"><span> num_trials[choice] <span style=\"color:#f92672\">+=</span> <span style=\"color:#ae81ff\">1</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> choice, reward, num_rewards, num_trials\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> __name__ <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;__main__&#34;</span>:\n</span></span><span style=\"display:flex;\"><span> pull, num_bandits <span style=\"color:#f92672\">=</span> make_bandits([<span style=\"color:#ae81ff\">4.0</span>, <span style=\"color:#ae81ff\">4.5</span>, <span style=\"color:#ae81ff\">5.0</span>])\n</span></span><span style=\"display:flex;\"><span> play <span style=\"color:#f92672\">=</span> bayesian_strategy(pull, num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">for</span> _ <span style=\"color:#f92672\">in</span> range(<span style=\"color:#ae81ff\">100</span>):\n</span></span><span style=\"display:flex;\"><span> choice, reward, num_rewards, num_trials <span style=\"color:#f92672\">=</span> next(play)\n</span></span></code></pre></div><figure>\n<a href=\"https://www.georgeho.org/assets/images/gamma-poisson.png\"><img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/gamma-poisson.png\" alt=\"Posterior distribution after several pulls for the Gamma-Poisson model\"></a>\n</figure>\n<p>This really demonstrates how lean and mean conjugate models can be, especially\nconsidering how much of a pain MCMC or approximate inference methods would be,\ncompared to literal <em>counting</em>. Conjugate models aren&rsquo;t just textbook examples:\nthey&rsquo;re <em>(gasp)</em> actually useful!</p>\n<h3 id=\"generalizing-to-arbitrary-rewards-distributions\">Generalizing to arbitrary rewards distributions</h3>\n<p>OK, so if we have a conjugate model, we can use Thompson sampling to solve the\nmulti-armed bandit problem. But what if our rewards distribution doesn&rsquo;t have a\nconjugate prior, or what if we don&rsquo;t even <em>know</em> our rewards distribution?</p>\n<p>In general this problem is very difficult to solve. Theoretically, we could\nplace some fairly uninformative prior on our rewards, and after every pull we\ncould run MCMC to get our posterior, but that doesn&rsquo;t scale, especially for the\nonline algorithms that we have in mind. Luckily a recent paper by Agrawal and\nGoyal<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup> gives us some help, <em>if we assume rewards are bounded on the interval\n$[0, 1]$</em> (of course, if we have bounded rewards, then we can just normalize\nthem by their maximum value to get rewards between 0 and 1).</p>\n<p>This solutions bootstraps the first Beta-Bernoulli model to this new situation.\nHere&rsquo;s what happens:</p>\n<ol>\n<li>Sample a random variable $X_b$ from the (Beta) prior of bandit $b$, for\nall $b$.</li>\n<li>Select the bandit with largest sample, i.e. select bandit $B =\n\\text{argmax}(X_b)$.</li>\n<li>Observe the reward $R$ from bandit $B$.</li>\n<li><strong>Observe the outcome $r$ from a Bernoulli trial with probability of success $R$.</strong></li>\n<li>Update posterior of $B$ with this observation $r$.</li>\n<li>Repeat!</li>\n</ol>\n<p>Here I do this for the logit-normal distribution (i.e. a random variable whose\nlogit is normally distributed). Note that <code>np.expit</code> is the inverse of the logit\nfunction.</p>\n<div class=\"highlight\"><pre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"><code class=\"language-python\" data-lang=\"python\"><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">make_bandits</span>(params):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">pull</span>(arm, size<span style=\"color:#f92672\">=</span><span style=\"color:#66d9ef\">None</span>):\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Logit-normal distributed returns (or any distribution with finite support)</span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># `expit` is the inverse of `logit`</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> expit(np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>normal(loc<span style=\"color:#f92672\">=</span>params[arm], scale<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span>, size<span style=\"color:#f92672\">=</span>size))\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> reward\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">return</span> pull, len(params)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">def</span> <span style=\"color:#a6e22e\">bayesian_strategy</span>(pull, num_bandits):\n</span></span><span style=\"display:flex;\"><span> num_rewards <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>zeros(num_bandits)\n</span></span><span style=\"display:flex;\"><span> num_trials <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>zeros(num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">while</span> <span style=\"color:#66d9ef\">True</span>:\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample from the bandits&#39; priors, and choose largest</span>\n</span></span><span style=\"display:flex;\"><span> choice <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>argmax(\n</span></span><span style=\"display:flex;\"><span> np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>beta(<span style=\"color:#ae81ff\">2</span> <span style=\"color:#f92672\">+</span> num_rewards, <span style=\"color:#ae81ff\">2</span> <span style=\"color:#f92672\">+</span> num_trials <span style=\"color:#f92672\">-</span> num_rewards)\n</span></span><span style=\"display:flex;\"><span> )\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample the chosen bandit</span>\n</span></span><span style=\"display:flex;\"><span> reward <span style=\"color:#f92672\">=</span> next(pull(choice))\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Sample a Bernoulli with probability of success = reward</span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Remember, reward is normalized to be in [0, 1]</span>\n</span></span><span style=\"display:flex;\"><span> outcome <span style=\"color:#f92672\">=</span> np<span style=\"color:#f92672\">.</span>random<span style=\"color:#f92672\">.</span>binomial(n<span style=\"color:#f92672\">=</span><span style=\"color:#ae81ff\">1</span>, p<span style=\"color:#f92672\">=</span>reward)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#75715e\"># Update</span>\n</span></span><span style=\"display:flex;\"><span> num_rewards[choice] <span style=\"color:#f92672\">+=</span> outcome\n</span></span><span style=\"display:flex;\"><span> num_trials[choice] <span style=\"color:#f92672\">+=</span> <span style=\"color:#ae81ff\">1</span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">yield</span> choice, reward, num_rewards, num_trials\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span><span style=\"color:#66d9ef\">if</span> __name__ <span style=\"color:#f92672\">==</span> <span style=\"color:#e6db74\">&#34;__main__&#34;</span>:\n</span></span><span style=\"display:flex;\"><span> pull, num_bandits <span style=\"color:#f92672\">=</span> make_bandits([<span style=\"color:#ae81ff\">0.2</span>, <span style=\"color:#ae81ff\">1.8</span>, <span style=\"color:#ae81ff\">2</span>])\n</span></span><span style=\"display:flex;\"><span> play <span style=\"color:#f92672\">=</span> bayesian_strategy(pull, num_bandits)\n</span></span><span style=\"display:flex;\"><span>\n</span></span><span style=\"display:flex;\"><span> <span style=\"color:#66d9ef\">for</span> _ <span style=\"color:#f92672\">in</span> range(<span style=\"color:#ae81ff\">100</span>):\n</span></span><span style=\"display:flex;\"><span> choice, reward, num_rewards, num_trials <span style=\"color:#f92672\">=</span> next(play)\n</span></span></code></pre></div><figure>\n<a href=\"https://www.georgeho.org/assets/images/bounded.png\"><img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/bounded.png\" alt=\"Posterior distribution after several pulls with an arbitrary reward distribution (e.g. the logit normal)\"></a>\n</figure>\n<h2 id=\"final-remarks\">Final Remarks</h2>\n<p>None of this theory is new: I&rsquo;m just advertising it! See Cam Davidson-Pilon&rsquo;s\ngreat blog post about Bayesian bandits<sup id=\"fnref2:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup> for a much more in-depth treatment,\nand of course, read around papers on arXiv if you want to go deeper!</p>\n<p>Also, if you want to see all the code that went into this blog post, check out\n<a href=\"https://github.com/eigenfoo/wanderings/blob/afcf37a8c6c2a2ac38f6708c1f3dd50db2ebe71f/bayes/bayesian-bandits.ipynb\">the notebook\nhere</a>.</p>\n<blockquote>\n<p>This is the first of a two-part series about Bayesian bandit algorithms. Check\nout the second post <a href=\"https://www.georgeho.org/bayesian-bandits-2/\">here</a>.</p>\n</blockquote>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>I&rsquo;ve hopped on board the functional programming bandwagon, and couldn&rsquo;t\nhelp but think that to demonstrate this idea, I didn&rsquo;t need a framework, a\nlibrary or even a class. Just two functions!&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>Davidson-Pilon, Cameron. “Multi-Armed Bandits.” DataOrigami, 6 Apr. 2013,\n<a href=\"https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits\">dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits</a>&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a>&#160;<a href=\"#fnref1:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a>&#160;<a href=\"#fnref2:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\">\n<p><a href=\"https://arxiv.org/abs/1111.1797\">arXiv:1111.1797</a> [cs.LG]&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}