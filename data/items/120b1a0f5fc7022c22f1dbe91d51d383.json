{
  "title": "Recognizing Human Activities with Kinect - The implementation",
  "description": "<p><em>Disclaimer: The work described in this post was done by me and my classmate at IIT-Kanpur, Ankit Goyal. <a href=\"/assets/activity-classification.pdf\">Here</a> is a link to the presentation that we gave.</em></p>\n\n<p>This is a follow up of my earlier <a href=\"/machinelearning/classifying-human-activities-kinect/\">post</a>, in which I explored\ntemporal models, that can be applied to things like part-of-speech tagging, gesture recognition, and any sequential \nor temporal sources of data in general. In this post, I will describe in more detail the implementation of our\nproject that classified RGBD videos according to the activity being performed in them.</p>\n\n<h3 id=\"dataset\">Dataset</h3>\n<p>Quite a few <a href=\"http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/\">RGBD datasets</a> \nare available for human activity detection/classification, and we chose to use the \nMSR Daily Activity 3D dataset. Since we had limited computational resources (the mathserver of IITK),\nand a limited time before the submission deadline, we chose to use a subset of the above dataset, \nand worked with only 6 activities. So, our problem was now reduced to 6-class classification.</p>\n\n<h3 id=\"features\">Features</h3>\n<p>In any machine learning problem, your model or learning algorithm is useless without a good set of\nfeatures. I read a <a href=\"http://www.sciencedirect.com/science/article/pii/S0167865514001299\">recent paper</a> which\nhad a decent review of the various features used. They were:</p>\n\n<ol>\n  <li>3D silhouettes - Finding the outline of the human body, and using the shape of this outline as features.</li>\n  <li>Skeletal joints or body part tracking - Kinect comes with an algorithm to determine the pose of the body\nfrom the depth image alone. Pose here refers to the 3D coordinates of 15 body joints.</li>\n  <li>Local Spatio-temporal features - Just like some 2D/3D image feature detector, but with the added dimension of time.</li>\n  <li>Local 3D occupancy features - This one seemed the most interesting. What this does is to treat an RGBD video as\na function I(x, y, z, t). Now, this a very sparse function, and would be zero at most points in a 4D space. But, \nwhenever a certain activity is performed, certain regions of this 4D space will become filled. Inferring from \nsuch data is now a matter sampling it efficiently, and this where all the innovation must lie, if this technique \nis to work.</li>\n  <li>3D optical flow - The 3D counter part of the popular <a href=\"http://en.wikipedia.org/wiki/Optical_flow\">optical flow</a>, \nit is also known as [Scene Flow] in the academic literature. <a href=\"http://www.sciencedirect.com/science/article/pii/S1077314210001748\">This</a> \nis one paper that makes use of these features.</li>\n</ol>\n\n<p>The features that we ultimately went ahead were the skeletal joints. The MSR Daily Activity 3D dataset already provides\nthe skeletal joint coordinates to us, so all we had to was to take that data, and do some basic pre-processing on it.</p>\n\n<h4 id=\"preprocessing-the-features\">Preprocessing the features.</h4>\n\n<p>The dataset provides us with the 3D coordinates of 15 human body joints. These cordinates are in the frame of reference of the Kinect.\nThe first operation that we perform on them is the following: to transform the points from the Kinect reference frame to the frame\nof the person. By frame of the person, we refer to the joint corresponding to the torso.</p>\n\n<p>Next thing that we do is what we call “body size normalization”. Basically all the body lengths, such as the distance between the elbo and \nhand, are scaled up or down to a standard body size. This ensures that the variation in bosy sizes is captured at the feature level itself,\nand our model does not have to worry about it anymore.</p>\n\n<p><a href=\"https://gist.github.com/avisingh599/73ac41db59d87115c99e\">Clicke here</a> to get the MATLAB code that does the feature extraction part from skeleton files that were obtained from the MSR dataset.</p>\n\n<h3 id=\"model\">Model</h3>\n\n<p>Now, as I discussed in my <a href=\"/machinelearning/classifying-human-activities-kinect/\">previous post</a>, Hidden Conditional\nRandom Fields (HCRFs) was the model that we finally selected. The original authors had released a well documented \n<a href=\"http://sourceforge.net/projects/hcrf/\">toolbox</a>, to which we directly fed the features that were computed above.</p>\n\n<h3 id=\"results\">Results</h3>\n<p>Five-fold cross-validation without any hyper-parameter tuning yielded a precision of 71%. These results do not seem impressive\non first glance, but it must be noted that all our experiments were performed in the “new person” setting i.e. the person in the\ntest set did not appear in the training set, and we did not do any hyper parameter tuning. Our results can be summarised in the \nollowing heatmap:</p>\n\n<figure>\n\t<img src=\"/images/kinect_activity/heatmap.bmp\" />\n\t<figcaption>Where the algorithm succeeds and fails</figcaption>\n</figure>\n\n<p>The above figure made one thing clear: that accuracy is being seriously harmed by the algorithm’s inability\nto correctly distinguish between drinking and talking on phone. The reason for this is relatively simple. \nThe features that we are using are skeletal features, and therefore we do not pay any attention to what\nobjects the human is interacting with. If you look at the skelat stream, talking on the phone, and drinking\nwater seem extrmemly similar! In both the cases, the human raises a hand, and brings it near his head. \nThus, in order to make a truly useful activity detection system, it is important to model these interactions\nexplicitly.</p>\n\n<p>If we do get around to improving this model, I will post it here.</p>",
  "pubDate": "Tue, 02 Jun 2015 00:00:00 +0000",
  "link": "https://avisingh599.github.io/machinelearning/classifying-human-activities-kinect-2/",
  "guid": "https://avisingh599.github.io/machinelearning/classifying-human-activities-kinect-2/"
}