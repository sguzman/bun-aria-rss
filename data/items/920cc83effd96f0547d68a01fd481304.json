{
  "title": "Dask and Pandas and XGBoost",
  "link": "",
  "updated": "2017-03-28T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/03/28/dask-xgboost",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>This post talks about distributing Pandas Dataframes with Dask and then handing\nthem over to distributed XGBoost for training.</p>\n\n<p>More generally it discusses the value of launching multiple distributed systems\nin the same shared-memory processes and smoothly handing data back and forth\nbetween them.</p>\n\n<ul>\n  <li><a href=\"https://gist.github.com/mrocklin/3696fe2398dc7152c66bf593a674e4d9\">Notebook</a></li>\n  <li><a href=\"https://youtu.be/Cc4E-PdDSro\">Screencast</a></li>\n  <li><a href=\"https://github.com/dmlc/xgboost/issues/2032\">Github issue</a></li>\n</ul>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>XGBoost is a well-loved library for a popular class of machine learning\nalgorithms, gradient boosted trees.  It is used widely in business and is one\nof the most popular solutions in Kaggle competitions.  For larger datasets or\nfaster training, XGBoost also comes with its own distributed computing system\nthat lets it scale to multiple machines on a cluster.  Fantastic.  Distributed\ngradient boosted trees are in high demand.</p>\n\n<p>However before we can use distributed XGBoost we need to do three things:</p>\n\n<ol>\n  <li>Prepare and clean our possibly large data, probably with a lot of Pandas wrangling</li>\n  <li>Set up XGBoost master and workers</li>\n  <li>Hand data our cleaned data from a bunch of distributed Pandas dataframes to\nXGBoost workers across our cluster</li>\n</ol>\n\n<p>This ends up being surprisingly easy.  This blogpost gives a quick example\nusing Dask.dataframe to do distributed Pandas data wrangling, then using a new\n<a href=\"https://github.com/dask/dask-xgboost\">dask-xgboost</a> package to setup an\nXGBoost cluster inside the Dask cluster and perform the handoff.</p>\n\n<p>After this example we’ll talk about the general design and what this means for\nother distributed systems.</p>\n\n<h2 id=\"example\">Example</h2>\n\n<p>We have a ten-node cluster with eight cores each (<code class=\"language-plaintext highlighter-rouge\">m4.2xlarges</code> on EC2)</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span><span class=\"p\">,</span> <span class=\"n\">progress</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'172.31.33.0:8786'</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">restart</span><span class=\"p\">()</span>\n<span class=\"o\">&lt;</span><span class=\"n\">Client</span><span class=\"p\">:</span> <span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'tcp://172.31.33.0:8786'</span> <span class=\"n\">processes</span><span class=\"o\">=</span><span class=\"mi\">10</span> <span class=\"n\">cores</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"o\">&gt;</span>\n</code></pre></div></div>\n\n<p>We load the Airlines dataset using dask.dataframe (just a bunch of Pandas\ndataframes spread across a cluster) and do a bit of preprocessing:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"c1\"># Subset of the columns to use\n</span><span class=\"n\">cols</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'Year'</span><span class=\"p\">,</span> <span class=\"s\">'Month'</span><span class=\"p\">,</span> <span class=\"s\">'DayOfWeek'</span><span class=\"p\">,</span> <span class=\"s\">'Distance'</span><span class=\"p\">,</span>\n        <span class=\"s\">'DepDelay'</span><span class=\"p\">,</span> <span class=\"s\">'CRSDepTime'</span><span class=\"p\">,</span> <span class=\"s\">'UniqueCarrier'</span><span class=\"p\">,</span> <span class=\"s\">'Origin'</span><span class=\"p\">,</span> <span class=\"s\">'Dest'</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># Create the dataframe\n</span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s\">'s3://dask-data/airline-data/20*.csv'</span><span class=\"p\">,</span> <span class=\"n\">usecols</span><span class=\"o\">=</span><span class=\"n\">cols</span><span class=\"p\">,</span>\n                  <span class=\"n\">storage_options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'anon'</span><span class=\"p\">:</span> <span class=\"bp\">True</span><span class=\"p\">})</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">frac</span><span class=\"o\">=</span><span class=\"mf\">0.2</span><span class=\"p\">)</span> <span class=\"c1\"># XGBoost requires a bit of RAM, we need a larger cluster\n</span>\n<span class=\"n\">is_delayed</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">DepDelay</span><span class=\"p\">.</span><span class=\"n\">fillna</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">15</span><span class=\"p\">)</span>  <span class=\"c1\"># column of labels\n</span><span class=\"k\">del</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'DepDelay'</span><span class=\"p\">]</span>  <span class=\"c1\"># Remove delay information from training dataframe\n</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'CRSDepTime'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'CRSDepTime'</span><span class=\"p\">].</span><span class=\"n\">clip</span><span class=\"p\">(</span><span class=\"n\">upper</span><span class=\"o\">=</span><span class=\"mi\">2399</span><span class=\"p\">)</span>\n\n<span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">is_delayed</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">is_delayed</span><span class=\"p\">)</span>  <span class=\"c1\"># start work in the background\n</span></code></pre></div></div>\n\n<p>This loaded a few hundred pandas dataframes from CSV data on S3.  We then had\nto downsample because how we are going to use XGBoost in the future seems to\nrequire a lot of RAM.  I am not an XGBoost expert.  Please forgive my ignorance\nhere.  At the end we have two dataframes:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">df</code>: Data from which we will learn if flights are delayed</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">is_delayed</code>: Whether or not those flights were delayed.</li>\n</ul>\n\n<p>Data scientists familiar with Pandas will probably be familiar with the code\nabove.  Dask.dataframe is <em>very</em> similar to Pandas, but operates on a cluster.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<div>\n  <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Month</th>\n      <th>DayOfWeek</th>\n      <th>CRSDepTime</th>\n      <th>UniqueCarrier</th>\n      <th>Origin</th>\n      <th>Dest</th>\n      <th>Distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>182193</th>\n      <td>2000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>800</td>\n      <td>WN</td>\n      <td>LAX</td>\n      <td>OAK</td>\n      <td>337</td>\n    </tr>\n    <tr>\n      <th>83424</th>\n      <td>2000</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1650</td>\n      <td>DL</td>\n      <td>SJC</td>\n      <td>SLC</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>346781</th>\n      <td>2000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1140</td>\n      <td>AA</td>\n      <td>ORD</td>\n      <td>LAX</td>\n      <td>1745</td>\n    </tr>\n    <tr>\n      <th>375935</th>\n      <td>2000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1940</td>\n      <td>DL</td>\n      <td>PHL</td>\n      <td>ATL</td>\n      <td>665</td>\n    </tr>\n    <tr>\n      <th>309373</th>\n      <td>2000</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1028</td>\n      <td>CO</td>\n      <td>MCI</td>\n      <td>IAH</td>\n      <td>643</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">is_delayed</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n<span class=\"mi\">182193</span>    <span class=\"bp\">False</span>\n<span class=\"mi\">83424</span>     <span class=\"bp\">False</span>\n<span class=\"mi\">346781</span>    <span class=\"bp\">False</span>\n<span class=\"mi\">375935</span>    <span class=\"bp\">False</span>\n<span class=\"mi\">309373</span>    <span class=\"bp\">False</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">DepDelay</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>\n</code></pre></div></div>\n\n<h3 id=\"categorize-and-one-hot-encode\">Categorize and One Hot Encode</h3>\n\n<p>XGBoost doesn’t want to work with text data like destination=”LAX”. Instead we\ncreate new indicator columns for each of the known airports and carriers. This\nexpands our data into many boolean columns. Fortunately Dask.dataframe has\nconvenience functions for all of this baked in (thank you Pandas!)</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">get_dummies</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">categorize</span><span class=\"p\">()).</span><span class=\"n\">persist</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>This expands our data out considerably, but makes it easier to train on.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">columns</span><span class=\"p\">)</span>\n<span class=\"mi\">685</span>\n</code></pre></div></div>\n\n<h3 id=\"split-and-train\">Split and Train</h3>\n\n<p>Great, now we’re ready to split our distributed dataframes</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">data_train</span><span class=\"p\">,</span> <span class=\"n\">data_test</span> <span class=\"o\">=</span> <span class=\"n\">df2</span><span class=\"p\">.</span><span class=\"n\">random_split</span><span class=\"p\">([</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">],</span>\n                                         <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1234</span><span class=\"p\">)</span>\n<span class=\"n\">labels_train</span><span class=\"p\">,</span> <span class=\"n\">labels_test</span> <span class=\"o\">=</span> <span class=\"n\">is_delayed</span><span class=\"p\">.</span><span class=\"n\">random_split</span><span class=\"p\">([</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">],</span>\n                                                    <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1234</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Start up a distributed XGBoost instance, and train on this data</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">%%</span><span class=\"n\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dask_xgboost</span> <span class=\"k\">as</span> <span class=\"n\">dxgb</span>\n\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'objective'</span><span class=\"p\">:</span> <span class=\"s\">'binary:logistic'</span><span class=\"p\">,</span> <span class=\"s\">'nround'</span><span class=\"p\">:</span> <span class=\"mi\">1000</span><span class=\"p\">,</span>\n          <span class=\"s\">'max_depth'</span><span class=\"p\">:</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s\">'eta'</span><span class=\"p\">:</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"s\">'subsample'</span><span class=\"p\">:</span> <span class=\"mf\">0.5</span><span class=\"p\">,</span>\n          <span class=\"s\">'min_child_weight'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s\">'tree_method'</span><span class=\"p\">:</span> <span class=\"s\">'hist'</span><span class=\"p\">,</span>\n          <span class=\"s\">'grow_policy'</span><span class=\"p\">:</span> <span class=\"s\">'lossguide'</span><span class=\"p\">}</span>\n\n<span class=\"n\">bst</span> <span class=\"o\">=</span> <span class=\"n\">dxgb</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">data_train</span><span class=\"p\">,</span> <span class=\"n\">labels_train</span><span class=\"p\">)</span>\n\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mi\">355</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">29.7</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mi\">385</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">54.5</span> <span class=\"n\">s</span>\n</code></pre></div></div>\n\n<p>Great, so we were able to train an XGBoost model on this data in about a minute\nusing our ten machines.  What we get back is just a plain XGBoost Booster\nobject.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">bst</span>\n<span class=\"o\">&lt;</span><span class=\"n\">xgboost</span><span class=\"p\">.</span><span class=\"n\">core</span><span class=\"p\">.</span><span class=\"n\">Booster</span> <span class=\"n\">at</span> <span class=\"mh\">0x7fa1c18c4c18</span><span class=\"o\">&gt;</span>\n</code></pre></div></div>\n\n<p>We could use this on normal Pandas data locally</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">xgboost</span> <span class=\"k\">as</span> <span class=\"n\">xgb</span>\n<span class=\"n\">pandas_df</span> <span class=\"o\">=</span> <span class=\"n\">data_test</span><span class=\"p\">.</span><span class=\"n\">head</span><span class=\"p\">()</span>\n<span class=\"n\">dtest</span> <span class=\"o\">=</span> <span class=\"n\">xgb</span><span class=\"p\">.</span><span class=\"n\">DMatrix</span><span class=\"p\">(</span><span class=\"n\">pandas_df</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">bst</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">dtest</span><span class=\"p\">)</span>\n<span class=\"n\">array</span><span class=\"p\">([</span> <span class=\"mf\">0.464578</span>  <span class=\"p\">,</span>  <span class=\"mf\">0.46631625</span><span class=\"p\">,</span>  <span class=\"mf\">0.47434333</span><span class=\"p\">,</span>  <span class=\"mf\">0.47245741</span><span class=\"p\">,</span>  <span class=\"mf\">0.46194169</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Of we can use <code class=\"language-plaintext highlighter-rouge\">dask-xgboost</code> again to train on our distributed holdout data,\ngetting back another Dask series.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">dxgb</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">bst</span><span class=\"p\">,</span> <span class=\"n\">data_test</span><span class=\"p\">).</span><span class=\"n\">persist</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">predictions</span>\n<span class=\"n\">Dask</span> <span class=\"n\">Series</span> <span class=\"n\">Structure</span><span class=\"p\">:</span>\n<span class=\"n\">npartitions</span><span class=\"o\">=</span><span class=\"mi\">93</span>\n<span class=\"bp\">None</span>    <span class=\"n\">float32</span>\n<span class=\"bp\">None</span>        <span class=\"p\">...</span>\n         <span class=\"p\">...</span>\n<span class=\"bp\">None</span>        <span class=\"p\">...</span>\n<span class=\"bp\">None</span>        <span class=\"p\">...</span>\n<span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">predictions</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">float32</span>\n<span class=\"n\">Dask</span> <span class=\"n\">Name</span><span class=\"p\">:</span> <span class=\"n\">_predict_part</span><span class=\"p\">,</span> <span class=\"mi\">93</span> <span class=\"n\">tasks</span>\n</code></pre></div></div>\n\n<h3 id=\"evaluate\">Evaluate</h3>\n\n<p>We can bring these predictions to the local process and use normal Scikit-learn\noperations to evaluate the results.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">roc_auc_score</span><span class=\"p\">,</span> <span class=\"n\">roc_curve</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">roc_auc_score</span><span class=\"p\">(</span><span class=\"n\">labels_test</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(),</span>\n<span class=\"p\">...</span>                     <span class=\"n\">predictions</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()))</span>\n<span class=\"mf\">0.654800768411</span>\n</code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">roc_curve</span><span class=\"p\">(</span><span class=\"n\">labels_test</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(),</span> <span class=\"n\">predictions</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">())</span>\n<span class=\"c1\"># Taken from\n</span><span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">scikit</span><span class=\"o\">-</span><span class=\"n\">learn</span><span class=\"p\">.</span><span class=\"n\">org</span><span class=\"o\">/</span><span class=\"n\">stable</span><span class=\"o\">/</span><span class=\"n\">auto_examples</span><span class=\"o\">/</span><span class=\"n\">model_selection</span><span class=\"o\">/</span><span class=\"n\">plot_roc</span><span class=\"p\">.</span><span class=\"n\">html</span><span class=\"c1\">#sphx-glr-auto-examples-model-selection-plot-roc-py\n</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>\n<span class=\"n\">lw</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">fpr</span><span class=\"p\">,</span> <span class=\"n\">tpr</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s\">'darkorange'</span><span class=\"p\">,</span> <span class=\"n\">lw</span><span class=\"o\">=</span><span class=\"n\">lw</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s\">'ROC curve'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s\">'navy'</span><span class=\"p\">,</span> <span class=\"n\">lw</span><span class=\"o\">=</span><span class=\"n\">lw</span><span class=\"p\">,</span> <span class=\"n\">linestyle</span><span class=\"o\">=</span><span class=\"s\">'--'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlim</span><span class=\"p\">([</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylim</span><span class=\"p\">([</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">1.05</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'False Positive Rate'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'True Positive Rate'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">'Receiver operating characteristic example'</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s\">\"lower right\"</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/dask-xgboost-roc-curve.png\" width=\"50%\" /></p>\n\n<p>We might want to play with our parameters above or try different data to\nimprove our solution.  The point here isn’t that we predicted airline delays\nwell, it was that if you are a data scientist who knows Pandas and XGBoost,\neverything we did above seemed <em>pretty familiar</em>.  There wasn’t a whole lot of\nnew material in the example above.  We’re using the same tools as before, just\nat a larger scale.</p>\n\n<h2 id=\"analysis\">Analysis</h2>\n\n<p>OK, now that we’ve demonstrated that this works lets talk a bit about what\njust happened and what that means generally for cooperation between distributed\nservices.</p>\n\n<h3 id=\"what-dask-xgboost-does\">What dask-xgboost does</h3>\n\n<p>The <a href=\"https://github.com/dask/dask-xgboost\">dask-xgboost</a> project is pretty\nsmall and pretty simple (200 TLOC).  Given a Dask cluster of one central scheduler and\nseveral distributed workers it starts up an XGBoost scheduler in the same\nprocess running the Dask scheduler and starts up an XGBoost worker within each\nof the Dask workers.  They share the same physical processes and memory\nspaces.  Dask was built to support this kind of situation, so this is\nrelatively easy.</p>\n\n<p>Then we ask the Dask.dataframe to fully materialize in RAM and we ask where all\nof the constituent Pandas dataframes live.  We tell each Dask worker to give\nall of the Pandas dataframes that it has to its local XGBoost worker and then\njust let XGBoost do its thing.  Dask doesn’t power XGBoost, it’s just\nsets it up, gives it data, and lets it do it’s work in the background.</p>\n\n<p>People often ask what machine learning capabilities Dask provides, how they\ncompare with other distributed machine learning libraries like H2O or Spark’s\nMLLib.  For gradient boosted trees the 200-line dask-xgboost package is the\nanswer.  Dask has no need to make such an algorithm because XGBoost already\nexists, works well and provides Dask users with a fully featured and efficient\nsolution.</p>\n\n<p>Because both Dask and XGBoost can live in the same Python process they can\nshare bytes between each other without cost, can monitor each other, etc..\nThese two distributed systems co-exist together in multiple processes in the\nsame way that NumPy and Pandas operate together within a single process.\nSharing distributed processes with multiple systems can be really beneficial if\nyou want to use multiple specialized services easily and avoid large monolithic\nframeworks.</p>\n\n<h3 id=\"connecting-to-other-distributed-systems\">Connecting to Other distributed systems</h3>\n\n<p>A while ago I wrote\n<a href=\"http://matthewrocklin.com/blog/work/2017/02/11/dask-tensorflow\">a similar blogpost</a>\nabout hosting TensorFlow from Dask in exactly the same way that we’ve done\nhere.  It was similarly easy to setup TensorFlow alongside Dask, feed it data,\nand let TensorFlow do its thing.</p>\n\n<p>Generally speaking this “serve other libraries” approach is how Dask operates\nwhen possible.  We’re only able to cover the breadth of functionality that we\ndo today because we lean heavily on the existing open source ecosystem.\nDask.arrays use Numpy arrays, Dask.dataframes use Pandas, and now the answer to\ngradient boosted trees with Dask is just to make it really really easy to use\ndistributed XGBoost.  Ta da!  We get a fully featured solution that is\nmaintained by other devoted developers, and the entire connection process was\ndone over a weekend (see <a href=\"https://github.com/dmlc/xgboost/issues/2032\">dmlc/xgboost\n#2032</a> for details).</p>\n\n<p>Since this has come out we’ve had requests to support other distributed systems\nlike <a href=\"http://libelemental.org/\">Elemental</a> and to do general hand-offs to MPI\ncomputations.  If we’re able to start both systems with the same set of\nprocesses then all of this is pretty doable.  Many of the challenges of\ninter-system collaboration go away when you can hand numpy arrays between the\nworkers of one system to the workers of the other system within the same\nprocesses.</p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Thanks to <a href=\"http://homes.cs.washington.edu/~tqchen/\">Tianqi Chen</a> and <a href=\"http://ogrisel.com/\">Olivier\nGrisel</a> for their help when <a href=\"https://github.com/dmlc/xgboost/issues/2032\">building and\ntesting</a> <code class=\"language-plaintext highlighter-rouge\">dask-xgboost</code>.  Thanks\nto <a href=\"http://github.com/electronwill\">Will Warner</a> for his help in editing this\npost.</p>"
}