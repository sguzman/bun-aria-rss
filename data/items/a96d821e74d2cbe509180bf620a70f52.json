{
  "title": "Forecasting Bike Sharing Demand",
  "link": "",
  "published": "2015-03-26T09:20:00-07:00",
  "updated": "2015-03-26T09:20:00-07:00",
  "author": {
    "name": "Damien RJ"
  },
  "id": "tag:efavdb.com,2015-03-26:/bike-share-forecasting",
  "summary": "<p>In today&#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &#8212; a problem posed in a recent <a href=\"https://www.kaggle.com/c/bike-sharing-demand\">Kaggle</a>competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers …</p>",
  "content": "<p>In today&#8217;s post, we document our efforts at applying a gradient boosted trees model to forecast bike sharing demand &#8212; a problem posed in a recent <a href=\"https://www.kaggle.com/c/bike-sharing-demand\">Kaggle</a>competition. For those not familiar, Kaggle is a site where one can compete with other data scientists on various data challenges. Top scorers often win prize money, but the site more generally serves as a great place to grab interesting datasets to explore and play with. With the simple optimization steps discussed below, we managed to quickly move from the bottom 10% of the competition &#8212; our first-pass attempt&#8217;s score &#8212; to the top 10%: no&nbsp;sweat!</p>\n<p>Our work here was inspired by a <a href=\"http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demand\">post</a> by the people at <a href=\"http://blog.dato.com/\">Dato.com</a>, who used the bike sharing competition as an opportunity to demonstrate their software. Here, we go through a similar, but more detailed discussion using the python package <a href=\"http://scikit-learn.org/stable/\">SKlearn</a>.</p>\n<h2>Introduction</h2>\n<p>Bike sharing systems are gaining popularity around the world &#8212; there are over 500 different programs currently operating in various cities, and counting!  These programs are generally funded through rider membership fees, or through pay-to-ride one time rental fees. Key to the convenience of these programs is the fact that riders who pick up a bicycle from one station can return the bicycle to any other in the network.  These systems generate a great deal of data relating to various ride details, including travel time, departure location, arrival location, and so on.  This data has the potential to be very useful for studying city mobility. The data we look at today comes from Washington D. C.&#8217;s <a href=\"https://www.capitalbikeshare.com/\">Capital Bikeshare</a> program. The goal of the Kaggle competition is to leverage the historical data provided in order to forecast future bike rental demand within the&nbsp;city.</p>\n<p>As we detailed in an earlier <a href=\"http://efavdb.github.io/notes-on-trees\">post</a>, boosting provides a general method for increasing a machine learning algorithm&#8217;s performance. Here, in order to model the Capital Bikeshare program&#8217;s demand curves, we&#8217;ll be applying a gradient boosted trees model (<span class=\"caps\">GBM</span>).  Simply put, <span class=\"caps\">GBM</span>&#8217;s are constructed by iteratively fitting a series of simple trees to a training set, where each new tree attempts to fit the residuals, or errors, of the trees that came before it. With the addition of each new tree the training error is further reduced, typically asymptoting to a reasonably accurate model &#8212; but one must watch out for overfitting &#8212; see&nbsp;below!</p>\n<h2><strong>Loading package and&nbsp;data</strong></h2>\n<p>Below, we show the relevant commands needed to load all the packages and training/test data we will be using. We work with the package <a href=\"http://pandas.pydata.org/\">Pandas</a>, whose DataFrame data structure enables quick and easy data loading and wrangling. We take advantage of this package immediately below, where in the last lines we use its parse_dates method to convert the first column of our provided data &#8212; which can be downloaded <a href=\"https://www.kaggle.com/c/bike-sharing-demand\">here</a> &#8212; from string to datetime&nbsp;format.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">ensemble</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.cross_validation</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"kn\">import</span> <span class=\"n\">mean_absolute_error</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.grid_search</span> <span class=\"kn\">import</span> <span class=\"n\">GridSearchCV</span>\n<span class=\"kn\">from</span> <span class=\"nn\">datetime</span> <span class=\"kn\">import</span> <span class=\"n\">datetime</span>\n\n<span class=\"c1\">#Load Data with pandas, and parse the</span>\n<span class=\"c1\">#first column into datetime</span>\n<span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;train.csv&#39;</span><span class=\"p\">,</span> <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n<span class=\"kp\">test</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;test.csv&#39;</span><span class=\"p\">,</span> <span class=\"n\">parse_dates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</pre></div>\n\n\n<p>The training data provided contains the following&nbsp;fields:</p>\n<p><strong><em>datetime</em></strong> - hourly date + timestamp\n<strong><em>season</em></strong> -  1 = spring, 2 = summer, 3 = fall, 4 = winter\n<strong><em>holiday</em></strong> - whether the day is considered a holiday\n<strong><em>workingday</em></strong> - whether the day is neither a weekend nor holiday\n<strong><em>weather</em></strong>:</p>\n<ol>\n<li>Clear, Few clouds, Partly cloudy, Partly&nbsp;cloudy</li>\n<li>Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds,&nbsp;Mist</li>\n<li>Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered&nbsp;clouds</li>\n<li>Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow +&nbsp;Fog</li>\n</ol>\n<p><strong><em>temp</em></strong> - temperature in Celsius\n<strong><em>atemp</em></strong> - &#8220;feels like&#8221; temperature in Celsius\n<strong><em>humidity</em></strong> - relative humidity\n<strong><em>windspeed</em></strong> - wind speed\n<strong><em>casual</em></strong> - number of non-registered user rentals initiated\n<strong><em>registered</em></strong> - number of registered user rentals initiated\n<strong><em>count</em></strong> - number of total&nbsp;rentals</p>\n<p>The data provided spans two years. The training set contains the first 19 days of each month considered, while the test set data corresponds to the remaining days in each&nbsp;month.</p>\n<p>Looking ahead, we anticipate that the year, month, day of week, and hour will serve as important features for characterizing the bike demand at any given moment.  These features are easily extracted from the datetime formatted-values loaded above. In the following lines, we add these features to our&nbsp;DataFrames.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">#</span><span class=\"n\">Feature</span> <span class=\"n\">engineering</span>\n<span class=\"n\">temp</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DatetimeIndex</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">])</span>\n<span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;year&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"k\">year</span>\n<span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;month&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"k\">month</span>\n<span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;hour&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"n\">hour</span>\n<span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;weekday&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"n\">weekday</span>\n\n<span class=\"n\">temp</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DatetimeIndex</span><span class=\"p\">(</span><span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">])</span>\n<span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;year&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"k\">year</span>\n<span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;month&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"k\">month</span>\n<span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;hour&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"n\">hour</span>\n<span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;weekday&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">temp</span><span class=\"p\">.</span><span class=\"n\">weekday</span>\n\n<span class=\"o\">#</span><span class=\"n\">Define</span> <span class=\"n\">features</span> <span class=\"n\">vector</span>\n<span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;season&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;holiday&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;workingday&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;weather&#39;</span><span class=\"p\">,</span>\n<span class=\"s1\">&#39;temp&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;atemp&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;humidity&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;windspeed&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;year&#39;</span><span class=\"p\">,</span>\n<span class=\"s1\">&#39;month&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;weekday&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;hour&#39;</span><span class=\"p\">]</span>\n</pre></div>\n\n\n<h2><strong>Evaluation&nbsp;metric</strong></h2>\n<p>The evaluation metric that Kaggle uses to rank competing algorithms is the Root Mean Squared Logarithmic Error (<span class=\"caps\">RMSLE</span>).</p>\n<div class=\"math\">\\begin{eqnarray}\nJ = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n [\\ln(p_i + 1) - \\ln(a_i+1)]^2 }\n\\end{eqnarray}</div>\n<p>\nHere,</p>\n<ul>\n<li><span class=\"math\">\\(n\\)</span> is the number of hours in the test&nbsp;set</li>\n<li><span class=\"math\">\\(p_i\\)</span> is the predicted number of bikes rented in a given&nbsp;hour</li>\n<li><span class=\"math\">\\(a_i\\)</span> is the actual rent&nbsp;count</li>\n<li><span class=\"math\">\\(ln(x)\\)</span> is the natural&nbsp;logarithm</li>\n</ul>\n<p>With ranking determined as above, our aim becomes to accurately guess the natural logarithm of bike demand at different times (actually demand count plus one, in order to avoid infinities associated with times where demand is nil). To facilitate this, we add the logarithm of the casual, registered, and total counts to our training DataFrame&nbsp;below.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\">#the evaluation metric is the RMSE in the log domain,</span>\n<span class=\"c1\">#so we should transform the target columns into log domain as well.</span>\n<span class=\"k\">for</span> <span class=\"n\">col</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;casual&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;registered&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;count&#39;</span><span class=\"p\">]:</span>\n  <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-&#39;</span> <span class=\"o\">+</span> <span class=\"n\">col</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"n\">col</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log1p</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n</pre></div>\n\n\n<p>Notice that in the code above we use the <span class=\"math\">\\(log1p()\\)</span> function instead of the more familiar <span class=\"math\">\\(log(1+x)\\)</span>. For large values of <span class=\"math\">\\(x\\)</span>, these two functions are actually equivalent. However, at very small values of <span class=\"math\">\\(x\\)</span>, the two can disagree. The source of the discrepancy is floating point error: For very small <span class=\"math\">\\(x\\)</span>, python will send <span class=\"math\">\\(1+x \\to 1\\)</span>, which when supplied as an argument to <span class=\"math\">\\(log(1+x)\\)</span> will return <span class=\"math\">\\(log(1)=0\\)</span>. The function <span class=\"math\">\\(log1p(x) \\sim x\\)</span> in this limit. The difference is not very important when the result is being added to other numbers, but can be very important in a multiplicative operation. We use this function instead for this reason. The inverse of <span class=\"math\">\\(log(x+1)\\)</span> is <span class=\"math\">\\(e^{x} -1\\)</span> &#8212; an operation we will also need to make use of later, in order to return linear-scale demand values. We&#8217;ll use an analog of the <span class=\"math\">\\(log1p()\\)</span> function, numpy&#8217;s <span class=\"math\">\\(expm1()\\)</span> function, to carry out this&nbsp;inversion.</p>\n<h2><strong>Model&nbsp;development</strong></h2>\n<h4><strong>A first&nbsp;pass</strong></h4>\n<p>The Gradient Boosting Machine (<span class=\"caps\">GBM</span>) we will be using has some associated hyperparameters that will eventually need to be optimized. These&nbsp;include:</p>\n<ul>\n<li>n_estimators = the number of boosting stages, or trees, to&nbsp;use.</li>\n<li>max_depth = maximum depth of the individual regression&nbsp;trees.</li>\n<li>learning_rate = shrinks the contribution of each tree by the learning&nbsp;rate.</li>\n<li>in_samples_leaf = the minimum number of samples required to be at a leaf&nbsp;node</li>\n</ul>\n<p>However, in order to get our feet wet, we&#8217;ll begin by just picking some ad hoc values for these parameters. The code below fits a <span class=\"caps\">GBM</span> to the log-demand training data, and then converts predicted log-demand into the competition&#8217;s required format &#8212; in particular, the demand is output in linear&nbsp;scale.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">clf</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">ensemble</span><span class=\"p\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span><span class=\"w\"></span>\n<span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"o\">[</span><span class=\"n\">features</span><span class=\"o\">]</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">train</span><span class=\"o\">[</span><span class=\"n\">&#39;log-count&#39;</span><span class=\"o\">]</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"k\">result</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">clf</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test</span><span class=\"o\">[</span><span class=\"n\">features</span><span class=\"o\">]</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"k\">result</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">expm1</span><span class=\"p\">(</span><span class=\"k\">result</span><span class=\"p\">)</span><span class=\"w\"></span>\n\n<span class=\"n\">df</span><span class=\"o\">=</span><span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"err\">{</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"err\">:</span><span class=\"n\">test</span><span class=\"o\">[</span><span class=\"n\">&#39;datetime&#39;</span><span class=\"o\">]</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s1\">&#39;count&#39;</span><span class=\"err\">:</span><span class=\"k\">result</span><span class=\"err\">}</span><span class=\"p\">)</span><span class=\"w\"></span>\n<span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;results1.csv&#39;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"k\">index</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"k\">False</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">columns</span><span class=\"o\">=[</span><span class=\"n\">&#39;datetime&#39;,&#39;count&#39;</span><span class=\"o\">]</span><span class=\"p\">)</span><span class=\"w\"></span>\n</pre></div>\n\n\n<p>In the last lines above, we have used the DataFrames to_csv() method in order to output results for competition submission. Example output is shown below. Without a hitch, we successfully submitted the results of this preliminary analysis to Kaggle. The only bad news was that our model scored in the bottom 10%. Fortunately, some simple optimizations that follow led to significant improvements in our&nbsp;standing.</p>\n<table>\n<thead>\n<tr>\n<th>datetime</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2011-01-20 0:00:00</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2011-01-20 0:01:00</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2011-01-20 0:02:00</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<h4><strong>Hyperparameter&nbsp;tuning</strong></h4>\n<p>We now turn to the challenge of tuning our <span class=\"caps\">GBM</span>&#8217;s hyperparameters. In order to carry this out, we segmented our training data into a training set and a validation set. The validation set allowed us to check the accuracy of our model locally, without having to submit to Kaggle. This also helped us to avoid overfitting&nbsp;issues.</p>\n<p>As mentioned earlier, the training data provided covers the first 19 days of each month. In segmenting this data, we opted to use days 17-19 for validation. We then used this validation set to optimize the model&#8217;s hyperparameters. As a first-pass at this, we again chose an ad hoc value for n_estimators, but optimized over the remaining degrees of freedom. The code follows, where we make use of GridSearchCV() to perform our parameter&nbsp;sweep.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\">#Split data into training and validation sets</span>\n<span class=\"n\">temp</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DatetimeIndex</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">])</span>\n<span class=\"n\">training</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"n\">temp</span><span class=\"o\">.</span><span class=\"n\">day</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">16</span><span class=\"p\">]</span>\n<span class=\"n\">validation</span> <span class=\"o\">=</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"n\">temp</span><span class=\"o\">.</span><span class=\"n\">day</span> <span class=\"o\">&gt;</span> <span class=\"mi\">16</span><span class=\"p\">]</span>\n\n<span class=\"n\">param_grid</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;learning_rate&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">],</span>\n              <span class=\"s1\">&#39;max_depth&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">],</span>\n              <span class=\"s1\">&#39;min_samples_leaf&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">est</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">)</span>\n<span class=\"c1\"># this may take awhile</span>\n<span class=\"n\">gs_cv</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span>\n<span class=\"n\">est</span><span class=\"p\">,</span> <span class=\"n\">param_grid</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n<span class=\"n\">training</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">],</span> <span class=\"n\">training</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-count&#39;</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># best hyperparameter setting</span>\n<span class=\"n\">gs_cv</span><span class=\"o\">.</span><span class=\"n\">best_params_</span>\n\n<span class=\"c1\">#Baseline error</span>\n<span class=\"n\">error_count</span> <span class=\"o\">=</span> <span class=\"n\">mean_absolute_error</span><span class=\"p\">(</span><span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-count&#39;</span><span class=\"p\">],</span> <span class=\"n\">gs_cv</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">]))</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">gs_cv</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">])</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expm1</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">:</span><span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">],</span> <span class=\"s1\">&#39;count&#39;</span><span class=\"p\">:</span><span class=\"n\">result</span><span class=\"p\">})</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;results2.csv&#39;</span><span class=\"p\">,</span> <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;count&#39;</span><span class=\"p\">])</span>\n</pre></div>\n\n\n<ul>\n<li>Note: If you want to run n_jobs &gt; 1 on a Windows machine, the script needs to be in an &#8220;if <strong>name</strong> == &#8216;<strong>main</strong>&#8216;:&#8221; block. Otherwise the script will&nbsp;fail.</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>day</th>\n<th>Best Parms</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>learning_rate</td>\n</tr>\n<tr>\n<td>2</td>\n<td>max_depth</td>\n</tr>\n<tr>\n<td>2</td>\n<td>min_samples_leaf</td>\n</tr>\n</tbody>\n</table>\n<p>The optimized parameters are shown above. Submitting the resulting model to Kaggle, we found that we had moved from the bottom 10% of models to the top 20%!  An awesome improvement, but we still have one final hyperparameter to&nbsp;optimize.</p>\n<h4><strong>Tuning the number of&nbsp;estimators</strong></h4>\n<p>In boosted models, training set performance will always improve as the number of estimators is increased. However, at large estimator number, overfitting can start to become an issue. Learning curves provide a method for optimization. These are constructed by plotting the error on both the training and validation sets as a function of the number of estimators used. The code below generates such a curve for our&nbsp;model.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">error_train</span><span class=\"o\">=</span><span class=\"p\">[]</span>\n<span class=\"n\">error_validation</span><span class=\"o\">=</span><span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">501</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">):</span>\n  <span class=\"n\">clf</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span>\n  <span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">05</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">,</span>\n  <span class=\"n\">min_samples_leaf</span> <span class=\"o\">=</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n\n  <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">],</span> <span class=\"n\">training</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-count&#39;</span><span class=\"p\">])</span>\n  <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">training</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">])</span>\n  <span class=\"n\">error_train</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n  <span class=\"n\">mean_absolute_error</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">training</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-count&#39;</span><span class=\"p\">]))</span>\n\n  <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">])</span>\n  <span class=\"n\">error_validation</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n  <span class=\"n\">mean_absolute_error</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">,</span> <span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-count&#39;</span><span class=\"p\">]))</span>\n\n<span class=\"c1\">#Plot the data</span>\n<span class=\"n\">x</span><span class=\"o\">=</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">501</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">style</span><span class=\"o\">.</span><span class=\"n\">use</span><span class=\"p\">(</span><span class=\"s1\">&#39;ggplot&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">error_train</span><span class=\"p\">,</span> <span class=\"s1\">&#39;k&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">error_validation</span><span class=\"p\">,</span> <span class=\"s1\">&#39;b&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Number of Estimators&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">18</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Error&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">18</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">([</span><span class=\"s1\">&#39;Train&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Validation&#39;</span><span class=\"p\">],</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">18</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Error vs. Number of Estimators&#39;</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">)</span>\n</pre></div>\n\n\n<p><img alt=\"Error vs Number of Estimators\" src=\"https://efavdb.com/wp-content/uploads/2015/03/figure_1-e1427234375629.png\"></p>\n<p>Notice in the plot that by the time the number estimators in our <span class=\"caps\">GBM</span> reaches about 80, the error of our model as applied to the validation set starts to slowly increase, though the error on the training set continues to decrease steadily. The diagnosis is that the model begins to overfit at this point. Moving forward, we will set n_estimators to 80, rather than 500, the value we were using above. Reducing the number of estimators reduced the calculated error and moved us to a higher position on the&nbsp;leaderboard.</p>\n<h2><strong>Separate models for registered and casual&nbsp;users</strong></h2>\n<p>Reviewing the data, we see that we have info regarding two types of riders: casual and registered riders. It is plausible that each group&#8217;s behavior differs, and that we might be able to improve our performance by modeling each separately. Below, we carry this out, and then also merge the two group&#8217;s predicted values to obtain a net predicted demand. We also repeat the hyperparameter sweep steps covered above &#8212; this returned similar values. Resubmitting the resulting model, we found we had increased our standing in the competition by a few&nbsp;percent.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">merge_predict</span><span class=\"p\">(</span><span class=\"n\">model1</span><span class=\"p\">,</span> <span class=\"n\">model2</span><span class=\"p\">,</span> <span class=\"n\">test_data</span><span class=\"p\">):</span>\n  <span class=\"c1\"># Combine the predictions of two separately trained models.</span>\n  <span class=\"c1\"># The input models are in the log domain and returns the predictions</span>\n  <span class=\"c1\"># in original domain.</span>\n  <span class=\"n\">p1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expm1</span><span class=\"p\">(</span><span class=\"n\">model1</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test_data</span><span class=\"p\">))</span>\n  <span class=\"n\">p2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expm1</span><span class=\"p\">(</span><span class=\"n\">model2</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">test_data</span><span class=\"p\">))</span>\n  <span class=\"n\">p_total</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">p1</span><span class=\"o\">+</span><span class=\"n\">p2</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span><span class=\"p\">(</span><span class=\"n\">p_total</span><span class=\"p\">)</span>\n<span class=\"n\">est_casual</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">05</span><span class=\"p\">)</span>\n<span class=\"n\">est_registered</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">05</span><span class=\"p\">)</span>\n<span class=\"n\">param_grid2</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;max_depth&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">],</span>\n               <span class=\"s1\">&#39;_samples_leaf&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">],</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">gs_casual</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">est_casual</span><span class=\"p\">,</span> <span class=\"n\">param_grid2</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">],</span> <span class=\"n\">training</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-casual&#39;</span><span class=\"p\">])</span>\n<span class=\"n\">gs_registered</span> <span class=\"o\">=</span> <span class=\"n\">GridSearchCV</span><span class=\"p\">(</span><span class=\"n\">est_registered</span><span class=\"p\">,</span> <span class=\"n\">param_grid2</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">training</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">],</span> <span class=\"n\">training</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-registered&#39;</span><span class=\"p\">])</span>\n\n<span class=\"n\">result3</span> <span class=\"o\">=</span> <span class=\"n\">merge_predict</span><span class=\"p\">(</span><span class=\"n\">gs_casual</span><span class=\"p\">,</span> <span class=\"n\">gs_registered</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">])</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">:</span><span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">],</span> <span class=\"s1\">&#39;count&#39;</span><span class=\"p\">:</span><span class=\"n\">result3</span><span class=\"p\">})</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;results3.csv&#39;</span><span class=\"p\">,</span> <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;count&#39;</span><span class=\"p\">])</span>\n</pre></div>\n\n\n<p>The last step is to submit a final set of model predictions, this time training on the full labeled dataset provided. With these simple steps, we ended up in the top 11% on the competition&#8217;s leaderboard with a rank of&nbsp;280/2467!</p>\n<p><a href=\"https://efavdb.com/wp-content/uploads/2015/03/score.png\"><img alt=\"score\" src=\"https://efavdb.com/wp-content/uploads/2015/03/score.png\"></a></p>\n<div class=\"highlight\"><pre><span></span><span class=\"o\">&lt;</span><span class=\"n\">pre</span><span class=\"o\">&gt;</span>\n<span class=\"n\">est_casual</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span>\n<span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">05</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"n\">min_samples_leaf</span> <span class=\"o\">=</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n<span class=\"n\">est_registered</span> <span class=\"o\">=</span> <span class=\"n\">ensemble</span><span class=\"o\">.</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span>\n<span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"o\">.</span><span class=\"mi\">05</span><span class=\"p\">,</span> <span class=\"n\">max_depth</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"n\">min_samples_leaf</span> <span class=\"o\">=</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n\n<span class=\"n\">est_casual</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-casual&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span>\n<span class=\"n\">est_registered</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">[</span><span class=\"s1\">&#39;log-registered&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span>\n<span class=\"n\">result4</span> <span class=\"o\">=</span> <span class=\"n\">merge_predict</span><span class=\"p\">(</span><span class=\"n\">est_casual</span><span class=\"p\">,</span> <span class=\"n\">est_registered</span><span class=\"p\">,</span> <span class=\"n\">test</span><span class=\"p\">[</span><span class=\"n\">features</span><span class=\"p\">])</span>\n\n<span class=\"n\">df</span><span class=\"o\">=</span><span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">:</span><span class=\"n\">test</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">],</span> <span class=\"s1\">&#39;count&#39;</span><span class=\"p\">:</span><span class=\"n\">result4</span><span class=\"p\">})</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">to_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;results4.csv&#39;</span><span class=\"p\">,</span> <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;datetime&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;count&#39;</span><span class=\"p\">])</span>\n</pre></div>\n\n\n<p><strong><span class=\"caps\">DISCUSSION</span></strong></p>\n<p>By iteratively tuning a <span class=\"caps\">GBM</span>, we were able to quickly climb the leaderboard for this particular Kaggle competition. With further feature extraction work, we believe further improvements could readily be made. However, our goal here was only to practice our rapid development skills, so we won&#8217;t be spending much time on further fine-tuning. At any rate, our results have convinced us that simple boosted models can often provide excellent&nbsp;results.</p>\n<p><a href=\"https://github.com/EFavDB/bike-forecast\" title=\"GitHub Repo\"><img alt=\"Open GitHub Repo\" src=\"https://efavdb.com/wp-content/uploads/2015/03/GitHub_Logo.png\"></a>\nOpen GitHub&nbsp;Repo</p>\n<p>Note: With this post, we have begun to post our python scripts and data at GitHub. Clicking on the icon at left will take you to our repository. Feel free to stop by and take a&nbsp;look!</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}