{
  "title": "Building ML Pipeline: 6 Problems &#038; Solutions [From a Data Scientist&#8217;s Experience]",
  "link": "https://neptune.ai/blog/building-ml-pipeline-problems-solutions",
  "dc:creator": "Thomas Epelbaum",
  "pubDate": "Fri, 02 Sep 2022 20:00:55 +0000",
  "category": [
    "MLOps",
    "mlops"
  ],
  "guid": "https://neptune.ai/?p=65705",
  "description": "<p>Long gone is the time where ML jobs start and end with a jupyter notebook.&#160;&#160; Since all companies want to deploy their models into production, having an efficient and rigorous MLOps pipeline to do so is a real challenge that ML engineers have to face nowadays.&#160; But creating such a pipeline is not an easy [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://neptune.ai/blog/building-ml-pipeline-problems-solutions\">Building ML Pipeline: 6 Problems &#038; Solutions [From a Data Scientist&#8217;s Experience]</a> appeared first on <a rel=\"nofollow\" href=\"https://neptune.ai\">neptune.ai</a>.</p>\n",
  "content:encoded": "\n<p>Long gone is the time where ML jobs start and end with a jupyter notebook.&nbsp;&nbsp;</p>\n\n\n\n<p>Since all companies want to deploy their models into production, having an efficient and rigorous MLOps pipeline to do so is a real challenge that ML engineers have to face nowadays.&nbsp;</p>\n\n\n\n<p>But creating such a pipeline is not an easy task, given how new the MLOps tools are. Indeed, the field itself is no more than a couple of years old for the vast majority of medium-sized companies. Thus creating such a pipeline can only be accomplished through trial and error, and the mastering of numerous tools/libraries is needed.</p>\n\n\n\n<p>In this article, I will introduce you to</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>common pitfalls I have seen in the previous companies I have been working at,&nbsp;</li><li>and how I managed to solve them.</li></ul>\n</div>\n\n\n<p>This is by no means the end of the story, though, and I am sure that the MLOps field will be at a way more mature level two years from now. But by showing you the challenges I faced, I hope you will learn something in the process. I sure did!</p>\n\n\n\n<p>So here we go!</p>\n\n\n\n<p><strong>A short note on the author</strong></p>\n\n\n\n<p>Before proceeding, it may be enlightening for you to have a little background on me.</p>\n\n\n\n<p>I am a French engineer who did a master&#8217;s and a Ph.D. in particle physics before leaving the research ecosystem to join the industry one, as I wanted to have a more direct impact on society. At the time (2015), I only developed codes for myself and maybe 1-2 co-authors, and you can therefore guess my production-compatible coding abilities (in case you did not: there were none :)).&nbsp;</p>\n\n\n\n<p>But since then, I have contributed to different codebases in different languages (C# and Python mostly), and even if I am not a developer by formation, I have seen more than once what works and what does not :).</p>\n\n\n\n<p>In order to not destroy all of my credibility before even starting the journey, let me hasten to add that I do have a non-zero knowledge of deep learning (this <a href=\"https://arxiv.org/abs/1709.01412\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">white book</a> made available to the community on <a href=\"https://github.com/tomepel/Technical_Book_DL\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">github</a> in 2017 can hopefully attest to this fact :)).</p>\n\n\n\n<h2>Building MLOps pipelines: the most common problems I encountered</h2>\n\n\n\n<p>Here are the 6 most common pitfalls I have encountered during my ML activity in the past 6 years.</p>\n\n\n<div id=\"block_63123477d1bd2\" class=\"separator separator-10\"></div>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Building-mlops-pipeline-problems.jpg?ssl=1\"><img data-attachment-id=\"71149\" data-permalink=\"https://neptune.ai/building-mlops-pipeline-lessons-learned/attachment/building-mlops-pipeline-problems\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Building-mlops-pipeline-problems.jpg?fit=1024%2C1331&ssl=1\" data-orig-size=\"1024,1331\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Building-mlops-pipeline-problems\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Building-mlops-pipeline-problems.jpg?fit=231%2C300&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Building-mlops-pipeline-problems.jpg?fit=788%2C1024&ssl=1\" decoding=\"async\" width=\"788\" height=\"1024\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Building-mlops-pipeline-problems.jpg?resize=788%2C1024&#038;ssl=1\" alt=\"\" class=\"wp-image-71149\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>Iâ€™ll dig into each of one them throughout the article, first presenting the problem and then offering a possible solution.&nbsp;</p>\n\n\n\n<h3>Problem 1:&nbsp;POC &#8211; style code</h3>\n\n\n\n<p>More often than not, I encountered code bases developed in a Proof Of Concept (POC) style.</p>\n\n\n\n<p>For instance, to release a model into production,&nbsp; one may have to chain 5 to 10 <a href=\"https://github.com/pallets/click\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">click</a> commands (or even worse, argparse!)&nbsp; in order to be able to:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>preprocess data</li><li>featurize data</li><li>train an ML model</li><li>export the ML model into production</li><li>produce a CSV report on the model performance</li></ul>\n</div>\n\n\n<p>In addition, it is very common to need to edit the code between two commands for the full process to work.&nbsp;</p>\n\n\n\n<p>This is normal in startups, they want to build innovative products, and they want to build them fast. But in my experience, leaving a code base at the POC level is a long-term recipe for disaster.&nbsp;</p>\n\n\n\n<p>Indeed, adding new features in this manner becomes more and more costly as maintenance costs become higher and higher. Another factor worth considering is that in companies with even regular turnover, each leave with this kind of code base has a real impact on the structure speed.</p>\n\n\n\n<h3>Problem 2: No high-level separation of concerns</h3>\n\n\n\n<p>The separation of concerns in ML code bases is often missing at a high level. What this means is that more often than not, so-called ML code is also doing feature transformations like operations that have nothing to do with ML â€“ think physical document ingestion, conversion of administrative data, etc.</p>\n\n\n\n<p>In addition, the dependencies between these modules are often not well thought out. Look at fantasy diagram created by a small wrapper coded by me (I aim to release it on PyPI one day :)) and based on the excellent <a href=\"https://github.com/thebjorn/pydeps\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">pydeps</a> that gives a code base dependencies at the regroupment of module levels (this is closer to real life situations that you might think :):</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies.png?ssl=1\"><img data-attachment-id=\"65722\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-dependencies\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies.png?fit=1600%2C938&ssl=1\" data-orig-size=\"1600,938\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-dependencies\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies.png?fit=300%2C176&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies.png?fit=1024%2C600&ssl=1\" decoding=\"async\" width=\"1024\" height=\"600\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies.png?resize=1024%2C600&#038;ssl=1\" alt=\"MLOps pipeline dependencies\" class=\"wp-image-65722\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>To me, the most worrisome aspect of this diagram is the number of cyclic dependencies present between what seems to be low-level packages and high-level ones.</p>\n\n\n\n<p>Another thing that I personally interpret as a not well-thought-out architecture is a large utils folder, and it is very common to see utils folders with dozen of modules in ML codebases.</p>\n\n\n\n<h3>Problem 3: No low-level separation of concerns</h3>\n\n\n\n<p>The separation of concerns in the code is unfortunately often missing at a low level as well. When this happens, you end up with 2000+ line classes handling almost everything: Featurization, preprocessing, building the model graph, training, predicting, exportingâ€¦ You name it, those master classes have your bases covered (only coffee is missing, and sometimes you never knowâ€¦ :)). But as you know, this is not what the S of the <a href=\"https://en.wikipedia.org/wiki/SOLID\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">SOLID</a> would recommend.&nbsp;</p>\n\n\n\n<h3>Problem 4: No configuration Data Model</h3>\n\n\n\n<p>A data model for handling ML configuration is often missing. For instance, this is what a fantasy model hyperparameter declaration could look like (again, closer to real-life situations than you might think).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full is-resized\"><img data-attachment-id=\"65727\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-hyperparameters-2\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameters-2.png?fit=375%2C87&ssl=1\" data-orig-size=\"375,87\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-hyperparameters-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameters-2.png?fit=300%2C70&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameters-2.png?fit=375%2C87&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameters-2.png?resize=375%2C87&#038;ssl=1\" alt=\"MLOps pipeline hyperparameters\" class=\"wp-image-65727\" width=\"375\" height=\"87\" data-recalc-dims=\"1\" /><figcaption><em>350+ lines dictionaries :â€™(</em></figcaption></figure></div>\n\n\n<p>Even more problematic (but comprehensible), this allowed for dynamic modification of the model configuration (fantasy snippet inspired from numerous real-life situations):</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-model-configuration-1.png?ssl=1\"><img data-attachment-id=\"69781\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-model-configuration-1\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-model-configuration-1.png?fit=342%2C311&ssl=1\" data-orig-size=\"342,311\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-model-configuration-1\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-model-configuration-1.png?fit=300%2C273&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-model-configuration-1.png?fit=342%2C311&ssl=1\" decoding=\"async\" width=\"342\" height=\"311\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-model-configuration-1.png?resize=342%2C311&#038;ssl=1\" alt=\"MLOps pipeline model configuration\" class=\"wp-image-69781\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>As one can see in the fantasy code snippet above, the `params` attribute is modified in place. When this happens at several places in the code (and trust me, it does when you start going down that road), you end up with a code that is a real nightmare to debug, as what you put into configurations is not necessarily what arrives in the subsequent ML pipeline steps.</p>\n\n\n\n<h3>Problem 5: Handling legacy models&nbsp;</h3>\n\n\n\n<p>Since the process of training a ML model often involves manual efforts (see problem 1) it can take really long to do so. It is also prone to some errors (when a human is in the loop errors are also :)). In that case, you end up with (fantasy code snippet) stuff like this:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-legacy-models.png?ssl=1\"><img data-attachment-id=\"65729\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-legacy-models\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-legacy-models.png?fit=749%2C181&ssl=1\" data-orig-size=\"749,181\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-legacy-models\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-legacy-models.png?fit=300%2C72&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-legacy-models.png?fit=749%2C181&ssl=1\" decoding=\"async\" width=\"749\" height=\"181\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-legacy-models.png?resize=749%2C181&#038;ssl=1\" alt=\"MLOps pipeline legacy models\" class=\"wp-image-65729\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p><em>Hint: look at the docstring date <img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png\" alt=\"ðŸ™‚\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></em></p>\n\n\n\n<h3>Problem 6: Code quality: type hinting, documentation, complexity, dead code&nbsp;</h3>\n\n\n\n<p>As the above fantasy code snippets can attest, type hinting is rarely present when it is needed the most. I can guess that <em>n_model_to_keep</em> is an int, but would be hard-pressed naming the types of <em>graph_configuration</em> in the code snippet of problem 5 .&nbsp;</p>\n\n\n\n<p>In addition, ML code bases I encountered often had a limited amount of docstring, and modern concepts for code quality like cyclomatic/cognitive complexity or working memory (see <a href=\"https://sourcery.ai/blog/working-memory/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">this</a> post to learn more about it) are not respected.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-code-quality.png?ssl=1\"><img data-attachment-id=\"65730\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-code-quality\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-code-quality.png?fit=945%2C234&ssl=1\" data-orig-size=\"945,234\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-code-quality\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-code-quality.png?fit=300%2C74&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-code-quality.png?fit=945%2C234&ssl=1\" decoding=\"async\" width=\"945\" height=\"234\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-code-quality.png?resize=945%2C234&#038;ssl=1\" alt=\"MLOps pipeline code quality\" class=\"wp-image-65730\" data-recalc-dims=\"1\"/></a><figcaption><em>Fantasy code snippet. even the </em><a href=\"https://github.com/marcoemrich/Refactoring-Katas/blob/master/GildedRose/python/gilded_rose.py\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"><em>famous python refactoring kata</em></a><em> is beaten!&nbsp;</em></figcaption></figure></div>\n\n\n<p>Finally, unknown to all, a lot of dead code is often present in the solution. In this case, you might scratch your head during several days when adding a new feature before realizing that the code you do not manage to make it work with this new feature is not even called (again, true story)!</p>\n\n\n\n<h2>Building MLOps pipelines: how I solved these problems</h2>\n\n\n\n<p>Let&#8217;s now look at solutions I found (of course with the help of my collaborators along the years) to the 6 pressing problems discussed above and give you an overview of where I would be if I had to develop a new MLOPS pipeline now.</p>\n\n\n\n<h3>Solution 1: from POC to prod</h3>\n\n\n\n<p>Thanks to <a href=\"https://typer.tiangolo.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Typer</a>, a lot of click/argpase boilerplate code can be suppressed from command lines.&nbsp;</p>\n\n\n\n<p>I am a big fan of a couple of mantras:</p>\n\n\n<div class=\"custom-point-list\">\n<ol><li>The best code is the one you donâ€™t need to write (<a href=\"https://www.folklore.org/StoryView.py?story=Negative_2000_Lines_Of_Code.txt\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">funny</a> folklore on this).&nbsp;</li><li>When an observation starts to be used as a metric (in this case, the number of lines written to attest all the work done), it stops being a good observation.&nbsp;</li></ol>\n</div>\n\n\n<p>Here is, in my opinion, a good high-level command signature to launch an end-to-end ML model training:&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameter-optimization.png?ssl=1\"><img data-attachment-id=\"65731\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-hyperparameter-optimization\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameter-optimization.png?fit=809%2C238&ssl=1\" data-orig-size=\"809,238\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-hyperparameter-optimization\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameter-optimization.png?fit=300%2C88&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameter-optimization.png?fit=809%2C238&ssl=1\" decoding=\"async\" width=\"809\" height=\"238\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-hyperparameter-optimization.png?resize=809%2C238&#038;ssl=1\" alt=\"MLOps pipeline hyperparameter optimization\" class=\"wp-image-65731\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>TL DR: use Typer for all your command line tools.</p>\n\n\n\n<h3>Solution 2: Handling high-level separation of concerns â€“ from ML monolith to ML microservices</h3>\n\n\n\n<p>This is a big one that took me a long time to improve on. As I guess most of my readers are today, I am on the side of the microservice in the microservice/monolith battle (though I know that microservices are not a miracle that solve all development issues with a finger snap). With docker and docker-compose used to encompass the different services, you can improve on the functionalities of your architecture incrementally and in isolation with the rest of the already implemented features. Unfortunately, ML docker architecture often looks like this:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"65732\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-ml-container\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container.png?fit=164%2C77&ssl=1\" data-orig-size=\"164,77\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-ML-container\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container.png?fit=164%2C77&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container.png?fit=164%2C77&ssl=1\" decoding=\"async\" width=\"164\" height=\"77\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container.png?resize=164%2C77&#038;ssl=1\" alt=\"MLOps pipeline ML container\" class=\"wp-image-65732\" data-recalc-dims=\"1\"/><figcaption><em>A typical docker ML container architecture</em></figcaption></figure></div>\n\n\n<p>Now I would advocate for something more like this (with the data processing parts also acknowledged):</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container-2.png?ssl=1\"><img data-attachment-id=\"65733\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-ml-container-2\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container-2.png?fit=1353%2C485&ssl=1\" data-orig-size=\"1353,485\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-ML-container-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container-2.png?fit=300%2C108&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container-2.png?fit=1024%2C367&ssl=1\" decoding=\"async\" width=\"1024\" height=\"367\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-ML-container-2.png?resize=1024%2C367&#038;ssl=1\" alt=\"MLOps pipeline ML container\" class=\"wp-image-65733\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>The data ingestion and storing functionalities that are not ML related are now&nbsp; delegated to a dedicated feature-store container. It stores the data it ingests into a <a href=\"https://hub.docker.com/_/mongo\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">MongoDB</a> (I am used to work with non structured documents, but of course if you are also/only dealing with structured data use a <a href=\"https://hub.docker.com/_/postgres\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Postgresql</a> container) container, after having processed the documents it is fed with via calls to a <a href=\"https://gotenberg.dev/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">gotenberg</a> container (a very useful off the shelf container to handle documents).</p>\n\n\n\n<p>The ML is here split into three parts:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>A Computer Vision part: document-recognition container,</strong> applying computer vision techniques to documents Think the usual suspects: open-cv, Pillowâ€¦ . I have experience doing the labeling with the help of a <a href=\"https://github.com/Slava/label-tool\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">label-tool</a> container, but there are a lot of alternatives out there.</li><li><strong>An NLP part: NLP</strong>, with a container applying NLP techniques to the texts extracted from the documents. Think the usual suspects: nltk, Spacy,&nbsp; DL/BERTâ€¦ I have experience doing the labeling with the help of a <a href=\"https://github.com/doccano/doccano\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">doccano</a> container, and in my opinion there are no better alternatives out there :).</li><li><strong>A core DL part: a&nbsp; pytorch_dl container</strong>. I migrated from TensorFlow to PyTorch in all my DL activities, as interacting with TensorFlow was a source of frustration for me. Some of the problems I faced:<ul><li>It was slow and prone to error in my developments,&nbsp;</li><li>Lack of support on the official github (some issues have been sitting there for years!),&nbsp;</li><li>Difficulty to debug (even if the eager mode of tensorflow2 has mitigated this point to some extent).&nbsp;</li></ul></li></ul>\n</div>\n\n\n<p>You must have heard that codebases and functionalities should only be changed incrementally. In my experience, this is true and good advice 95% percent of the time. But 5% of the time things are so entangled and the danger of silently breaking by doing incremental changes is so high (low test coverage, I am looking at you) that I recommend rewriting everything from scratch in a new package, ensuring that the new package has the same features as the old one and thereafter, unplugging the faulty code in one stroke to plug in the new one.&nbsp;&nbsp;</p>\n\n\n\n<p>I have handled TensorFlow to PyTorch migrations in my previous experiences as one of these occasions.</p>\n\n\n\n<p>To implement PyTorch networks, I recommend using <a href=\"https://www.pytorchlightning.ai/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Pytorch Lightning</a> which is a very concise and easy-to-use high-level library around PyTorch. To gauge the difference, the lines of code in my old TensorFlow codebases are in the order of thousands, whereas with Pytorch Lightning you can accomplish more with ten times less code. I usually handle in these different modules the DL concepts:</p>\n\n\n\n<p>&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><img data-attachment-id=\"65734\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-pytorch-lightning\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning.png?fit=142%2C157&ssl=1\" data-orig-size=\"142,157\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-Pytorch-lightning\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning.png?fit=142%2C157&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning.png?fit=142%2C157&ssl=1\" decoding=\"async\" width=\"142\" height=\"157\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning.png?resize=142%2C157&#038;ssl=1\" alt=\"MLOps pipeline Pytorch lightning\" class=\"wp-image-65734\" data-recalc-dims=\"1\"/><figcaption><em>The PyTorch-dependent code</em></figcaption></figure></div>\n\n\n<p>Thanks to PyTorch Lightning, each module is less than 50 lines long (except for network :)).&nbsp;</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning-2.png?ssl=1\"><img data-attachment-id=\"65735\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-pytorch-lightning-2\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning-2.png?fit=823%2C645&ssl=1\" data-orig-size=\"823,645\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-Pytorch-lightning-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning-2.png?fit=300%2C235&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning-2.png?fit=823%2C645&ssl=1\" decoding=\"async\" width=\"823\" height=\"645\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-Pytorch-lightning-2.png?resize=823%2C645&#038;ssl=1\" alt=\"MLOps pipeline Pytorch lightning\" class=\"wp-image-65735\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>The Trainer is a marvel, and you can use the experiment logger of your choice in a finger snap. I started my journey with the good old TensorBoard logger, coming from the TensorFlow ecosystem. But as you can see on the above screen, I recently started to use one of its alternatives: yes, you guessed it, <a href=\"/\" target=\"_blank\" rel=\"noreferrer noopener\">neptune.ai</a>, and I am loving it so far. With <a href=\"https://docs.neptune.ai/getting-started/how-to-add-neptune-to-your-code\" target=\"_blank\" rel=\"noreferrer noopener\">as little code as the one you see in the code snippet above,</a> you end up with all your models stored in a very user-friendly manner on the Neptune dashboard.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-neptune-dashboard.png?ssl=1\"><img data-attachment-id=\"65738\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-neptune-dashboard\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-neptune-dashboard.png?fit=1316%2C838&ssl=1\" data-orig-size=\"1316,838\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-neptune-dashboard\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-neptune-dashboard.png?fit=300%2C191&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-neptune-dashboard.png?fit=1024%2C652&ssl=1\" decoding=\"async\" width=\"1024\" height=\"652\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-neptune-dashboard.png?resize=1024%2C652&#038;ssl=1\" alt=\"MLOps pipeline neptune dashboard\" class=\"wp-image-65738\" data-recalc-dims=\"1\"/></a><figcaption><em>Metrics and losses tracked in the Neptune UI</em></figcaption></figure></div>\n\n\n<p>For hyperparameter optimization, I switched from <a href=\"http://hyperopt.github.io/hyperopt/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Hyperopt</a> to <a href=\"https://optuna.org/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Optuna</a> over the years, following this in-depth <a href=\"/blog/optuna-vs-hyperopt\">blog post</a>. Reasons for this switch were numerous. Among others:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>Poor Hyperopt documentation</li><li>Ease of integration with PyTorch Lightning for optuna</li><li>Visualization of the hyperparameter search</li></ul>\n</div>\n\n\n<p>Tips that will save you a LOT of time: to allow graceful model restart after the pytorch_dl container crashes for whatever reason (server reboot, server low on resources, etc.), I replay the whole <a href=\"https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">TPEsamplings</a> of the finished runs with the same random seed, and start the unfinished trial from the last saved checkpoint. This allows me not to waste hours on an unfinished run each time something unexpected happens on a server.&nbsp;</p>\n\n\n\n<p>For my R&D experiments I use <a href=\"https://doc.ubuntu-fr.org/screen\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">screen</a> and more and more <a href=\"https://doc.ubuntu-fr.org/tmux\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">tmux</a> (a <a href=\"https://books.google.fr/books/about/Tmux_2.html?id=ugsMvgAACAAJ&redir_esc=y\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">good ref</a> on tmux) scripts to launch hyperparameter optimization runs.&nbsp;</p>\n\n\n\n<p>Hyperparameter comparison is very easy thanks to<a href=\"https://plotly.com/python/parallel-coordinates-plot/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> plotly parallel coordinate</a> plot.</p>\n\n\n\n<p>Finally, I use a custom reporter container to compile a tex template into a beamer pdf. Think jinja2 like <a href=\"https://en.wikipedia.org/wiki/LaTeX\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">tex</a> template that you fill with PNGs and CSVs specific to each run to produce a PDF that is the perfect conversation starter with the businesses/clients when they come to understand Machine Learning Model performance (main confusions, label repartition, performance, etc.).</p>\n\n\n\n<p>These architecture patterns drastically simplify coding new functionalities. If you are familiar with <a href=\"https://en.wikipedia.org/wiki/Accelerate_(book)\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Accelerate</a>, then you know it is no lie that having a good codebase can reduce the time taken to implement a new feature by a factor of 10 to 50, and I can attest to it :).</p>\n\n\n\n<p>Should you need to add a message broker to your microservice architecture, I can recommend <a href=\"https://www.rabbitmq.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">rabbit MQ</a> as it is a breeze to plug within a python code thanks to the pika library. But here I have nothing to say on the alternatives (except readings: <a href=\"https://www-inf.telecom-sudparis.eu/COURS/CILS-IAAIO/Articles/debs_kafka_versus_rabbitmq.pdf\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">kafka</a>, redisâ€¦) as I have never worked with them so far :).</p>\n\n\n\n<h3>Solution 3: Handling low-level separation of concerns â€“ good code architecture</h3>\n\n\n\n<p>Having a clear separation of concerns between containers allows to have a very clean container-level architecture. Look at this fantasy (but the one I advocate! :)) dependency graph for a pytorch_dl container:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-full\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies-2.png?ssl=1\"><img data-attachment-id=\"65746\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-dependencies-2\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies-2.png?fit=702%2C477&ssl=1\" data-orig-size=\"702,477\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-dependencies-2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies-2.png?fit=300%2C204&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies-2.png?fit=702%2C477&ssl=1\" decoding=\"async\" width=\"702\" height=\"477\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-dependencies-2.png?resize=702%2C477&#038;ssl=1\" alt=\"MLOps pipeline dependencies\" class=\"wp-image-65746\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>and the chronology of the different module actions:</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-chronology-of-actions.png?ssl=1\"><img data-attachment-id=\"65747\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-chronology-of-actions\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-chronology-of-actions.png?fit=1438%2C320&ssl=1\" data-orig-size=\"1438,320\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-chronology-of-actions\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-chronology-of-actions.png?fit=300%2C67&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-chronology-of-actions.png?fit=1024%2C228&ssl=1\" decoding=\"async\" width=\"1024\" height=\"228\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-chronology-of-actions.png?resize=1024%2C228&#038;ssl=1\" alt=\"MLOps pipeline chronology of actions\" class=\"wp-image-65747\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>High level view of the different regroupment of modules I advocate for:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>Adapters transform a raw CSV to a CSV dedicated to a particular prediction task.</li><li>Filterers remove rows of the passed CSV if they fail to pass given filtering criteria (too rare label, etc). For both filterers and adapters, I often have generic classes implementing all the adapting and filtering logic and inheriting classes overriding the specific adapting/filtering logic of each given filter/adapter (<a href=\"https://www.youtube.com/watch?v=xvb5hGLoK0A\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Resource on ABC/protocols)</a>.</li><li>Featurizers are always based on sklearn and essentially convert a CSV into a dictionary of feature names (string) to NumPy arrays. Here I wrap the usual suspects (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">TfidfVectorizer</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">StandardScaler</a>) into my own classes, essentially because (for a reason unknown to me), sklearn does not offer memoization for its featurizers. I do not want to use <a href=\"https://pypi.org/project/pickle5/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">pickle</a> as it is not a <a href=\"https://medium.com/ochrona/python-pickle-is-notoriously-insecure-d6651f1974c9\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">security-compliant</a> library and does not offer any protection against <a href=\"https://github.com/scikit-learn/scikit-learn/issues/16033#issuecomment-571656393\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">sklearn version changes</a>. I thus always use a homemade improvement on <a href=\"https://thiagomarzagao.com/2015/12/08/saving-TfidfVectorizer-without-pickles/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">t</a>his.</li><li>PyTorch contains the Dataset, Dataloader, and Trainer logic.</li><li>Model reports produce the pdf beamer reports already talked about above</li><li>Taggers regroup deterministic techniques to predict (think expert rules) on rare data, for instance. In my experience, the performance of&nbsp; DL models can be improved with human knowledge, and you should always consider the possibility of doing so if feasible.</li><li>MLConfiguration contains the ML data model: enums and classes that do not contain any processing methods. Think Hyperparameter class, PredictionType Enum, etc. Side note: use Enums over strings at all places where it makes sense (closed list of things)&nbsp;</li><li>The pipeline plugs together all the elementary bricks.</li><li>Routes contain the <a href=\"https://fastapi.tiangolo.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">FastAPI</a> routes that allow other containers to ask for predictions on new data. Indeed I left <a href=\"https://flask.palletsprojects.com/en/2.1.x/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Flask</a> aside for the same reasons that I left-click aside for Typer â€“ less boilerplate, ease of use and maintainability, and even more functionalities. <a href=\"https://github.com/tiangolo\">Tiangolo</a> is a god :).&nbsp; I glanced at <a href=\"https://pytorch.org/serve/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">TorchS</a>erve to serve models, but given the project sizes I have been working on in my career, I did not yet feel necessary to commit to it. Plus TorchServe is (as of July 2022) still in its infancy.&nbsp;</li></ul>\n</div>\n\n\n<p>I now always enforce regroupment of modules dependencies of my different codebases with a custom <a href=\"https://pre-commit.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">pre-commit</a> hook. This means that each time someone tries to add new code that adds a new depency, a discussion is triggered between collaborators to evaluate the relevance of this new dependency. For instance, I see no reason as of today to create a dependency on model reports from pytorch given the architecture I presented. And would always vote against ml_configuration depending on anything.</p>\n\n\n\n<h3>Solution 4: Simple configuration Data Model thanks to Pydantic</h3>\n\n\n\n<p>To avoid config in code as an untyped giant dictionary, I enforce the use of <a href=\"https://pydantic-docs.helpmanual.io/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Pydantic</a> for all configuration/Data model classes. I even got inspiration from the best Disney movies <img src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png\" alt=\"ðŸ™‚\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /> (see code snippet)</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-configuration.png?ssl=1\"><img data-attachment-id=\"65754\" data-permalink=\"https://neptune.ai/attachment/mlops-pipeline-configuration\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-configuration.png?fit=1197%2C592&ssl=1\" data-orig-size=\"1197,592\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"MLOps-pipeline-configuration\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-configuration.png?fit=300%2C148&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-configuration.png?fit=1024%2C506&ssl=1\" decoding=\"async\" width=\"1024\" height=\"506\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-pipeline-configuration.png?resize=1024%2C506&#038;ssl=1\" alt=\"MLOps pipeline configuration\" class=\"wp-image-65754\" data-recalc-dims=\"1\"/></a></figure></div>\n\n\n<p>This enforces a configuration defined in one and only one place, hopefully in a JSON file outside the code, and thanks to Pydantic one-liners to serialize and deserialize the configuration. I kept an eye on <a href=\"https://hydra.cc/docs/intro/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Hydra</a>, but as explained <a href=\"https://www.youtube.com/watch?v=tEsPyYnzt8s&t=1s\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">here</a> (very good channel) for instance, the framework may be too young and will presumably be more mature and more natural in a few months/years.&nbsp;</p>\n\n\n\n<p>In order to update the frozen configuration with the optuna trial, I usually just define a dictionary of mute actions (a mute action value for each hyperparameter key present in the optuna trial).</p>\n\n\n\n<h3>Solution 5: Handling legacy models with frequent automatic retrains</h3>\n\n\n\n<p>Since the entry point to train a model is a unique Typer command (if you followed solutions 1 to 4 :)), it is easy to <a href=\"https://doc.ubuntu-fr.org/cron\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">cron</a> it periodically and automatically re-train models. Thanks to the reports and the metrics it contains, you then have two levels to decide whether to put the new model in production or not.</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li><strong>Automatic, high-level:</strong> if the macro performance of the new model is better than the old one, put the new model in production.</li><li><strong>Manual, fine-grained:</strong> an expert can compare the two models in detail and conclude that even if a certain model is somewhat worse than another in terms of overall performance, it could be better if its predictions make more sense when it&#8217;s wrong. For instance&nbsp; (here comes a completely fake vision example to clearly illustrate the point on ImageNet), the second conflates tigers with lions when itâ€™s wrong whereas the first model predicts bees.</li></ul>\n</div>\n\n\n<p>What do I mean by exporting a model into production? In the framework depicted above, it is essentially just copying a model folder from one location to another. Then one of the high-level configuration classes can load all of this in one, in order to do new predictions via FastApi and (in fine) PyTorch. From my experience, PyTorch eases this procedure. With TensorFlow, I had to manually tweak the model checkpoints when I moved models from one folder to another.</p>\n\n\n\n<h3>Solution 6: Improving code quality, a constant battle with a little help from my tools</h3>\n\n\n\n<p>On code quality and affiliated, I have several battle horses:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>As already mentioned, all the data model classes I implement are based on <a href=\"https://pydantic-docs.helpmanual.io/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Pydantic</a> (another python god: <a href=\"https://github.com/samuelcolvin\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Samuel Covin</a>).&nbsp;</li><li>I docstring every method (but try to ban comments inside methods, which are, in my opinion, the sign of an urgent need to apply the good old extract method <a href=\"https://martinfowler.com/books/refactoring.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">refactoring</a> pattern :)). The<a href=\"https://google.github.io/styleguide/pyguide.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> Google style guide</a> is a must-read (even if you do not adhere to all its aspects, know why you do not :)).</li><li>I use <a href=\"https://sourcery.ai/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">sourcery</a> to automatically hunt down bad designs and apply suggested refactoring patterns (you can find the current list <a href=\"https://docs.sourcery.ai/refactorings/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">here</a>, and they add new ones on a regular basis). This tool is such a time saver â€“ bad code does not survive long and your colleagues do not have to read it nor point it out during a painful code review. In fact the only extensions that I advocate every one to use on pycharm are sourcery and <a href=\"https://www.tabnine.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">tabnine</a></li><li>Among other pre-commit hooks (remember the homemade one on the high-level dependencies I already talked about) I use <a href=\"https://github.com/pre-commit/mirrors-autopep8\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">autopep8</a>, <a href=\"https://github.com/pre-commit/pre-commit-hooks\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">flake</a>, and <a href=\"https://github.com/pre-commit/mirrors-mypy\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">mypy</a>.</li><li>I&nbsp; use <a href=\"https://pypi.org/project/pylint/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">pylint</a> to lint my code bases and aim for a 9-9.5 target. This is completely arbitrary, but as Richard Thaler said â€“ â€œIâ€™m sure thereâ€™s an evolutionary explanation for this, if you give them [men] a target, they will aim.â€</li><li>I use <a href=\"https://docs.python.org/3/library/unittest.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">unittest</a> (this is the one I have experience with and I did not feel the need to switch to pytest. Even if it does mean some boilerplate I am more tolerant on the test side, as long as the tests exist!). For the same reason as the one mentioned in the last point, I aim for 95% coverage.</li><li>I adopt the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/covariance/__init__.py\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">sklearn pattern</a> for imports, meaning everything that is imported outside the folder regroupment of modules where the __init__.py stands must be listed in this very __init__.py. Every class/method listed here is the interface of the â€œpackageâ€ and must be tested (unitary and/or functional).&nbsp;</li><li>I often tried to implement cross-platform deterministic tests (read <a href=\"https://pytorch.org/docs/stable/notes/randomness.html\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">this</a> and <a href=\"https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#reproducibility\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">this</a>) but failed (though I did succeed on a fixed platform). Since GitLab runners are changing from time to time this often leads to a lot of pain. I settle on having a performance â€œhigh enoughâ€ in&nbsp; end-to-end test.</li><li>To avoid code duplication across several containers, I advocate for a low-level homemade library that you then install in each of your containers (via a command line in their respective Dockerfiles).&nbsp;</li><li>Concerning <a href=\"https://en.wikipedia.org/wiki/Continuous_integration\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">CI</a>, build your docker images in their respective GitLab pipelines.</li><li>Try not to mount code in production (but do so locally to ease development. A very good <a href=\"https://pythonspeed.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">reference</a> blog on docker+python).</li><li>I do not ship the tests in production, nor the librairies needed to run the tests (you should thus aim for two requirement files, one requirement-dev.txt not used in prod).</li><li>I often have a custom python dev docker-compose file to ease my life (and the onboarding of new members) which is different from the production one.&nbsp;</li><li>I advocate to (extensively) use the wiki part of your GitLab repos :), as the oral tradition was good at some stage of human history but is definitely not for IT companies :).&nbsp;</li><li>I try to minimize the number of volumes mounted on my containers, the best number being 0 but for some data sources (like model checkpoint) it can be complicated.</li><li>Handling dead code&nbsp; has a simple solution: <a href=\"https://pypi.org/project/vulture/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Vulture</a>. Run it, inspect (closely, as they are some false positives) its output, unplug dead code, rinse and repeat.</li></ul>\n</div>\n\n\n<h2>Conclusion</h2>\n\n\n\n<p>All too often, you see self congratulating articles hiding what real life really is in the ML field. I hope that you leave this post knowing this is not one of these articles. This is the honest journey I went on in the past six years developing MLOPS pipelines, and I can be all the more proud when I look back at where I was when I started coding in 2006 (a one line method of more&nbsp; than 400 characters in a C code :)).&nbsp;</p>\n\n\n\n<p>In my experience, some switching decisions are easy to make and implement (flask to FastAPI), some are easy to make but not so easy to implement (like Hyperopt to Optuna) and some are hard to make as well as hard to implement (like TensorFlow to PyTorch), but all are worth the effort in the end to avoid the 6 pitfalls I presented.&nbsp;</p>\n\n\n\n<p>This mindset will hopefully allow you to transition from a POC-like ML environment&nbsp; to an<a href=\"https://en.wikipedia.org/wiki/Accelerate_(book)\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> Accelerate</a> compliant one where implementing new features can take less than an hour, and adding them to the code base takes less than another hour.&nbsp;</p>\n\n\n\n<p>At a personal level, I learned an awful lot and I am deeply indebted to my previous employers and my&nbsp;previous colleagues for that!</p>\n\n\n\n\n<div id=\"author-box-new-format-block_62751d9d7a06a\" class=\"article__footer article__author\">\n  <div class=\"article__authorImage\">\n          <img width=\"230\" height=\"230\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Thomas-Epelbaum.jpg?fit=230%2C230&ssl=1\" class=\"article__authorImage-img\" alt=\"Thomas Epelbaum\" decoding=\"async\" data-attachment-id=\"65761\" data-permalink=\"https://neptune.ai/attachment/thomas-epelbaum\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Thomas-Epelbaum.jpg?fit=323%2C323&ssl=1\" data-orig-size=\"323,323\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Thomas Epelbaum\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Thomas-Epelbaum.jpg?fit=300%2C300&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/Thomas-Epelbaum.jpg?fit=323%2C323&ssl=1\" />      </div>\n\n  <div class=\"article__authorContent\">\n          <h3 class=\"article__authorContent-name\">Thomas Epelbaum</h3>\n    \n          <p class=\"article__authorContent-text\">Senior Python Dev and ML at EcoAct. Heavy Euro style board game enthusiast. Always happy to read a new philosophy/sociology/economy/biology book.</p>\n    \n          <ul class=\"article__authorSocial\">\n        <li class=\"article__authorSocial-single article__authorSocial-name\">Follow me on</li>\n        \n                  <li class=\"article__authorSocial-single\"><a href=\"https://www.linkedin.com/in/thomas-epelbaum-64b09625/\" class=\"article__authorSocial-lk\" target=\"_blank\"></a></li>\n        \n                  <li class=\"article__authorSocial-single\"><a href=\"http://www.normalesup.org/~epelbaum/\" class=\"article__authorSocial-www\" target=\"_blank\"></a></li>\n              </ul>\n    \n  </div>\n</div>\n\n\n<div class=\"is-layout-flow wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<hr class=\"wp-block-separator\"/>\n\n\n\n<p class=\"has-text-color\" style=\"color:#4455a6\"><strong>READ NEXT</strong></p>\n\n\n\n<h2>MLOps at GreenSteam: Shipping Machine Learning [Case Study]</h2>\n\n\n\n<p class=\"has-small-font-size\">7 mins read | Tymoteusz WoÅ‚odÅºko | Posted March 31, 2021</p>\n\n\n<div id=\"block_5ffc75def9f8e\" class=\"separator separator-10\"></div>\n\n\n\n<div class=\"is-layout-flex wp-container-11 wp-block-columns are-vertically-aligned-center\">\n<div class=\"is-layout-flow wp-block-column is-vertically-aligned-center\" style=\"flex-basis:66.66%\">\n<p><a href=\"https://www.greensteam.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">GreenSteam</a> is a company that provides software solutions for the marine industry that help reduce fuel usage. Excess fuel usage is both costly and bad for the environment, and vessel operators are obliged to get more green by the International Marine Organization and reduce the CO2 emissions by 50 percent by 2050.</p>\n</div>\n\n\n\n<div class=\"is-layout-flow wp-block-column is-vertically-aligned-center\" style=\"flex-basis:33.33%\">\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img data-attachment-id=\"42633\" data-permalink=\"https://neptune.ai/blog/mlops-at-greensteam-shipping-machine-learning-case-study/attachment/greensteam-logo\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/GreenSteam-logo.png?fit=665%2C157&ssl=1\" data-orig-size=\"665,157\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"GreenSteam-logo\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/GreenSteam-logo.png?fit=300%2C71&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/GreenSteam-logo.png?fit=665%2C157&ssl=1\" decoding=\"async\" width=\"665\" height=\"157\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/GreenSteam-logo.png?resize=665%2C157&#038;ssl=1\" alt=\"Greensteam logo\" class=\"wp-image-42633\" data-recalc-dims=\"1\"/></figure></div>\n</div>\n</div>\n\n\n\n<p>Even though we are not a big company (50 people including business, devs, domain experts, researchers, and data scientists), we have already built several machine learning products over the last 13 years that help some <a href=\"https://blog.greensteam.com/crucial-tools-for-shipping-companies-looking-to-thrive-post-2020\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">major shipping companies make informed performance optimization decisions</a>.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img data-attachment-id=\"42643\" data-permalink=\"https://neptune.ai/blog/mlops-at-greensteam-shipping-machine-learning-case-study/attachment/mlops-shipping-4\" data-orig-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-shipping-4.jpg?fit=960%2C539&ssl=1\" data-orig-size=\"960,539\" data-comments-opened=\"0\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"1\"}\" data-image-title=\"MLOps shipping 4\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-shipping-4.jpg?fit=300%2C168&ssl=1\" data-large-file=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-shipping-4.jpg?fit=960%2C539&ssl=1\" decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-shipping-4.jpg?resize=720%2C404&#038;ssl=1\" alt=\"MLOps shipping\" class=\"wp-image-42643\" width=\"720\" height=\"404\" data-recalc-dims=\"1\" /></figure></div>\n\n\n\n<p>In this blog post, I want to share our journey to building the MLOps stack. Specifically, how we:</p>\n\n\n<div class=\"custom-point-list\">\n<ul><li>dealt with <strong>code dependencies</strong></li><li>approached <strong>testing ML models </strong>&nbsp;</li><li>built <strong>automated training and evaluation pipelines&nbsp;</strong></li><li><strong>deployed and served our models</strong></li><li>managed to keep <strong>human-in-the-loop in MLOps</strong></li></ul>\n</div>\n\n<a class=\"button continous-post blue-filled\" href=\"/blog/mlops\" target=\"_blank\">\n    Continue reading -></a>\n\n\n\n<hr class=\"wp-block-separator\"/>\n</div></div>\n<p>The post <a rel=\"nofollow\" href=\"https://neptune.ai/blog/building-ml-pipeline-problems-solutions\">Building ML Pipeline: 6 Problems &#038; Solutions [From a Data Scientist&#8217;s Experience]</a> appeared first on <a rel=\"nofollow\" href=\"https://neptune.ai\">neptune.ai</a>.</p>\n",
  "post-id": 65705
}