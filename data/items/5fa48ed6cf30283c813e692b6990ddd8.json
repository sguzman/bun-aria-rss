{
  "title": "On chain rule, computational graphs, and backpropagation",
  "link": "",
  "published": "2015-09-05T00:00:00-05:00",
  "updated": "2015-09-05T00:00:00-05:00",
  "author": {
    "name": "Brandon Brown"
  },
  "id": "tag:outlace.com,2015-09-05:/on-chain-rule-computational-graphs-and-backpropagation.html",
  "summary": "<p>Here I'm going to revisit backpropagation theory by thinking about neural networks as computational graphs upon which we can easily visualize the chain rule to compute partial derivatives.</p>",
  "content": "<hr>\n<script>\nMathJax.Hub.Config({\n  tex2jax: {\n    inlineMath: [['$','$'], ['\\\\(','\\\\)']],\n    processEscapes: true\n  },\n  TeX: {\n    extensions: [\"cancel.js\"]\n  },\n  displayAlign: \"left\",\n});\n</script>\n\n<h4>...Another post on backpropagation?</h4>\n<p>Sort of. I know, everyone and their brother has a post on backpropagation.\nIn fact, I <em>already</em> have a post on backpropagation here, so why am I writing <em>another</em>\npost? Well, for one, this post isn't really about backpropagation per se, it's\nabout how we can think about neural networks in a completely different way than\nwe're used to using a computational graph representation and then use it to\nderive backpropagation from a fundamental formula of calculus, chain rule.</p>\n<p>This post is entirely inspired from colah's blog post <a href=\"http://colah.github.io/posts/2015-08-Backprop/\">Calculus on Computational Graphs: Backpropagation</a> so please\nread through that first. I will reiterate a lot of what's on there but will surely\nnot do as good a job. My purpose here is more an extension of that post to\nthinking about neural networks as computational graphs.</p>\n<h6>Assumptions:</h6>\n<p>I assume you remember some high school calculus on taking derivatives, and I assume you already know\nthe basics about neural networks and have hopefully implemented one before.</p>\n<h5>Calculus' chain rule, a refresher</h5>\n<p>Before I begin, I just want to refresh you on how chain rule works. Of course you can skip\nthis section if you know chain rule as well as you know your own mother.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Chain_rule#Composites_of_more_than_two_functions\">Chain rule</a> is the process\nwe can use to analytically compute derivatives of <em>composite functions</em>. A composite function is a function\nof other function(s). That is, we might have one function <span class=\"math\">\\(f\\)</span> that is composed of multiple inner or nested functions.</p>\n<p>For example,\n</p>\n<div class=\"math\">$$f(g(h(x)))$$</div>\n<p> is a composite function. We have an outer function <span class=\"math\">\\(f\\)</span>, an inner function <span class=\"math\">\\(g\\)</span>, and the final inner function <span class=\"math\">\\(h(x)\\)</span></p>\n<p>Let's say <span class=\"math\">\\(f(x) = e^{sin(x^2)}\\)</span>, we can decompose this function into 3 separate functions:</p>\n<p><span class=\"math\">\\(f(x) = e^x $, $g(x) = sin(x)\\)</span>, and <span class=\"math\">\\(h(x) = x^2\\)</span>, or what it looks like as a nested function:\n</p>\n<div class=\"math\">$$f(g(h(x))) = e^{g(h(x))}$$</div>\n<p>And to get the derivative of this function with respect to x, <span class=\"math\">\\(\\frac{d{f}}{d{x}}\\)</span>, we use the chain rule:</p>\n<div class=\"math\">$$\\frac{d{f}}{d{x}} = \\frac{df}{dg} * \\frac{dg}{dh} * \\frac{dh}{dx}$$</div>\n<p>, such that,\n</p>\n<div class=\"math\">$$\\frac{df}{dg} = e^{g(h(x))} $$</div>\n<p> (because the derivative of <span class=\"math\">\\(e^x\\)</span> is just <span class=\"math\">\\(e^x\\)</span>),\n</p>\n<div class=\"math\">$$\\frac{dg}{dh} = cos(h(x))$$</div>\n<p> (because the derivative of <span class=\"math\">\\(sin\\)</span> is <span class=\"math\">\\(cos\\)</span>),\n</p>\n<div class=\"math\">$$\\frac{dh}{dx} = 2x$$</div>\n<p>\ntherefore...\n</p>\n<div class=\"math\">$$\\frac{d{f}}{d{x}} = e^{\\sin x^2} * \\cos x^2 * 2x$$</div>\n<p>\nSo in each of these cases, I just pretend that the inner function is a single variable and derive it as such.\nThe other way to view it,</p>\n<div class=\"math\">$$f(x) = e^{sin(x^2)}$$</div>\n<p>, then create some temporary, substitution variables\n<span class=\"math\">\\(u = sin(v)\\)</span>, and <span class=\"math\">\\(v = x^2\\)</span>, now <span class=\"math\">\\(f(u) = e^u\\)</span>, and you can use chain rule as above.</p>\n<h5>Computational graphs</h5>\n<p>A computational graph is a representation of a composite function as a network of connected nodes, where each node is an operation/function.\nIt is similar in appearance to a feedforward neural network (but don't get them confused). When we visualize these graphs, we can easily see all the nested relationships and follow some basic rules to come up with derivatives of any node we want.</p>\n<p>Let's visualize the above simple composite function as a computational graph.</p>\n<p><img alt=\"\" src=\"../images/compgraph/compgraph1.png\"></p>\n<p>As you can see, the graph shows what inputs get sent to each function. Every connection is an input, and every node is a function or operation\n(used here interchangeably).</p>\n<p>What's neat about these graphs is that we can <em>visualize</em> chain rule. All we need to do is get the derivative of each node with respect\nto each of its inputs.</p>\n<p><img alt=\"\" src=\"../images/compgraph/compgraph2.png\"></p>\n<p>Now you can follow along the graph and do some dimensional analysis to compute whichever derivatives you want by multiplying\nthe 'connection' derivatives (derivatives between a pair of connected nodes) along a path. For example, if we want to get <span class=\"math\">\\(df/dx\\)</span>, we simply multiply the connection derivatives starting from <span class=\"math\">\\(f\\)</span> all the way to <span class=\"math\">\\(x\\)</span>, which gives us the same equation as the chain rule formula above:\n</p>\n<div class=\"math\">$$\\frac{df}{dx} = \\frac{df}{\\cancel{dg}} * \\frac{\\cancel{dg}}{\\cancel{dh}} * \\frac{\\cancel{dh}}{dx} $$</div>\n<p>Now all of that was probably painfully obvious to a lot of you, but I just want to make this as accessible as possible.</p>\n<h5>Re-imagining neural networks as computational graphs</h5>\n<p>A neural network, if you haven't noticed, is essentially a massive nested composite function. Each layer of a feedforward\nneural network can be represented as a single function whose inputs are a weight vector and the outputs of the previous layer. This means we can build a nice computational graph out of it and use it to derive the backpropagation algorithm.</p>\n<p>Here's a 3 layer neural network (1 input layer, 1 hidden layer, 1 output layer), which could be used to solve something like the good ol' XOR problem.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NN.png\"></p>\n<p>So that is just a typical feedforward neural network visualization. Nothing new here, should be very familiar. Of course there are weight vectors hanging around in between the layers. It's a very intuitive and convenient representation of a neural network in terms of the information flow through the network. However, I don't think it's the best way to think about it or visualize it in terms of a computational implementation. We're going to try re-visualizing this as a computational graph, such that each node is no longer an abstract \"neuron\" with weights modulating the connection to other neurons, but instead where each node is a single computation or operation and the arrows are no longer weighted connections but merely indications of where inputs are being 'sent'. I'm going to switch these graphs to a vertical orientation just for better presentation, and the notation will be different.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NNCompGraph.png\"></p>\n<p>Here L3 denotes the output layer, L2 the hidden layer, and L1 the input layer. Similarly, <span class=\"math\">\\(\\theta\\_2\\)</span> denotes the weight vector 'between' layer 2 and layer 3; <span class=\"math\">\\(\\theta\\_1\\)</span> denotes the weight vector 'between' layer 1 and layer 2. The <span class=\"math\">\\(\\sigma\\)</span> notation just refers to the sigmoid operation that takes place within those particular nodes (however, the <em>outputs</em> of those nodes will be referred to using the L notation, i.e. L1, L2, and L3).</p>\n<p>As you can see, it's a fairly different way of looking at the neural network. This is a functional representation. It shows the step by step functions that occur and the inputs those functions operate on. For example, the L2 layer function operates on two inputs: the L1 layer outputs (a vector) and the weight vector <span class=\"math\">\\(\\theta\\_1\\)</span>. Likewise, the L3 function operates on L2 and <span class=\"math\">\\(\\theta\\_2\\)</span>, and is our final output. If you're wondering where the bias nodes went, they're implicitly included in each L layer's output (i.e. the scalar 1 is added to each L vector). Each weight vector still contains a weight for the bias.</p>\n<p>Let's list out the computational steps to compute the neural network output, L3.</p>\n<div class=\"math\">$$ \\begin{align}\n&amp;\\sigma(x) = \\frac{1}{1+e^{-x}} \\text{, define the sigmoid/logistic function} \\\\\\\\\n&amp;X = [x\\_1, x\\_2]       \\text{, the vector containing our two input values} \\\\\\\\\n&amp;L1 = [X, 1]            \\text{, add a bias value of 1}\\\\\\\\\n&amp;L2 = [\\sigma(L1 * \\theta\\_1), 1] \\text{, add bias and compute L2}\\\\\\\\\n&amp;L3 = \\sigma(L2 * \\theta\\_2) \\text{, compute L3}\\\\\\\\\n\\end{align}\n$$</div>\n<p>Now let's compute the 'connection' derivatives. This is just simple calculus on individual functions. Remember that <span class=\"math\">\\(\\sigma'=\\sigma(1-\\sigma)\\)</span></p>\n<div class=\"math\">$$\nL3 = \\frac{1}{1+e^{-\\theta\\_2 L2}} \\\\\\\\\nu = \\theta\\_2 L2 \\\\\\\\\nL3 = \\frac{1}{1+e^{-u}} \\\\\\\\\n\\frac{\\partial{L3}}{\\partial{L2}} = \\frac{\\partial L3}{\\partial u} * \\frac{\\partial u}{\\partial L2} \\\\\\\\\n\\frac{\\partial{L3}}{\\partial{L2}} = \\theta\\_2 * L3(1-L3) \\\\\\\\\n\\frac{\\partial{L3}}{\\partial{\\theta\\_2}} = L2 * L3(1-L3) \\\\\\\\\n\\frac{\\partial{L2}}{\\partial{L1}} = \\theta\\_1 * L2(1-L2) \\\\\\\\\n\\frac{\\partial{L2}}{\\partial{\\theta\\_1}} = L1 * L2(1-L2) \\\\\\\\\n$$</div>\n<p>Here's our computational graph again with our derivatives added.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NNCompGraph_deriv.png\"></p>\n<h5>Backpropagation</h5>\n<p>Remember that the purpose of backpropagation is to figure out the partial derivatives of our cost function (whichever cost function we choose to define), with respect to each individual weight in the network: <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_j}\\)</span>, so we can use those in gradient descent. We've already seen how we can create a computational graph out of our neural network where the output layer is a composite function of the previous layers and the weight vectors. In order to figure our <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_j}\\)</span>, we can extend our computational graph so that the 'outer' function is our cost function, which of course has to be a function of the neural network output.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NNCompGraph_withcost.png\"></p>\n<p>Everything is the same as the computational graph of just our neural network, except now we're taking the output of the neural network and feeding it into the cost function, which also requires the parameter <span class=\"math\">\\(y\\)</span> (the expected value given the input(s) <span class=\"math\">\\(x\\)</span>).</p>\n<p>The most common cost function used for a network like this would be the <strong>cross-entropy cost function</strong>, with the generic form being:\n</p>\n<div class=\"math\">$$C(\\theta) = \\frac 1m * \\sum\\_1^m [-y * log((h\\_{\\theta}(x))) - (1 - y)(log(1 - (h\\_{\\theta}(x)))]$$</div>\n<p>\nwhere <span class=\"math\">\\(h\\_{\\theta}(x)\\)</span> is the output (could be a scalar or vector depending on the number of output nodes) of the network, <span class=\"math\">\\(y\\)</span> is the expected value given input(s) <span class=\"math\">\\(x\\)</span>, and <span class=\"math\">\\(m\\)</span> is the number of training examples.</p>\n<p>We can get the derivative of this cost function with respect to the output layer function L3 (aka <span class=\"math\">\\(h\\_{\\theta}(x)\\)</span>) to add to our computational graph. It's not worth going through the steps to derive it here, but if you ask <a href=\"http://www.wolframalpha.com/input/?i=%E2%88%92y%E2%88%97log%28x%29%E2%88%92%281%E2%88%92y%29%28log%281%E2%88%92x%29%29%2C+d%2Fdx\">WolframAlpha</a> to do it, it will give you this:\n</p>\n<div class=\"math\">$${L3}' = \\frac{L3 - y}{L3(1-L3)}$$</div>\n<p>\nSo now we have everything we need to compute <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_j}\\)</span>. Let's compute <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_2}\\)</span> and see if it compares to the textbook backpropagation algorithm.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NNCompGraphDeriv2.png\"></p>\n<p>We're just following the path from the top (the cost function) down to the node we're interested in, in this case, <span class=\"math\">\\(\\theta\\_2\\)</span>. We multiply the partial derivatives along that path to get <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_2}\\)</span>. As you'll notice, it's obvious from a dimensional analysis perspective that doing this multiplication gets us what we want. We already calculated what the 'connection' derivatives were above, so we'll just substitute them back in and do the math to get what we want.</p>\n<div class=\"math\">$$\n\\frac{\\partial{C}}{\\partial{L3}} = \\frac{L3 - y}{L3(1-L3)} \\\\\\\\\n\\frac{\\partial{L3}}{\\partial{\\theta\\_2}} = L2 * L3(1-L3) \\\\\\\\\n\\frac{\\partial{C}}{\\partial\\theta\\_2} = \\frac{\\partial{C}}{\\partial{L3}} * \\frac{\\partial{L3}}{\\partial{\\theta\\_2}} \\text { this is what we want} \\\\\\\\\n\\frac{\\partial{C}}{\\partial\\theta\\_2} = \\frac{L3 - y}{\\cancel{L3(1-L3)}} * L2 * \\cancel{L3(1-L3)} \\\\\\\\\n\\frac{\\partial{C}}{\\partial\\theta\\_2} = (L3 - y) * L2\\\\\\\\\n$$</div>\n<p>It should be clear that this is in fact the same result we'd get from backpropagation by calculating the node deltas and all of that. We just derived a more general form of backpropagation by using a computational graph.</p>\n<p>But you might still be skeptical, let's calculate <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial\\theta\\_1}\\)</span>, one more layer deep, just to prove this is a robust method. Same process as above. Again, here's a visualization of the path we follow along the computational graph to where we want. You can also just think of it as dimensional analysis.</p>\n<p><img alt=\"\" src=\"../images/compgraph/NNCompGraphDeriv3.png\"></p>\n<div class=\"math\">$$\n\\frac{\\partial{C}}{\\partial{L3}} = \\frac{L3 - y}{L3(1-L3)} \\\\\\\\\n\\frac{\\partial{L3}}{\\partial{L2}} = \\theta\\_2 * L3(1-L3) \\\\\\\\\n\\frac{\\partial{L2}}{\\partial{\\theta\\_1}} = L1 * L2(1-L2) \\\\\\\\\n\\frac{\\partial{C}}{\\partial{\\theta\\_1}} = \\frac{\\partial{C}}{\\partial{L3}} * \\frac{\\partial{L3}}{\\partial{L2}} * \\frac{\\partial{L2}}{\\partial{\\theta\\_1}} \\text{this is what we want} \\\\\\\\\n\\frac{\\partial{C}}{\\partial{\\theta\\_1}} = \\frac{L3 - y}{\\cancel{L3(1-L3)}} * \\theta\\_2 * \\cancel{L3(1-L3)} * L1 * L2(1-L2) \\\\\\\\\n\\frac{\\partial{C}}{\\partial{\\theta\\_1}} = (L3 - y) * \\theta\\_2 *  L1 * L2(1-L2) \\\\\\\\\n$$</div>\n<p>And there you have it. Just to check against the usual backpropagation method:</p>\n<h6>Comparing against ordinary backpropagation</h6>\n<p>We would start by calculating the output node delta: <span class=\"math\">\\(\\delta^3 = (L3 - y)\\)</span>\nThen we would calculate the hidden layer delta:\n</p>\n<div class=\"math\">$$\n\\delta^2 = (\\theta\\_2 * \\delta^3) \\odot (L2(1 - L2))\n$$</div>\n<p>\nNow that we have all the layer deltas (we don't calculate deltas for input layer), we use this formula to get <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial{\\theta\\_j^l}}\\)</span>:</p>\n<div class=\"math\">$$ \\frac{\\partial{C}}{\\partial{\\theta\\_j^l}} = \\delta^{l+1} * L\\_l$$</div>\n<p>\nwhere <span class=\"math\">\\(l\\)</span> is the layer number, thus L2 is the hidden layer output.\nIn our case, we'll find <span class=\"math\">\\(\\frac{\\partial{C}}{\\partial{\\theta\\_1}}\\)</span>\n</p>\n<div class=\"math\">$$ \\frac{\\partial{C}}{\\partial{\\theta\\_1}} = \\delta^{2} * L1$$</div>\n<p> then we'll substitute in <span class=\"math\">\\(\\delta^2\\)</span> from our calculation above\n</p>\n<div class=\"math\">$$ \\frac{\\partial{C}}{\\partial{\\theta\\_1}} = (\\theta\\_2 * \\delta^3) \\odot (L2(1 - L2)) * L1$$</div>\n<p>\nand we'll substitute in <span class=\"math\">\\(\\delta^3\\)</span>\n</p>\n<div class=\"math\">$$ \\frac{\\partial{C}}{\\partial{\\theta\\_1}} = (\\theta\\_2 * (L3 - y)) \\odot (L2(1 - L2)) * L1$$</div>\n<p>Important to note that with the derivation using the computational graph, we have to be careful about dimensional analysis and when to use which multiplication operator (dot product or element-wise), whereas with textbook backpropagation, those things are made more explicit. The computational graph method above makes it seem like we only use dot products when that might be computationally impossible. Thus, the computational graph method requires a bit more thought.</p>\n<h5>Conclusion</h5>\n<p>I think representing neural networks (and probably all types of algorithms based on composite functions) as computational graphs is a fantastic way of understanding how to implement one in code in a functional way and allows you to have all sorts of differential fun with ease. They give us the tools to derive backpropagation from an arbitrary neural network architecture. For example, by unfolding a recurrent neural network, we could generate a computational graph and apply the same principles to derive backpropagation through time.</p>\n<h6>References</h6>\n<ol>\n<li>http://colah.github.io/posts/2015-08-Backprop/</li>\n<li>https://en.wikipedia.org/wiki/Chain_rule</li>\n<li>https://en.wikipedia.org/wiki/Automatic_differentiation</li>\n<li>http://www.wolframalpha.com</li>\n</ol>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}