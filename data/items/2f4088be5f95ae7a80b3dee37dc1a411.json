{
  "title": "An orientational integral",
  "link": "",
  "published": "2019-07-02T07:04:00-07:00",
  "updated": "2019-07-02T07:04:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2019-07-02:/an-orientational-integral",
  "summary": "<p>We evaluate an integral having to do with vector averages over all\norientations in an n-dimensional&nbsp;space.</p>\n<h2>Problem&nbsp;definition</h2>\n<p>Let <span class=\"math\">\\(\\hat{v}\\)</span> be a unit vector in <span class=\"math\">\\(n\\)</span>-dimensions and consider the orientation average&nbsp;of\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{1} \\label{1}\nJ \\equiv \\langle \\hat{v} \\cdot \\vec{a}_1 …</div>",
  "content": "<p>We evaluate an integral having to do with vector averages over all\norientations in an n-dimensional&nbsp;space.</p>\n<h2>Problem&nbsp;definition</h2>\n<p>Let <span class=\"math\">\\(\\hat{v}\\)</span> be a unit vector in <span class=\"math\">\\(n\\)</span>-dimensions and consider the orientation average&nbsp;of\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{1} \\label{1}\nJ \\equiv \\langle \\hat{v} \\cdot \\vec{a}_1 \\hat{v} \\cdot \\vec{a}_2 \\ldots \\hat{v} \\cdot \\vec{a}_k \\rangle\n\\end{eqnarray}</div>\n<p>\nwhere <span class=\"math\">\\(\\vec{a}_1, \\ldots, \\vec{a}_k\\)</span> are some given fixed vectors. For example, if all <span class=\"math\">\\(\\vec{a}_i\\)</span> are equal to <span class=\"math\">\\(\\hat{x}\\)</span>, we want the orientation average of <span class=\"math\">\\(v_x^k\\)</span>.</p>\n<h2>Solution</h2>\n<p>We&#8217;ll evaluate our integral using parameter differentiation of the multivariate Gaussian integral.&nbsp;Let\n</p>\n<div class=\"math\">\\begin{eqnarray} \\nonumber\nI &amp;=&amp; \\frac{1}{(2 \\pi)^{n/2}} \\int e^{- \\frac{\\vert \\vec{v} \\vert^2}{2} + \\sum_{i=1}^k \\alpha_i \\vec{v} \\cdot \\vec{a}_i} d^nv \\\\ \\tag{2} \\label{2}\n&amp;=&amp; \\exp \\left [- \\frac{1}{2} \\vert \\sum_{i=1}^k \\alpha_i \\vec{a}_i \\vert^2 \\right]\n\\end{eqnarray}</div>\n<p>\nThe expression in the second line follows from completing the square in the exponent in the first &#8212; for review, see our post on the normal distribution, <a href=\"http://efavdb.github.io/normal-distributions\">here</a>. Now, we consider a particular derivative of <span class=\"math\">\\(I\\)</span> with respect to the <span class=\"math\">\\(\\alpha\\)</span> parameters. From the first line of (\\ref{2}), we&nbsp;have\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{3} \\label{3}\n\\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k}I \\vert_{\\vec{\\alpha}=0} &amp;=&amp; \\frac{1}{(2 \\pi)^{n/2}} \\int e^{- \\frac{\\vert \\vec{v} \\vert^2}{2}} \\prod_{i=1}^k \\vec{v} \\cdot \\vec{a}_i d^n v \\\\\n&amp;\\equiv &amp; \\frac{1}{(2 \\pi)^{n/2}} \\int_0^{\\infty} e^{- \\frac{\\vert \\vec{v} \\vert^2}{2}} v^{n + k -1} dv \\int \\prod_{i=1}^k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v \\\\\n&amp;=&amp; \\frac{2^{k/2 - 1}}{\\pi^{n/2}} \\Gamma(\\frac{n+k}{2}) \\times \\int \\prod_{i=1}^k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v\n\\end{eqnarray}</div>\n<p>\nThe second factor above is almost our desired orientation average <span class=\"math\">\\(J\\)</span> &#8212; the only thing it&#8217;s missing is the normalization, which we can get by evaluating this integral without any <span class=\"math\">\\(\\vec{a}\\)</span><span class=\"quo\">&#8216;</span>s.</p>\n<p>Next, we evaluate the parameter derivative considered above in a second way, using the second line of (\\ref{2}). This&nbsp;gives,\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{4} \\label{4}\n\\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k}I \\vert_{\\vec{\\alpha}=0} &amp;=&amp; \\partial_{\\alpha_1}\\ldots \\partial_{\\alpha_k} \\exp \\left [- \\frac{1}{2} \\vert \\sum_{i=1}^k \\alpha_i \\vec{a}_i \\vert^2 \\right] \\vert_{\\vec{\\alpha}=0} \\\\\n&amp;=&amp; \\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k})\n\\end{eqnarray}</div>\n<p>\nThe sum here is over all possible, unique pairings of the indices. You can see this is correct by carrying out the differentiation one parameter at a&nbsp;time.</p>\n<p>To complete the calculation, we equate (\\ref{3}) and (\\ref{4}). This&nbsp;gives\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{5}\\label{5}\n\\int \\prod_{i=1}^k \\hat{v} \\cdot \\vec{a}_i d \\Omega_v = \\frac{\\pi^{n/2}} {2^{k/2 - 1}\\Gamma(\\frac{n+k}{2})}\\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k})\n\\end{eqnarray}</div>\n<p>\nAgain, to get the desired average, we need to divide the above by the normalization factor. This is given by the value of the integral (\\ref{5}) when <span class=\"math\">\\(k = 0\\)</span>. This&nbsp;gives,\n</p>\n<div class=\"math\">\\begin{eqnarray}\\tag{6}\\label{6}\nJ = \\frac{1}{2^{k/2}}\\frac{\\Gamma(n/2)}{\\Gamma(\\frac{n+k}{2})} \\sum_{\\text{pairings}} (\\vec{a}_{i_1} \\cdot \\vec{a}_{i_2}) (\\vec{a}_{i_3} \\cdot \\vec{a}_{i_4})\\ldots (\\vec{a}_{i_{k-1}} \\cdot \\vec{a}_{i_k})\n\\end{eqnarray}</div>\n<h2>Example</h2>\n<p>Consider the case where <span class=\"math\">\\(k=2\\)</span> and <span class=\"math\">\\(\\vec{a}_1 = \\vec{a}_2 = \\hat{x}\\)</span>. In this case, we note that the average of <span class=\"math\">\\(\\hat{v}_x^2\\)</span> is equal to the average along any other orientation. This means we&nbsp;have\n</p>\n<div class=\"math\">\\begin{eqnarray}\\nonumber \\tag{7} \\label{7}\n\\langle \\hat{v}_x^2 \\rangle &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n \\langle \\hat{v}_x^2 + \\hat{v}_y^2 + \\ldots \\rangle \\\\\n&amp;=&amp; \\frac{1}{n}\n\\end{eqnarray}</div>\n<p>\nWe get this same result from our more general formula: Plugging in <span class=\"math\">\\(k=2\\)</span> and <span class=\"math\">\\(\\vec{a}_1 = \\vec{a}_2 = \\hat{x}\\)</span> into (\\ref{6}), we&nbsp;obtain\n</p>\n<div class=\"math\">\\begin{eqnarray}\\nonumber \\tag{8} \\label{8}\n\\langle \\hat{v}_x^2 \\rangle &amp;=&amp; \\frac{1}{2}\\frac{\\Gamma(n/2)}{\\Gamma(\\frac{n}{2} + 1)} \\\\\n&amp;=&amp; \\frac{1}{n}\n\\end{eqnarray}</div>\n<p>\nThe two results&nbsp;agree.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}