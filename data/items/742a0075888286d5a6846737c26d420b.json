{
  "title": "Inverting a Neural Net",
  "link": "",
  "published": "2016-04-05T00:00:00-04:00",
  "updated": "2016-04-05T00:00:00-04:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2016-04-05:/inverting-a-neural-net.html",
  "summary": "In this experiment, I \"invert\" a simple two-layer MNIST model to visualize what the final hidden layer representations look like when projected back into the original sample space.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>In this experiment, I “invert” a simple two-layer MNIST model to visualize what the final hidden layer representations look like when projected back into the original sample space.</p>\n<p>[<strong>Note 2017/03/05</strong>: At the time of writing this post, I did not know what an autoencoder was.]</p>\n<h3 id=\"model-setup\">Model Setup</h3>\n<p>This is a fully-connected model with two-hidden layers of 100 hidden neurons. The model also contains inverse weight matrices (<code>w2_inv</code> and <code>w1_inv</code>) that are trained after the fact by minimizing the l1 difference (<code>x_inv_similarity</code>) between the inverse projection of a sample and the original sample.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> tensorflow <span class=\"im\">as</span> tf\n<span class=\"im\">import</span> numpy <span class=\"im\">as</span> np\n<span class=\"im\">import</span> load_mnist\n<span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt\n<span class=\"im\">import</span> matplotlib.image <span class=\"im\">as</span> mpimg\n<span class=\"im\">import</span> matplotlib.cm <span class=\"im\">as</span> cm\n<span class=\"im\">from</span> mpl_toolkits.axes_grid1 <span class=\"im\">import</span> ImageGrid\n<span class=\"op\">%</span>matplotlib inline\nmnist <span class=\"op\">=</span> load_mnist.read_data_sets(<span class=\"st\">&#39;MNIST_data&#39;</span>, one_hot<span class=\"op\">=</span><span class=\"va\">True</span>)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\n\n<span class=\"kw\">def</span> weight_variable(shape,name<span class=\"op\">=</span><span class=\"va\">None</span>):\n    initial <span class=\"op\">=</span> tf.truncated_normal(shape, stddev<span class=\"op\">=</span><span class=\"fl\">0.1</span>)\n    <span class=\"cf\">return</span> tf.Variable(initial,name<span class=\"op\">=</span>name)\n\n<span class=\"kw\">def</span> bias_variable(shape):\n  initial <span class=\"op\">=</span> tf.constant(<span class=\"fl\">0.1</span>, shape<span class=\"op\">=</span>shape)\n  <span class=\"cf\">return</span> tf.Variable(initial)\n\n\n<span class=\"kw\">def</span> logit(p):\n    <span class=\"co\">&quot;&quot;&quot;element-wise logit of tensor p&quot;&quot;&quot;</span>\n    <span class=\"cf\">return</span> tf.log(tf.div(p,<span class=\"dv\">1</span><span class=\"op\">-</span>p))\n\n<span class=\"kw\">def</span> squash(p, dim):\n    <span class=\"co\">&quot;&quot;&quot;element-wise squash of dim of tensor MxN p to be between 0 and 1&quot;&quot;&quot;</span>\n    p_ <span class=\"op\">=</span> p <span class=\"op\">-</span> tf.reduce_min(p,dim,keep_dims<span class=\"op\">=</span><span class=\"va\">True</span>) <span class=\"co\"># add the minimum so all above 0</span>\n    p_norm <span class=\"op\">=</span> (p_ <span class=\"op\">/</span> tf.reduce_max(p_,dim,keep_dims<span class=\"op\">=</span><span class=\"va\">True</span>))\n    p_norm_ <span class=\"op\">=</span> (p_norm <span class=\"op\">-</span> <span class=\"fl\">0.5</span>) <span class=\"op\">*</span> <span class=\"fl\">0.99</span> <span class=\"op\">+</span> <span class=\"fl\">0.5</span> <span class=\"co\">#squashs to be strictly 0 &lt; p_norm_ &lt; 1</span>\n    <span class=\"cf\">return</span> p_norm_\n\nx <span class=\"op\">=</span> tf.placeholder(<span class=\"st\">&quot;float&quot;</span>, shape<span class=\"op\">=</span>[<span class=\"va\">None</span>, <span class=\"dv\">784</span>])\ny_ <span class=\"op\">=</span> tf.placeholder(<span class=\"st\">&quot;float&quot;</span>, shape<span class=\"op\">=</span>[<span class=\"va\">None</span>, <span class=\"dv\">10</span>])\n\nw1 <span class=\"op\">=</span> weight_variable([<span class=\"dv\">784</span>,<span class=\"dv\">100</span>])\nb1 <span class=\"op\">=</span> bias_variable([<span class=\"dv\">100</span>])\nl1 <span class=\"op\">=</span> tf.nn.sigmoid(tf.matmul(x,w1) <span class=\"op\">+</span> b1) <span class=\"co\">#100</span>\n\nw2 <span class=\"op\">=</span> weight_variable([<span class=\"dv\">100</span>,<span class=\"dv\">100</span>])\nb2 <span class=\"op\">=</span> bias_variable([<span class=\"dv\">100</span>])\nl2 <span class=\"op\">=</span> tf.nn.sigmoid(tf.matmul(l1,w2) <span class=\"op\">+</span> b2) <span class=\"co\">#100</span>\n\nw2_inv <span class=\"op\">=</span> weight_variable([<span class=\"dv\">100</span>,<span class=\"dv\">100</span>])\nl1_inv <span class=\"op\">=</span> tf.matmul(logit(l2) <span class=\"op\">-</span> b2, w2_inv)\nl1_inv_norm <span class=\"op\">=</span> squash(l1_inv, <span class=\"dv\">1</span>)\n\n<span class=\"co\"># this &quot;excess l1 inv&quot; is minimized so as to try to get the l1_inv to be compatible</span>\n<span class=\"co\"># with the logit (inverse sigmoid) function without requiring the squash operation</span>\nexcess_l1_inv <span class=\"op\">=</span> tf.nn.l2_loss(tf.reduce_min(l1_inv)) <span class=\"op\">+</span> tf.nn.l2_loss(tf.reduce_max(l1_inv <span class=\"op\">-</span> <span class=\"dv\">1</span>))\n\nw1_inv <span class=\"op\">=</span> weight_variable([<span class=\"dv\">100</span>,<span class=\"dv\">784</span>])\nx_inv <span class=\"op\">=</span> tf.matmul(logit(l1_inv_norm) <span class=\"op\">-</span> b1,w1_inv)\n\nw <span class=\"op\">=</span> weight_variable([<span class=\"dv\">100</span>,<span class=\"dv\">10</span>])\nb <span class=\"op\">=</span> bias_variable([<span class=\"dv\">10</span>])\ny <span class=\"op\">=</span> tf.nn.softmax(tf.matmul(l2,w) <span class=\"op\">+</span> b)\n\ncross_entropy <span class=\"op\">=</span> <span class=\"op\">-</span>tf.reduce_sum(y_<span class=\"op\">*</span>tf.log(y))\nx_inv_similarity <span class=\"op\">=</span> tf.reduce_sum(tf.<span class=\"bu\">abs</span>(x <span class=\"op\">-</span> x_inv))\n\nopt <span class=\"op\">=</span> tf.train.AdagradOptimizer(<span class=\"fl\">0.01</span>)\ngrads <span class=\"op\">=</span> opt.compute_gradients(x_inv_similarity<span class=\"op\">+</span>excess_l1_inv, [w1_inv, w2_inv])\ninv_train_step <span class=\"op\">=</span> opt.apply_gradients(grads)\n\ntrain_step <span class=\"op\">=</span> tf.train.AdagradOptimizer(<span class=\"fl\">0.01</span>).minimize(cross_entropy)\n\nsess.run(tf.initialize_all_variables())\n\ncorrect_prediction <span class=\"op\">=</span> tf.equal(tf.argmax(y,<span class=\"dv\">1</span>), tf.argmax(y_,<span class=\"dv\">1</span>))\naccuracy <span class=\"op\">=</span> tf.reduce_mean(tf.cast(correct_prediction, <span class=\"st\">&quot;float&quot;</span>))</code></pre></div>\n<h3 id=\"training-the-model\">Training the Model</h3>\n<p>First, we train the model, and then we train the inverse operations. The model achieves an accuracy of about 95%. Because we don’t want to confuse the inverse training with bad samples, we only train the model using samples that the model itself is confident it has classified correctly. This reduces noise in the inverse projections.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">2000</span>):\n    batch <span class=\"op\">=</span> mnist.train.next_batch(<span class=\"dv\">1000</span>)\n    train_step.run(feed_dict<span class=\"op\">=</span>{x: batch[<span class=\"dv\">0</span>], y_: batch[<span class=\"dv\">1</span>]})\n    <span class=\"cf\">if</span> i <span class=\"op\">%</span> <span class=\"dv\">100</span> <span class=\"op\">==</span> <span class=\"dv\">0</span>:\n        <span class=\"bu\">print</span>(i,end<span class=\"op\">=</span><span class=\"st\">&quot; &quot;</span>)\n        <span class=\"bu\">print</span>(accuracy.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span>{x: mnist.test.images,\n                                       y_: mnist.test.labels}), end<span class=\"op\">=</span><span class=\"st\">&quot;</span><span class=\"ch\">\\r</span><span class=\"st\">&quot;</span>)\n\n<span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1000</span>):\n    batch <span class=\"op\">=</span> mnist.train.next_batch(<span class=\"dv\">1000</span>)\n    confidence <span class=\"op\">=</span> np.<span class=\"bu\">max</span>(y.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span> {x: batch[<span class=\"dv\">0</span>]}),axis<span class=\"op\">=</span><span class=\"dv\">1</span>)\n    inv_train_step.run(feed_dict<span class=\"op\">=</span>{x: batch[<span class=\"dv\">0</span>][confidence<span class=\"op\">&gt;</span>.<span class=\"dv\">8</span>], y_: batch[<span class=\"dv\">1</span>][confidence<span class=\"op\">&gt;</span>.<span class=\"dv\">8</span>]})\n    <span class=\"cf\">if</span> i <span class=\"op\">%</span> <span class=\"dv\">100</span> <span class=\"op\">==</span> <span class=\"dv\">0</span>:\n        <span class=\"bu\">print</span>(i,end<span class=\"op\">=</span><span class=\"st\">&quot;</span><span class=\"ch\">\\r</span><span class=\"st\">&quot;</span>)\n\n<span class=\"bu\">print</span>(<span class=\"st\">&quot;Final Accuracy: &quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(accuracy.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span>{x: mnist.test.images,\n                                       y_: mnist.test.labels})))</code></pre></div>\n<pre><code>Final Accuracy: 0.9521</code></pre>\n<h3 id=\"visualizing-inverse-projections\">Visualizing inverse projections</h3>\n<p>We now show a visual comparison of the first 36 test samples and their projections.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> plot_nxn(n, images):\n    images <span class=\"op\">=</span> images.reshape((n<span class=\"op\">*</span>n,<span class=\"dv\">28</span>,<span class=\"dv\">28</span>))\n    fig <span class=\"op\">=</span> plt.figure(<span class=\"dv\">1</span>, (n, n))\n    grid <span class=\"op\">=</span> ImageGrid(fig, <span class=\"dv\">111</span>,  <span class=\"co\"># similar to subplot(111)</span>\n                     nrows_ncols<span class=\"op\">=</span>(n, n),  <span class=\"co\"># creates grid of axes</span>\n                     axes_pad<span class=\"op\">=</span><span class=\"fl\">0.1</span>,  <span class=\"co\"># pad between axes in inch.</span>\n                     )\n\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(n<span class=\"op\">*</span>n):\n        grid[i].imshow(images[i], cmap <span class=\"op\">=</span> cm.Greys_r)  <span class=\"co\"># The AxesGrid object work as a list of axes.</span>\n\n    plt.show()\n\nplot_nxn(<span class=\"dv\">6</span>,mnist.test.images[:<span class=\"dv\">36</span>])</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/INN_output_11_0.png\" />\n</figure>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">x1 <span class=\"op\">=</span> x_inv.<span class=\"bu\">eval</span>(feed_dict<span class=\"op\">=</span>{x: mnist.test.images})[:<span class=\"dv\">36</span>]\nplot_nxn(<span class=\"dv\">6</span>,x1)</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/INN_output_13_0.png\" />\n</figure>\n<p>I think the most interesting this about this is how the model completely transforms the misclassified digits. For example, the 9th sample and the 3rd to last sample each get transformed to a 6.</p>\n<p>It’s also interesting that the inverse projections are somewhat “idealized” versions of each digit. For example, the orientations of the inversely projected 3s and 9s and the stroke width of the inversely projected 0s are now all the same.</p>\n<h3 id=\"generating-samples\">Generating samples</h3>\n<p>Here we generate samples of digits 1-9 by first optimizing the hidden representation so that the neural network is confident that the representaton is of a specific class, and then outputting the inverse projection.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> generate(n_samples,fake_labels):\n    <span class=\"kw\">global</span> w\n    <span class=\"kw\">global</span> b\n    fake_l2 <span class=\"op\">=</span> tf.Variable(tf.zeros([n_samples, <span class=\"dv\">100</span>]))\n    fake_y <span class=\"op\">=</span> tf.nn.softmax(tf.matmul(tf.nn.sigmoid(fake_l2),w) <span class=\"op\">+</span> b)\n    fake_labels <span class=\"op\">=</span> tf.constant(fake_labels)\n    diff <span class=\"op\">=</span> tf.reduce_sum(tf.<span class=\"bu\">abs</span>(fake_labels <span class=\"op\">-</span> fake_y))\n    <span class=\"co\">#train the fake_l2 to minimize diff</span>\n    opt <span class=\"op\">=</span> tf.train.GradientDescentOptimizer(<span class=\"fl\">1.</span>)\n    grads <span class=\"op\">=</span> opt.compute_gradients(diff, [fake_l2])\n    tstep <span class=\"op\">=</span> opt.apply_gradients(grads)\n    sess.run(tf.initialize_variables([fake_l2]))\n\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1000</span>):\n        tstep.run()\n\n    fake_l1_inv <span class=\"op\">=</span> tf.matmul(fake_l2 <span class=\"op\">-</span> b2, w2_inv)\n    fake_l1_inv_norm <span class=\"op\">=</span> squash(fake_l1_inv,<span class=\"dv\">1</span>)\n    fake_x_inv <span class=\"op\">=</span> tf.matmul(logit(fake_l1_inv_norm) <span class=\"op\">-</span> b1,w1_inv)\n\n    <span class=\"cf\">return</span> fake_x_inv.<span class=\"bu\">eval</span>(), fake_y.<span class=\"bu\">eval</span>()\n\n\ngenned, fakes <span class=\"op\">=</span> generate(<span class=\"dv\">9</span>, np.eye(<span class=\"dv\">10</span>)[[<span class=\"dv\">1</span>,<span class=\"dv\">2</span>,<span class=\"dv\">3</span>,<span class=\"dv\">4</span>,<span class=\"dv\">5</span>,<span class=\"dv\">6</span>,<span class=\"dv\">7</span>,<span class=\"dv\">8</span>,<span class=\"dv\">9</span>]].astype(<span class=\"st\">&quot;float32&quot;</span>))</code></pre></div>\n<p>Here we see that the network is over 99.5% confident that each of its hidden layer representations are good representations. Below that we see their inverse projections.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">np.<span class=\"bu\">max</span>(fakes,axis<span class=\"op\">=</span><span class=\"dv\">1</span>)</code></pre></div>\n<pre><code>array([ 0.99675035,  0.99740452,  0.99649602,  0.99652439,  0.99734575,\n        0.99607605,  0.99735802,  0.99755549,  0.99680138], dtype=float32)</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plot_nxn(<span class=\"dv\">3</span>,genned)</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/INN_output_20_0.png\" />\n</figure>\n<p>A bit noisy, but it works!</p>\n<h3 id=\"visualizing-features\">Visualizing Features</h3>\n<p>We will now show the inverse projection of each of the 100 features of the hidden representation, to get an idea of what the neural network has learned. Unfortunately, the noise is overwhelming, but we can sort of make out shadows of the learned features.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> generate_features():\n    fake_l2 <span class=\"op\">=</span> tf.constant(np.eye(<span class=\"dv\">100</span>).astype(<span class=\"st\">&quot;float32&quot;</span>)<span class=\"op\">*</span>(<span class=\"fl\">1e8</span>))\n    fake_l1_inv <span class=\"op\">=</span> tf.matmul(fake_l2 <span class=\"op\">-</span> b2, w2_inv)\n    fake_l1_inv_norm <span class=\"op\">=</span> squash(fake_l1_inv,<span class=\"dv\">1</span>)\n    fake_x_inv <span class=\"op\">=</span> tf.matmul(logit(fake_l1_inv_norm) <span class=\"op\">-</span> b1,w1_inv)\n    <span class=\"cf\">return</span> fake_x_inv.<span class=\"bu\">eval</span>()\n\n\ngenned <span class=\"op\">=</span> generate_features()\nplot_nxn(<span class=\"dv\">10</span>,np.<span class=\"bu\">round</span>(genned,<span class=\"dv\">1</span>))</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/INN_output_24_0.png\" />\n</figure>\n</body>\n</html>"
}