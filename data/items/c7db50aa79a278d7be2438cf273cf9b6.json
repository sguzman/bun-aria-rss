{
  "title": "My Qualifying Exam (Oral)",
  "link": "http://dustintran.com/blog/my-qualifying-exam-oral",
  "guid": "http://dustintran.com/blog/my-qualifying-exam-oral",
  "description": "<p>I’m taking my qualifying exam this Tuesday—which may surprise some of\nyou that I haven’t already done it! This is mostly due to logistical\nkerfuffles as I transferred Ph.D.’s and I also tend to avoid coursework\nlike the plague.</p>\n\n<p>Each university has its own culture around an oral or qualifying exam.\nColumbia’s Computer Science department involves the following:</p>\n\n<blockquote>\n  <p>The committee, after consideration of the student’s input, selects a syllabus of the 20-30 most significant documents that encompass the state of the art in the area. […] The oral exam begins with the student’s 30 minute critical evaluation of the syllabus materials, and is followed by no more than 90 minutes of questioning by the committee on any subject matter related to their contents. The student is judged primarily on the oral evidence, but the content and style of the presentation can account for part of the decision.\n<a href=\"http://www.cs.columbia.edu/education/phd/requirements/candidacy/\">[url]</a></p>\n</blockquote>\n\n<p>My syllabus concerns <em>Bayesian deep learning</em>, which is the\nsynthesis of modern Bayesian analysis with deep learning.\nThe syllabus includes 29 papers published in 2014 or later,\nrepresenting “the most significant documents that encompass the\nstate of the art in the area.”\nI got multiple requests from friends to share the list, so I decided\nto just share it publically.</p>\n\n<p><strong>Probabilistic programming &amp; AI systems</strong></p>\n<ol>\n  <li><a href=\"#mansinghka2014venture\">Mansinghka, Selsam, &amp; Perov (2014)</a></li>\n  <li><a href=\"#tristan2014augur\">Tristan et al. (2014)</a></li>\n  <li><a href=\"#schulman2015gradient\">Schulman, Heess, Weber, &amp; Abbeel (2015)</a></li>\n  <li><a href=\"#narayanan2016probabilistic\">Narayanan, Carette, Romano, Shan, &amp; Zinkov (2016)</a></li>\n  <li><a href=\"#abadi2016tensorflow\">Abadi et al. (2016)</a></li>\n  <li><a href=\"#carpenter2016stan\">Carpenter et al. (2016)</a></li>\n  <li><a href=\"#tran2016edward\">Tran et al. (2016)</a></li>\n  <li><a href=\"#kucukelbir2017automatic\">Kucukelbir, Tran, Ranganath, Gelman, &amp; Blei (2017)</a></li>\n  <li><a href=\"#tran2017deep\">Tran et al. (2017)</a></li>\n  <li><a href=\"#neubig2017dynet\">Neubig et al. (2017)</a></li>\n</ol>\n\n<p><strong>Variational inference</strong></p>\n<ol>\n  <li><a href=\"#kingma2014autoencoding\">Kingma &amp; Welling (2014)</a></li>\n  <li><a href=\"#ranganath2014black\">Ranganath, Gerrish, &amp; Blei (2014)</a></li>\n  <li><a href=\"#rezende2014stochastic\">Rezende, Mohamed, &amp; Wierstra (2014)</a></li>\n  <li><a href=\"#mnih2014neural\">Mnih &amp; Gregor (2014)</a></li>\n  <li><a href=\"#rezende2015variational\">Rezende &amp; Mohamed (2015)</a></li>\n  <li><a href=\"#salimans2015markov\">Salimans, Kingma, &amp; Welling (2015)</a></li>\n  <li><a href=\"#tran2016variational\">Tran, Ranganath, &amp; Blei (2016)</a></li>\n  <li><a href=\"#ranganath2016hierarchical\">Ranganath, Tran, &amp; Blei (2016)</a></li>\n  <li><a href=\"#maaloe2016auxiliary\">Maaløe, Sønderby, Sønderby, &amp; Winther (2016)</a></li>\n  <li><a href=\"#johnson2016composing\">Johnson, Duvenaud, Wiltschko, Datta, &amp; Adams (2016)</a></li>\n  <li><a href=\"#ranganath2016operator\">Ranganath, Altosaar, Tran, &amp; Blei (2016)</a></li>\n  <li><a href=\"#gelman2017expectation\">Gelman et al. (2017)</a></li>\n</ol>\n\n<p><strong>Implicit probabilistic models &amp; adversarial training</strong></p>\n<ol>\n  <li><a href=\"#goodfellow2014generative\">Goodfellow et al. (2014)</a></li>\n  <li><a href=\"#dziugaite2015training\">Dziugaite, Roy, &amp; Ghahramani (2015)</a></li>\n  <li><a href=\"#li2015generative\">Li, Swersky, &amp; Zemel (2015)</a></li>\n  <li><a href=\"#radford2016unsupervised\">Radford, Metz, &amp; Chintala (2016)</a></li>\n  <li><a href=\"#mohamed2016learning\">Mohamed &amp; Lakshminarayanan (2016)</a></li>\n  <li><a href=\"#arjovsky2017wasserstein\">Arjovsky, Chintala, &amp; Bottou (2017)</a></li>\n  <li><a href=\"#tran2017deepand\">Tran, Ranganath, &amp; Blei (2017)</a></li>\n</ol>\n\n<p>Committee: David Blei, Andrew Gelman, Daniel Hsu.</p>\n\n<p>Disclaimer: I favored papers which have\nshown to be or are most likely to be long-lasting in influence (this\nmeans fewer papers from 2017); papers on methodology rather than\napplications (only to narrow the scope); original papers over surveys;\nand my own papers (because it’s my oral). If I did not cite you or if\nyou have strong opinions about a missing paper, recall <a href=\"https://en.wikipedia.org/wiki/Hanlon%27s_razor\">Hanlon’s\nrazor</a>. E-mail me your\nsuggestions.</p>\n\n<p>Update (08/08/2017): I passed the oral. :-)</p>\n\n<h2 id=\"references\">References</h2>\n\n<ol class=\"bibliography\"><li><span id=\"abadi2016tensorflow\">Abadi, Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … Zhang, X. (2016). TensorFlow: A system for large-scale machine learning, <i>cs.DC</i>, 1–18.</span></li>\n<li><span id=\"arjovsky2017wasserstein\">Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). Wasserstein GAN. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"carpenter2016stan\">Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., … Riddell, A. (2016). Stan: a probabilistic programming language. <i>Journal of Statistical Software</i>.</span></li>\n<li><span id=\"dziugaite2015training\">Dziugaite, G. K., Roy, D. M., &amp; Ghahramani, Z. (2015). Training generative neural networks via Maximum Mean Discrepancy optimization. In <i>Uncertainty in Artificial Intelligence</i>.</span></li>\n<li><span id=\"gelman2017expectation\">Gelman, A., Vehtari, A., Jylänki, P., Sivula, T., Tran, D., Sahai, S., … Robert, C. (2017). Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"goodfellow2014generative\">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In <i>Neural Information Processing Systems</i>.</span></li>\n<li><span id=\"johnson2016composing\">Johnson, M. J., Duvenaud, D., Wiltschko, A. B., Datta, S. R., &amp; Adams, R. P. (2016). Composing graphical models with neural networks for structured representations and fast inference. In <i>Neural Information Processing Systems</i>.</span></li>\n<li><span id=\"kingma2014autoencoding\">Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. In <i>International Conference on Learning Representations</i>.</span></li>\n<li><span id=\"kucukelbir2017automatic\">Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., &amp; Blei, D. M. (2017). Automatic Differentiation Variational Inference. <i>Journal of Machine Learning Research</i>, <i>18</i>, 1–45.</span></li>\n<li><span id=\"li2015generative\">Li, Y., Swersky, K., &amp; Zemel, R. (2015). Generative Moment Matching Networks. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"maaloe2016auxiliary\">Maaløe, L., Sønderby, C. K., Sønderby, S. K., &amp; Winther, O. (2016). Auxiliary Deep Generative Models. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"mansinghka2014venture\">Mansinghka, V., Selsam, D., &amp; Perov, Y. (2014). Venture: a higher-order probabilistic programming platform with programmable inference. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"mnih2014neural\">Mnih, A., &amp; Gregor, K. (2014). Neural Variational Inference and Learning in Belief Networks. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"mohamed2016learning\">Mohamed, S., &amp; Lakshminarayanan, B. (2016). Learning in Implicit Generative Models. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"narayanan2016probabilistic\">Narayanan, P., Carette, J., Romano, W., Shan, C.-chieh, &amp; Zinkov, R. (2016). Probabilistic Inference by Program Transformation in Hakaru (System Description). In <i>International Symposium on Functional and Logic Programming</i>. Springer, Cham.</span></li>\n<li><span id=\"neubig2017dynet\">Neubig, G., Dyer, C., Goldberg, Y., Matthews, A., Ammar, W., Anastasopoulos, A., … Yin, P. (2017). DyNet: The Dynamic Neural Network Toolkit. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"radford2016unsupervised\">Radford, A., Metz, L., &amp; Chintala, S. (2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In <i>International Conference on Learning Representations</i>.</span></li>\n<li><span id=\"ranganath2016operator\">Ranganath, R., Altosaar, J., Tran, D., &amp; Blei, D. M. (2016). Operator Variational Inference. In <i>Neural Information Processing Systems</i>.</span></li>\n<li><span id=\"ranganath2014black\">Ranganath, R., Gerrish, S., &amp; Blei, D. M. (2014). Black Box Variational Inference. In <i>Artificial Intelligence and Statistics</i>.</span></li>\n<li><span id=\"ranganath2016hierarchical\">Ranganath, R., Tran, D., &amp; Blei, D. M. (2016). Hierarchical Variational Models. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"rezende2015variational\">Rezende, D. J., &amp; Mohamed, S. (2015). Variational Inference with Normalizing Flows. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"rezende2014stochastic\">Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"salimans2015markov\">Salimans, T., Kingma, D. P., &amp; Welling, M. (2015). Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. In <i>International Conference on Machine Learning</i>.</span></li>\n<li><span id=\"schulman2015gradient\">Schulman, J., Heess, N., Weber, T., &amp; Abbeel, P. (2015). Gradient Estimation Using Stochastic Computation Graphs. In <i>Neural Information Processing Systems</i>.</span></li>\n<li><span id=\"tran2017deep\">Tran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., &amp; Blei, D. M. (2017). Deep Probabilistic Programming. In <i>International Conference on Learning Representations</i>.</span></li>\n<li><span id=\"tran2016edward\">Tran, D., Kucukelbir, A., Dieng, A. B., Rudolph, M., Liang, D., &amp; Blei, D. M. (2016). Edward: A library for probabilistic modeling, inference, and criticism. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"tran2017deepand\">Tran, D., Ranganath, R., &amp; Blei, D. M. (2017). Deep and Hierarchical Implicit Models. <i>ArXiv.org</i>.</span></li>\n<li><span id=\"tran2016variational\">Tran, D., Ranganath, R., &amp; Blei, D. M. (2016). The Variational Gaussian Process. In <i>International Conference on Learning Representations</i>.</span></li>\n<li><span id=\"tristan2014augur\">Tristan, J.-B., Huang, D., Tassarotti, J., Pocock, A. C., Green, S., &amp; Steele, G. L. (2014). Augur: Data-Parallel Probabilistic Modeling. In <i>Neural Information Processing Systems</i> (pp. 2600–2608).</span></li></ol>",
  "pubDate": "Mon, 07 Aug 2017 00:00:00 -0700"
}