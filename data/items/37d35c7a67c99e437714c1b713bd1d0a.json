{
  "title": "A Full Hardware Guide to Deep Learning",
  "link": "https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/",
  "comments": "https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/#comments",
  "dc:creator": "Tim Dettmers",
  "pubDate": "Sun, 16 Dec 2018 18:25:41 +0000",
  "category": [
    "Hardware",
    "AMD",
    "CPU",
    "GPU",
    "Intel",
    "PCIe Lanes"
  ],
  "guid": "https://timdettmers.wordpress.com/?p=121",
  "description": "<p>Here I will guide you step by step through the hardware you will need for a cheap high performance system for deep learning.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/\">A Full Hardware Guide to Deep Learning</a> appeared first on <a rel=\"nofollow\" href=\"https://timdettmers.com\">Tim Dettmers</a>.</p>\n",
  "content:encoded": "\n<p class=\"eplus-Jtk1uQ\">Deep Learning is very computationally intensive, so you will need a fast CPU with many cores, right? Or is it maybe wasteful to buy a fast CPU? One of the worst things you can do when building a deep learning system is to waste money on hardware that is unnecessary. Here I will guide you step by step through the hardware you will need for a cheap high-performance system.</p>\n\n\n\n<span id=\"more-121\"></span>\n\n\n\n<p class=\"eplus-UTx6Bt\">Over the years, I build a total of 7 different deep learning workstations and despite careful research and reasoning, I made my fair share of mistake in selecting hardware parts. In this guide, I want to share my experience that I gained over the years so that you do not make the same mistakes that I did before.</p>\n\n\n\n<p class=\"eplus-LmAMuN\">The blog post is ordered by mistake severity. This means the mistakes where people usually waste the most money come first.</p>\n\n\n\n\n<h2><strong>GPU</strong></h2>\n<p>This blog post assumes that you will use a GPU for deep learning. If you are building or upgrading your system for deep learning, it is not sensible to leave out the GPU. The GPU is just the heart of deep learning applications – the improvement in processing speed is just too huge to ignore.</p>\n<p>I talked at length about GPU choice in <a href=\"https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/\">my GPU recommendations blog post</a>, and the choice of your GPU is probably the most critical choice for your deep learning system. There are three main mistakes that you can make when choosing a GPU: (1) bad cost/performance, (2) not enough memory, (3) poor cooling.</p>\n<p>For good cost/performance, I generally recommend an RTX 2070 or an RTX 2080 Ti. If you use these cards you should use 16-bit models. Otherwise, GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are fair choices and you can use these GPUs with 32-bit (but not 16-bit).</p>\n<p>Be careful about the memory requirements when you pick your GPU. RTX cards, which can run in 16-bits, can train models which are twice as big with the same memory compared to GTX cards. As such RTX cards have a memory advantage and picking RTX cards and learn how to use 16-bit models effectively will carry you a long way. In general, the requirements for memory are roughly the following:</p>\n<ul>\n<li>Research that is hunting state-of-the-art scores: >=11 GB</li>\n<li>Research that is hunting for interesting architectures: >=8 GB</li>\n<li>Any other research: 8 GB</li>\n<li>Kaggle: 4 &#8211; 8 GB</li>\n<li>Startups: 8 GB (but check the specific application area for model sizes)</li>\n<li>Companies: 8 GB for prototyping, >=11 GB for training</li>\n</ul>\n<p>Another problem to watch out for, especially if you buy multiple RTX cards is cooling. If you want to stick GPUs into PCIe slots which are next to each other you should make sure that you get GPUs with a blower-style fan. Otherwise you might run into temperature issues and your GPUs will be slower (about 30%) and die faster.</p>\n<figure id=\"attachment_124\" aria-describedby=\"caption-attachment-124\" style=\"width: 700px\" class=\"wp-caption aligncenter\"><a href=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg\"><img data-attachment-id=\"124\" data-permalink=\"https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/suspectlineup/\" data-orig-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=3264%2C1836&ssl=1\" data-orig-size=\"3264,1836\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"2.4\",\"credit\":\"\",\"camera\":\"U9200\",\"caption\":\"\",\"created_timestamp\":\"1403265762\",\"copyright\":\"\",\"focal_length\":\"4.13\",\"iso\":\"122\",\"shutter_speed\":\"0.016666\",\"title\":\"\",\"orientation\":\"1\"}\" data-image-title=\"suspectlineup\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=300%2C169&ssl=1\" data-large-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?fit=1024%2C576&ssl=1\" class=\"wp-image-124 size-full\" src=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/suspectlineup.jpg?resize=700%2C394\" alt=\"\" width=\"700\" height=\"394\" border=\"None\" data-recalc-dims=\"1\" /></a><figcaption id=\"caption-attachment-124\" class=\"wp-caption-text\"><strong>Suspect line-up</strong><br />Can you identify the hardware part which is at fault for bad performance? One of these GPUs? Or maybe it is the fault of the CPU after all?</figcaption></figure>\n<h2>RAM</h2>\n<p>The main mistakes with RAM is to buy RAM with a too high clock rate. The second mistake is to buy not enough RAM to have a smooth prototyping experience.</p>\n<h3>Needed RAM Clock Rate</h3>\n<p>RAM clock rates are marketing stints where RAM companies lure you into buying &#8220;faster&#8221; RAM which actually yields little to no performance gains. This is best explained by &#8220;<a href=\"https://www.youtube.com/watch?v=D_Yt4vSZKVk\">Does RAM speed REALLY matter?</a>&#8221; video on RAM von Linus Tech Tips.</p>\n<p>Furthermore, it is important to know that RAM speed is pretty much irrelevant for fast CPU RAM->GPU RAM transfers. This is so because (1) if you used <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers\">pinned memory</a>, your mini-batches will be transferred to the GPU without involvement from the CPU, and (2) if you do not use pinned memory the performance gains of fast vs slow RAMs is about 0-3% &#8212; spend your money elsewhere!</p>\n<h3>RAM Size</h3>\n<p>RAM size does not affect deep learning performance. However, it might hinder you from executing your GPU code comfortably (without swapping to disk). You should have enough RAM to comfortable work with your GPU. This means you should have at least the amount of RAM that matches your biggest GPU. For example, if you have a Titan RTX with 24 GB of memory you should have at least 24 GB of RAM. However, if you have more GPUs you do not necessarily need more RAM.</p>\n<p>The problem with this &#8220;match largest GPU memory in RAM&#8221; strategy is that you might still fall short of RAM if you are processing large datasets. The best strategy here is to match your GPU and if you feel that you do not have enough RAM just buy some more.</p>\n<p>A different strategy is influenced by psychology: Psychology tells us that concentration is a resource that is depleted over time. RAM is one of the few hardware pieces that allows you to conserve your concentration resource for more difficult programming problems. Rather than spending lots of time on circumnavigating RAM bottlenecks, you can invest your concentration on more pressing matters if you have more RAM.  With a lot of RAM you can avoid those bottlenecks, save time and increase productivity on more pressing problems. Especially in Kaggle competitions, I found additional RAM very useful for feature engineering. So if you have the money and do a lot of pre-processing then additional RAM might be a good choice. So with this strategy, you want to have more, cheap RAM now rather than later.</p>\n<h2>CPU</h2>\n<p>The main mistake that people make is that people pay too much attention to PCIe lanes of a CPU. You should not care much about PCIe lanes. Instead, just look up if your CPU and motherboard combination supports the number of GPUs that you want to run. The second most common mistake is to get a CPU which is too powerful.</p>\n<h3>CPU and PCI-Express</h3>\n<p>People go crazy about PCIe lanes! However, the thing is that it has almost no effect on deep learning performance. If you have a single GPU, PCIe lanes are only needed to transfer data from your CPU RAM to your GPU RAM quickly. However, an ImageNet batch of 32 images (32x225x225x3) and 32-bit needs 1.1 milliseconds with 16 lanes, 2.3 milliseconds with 8 lanes, and 4.5 milliseconds with 4 lanes. These are theoretic numbers, and in practice you often see PCIe be twice as slow — but this is still lightning fast! PCIe lanes often have a latency in the nanosecond range and thus latency can be ignored.</p>\n<p>Putting this together we have for an ImageNet mini-batch of 32 images and a ResNet-152 the following timing:</p>\n<ul>\n<li>Forward and backward pass: 216 milliseconds (ms)</li>\n<li>16 PCIe lanes CPU->GPU transfer: About 2 ms (1.1 ms theoretical)</li>\n<li>8 PCIe lanes CPU->GPU transfer: About 5 ms (2.3 ms)</li>\n<li>4 PCIe lanes CPU->GPU transfer: About 9 ms (4.5 ms)</li>\n</ul>\n<p>Thus going from 4 to 16 PCIe lanes will give you a performance increase of roughly 3.2%. However, if you use <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">PyTorch&#8217;s data loader</a> with pinned memory you gain exactly 0% performance. So do not waste your money on PCIe lanes if you are using a single GPU!</p>\n<p>When you select CPU PCIe lanes and motherboard PCIe lanes make sure that you select a combination which supports the desired number of GPUs. If you buy a motherboard that supports 2 GPUs, and you want to have 2 GPUs eventually, make sure that you buy a CPU that supports 2 GPUs, but do not necessarily look at PCIe lanes.</p>\n<h3>PCIe Lanes and Multi-GPU Parallelism</h3>\n<p>Are PCIe lanes important if you train networks on multiple GPUs with data parallelism? I have <a href=\"https://arxiv.org/abs/1511.04561\">published a paper on this at ICLR2016</a>, and I can tell you if you have 96 GPUs then PCIe lanes are really important. However, if you have 4 or fewer GPUs this does not matter much. If you parallelize across 2-3 GPUs, I would not care at all about PCIe lanes. With 4 GPUs, I would make sure that I can get a support of 8 PCIe lanes per GPU (32 PCIe lanes in total). Since almost nobody runs a system with more than 4 GPUs as a rule of thumb: Do not spend extra money to get more PCIe lanes per GPU — it does not matter!</p>\n<h3>Needed CPU Cores</h3>\n<p>To be able to make a wise choice for the CPU we first need to understand the CPU and how it relates to deep learning. What does the CPU do for deep learning? The CPU does little computation when you run your deep nets on a GPU. Mostly it (1) initiates GPU function calls, (2) executes CPU functions.</p>\n<p>By far the most useful application for your CPU is data preprocessing. There are two different common data processing strategies which have different CPU needs.</p>\n<p>The first strategy is preprocessing while you train:</p>\n<p>Loop:</p>\n<ol>\n<li>Load mini-batch</li>\n<li>Preprocess mini-batch</li>\n<li>Train on mini-batch</li>\n</ol>\n<p>The second strategy is preprocessing before any training:</p>\n<ol>\n<li>Preprocess data</li>\n<li>Loop:\n<ol>\n<li>Load preprocessed mini-batch</li>\n<li>Train on mini-batch</li>\n</ol>\n</li>\n</ol>\n<p>For the first strategy, a good CPU with many cores can boost performance significantly. For the second strategy, you do not need a very good CPU. For the first strategy, I recommend a minimum of 4 threads per GPU &#8212; that is usually two cores per GPU. I have not done hard tests for this, but you should gain about 0-5% additional performance per additional core/GPU.</p>\n<p>For the second strategy, I recommend a minimum of 2 threads per GPU &#8212; that is usually one core per GPU. You will not see significant gains in performance when you have more cores if you are using the second strategy.</p>\n<h3>Needed CPU Clock Rate (Frequency)</h3>\n<p>When people think about fast CPUs they usually first think about the clock rate.  4GHz is better than 3.5GHz, or is it? This is generally true for comparing processors with the same architecture, e.g. “Ivy Bridge”, but it does not compare well between processors. Also, it is not always the best measure of performance.</p>\n<p>In the case of deep learning there is very little computation to be done by the CPU: Increase a few variables here, evaluate some Boolean expression there, make some function calls on the GPU or within the program – all these depend on the CPU core clock rate.</p>\n<p>While this reasoning seems sensible, there is the fact that the CPU has 100% usage when I run deep learning programs, so what is the issue here? I did some CPU core rate underclocking experiments to find out.</p>\n<figure id=\"attachment_161\" aria-describedby=\"caption-attachment-161\" style=\"width: 804px\" class=\"wp-caption aligncenter\"><a href=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png\"><img data-attachment-id=\"161\" data-permalink=\"https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/cpu_underclocking/\" data-orig-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=603%2C406&ssl=1\" data-orig-size=\"603,406\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"cpu_underclocking\" data-image-description=\"\" data-image-caption=\"<p>CPU underclocking on MNIST and ImageNet: Performance is measured as time taken on 100 epochs MNIST or half an epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a base line for each CPU. For comparison: Upgrading from a GTX 580 to a GTX Titan is about +20% performance; from GTX Titan to GTX 980 another +30% performance; GPU overclocking yields about +5% performance for any GPU</p>\n\" data-medium-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=300%2C202&ssl=1\" data-large-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?fit=603%2C406&ssl=1\" class=\"wp-image-161\" src=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/cpu_underclocking2.png?resize=804%2C541\" alt=\"CPU underclocking on MNIST and ImageNet: Performance is measured as time taken on 100 epochs MNIST or half an epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a base line for each CPU. For comparison: Upgrading from a GTX 580 to a GTX Titan is about +20% performance; from GTX Titan to GTX 980 another +30% performance; GPU overclocking yields about +5% performance for any GPU\" width=\"804\" height=\"541\" data-recalc-dims=\"1\" /></a><figcaption id=\"caption-attachment-161\" class=\"wp-caption-text\"><strong>CPU underclocking on MNIST and ImageNet</strong>: Performance is measured as time taken on 200 epochs MNIST or a quarter epoch on ImageNet with different CPU core clock rates, where the maximum clock rate is taken as a baseline for each CPU. For comparison: Upgrading from a GTX 680 to a GTX Titan is about +15% performance; from GTX Titan to GTX 980 another +20% performance; GPU overclocking yields about +5% performance for any GPU</figcaption></figure>\n<p style=\"text-align: justify;\">Note that these experiments are on a hardware that is dated, however, these results should still be the same for modern CPUs/GPUs.</p>\n<h2 style=\"text-align: justify;\"><strong>Hard drive/SSD</strong></h2>\n<p>The hard drive is not usually a bottleneck for deep learning. However, if you do stupid things it will hurt you: If you read your data from disk when they are needed (blocking wait) then a 100 MB/s hard drive will cost you about 185 milliseconds for an ImageNet mini-batch of size 32 — ouch! However, if you asynchronously fetch the data before it is used (for example torch vision loaders), then you will have loaded the mini-batch in 185 milliseconds while the compute time for most deep neural networks on ImageNet is about 200 milliseconds. Thus you will not face any performance penalty since you load the next mini-batch while the current is still computing.</p>\n<p>However, I recommend an SSD for comfort and productivity: Programs start and respond more quickly, and pre-processing with large files is quite a bit faster. If you buy an NVMe SSD you will have an even smoother experience when compared to a regular SSD.</p>\n<p>Thus the ideal setup is to have a large and slow hard drive for datasets and an SSD for productivity and comfort.</p>\n<h2 style=\"text-align: justify;\"><strong>Power supply unit (PSU)</strong></h2>\n<p>Generally, you want a PSU that is sufficient to accommodate all your future GPUs. GPUs typically get more energy efficient over time; so while other components will need to be replaced, a PSU should last a long while so a good PSU is a good investment.</p>\n<p>You can calculate the required watts by adding up the watt of your CPU and GPUs with an additional 10% of watts for other components and as a buffer for power spikes. For example, if you have 4 GPUs with each 250 watts TDP and a CPU with 150 watts TDP, then you will need a PSU with a minimum of 4&#215;250 + 150 + 100 = 1250 watts. I would usually add another 10% just to be sure everything works out, which in this case would result in a total of 1375 Watts. I would round up in this case an get a 1400 watts PSU.</p>\n<p>One important part to be aware of is that even if a PSU has the required wattage, it might not have enough PCIe 8-pin or 6-pin connectors. Make sure you have enough connectors on the PSU to support all your GPUs!</p>\n<p>Another important thing is to buy a PSU with high power efficiency rating – especially if you run many GPUs and will run them for a longer time.</p>\n<p>Running a 4 GPU system on full power (1000-1500 watts) to train a convolutional net for two weeks will amount to 300-500 kWh, which in Germany – with rather high power costs of 20 cents per kWh – will amount to 60-100€ ($66-111). If this price is for a 100% efficiency, then training such a net with an 80% power supply would increase the costs by an additional 18-26€ – ouch! This is much less for a single GPU, but the point still holds – spending a bit more money on an efficient power supply makes good sense.</p>\n<p>Using a couple of GPUs around the clock will significantly increase your carbon footprint and it will overshadow transportation (mainly airplane) and other factors that contribute to your footprint. If you want to be responsible, please consider going <a href=\"https://wp.nyu.edu/ml2/carbon-neutral-lab/\">carbon neutral like the NYU Machine Learning for Language Group (ML2)</a> — it is easy to do, cheap, and should be standard for deep learning researchers.</p>\n<h2>CPU and GPU Cooling</h2>\n<p>Cooling is important and it can be a significant bottleneck which reduces performance more than poor hardware choices do. You should be fine with a standard heat sink or all-in-one (AIO) water cooling solution for your CPU, but what for your GPU you will need to make special considerations.</p>\n<h3>Air Cooling GPUs</h3>\n<p>Air cooling is safe and solid for a single GPU or if you have multiple GPUs with space between them (2 GPUs in a 3-4 GPU case). However, one of the biggest mistakes can be made when you try to cool 3-4 GPUs and you need to think carefully about your options in this case.</p>\n<p>Modern GPUs will increase their speed – and thus power consumption – up to their maximum when they run an algorithm, but as soon as the GPU hits a temperature barrier – often 80 °C – the GPU will decrease the speed so that the temperature threshold is not breached. This enables the best performance while keeping your GPU safe from overheating.</p>\n<p>However, typical pre-programmed schedules for fan speeds are badly designed for deep learning programs, so that this temperature threshold is reached within seconds after starting a deep learning program. The result is a decreased performance (0-10%) which can be significant for multiple GPUs (10-25%) where the GPU heat up each other.</p>\n<p>Since NVIDIA GPUs are first and foremost gaming GPUs, they are optimized for Windows. You can change the fan schedule with a few clicks in Windows, but not so in Linux, and as most deep learning libraries are written for Linux this is a problem.</p>\n<p>The only option under Linux is to use to set a configuration for your Xorg server (Ubuntu) where you set the option &#8220;coolbits&#8221;. This works very well for a single GPU, but if you have multiple GPUs where some of them are headless, i.e. they have no monitor attached to them, you have to emulate a monitor which is hard and hacky. I tried it for a long time and had frustrating hours with a live boot CD to recover my graphics settings – I could never get it running properly on headless GPUs.</p>\n<p>The most important point of consideration if you run 3-4 GPUs on air cooling is to pay attention to the fan design. The &#8220;blower&#8221; fan design pushes the air out to the back of the case so that fresh, cooler air is pushed into the GPU. Non-blower fans suck in air in the vincity of the GPU and cool the GPU. However, if you have multiple GPUs next to each other then there is no cool air around and GPUs with non-blower fans will heat up more and more until they throttle themselves down to reach cooler temperatures. Avoid non-blower fans in 3-4 GPU setups at all costs.</p>\n<h3>Water Cooling GPUs For Multiple GPUs</h3>\n<p>Another, more costly, and craftier option is to use water cooling. I do not recommend water cooling if you have a single GPU or if you have space between your two GPUs (2 GPUs in 3-4 GPU board). However, water cooling makes sure that even the beefiest GPU stay cool in a 4 GPU setup which is not possible when you cool with air. Another advantage of water cooling is that it operates much more silently, which is a big plus if you run multiple GPUs in an area where other people work. Water cooling will cost you about $100 for each GPU and some additional upfront costs (something like $50). Water cooling will also require some additional effort to assemble your computer, but there are many detailed guides on that and it should only require a few more hours of time in total. Maintenance should not be that complicated or effortful.</p>\n<h3>A Big Case for Cooling?</h3>\n<p>I bought large towers for my deep learning cluster, because they have additional fans for the GPU area, but I found this to be largely irrelevant: About 2-5 °C decrease, not worth the investment and the bulkiness of the cases. The most important part is really the cooling solution directly on your GPU — do not select an expensive case for its GPU cooling capability. Go cheap here. The case should fit your GPUs but thats it!</p>\n<h3>Conclusion Cooling</h3>\n<p>So in the end it is simple: For 1 GPU air cooling is best. For multiple GPUs, you should get blower-style air cooling and accept a tiny performance penalty (10-15%), or you pay extra for water cooling which is also more difficult to setup correctly and you have no performance penalty. Air and water cooling are all reasonable choices in certain situations. I would however recommend air cooling for simplicity in general — get a blower-style GPU if you run multiple GPUs. If you want to user water cooling try to find all-in-one (AIO) water cooling solutions for GPUs.</p>\n<h2>Motherboard</h2>\n<p>Your motherboard should have enough PCIe ports to support the number of GPUs you want to run (usually limited to four GPUs, even if you have more PCIe slots); remember that most GPUs have a width of two PCIe slots, so buy a motherboard that has enough space between PCIe slots if you intend to use multiple GPUs. Make sure your motherboard not only has the PCIe slots, but actually supports the GPU setup that you want to run. You can usually find information in this if you search your motherboard of choice on newegg and look at PCIe section on the specification page.</p>\n<h2>Computer Case</h2>\n<p>When you select a case, you should make sure that it supports full length GPUs that sit on top of your motherboard. Most cases support full length GPUs, but you should be suspicious if you buy a small case. Check its dimensions and specifications; you can also try a google image search of that model and see if you find pictures with GPUs in them.</p>\n<p>If you use custom water cooling, make sure your case has enough space for the radiators. This is especially true if you use water cooling for your GPUs. The radiator of each GPU will need some space — make sure your setup actually fits into the GPU.</p>\n<h3 style=\"text-align: justify;\"><strong>Monitors</strong></h3>\n<p style=\"text-align: justify;\">I first thought it would be silly to write about monitors also, but they make such a huge difference and are so important that I just have to write about them.</p>\n<p style=\"text-align: justify;\">The money I spent on my 3 27 inch monitors is probably the best money I have ever spent. Productivity goes up by a lot when using multiple monitors. I feel desperately crippled if I have to work with a single monitor.  Do not short-change yourself on this matter. What good is a fast deep learning system if you are not able to operate it in an efficient manner?</p>\n<figure id=\"attachment_123\" aria-describedby=\"caption-attachment-123\" style=\"width: 700px\" class=\"wp-caption aligncenter\"><a href=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg\"><img data-attachment-id=\"123\" data-permalink=\"https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/2015-03-04-13-58-10/\" data-orig-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=3264%2C1836&ssl=1\" data-orig-size=\"3264,1836\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"2.4\",\"credit\":\"\",\"camera\":\"U9200\",\"caption\":\"\",\"created_timestamp\":\"1425477490\",\"copyright\":\"\",\"focal_length\":\"4.13\",\"iso\":\"104\",\"shutter_speed\":\"0.016666\",\"title\":\"\",\"orientation\":\"1\"}\" data-image-title=\"2015-03-04 13.58.10\" data-image-description=\"<p>Typical layout when I do deep learning: Left: Papers, google searcheres, gmail, stackoverflow threads; middle: Code; right: Output windows, R, folders, systems monitors, GPU monitors, to-do list, and other small applications.</p>\n\" data-image-caption=\"\" data-medium-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=300%2C169&ssl=1\" data-large-file=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?fit=1024%2C576&ssl=1\" class=\"wp-image-123 size-full\" src=\"https://i0.wp.com/timdettmers.com/wp-content/uploads/2015/03/2015-03-04-13-58-10.jpg?resize=700%2C394\" alt=\"2015-03-04 13.58.10\" width=\"700\" height=\"394\" data-recalc-dims=\"1\" /></a><figcaption id=\"caption-attachment-123\" class=\"wp-caption-text\"><strong>Typical monitor layout when I do deep learning:</strong> Left: Papers, Google searches, gmail, stackoverflow; middle: Code; right: Output windows, R, folders, systems monitors, GPU monitors, to-do list, and other small applications.</figcaption></figure>\n<h4 style=\"text-align: justify;\"><strong>Some words on building a PC</strong></h4>\n<p style=\"text-align: justify;\">Many people are scared to build computers. The hardware components are expensive and you do not want to do something wrong. But it is really simple as components that do not belong together do not fit together. The motherboard manual is often very specific how to assemble everything and there are tons of guides and step by step videos which guide you through the process if you have no experience.</p>\n<p style=\"text-align: justify;\">The great thing about building a computer is, that you know everything that there is to know about building a computer when you did it once, because all computer are built in the very same way – so building a computer will become a life skill that you will be able to apply again and again. So no reason to hold back!</p>\n<h2><strong>Conclusion / TL;DR</strong></h2>\n<p><strong>GPU</strong>: RTX 2070 or RTX 2080 Ti. GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are good too!<br /><strong>CPU</strong>: 1-2 cores per GPU depending how you preprocess data. > 2GHz; CPU should support the number of GPUs that you want to run. PCIe lanes do not matter.</p>\n<p><strong>RAM</strong>:<br />&#8211; Clock rates do not matter — buy the cheapest RAM.<br />&#8211; Buy at least as much CPU RAM to match the RAM of your largest GPU.<br />&#8211; Buy more RAM only when needed.<br />&#8211; More RAM can be useful if you frequently work with large datasets.</p>\n<p><strong>Hard drive/SSD</strong>:<br />&#8211; Hard drive for data (>= 3TB)<br />&#8211; Use SSD for comfort and preprocessing small datasets.</p>\n<p><strong>PSU</strong>:<br />&#8211; Add up watts of GPUs + CPU. Then multiply the total by 110% for required Wattage.<br />&#8211; Get a high efficiency rating if you use a multiple GPUs.<br />&#8211; Make sure the PSU has enough PCIe connectors (6+8pins)</p>\n<p><strong>Cooling</strong>:<br />&#8211; CPU: get standard CPU cooler or all-in-one (AIO) water cooling solution<br />&#8211; GPU:<br />&#8211; Use air cooling<br />&#8211; Get GPUs with &#8220;blower-style&#8221; fans if you buy multiple GPUs<br />&#8211; Set coolbits flag in your Xorg config to control fan speeds</p>\n<p><strong>Motherboard</strong>:<br />&#8211; Get as many PCIe slots as you need for your (future) GPUs (one GPU takes two slots; max 4 GPUs per system)</p>\n<p><strong>Monitors</strong>:<br />&#8211; An additional monitor might make you more productive than an additional GPU.</p>\n<p>Update 2018-12-14: Reworked entire blog post with up-to-date recommendations.<br />Update 2015-04-22: Removed recommendation for GTX 580</p><p>The post <a rel=\"nofollow\" href=\"https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/\">A Full Hardware Guide to Deep Learning</a> appeared first on <a rel=\"nofollow\" href=\"https://timdettmers.com\">Tim Dettmers</a>.</p>\n",
  "wfw:commentRss": "https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/feed/",
  "slash:comments": 945,
  "post-id": 121
}