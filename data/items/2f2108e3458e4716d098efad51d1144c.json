{
  "title": "Yet another (not) winning solution: Kaggle Flavours of Physics for finding Ï„ â†’ 3Î¼",
  "link": "https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/",
  "comments": "https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/#respond",
  "dc:creator": "phunterlau",
  "pubDate": "Mon, 26 Oct 2015 09:36:31 +0000",
  "category": "Uncategorized",
  "guid": "http://no2147483647.wordpress.com/?p=94",
  "description": "TL,DR: this blog describes feature engineering and models without implicitly/explicitly using tau invariant mass. This blog is for describing the &#8220;Hi from CMS&#8221; team solution of the Kaggle Flavours of Physics competition.Â It has the public score of 0.988420 and the private score of 0.989161 which has ranked at 29th. I thought I didn&#8217;t needÂ to publish [&#8230;]",
  "content:encoded": "<p>TL,DR: this blog describes feature engineering and models without implicitly/explicitly using tau invariant mass.</p>\n<p>This blog is for describing the &#8220;Hi from CMS&#8221; team solution of the <a href=\"https://www.kaggle.com/c/flavours-of-physics\">Kaggle Flavours of Physics competition</a>.Â It has the public score of 0.988420 and the private score of 0.989161 which has ranked at 29th. I thought I didn&#8217;t needÂ to publish this &#8216;not winning&#8217; solution, but I found allÂ recent published top solutions heavily depended on invariant mass reconstruction in simulation (<a href=\"https://www.kaggle.com/c/flavours-of-physics/forums/t/17142/first-place-solution\">first place</a>, <a href=\"https://www.kaggle.com/c/flavours-of-physics/forums/t/17062/second-best-solution\">second place</a>), so I decided to <strong>postÂ some discussion here about more &#8216;physical sounds&#8217; features and models</strong>, plus some other work.</p>\n<p>The github link to all code is available <a href=\"https://github.com/phunterlau/kaggle-tau-flavour\">here</a>. TheÂ simplified version was also put on Kaggle script <a href=\"https://www.kaggle.com/phunter/flavours-of-physics/gridsearchcv-with-feature-in-xgboost/code\">here</a>Â during the competition (thanks for many forks). All code wereÂ credited to the team members ofÂ &#8220;Hi from CMS&#8221;: <a href=\"https://www.kaggle.com/phunter\">phunter</a>, <a href=\"https://www.kaggle.com/users/106466/dlop78\">dlp78</a>, <a href=\"https://www.kaggle.com/michaelbroughton\">Michael Broughton</a> and <a href=\"https://www.kaggle.com/xiaozhouwang\">littleboat</a>.</p>\n<p>Unlike the last competition of Higgs search where a single xgboost classifier was used from one year ago, this solution explored more on: physical sounds feature engineering, a specific neural network model, as well as the model ensemble method for Random Forest, xgboost, UGradient and neural network,Â with a (fancy) automatic model selection under the agreement test constraint.</p>\n<p><strong>1. Understanding the constraints: KS test, CvM test, invariant mass and SPDhits</strong></p>\n<p>This competition hadÂ two tests: agreement test and correlation test, which required the classifier beÂ some robust against not-well simulated variable like SPDhits and independence to the tau invariant mass. The general ideas was not usingÂ SPDhits orÂ tau invariant mass, but for different reasons.</p>\n<p><strong>SPDhits: not well simulated</strong></p>\n<p>SPDhits wasÂ the number of hits in the SPD calorimeter, and this variable wasÂ not well simulated in the training data because of the limited understanding of LHCb SPD calorimeter and LHC bunch intensity. In the real life analysis, SPDhits variable needed additional calibration for simulation vs collision data, and the SPDhits distribution in the control samples looked very different from that in the simulation. For passing KS test, SPDhits feature wasÂ <strong>not</strong> suggested.</p>\n<p>A tricky but not-used feature engineering for &#8216;<strong>weakening SPDhits</strong>&#8216; wasÂ binning SPDhits: a vector of feature of SPDhits on if SPDhits<10, <20, <30, &#8230; , <600 etc. Some experiment showed bins up to Â <150 could effectively help on up to +0.0001 AUC score without hurting the KS score, but it expandedÂ the feature spaces too much and caused some confusions to the classifier, so I decided <strong>not</strong> to use these features, althoughÂ it had some physics meanings that LHCb analysis with SPDhits were usually binned with SPDhits<100, SPDhits<200 etc for different track multiplicity.</p>\n<p>In the later model selection for the final submission, a combination of models with and without SPDhits were used for an extra +0.0005 AUC score boost (discussed later in this blog). In my opinion, this ensemble was fine for physical sound because SPDhits calibration should be considered as systematic errors.</p>\n<p><strong>Invariant mass: it is not a good idea for physical sounds.</strong></p>\n<p>Tau invariant mass couldÂ be (easily) reconstructed using energy conservation and basic kinematics provided in the dataset, and may winning solutions were using it, which worried me. The detailed discussion on tau invariant mass wasÂ here (<a href=\"https://www.kaggle.com/vicensgaitan/flavours-of-physics/why-is-so-hard-to-learn-the-tau-mass\">Kaggle forum link</a>).</p>\n<p>My thought on tau invariant mass and correlation test wasÂ that: it wasÂ not a good idea of reconstructing tau invariant mass from simulation and having this correlation test. In the real life analysis, a robust classifier for search for particles should not depend on the parent particle mass much:</p>\n<ol>\n<li>if this particle mass was unknown like the Higgs boson, there was anÂ <a href=\"https://en.wikipedia.org/wiki/Look-elsewhere_effect\">look-elsewhere effect</a> where the invariant mass couldÂ bias the analysis;</li>\n<li>if this particle mass was known like the tau particle, the simulation with detector was not perfect thus the mass reconstruction was too good to be true.</li>\n</ol>\n<p>The real life <strong>M_3body</strong> was reconstructed <a href=\"https://muon.wordpress.com/2014/10/04/lhcb-searches-for-lfv-in-tau-decays/\">usingÂ likelihood methods</a>, but repeating this method for this competition was impossible with the provided dataset. Using reconstructed tau invariant mass in the classifier was strongly biased to the simulation, and feature engineering with the tau invariant mass in simulation would probably have artificial dependence on the mass and mightÂ make the classifier useless in the real life analysis.</p>\n<p>Surely, models with and without invariant mass couldÂ be ensembled for balancing CvM score, but I wasÂ afraid it wasÂ against the original idea of physical sounds, because the bias of invariant mass wasÂ physical, instead of systematic errors.</p>\n<p>Using invariant mass feature was <strong>not a goodÂ idea</strong>Â in my opinion, so I had the followingÂ feature engineering without invariant mass. All these following feature engineering hadÂ very small CvM scoreÂ which meant they had little dependency on the tau invariant mass, and the model didn&#8217;t create artificial dependency on mass.</p>\n<p><strong>2. Physical sounds feature engineering: what are good and what are bad.</strong></p>\n<p><strong>Kinematic features.</strong></p>\n<p>It is a 3-body decay where Ds and tau has similar invariant mass, thus tau is almost at rest in the Ds rest reference frame, soÂ the 3 children particles are almost symmetric in the Ds rest frame, thus individual particle&#8217;s kinematic feature may not be important, but some separations can be seen by pair-wise features. However, unfortunately we didn&#8217;t have phi and charge info for each particle, thus pair-wise mass reconstruction was not available, nor background veto by mu-mu mass <img src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png\" alt=\"ðŸ˜¦\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n<p>Since tau is boosted andÂ Ds and tau has similar invariant mass, tau&#8217;s IP and open angle (dira) should be small as expected, and tau has its lifetime thus it could fly for some distance. In the HEP analysis, usually IP significance was used, and fortunately it could be calculated as `flight_dist_sig` by dividing FlightDistanceÂ by FlightDistanceError.</p>\n<p>The distance from PV to the secondary vertex could be approximately calculated too by IP/(1-dira) where dira was the cosine value of tau open angle, and (1-dira) was the approximation when the open angle was very small (phi = sin(phi) = tan(phi) if phi is very small). A trick was that, dira was almost 1, thus a very small float number was added, and the feature was <a href=\"https://www.kaggle.com/c/higgs-boson/forums/t/9576/reducing-the-feature-space\">inv-log transformed</a>.</p>\n<p><a href=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png\"><img data-attachment-id=\"112\" data-permalink=\"https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/pn_distance_sec_vtx2/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png\" data-orig-size=\"720,432\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"pn_distance_sec_vtx2\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png?w=720\" class=\"aligncenter size-full wp-image-112\" src=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png?w=1008\" alt=\"pn_distance_sec_vtx2\" srcset=\"https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png 720w, https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/10/pn_distance_sec_vtx2.png?w=300 300w\" sizes=\"(max-width: 720px) 100vw, 720px\"   /></a></p>\n<p>Some features fromÂ <a href=\"https://www.kaggle.com/stepobr\">Stepan Obraztsov</a> (another CMS physicist, yeah!) were also included in <a href=\"https://www.kaggle.com/phunter/flavours-of-physics/gridsearchcv-with-feature-in-xgboost/discussion\">my public script</a>.Â They were:</p>\n<ol>\n<li><strong>NEW_FD_SUMP</strong> which was FlightDistance divided by the summation of daughter particle momenta. It is considered as the total flight time in the lab reference frame.</li>\n<li><strong>NEW5_lt</strong> which was Tau lifetime times the averaged daughterÂ particle IP. It could be interesting to understand the effect of this feature.</li>\n<li><strong>p_track_Chi2Dof_MAX</strong> which was the maximum of track chi2 quality. It is as in the selection features about the track quality.</li>\n</ol>\n<p><strong>Track/Vertex selection features.</strong></p>\n<p>The primary reason of using selection features was for distinguishing from the Ds-> eta mu nu background: eta could fly for a short distance and decay to 2 muons while tau-> 3 muons decay was immediate, which made the reconstructed Ds 3 muon vertex not as good as tau-> 3 muons. Thus, the track quality and vertex reconstruction quality (VertexChi2) should be used.</p>\n<p>CDF, DCAÂ features wereÂ otherÂ selection features from some TMVA analyses, but xgboost and other models didn&#8217;t nicely picked them up and the feature importance for these individual features were minimal, which meant they needed to be dropped (aka feature selection), otherwise they would confuse xgboost-like decision tree classifiers. After dropping them out, xgboost improvedÂ significantly.</p>\n<p>However, there was some hope: this LHCb thesis about lepton flavour violation search (<a href=\"https://cds.cern.ch/record/2002363?ln=en\">LINK</a>) provided some good ideas about using these selection features, and a good one from this thesis was, max and min values of these selection features could help classification. It was understandable that, if one of these 3 tracks hadÂ bad or good quality, the pair-wise and 3 body reconstruction couldÂ be heavily effected.</p>\n<p><strong>An additional list</strong> of good selection features are (approximately +0.0003 AUC) :</p>\n<ol>\n<li>MinÂ value of IsoBDT</li>\n<li>Max value of DCA</li>\n<li>Min value of isolation from a to f (which are pair-wise track isolation of the 3 tracks)</li>\n<li>Max value of track chi2 quality.</li>\n<li>Square summation of track chi2 quality: surprisedly good feature, maybe the tau vertex selection needed to consider multiple track quality instead of the max.</li>\n</ol>\n<p><strong>Pair-wise track IP feature</strong></p>\n<p>Although there was no phi nor charge informationÂ of each daughter track for pair-wise parent particle reconstruction, I hadÂ two features which improved the score and had good feature importance: The ratio ofÂ <strong>Tau IP vs p0p2 pair IP </strong>and<strong>Â <strong>Tau IP vsÂ </strong>p1p2 pair IP</strong>.</p>\n<p>It wasÂ interestingÂ Â of understanding this: the signal process wasÂ tau -> Z&#8217; mu and Z&#8217;->2 muon immediately, while the background was Ds->eta mu nu where a small missing energy nu was present, thus the pair-wise IP from the signal was surelyÂ large by comparing with tau IP, but the missing energy from background confusedÂ this comparison. The in-depth reason might need more information about the process. In the experiment, these two ratio values gave good separation and good feature importance.</p>\n<p><a href=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png\"><img data-attachment-id=\"110\" data-permalink=\"https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/select_p1p2_ip_ratio/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png\" data-orig-size=\"720,432\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"select_p1p2_ip_ratio\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png?w=720\" class=\"aligncenter size-full wp-image-110\" src=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png?w=1008\" alt=\"select_p1p2_ip_ratio\" srcset=\"https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png 720w, https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/10/select_p1p2_ip_ratio.png?w=300 300w\" sizes=\"(max-width: 720px) 100vw, 720px\"   /></a> <a href=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png\"><img data-attachment-id=\"111\" data-permalink=\"https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/select_p0p2_ip_ratio/#main\" data-orig-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png\" data-orig-size=\"720,432\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"select_p0p2_ip_ratio\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png?w=300\" data-large-file=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png?w=720\" class=\"aligncenter size-full wp-image-111\" src=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png?w=1008\" alt=\"select_p0p2_ip_ratio\" srcset=\"https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png 720w, https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png?w=150 150w, https://no2147483647.files.wordpress.com/2015/10/select_p0p2_ip_ratio.png?w=300 300w\" sizes=\"(max-width: 720px) 100vw, 720px\"   /></a></p>\n<p><strong>Other experimented but not used features:</strong></p>\n<ol>\n<li>IP error of each particle.</li>\n<li>Pz of each particle plus combinations.</li>\n<li>Pair-wise open angle in eta.</li>\n</ol>\n<p><strong>Feature selection using trees</strong></p>\n<p>Decision tree classifiers didn&#8217;tÂ like confusing features, e.g. some features withÂ very small separation for the positive and negative, so feature selection was needed. I only had time for aÂ very simple feature selection by giving all these features to a random forest classifier for optimizing AUC, and dropped features with low feature importance. This tree-based selection gave consistent results as from human mind: individual momentum feature and individual quality selection features were dropped. It wasÂ also interesting that, <strong>isolationb</strong> and <strong>isolationc</strong> were more lessÂ important than the other 4 pair-wise track isolation variables. Keeping onlyÂ the other 4 track isolation variables helped AUC, the reason behind this may need some in-depth analysis.</p>\n<p><strong>3. The neural network model</strong></p>\n<p>More details to be updatedÂ here from Michael. Michael had a two-layer neural network using all these features, and the code was self-explained, please check the code.</p>\n<p>The final model includes this neural network model with SPDhits. The model itself with SPDhits had very lowÂ CvM score but disaster KS score (almost 0.3 and surely didn&#8217;t pass the KS test <0.09), but ensemble this model with other conservative models with a very low weight gave +0.0005 AUC score without failure in KS test.</p>\n<p><strong>4. &#8220;Weakening&#8221; decision tree models under the constraints</strong></p>\n<p>With good features, models have easier jobs. UGrad, Random Forest classifiers have good performance with these feature plus GridSearch for parameters, and their KS scoresÂ wereÂ around 0.06 which couldÂ easily pass. Since no invariant mass features were used, CvM score was round 0.0008 which had no problem either.</p>\n<p>Xgboost is an aggressive and strong classifer, where it mightÂ give bad KS test score even without SPDhits features, e.g. 0.12, so weakening this model wasÂ needed for passing KS test. There were two tricks in xgboost parameters: large <strong>eta</strong> value for controlling convergence, and largeÂ <strong>colsample_bytree</strong> value plus small <strong>subsample</strong> by adding randomness, as mentioned in the <a href=\"http://xgboost.readthedocs.org/en/latest/param_tuning.html\">xgboost document</a>. The good combination was about:</p>\n<ul>\n<li>eta = 0.2</li>\n<li>colsample_bytree=13</li>\n<li>subsample = 0.7</li>\n</ul>\n<p><strong>5. CV-based ensemble and automatic model selection</strong></p>\n<p>Littleboat had a cool randomized automatic model selection by the CV score. The idea was that, each model had some variance, thus the best ensemble could be summing up similar models with small parameter changes, e.g. different seeds, slightly different eta etc. The CV score was used for the ensemble: most of the time, CV scores were more reliable than the public leaderboard scores.</p>\n<p>Each model had two versions: high score (with SPDhits) and low score (all other features, no SPDhits). The same parameters are used for both two versions of each model, and surely the high score one doesn&#8217;t pass KS test for most time.Â After generating a list of model candidates, the script randomly pickup some models with good CV scores, searched for best weight to average them and evaluated the final CV score, Â as well as the KS score and CvM score.Â The ensemble method was weighted average, and the weight was brute-force searched after selecting models for passing KS test.Â If the result could pass the tests, it was saved as an candidate for submission.</p>\n<p>After these selection, a manual process of weighted averaging with theÂ neural network (with SPDhits) model was used. The weight was around 0.04 for the neural network model, which helped the final score by +0.0005 AUC.Â The final two submissions were weighted average ofÂ Â XgboostÂ andÂ UGrad, plus a small weight of neural network with SPDhits.</p>\n<p><strong>6. Lesson learned</strong></p>\n<ol>\n<li>Kaggle competition really needs time. Thanks to my wife for her support.</li>\n<li>Team-up is great: I know some amazing data scientists from this competition and have learned much from them.</li>\n<li>Ensemble is needed, but it is not everything: feature engineering firstly, CV for each model, and ensemble them.</li>\n<li>Data leakage: every kaggle competition has this risk, and dealing with itÂ is a good lesson for both admins and competitors. Exploiting it for winning? blaming it? having fun without using it? Any of them is OK, no one is perfect and no one can hide from statistics.</li>\n<li>When in doubt, use <a href=\"https://github.com/dmlc/xgboost\">xgboost</a>! Do you know xgboost can be installed by &#8220;pip install xgboost&#8221;. Submit an issue if you can&#8217;t install it.</li>\n</ol>\n<p><strong>7. One more thing for HEP: UGrad and xgboost</strong></p>\n<p>As learned from other high rank solution from the forum, UGrad itself was <a href=\"https://www.kaggle.com/c/flavours-of-physics/forums/t/17008/sharing-code-ugbc\">a strong classifier</a>Â withÂ BinFlatnessLossFunction,Â and a simple ensemble of UGrad models could easily go 0.99+. But UGrad usedÂ single thread and was very slow, so I didn&#8217;tÂ experiment more aboutÂ it.Â I have submitted <a href=\"https://github.com/dmlc/xgboost/issues/550\">a wish-list issue </a>on xgboost github repoÂ for implementing a xgboost loss function which can takeÂ BinFlatnessLossFunction as in UGrad and using multi-thread xgboost for speed up, so HEP people can be have both BinFlatnessLossFunction and a fast gradient boosting classifier. It is do-able as Tianqi (author of xgboost) has described in the issue, and hope I (or someone) can find a time working on it. I hope it can help HEP research.</p>\n<p><strong>Advertisement time</strong>: <a href=\"http://dmlc.ml/\">DMLC</a> is a great toolbox for machine learning. xgboost is part of it, and DMLC has a recent new tool for deep learning <a href=\"https://github.com/dmlc/mxnet\">MXnet</a>:Â it has <a href=\"http://mxnet.readthedocs.org/en/latest/#open-source-design-notes\">nice design</a>, it can implement LSTM in 25 lines of python code, and it can train ImageNet on 4 GTX 980 card in 8.5 days, go star it!</p>\n",
  "wfw:commentRss": "https://no2147483647.wordpress.com/2015/10/26/yet-another-not-winning-solution-kaggle-flavours-of-physics-for-finding-%cf%84-%e2%86%92-3%ce%bc/feed/",
  "slash:comments": 0,
  "media:content": [
    {
      "media:title": "phunterlau"
    },
    {
      "media:title": "pn_distance_sec_vtx2"
    },
    {
      "media:title": "select_p1p2_ip_ratio"
    },
    {
      "media:title": "select_p0p2_ip_ratio"
    }
  ]
}