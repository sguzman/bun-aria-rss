{
  "title": "Towards practicing differential privacy",
  "description": "<p>More than a year ago I wrote an article with the provocative title: <a href=\"http://blog.mrtz.org/2013/08/21/dp-practical.html\">Is\nDifferential Privacy practical?</a>\nThe post was essentially one big buildup for\nan epic follow-up post that I simply never wrote. Since then dozens\nhave asked me for an answer to this urgent question. Recently, after the post\nhit the front page of Hacker News, half a dozen emails\ninquired about the follow-up post that I had promised. Some <a href=\"https://news.ycombinator.com/item?id=9184479\">speculated</a> that owing to <a href=\"http://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines\">Betteridge’s law of\nheadlines</a>, the\nanswer was simply <em>no</em>.</p>\n\n<p>Despite my venerable history of failing on various commitments and my apparent\npeace with it, this situation went too far even by my own low standards.\nSo, I decided to write a not so epic version of that promised blog\npost.</p>\n\n<h2 id=\"the-california-public-utilities-commission\">The California Public Utilities Commission</h2>\n\n<p>I’ll arrange my thoughts around the case of the <a href=\"http://www.cpuc.ca.gov/puc/\">California Public\nUtilities Commission</a> (CPUC). The CPUC is a\nregulatory agency that regulates privately owned public utilities in\nCalifornia. In recent years there has been political pressure on the utilities\nto give third parties access to smart meter data. As discussed in <a href=\"http://blog.mrtz.org/2013/08/21/dp-practical.html\">my previous\npost</a>, smart meter data is\nof enormous value to many, but comes with serious privacy challenges.</p>\n\n<p>To settle these issues the CPUC organized a major legal proceeding with the\ngoal of creating rules that provide access to energy usage data to local\ngovernment entities, researchers, and state and federal agencies while\nestablishing procedures that protect the privacy of consumer data.</p>\n\n<p>I served as a privacy expert within the proceeding together with Cynthia Dwork, Lee Tien from the\n<a href=\"http://www.eff.org\">EFF</a>, and Jennifer Urban and her team from Berkeley.\nOur goal was to inform various parties about the pitfalls\nof insufficient privacy mechanisms and to propose better ones. Our proposed\nsolution focused on differential privacy for the uses cases in which it made\nsense. There were a number of use cases that the CPUC considered. Not all of\nthem were well suited for differential privacy to begin with.</p>\n\n<h3 id=\"a-proposed-decision\">A proposed decision</h3>\n\n<p>My involvement with the case ended in 2014 after a <a href=\"http://docs.cpuc.ca.gov/PublishedDocs/Efile/G000/M088/K947/88947979.PDF\">proposed\ndecision</a>\nof the administrative judge. To summarize a 120 page document in one sentence,\nthe ruling did not endorse differential privacy strongly enough for me to further pursue the case actively.\nNevertheless, there was still significant interest in differential privacy\nfrom some of the utilities. I believe that one utilities company\nengaged with Microsoft with the goal of building a prototype of a\ndifferentially private solution for their data sharing needs.</p>\n\n<p>The ruling was disappointing from my perspective in that it did not advocate\nthe use of differential privacy in any of the use cases. Meanwhile it shot\ndown several uses cases essentially not giving the use\ncase sponsors meaningful access to energy data at all. In those cases\ndifferential privacy could’ve provided an obviously better trade-off for everyone.</p>\n\n<p>The ruling didn’t so much reflect a technical verdict about differential privacy. Rather it reflected our inability to successfully anticipate and maneuver the highly complex political and legal environment in which the decision was made.</p>\n\n<h3 id=\"a-post-mortem\">A post mortem</h3>\n\n<p>Our proposal based on differential privacy initially met with resoundingly positive\nresponses when we first presented it to the administrative judge and various\nparties present in the meeting. We did however face bitter opposition from a\ngroup of researchers who sponsored one use case. Those researchers, who had\nbeen working with raw smart meter data in the past, were worried that differential privacy\nwould create an obstacle for them. We quickly realized that it would be\ndifficult to agree with them on the extent to which their research practices\nare compatible with differential privacy. So, we specifically excluded their\nuse case from the scope of our proposal focusing on some of the remaining use cases instead. This\ndidn’t stop the researchers from lobbying relentlessly against differential\nprivacy. In particular, they filed a last minute comment in which they\nattacked differential privacy sharply based on many profound factual misunderstandings\nof the privacy notion. Due to the perfect timing of their comment, we were\nunable to submit a rebuttal. In the end, I believe this alone was enough for the\nadministrative judge to conclude that the use of differential privacy was at\npresent too controversial to be proposed as a solution in the ruling.</p>\n\n<p>My point is not to criticize this group of researchers. I’m sympathetic with\nthem. They’ve been working with energy data for many years. They’re doing important work which is probably already difficult enough as it is. We respected their\nposition and did not want to interfere with their research. My guess\nis that their research practices are actually largely consistent with what’s\npossible under differential privacy, but that’s an entirely separate\ndiscussion.</p>\n\n<p>What’s tragic is that their opposition ended up hurting a consumer advocacy\ngroup who could’ve used differential privacy as a means to gain <em>more</em> access to\nenergy data than they were able to get in the end (essentially nothing). There was a lot of miscommunication throughout the proceeding that clearly didn’t help. For instance, initially the consumer advocacy group proposed their own ad-hoc privacy solution (which we didn’t support). Only later did we find some common ground. In hindsight, we should’ve agreed on and jointly represented the same solution from the beginning. In my understanding, the use case didn’t require more than the kind of aggregate usage statistics that we could’ve easily produced while preserving differential privacy without any major engineering efforts.</p>\n\n<h2 id=\"towards-practicing-differential-privacy\">Towards practicing differential privacy</h2>\n\n<p>Drawing on my experience with the CPUC case, I want to end with some concrete\nsuggestions and questions hoping that they will help others when applying\ndifferential privacy. When I speak of “the community”, I will make some very broad generalizations knowing full well that in each instance there are certainly exceptions to what I claim. The discussion below is by no means a survey as it contains very few links to the rich literature on differential privacy. I strongly encourage you to fill in relevant missing pointers in the comments.</p>\n\n<h3 id=\"focus-on-win-win-applications\">Focus on win-win applications</h3>\n\n<p>Apply differential privacy as a tool to provide access to data where\ncurrently access is problematic due to privacy regulations. Don’t fight the\ndata analyst. Don’t play the moral police. Imagine you are the analyst.</p>\n\n<p>As a privacy expert,\nyou will find yourself having to shoot down inadequate solutions all the time.\nWhy can’t we just omit those 18 sensitive attributes like in the HIPAA safe harbor provision?\nWhy isn’t it safe to release any statistic that is aggregated over at least 15 households in which no\nsingle household contributes 15% of the total number (i.e., the “15/15” rule)?\nSuch ad-hoc rules sound intuitively appealing to non-experts. Refuting them is time-consuming and makes\nyou look defensive.</p>\n\n<p>Rather than shooting down what doesn’t work, point out why differential privacy is better than those solutions not just from a privacy perspective but rather from a <em>utility</em> perspective. Unlike these solutions,\ndifferential privacy does not alter your data set at all. In particular, from a statistical perspective\nyou do not change the distribution from which the data were drawn. This is an incredibly powerful\nproposition. I think that data analysis with differential privacy can be vastly more useful than\nwhat you get after applying, for instance, the HIPAA safe harbor mechanism.</p>\n\n<p>My point is that there are many “win/win” applications of differential privacy where it simultaneously can give better utility and better privacy than its alternatives. As the CPUC case showed, sometimes the choice is even between no access at all and differentially private access. It’s really a no-brainer. We should start with such applications instead of arguing about completely unrestricted access versus differentially private access.</p>\n\n<h3 id=\"dont-empower-the-naysayers\">Don’t empower the naysayers</h3>\n\n<p>In my opinion, for differential privacy to be a major success in practice it would be sufficient if it were  successful in <em>some</em> applications but certainly not in <em>all</em>—not even in most. There’s a culture of criticizing differential privacy based on the perfectly correct observation that some differentially private algorithm (say, Laplace) didn’t give enough utility in some application. These kind of observations—valid as they may be—say very little about the potential of differential privacy in practice. First of all, they only evaluate one algorithm while there could be much better algorithms. Second, they commit to one specific application and, more importantly, one particular modeling of the problem. Perhaps there’s a different approach to the same problem that’s more compatible with differential privacy. It’s simply impossible to rule out differential privacy as a solution through these kind of straw man experiments.</p>\n\n<p>The differential privacy community is partially at blame for empowering the naysayers, since they have advertised differential privacy as a <em>universal</em> solution concept to the privacy problem. This is theoretically true in some sense, but the situation in practice is much more delicate. So, stop feeding the naysayers. Start presenting differential privacy as a promising technology for <em>some</em> applications but certainly not <em>all</em>.</p>\n\n<h3 id=\"change-your-narrative\">Change your narrative</h3>\n\n<p>Don’t present differential privacy as a fear inducing crypto hammer\ndesigned to obfuscate data access. That’s not what it is.\nDifferential privacy is a rigorous way of doing machine learning, not a way of\npreventing machine learning from being done. We understand perfectly well now\nthat differential privacy is a stability guarantee which is fundamentally\naligned with the central goal of statistics, namely, to learn from data about the population\nas a whole and not about specific individuals. This understanding perhaps wasn’t quite there\nin the beginning, but it is now. Academics should from time to time come up\nwith a new page 1 for their papers.</p>\n\n<h3 id=\"build-reliable-code-repositories\">Build reliable code repositories</h3>\n\n<p>A  weakness of the differential privacy community has been the scarcity of\navailable high quality code. There are many academic code pieces available\nby emailing someone, but we don’t have many visible repositories on github\nor elsewhere that provide robust implementations of common differentially\nprivate algorithms. Frank McSherry’s <a href=\"http://research.microsoft.com/en-us/projects/pinq/\">PINQ</a> was a really wonderful step in the right direction,\nbut it is no longer maintained and by now out of date. Written in C#, it hasn’t been easy for many to build on and extend PINQ. A more recent notable effort is the <a href=\"https://github.com/ejgallego/dualquery\">Dual Query</a> code though it requires CPLEX to run.</p>\n\n<p>What scares me a bit is that even a project as solidly designed and carefully executed as PINQ\ndid not address low-level implementation issues such as <a href=\"http://dl.acm.org/citation.cfm?id=2382264\">floating point vulnerabilities</a> in differential privacy.</p>\n\n<p>I’m guilty myself. Many have used or tried to use <a href=\"http://papers.nips.cc/paper/4548-a-simple-and-practical-algorithm-for-differentially-private-data-release.pdf\">MWEM</a>, an algorithm Katrina Ligett, Frank McSherry and I presented at NIPS a few years ago. Yet we don’t have a great implementation publicly available. You can email us for a decent C# implementation (alas!), but instead a lot of people have\nproduced their own implementations of our algorithm over the years. I regularly have the urge to start an open source project for it, but then I realize it’s a bit of a bottomless pit. In order to have a solid implementation of MWEM, I’d first need to have a solid implementation of all the primitives with all the low-level issues that come up. In any case, if somebody more brave then myself took the first step on an open source effort (preferably not in C#), I’d be very eager to contribute.</p>\n\n<p>Taking a more modest step, I feel compelled to compile a list of available code repositories.\nIf you have any pointers, please leave a comment!</p>\n\n<h3 id=\"be-less-general-and-more-domain-specific\">Be less general and more domain-specific</h3>\n\n<p>Much of the academic research on differential privacy has focused on generality. That makes sense theoretically, but it means that reading the scientific literature on differential privacy from the point of\nview of a domain expert can be very frustrating. Most papers start with toy\nexamples that make perfect sense on a theoretical level, but will appear\nalarmingly naïve to a domain expert.</p>\n\n<p>The community is at a point where we need to transition <em>from generality to specificity</em>.\nFor example, what’s needed are domain-specific tutorials\nthat walk practitioners through real examples. One reason why such\ntutorials don’t exist is that they take a lot of time and writing them isn’t\nincentivized by academia. One way out of this is for journal editors and\nconferences to specifically invite such tutorials. Similarly, the community should at this point have very high regard for positive results and case studies in specific application domains even if they are limited in scope and don’t contribute technically new solutions.</p>\n\n<h3 id=\"be-more-entrepreneurial\">Be more entrepreneurial</h3>\n\n<p>The CPUC case highlighted that the application of differential privacy in\npractice can fail as a result of many non-technical issues. These important\nissues are often not on the radar of academic researchers. We spent an awful\nlot of time talking about the technical strengths or limitations of\ndifferential privacy, while missing out on some very real challenges. It’s quite reasonable to argue\nthat these challenges  should be outside the scope of academia. On the other hand, academics are currently the\nonly available experts on differential privacy and there’s obvious demand for it.\nWhere should we draw the line?</p>\n\n<p>To be blunt, I think an important ingredient that’s missing in the current differential\nprivacy ecosystem is <em>money</em>. There is only so much that academic researchers can do to promote a technology.\nBeyond a certain point businesses have to commercialize the technology for it be successful. The CPUC\ncase was much better suited as the full-time job for a group of paid professionals\nrather than a volunteering effort. I’m surprised none of the researchers working on differential privacy\nhave devoted a sabbatical to running a privacy startup. It’s needed and the potential upside is big.\nWhy not give it a shot? I hear tenured jobs are meant for running startups.</p>\n\n<h3 id=\"so-is-differential-privacy-practical\">So, is differential privacy practical?</h3>\n\n<p>I like the answer Aaron Roth gave when I asked him:</p>\n\n<div style=\"text-align:center;\">\n<em>It's within striking distance.</em>\n</div>",
  "pubDate": "Fri, 13 Mar 2015 22:30:00 +0000",
  "link": "http://blog.mrtz.org/2015/03/13/practicing-differential-privacy.html",
  "guid": "http://blog.mrtz.org/2015/03/13/practicing-differential-privacy.html",
  "category": [
    "tcs",
    "differential privacy",
    "practice"
  ]
}