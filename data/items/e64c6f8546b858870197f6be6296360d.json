{
  "title": "Stochastic Computation Graphs: Discrete Relaxations",
  "link": "http://artem.sobolev.name/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html",
  "description": "<p>This is the second post of the <a href=\"/tags/stochastic%20computation%20graphs%20series.html\">stochastic computation graphs series</a>. Last time we discussed models with <a href=\"/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html\">continuous stochastic nodes</a>, for which there are powerful reparametrization technics.</p>\n<p>Unfortunately, these methods don’t work for discrete random variables. Moreover, it looks like there’s no way to backpropagate through discrete stochastic nodes, as there’s no infinitesimal change of random values when you infinitesimally perturb their parameters.</p>\n<p>In this post I’ll talk about continuous relaxations of discrete random variables.</p>\n<!--more-->\n<h2 id=\"asymptotic-reparametrization\">Asymptotic reparametrization</h2>\n<p>One way to train models with discrete random variables is to consider an equivalent model with continuous random variables. Let me show you an example. Suppose you have a feed-forward neural network for classification that receives <span class=\"math inline\">\\(x\\)</span> and outputs distribution over targets <span class=\"math inline\">\\(p(y \\mid x)\\)</span>, where typical layer looks like <span class=\"math inline\">\\(h_k = \\sigma(W_k h_{k-1} + b_k)\\)</span>. You’d like to apply dropout to each weight of this layer and <em>tune its probabilities</em>. To do so we introduce binary latent variables <span class=\"math inline\">\\(z^{(k)}_{ij}\\)</span> denoting if weight is on or off. There’s one such variable for each weight, so we can stack them into a matrix <span class=\"math inline\">\\(Z_k\\)</span> of the same shape as weight matrix <span class=\"math inline\">\\(W_k\\)</span>. Then element-wise multiplication <span class=\"math inline\">\\(W_k \\circ Z_k\\)</span> would zero out dropped weights, so the formula becomes <span class=\"math inline\">\\(h_k = \\sigma((W_k \\circ Z_k) h_{k-1} + b_k)\\)</span>. We assume each dropout mask independently follows Bernoulli distribution: <span class=\"math inline\">\\(z_{ij}^{(k)} \\sim \\text{Bernoulli}(p_{ij}^{(k)})\\)</span> (<span class=\"math inline\">\\(Z^{(k)} \\sim \\text{Bernoulli}(P^{(k)})\\)</span> for short)</p>\n<p>In order to learn these masks (or, rather parameters of the distribution <span class=\"math inline\">\\(q(Z \\mid \\Lambda)\\)</span> over masks parametrized by <span class=\"math inline\">\\(\\Lambda\\)</span>) we employ variational inference approach:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\log p(y|x) \\ge\n\\mathcal{L}(\\Lambda)\n&= \\mathbb{E}_{q(Z \\mid \\Lambda)} \\log \\frac{p(y, Z \\mid x)}{q(Z \\mid \\Lambda)} \\\\\n&= \\underbrace{\\mathbb{E}_{q(Z \\mid \\Lambda)} \\log p(y \\mid Z, x)}_{\\text{expected likelihood}} - \\underbrace{D_{KL}(q(Z \\mid \\Lambda) \\mid\\mid p(Z))}_{\\text{KL-divergence}}\n\\to \\max_\\Lambda\n\\end{align*}\n\\]</span></p>\n<p>We can’t backpropagate gradients through discrete sampling procedure, so we need to overcome this problem somehow. Notice, however, that each unit in a layer <span class=\"math inline\">\\(h_{k+1}\\)</span> is an affine transformation of <span class=\"math inline\">\\(k\\)</span>th layer’s nodes followed by a nonlinear activation function. If <span class=\"math inline\">\\(k\\)</span>th layer has sufficiently many neurons, then one might expect the <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\">Central Limit Theorem</a> to hold at least approximately for the preactivations. Namely, consider a single neuron <span class=\"math inline\">\\(s\\)</span> that takes an affine combination of previous layer’s neurons <span class=\"math inline\">\\(h_{k-1}\\)</span> and applies a nonlinearity: <span class=\"math inline\">\\(s = \\sigma(w^T h + b)\\)</span>. In our case, however, we have a vector of masks <span class=\"math inline\">\\(z \\sim \\text{Bernoulli}(P)\\)</span>, so the formula becomes <span class=\"math inline\">\\(s = \\sigma((w \\odot \\tfrac{z}{P})^T h + b)\\)</span> (<span class=\"math inline\">\\(\\odot\\)</span> stands for element-wise multiplication), and if <span class=\"math inline\">\\(K=\\text{dim}(z)\\)</span> is large enough, then we might expect the preactivations <span class=\"math inline\">\\(\\sum_{k=1}^K \\tfrac{z_k}{p_k} w_k h_k + b\\)</span> (we divide each weight by its keeping probability <span class=\"math inline\">\\(p_k\\)</span> to make keep the expectation unaffected by noise) to be approximately distributed as <span class=\"math inline\">\\(\\mathcal{N}\\left(w^T h + b, \\sum_{k=1}^K \\tfrac{1 - p_k}{p_k} w_k^2 h_k^2 \\right)\\)</span>.</p>\n<p>Now suppose that instead of Bernoulli multiplicative noise <span class=\"math inline\">\\(z\\)</span> we actually used multiplicative Gaussian noise <span class=\"math inline\">\\(\\zeta \\sim \\mathcal{N}(1, (1-P) / P)\\)</span> (element-wise division). It’s easy to check then that the preactivations <span class=\"math inline\">\\((w \\odot \\zeta)^T h + b\\)</span> would have the same Gaussian distribution with exactly the same parameters. Therefore, we can replace expected likelihood term in objective <span class=\"math inline\">\\(\\mathcal{L}(\\Lambda)\\)</span> with a continuous distribution <span class=\"math inline\">\\(q(\\zeta|\\Lambda) = \\prod_{i,j,k} \\mathcal{N}\\left(\\zeta^{(k)}_{ij} \\mid 1, (1-\\lambda^{(k)}_{ij})/\\lambda^{(k)}_{ij}\\right)\\)</span>. However, we can’t simply do the same in the KL divergence term. Instead, we need to use simple priors (like factorized Bernoulli) so that closed form can be computed – then we can take deterministic gradients w.r.t. <span class=\"math inline\">\\(\\Lambda\\)</span>.</p>\n<p>This example shows us that for some simple models collective behavior of discrete random variables can be accurately approximated by continuous equivalents. I’d call this approach the <strong>asymptotic reparametrization</strong> <a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a>.</p>\n<h2 id=\"naive-relaxation\">Naive Relaxation</h2>\n<p>The previous trick is nice and appealing, but has very limited scope of applicability. If you have just a few discrete random variables or have other issues preventing you from relying on the CLT, you’re out of luck.</p>\n<p>However, consider a binary discrete random variable: <span class=\"math inline\">\\(z \\sim \\text{Bernoulli}(p)\\)</span>. How would you sample it? Easy! Just sample a uniform r.v. <span class=\"math inline\">\\(u \\sim U[0,1]\\)</span> and see if it’s less than <span class=\"math inline\">\\(p\\)</span>: <span class=\"math inline\">\\(z = [u > q]\\)</span> where <span class=\"math inline\">\\(q = 1 - p\\)</span> and brackets denote an indicator function that is equal to one when the argument is True, and zero otherwise. Equivalently we can rewrite it as <span class=\"math inline\">\\(z = H(u - q)\\)</span> where <span class=\"math inline\">\\(H(x)\\)</span> is a step function: it’s zero for negative <span class=\"math inline\">\\(x\\)</span> and 1 for positive ones <a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>. Now, this is a nice-looking reparametrization, but <span class=\"math inline\">\\(H\\)</span> is not differentiable <a href=\"#fn3\" class=\"footnoteRef\" id=\"fnref3\"><sup>3</sup></a>, so you can’t backpropagate through it. What if we replace <span class=\"math inline\">\\(H\\)</span> with some differentiable analogue that has a similar shape? One candidate is a sigmoid with temperature: <span class=\"math inline\">\\(\\sigma_\\tau(x) = \\sigma\\left(\\tfrac{x}{\\tau}\\right)\\)</span>: by varying temperature you can control steepness of the function. In the limit of <span class=\"math inline\">\\(\\tau \\to 0\\)</span> we actually recover the step function <span class=\"math inline\">\\(\\lim_{\\tau \\to 0} \\sigma_\\tau(x) = H(x)\\)</span>.</p>\n<p>So the relaxation we’ll consider is <span class=\"math inline\">\\(\\zeta = \\sigma_\\tau(u - q)\\)</span>. How can we see if it’s a good one? What do we even want from the relaxation? Well, in the end we’ll be using the discrete version of the model, the one with zeros and ones, so we’d definitely like our relaxation to sample zeros and ones often. Actually, we’d even want them to be the modes of the underlying distribution. Let’s see if that’s the case for the proposed relaxation.</p>\n<p>CDF of the relaxed r.v. <span class=\"math inline\">\\(\\zeta\\)</span> <span class=\"math display\">\\[\n\\mathbb{P}(\\zeta < x) = \\mathbb{P}(u < q + \\tau \\sigma^{-1}(x)) = \\min(1, \\max(0, q + \\tau \\sigma^{-1}(x)))\n\\]</span> And the corresponding PDF <span class=\"math display\">\\[\n\\frac{\\partial}{\\partial x}\\mathbb{P}(\\zeta < x)\n=\n\\begin{cases}\n\\frac{\\tau}{x (1-x)}, & \\sigma\\left(\\frac{-q}{\\tau}\\right) < x < \\sigma\\left(\\frac{1-q}{\\tau}\\right) \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]</span></p>\n<p>Even the formula suggests that the support of the distribution of <span class=\"math inline\">\\(\\zeta\\)</span> is never the whole <span class=\"math inline\">\\((0, 1)\\)</span>, but only approaches it as temperature <span class=\"math inline\">\\(\\tau\\)</span> goes to zero. For all non-zero temperatures though the support will exclude some neighborhood of the endpoints which might bias the model towards the intermediate values. This is why you want to have a random variable with infinite support <a href=\"#fn4\" class=\"footnoteRef\" id=\"fnref4\"><sup>4</sup></a>. If the distribution is skewed, then the resulting relaxation will also be skewed, but it’s not a problem since the probabilities are adjusted according to the CDF.</p>\n<p>Let’s plot some densities for different <span class=\"math inline\">\\(\\tau\\)</span> (let <span class=\"math inline\">\\(q\\)</span> be 0.1).</p>\n<div class=\"post-image\">\n<p><img src=\"/files/naive-relaxation-densities.png\" style=\"max-width: 90%\" /></p>\n</div>\n<p>But having infinite support is not enough. It’s hard to see from plots, but if the distribution has very light tails (like Gaussian), then it’s effective support is still finite. Authors of the Concrete Distribution notice this in their paper, saying that sigmoid squashing rate is not enough to compensate (even if you twist the temperature!) for quickly decreasing Gaussian density as you approach either of infinities.</p>\n<p>Let’s also think about the impact of the temperature on the relaxation. Intuitively one would expect that as we decrease the temperature, the relaxation becomes more accurate and the problem becomes “more discrete”, hence it should be harder to optimize. Indeed, <span class=\"math inline\">\\(\\tfrac{d}{dx}\\sigma_\\tau(x) = \\tfrac{1}{\\tau} \\sigma_\\tau(x) \\sigma_\\tau(-x)\\)</span> – as you decrease the temperature, both sigmoids become more steep, and the derivative approaches an infinitely tall spike at 0 and zero everywhere else <a href=\"#fn5\" class=\"footnoteRef\" id=\"fnref5\"><sup>5</sup></a>.</p>\n<div class=\"post-image\">\n<p><img src=\"/files/naive-relaxation-variance-by-tau.png\" style=\"max-width: 90%\" /></p>\n</div>\n<p>As expected, higher approximation accuracy (obtained by lowering the temperature) comes at a cost of increased variance.</p>\n<h2 id=\"gumbel-softmax-relaxation-aka-concrete-distribution\">Gumbel-Softmax Relaxation (aka Concrete Distribution)</h2>\n<p>We could consider some other distributions (with larger support, like the gaussian one) instead of uniform in our relaxation, but let’s try a different approach. Let’s see how we can sample arbitrary <span class=\"math inline\">\\(K\\)</span>-valued discrete random variables. It’s well-known fact (the so called <a href=\"https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/\">Gumbel Max Trick</a>) that if <span class=\"math inline\">\\(\\gamma_k\\)</span> are i.i.d. <span class=\"math inline\">\\(\\text{Gumbel}(0, 1)\\)</span> random variables, then <span class=\"math inline\">\\(\\text{argmax}_k \\{\\gamma_k + \\log p_k\\} \\sim \\text{Categorical}(p_1, \\dots, p_K)\\)</span>, that is, probability that <span class=\"math inline\">\\(k\\)</span>th perturbed r.v. attains maximal value is exactly <span class=\"math inline\">\\(p_k\\)</span> <a href=\"#fn6\" class=\"footnoteRef\" id=\"fnref6\"><sup>6</sup></a>. This gives you a sampling procedure: just sample <span class=\"math inline\">\\(K\\)</span> independent Gumbels, add corresponding log probabilities, and then take the argmax. However, though mathematically elegant, this formula won’t help us much since argmax is not differentiable. Let’s relax it then! We have already seen that the step function <span class=\"math inline\">\\(H(x)\\)</span> can be seen as a limit of a sigmoid with temperature: <span class=\"math inline\">\\(H(X) = \\lim_{\\tau \\to 0} \\sigma_\\tau(x)\\)</span>, so we might expect (and indeed it is) that if you assume that <span class=\"math inline\">\\(\\text{argmax}(x)\\)</span> returns you a one-hot vector indicating the maximum index, it can be viewed as a zero-temperature version of a softmax with temperature: <span class=\"math inline\">\\(\\text{argmax}(x)_j = \\lim_{\\tau \\to 0} \\text{softmax}_\\tau(x)_j\\)</span> where</p>\n<p><span class=\"math display\">\\[\n\\text{softmax}_\\tau(x)_j\n= \\frac{\\exp(x_j / \\tau)}{\\sum_{k=1}^K \\exp(x_k / \\tau)}\n\\]</span></p>\n<p>This formula gives us continuous relaxation of discrete random variables. Let’s see what it corresponds to in binary case:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\zeta\n&= \\frac{\\exp((\\gamma_1 + \\log p) / \\tau)}{\\exp((\\gamma_1 + \\log p) / \\tau) + \\exp((\\gamma_0 + \\log (1-p)) / \\tau)} \\\\\n&= \\frac{1}{1 + \\exp((\\gamma_0 + \\log (1-p) - \\gamma_1 - \\log p) / \\tau)}\\\\\n&= \\sigma_\\tau\\left(\\gamma_1 - \\gamma_0 + \\log \\tfrac{p}{1-p}\\right)\n\\end{align*}\n\\]</span></p>\n<p>Then <span class=\"math inline\">\\(\\gamma_1 - \\gamma_0\\)</span> has <a href=\"https://en.wikipedia.org/wiki/Logistic_distribution\">Logistic</a>(0, 1) distribution <a href=\"#fn7\" class=\"footnoteRef\" id=\"fnref7\"><sup>7</sup></a>. This estimator is a bit more efficient since you can generate Logistic random variables faster than generating two independent Gumbel r.v.s. <a href=\"#fn8\" class=\"footnoteRef\" id=\"fnref8\"><sup>8</sup></a></p>\n<p>Even though this choice of Logistic distribution in binary case seems arbitrary, let’s not forget, that it’s a special case of a more general relaxation of any categorical r.v. If we chose some other<a href=\"#fn9\" class=\"footnoteRef\" id=\"fnref9\"><sup>9</sup></a> distribution in a binary case, we’d have to construct some cumbersome stick-breaking procedure to generalize it to the multivariate case.</p>\n<h2 id=\"marginalization-via-continuous-noise\">Marginalization via Continuous Noise</h2>\n<p>An interesting approach was proposed in the <a href=\"https://arxiv.org/abs/1609.02200\">Discrete Variational Autoencoders paper</a>. The core idea is that you can smooth binary r.v. <span class=\"math inline\">\\(z\\)</span> with p.m.f. <span class=\"math inline\">\\(p(z)\\)</span> by adding extra noise <span class=\"math inline\">\\(\\tau_1\\)</span> and <span class=\"math inline\">\\(\\tau_2\\)</span> and treating <span class=\"math inline\">\\(z\\)</span> as a swticher between these. Indeed, consider such smoothed r.v. <span class=\"math inline\">\\(\\zeta = z \\cdot \\tau_1 + (1 - z) \\tau_0\\)</span>. Now if we choose <span class=\"math inline\">\\(\\tau_0\\)</span> and <span class=\"math inline\">\\(\\tau_1\\)</span> such that the CDF of marginal <span class=\"math inline\">\\(\\zeta\\)</span> can be computed and inverted, we would be able to devise a reparametrization for this scheme.</p>\n<p>Consider a particular example of <span class=\"math inline\">\\(\\tau_0 = 0\\)</span> – a constant zero, and <span class=\"math inline\">\\(\\tau_1\\)</span> having some continuous distribution. The marginal CDF of <span class=\"math inline\">\\(\\zeta\\)</span> would then be</p>\n<p><span class=\"math display\">\\[\n\\mathbb{P}(\\zeta < x) = \\mathbb{P}(z = 0) [\\zeta > 0] + \\mathbb{P}(z=1) \\mathbb{P}(\\tau_1 < x)\n\\]</span></p>\n<p>Now we can invert this CDF:</p>\n<p><span class=\"math display\">\\[\n\\mathbb{Q}_\\zeta(\\rho) = \\begin{cases}\n\\mathbb{Q}_\\tau \\left( \\frac{\\rho}{p(z=1)} \\right), & \\rho \\le p(z=1) \\mathbb{P}(\\tau < 0) \\\\\n\\mathbb{Q}_\\tau \\left( \\frac{\\rho - p(z = 0)}{1 - p(z = 0)} \\right), & \\rho \\ge p(z = 0) + p(z=1) \\mathbb{P}(\\tau < 0) \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\mathbb{Q}_\\tau(\\rho)\\)</span> is an inverse of CDF of <span class=\"math inline\">\\(\\tau\\)</span>, that is <span class=\"math inline\">\\(\\mathbb{P}(\\tau < \\mathbb{Q}_\\tau(\\rho)) = \\rho\\)</span>. This formula clearly suggests that if you can compute and invert CDF of the smoothing noise <span class=\"math inline\">\\(\\tau\\)</span>, you can do the same with the smoothed variable <span class=\"math inline\">\\(\\zeta\\)</span>, essentially giving us reparametrization for the smoothed r.v. <span class=\"math inline\">\\(\\zeta\\)</span>, so we can backpropagate as usual. <a href=\"#fn10\" class=\"footnoteRef\" id=\"fnref10\"><sup>10</sup></a></p>\n<p>However, an attentive reader could spot a problem here. In the multivariate case we typically have some dependency structure, hence probabilities <span class=\"math inline\">\\(p(z_k = 0)\\)</span> and <span class=\"math inline\">\\(p(z_k = 1)\\)</span> depend on previous samples <span class=\"math inline\">\\(z_{<k}\\)</span>, which we can’t backpropagate through, and need to relax in the same way.</p>\n<p>Consider, for example, a general stochastic computation graph with 4-dimensional binary random variable <span class=\"math inline\">\\(z\\)</span>:</p>\n<div class=\"post-image\">\n<p><img src=\"/files/dvae.png\" style=\"width: 400px\" /></p>\n</div>\n<p>When applying this trick, we introduce relaxed continuous random variables <span class=\"math inline\">\\(\\zeta\\)</span> as simple transformations of corresponding binary random variables <span class=\"math inline\">\\(z\\)</span> (red lines), and make <span class=\"math inline\">\\(z\\)</span> depend on each other only through relaxed variables (purple lines).</p>\n<div class=\"post-image\">\n<p><img src=\"/files/dvae-smoothed.png\" style=\"width: 400px\" /></p>\n</div>\n<p>This trick is somewhat similar to the asymptotic reparametrization – you end up with a model that only has continuous random variables, but is equivalent to the original one that has discreteness. However, it requires you to significantly alter the model by re-expressing dependence in <span class=\"math inline\">\\(z\\)</span> using continuous relaxations <span class=\"math inline\">\\(\\zeta\\)</span>. It worked fine for the Discrete VAE application, where you want to learn this dependence structure in <span class=\"math inline\">\\(z\\)</span>, but if you have a specific one in mind, you might be in trouble.</p>\n<p>Also, we don’t want to introduce this noise at the test stage. So we’d like to fix the discrepancy between train and test somehow. One way to do so is to choose <span class=\"math inline\">\\(\\tau\\)</span> that depends on some parameter and adjust it so that <span class=\"math inline\">\\(\\tau\\)</span>’s distribution becomes closer to <span class=\"math inline\">\\(\\delta(\\tau-1)\\)</span>. In the paper authors use “truncated exponential” distribution <span class=\"math inline\">\\(p(\\tau) \\propto \\exp(\\beta \\tau) [0 \\le \\tau \\le 1]\\)</span> where <span class=\"math inline\">\\(\\beta\\)</span> is a (bounded) learnable parameter. The upper bound grows linearly as training progresses – essentially shrinking the noise towards 1 (in the limit of infinite <span class=\"math inline\">\\(\\beta\\)</span> we have <span class=\"math inline\">\\(p(\\tau) = \\delta(\\tau-1)\\)</span>).</p>\n<h2 id=\"gradient-relaxations\">Gradient Relaxations</h2>\n<p>There’s been also some research around the following idea: we don’t have any problems with discrete random variable during the forward pass, it’s differentiation during the backward one that brings difficulties. Can we approximate the gradient only? Essentially the idea is to compute the forward pass as usual, but replace the gradient through random samples with some approximation. To a mathematician (like I pretend to be) this sounds very suspicious – the gradient will no longer correspond to the objective, it’s not even clear which objective it’d correspond to. However, these methods have some attractive properties, and are well-known in the area, so I feel I have to cover them as well.</p>\n<p>One of such methods is the <a href=\"https://arxiv.org/abs/1308.3432\"><strong>Straight Through</strong> estimator</a>, which backpropagates the gradient <span class=\"math inline\">\\(\\nabla_\\theta\\)</span> through binary random variable <span class=\"math inline\">\\(z \\sim \\text{Bernoulli}(p(\\theta))\\)</span> as if there was no stochasticity (and nonlinearity!) in the first place. So in the forward pass you take your outputs (logits) of a layer preceding to the discrete stochastic node, squash it by a sigmoid function, then sample a Bernoulli random variable with such probability, and move on to the next layer, possibly sampling some more stochastic nodes along the way. In the backward phase, though, when it comes to differentiate through the discrete sampling procedure, you just go straight to the gradients of logits, like if there was no sigmoid and sampling in the first place: <span class=\"math display\">\\[\\nabla_\\theta \\text{Bernoulli}(\\sigma(g(\\theta))) := \\nabla_\\theta g(\\theta)\\]</span></p>\n<p>This estimator is clearly computing anything but an estimate of the gradient of your model, but authors claim that at least it gives you right direction for the gradient. You can also keep the sigmoid function – that’s what <a href=\"https://arxiv.org/abs/1406.2989\">Raiko et. al</a> do: <span class=\"math display\">\\[\\nabla_\\theta \\text{Bernoulli}(\\sigma(g(\\theta))) := \\nabla_\\theta \\sigma(g(\\theta))\\]</span></p>\n<p>Finally, authors of one of the <a href=\"https://arxiv.org/abs/1611.01144\">Gumbel-Softmax relaxation</a> papers proposed Straight Through Gumbel where you, again, compute forward pass as usual, but in the backward pass assume there’s Gumbel-Softmax relaxation and backpropagate through it. Hypothetically as you decrease the temperature, this relaxation becomes more exact. I guess one can try to convince themselves that for small enough <span class=\"math inline\">\\(\\tau\\)</span> this is a reasonable approximation. <span class=\"math display\">\\[\\nabla_\\theta \\text{Bernoulli}(\\sigma(g(\\theta))) := \\nabla_\\theta \\text{RelaxedBernoulli}(\\sigma(g(\\theta)))\\]</span></p>\n<p>I personally consider these methods mathematically unsound, and advise you to refrain form using them (unless you know what you’re doing – then tell me what was your rationale for this particular choice).</p>\n<h2 id=\"experiments\">Experiments</h2>\n<p>To please your eyes with some experimental plots, I used all these relaxations to train a Discrete Variational Autoencoder on MNIST. I used a single stochastic layer (shallow encoder) with 3 layers in between, and evaluated the result using <a href=\"/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html\">10,000-sample lower bound</a>, which I assume approximates marginal log-likelihood relatively well.</p>\n<div class=\"post-image\">\n<p><img src=\"/files/dvae-experiments.png\" /></p>\n</div>\n<p>First, the Relaxed ELBO pane tells us all methods have no problems optimizing their target objective. However, one should refrain from comparing them according to this number, since these are different relaxations, they are not even lower bounds for a marginal log-likelihood for some relaxed model <a href=\"#fn11\" class=\"footnoteRef\" id=\"fnref11\"><sup>11</sup></a>.</p>\n<p>Instead, let’s look at the second and third panes. The second shows the Evidence Lower Bound for the original discrete model, and the third shows the gap between the discrete ELBO and the relaxed one. First, the marginal likelihood estimation agrees with the discrete ELBO – that’s a good thing and means nothing bad is happening to the KL-divergence between the true posterior and the approximate one.</p>\n<p>You can see that the green line – the logistic distribution-based relaxation with unit temperature – actually diverges. This is a direct consequence of the chosen temperature: for <span class=\"math inline\">\\(\\tau = 1\\)</span> the density of relaxed random variables is unimodal and has its mode somewhere in the interior of the [0, 1] interval. This leads the network to adjust to the values around this mode, which poorly represent test time samples.</p>\n<p>As you can see the normal distribution with temperature 0.4 works very well at first, but then starts diverging. This might be because of the problems of Gaussian distribution we discussed earlier: namely, it has zero mass at exact 0 and 1: the network might adapt to having some small non-zero elements, and be very surprised to see them zeroed out completely at the testing time.</p>\n<p>The asymptotic reparametrization seems to be suffering from the inaccuracy of the CLT approximation. Latent code of 200 units is big, but not infinitely big for the approximation to be exact. Unfortunately, there’s no hyperparameter to adjust the approximation quality. Moreover, the relaxation gap keeps increasing.</p>\n<p>The Noise-Relaxed model performs poorly compared to other methods. This might be a result of a poor hyperparameter management: recall that we introduce continuous noise that’s missing at the test time. To make the net adjust to the test time regime we need to make the noise approach <span class=\"math inline\">\\(\\delta(\\tau - 1)\\)</span>. However, if you approach it too fast, you’ll see the relaxation gap decreasing, but the learning wouldn’t progress much as the variance of your gradients would be too high.</p>\n<p>Straight through estimators perform surprisingly good: not as good as the Gumbel-Softmax relaxation, but better than what you’d expect from a mathematically unsound method.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In this post we talked about continuous relaxations of discrete models. These relaxations allow you to invoke the reparametrization trick to backpropagate the gradients, but this comes at a cost of bias. The gradients are no more unbiased as you essentially optimize different objective. In the next blogpost we will return back to where we started – to the score-function estimator, and try to reduce its variance keeping zero bias.</p>\n<p>By the way, if you’re interested, the code for DVAE implementations is <a href=\"https://github.com/artsobolev/dvaes\">available on my GitHub</a>. However, I should warn you: it’s still work-in-progress, and lacks any documentation. I’ll add it one I’m finished with the series.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>There’s no established name for such technique. Gaussian dropout has been proposed in the original <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout paper</a>, but their equivalence under CLT was not stated formally until the <a href=\"http://proceedings.mlr.press/v28/wang13a.html\">Fast dropout training</a> paper. Nor has anyone applied this trick to, say, Discrete Variational Autoencoders. <strong>UPD</strong>: I just discovered an <a href=\"https://openreview.net/forum?id=BySRH6CpW&noteId=BySRH6CpW\">ICLR2018 submission</a> using this technique to learn discrete weights.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p><span class=\"math inline\">\\(H\\)</span> is called a Heavyside function, and you can define it’s behavior at 0 as you like, it doesn’t matter in most cases as it’s a zero-measure point.<a href=\"#fnref2\">↩</a></p></li>\n<li id=\"fn3\"><p>Unless you use generalized functions from the distributions theory (do not confuse with probability distributions!). However, that’s a whole different world, and one should be careful doing derivations there.<a href=\"#fnref3\">↩</a></p></li>\n<li id=\"fn4\"><p>Consider a random variable <span class=\"math inline\">\\(U\\)</span> with its support being <span class=\"math inline\">\\(\\mathbb{R}\\)</span>. Then <span class=\"math inline\">\\(\\mathbb{P}(H(U + c) = 1) = \\mathbb{P}(U > -c) = 1 - \\Phi(-c)\\)</span> where <span class=\"math inline\">\\(\\Phi\\)</span> is CDF of <span class=\"math inline\">\\(U\\)</span>. Then if you want this probability to be equal to some value <span class=\"math inline\">\\(p\\)</span>, you should shift <span class=\"math inline\">\\(U\\)</span> by <span class=\"math inline\">\\(c = -\\Phi^{-1}(1-p)\\)</span><a href=\"#fnref4\">↩</a></p></li>\n<li id=\"fn5\"><p>This sounds a lot like Dirac’s delta function, which is a well-known distributional derivative of the Heavyside function.<a href=\"#fnref5\">↩</a></p></li>\n<li id=\"fn6\"><p>An alternative derivation of this fact can be seen through a property of exponentially distributed random variables.<a href=\"#fnref6\">↩</a></p></li>\n<li id=\"fn7\"><p>This is clearly a special case of the general one with <span class=\"math inline\">\\(U\\)</span> being <span class=\"math inline\">\\(\\text{Logistic}(0, 1)\\)</span> and <span class=\"math inline\">\\(\\Phi\\)</span> being its CDF.<a href=\"#fnref7\">↩</a></p></li>\n<li id=\"fn8\"><p>Let’s say a word or two on how to sample Gumbels and Logistics. For both of them one can analytically derive and invert the CDF, and hence come up with formulas to transform samples from uniform distribution. For <span class=\"math inline\">\\(\\text{Gumbel}(\\mu, \\beta)\\)</span> distribution this transformation is <span class=\"math inline\">\\(u \\mapsto \\mu - \\beta \\log \\log \\tfrac{1}{u}\\)</span>, for <span class=\"math inline\">\\(\\text{Logistic}(\\mu, \\beta)\\)</span> it’s <span class=\"math inline\">\\(u \\mapsto \\mu + \\beta \\sigma^{-1}(u)\\)</span>. Hence if you use the general case, you’d need to generate 2 random variables, whereas in binary case you can use just one. I guess in general <span class=\"math inline\">\\(K\\)</span>-variate case you <em>could theoretically</em> use just <span class=\"math inline\">\\(K-1\\)</span> random variables, but that’d induce some possibly complicated dependence structure on them, and thus unnecessary complicate the sampling process.<a href=\"#fnref8\">↩</a></p></li>\n<li id=\"fn9\"><p>Consider a random variable <span class=\"math inline\">\\(U\\)</span> with its support being <span class=\"math inline\">\\(\\mathbb{R}\\)</span>. Then <span class=\"math inline\">\\(\\mathbb{P}(H(U + c) = 1) = \\mathbb{P}(U > -c) = 1 - \\Phi(-c)\\)</span> where <span class=\"math inline\">\\(\\Phi\\)</span> is CDF of <span class=\"math inline\">\\(U\\)</span>. Then if you want this probability to be equal to some value <span class=\"math inline\">\\(p\\)</span>, you should shift <span class=\"math inline\">\\(U\\)</span> by <span class=\"math inline\">\\(c = -\\Phi^{-1}(1-p)\\)</span><a href=\"#fnref9\">↩</a></p></li>\n<li id=\"fn10\"><p>There’s an alternative way to write the sampling formula <span class=\"math display\">\\[\n \\mathbb{Q}_\\zeta(\\rho) = \\begin{cases}\n 0, & \\rho \\le p(z=0) \\\\\n \\mathbb{Q}_\\tau \\left( \\frac{\\rho - p(z = 0)}{1 - p(z = 0)} \\right), & \\text{otherwise}\n \\end{cases}\n \\]</span> This formula has less branching, and thus is more efficient to compute. <br/> Moreover, in general one can avoid inverting the CDF by noticing that <span class=\"math inline\">\\(y = [\\rho < \\mathbb{P}(z=0)] \\mathbb{Q}_{\\tau_0}\\left(\\tfrac{\\rho}{\\mathbb{P}(z=0)}\\right) + [\\rho > \\mathbb{P}(z=0)] \\mathbb{Q}_{\\tau_1}\\left(\\tfrac{1-\\rho}{1-\\mathbb{P}(z=0)}\\right)\\)</span> for <span class=\"math inline\">\\(\\rho \\sim U[0,1]\\)</span> has exactly the same distribution as the marginal <span class=\"math inline\">\\(p(\\zeta)\\)</span>.<a href=\"#fnref10\">↩</a></p></li>\n<li id=\"fn11\"><p>Authors of the Concrete Distribution paper did also relax the KL-divergence term, which means they optimized lower bound for marginal likelihood in a different model, however, it’s reported to lead to better results.<a href=\"#fnref11\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Sat, 28 Oct 2017 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2017-10-28-stochastic-computation-graphs-discrete-relaxations.html",
  "dc:creator": "Artem"
}