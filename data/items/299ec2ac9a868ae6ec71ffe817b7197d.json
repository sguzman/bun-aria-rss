{
  "title": "Data Collection is Hard. You Should Try It.",
  "link": "",
  "id": "https://www.georgeho.org/data-collection-is-hard/",
  "updated": "2022-03-03T00:00:00Z",
  "published": "2022-03-03T00:00:00Z",
  "content": "<p>For people who make careers out of data, data scientists don&rsquo;t have <em>nearly</em>\nenough experience in data collection &mdash; and many data scientists don&rsquo;t seem to\nfeel much cognitive dissonance from this fact, despite (very persuasive!)\n<a href=\"https://counting.substack.com/p/go-collect-some-and-data\">overtures by a few valiant data\nprofessionals</a><sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>.</p>\n<p>With this blog post I want to give a defense of data collection — not as an\nactivity that&rsquo;s inherently worthwhile pursuing (I assume data professionals\ndon&rsquo;t need to be convinced of that!), but as something that is worth doing even\nfor <em>selfish</em> reasons. Why should you spend time learning about that data\ncollection system that&rsquo;s being maintained by that other team at work? Why\nshould you consider collecting some data for your next side project? <em>What&rsquo;s\nin it for you?</em></p>\n<p>Throughout this blog post, I’ll be making comparisons to a recent project of\nmine, <a href=\"https://cryptics.georgeho.org/\"><code>cryptics.georgeho.org</code></a>, a dataset of\ncryptic crossword clues which I created and published last year.</p>\n<h2 id=\"learn-data-adjacent-technologies\">Learn Data-Adjacent Technologies</h2>\n<p>The most obvious reason is that <strong>collecting data is a unique opportunity to\nlearn many staple technologies in data</strong> &mdash; and there aren&rsquo;t many projects\nthat run the entire data tech stack.</p>\n<p>To enumerate these technologies:</p>\n<ol>\n<li>Compute services\n<ul>\n<li>Your data collection pipelines will need to run somewhere. Will that be in\nthe cloud, or on your local computer? How do you think about trading off\ncost, compute and convenience?</li>\n<li>I ran most of my web scraping on DigitalOcean Droplets, but I could just\nas easily have taken the opportunity to learn more about cloud compute\nsolutions or serverless functions like AWS EC2 or Lambda. These days, the\nproject runs incremental scrapes entirely on my laptop.</li>\n</ul>\n</li>\n<li>Data storage\n<ul>\n<li>You’ll need to store your data somewhere, whether it be a relational or\nNoSQL database, or just flat files. Since your data will outlive any code\nyou write, careful design of the data storage solution and schema will pay\ndividends in the long run.</li>\n<li>I used SQLite for its simplicity and performance. However, as the scope of\nthe project expanded, I had to redesign the schema multiple times, which\nwas painful.</li>\n</ul>\n</li>\n<li>Labeling, annotation or other data transformations\n<ul>\n<li>After collecting your data, you may want to label, annotate, structure or\notherwise transform your data. For example, perhaps you’ll want to pull\nstructured tabular data out of unstructured PDFs or HTML tag soups;\nanother example might be to have humans label the data.</li>\n<li>This is the main “value-add” of your dataset &mdash; while the time and effort\nrequired to collect and store the data constitutes a moat, ultimately what\nwill distinguish your dataset to <em>users</em> will be the transformations done\nhere.</li>\n<li>For me, this involved a lot of <code>BeautifulSoup</code> to parse structured data\nout of HTML pages. This required a <a href=\"https://cryptics.georgeho.org/datasheet#collection-process\">significant amount of development and\nengineering\neffort</a>.</li>\n</ul>\n</li>\n<li>Data licensing and copyright\n<ul>\n<li>Once you have your dataset, can you license, share or even sell your data?\nThe legality of data are a huge grey area (especially if there&rsquo;s web\nscraping involved), and while navigating these waters will be tricky, it&rsquo;s\ninstructive to learn about it.</li>\n<li>I feel like the collection and structuring of cryptic crossword clues for\nacademic/archival purposes was fair use, and so didn&rsquo;t worry too much\nabout the legality of my project — but it was an educational rabbit hole\nto fall down!</li>\n</ul>\n</li>\n<li>Sharing and publishing data\n<ul>\n<li>The legal nuances of data aside, the technical problem of sharing data is\npretty tricky!</li>\n<li>This problem sits at the intersection of MLOps and information design: you\nwant to share the data in a standardized way, while having an interface\nthat making it easy for users to explore your data. Serving a tarball on a\nweb server technically works, but leaves so much on the table.</li>\n<li><code>cryptics.georgeho.org</code> uses <a href=\"https://datasette.io/\">Datasette</a>, which I\ncan&rsquo;t recommend highly enough.</li>\n</ul>\n</li>\n<li>Writing documentation\n<ul>\n<li>If you think it&rsquo;s hard to write and maintain good documentation for\nsoftware, imagine how difficult it must be to do the same for data, which\noutlives software and is much harder to both create and version control.</li>\n<li>I&rsquo;ve found <a href=\"https://arxiv.org/abs/1803.09010\">Gebru et al.&rsquo;s <em>Datasheets for\nDatasets</em></a> to be an excellent template\nfor documenting data.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"design-a-data-collection-system\">Design a Data Collection System</h2>\n<p>Hopefully by now you can appreciate that every part of the data collection\npipeline involves not just technical proficiency with some system or framework,\nbut also an element sound architecture.</p>\n<p><strong>Collecting data is a great way to get experience designing an entire data\npipeline from end to end, from creation to delivery.</strong> This kind of opportunity\ndoesn&rsquo;t come easily (even in industry!), and while your data pipeline won&rsquo;t be\nas sophisticated as the kinds you&rsquo;ll find at data companies, you&rsquo;ll still be\nable to take away some valuable lessons from it.</p>\n<p>For <code>cryptics.georgeho.org</code>, I found that the most valuable pattern for storing\ndata was to dump raw and unstructured data into a database (a &ldquo;data lake&rdquo;), and\nthen extract useful and structured data into a separate database (a &ldquo;data\nwarehouse&rdquo;). I also learnt that the historical backfilling ETL job required a\nlot of time and compute, but subsequent incremental ETL jobs could just run off\nof my laptop. These best practice patterns around data collection and\nmanagement are all applicable far beyond my simple side project, and were\nvaluable lessons to learn first-hand.</p>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>Puzzlingly, this trend doesn&rsquo;t seem to be true of other forms of\nunglamorous data work like data cleaning, where people generally accept that\n<a href=\"https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt\">data cleaning is not grunt\nwork</a>.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}