{
  "title": "Fast Adaptive Federated Bilevel Optimization. (arXiv:2211.01122v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2211.01122",
  "description": "<p>Bilevel optimization is a popular hierarchical model in machine learning, and\nhas been widely applied to many machine learning tasks such as meta learning,\nhyperparameter learning and policy optimization. Although many bilevel\noptimization algorithms recently have been developed, few adaptive algorithm\nfocuses on the bilevel optimization under the distributed setting. It is well\nknown that the adaptive gradient methods show superior performances on both\ndistributed and non-distributed optimization. In the paper, thus, we propose a\nnovel adaptive federated bilevel optimization algorithm (i.e.,AdaFBiO) to solve\nthe distributed bilevel optimization problems, where the objective function of\nUpper-Level (UL) problem is possibly nonconvex, and that of Lower-Level (LL)\nproblem is strongly convex. Specifically, our AdaFBiO algorithm builds on the\nmomentum-based variance reduced technique and local-SGD to obtain the best\nknown sample and communication complexities simultaneously. In particular, our\nAdaFBiO algorithm uses the unified adaptive matrices to flexibly incorporate\nvarious adaptive learning rates to update variables in both UL and LL problems.\nMoreover, we provide a convergence analysis framework for our AdaFBiO\nalgorithm, and prove it needs the sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$ with communication complexity of\n$\\tilde{O}(\\epsilon^{-2})$ to obtain an $\\epsilon$-stationary point.\nExperimental results on federated hyper-representation learning and federated\ndata hyper-cleaning tasks verify efficiency of our algorithm.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>"
}