{
  "title": "Quan Sun on finishing in second place in Predict Grant Applications",
  "link": "https://medium.com/kaggle-blog/quan-sun-on-finishing-in-second-place-in-predict-grant-applications-3770a0bedaad?source=rss----4b0982ce16a3---4",
  "guid": "https://medium.com/p/3770a0bedaad",
  "category": [
    "kaggle-competition",
    "kaggle"
  ],
  "dc:creator": "Kaggle Team",
  "pubDate": "Mon, 20 Apr 2020 17:18:21 GMT",
  "atom:updated": "2020-04-20T17:18:21.023Z",
  "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*7qfpYxj1Xs3WY30Z.jpg\" /></figure><p>I’m a PhD student of the Machine Learning Group in the University of Waikato, Hamilton, New Zealand. I’m also a part-time software developer for 11ants analytics. My PhD research focuses on meta-learning and the full model selection problem. In 2009 and 2010, I participated the UCSD/FICO data mining contests.</p><p><strong>What I tried and What ended up working</strong></p><p>I tried many different algorithms (mainly weka and matlab implementations) and feature sets in nearly 80 submissions. This report will briefly introduce two approaches that worked for this competition. Each of them will be discussed sequentially in the order of submissions.</p><p>After the first 10 testing submissions, I realised that there was a concept drift happening between 2007 and 2008. The success rates decline gradually from 2007. Also, on the information page of the contest, it states that “In Australia, success rates have fallen to 20–25 per cent…”. To me, this probably means, the decision rules for grant applications were somehow changed during 2007 and 2008. Here are some consequences that I could think of, including but not limited to:</p><ul><li>The overall success rates will continue to drop</li><li>Successful applications in 2005/2006 would be declined in 2007/2008, so for 2009/2010</li><li>Success patterns becoming to be “more” random</li><li>Decision rules for year 2009/2010 will be close to that for 2007/2008, compared with rules for year 2006 and prior.</li></ul><p>Based on the information and assumptions above, I decided to mainly use data points from 2007 and 2008 for training my classifiers, which turns out to be a reasonable choice.</p><p><strong>Approach A: Ensemble Selection with transformed feature set (used in the first 20 submissions)</strong> <strong>Data engineering/transformation part</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*KtnUYvxezcoYvSzk.jpg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*E86BgKdnqF6ooURU.jpg\" /></figure><p>Start.date</p><p>to numeric, year, month, day in numbers</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*TIONF4w3-6vK8KHG.jpg\" /></figure><p>RFCD.Code.X (X=1 to 5)</p><p>to nominal</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*fmA6nP2K4MhFR6Tj.jpg\" /></figure><p>Person.ID.X (X=1 to 15)</p><p>to nominal</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*txT8bXlwoU3Yw0NA.jpg\" /></figure><p>Number.of.Grant.X (X=1 to 15)</p><p>Total number of successful/unsuccessful grants per application</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*QUUmRcW_ZqpWssUq.jpg\" /></figure><p>Publications AA, A, B, C</p><p>Total number of AA, A, B, C publications per application</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*Ts75108MC88tUGqu.jpg\" /></figure><p>Role.X</p><p>Total number of CHIEF_INVESTIGATORs, PRINCIPAL_SUPERVISORs, DELEGATED_RESEARCHER, EXT_CHIEF_INVESTIGATORs per application</p><p>Country.of.Birth.X</p><p>Total number of Asia_Pacific born, Australia, Great_Britain, Western_Europe, Eastern_Europe, North_America, New_Zealand, Middle_East_and_Africa per application</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*7AJuBy-gfvOqq8Ka.jpg\" /></figure><p>With.PHD</p><p>Total number of PhDs per application</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*4497P8zH9AohGEea.jpg\" /></figure><p>Years.IN.UNI</p><p>Total number of people who has been in the University for more than 5 years</p><p>After all those transformations are done, I also had a java program to transform all nominal attributes to its corresponding frequency. The frequency counting is based on all the available data points. So, the final feature set consists of the original features, transformed features and frequency.</p><p><strong>Modeling part</strong></p><p>My main method is called Ensemble Selection, originally proposed by Rich Caruana and co-authors of Cornell University (http://portal.acm.org/citation.cfm?id=1015432). The following pseudocode demonstrates the basic idea of Ensemble Selection:</p><p>0. Split the data into two parts: The build set and the hillclimb set</p><p>1. Start with the empty ensemble.</p><p>2. Add to the ensemble the model (trained on “build” set) in the library that maximizes the ensemble’s performance to the error metric (AUC for this contest) on a “hillclimb” (validation) set.</p><p>3. Repeat Step 2 for a ﬁxed number of iterations or until all the models have been used.</p><p>4. Return the ensemble from the nested set of ensembles that has maximum performance on the hillclimb (validation) set.</p><p><strong>Model library used for my Ensemble Selection system:</strong></p><p>AdaBoost, LogitBoost, RealAdaBoost, DecisionTable, RotationForest, BayesNet, NaiveBayes, 7 algorithms with different parameters, in total 28 base classifiers.</p><p><strong>Building set and hillclimb set for Ensemble Selection:</strong></p><p>Data points from year 2007 are used as the “build set”</p><p>Data points from year 2008 are used as the “hillclimb set”</p><p>Or</p><p>Data points from year 2007/01/01 to 2008/04/30 are used as the “build set”</p><p>Data points after year 2008/04/30 are used as the “hillclimb set”</p><p>Both setups worked well for the Ensemble Selection approach.</p><p>In summary, the final system for Approach A consists of three main components:</p><p>Data points from 2007 for training and 2008 for hillclimbing.</p><p>Ensemble Selection, num of bags: 10, hillclimb iterations = size of the model library.</p><p>In total 352 features.</p><p>Learderboard AUC: 0.956X, Best final test set AUC: 0.961X</p><p>From submission 20 to the end of the competition, the following features are added to Approach A feature set:</p><p>Number of missing values</p><p>Number of non-missing values</p><p>Missing value rate</p><p>Transform “Contract.Value.Band” to numeric values</p><p>Average contract value</p><p>RFCD.CODE mean, sum, max, min, standard deviation per application based</p><p>RFCD.PCT mean, sum, max, min, std per application based</p><p>SEO.CODE mean, sum, max, min, std per application based</p><p>SEO.PCT mean, sum, max, min, std per application based</p><p>Successful.grant mean, sum, max, min, std per application based</p><p>Unsuccessful.grant mean, sum, max, min, std per application based</p><p>Successful.grant mean average per application based</p><p>Successful.grant sum average per application based</p><p>All the above features for the first three applicants</p><p>All the above features for Unsuccessful.grant</p><p>Success rate of applicant 1, applicant 2, and applicant 3 per application based</p><p>Success rate of all applicants per application based</p><p>Mean, max, std success rates of all applicants per application based</p><p>Number of publications mean, sum, max, min, std per application based</p><p>Except the frequency counting described in Approach A, only “row-based (per-application-based)” statistical features were gradually introduced to my system during the competition, because I thought that, compared with “time based/column based features”, “row-based” statistical features would reduce the chance of overfitting.</p><p>Also, the following algorithms (with different/diverse parameter settings) were gradually added to the model library while the competition:</p><p>RandomForest</p><p>RacedIncrementalLogitBoost</p><p>Bagging with trees</p><p>ADTree</p><p>Linear Regression</p><p>RandomCommittee with Random Trees</p><p>Dagging</p><p>J48</p><p><strong>Approach B: Rotation Forest with the feature set from Approach A</strong></p><p>I tried using only Rotation forest (http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2006.211) with the following setup:</p><p>Base classifier: M5P model tree (weka default is J48)</p><p>Rotation method: Random Projection with Gaussian distribution (weka default is PCA)</p><p>The Rotation forest classifier was trained on data points from 2007 and 2008 with the feature set from Approach A. Here are the results:</p><p>Leaderboard AUC: 0.947X, Final test set AUC: 0.962X</p><p>Averaging the two approaches could improve the final test set AUC to 0.963X.</p><p><strong>What tools I used</strong></p><p>Software/Tools used for modelling and data analysis:</p><p>Weka 3.7.1 is used for modelling (with my own improved version of the Ensemble Selection algorithm)</p><p>Matlab and SAS are used for data visualization and statistical analysis</p><p>Java is used as the main programming language for this project</p><p>Most experiments were done on my home PC: AMD 6-core, 16G ram on Windows system.</p><p><em>Originally published at </em><a href=\"https://web.archive.org/web/20180527173810/http://blog.kaggle.com/2011/02/21/quan-sun-on-finishing-in-second-place-in-predict-grant-applications/\"><em>b</em></a><em>log.kaggle.com on February 22, 2011.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3770a0bedaad\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/kaggle-blog/quan-sun-on-finishing-in-second-place-in-predict-grant-applications-3770a0bedaad\">Quan Sun on finishing in second place in Predict Grant Applications</a> was originally published in <a href=\"https://medium.com/kaggle-blog\">Kaggle Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}