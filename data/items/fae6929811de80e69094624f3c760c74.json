{
  "title": "Notes on iMAML: Meta-Learning with Implicit Gradients",
  "description": "<p>This week I read this cool new paper on meta-learning: it a slightly different approach compared to its predecessors based on some observations about differentiating the optima of regularized optimization.</p><ul><li>Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine (2019) <a href=\"https://arxiv.org/abs/1909.04630\">Meta-Learning with Implicit Gradients</a></li></ul><p>Another paper that came out at the</p>",
  "link": "https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/",
  "guid": "5d80a05552573a0038bd2a55",
  "dc:creator": "Ferenc Huszar",
  "pubDate": "Thu, 19 Sep 2019 14:08:40 GMT",
  "media:content": "",
  "content:encoded": "<img src=\"https://www.inference.vc/content/images/2019/09/download-69.png\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\"><p>This week I read this cool new paper on meta-learning: it a slightly different approach compared to its predecessors based on some observations about differentiating the optima of regularized optimization.</p><ul><li>Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine (2019) <a href=\"https://arxiv.org/abs/1909.04630\">Meta-Learning with Implicit Gradients</a></li></ul><p>Another paper that came out at the same time has discovered similar techniques, so I thought I'd update the post and mention it, although I won't cover it in detail and the post was written primarily about Rajeswaran et al (2019)</p><ul><li>Yutian Chen, Abram L. Friesen, Feryal Behbahani, David Budden, Matthew W. Hoffman, Arnaud Doucet, Nando de Freitas (2019) <a href=\"https://arxiv.org/abs/1909.05557\">Modular Meta-Learning with Shrinkage</a></li></ul><h3 id=\"outline-of-this-post\">Outline of this post</h3><ul><li>I will give a high-level overview of the meta-learning setup, where our goal is to learn a good initialization or regularization strategy for SGD so it converges to better minima across a range of tasks.</li><li>I illustrate how iMAML works on a 1D toy-example, and discuss the behaviour and properties of the meta-objective.</li><li>I will then discuss a limitation of iMAML: that it only considers the location of minima, and not the probability with which a stochastic algorithm ends up in a specific minimum.</li><li>I will finally relate iMAML to a variational approach to meta-learning.</li></ul><h2 id=\"meta-learning-and-maml\">Meta-Learning and MAML</h2><p>Meta-learning has several possible formulations, I will try to explain the setup of this paper following my own interpretation and notation that differs from the paper but will make my explanations clearer (hopefully).</p><p>In meta-learning we have a series of independent tasks, with associated training and validation loss functions $f_i$ and $g_i$, respectively. We have a set of model parameters $\\theta$ which are shared across the tasks, and the loss functions $f_i(\\theta)$ and $g_i(\\theta)$ evaluate how well the model with parameters $\\theta$ does on the training and test cases of task $i$. We have an algorithm that has access to the training loss $f_i$ and some meta-parameters $\\theta_0$, and output some optimal or learned parameters $\\theta_i^\\ast = Alg(f_i, \\theta_0)$. The goal of the meta-learning algorithm is to optimize the meta-objective</p><p>$$<br>\\mathcal{M}(\\theta_0) = \\sum_i g_i(Alg(f_i, \\theta_0))<br>$$</p><p>with respect to the meta-parameters $\\theta_0$.</p><p>In early versions of this work, MAML, the algorithm was chosen to be stochastic gradient descent, $f_i$ and $g_i$ being the training and test loss of a neural network, for example. The meta-parameter $\\theta_0$ was the point of initialization for the SGD algorithm, shared between all the tasks. Since SGD updates are differentiable, one can compute the gradient of the meta-objective with respect to the initial value $\\theta_0$ by simply backpropagating through the SGD steps. This was essentially what MAML did.</p><p>However, the effect of initialization on the final value of $\\theta$ is pretty weak, and difficult - if at all possible - to characterise analytically. If we allow the SGD to go on for many steps, we might converge to a better parameter, but the trajectory will be very long, and the gradients with respect to the initial value vanish. If we make the trajectories short enough, the gradients w.r.t. $\\theta_0$ are informative but we may not reach a very good final value.</p><h2 id=\"imaml\">iMAML</h2><p>This is why Rajeswaran et al opted to make the dependence of the final point of the trajectory on meta-paramteter $\\theta\\_0$ way stronger: Instead of simply initializing SGD from $\\theta\\_0$ they also anchor the parameter to stay in the vicinity of $\\theta\\_0$ by adding a quadratic regularizer $\\|\\theta - \\theta_0\\|$ to their loss. Because of this, two things happen:</p><ul><li>now all steps of the SGD depend on $\\theta$, not just the initial point</li><li>now the location of &#xA0;the minimum SGD eventually converges to also depend on #\\theta\\_0#</li></ul><p>It is this second property that iMAML exploits. Let me illustrate what that dependence looks like:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/09/download-60.png\" class=\"kg-image\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\" loading=\"lazy\"></figure><p>In the figure above, let's say that we would like to minimise an objective function $f(\\theta)$. This would be the training loss of one of the tasks the meta-learning algorithm has to solve. Our current meta-parameter $\\theta_0$ is marked on the x axis, and the orange curve shows the associated quadratic penalty. The teal curve shows the sum of the objective with the penalty. The red star shows the location of the minimum, which is what the learning algorithm finds.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/09/first_animation--3-.gif\" class=\"kg-image\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\" loading=\"lazy\"></figure><p>Now let's animate this plot. I'm going to move the anchor point $\\theta_0$ around, and reproduce the same plots. You can see that, as we move $\\theta_0$ and the associate penalty, the local (and therefore global) minima of the regularized objective move change:</p><p>So it's clear that there is a non-trivial, non-linear relationship between the anchor-point $\\theta_0$ and the location of a local minimum $\\theta^\\ast$. Let's plot this relationship as a function of the anchor point:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/09/download-63.png\" class=\"kg-image\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\" loading=\"lazy\"></figure><p>We can see that this function is not at all nice to work with, it has sharp jumps when the closest local minimum to $\\theta_0$ changes, and it is relatively flat between these jumps. In fact, you can observe that the sharpest the local minimum nearest to $\\theta_0$ is, the flatter the relationship between $\\theta_0$ and $\\theta$. This is because if $f$ has a sharp local minimum near $\\theta_0$, then the location of the regularized minimum will be mostly determined by $f$, and the location of the anchor point $\\theta_0$ doesn't matter much. If the local minimum around f is wide, there's a lot of wiggle room for the optimum and the effect of the regularization will be larger.</p><h2 id=\"implicit-gradients\">Implicit Gradients</h2><p><br>And now we come to the whole point of the iMAML procedure. The gradient of this function $\\theta^\\ast(\\theta_0)$ in fact can be calculated in closed form. It is, indeed, related to the curvature, or second derivative, of $f$ around the minimum we find:</p><p>$$<br>\\frac{d\\theta^\\ast}{d\\theta_0} = \\frac{1}{1 + f''(\\theta^\\ast)}<br>$$</p><p>In order to check that this formula works, I calculated the derivative numerically and compared it with what the theory predicts, they match perfectly:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/09/download-66.png\" class=\"kg-image\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\" loading=\"lazy\"></figure><p>When the parameter space is high-dimensional, we have a similar formula involving the inverse of the Hessian plus the identity. In high dimensions, inverting or even calculating and storing the Hessian is not very practical. One of the main contributions of the iMAML paper is a practical way to approximate gradients, using a conjugate gradient inner optimization loop. For details, please read the paper.</p><h3 id=\"optimizing-the-meta-objective\">Optimizing the meta-objective</h3><p>When optimizing the anchor point in a meta-learning setting, it is not the location $\\theta^\\ast$ we are interested in, only the value that the function $f$ takes at this location. (in reality, we would now use the validation loss, in place of the training loss used for gradient descent, but for simplicity, I assume the two losses overlap). The value of $f$ at its local optimum is plotted below:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.inference.vc/content/images/2019/09/download-65.png\" class=\"kg-image\" alt=\"Notes on iMAML: Meta-Learning with Implicit Gradients\" loading=\"lazy\"></figure><p>Oh dear. This function is not very pretty. The meta-objective $f(\\theta^\\ast(\\theta_0))$ becomes a piecewise continuous function, a connection of neighbouring basins, with non-smooth boundaries. The local gradients of this function contain very little information about the global structure of the loss function, it only tells you where to go to reach the nearest local minimum. So I wouldn't say this is the nicest function to optimize.</p><p>Thankfully, though, this function is not what we have to optimize. In meta-learning, we have a distribution over functions $f$ we optimize, so the actual meta-objective is something like $\\sum_i f_i(\\theta_i^\\ast(\\theta_0))$. And the sum of a bunch of ugly functions might well turn into something smooth and nice. In addition, the 1-D function I use for this blog post is not representative of the high-dimensional loss functions of neural networks which we want to apply iMAML to. Take for example the concept of mode connectivity (see e.g. <a href=\"https://arxiv.org/pdf/1802.10026.pdf\">Garipov et al, 2018</a>): it seems that the modes found by SGD using different random seeds are not just isolated basins, but they are connected by smooth valleys along which the training and test error are low. This may in turn make the meta-objective behave more smoothly between minima.</p><h2 id=\"what-is-missing-stochasticity\">What is missing? Stochasticity</h2><p>An important aspect that MAML or iMAML do not not consider is the fact that we usually use stochastic optimization algorithms. Rather than deterministically finding a particular local minimum, SGD samples different minima: when run with different random seeds it will find different minima.</p><p>A more generous formulation of the meta-objective would allow for stochastic algorithms. If we denote by $\\mathcal{Alg}(f_i, \\theta_0)$ the distribution over solutions the algorithm finds, the meta-objective would be</p><p>$$<br>\\mathcal{M}_{stochastic}(\\theta) = \\sum_i \\mathbb{E}_{\\theta \\sim \\mathcal{Alg}(f_i, \\theta_0)} g_i(\\theta)<br>$$</p><p>Allowing for stochastic behaviour might actually be a great feature for meta-learning. While the position of the global minimum of the regularized objective can change abruptly s a function $\\theta_0$ (as illustrated in the third figure above), allowing for stochastic behaviour might smooth our the meta-learning objective.</p><p>Now suppose that SGD anchored to $\\theta_0$ converges to one of a finite set of local minima. The meta-learning objective now depends on $\\theta_0$ in two different ways:</p><ul><li>as we change the anchor $\\theta_0$, the location of the minima change, as illustrated above. This change is differentiable, and we know its derivative.</li><li>as we change the anchor $\\theta_0$, the probability with which we find the different solutions changes. Some solutions will be found more often, some less often.</li></ul><p>iMAML accounts for the first influence, but it ignores the influence through the second mechanism. This is not to say that iMAML is broken, but that it misses a possibly crucial contribution of stochastic behaviour that MAML or explicitly differentiating through the algorithm does not.</p><h2 id=\"compare-with-a-variational-approach\">Compare with a Variational Approach</h2><p>Of course this work reminded me of a Bayesian approach. Whenever someone describes quadratic penalties, all I see are Gaussian distributions.</p><p>In a Bayesian interpretation of iMAML, one can think of the anchor point $\\theta_0$ as the mean of a prior distribution over the neural network's weights. The inner loop of the algorithm, or $Alg(f_i, \\theta_0)$ then finds the maximum-a-posteriori (MAP) approximation to the posterior over $\\theta$ given the dataset in question. This is assuming that the loss is a log likelihood of some kind. The question is, how should one update the meta-parameter $\\theta_0$?</p><p>In the Bayesian world, we would seek to optimize $\\theta_0$ by maximising the marginal likelihood. As this is usually intractable, so it is common to turn to a variational approximation, which in this case would look something like this:</p><p>$$<br>\\mathcal{M}_{\\text{variational}}(\\theta_0, Q_i) = \\sum_i \\left( KL[Q_i\\vert \\mathcal{N}_{\\theta_0}] + \\mathbb{E}_{\\theta \\sim Q_i} f_i(\\theta) \\right),<br>$$</p><p>where $Q_i$ approximates the posterior over model parameters for task $i$. A specific choice of $Q_i$ is a dirac delta distribution centred at a specific point $Q_i(</p><p>theta) = \\delta(\\theta - \\theta^{\\ast}_i)$. If we generously ignore some constants that blow up to infinitely large, the KL divergence between the Gaussian prior and the degenerate point-posterior is a simple Euclidean distance, and our variational objective reduces to:</p><p>$$<br>\\mathcal{M}_{\\text{variational}}(\\theta_0, \\theta_i) = \\sum_i \\left( \\|\\theta_i - \\theta_0\\|^2 + f_i(\\theta_i) \\right)<br>$$</p><p>Now this objective function looks very much like the optimization problem that the inner loop of iMAML attempts to solve. If we were working in the pure variational framework, this may be where we leave things, and we could jointly optimize all the $\\theta_i$s as well as $\\theta_0$. Someone in the know, please comment below pointing me to the best references where this is being done for meta-learning.</p><p>This objective is significantly easier to optimize with and involves no inner-loop optimization or black magic. It simply ends up pulling $\\theta_0$ closer to the centre of gravity of the various optima found for each task $i$. Not sure if this is such a good idea though for meta-learning, as the final values of $\\theta_i$ which we reach by jointly optimizing over everything may not be reachable by doing SGD from $\\theta_0$ from scratch. But who knows. A good idea may be, given the observations above, to jointly minimize the variational objective with respect to $\\theta_0$ and $\\theta_i$, but every once in a while reinitialize $\\theta_i$ to be $\\theta_0$. But at this point, I'm really just making stuff up...</p><p>Anyway, back to iMAML, which does something interesting with this variational objective, and I think it can be understood as a kind of amortized computation: Instead of treating $\\theta_i$ as separate auxiliary parameters, it specifies that $\\theta_i$ are in fact a deterministic function of $\\theta_0$. As the variational objective is a valid upper bound for any value of $\\theta_i$, it is also a valid upper bound if we make $\\theta_i$ explicitly dependent on $\\theta_0$. The variational objective thus becomes a function of $\\theta_0$ only (and also of hyperparameters of the algorithm $Alg$ if it has any):</p><p>$$<br>\\mathcal{M}_{\\text{variational}}(\\theta_0) = \\sum_i \\left( \\|Alg(f_i, \\theta_0) - \\theta_0\\|^2 + f_i(Alg(f_i, \\theta_0)) \\right)<br>$$</p><p>And there we have it. A variational objective for meta-learning $\\theta_0$ which is very similar to the MAML/iMAML meta-objective, except it also has the $\\|Alg(f_i, \\theta_0) - \\theta_0\\|^2$ term which factors into updating $\\theta_0$ which we didn't have before. Also notice that I did not use separate training and validation loss $f_i$ and $g_i$ but that would be a very justified choice as well.</p><p>What is cool about this is that this provides extra justification and interpretation for what iMAML is trying to do, and suggests directions in which iMAML could perhaps be improved. On the flipside, the implicit differentiation trick in iMAML might be useful in other situations where we want to amortize the variational posterior similarly.</p><p>I'm pretty sure I missed many references, please comment below if you think I should add anything, especially on the variational bit.</p>"
}