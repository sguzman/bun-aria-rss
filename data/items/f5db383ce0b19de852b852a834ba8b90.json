{
  "title": "Labels and Patterns",
  "link": "http://mimno.infosci.cornell.edu/b/articles/labelsandpatterns/",
  "pubDate": "Wed, 17 Sep 2014 20:00:00 -0400",
  "guid": "http://mimno.infosci.cornell.edu/b/articles/labelsandpatterns/",
  "author": "",
  "description": "<p>I’ve been using this blog as a more philosophical platform, this is going to be about some new features in the machine learning\npackage that I work on, <a href=\"http://mallet.cs.umass.edu/\">Mallet</a>. \nOne of these, LabeledLDA, is some code that I’ve had lying around for a few years.\nThe other, stop patterns, is a simple addition that may be useful in vocabulary curation for text mining.\nYou’ll need to grab the latest development version from <a href=\"https://github.com/mimno/Mallet\">GitHub</a> to run these.</p>\n<p><span class=\"more\"></span></p>\n<h3 id=\"labeledlda\">LabeledLDA</h3>\n<p>The first tool, LabeledLDA, is a simplified topic model for use when documents have labels, and labels correspond to topics.\nWhen each document in a text collection has exactly one label, you can estimate a Na&iuml;ve Bayes model by counting all the\nwords in the subset of documents that have a given label value.\nWhen you have no labels, a topic model like Latent Dirichlet Allocation can find semantically coherent “topics”.\nBut what if you have multiple labels per document?\nFor example, consider a 5-star review of a Chinese restaurant.\nWe can think of this as a document with three labels: 5-star, Chinese, and Restaurant.\nSome of the words in the review might be about being a great restaurant, others might be about Chinese cuisine, and others about restaurants in general.\nLabeledLDA (Ramage et al., EMNLP 2009; see also McCallum, Multi-label text classification…, 1999) is a way of measuring this kind of connections between words and labels.</p>\n<p>LabeledLDA asserts a one-to-one relationship between labels and topics. Every document is a mixture of the\nwords associated with its labels, and nothing else.\nThis assumption simplifies inference because in a given document we only have to assign each word token to one of the labels for that document, rather than the entire set of documents.\nAn additional advantage is that each topic has, by definition, an existing label, so we can skip the “squinting at it” stage of fully unsupervised topic model analysis.\nThe cost of that assumption is that we may miss emergent patterns or language that’s not well modeled by the labels. There has been work on models that make looser connections between labels and topics, but that’s a question for another day.</p>\n<p>Running LabeledLDA requires that we have documents with multiple labels.\nThe easiest way to do this in Mallet is with the new “—label-as-features” option in the “import-file” command.\nThe input for this command is a text file with one line per document and three columns: an ID, a set of labels,\nand the text.\nBy default, Mallet interprets the first two whitespace-delimited fields of each line as the ID and label.\nThat’s a problem because we’d like to use some whitespace to delimit our multiple labels, but it’s easy to \nfix by changing the line-parsing regular expression to separate columns with tabs, and thus allow spaces to exist in the label field.\nHere’s a full command line example:</p>\n<pre><code>bin/mallet import-file --input yelp-short.txt --output yelp-short.seq --stoplist-file yelp.stops --label-as-features --keep-sequence --line-regex &#39;([^\\t]+)\\t([^\\t]+)\\t(.*)&#39;\n</code></pre><p>As an example I’m going to use the fourth round <a href=\"http://www.yelp.com/dataset_challenge\">Yelp dataset</a> —- it’s freely available, but please go to the source.\nAs labels or “features”, I’m using the tags, city and region for each business, and the review stars.\nFrom a subset of 100,000 reviews, this results in 553 labels.\nHere’s the command to run a model and save “topic keys” to a file:</p>\n<pre><code>bin/mallet run cc.mallet.topics.LabeledLDA --input yelp-short.seq --output-topic-keys yelp-llda.keys\n</code></pre><p>The command supports most of the input and output options from standard Mallet LDA, in many cases with \nthe exact same code. Use the “—help” option to find out more. Sharing code makes it easy to add functionality, but sometimes causes problems —- for example the held-out probability evaluator and topics-for-new-documents inferencer don’t pay attention to document labels.</p>\n<p>The “keys” file contains one line per label/topic. The first column is the topic ID number, corresponding\nto the original order that each label first appeared in the training data. The second column is the \nlabel string. The third column is the total number of tokens assigned to that topic at the particular Gibbs \nsampling state where we stopped. The last column is a space-delimited list of 20 words in descending order by probability in the topic. Topics with very few total words may have less than 20 words.</p>\n<p>Here are some examples of high-frequency topics:</p>\n<pre><code>10      Restaurants     727089  salad sauce meal cheese table dinner wasn&#39;t side server bit both flavor taste took served tasted something tasty hot though \n232     Sushi_Bars      38441   sushi roll rolls fish happy hour tuna spicy sashimi salmon favorite sakana quality nigiri tempura chefs places japanese special rice \n29      Hotels_&amp;_Travel 30017   room hotel stay desk rooms front stayed day bed marriott check pool property booked area clean airport car checked resort \n32      Hotels  29163   room hotel stay breakfast rooms free clean pool bed stayed comfortable bathroom area shower inn suites desk hotels hot water \n16      Chinese 51834   chinese rice fried soup beef egg sauce hot dishes pork shrimp sour spicy dish crab orange noodles delivery china mein \n</code></pre><p>Moderate-frequency topics are also pretty good, although there are some specific words mixed in like “Zoe’s” for “Southern”:</p>\n<pre><code>477     Dim_Sum 730     dim sum c-fu dimsum carts phoenix cart variety dim-sum feet palace balls items xmas tripe tradition leaf lotus steamed tarts \n298     Office_Equipment        728     office staples max ink officemax print printer cartridge toner printing cartridges supplies printed router copy computer file copies depot computers \n113     Videos_&amp;_Video_Game_Rental      724     movies video movie blockbuster netflix films rent paradise releases rentals selection section games title rental foreign dvd titles film adult \n468     Apache_Junction_AZ      721     apache junction elvira&#39;s lake canyon patio bloody mechanic girlfriend mary gringos lglg&amp;c trail hiking miles beautiful truck hacker&#39;s locos cantina \n155     Home_Decor      659     tile candles boone granite holland slab stone rocks kitchen slabs wood pewter minerals stones rock world decorate molding marling linen \n234     Southern        657     zoe&#39;s sandwich cake salad zoes feta chocolate slaw pasta grilled healthy tuna potato kabobs dry limeade chips pimento sandwiches roll-ups \n471     Gelato  648     gelato angel sweet flavors chocolate dark cotta panna italy hazelnut peanut flavor super texture taste creamy size dairy sweet&#39;s gelatos \n</code></pre><p>Rare topics are increasingly noisy:</p>\n<pre><code>423     Interior_Design 8       copehaigen whatzits registries adhd distain untrained charlotte step \n410     Airlines        8       unshaven sweatshirt csa eqipment scruffy hooded fbo vending \n380     Russian 8       fsu embarrasses semi-salted latvia selection.i yearning jewish \n552     Divorce_&amp;_Family_Law    6       pontoni attornies results.there jea retained \n506     Motorcycle_Rental       6       registration breach brags corky harley \n504     Tutoring_Centers        6       peripherals really-really-smart clock-mini-speaker reassurances pro&#39;s \n</code></pre><p>The star-rating labels are particularly interesting.</p>\n<pre><code>4       5_star  255897  amazing years every recommend awesome family excellent highly everything favorite wonderful times prices many can&#39;t fantastic feel most everyone day \n6       4_star  204279  times prices usually area location bit its most price many every find years lot though favorite enjoy you&#39;re recommend excellent \n11      3_star  92213   decent nothing bad bit prices stars though times average its area overall price however lot location most wasn&#39;t probably give \n5       2_star  85203   minutes asked told another table took bad wasn&#39;t times server last manager finally waited not\n7       1_star  213432  told asked minutes manager customer another rude called him took worst bad money business horrible left his call give finally \n</code></pre><p>To me these look great. Five-star reviews have lots of positive adjectives, one-star reviews are narratives about employees who were rude or slow, and three-star words seem to capture the essence of ho-hum averageness.\nBut they’re also not that different from the overall sub-corpus frequencies.\nThe top words for five-star reviews (minus stopwords) are <em>amazing, years, every, recommend, awesome, favorite, everything,</em> and <em>day</em>.\nThe most significant difference seems to be <em>family</em>, which is the 19th most common word and occurs 25% less often than <em>everything</em>, but ranks higher within this five-star topic.\nThis result is stable —- running the algorithm again with a random initialization results in essentially the same order with a few word pairs swapped. </p>\n<h3 id=\"stop-patterns\">Stop Patterns</h3>\n<p>There was a request on the topic list for a way to remove <em>patterns</em> of words rather than just specific stopwords. I’ve added code to do that.</p>\n<p>Here’s a sample regular expression file. Note that I’m using the Java <code>matches</code> function, so a pattern must account for the <em>entire</em> token —- thus the trailing .*.</p>\n<pre><code>.*ly\nphotog.*\n.*\\.{2,}.*\n</code></pre><p>The first is an attempt to remove English adverbs, but may not be ideal for people named Kelly or my friends at <a href=\"http://www.yummly.com/\">Yummly</a>. The second looks for a prefix. The third looks for multiple dots, which seems to be a common pattern in some Yelp reviews. \nIf there is a problem with a regular expression, Mallet will print an error and will ignore all subsequent patterns, but will not stop processing.\nHere’s a sample file with examples:</p>\n<pre><code>1    X    this is a terribly useful test for....things involving photographs, photographers, and photography called test.txt\n</code></pre><p>Here’s what happens when I run the standard method:</p>\n<pre><code>% bin/mallet import-file --input test.txt --print-output\nname: 1\ntarget: X\ninput: this(0)=1.0\nterribly(1)=1.0\nuseful(2)=1.0\ntest(3)=1.0\nfor....things(4)=1.0\ninvolving(5)=1.0\nphotographs(6)=1.0\nphotographers(7)=1.0\nand(8)=1.0\nphotography(9)=1.0\ncalled(10)=1.0\ntest.txt(11)=1.0\n</code></pre><p>Notice that the new default token pattern ignores one- and two-letter words (is, a), but includes tokens that contain punctuation, which picks up URLs, apostrophes, and hyphens.\nKeep in mind these interactions with the tokenization pattern when designing regular expressions.\nHere’s what the output looks like afterwards.</p>\n<pre><code>% bin/mallet import-file --input test.txt --stop-pattern-file stoplists/patterns.txt --print-output\nname: 1\ntarget: X\ninput: this(0)=1.0\nuseful(1)=1.0\ntest(2)=1.0\ninvolving(3)=1.0\nand(4)=1.0\ncalled(5)=1.0\ntest.txt(6)=1.0\n</code></pre><p>Using stop patterns involves creating a Matcher object for each token and each pattern, and checking that pattern against the\ntoken string. In contrast, static stopword removal is a hash lookup. I haven’t measured performance yet, but I \nwould guess that stop patterns should be used sparingly.</p>"
}