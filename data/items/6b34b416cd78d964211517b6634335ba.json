{
  "title": "HDF in the Cloud",
  "link": "",
  "updated": "2018-02-06T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/02/06/hdf-in-the-cloud",
  "content": "<p>Multi-dimensional data,\nsuch as is commonly stored in HDF and NetCDF formats,\nis difficult to access on traditional cloud storage platforms.\nThis post outlines the situation, the following possible solutions, and their strengths and weaknesses.</p>\n\n<ol>\n  <li><strong>Cloud Optimized GeoTIFF:</strong> We can use modern and efficient formats from other domains, like Cloud Optimized GeoTIFF</li>\n  <li><strong>HDF + FUSE:</strong> Continue using HDF, but mount cloud object stores as a file system with <a href=\"https://en.wikipedia.org/wiki/Filesystem_in_Userspace\">FUSE</a></li>\n  <li><strong>HDF + Custom Reader:</strong> Continue using HDF, but teach it how to read from S3, GCS, ADL, …</li>\n  <li><strong>Build a Distributed Service:</strong> Allow others to serve this data behind a web API, built however they think best</li>\n  <li><strong>New Formats for Scientific Data:</strong> Design a new format, optimized for scientific data in the cloud</li>\n</ol>\n\n<h2 id=\"not-tabular-data\">Not Tabular Data</h2>\n\n<p>If your data fits into a tabular format,\nsuch that you can use tools like SQL, Pandas, or Spark,\nthen this post is not for you.\nYou should consider Parquet, ORC,\nor any of a hundred other excellent formats or databases that are well designed for use on cloud storage technologies.</p>\n\n<h2 id=\"multi-dimensional-data\">Multi-Dimensional Data</h2>\n\n<p>We’re talking about data that is multi-dimensional and regularly strided.\nThis data often occurs in simulation output (like climate models),\nbiomedical imaging (like an MRI scan),\nor needs to be efficiently accessed across a number of different dimensions (like many complex time series).\nHere is an image from the popular <a href=\"http://xarray.pydata.org/en/stable/\">XArray</a> library to put you in the right frame of mind:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/xarray-boxes-2.png\" width=\"100%\" /></p>\n\n<p>This data is often stored in blocks such that, say,\neach 100x100x100 chunk of data is stored together,\nand can be accessed without reading through the rest of the file.</p>\n\n<p>A few file formats allow this layout,\nthe most popular of which is the HDF format,\nwhich has been the standard for decades\nand forms the basis for other scientific formats like NetCDF.\nHDF is a powerful and efficient format capable of handling both complex hierarchical data systems\n(filesystem-in-a-file)\nand efficiently blocked numeric arrays.\nUnfortunately HDF is difficult to access from cloud object stores (like S3),\nwhich presents a challenge to the scientific community.</p>\n\n<h2 id=\"the-opportunity-and-challenge-of-cloud-storage\">The Opportunity and Challenge of Cloud Storage</h2>\n\n<p>The scientific community generates several petabytes of HDF data annually.\nSupercomputer simulations (like a large climate model) produce a few petabytes.\nPlanned NASA satellite missions will produce <a href=\"https://earthdata.nasa.gov/cmr-and-esdc-in-cloud\">hundreds of petabytes</a> a year of observational data.\nAll of these tend to be stored in HDF.</p>\n\n<p>To increase access,\ninstitutions now place this data on the cloud.\nHopefully this generates more social value from existing simulations and observations,\nas they are ideally now more accessible to any researcher\nor any company\nwithout coordination with the host institution.</p>\n\n<p>Unfortunately, the layout of HDF files makes them difficult to query efficiently on cloud storage systems\n(like Amazon’s S3, Google’s GCS, or Microsoft’s ADL).\nThe HDF format is complex and metadata is strewn throughout the file,\nso that a complex sequence of reads is required to reach a specific chunk of data.\nThe only pragmatic way to read a chunk of data from an HDF file today is to use the existing HDF C library,\nwhich expects to receive a C <code class=\"language-plaintext highlighter-rouge\">FILE</code> object, pointing to a normal file system\n(not a cloud object store) (this is not entirely true, as we’ll see below).</p>\n\n<p>So organizations like NASA are dumping large amounts of HDF onto Amazon’s S3\nthat no one can actually read, except by downloading the entire file to their local hard drive,\nand then pulling out the particular bits that they need with the HDF library.\nThis is inefficient.\nIt misses out on the potential that cloud-hosted public data can offer to our society.</p>\n\n<p>The rest of this post discusses a few of the options to solve this problem,\nincluding their advantages and disadvantages.</p>\n\n<ol>\n  <li>\n    <p><strong>Cloud Optimized GeoTIFF:</strong> We can use modern and efficient formats from other domains, like Cloud Optimized GeoTIFF</p>\n\n    <p><em>Good:</em> Fast, well established</p>\n\n    <p><em>Bad:</em> Not sophisticated enough to handle some scientific domains</p>\n  </li>\n  <li>\n    <p><strong>HDF + FUSE:</strong> Continue using HDF, but mount cloud object stores as a file system with <a href=\"https://en.wikipedia.org/wiki/Filesystem_in_Userspace\">Filesystem in Userspace, aka FUSE</a></p>\n\n    <p><em>Good:</em> Works with existing files, no changes to the HDF library necessary, useful in non-HDF contexts as well</p>\n\n    <p><em>Bad:</em> It’s complex, probably not performance-optimal, and has historically been brittle</p>\n  </li>\n  <li>\n    <p><strong>HDF + Custom Reader:</strong> Continue using HDF, but teach it how to read from S3, GCS, ADL, …</p>\n\n    <p><em>Good:</em> Works with existing files, no complex FUSE tricks</p>\n\n    <p><em>Bad:</em> Requires plugins to the HDF library and tweaks to downstream libraries (like Python wrappers).  Will require effort to make performance optimal</p>\n  </li>\n  <li>\n    <p><strong>Build a Distributed Service:</strong> Allow others to serve this data behind a web API, built however they think best</p>\n\n    <p><em>Good:</em> Lets other groups think about this problem and evolve complex backend solutions while maintaining stable frontend API</p>\n\n    <p><em>Bad:</em> Complex to write and deploy.  Probably not free.  Introduces an intermediary between us and our data.</p>\n  </li>\n  <li>\n    <p><strong>New Formats for Scientific Data:</strong> Design a new format, optimized for scientific data in the cloud</p>\n\n    <p><em>Good:</em> Fast, intuitive, and modern</p>\n\n    <p><em>Bad:</em> Not a community standard</p>\n  </li>\n</ol>\n\n<p>Now we discuss each option in more depth.</p>\n\n<h2 id=\"use-other-formats-like-cloud-optimized-geotiff\">Use Other Formats, like Cloud Optimized GeoTIFF</h2>\n\n<p>We could use formats other than HDF and NetCDF that are already well established.\nThe two that I hear most often proposed are <a href=\"http://www.cogeo.org/\">Cloud Optimized GeoTIFF</a> and <a href=\"https://parquet.apache.org/\">Apache Parquet</a>.\nBoth are efficient, well designed for cloud storage, and well established as community standards.\nIf you haven’t already, I strongly recommend reading Chris Holmes’ (Planet) blog series on <a href=\"https://medium.com/tag/cloud-native-geospatial/latest\">Cloud Native Geospatial</a>.</p>\n\n<p>These formats are well designed for cloud storage because they support random access well with relatively few communications and with relatively simple code.\nIf you needed to you could look at the <a href=\"https://trac.osgeo.org/gdal/wiki/CloudOptimizedGeoTIFF\">Cloud Optimized GeoTIFF spec</a>,\nand within an hour of reading,\nget an image that you wanted using nothing but a few <code class=\"language-plaintext highlighter-rouge\">curl</code> commands.\nMetadata is in a clear centralized place.\nThat metadata provides enough information to issue further commands to get the relevant bytes from the object store.\nThose bytes are stored in a format that is easily interpreted by a variety of common tools across all platforms.</p>\n\n<p>However, neither of these formats are sufficiently expressive to handle some of the use cases of HDF and NetCDF.\nRecall our image earlier about atmospheric data:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/xarray-boxes-2.png\" width=\"100%\" /></p>\n\n<p>Our data isn’t a parquet table, nor is it a stack of geo-images.\nWhile it’s true that you could store any data in these formats,\nfor example by saving each horizontal slice as a GeoTIFF,\nor each spatial point as a row in a Parquet table,\nthese storage layouts would be <em>inefficient</em> for regular access patterns.\nSome parts of the scientific community <em>need</em> blocked layouts for multi-dimensional array data.</p>\n\n<h2 id=\"hdf-and-filesystems-in-userspace-fuse\">HDF and Filesystems in Userspace (FUSE)</h2>\n\n<p>We could access HDF data on the cloud <em>now</em> if we were able to convince our operating system that S3 or GCS or ADL were a normal file system.\nThis is a reasonable goal; cloud object stores look and operate much like normal file systems.\nThey have directories that you can list and navigate.\nThey have files/objects that you can copy, move, rename,\nand from which you can read or write small sections.</p>\n\n<p>We can achieve this using an operating systems trick, <a href=\"https://en.wikipedia.org/wiki/Filesystem_in_Userspace\">FUSE, or Filesystem in Userspace</a>.\nThis allows us to make a program that the operating system treats as a normal file system.\nSeveral groups have already done this for a variety of cloud providers.\nHere is an example with the <a href=\"https://gcsfs.readthedocs.org\">gcsfs</a> Python library</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ pip install gcsfs --upgrade\n$ mkdir /gcs\n$ gcsfuse bucket-name /gcs --background\nMounting bucket bucket-name to directory /gcs\n\n$ ls /gcs\nmy-file-1.hdf\nmy-file-2.hdf\nmy-file-3.hdf\n...\n</code></pre></div></div>\n\n<p>Now we point our HDF library to a NetCDF file in that directory\n(which actually points to an object on Google Cloud Storage),\nand it happily uses C File objects to read and write data.\nThe operating system passes the read/write requests to <code class=\"language-plaintext highlighter-rouge\">gcsfs</code>,\nwhich goes out to the cloud to get data, and then hands it back to the operating system, which hands it to HDF.\nAll normal HDF operations <em>just work</em>,\nalthough they may now be significantly slower.\nThe cloud is further away than local disk.</p>\n\n<p>This slowdown is significant because the HDF library makes many small 4kB reads\nin order to gather the metadata necessary to pull out a chunk of data.\nEach of those tiny reads made sense when the data was local,\nbut now that we’re sending out a web request each time.\nThis means that users can sit for minutes just to open a file.</p>\n\n<p>Fortunately, we can be clever.\nBy buffering and caching data, we can reduce the number of web requests.\nFor example, when asked to download 4kB we actually download 100kB or 1MB.\nIf some of the future 4kB reads are within this 1MB then we can return them immediately.,\nLooking at HDF traces it looks like we can probably reduce “dozens” of web requests to “a few”.</p>\n\n<p>FUSE also requires elevated operating system permissions,\nwhich can introduce challenges if working from Docker containers\n(which is likely on the cloud).\nDocker containers running FUSE need to be running in privileged mode.\nThere are some tricks around this,\nbut generally FUSE brings some excess baggage.</p>\n\n<h2 id=\"hdf-and-a-custom-reader\">HDF and a Custom Reader</h2>\n\n<p>The HDF library doesn’t <em>need</em> to use C File pointers,\nwe can extend it to use other storage backends as well.\n<a href=\"https://support.hdfgroup.org/HDF5/doc/TechNotes/VFL.html\">Virtual File Layers</a>\nare an extension mechanism within HDF5 that could allow it to target cloud object stores.\nThis has already been done to support Amazon’s S3 object store twice:</p>\n\n<ol>\n  <li>Once by the <a href=\"https://www.hdfgroup.org/\">HDF group</a>, S3VFD (currently private),</li>\n  <li>Once by <a href=\"https://github.com/llllllllll\">Joe Jevnik</a> and <a href=\"https://github.com/ssanderson\">Scott Sanderson</a> (<a href=\"https://www.quantopian.com/\">Quantopian</a>) at <a href=\"https://h5s3.github.io/h5s3/\">https://h5s3.github.io/h5s3/</a> (highly <a href=\"https://h5s3.github.io/h5s3/warnings.html\">experimental</a>)</li>\n</ol>\n\n<p>This provides an alternative to FUSE\nthat is better because it doesn’t require privileged access,\nbut is worse because it only solves this problem for HDF and not all file access.</p>\n\n<p>In either case we’ll need to do look-ahead buffering and caching to get reasonable performance (or see below).</p>\n\n<h2 id=\"centralize-metadata\">Centralize Metadata</h2>\n\n<p>Alternatively, we might centralize metadata in the HDF file in order to avoid many hops throughout that file.\nThis would remove the need to perform clever file-system caching and buffering tricks.</p>\n\n<p>Here is a brief technical explanation from <a href=\"https://github.com/llllllllll\">Joe Jevnik</a>:</p>\n\n<blockquote>\n  <p>Regarding the centralization of metadata: this is already a feature of hdf5\n  and is used by many of the built-in drivers. This optimization is enabled by\n  setting the <code class=\"language-plaintext highlighter-rouge\">H5FD_FEAT_AGGREGATE_METADATA</code> and\n  <code class=\"language-plaintext highlighter-rouge\">H5FD_FEAT_ACCUMULATE_METADATA</code> feature flags in your VFL driver’s query\n  function. The first flag says that the hdf5 library should pre-allocate a\n  large region to store metadata, future metadata allocations will be served\n  from this pool. The second flag says that the library should buffer metadata\n  changes into large chunks before dispatching the VFL driver’s write\n  function. Both the default driver (sec2) and h5s3 enable these\n  optimizations. This is further supported by using the <code class=\"language-plaintext highlighter-rouge\">H5FD_FLMAP_DICHOTOMY</code>\n  free list option which uses one free list for metadata allocations and\n  another for non-metadata allocations. If you really want to ensure that the\n  metadata is aggregated, even without a repack, you can use the built-in\n  ‘multi’ driver which dispatches different allocation flavors to their own\n  driver.</p>\n</blockquote>\n\n<h2 id=\"distributed-service\">Distributed Service</h2>\n\n<p>We could offload this problem to a company,\nlike the non-profit <a href=\"https://www.hdfgroup.org/\">HDF group</a>\nor a for-profit cloud provider like Google, Amazon, or Microsoft.\nThey would solve this problem however they like,\nand expose a web API that we can hit for our data.</p>\n\n<p>This would be a distributed service of several computers on the cloud near our data,\nthat takes our requests for what data we want,\nperform whatever tricks they deem appropriate to get that data,\nand then deliver it to us.\nThis fleet of machines will still have to address the problems listed above,\nbut we can let them figure it out, and presumably they’ll learn as they go.</p>\n\n<p>However, this has both technical and social costs.\nTechnically this is complex, and they’ll have to handle a new set of issues around scalability, consistency, and so on that are already solved(ish) in the cloud object stores.\nSocially this creates an intermediary between us and our data, which we may not want both for reasons of cost and trust.</p>\n\n<p>The HDF group is working on such a service, HSDS that works on Amazon’s S3 (or anything that looks like S3).\nThey have created a <a href=\"https://github.com/HDFGroup/h5pyd\">h5pyd</a> library that is a drop-in replacement for the popular <a href=\"https://github.com/HDFGroup/h5pyd\">h5py</a> Python library.</p>\n\n<p>Presumably a cloud provider, like Amazon, Google, or Microsoft could do this as well.\nBy providing open standards like <a href=\"https://www.opendap.org/\">OpenDAP</a> they might attract more science users onto their platform\nto more efficiently query their cloud-hosted datasets.</p>\n\n<p>The satellite imagery company <a href=\"https://planet.com\">Planet</a> already has <a href=\"https://medium.com/planet-os/an-api-for-the-worlds-weather-climate-data-7e9946169f48\">such a service</a>.</p>\n\n<h2 id=\"new-formats-for-scientific-data\">New Formats for Scientific Data</h2>\n\n<p>Alternatively, we can move on from the HDF file format,\nand invent a new data storage specification that fits cloud storage\n(or other storage)\nmore cleanly without worrying about supporting the legacy layout of existing HDF files.</p>\n\n<p>This has already been going on, informally, for years.\nMost often we see people break large arrays into blocks,\nstore each block as a separate object in the cloud object store with a suggestive name,\nand store a metadata file describing how the blocks relate to each other.\nThis looks something like the following:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>/metadata.json\n/0.0.0.dat\n/0.0.1.dat\n/0.0.2.dat\n ...\n/10.10.8.dat\n/10.10.9.dat\n/10.10.10.dat\n</code></pre></div></div>\n\n<p>There are many variants:</p>\n\n<ul>\n  <li>They might extend this to have groups or sub-arrays in sub-directories.</li>\n  <li>They might choose to compress the individual blocks in the <code class=\"language-plaintext highlighter-rouge\">.dat</code> files or not.</li>\n  <li>They might choose different encoding schemes for the metadata and the binary blobs.</li>\n</ul>\n\n<p>But generally most array people on the cloud do something like this with their research data,\nand they’ve been doing it for years.\nIt works efficiently,\nis easy to understand and manage,\nand transfers well to any cloud platform,\nonto a local file system,\nor even into a standalone zip file or small database.</p>\n\n<p>There are two groups that have done this in a more mature way,\ndefining both modular standalone libraries to manage their data,\nas well as proper specification documents that inform others how to interpret this data in a long-term stable way.</p>\n\n<ul>\n  <li><a href=\"http://zarr.readthedocs.io/en/stable/\">Zarr</a></li>\n  <li><a href=\"https://github.com/saalfeldlab/n5\">N5</a></li>\n</ul>\n\n<p>These are both well maintained and well designed libraries (by my judgment),\nin Python and Java respectively.\nThey offer layouts like the layout above, although with more sophistication.\nEntertainingly <a href=\"https://github.com/saalfeldlab/n5#filesystem-specification-version-100\">their</a> <a href=\"http://zarr.readthedocs.io/en/stable/spec/v2.html\">specs</a> are similar enough that another library,\n<a href=\"https://github.com/constantinpape/z5\">Z5</a>,\nbuilt a cross-compatible parser for each in C++.\nThis unintended uniformity is a good sign.\nIt means that both developer groups were avoiding creativity,\nand have converged on a sensible common solution.\nI encourage you to read the <a href=\"http://zarr.readthedocs.io/en/stable/spec/v2.html\">Zarr Spec</a> in particular.</p>\n\n<p>However, technical merits are not alone sufficient to justify a shift in data format,\nespecially for archival datasets of record that we’re discussing.\nThe institutions in charge of this data and have multi-decade horizons and so move slowly.\nFor them, moving off of the historically community standard would be major shift.</p>\n\n<p>And so we need to answer a couple of difficult questions:</p>\n\n<ol>\n  <li>How hard is it to make HDF efficient in the cloud?</li>\n  <li>How hard is it to shift the community to a new standard?</li>\n</ol>\n\n<h2 id=\"a-sense-of-urgency\">A Sense of Urgency</h2>\n\n<p>These questions are important now.\nNASA and other agencies are pushing NetCDF data into the Cloud today and will be increasing these rates substantially in the coming years.</p>\n\n<p>From <a href=\"https://earthdata.nasa.gov/cmr-and-esdc-in-cloud\">earthdata.nasa.gov/cmr-and-esdc-in-cloud</a> (via <a href=\"http://rabernat.github.io/\">Ryan Abernathey</a>)</p>\n\n<blockquote>\n  <p>From its current cumulative archive size of almost 22 petabytes (PB), the volume of data in the EOSDIS archive is expected to grow to almost 247 PB by 2025, according to estimates by NASA’s Earth Science Data Systems (ESDS) Program. Over the next five years, the daily ingest rate of data into the EOSDIS archive is expected to reach 114 terabytes (TB) per day, with the upcoming joint NASA/Indian Space Research Organization Synthetic Aperture Radar (NISAR) mission (scheduled for launch by 2021) contributing an estimated 86 TB per day of data when operational.</p>\n</blockquote>\n\n<p>This is only one example of many agencies in many domains pushing scientific data to the cloud.</p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Thanks to\n<a href=\"https://github.com/llllllllll\">Joe Jevnik</a> (Quantopian),\n<a href=\"https://github.com/jreadey\">John Readey</a> (HDF Group),\n<a href=\"https://github.com/rsignell-usgs\">Rich Signell</a> (USGS),\nand <a href=\"https://rabernat.github.io/\">Ryan Abernathey</a> (Columbia University)\nfor their feedback when writing this article.\nThis conversation started within the <a href=\"https://pangeo-data.github.io/\">Pangeo</a> collaboration.</p>"
}