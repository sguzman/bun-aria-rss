{
  "title": "CUDA Tutorial: Implicit Matrix Factorization on the GPU",
  "link": "",
  "updated": "2017-11-15T00:00:00-08:00",
  "id": "http://www.benfrederickson.com/implicit-matrix-factorization-on-the-gpu/",
  "content": "\n        \n            <img src=\"http://www.benfrederickson.com/images/implicit-matrix-factorization-on-the-gpu/speed.png\" width=\"100%\" style=\"max-width:500px\">\n        \n        <p>I recently bought a system that actually has a <a href=\"https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti/\">decent\nGPU</a> on it, and I\nthought it would be cool to learn a little bit about CUDA programming to really take advantage\nof it.</p>\n\n<p>The obvious choice of problems to get started with was extending my <a href=\"https://github.com/benfred/implicit\">implicit matrix\nfactorization</a> code to run on the GPU. I’ve written a\n<a href=\"/fast-implicit-matrix-factorization/\">couple</a> of <a href=\"/matrix-factorization/\">posts</a> about this\nrecommendation algorithm already, but the task is basically to learn a weighted regularized matrix factorization\ngiven a set of positive only implicit user feedback.\nThe nice thing about this model is that it is relatively simple while still not being possible to express efficiently on higher level frameworks like TensorFlow or PyTorch.\nIt’s also inherently embarrassingly parallel and well suited for running on the GPU.</p>\n\n<p>This post aims to serve as a really basic tutorial on how to write code for the GPU using the CUDA\ntoolkit. I found that CUDA programming was pretty interesting, but it took me a little bit to learn\nhow to do this effectively - and I wanted to share what I learned while it is still fresh in my mind.</p>\n\n<p class='more'><a href='http://www.benfrederickson.com/implicit-matrix-factorization-on-the-gpu/'>Read more ...</a></p>\n     "
}