{
  "id": "tag:blogger.com,1999:blog-5547907074344788039.post-8652689743916445587",
  "published": "2013-02-14T16:22:00.000-08:00",
  "updated": "2013-02-14T16:22:43.103-08:00",
  "category": "",
  "title": "20 Tips and tricks working with data",
  "content": "<br /><div class=\"p1\">In this post I lay out a number of tips, tricks and suggestions for working with data, from collection to storage to analysis. The main idea is to help students and other younger analysts and data scientists.&nbsp;However, they apply to all ages and experiences.&nbsp;Indeed, while many of the following ideas may sound obvious and trivial, it is surprising how often I see the counter-examples and anti-patterns.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">This is almost certainly a biased selection based on my personal experience over the years. Thus, if you have any additional comments or tips, feel free to add those in the comments.</div><div class=\"p2\"><br /></div><div class=\"p1\">This section is about data itself.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>1) Inspect your data</b>.&nbsp;</span></div><div class=\"p1\">When you receive a dataset, open a text editor and inspect the data. Don't assume all is present and correct. Page through the data quickly to get a sense of columns that are mostly NULL, data that are zero or negative, the ranges of dates and so on. It is very easy to make mistakes when reformatting data and data entry people are often low-paid, low-skilled workers. Assume that there is something wrong with your data. In *nix, use the <i><b>head</b></i> and <i><b>more</b></i> commands to get a sense of the data.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>2) Think through what you would expect</b>.</span></div><div class=\"p1\">Related to above, it helps to have an expectation of what you might expect in each column. This will help in spotting errors. For instance, would you expect to find negative values? I once worked with a government-recorded health dataset that included data of&nbsp;people's height, something one might expect would be easy to measure and record. However, we found a chunk of people listed as 5 inches tall. Likely, they should have been 5 feet tall but such outliers are easy to spot with a frequency histogram. In R, get in the habit of calling <i><b>summary()</b></i><b> </b>on your dataset which provides a 5-number summary of each variable. Plot univariate distribution and in R <i><b>pairs()</b></i>, a function that produces a scatter plot of each pair of variables, can be very valuable.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>3) Check your units</b>.</span></div><div class=\"p1\">It may sound obvious but check your units. CNN <a href=\"http://www.cnn.com/TECH/space/9909/30/mars.metric.02/\">reports</a>&nbsp;</div><blockquote class=\"tr_bq\">NASA lost a $125 million Mars orbiter because a Lockheed Martin engineering team used English units of measurement while the agency's team used the more conventional metric system for a key spacecraft operation.</blockquote><div class=\"p1\">Yeah, it's that important.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>4) Plain text &gt; binary.</b>&nbsp;</span></div><div class=\"p1\">Plain text data files are always better than binaries. You get to see the raw data, to see how columns are delimited and to see which line ending types are used. Any text editor can be used to view the data. With a simple text file, it can be easy to write a python script to iterate through the data.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>5) Structured &gt; unstructured</b>.</span></div><div class=\"p1\">While there has been a recent focus on NoSQL technologies that store schemaless documents, if data are or should be well-structured or have a strict, static schema, keep them that way. It will make querying, slicing and dicing much easier later.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>6) Clean &gt; more &gt; less</b>.&nbsp;</span></div><div class=\"p1\">Much of the time building models, classifiers and so on is spent <a href=\"http://blog.engineyard.com/2012/data-science-at-engine-yard\">cleaning data</a>. It takes a lot of time and can be depressing to see huge swathes of data thrown away or excluded. It is better to start with clean data. This means having quality control on data entry. Run validators on HTML forms where possible. If you can't get clean data, <a href=\"http://www.p-value.info/2012/12/on-unreasonable-effectiveness-of-data.html\">at least get a lot of it</a> (assuming it is a well-structured experiment). Clean data are better than more data which is better than less data.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>7) Track data provenance</b>.&nbsp;</span></div><div class=\"p1\">Provenance means where data came from. In many cases, provenance is is explicit as it comes from a single source or there is some source/vendor code or name. Imagine though you create a corpus for a model in which you combine data from multiple sources. If you later find a problem with one of those souces, you will likey want to back out those data or weight them less. If your corpus is just a big set of numbers and you don't know which data came from where, you now have polluted low-quality data.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>8) Be honest about data loss acceptability</b>.&nbsp;</span></div><div class=\"p1\">When processing data, there can be problems especially when working with remote services. Servers can be down, databases connection can momentarily drop etc. Think through how important it is to process each item. If you are a bank or many other transaction based businesses, everything must work perfectly. This though comes at a high cost. However, if you are running say sentiment analysis on a twitter stream, or running some summary metrics on historical data, a small data loss may be perfectly acceptable. Will the extra cost of getting every last tweet make a big difference to your analysis?</div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>9) Embrace the *nix command line</b>.</span></div><div class=\"p1\">It is hard not to stress this enough. Learn to love the command line. While the commands may seem terse and obtuse, you will be amazed how much more productive you can be with just a few basic commands such as:</div><ul class=\"ul1\"><li class=\"li1\"><b><i>wc -l filename</i></b>: returns the number of lines in the file.</li><li class=\"li1\"><b><i>head -n filename</i></b>: shows the first n lines of a file</li><li class=\"li1\"><b><i>grep searchterm filename</i></b>: filter in the lines containing this search term.</li></ul><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>10) Denormalize data for warehousing</b>.&nbsp;</span></div><div class=\"p1\">There are very good reasons for normalized databases tables. A well-designed schema will be flexible for future data and grow by rows not columns over time. For wareshousing where data are stored for longer term, it can often be a good idea to denormalize data. This can make the provenance more explicit, can make it easier to split into chunks, and it is often easier for analysts to work with as you can avoid costly, complex joins.</div><div class=\"p2\"><br /></div><div class=\"p1\">This section refers mostly to processing data with scripts</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>11) Learn a scripting language</b>.&nbsp;</span></div><div class=\"p1\">Python, ruby, it doesn't really matter which but pick one. You want a language with at least simple I/O, a range of data structures and libraries to make HTTP requests. You will end up doing a lot of file manipulation in your career and so you need a good general tool to automate tasks. Also, some analyses, especially involving time series of individuals, are far easier to do in scripting code than with SQL self-join queries.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>12) Comment your code</b>.&nbsp;</span></div><div class=\"p1\">This is a general tip that applies to all source code not just scripts for analyzing data. When your head is in a problem there are subtle nuances about why you're doing something the way that you are. You'll forget that in a week, especially if you work on multiple projects simultaneously. Comment on <i>why</i> you are taking a certain approach or why this is an edge case. Focus less on <i>what</i> you are doing--ideally your code will be readable enough to show that---but a few such comments, in combination with well chosen function and variable names, will help you, or someone else, get into the code later. Comment for your future self.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>13) Use source control</b>.&nbsp;</span></div><div class=\"p1\">Again, a general coding tip. Source control (CSV, SVN, git, mercurial etc.) is indispensible. First, it is a form of backup. Second, it allows one to share code and work collaboratively. Third, it allows one the freedom to experiment more and try things out without fear of getting back to your last iteration--you can always revert back to the old code if it doesn't work out. Don't leave home without it. Also, always always write a commit message when committing. Like code comments, commit comments should focus more on what new code does rather than how the new code works. Again, comment for your future self.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>14) Automate</b>.&nbsp;</span></div><div class=\"p1\">This is obvious but is a lesson that most people learn only after they have had to do a task manually several times when they thought it was a one off. If there is any chance that you might need to do a task twice, it probably means you will actually need to do it five times. Automate the task where possible. This might mean a python script, a SQL stored procedure, or a cron job but it most cases, it is worth it. Even if only some of the steps are automated, it can make a huge difference.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>15) Create services</b>.&nbsp;</span></div><div class=\"p1\">In years of creating data-related tooling, I've found that people always use the tool in unexpected ways. Make the tools flexible as much of possible. Doing so, will provide an ecosystem of functionality.&nbsp;Create top-level functions that serve the 80% of uses as well as provide access to the lower-level functionality. Where possible make the functionality as web services that can talk to each other in a simple clean manner. In many cases, JSON is preferable to XML or SOAP.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>16) Don't obsess over big data</b>.&nbsp;</span></div><div class=\"p1\">I might get lynched for this one but there is a lot of hype over big data as service companies vie to take a slice of the analytics pie. While big data certainly has a place, many people who think they need big data don't have big data and don't need big data technologies. If you work in mega- or giga-bytes, it is not big. Take a deep breath and answer honestly, do I have big data or do I need to scale to huge proportions; do I have a problem that can be parallelized well (some problems don't); do I need to process all my data or can I sample? I should stress that I am in no way against big data approaches---I use them myself---but it is not a panacea. Other, simpler approaches may get you what you need.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>17) Be honest about required latency</b>.&nbsp;</span></div><div class=\"p1\">In most websites, data are replicated, warehoused and analytics performed <i>offline</i>. That latency for analysis can often be hours or a day without any real impact on the business. There are cases where more real time analysis is required, for instance collaborative filtering or computing real time twitter trends. Such short latencies comes at a cost, and significantly more so, the smaller the value. Don't waste money creating a system capable of 1 second sales data latency if you only analyze it in batch mode overnight.</div><div class=\"p2\"><br /></div><div class=\"p1\">I am hesitant to dip into data visualization as that is a whole post in itself but I will include the following two:</div><div class=\"p2\"><b></b><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>18) Use color wisely</b>.&nbsp;</span></div><div class=\"p1\">While neon colors might be in vogue at American Apparel this season that doesn't mean I want to see it in your charts. Take note of default color palettes in programs such as Excel and libraries such as ggplot2. A lot of thought, research and experience have gone into those. If you break the rules do it for good reason and for good effect. P.S. don't use yellow text on a white background. While it might look good on your laptop screen in almost all cases, it is unreadable when projected.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>19) Declutter charts and visualizations</b>.&nbsp;</span></div><div class=\"p1\">When producing charts, infographics and other data visualizations, think very carefully about what you want the reader to take away from the data. Focus on that and make that message shine through with the least amount of reader effort. Think about information density. If I need a magnifying glass to decipher all the details, you're doing it wrong. If a chart has a lot of textual annotations, you are likely doing it wrong. If you need 6 charts to convey one main point, you are probably missing some key summary metric. Use larger fonts than you might think if you are projecting to a large room. (Where possible, actually stand at the back of the room and check readability of your presentation.) You would do well to read <a href=\"http://www.edwardtufte.com/tufte/books_vdqi\">Edward Tufte</a>'s earlier books.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\"><b>20) Use # dashboard users as a metric in your dashboards</b>.&nbsp;</span></div><div class=\"p1\">In his foundation video series, Kevin Rose interviewed Jack Dorsey (CEO Square and Exec Chairman, Twitter) [<a href=\"http://revision3.com/foundation/jackdorsey\">video</a>]. Dorsey made a very interesting point in how he has tried to make <a href=\"https://squareup.com/\">Square</a> data-centric. A dashboard is useless if no one is using it. How valuable a dashboard or other metrics are should be reflected in the very dashboard itself: use the numbers of users as a metric in the dashboard itself. I would add that such metrics should be honest signals. That is, if reports are emailed out to recipients, the number of recipients is not necessarily a true reflection of use. Perhaps most people don't open it. Email open rate, however, is a more reliable and reflective indicator.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">There you go. A few tips and suggestions for working with data. Let me know what you think and what I missed.</div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Carl Anderson",
    "uri": "http://www.blogger.com/profile/11930448254473684406",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 0
}