{
  "title": "Generating Constitution with recurrent neural networks",
  "link": "",
  "updated": "2015-11-12T00:00:00+00:00",
  "id": "http://yerevann.github.io//2015/11/12/generating-constitution-with-recurrent-neural-networks",
  "content": "<p>By <a href=\"https://github.com/hnhnarek\">Narek Hovsepyan</a> and <a href=\"https://github.com/Hrant-Khachatrian\">Hrant Khachatrian</a></p>\n\n<p>Few months ago <a href=\"http://cs.stanford.edu/people/karpathy/\">Andrej Karpathy</a> wrote a <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">great blog post</a> about recurrent neural networks. He explained how these networks work and implemented a character-level RNN language model which learns to generate Paul Graham essays, <a href=\"http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\">Shakespeare works</a>, <a href=\"http://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt\">Wikipedia articles</a>, <a href=\"http://cs.stanford.edu/people/jcjohns/fake-math/4.pdf\">LaTeX articles</a> and even C++ code. He also released the code of the network on <a href=\"https://github.com/karpathy/char-rnn\">Github</a>. Lots of people did experiments, like generating <a href=\"https://gist.github.com/nylki/1efbaa36635956d35bcc\">recipes</a>, <a href=\"http://cpury.github.io/learning-holiness/\">Bible</a> or <a href=\"https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music\">Irish folk music</a>. We decided to test it on some legal texts in Armenian.</p>\n\n<!--more-->\n\n<h2 class=\"no_toc\" id=\"contents\">Contents</h2>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#character-level-rnn-language-model\" id=\"markdown-toc-character-level-rnn-language-model\">Character-level RNN language model</a></li>\n  <li><a href=\"#data\" id=\"markdown-toc-data\">Data</a></li>\n  <li><a href=\"#network-parameters\" id=\"markdown-toc-network-parameters\">Network parameters</a></li>\n  <li><a href=\"#analysis\" id=\"markdown-toc-analysis\">Analysis</a></li>\n  <li><a href=\"#generated-samples\" id=\"markdown-toc-generated-samples\">Generated samples</a></li>\n  <li><a href=\"#nanogenmo\" id=\"markdown-toc-nanogenmo\">NaNoGenMo</a></li>\n</ul>\n\n<h2 id=\"character-level-rnn-language-model\">Character-level RNN language model</h2>\n\n<p>Andrej did a great job explaining how the recurrent networks learn and even visualized how they work on text input in <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">his blog</a>. The program, called <code class=\"highlighter-rouge\">char-rnn</code>, treats the input as a sequence of characters and has no prior knowledge about them. For example, it doesn’t know that the text is in English, that there are words and there are sentences, that the space character has a special meaning and so on. After some training it manages to figure out that some character combinations appear more often than the others, learns to predict English words, uses proper punctuation, and even understands that open parentheses must be closed. When trained on Wikipedia articles it can generate text in MediaWiki format without syntax errors, although the text has little or no meaning.</p>\n\n<h2 id=\"data\">Data</h2>\n\n<p>We decided to test Karpathy’s RNN on Armenian text. Armenian language has a <a href=\"https://en.wikipedia.org/wiki/Armenian_alphabet\">unique alphabet</a>, and the characters are encoded in the Unicode space by the codes <a href=\"http://www.unicode.org/charts/PDF/U0530.pdf\">U+0530 - U+058F</a>. In UTF-8 these symbols use two bytes where the first byte is always <code class=\"highlighter-rouge\">0xD4</code>, <code class=\"highlighter-rouge\">0xD5</code> or <code class=\"highlighter-rouge\">0xD6</code>. So the neural net has to look at almost 2 times larger distances (when compared to English) in order to be able to learn the words. Also, the Armenian alphabet contains 39 letters, 50% more than Latin.</p>\n\n<p>Recently the main political topic in Armenia is the Constitutional reform. This helped us to choose the corpus for training. We took all three versions of the Constitution of Armenia (the <a href=\"http://www.arlis.am/documentview.aspx?docID=1\">first version</a> voted in 1995, the <a href=\"http://www.arlis.am/documentview.aspx?docID=75780\">updated version</a> of 2005, and the <a href=\"http://moj.am/storage/uploads/Sahmanadrakan_1-15.docx\">new proposal</a> which will be voted later this year) and concatenated them in a <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt\">single text file</a>. The size of the corpus is just 440 KB, which is roughly 224 000 Unicode symbols (all non-Armenian symbols, including spaces and numbers use 1 byte). Andrej suggests to use at least 1MB data, so our corpus is very small. On the other hand the text is quite specific, the vocabulary is very small and the structure of the text is fairly simple.</p>\n\n<p>All articles are of the following form:</p>\n\n<blockquote>\n  <p>Հոդված 1. Հայաստանի Հանրապետությունը ինքնիշխան, ժողովրդավարական, սոցիալական, իրավական պետություն է:</p>\n</blockquote>\n\n<p>The first word, <code class=\"highlighter-rouge\">Հոդված</code>, means “Article”. Sentences end with the symbol <code class=\"highlighter-rouge\">:</code>.</p>\n\n<h2 id=\"network-parameters\">Network parameters</h2>\n\n<p><code class=\"highlighter-rouge\">char-rnn</code> works with basic recurrent neural networks, LSTM networks and GRU-RNNs. In our experiments we only used LSTM network with 2 layers. Actually we don’t really understand how LSTM networks work in details, but we hope to improve our understanding by watching the videos of Richard Socher’s excellent <a href=\"http://cs224d.stanford.edu/index.html\">NLP course</a>.</p>\n\n<p>We trained the network for 50 epochs with the default learning rate parameters (base rate is <code class=\"highlighter-rouge\">2e-3</code>, which decays by a factor of <code class=\"highlighter-rouge\">0.97</code> after each <code class=\"highlighter-rouge\">10</code> epochs). We wanted to understand how the size of LSTM internal state (<code class=\"highlighter-rouge\">rnn_size</code>), <a href=\"https://www.youtube.com/watch?v=UcKPdAM8cnI\">dropout</a> and batch size affect the performance. We used <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search\">grid search</a> over the following values:</p>\n\n<ul>\n  <li><code class=\"highlighter-rouge\">rnn_size</code>: <code class=\"highlighter-rouge\">128</code>, <code class=\"highlighter-rouge\">256</code>, <code class=\"highlighter-rouge\">512</code></li>\n  <li><code class=\"highlighter-rouge\">batch_size</code>: <code class=\"highlighter-rouge\">25</code>, <code class=\"highlighter-rouge\">50</code>, <code class=\"highlighter-rouge\">100</code></li>\n  <li><code class=\"highlighter-rouge\">dropout</code>: <code class=\"highlighter-rouge\">0</code>, <code class=\"highlighter-rouge\">0.2</code>, <code class=\"highlighter-rouge\">0.4</code> and at the end we tried <code class=\"highlighter-rouge\">0.6</code></li>\n</ul>\n\n<p>After installing Lua, Torch and CUDA (as described on <a href=\"https://github.com/karpathy/char-rnn#requirements\"><code class=\"highlighter-rouge\">char-rnn</code> page</a>) we have moved our mini-corpus to <code class=\"highlighter-rouge\">/data/input.txt</code> and ran the <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/run.sh\"><code class=\"highlighter-rouge\">run.sh</code> file</a>, which contains commands like this:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\">th train.lua <span class=\"nt\">-data_dir</span> data/ <span class=\"nt\">-batch_size</span> 50 <span class=\"nt\">-dropout</span> 0.4 <span class=\"nt\">-rnn_size</span> 512 <span class=\"nt\">-gpuid</span> 0 <span class=\"nt\">-savefile</span> bs50s512d0.4 | tee log_bs50s512d0.4</code></pre></figure>\n\n<p>File names encode the hyperparameters, and the output of <code class=\"highlighter-rouge\">char-rnn</code> is logged using <a href=\"https://en.wikipedia.org/wiki/Tee_(command)\"><code class=\"highlighter-rouge\">tee</code> command</a>.</p>\n\n<h2 id=\"analysis\">Analysis</h2>\n\n<p>We have adapted <a href=\"https://github.com/YerevaNN/Caffe-python-tools/blob/master/plot_loss.py\">this script</a> written by Hrayr to plot the behavior of loss functions during the 50 epochs. The script, which runs on <code class=\"highlighter-rouge\">char-rnn</code> output is available on <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/plot_loss.py\">Github</a>. These graphs show, for example, that we practically do not gain anything after 25 epochs.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th><img src=\"/public/2015-11-11/plot_bs50s256all.png\" alt=\"Training and validation loss\" title=\"Training and validation loss\" /></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Training (blue to aqua) and validation (red to green) loss over 50 epochs. RNN size was set to 256 and the batch size was 50. In particular, this graph shows that when no dropout is used, validation loss actually increases after 20 epochs. Plotted using <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/plot_loss.py\">this script</a>.</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Experiments showed that, unsuprisingly, training loss is better (after 50 epochs) when RNN size is increased and when dropout ratio is decreased. Under all configurations we got the lowest train losses using batch size 50 (compared to 25 and 100) and we don’t have explanation for this.</p>\n\n<p>For validation loss, we have the following tables.</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n      <td><strong>Dropout</strong></td>\n      <td><strong>0</strong></td>\n      <td><strong>0.2</strong></td>\n      <td><strong>0.4</strong></td>\n      <td><strong>0.6</strong></td>\n    </tr>\n    <tr>\n      <td><strong>Batch size</strong></td>\n      <td><strong>RNN Size</strong></td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td><strong>25</strong></td>\n      <td>128</td>\n      <td>0.5060</td>\n      <td>0.4307</td>\n      <td>0.4813</td>\n      <td>0.5373</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- </code>256</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.5322</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4185</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4021</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4261</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- - </code>512</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.5596</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4495</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4380</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4126</td>\n    </tr>\n    <tr>\n      <td><strong>50</strong></td>\n      <td>128</td>\n      <td>0.4883</td>\n      <td>0.4452</td>\n      <td>0.4813</td>\n      <td>0.5373</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- </code>256</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.5249</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.3887</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.3996</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4280</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- - </code>512</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.5340</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4420</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.3997</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.3800</td>\n    </tr>\n    <tr>\n      <td><strong>100</strong></td>\n      <td>128</td>\n      <td>0.5341</td>\n      <td>0.5144</td>\n      <td>0.5454</td>\n      <td>0.6094</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- </code>256</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.5660</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4464</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4500</td>\n      <td><code class=\"highlighter-rouge\">- </code>0.4723</td>\n    </tr>\n    <tr>\n      <td> </td>\n      <td><code class=\"highlighter-rouge\">- - </code>512</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.6032</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4804</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4599</td>\n      <td><code class=\"highlighter-rouge\">- - </code>0.4399</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>When RNN size is only 128, we notice that the best performance is achieved when dropout is 20%. Larger dropout values do not allow the network to learn enough. When RNN size is increased to 256, the optimal dropout value is somewhere between 20% and 40%. For RNN size 512,  the best performance we observed used 60% dropout. We didn’t try to go any further.</p>\n\n<p>As for the batch sizes, we see the best performance on 25 if RNN size is only 128. For larger networks, batch size 50 performs better. Overall we obtained the lowest validation score, 0.38, using 60% dropout, 50 batch size and 512 RNN size.</p>\n\n<h2 id=\"generated-samples\">Generated samples</h2>\n\n<p>When the trained models are ready, we can generate text samples by using <code class=\"highlighter-rouge\">sample.lua</code> script included in the repository. It accepts one important parameter called <code class=\"highlighter-rouge\">temperature</code> which determines how much the network can “fantasize”. Higher temperature gives more diversity but at a cost of making more mistakes, as Andrej explains in his blog post. The command looks like this</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\">th sample.lua cv/lm_bs50s128d0_epoch50.00_0.4883.t7 <span class=\"nt\">-length</span> 3000 <span class=\"nt\">-temperature</span> 0.5 <span class=\"nt\">-gpuid</span> 0 <span class=\"nt\">-primetext</span> <span class=\"s2\">\"Հոդված\"</span></code></pre></figure>\n\n<p><code class=\"highlighter-rouge\">primetext</code> parameter allows to predefine the first characters of the generated sequence. Also it makes the output fully reproducible. Here is a snippet from <code class=\"highlighter-rouge\">bs50s128d0</code> model, which is available <a href=\"https://github.com/YerevaNN/char-rnn-constitution/tree/master/models\">on Github</a> (validation loss is 0.4883, sampled with 0.5 temperature).</p>\n\n<blockquote>\n  <p>Հոդված 111. Սահմանադրական դատարանի կազմավորումը, եթե այլ չեն <em>հասատատիրի</em> <em>առնչամի</em> կարելի սահմանափակվել միայն օրենքով, եթե դա անհրաժեշտ է հանցագործությունների իրավունք:\nՅուրաքանչյուր ոք ունի Հայաստանի Հանրապետության քաղաքացիությունը որոշում է կայացնում դատավորին կազմավորման կարգը</p>\n</blockquote>\n\n<p>There are 2 nonexistent words here (marked by italic), others are fine. The sentences have no meaning, some parts are quite unnatural, making them difficult to read.</p>\n\n<p>The network easily (even with 128 RNN size) learns to separate the articles by new line and starts them by the word <code class=\"highlighter-rouge\">Հոդված</code> followed by some number. But even the best one doesn’t manage to use increasing numbers for consecutive articles. Actually, very often the article number starts with <code class=\"highlighter-rouge\">1</code>, because more than one third of the articles in the corpus have numbers starting with <code class=\"highlighter-rouge\">1</code>. It also understands some basic punctuation. It correctly puts commas before the word <code class=\"highlighter-rouge\">եթե</code>, which is the Armenian word for “if”.</p>\n\n<p>With 256 RNN size and 40% dropout the result is much more readable.</p>\n\n<blockquote>\n  <p>Հոդված 14. Պատգամավոր կարող է դնել իր վստահության հարցը: Կառավարության անդամների լիազորությունները <em>համապատասխանական</em> կազմակերպությունների կամ միավորման և գործունեության կարգը սահմանվում է օրենքով: <br />\n Հոդված 107. Պատգամավորի լիազորությունները դադարեցնում է Սահմանադրությամբ և օրենքներով: Այդ իրավունքը կարող է սահմանափակվել միայն օրենքով: <br />\n Հոդված 126. Հանրապետության նախագահի հրամանագրերը և կարգադրությունները կամ այլ պետությունը միասնական կառավարման մարմինների կողմից հանցագործության կատարման պահին գործող դատարանների նախագահների թեկնածությունների և առաջարկությամբ սահմանադրական դատարանի նախագահ:<br />\n  Հայաստանի Հանրապետության իրավունքը<br /></p>\n  <ol>\n    <li>Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների պաշտպանության նպատակով:<br /></li>\n    <li>Ազգային ժողովի նախագահի վերահսկողության կամ Սահմանադրության 190-րդ հոդվածի 1-ին կետով նախատեսված դեպքերում և կարգով ընդունված որոշումները սահմանվում են օրենքով:<br /></li>\n    <li>Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների սահմանափակումների հետ <em>չապահողական</em> կամ այլ դեպքերում վարչապետի նախագահների նախնական հանձնաժողովներն ստեղծվում են Սահմանադրությամբ և օրենքներով: <br /></li>\n    <li>Յուրաքանչյուր ոք ունի իր ազգային որոշումները սահմանվում են օրենքով:<br /></li>\n    <li>Յուրաքանչյուր ոք ունի իր իրավունքների և ազատությունների պաշտպանության նպատակով:</li>\n  </ol>\n</blockquote>\n\n<p>Only 2 of the 140 words are nonexistent, but both are syntactically correct. For example there is no such word <code class=\"highlighter-rouge\">չապահողական</code> in Armenian, but <code class=\"highlighter-rouge\">չ</code> and <code class=\"highlighter-rouge\">ապա</code> are prefixes, <code class=\"highlighter-rouge\">հող</code> means “soil” and <code class=\"highlighter-rouge\">ական</code> is a suffix. Sentences still do not have valid structure.</p>\n\n<p>The network learned that sometimes ordered lists appear in the articles, but couldn’t learn to properly enumerate the points. Sometimes it counts up to 2 only :) It would be interesting to see on what kind of corpora it will be able to count a bit more.</p>\n\n<p>Here is one more snippet using the best performing model <code class=\"highlighter-rouge\">bs50s512d0.6</code> (temperature is again 0.5).</p>\n\n<blockquote>\n  <p>Հոդված 21. Յուրաքանչյուր ոք ունի ազատ տեղաշարժվելու և բնակություն է կառավարության անդամներին:\n Հանրապետության Նախագահը պաշտոնն ստանձնում է Հանրապետության Նախագահը չի կարող զբաղվել ձեռնարկատիրական գործունեությամբ: <br />\n Հոդված 50. Հանրապետության Նախագահը պաշտոնն ստանձնում է Հանրապետության Նախագահի պաշտոնը թափուր մնալու դեպքում Հանրապետության Նախագահի արտահերթ ընտրությունը կազմված է վարչապետի առաջարկությամբ վերահսկողությունը <br /></p>\n  <ol>\n    <li>Յուրաքանչյուր ոք ունի ազատ տեղաշարժվելու և բնակավայր ընտրելու իրավունք:</li>\n  </ol>\n</blockquote>\n\n<p>There are virtually no invalid words anymore (less than 0.5%, and most are one character typos). Sentences are better formed. Sometimes a sentence is composed of two exact copies of different sentences that actually occur in the corpus. For example the combination <code class=\"highlighter-rouge\">Հանրապետության Նախագահը պաշտոնն ստանձնում է</code> appears <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt#L130\">7 times</a> in the corpus, and <code class=\"highlighter-rouge\">Հանրապետության Նախագահը չի կարող զբաղվել ձեռնարկատիրական գործունեությամբ</code> appears <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/data/input.txt#L1501\">once</a>. So the generated samples are often boring. Although sometimes the combination of such two parts does have a meaning. The following <a href=\"https://github.com/YerevaNN/char-rnn-constitution/blob/master/samples/sample_bs50s512d0.6t0.5.txt#L222\">article</a> is a very good example, and doesn’t appear in the corpus.</p>\n\n<blockquote>\n  <p>Հոդված 151. Հանրապետության Նախագահի հրամանագրերը և կարգադրությունները կատարում է Ազգային ժողովի նախագահը:</p>\n</blockquote>\n\n<p>When the temperature is increased to 0.75, the samples become more interesting.</p>\n\n<blockquote>\n  <p>Հոդված 52. Հանրապետության Նախագահի լիազորությունները սահմանվում են Սահմանադրությամբ և սահմանադրական դատարանի դատավորների մեկ մտնում առաջին ատյանի դատարանները: <br />\n Հոդված 107. Ազգային ժողովի լիազորությունների ժամկետը <em>կեղերով</em> բացասական տեղեկատվության ազատության ենթարկելու հարց հարուցելու կամ այլ գործադիր իշխանության, տեղական ինքնակառավարման մարմինների անկախության մասին.<br />\n 7) եզրակացություն է տալիս իր լիազորությունների երաշխավորվում է միջազգային իրավունքի սկզբունքները և նախարարներից, ներկայացնում է Ազգային ժողովին եզրակացություններ ներկայացնելու համար:</p>\n</blockquote>\n\n<p>Typos are a bit more common. An “ordered list” is generated here which starts with <code class=\"highlighter-rouge\">7</code> and has only one entry. Article numbers are not tied to <code class=\"highlighter-rouge\">1</code>s anymore. Higher temperatures produce more nonexistent words.</p>\n\n<h2 id=\"nanogenmo\">NaNoGenMo</h2>\n\n<p>Since 1999 every November is declared a <a href=\"http://nanowrimo.org/\">National Novel Writing Month</a>, when people are encouraged to write a novel in one month. Since 2013, similar event is organized for algorithms. It’s called <a href=\"https://github.com/dariusk/NaNoGenMo-2015\">National Novel Generating Month</a>. The rules are very simple, each participant must share one generated novel (at least 50 000 words) and release the source code. <a href=\"http://www.theverge.com/2014/11/25/7276157/nanogenmo-robot-author-novel\">The Verge</a> wrote about last year’s results.</p>\n\n<p><a href=\"https://www.linkedin.com/in/armen-khachikyan-ba969218\">Armen Khachikyan</a> told us about this, and we thought that we can take part in it with a long enough generated Constitution. Here is <a href=\"https://github.com/dariusk/NaNoGenMo-2015/issues/154\">our entry</a>. It was generated by the following command:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\">th sample.lua cv/lm_bs50s512d0.6_epoch50.00_0.3800.t7 <span class=\"nt\">-length</span> 900000 <span class=\"nt\">-temperature</span> 0.5 <span class=\"nt\">-gpuid</span> 0 <span class=\"nt\">-primetext</span> <span class=\"s2\">\"Գ Լ ՈՒ Խ  1\"</span> <span class=\"o\">&gt;</span> sample_bs50s512d0.6t0.5.txt</code></pre></figure>\n\n<p>The model was generated by the following command:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\">th train.lua <span class=\"nt\">-data_dir</span> data/ <span class=\"nt\">-batch_size</span> 50 <span class=\"nt\">-dropout</span> 0.6 <span class=\"nt\">-rnn_size</span> 512 <span class=\"nt\">-gpuid</span> 0 <span class=\"nt\">-savefile</span> bs50s512d0.6 | tee log_bs50s512d0.6 </code></pre></figure>\n\n<p>All related files are in our <a href=\"https://github.com/YerevaNN/char-rnn-constitution\">Github repository</a>.</p>"
}