{
  "title": "Interpreting neurons in an LSTM network",
  "link": "",
  "updated": "2017-06-27T00:00:00+00:00",
  "id": "http://yerevann.github.io//2017/06/27/interpreting-neurons-in-an-LSTM-network",
  "content": "<p>By <a href=\"https://github.com/TigranGalstyan\">Tigran Galstyan</a> and\n   <a href=\"https://github.com/Hrant-Khachatrian\">Hrant Khachatrian</a>.</p>\n\n<p>A few months ago, we showed how effectively an LSTM network can perform text <a href=\"http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/\">\ntransliteration</a>.</p>\n\n<p>For humans, transliteration is a relatively easy and interpretable task, so it’s a good task for interpreting what the network is doing, and whether it is similar to how humans approach the same task.</p>\n\n<p>In this post we’ll try to understand: What do individual neurons of the network actually learn?  How are they used to make decisions?</p>\n\n<!--more-->\n\n<h2 class=\"no_toc\" id=\"contents\">Contents</h2>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#transliteration\" id=\"markdown-toc-transliteration\">Transliteration</a></li>\n  <li><a href=\"#network-architecture\" id=\"markdown-toc-network-architecture\">Network architecture</a></li>\n  <li><a href=\"#analyzing-the-neurons\" id=\"markdown-toc-analyzing-the-neurons\">Analyzing the neurons</a>    <ul>\n      <li><a href=\"#how-does-t-become-ծ\" id=\"markdown-toc-how-does-t-become-ծ\">How does “t” become “ծ”?</a></li>\n      <li><a href=\"#what-did-this-neuron-learn\" id=\"markdown-toc-what-did-this-neuron-learn\">What did this neuron learn?</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#visualizing-lstm-cells\" id=\"markdown-toc-visualizing-lstm-cells\">Visualizing LSTM cells</a></li>\n  <li><a href=\"#concluding-remarks\" id=\"markdown-toc-concluding-remarks\">Concluding remarks</a></li>\n</ul>\n\n<h2 id=\"transliteration\">Transliteration</h2>\n\n<p>About half of the billions of internet users speak languages written in non-Latin alphabets, like Russian, Arabic, Chinese, Greek and Armenian.  Very often, they haphazardly use the Latin alphabet to write those languages.</p>\n\n<p><code class=\"highlighter-rouge\">Привет</code>: <code class=\"highlighter-rouge\">Privet</code>, <code class=\"highlighter-rouge\">Privyet</code>, <code class=\"highlighter-rouge\">Priwjet</code>, …<br />\n<code class=\"highlighter-rouge\">كيف حالك</code>: <code class=\"highlighter-rouge\">kayf halk</code>, <code class=\"highlighter-rouge\">keyf 7alek</code>, …<br />\n<code class=\"highlighter-rouge\">Բարև Ձեզ</code>: <code class=\"highlighter-rouge\">Barev Dzez</code>, <code class=\"highlighter-rouge\">Barew Dzez</code>, …</p>\n\n<p>So a growing share of user-generated text content is in these “Latinized” or “romanized” formats that are difficult to parse, search or even identify.  Transliteration is the task of automatically converting this content into the native canonical format.</p>\n\n<p><code class=\"highlighter-rouge\">Aydpes aveli sirun e.</code>: <code class=\"highlighter-rouge\">Այդպես ավելի սիրուն է:</code></p>\n\n<p>What makes this problem non-trivial?</p>\n\n<ol>\n  <li>\n    <p>Different users romanize in different ways, as we saw above. \nFor example, <code class=\"highlighter-rouge\">v</code> or <code class=\"highlighter-rouge\">w</code> could be Armenian <code class=\"highlighter-rouge\">վ</code>.</p>\n  </li>\n  <li>\n    <p>Multiple letters can be romanized to the same Latin letter.\nFor example, <code class=\"highlighter-rouge\">r</code> could be Armenian <code class=\"highlighter-rouge\">ր</code> or <code class=\"highlighter-rouge\">ռ</code>.</p>\n  </li>\n  <li>\n    <p>A single letter can be romanized to a combination of multiple Latin letters.\nFor example, <code class=\"highlighter-rouge\">ch</code> could be Cyrillic <code class=\"highlighter-rouge\">ч</code> or Armenian <code class=\"highlighter-rouge\">չ</code>, but <code class=\"highlighter-rouge\">c</code> and <code class=\"highlighter-rouge\">h</code> by themselves are for other letters.</p>\n  </li>\n  <li>\n    <p>English words and translingual Latin tokens like URLs occur in non-Latin text.\nFor example, the letters in <code class=\"highlighter-rouge\">youtube.com</code> or <code class=\"highlighter-rouge\">MSFT</code> should not be changed.</p>\n  </li>\n</ol>\n\n<p>Humans are great at resolving these ambiguities.  We showed that LSTMs can also learn to resolve all these ambiguities, at least for Armenian. For example, our model correctly transliterated <code class=\"highlighter-rouge\">es sirum em Deep Learning</code> into <code class=\"highlighter-rouge\">ես սիրում եմ Deep Learning</code> and not <code class=\"highlighter-rouge\">ես սիրում եմ Դեեփ Լէարնինգ</code>.</p>\n\n<h2 id=\"network-architecture\">Network architecture</h2>\n\n<p>We took lots of Armenian text from Wikipedia and used <a href=\"https://github.com/YerevaNN/translit-rnn/blob/master/languages/hy-AM/transliteration.json\">probabilistic rules</a> to obtain romanized text. The rules are chosen in a way that they cover most of the romanization rules people use for Armenian.</p>\n\n<p>We encode Latin characters as one-hot vectors and apply character level bidirectional LSTM. At each time-step the network tries to guess the next character of the original Armenian sentence. Sometimes a single Armenian character is represented by multiple Latin letters, so it is very helpful to align the romanized and original texts before giving them to LSTM (otherwise we should use sequence-to-sequence networks, which are harder to train). Fortunately we can do the alignment, because the romanized version was generated by ourselves. For example, <code class=\"highlighter-rouge\">dzi</code> should be transliterated into <code class=\"highlighter-rouge\">ձի</code>, where <code class=\"highlighter-rouge\">dz</code> corresponds to <code class=\"highlighter-rouge\">ձ</code> and <code class=\"highlighter-rouge\">i</code> to <code class=\"highlighter-rouge\">ի</code>. So we add a placeholder character in the Armenian version: <code class=\"highlighter-rouge\">ձի</code> becomes <code class=\"highlighter-rouge\">ձ_ի</code>, so that now <code class=\"highlighter-rouge\">z</code> should be transliterated into <code class=\"highlighter-rouge\">_</code>. After the inference we just remove <code class=\"highlighter-rouge\">_</code>s from the output string.</p>\n\n<p>Our network consists of two LSTMs (228 cells) going forward and backward on the Latin sequence. The outputs of the LSTMs are concatenated at each step (<em>concat layer</em>), then a dense layer with 228 neurons is applied on top of it (<em>hidden layer</em>), and another dense layer (<em>output layer</em>) with softmax activations is used to get the output probabilities. We also concatenate the input vector to the hidden layer, so it has 300 neurons. This is a more simplified version of the network described in our <a href=\"http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/#network-architecture\">previous post</a> on this topic (the main difference is that we don’t use the second layer of biLSTM).</p>\n\n<h2 id=\"analyzing-the-neurons\">Analyzing the neurons</h2>\n\n<p>We tried to answer the following questions:</p>\n<ul>\n  <li>How does the network handle interesting cases with several possible outcomes (e.g. <code class=\"highlighter-rouge\">r</code> =&gt; <code class=\"highlighter-rouge\">ր</code> vs <code class=\"highlighter-rouge\">ռ</code> etc.)?</li>\n  <li>What are the problems particular neurons are helping solve?</li>\n</ul>\n\n<h3 id=\"how-does-t-become-ծ\">How does “t” become “ծ”?</h3>\n\n<p>First, we fixed one particular character for the input and one for the output.\nFor example we are interested in how <code class=\"highlighter-rouge\">t</code> becomes <code class=\"highlighter-rouge\">ծ</code> (we know <code class=\"highlighter-rouge\">t</code> can become <code class=\"highlighter-rouge\">տ</code>, <code class=\"highlighter-rouge\">թ</code> or <code class=\"highlighter-rouge\">ծ</code>). We now that it usually happens when <code class=\"highlighter-rouge\">t</code> appears in a bigram <code class=\"highlighter-rouge\">ts</code>, which should be converted to <code class=\"highlighter-rouge\">ծ_</code>.</p>\n\n<p>For every neuron, we draw the histograms of its activations in cases where the correct output is <code class=\"highlighter-rouge\">ծ</code>, and where the correct output is <em>not</em> <code class=\"highlighter-rouge\">ծ</code>. For most of the neurons these two histograms are pretty similar, but there are cases like this:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center\">Input = <code class=\"highlighter-rouge\">t</code>, Output = <code class=\"highlighter-rouge\">ծ</code></th>\n      <th style=\"text-align: center\">Input = <code class=\"highlighter-rouge\">t</code>,  Output != <code class=\"highlighter-rouge\">ծ</code></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center\"><img src=\"http://yerevann.github.io/public/2017-06-27/ts.png\" alt=\"\" /></td>\n      <td style=\"text-align: center\"><img src=\"http://yerevann.github.io/public/2017-06-27/chts.png\" alt=\"\" /></td>\n    </tr>\n  </tbody>\n</table>\n\n<p>These histograms show that by looking at the activation of this particular neuron we can guess with high accuracy whether the output for <code class=\"highlighter-rouge\">t</code> is <code class=\"highlighter-rouge\">ծ</code>. To quantify the difference between the two histograms we used <a href=\"https://en.wikipedia.org/wiki/Hellinger_distance\">Hellinger distance</a> (we take the minimum and maximum values of neuron activations, split the range into 1000 bins and apply discrete Hellinger distance formula on two histograms). We calculated this distance for all neurons and visualized the most interesting ones in a single image:</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/t-ծ.png\" alt=\"t=&gt;ծ\" /></p>\n\n<p>The color of a neuron indicates the distance between its two histograms (darker colors correspond to larger distances). The width of a line between two neurons indicate the mean of the value that the neuron on the lower end of the connection contributes to the neuron on the higher end. Orange and green lines correspond to positive and negative signals, respectively.</p>\n\n<p>The neurons at the top of the image are from the output layer, the neurons below the output layer are from the hidden layer (top 12 neurons in terms of the distance between histograms). Concat layer comes under the hidden layer. The neurons of the concat layer are split into two parts: the left half of the neurons are the outputs of the LSTM that goes forward on the input sequence and the right half contains the neurons from the LSTM that goes backwards. From each LSTM we display top 10 neurons in terms of the distance between histograms.</p>\n\n<p>In the case of <code class=\"highlighter-rouge\">t</code> =&gt; <code class=\"highlighter-rouge\">ծ</code>, it is obvious that all top 12 neurons of the hidden layer pass positive signals to <code class=\"highlighter-rouge\">ծ</code> and <code class=\"highlighter-rouge\">ց</code> (another Armenian character that is often romanized as <code class=\"highlighter-rouge\">ts</code>), and pass negative signals to <code class=\"highlighter-rouge\">տ</code>, <code class=\"highlighter-rouge\">թ</code> and others.</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/t-ծ-concat.png\" alt=\"t=&gt;ծ - concat layer\" /></p>\n\n<p>We can also see that the outputs of the right-to-left LSTM are darker, which implies that these neurons “have more knowledge” about whether to predict <code class=\"highlighter-rouge\">ծ</code>. On the other hand, the lines between those neurons and the hidden layer are thicker, which means that they have more contribution in activating the top 12 neurons in the hidden layer. This is a very natural result, because we know that <code class=\"highlighter-rouge\">t</code> usually becomes <code class=\"highlighter-rouge\">ծ</code> when the <em>next</em> symbol is <code class=\"highlighter-rouge\">s</code>, and only the right-to-left LSTM is aware of the next character.</p>\n\n<p>We did the same analysis for the neurons and gates inside the LSTMs. The results are visualized as six rows of neurons at the bottom of the image. In particular, it is interesting to note that the most “confident” neurons are the so called <em>cell inputs</em>. Recall that cell inputs, as well as all the gates, depend on the input at the current step and the hidden state of the previous step (which is the hidden state at the <em>next</em> character as we talk about the right-to-left LSTM), so all of them are “aware” of the next <code class=\"highlighter-rouge\">s</code>, but for some reason cell inputs are more confident than others.</p>\n\n<p>In the cases where <code class=\"highlighter-rouge\">s</code> should be transliterated into <code class=\"highlighter-rouge\">_</code> (the placeholder), the useful information is more likely to come from the LSTM that goes forward, as <code class=\"highlighter-rouge\">s</code> becomes <code class=\"highlighter-rouge\">_</code> mainly in case of <code class=\"highlighter-rouge\">ts</code> =&gt; <code class=\"highlighter-rouge\">ծ_</code>. We see that in the next plot:</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/s-_.png\" alt=\"s=&gt;placeholder\" /></p>\n\n<h3 id=\"what-did-this-neuron-learn\">What did this neuron learn?</h3>\n\n<p>In the second part of our analysis we tried to figure out in which ambiguous cases each of the neurons is most helpful. We took the set of Latin characters that can be transliterated into more than one Armenian letters. Then we removed the cases where one of the possible outcomes appears less than 300 times in our 5000 sample sentences, because our distance metric didn’t seem to work well with few samples. And we analyzed every fixed neuron for every possible input-output pair.</p>\n\n<p>For example, here is the analysis of the neuron #70 of the output layer of the left-to-right LSTM. We have seen in the previous visualization that it helps determining whether <code class=\"highlighter-rouge\">s</code> should be transliterated into <code class=\"highlighter-rouge\">_</code>. We see that the top input-output pairs for this neuron are the following:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center\">Hellinger distance</th>\n      <th style=\"text-align: center\">Latin character</th>\n      <th style=\"text-align: center\">Armenian character</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center\">0.9482</td>\n      <td style=\"text-align: center\">s</td>\n      <td style=\"text-align: center\">_</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">0.8285</td>\n      <td style=\"text-align: center\">h</td>\n      <td style=\"text-align: center\">հ</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">0.8091</td>\n      <td style=\"text-align: center\">h</td>\n      <td style=\"text-align: center\">_</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">0.6125</td>\n      <td style=\"text-align: center\">o</td>\n      <td style=\"text-align: center\">օ</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>So this neuron is most helpful when predicting <code class=\"highlighter-rouge\">_</code> from <code class=\"highlighter-rouge\">s</code> (as we already knew), but it also helps to determine whether Latin <code class=\"highlighter-rouge\">h</code> should be transliterated as Armenian <code class=\"highlighter-rouge\">հ</code> or the placeholder <code class=\"highlighter-rouge\">_</code> (e.g. Armenian <code class=\"highlighter-rouge\">չ</code> is usually romanized as <code class=\"highlighter-rouge\">ch</code>, so <code class=\"highlighter-rouge\">h</code> sometimes becomes <code class=\"highlighter-rouge\">_</code>).</p>\n\n<p>We visualize Hellinger distances of the histograms of neuron activations when the input is <code class=\"highlighter-rouge\">h</code> and the output is <code class=\"highlighter-rouge\">_</code>, and see that the neuron #70 is among the top 10 neurons of the left-to-right LSTM for the <code class=\"highlighter-rouge\">h</code>=&gt;<code class=\"highlighter-rouge\">_</code> pair.</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/h-_.png\" alt=\"h=&gt;placeholder\" /></p>\n\n<h2 id=\"visualizing-lstm-cells\">Visualizing LSTM cells</h2>\n\n<p>Inspired by <a href=\"https://arxiv.org/abs/1506.02078\">this paper</a> by Andrej Karpathy, Justin Johnson and Fei-Fei Li, we tried to find neurons or LSTM cells specialised in some language specific patterns in the sequences. In particular, we tried to find the neurons that react most to the suffix <code class=\"highlighter-rouge\">թյուն</code> (romanized as <code class=\"highlighter-rouge\">tyun</code>).</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/utyun1.png\" alt=\"tyun\" /></p>\n\n<p>The first row of this visualization is the output sequence. Rows below show the activations of the most interesting neurons:</p>\n<ol>\n  <li>Cell #6 in the LSTM that goes backwards,</li>\n  <li>Cell #147 in the LSTM that goes forward,</li>\n  <li>37th neuron in the hidden layer,</li>\n  <li>78th neuron in the concat layer.</li>\n</ol>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/utyun2.png\" alt=\"tyun\" /></p>\n\n<p>We can see that Cell #6 is active on <code class=\"highlighter-rouge\">tyun</code>s and is not active on the other parts of the sequence. Cell #144 of the forward LSTM behaves the opposite way, it is active on everything except <code class=\"highlighter-rouge\">tyun</code>s.</p>\n\n<p>We know that <code class=\"highlighter-rouge\">t</code> in the suffix <code class=\"highlighter-rouge\">tyun</code> should always become <code class=\"highlighter-rouge\">թ</code> in Armenian, so we thought that if a neuron is active on <code class=\"highlighter-rouge\">tyun</code>s, it may help in determining whether the Latin <code class=\"highlighter-rouge\">t</code> should be transliterated as <code class=\"highlighter-rouge\">թ</code> or <code class=\"highlighter-rouge\">տ</code>. So we visualized the most important neurons for the pair <code class=\"highlighter-rouge\">t</code> =&gt; <code class=\"highlighter-rouge\">թ</code>.</p>\n\n<p><img src=\"http://yerevann.github.io/public/2017-06-27/t-թ.png\" alt=\"t-&gt;թ\" /></p>\n\n<p>Indeed, Cell #147 in the forward LSTM is among the top 10.</p>\n\n<h2 id=\"concluding-remarks\">Concluding remarks</h2>\n\n<p>Interpretability of neural networks remains an important challenge in machine learning. CNNs and LSTMs perform well for many learning tasks, but there are very few tools to understand the inner workings of these systems. Transliteration is a pretty good problem for analyzing the impact of particular neurons.</p>\n\n<p>Our experiments showed that too many neurons are involved in the “decision making” even for the simplest cases, but it is possible to identify a subset of neurons that have more influence than the rest. On the other hand, most neurons are involved in multiple decision making processes depending on the context. This is expected, since nothing in the loss functions we use when training neural nets forces the neurons to be independent and interpretable. Recently, there have been <a href=\"https://arxiv.org/abs/1606.03657\">some attempts</a> to apply information-theoretic regularization terms in order to obtain more interpretability. It would be interesting to test those ideas in the context of transliteration.</p>\n\n<p>We would like to thank Adam Mathias Bittlingmayer and Zara Alaverdyan for  helpful comments and discussions.</p>"
}