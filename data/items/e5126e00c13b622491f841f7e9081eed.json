{
  "title": "Clustering applied to showers in the OPERA",
  "description": "<!-- TODO: tags for social networks -->\n<p>\n\t<i>\n\t\tAbstract: in this post I discuss clustering: \n\t\ttechniques that form this method and some peculiarities of using clustering in practice.\n\t\tThis post continues <a href='/2017/06/24/opera.html'>previous one</a> about the OPERA.\n\t</i>\n</p>\n\n<p>\n\t<strong>Update:</strong> now you can play with a 3-dimensional\n\t<a href=\"https://arogozhnikov.github.io/clustering_visualizations/\">visualization of clustering</a>.\n\n</p>\n<p>\n\t<img src='/images/opera/post/opera-step3.png' style='margin: 20px 0px;' />\n</p>\n\n<h2>\n\tWhat is clustering and when it is needed?\n</h2>\n<p>\n\tClustering is a typical problem of unsupervised machine learning. \n\tGiven a set of objects (also called observations), split them into groups (called clusters) so that objects in each group are more similar to each other \n\tthan to observations from other groups.\n</p>\n<p>\n\tClustering may become the right tool to identify structure of the data.\n\tFor instance, cluster analysis helped in 1998 to identify that <a href='https://en.wikipedia.org/wiki/Gamma-ray_burst'>gamma ray bursts</a> are falling quite nicely in <a href='http://cds.cern.ch/record/345177/files/9802085.pdf'>three</a> groups (clusters), not two as was thought before.\n\tSo we got a hint that there are three types of processes in far galaxies which result in energetic explosions. \n\tNow properties of each group can be analyzed individually and we can try to guess processes behind each type of bursts.\n</p>\n<p>\n\tFinding groups of users / customers / orders with clustering may turn out to be a good idea (and it may not, there are many factors).\n</p>\n<p>\n\tLet me first remind a bit about frequently used clustering methods.\n</p>\n<!--<h2>When clustering is <i>not</i> needed?</h2>\n<p>\n\tWhen you can apply directly classification technique, just do it &mdash; supervised learning is definitely going to work\n\tbetter\n</p>-->\n<h2>K-means clustering</h2>\n<p>\n\tK-means clustering (<a href='https://en.wikipedia.org/wiki/K-means_clustering'>wiki</a>) is probably the simplest approach to clustering data. \n\tThe number of clusters $k$ is predefined, and each cluster is described by its center (mean). \n\tAlgorithm starts from randomly defined positions of cluster centers, and iteratively\n\trefine their positions by repeating the following two steps\n</p>\n<ul>\n\t<li>\n\t\t<strong>Assignment:</strong> each observation is assigned to the cluster with the nearest center\n\t</li>\n\t<li>\n\t\t<strong>Update:</strong> center of each cluster is updated to the mean of observations in the cluster\n\t</li>\n</ul>\n<p>\n\t<!--Notably, both of these steps are minimizing so-called within-cluster variance.-->\n\tYou can track the described training process in the following animation:\n</p>\n<center>\n\t<img src='/images/opera/post/clustering-kmeans-circles.gif' alt='Example of clustering with k-means algorithm' style='margin: 20px 0px;' />\n</center>\n<small>\n\tExample of clustering with k-means algorithm. Points are observation to be clustered. \n\tExample dataset consists of 7 close clusters (circles).\n\tEach cluster is shown with its own color, cluster centers are shown with big circles.\n\t \n\tProcess converges to a local minimum after several iterations (note that nothing changes on last iterations).\n\tThis is a screen capture of a <a href=\"https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\">demonstration</a> by Naftali Harris.\n</small>\n<h2>Limitations of k-means clustering </h2>\n<p>\n\tHowever this simple algorithm in many cases fails to provide good clustering:\n</p>\n<center>\n\t<img src='/images/opera/post/clustering-kmeans-smiley.gif' style='margin: 20px 0px;' />\n</center>\n<p>\n\tThere is a fundamental limitation which prevents k-means from properly clustering above example: \n\tk-means clusters are convex polytopes (you can notice this from both demonstrations, but it can be proven quite easily). \n\t<!--TODO не надо  -->\n\tMoreover, k-means clustering partitions the space into so-called <a href='https://en.wikipedia.org/wiki/Voronoi_diagram'>Voronoi diagram</a>.\n</p>\n<p>\n\tAnother demerit frequently mentioned is that k-means creates clusters of comparable spatial extent \n\tand also isn't capable of creating clusters with covariance matrix very different from identity.\n</p>\n\n<h2>DBSCAN</h2>\n<p>\n\tThere are more sophisticated approaches to clustering which overcome these limitations of k-means to some extend. \n\tOne of such methods is <a href='https://en.wikipedia.org/wiki/DBSCAN#Preliminary'>DBSCAN</a> (Density-based spatial clustering of applications with noise), which is also quite popular. \n</p>\n<p>\n\tThe following animation gives some vague understanding about procedure behind DBSCAN clustering:\n</p>\n<img src='/images/opera/post/clustering-dbscan-smiley.gif' style='margin: 20px 0px;' />\n<p>\n\tDBSCAN starts from core points, that is points with several quite close neighbours and creates the cluster by adding closest points. \n\tIf one of added nearest points is also a core points (has sufficient number of close neighbours), \n\tall of its neighbours are also added (and this repeats recursively).\n</p>\n<p>\n\tThis way DBSCAN is capable of finding complex clusters, quite different in shapes and sizes, \n\tbut the clusters don't have any parametric description as in other methods \n\t(for instance, in k-means cluster is modeled by its center position).\n</p>\n<p>\n\tDBSCAN also handles outliers, i.e. observations that do not seem to belong to any clusters. \n\tIn the clustering of OPERA basetracks that I discuss below it is quite important, as many noise tracks present in the data.\n</p>\n<h2>\n\tVisual comparison of algorithms in scikit-learn\n</h2>\n<p>\n\tScikit-learn package <a href='http://scikit-learn.org/stable/modules/clustering.html'>documentation</a> \n\t(most popular machine learning package for python) has a very concise and informative\n\tvisual comparison of different clustering methods:\n</p>\n<img src='/images/opera/post/clustering-comparison.png' alt='visual comparison of clustering algorithm by sklearn' />\n<p>\n\tPlease refer to that documentation page when in trouble and need to choose the right clustering method for a particular problem.\n</p>\n<h2>Back to the OPERA example</h2>\n<p>\n\tLet's now get back to the problem at hand. \n\tIn <a href='/2017/06/24/opera.html'>the previous post</a> we started from the situation \n\twhen a brick from the OPERA experiment is developed and scanned.\n\tScanning procedure outputs millions of basetrack, most of which are instrumental background and should be removed. \n</p>\n<p>\n\tAs it was discussed in the previous post, we can clear background and leave only several thousands of tracks \n\tthanks to classification techniques and proper feature engineering.\n</p>\n<img src='/images/opera/post/opera_clustering_image1.png' alt='non-cluster data' />\n<p>\n\tNow we have several thousands tracks and the goal is to find patterns in this data.\n</p>\n<h2>\n\tImportance of distance choice in applications\n</h2>\n<p>\n\tImportant detail that I want to dwell upon in this post is how do we measure distance between tracks \n\t(also called <i>metric function</i> or simply <i>metric</i>).\n\tClustering techniques are unsupervised, and it sounds like they get useful information <i>out of nowhere</i>.\n\tUnfortunately, there are no miracles: information is obtained from the structure and provided metric function, \n\twhich makes it possible to find similarity between items or observations.\n</p>\n<p>\n\tLet's see this in example: what if we simply take euclidean distance and apply DBSCAN clustering?\n\t(in this approach we treat each track as a point in 3-dimensional space defined by its position)\n</p>\n<img src='/images/opera/post/opera_clustering_image2.png' />\n<p>\n\tThe result is disappointing: found clusters do not correspond to any tracks, \n\tthose are just groups of tracks placed nearby. \n\tAn algorithm had no chance to reveal anything useful given the way we defined similarity between tracks.\n\tYou can see from the scheme below that closest basetracks in euclidean distance rarely belong to the same pattern, \n\twhile basetracks left by same particle can be quite far from each other:\n</p>\n<img src='/images/opera/post/opera-distance1.png' style=\"margin: 20px;\"/>\n<p>\n\tOur clustering should be stable to appearance of noise in the data, \n\tin particular noise basetracks that were left in the data should not be attributed \n\tto any cluster and marked by algorithm as outliers (DBSCAN handles this situation as was shown in animation).\n</p>\n<p>\n\tAnother problem in the data is missing links (as shown in the scheme above), when one of the basetracks \n\tis missing and algorithm should still consider two parts to the right and to the left as a single pattern.\n\tIt is problematic, because parts are spaced apart.\n</p>\n\n<h2>\n\tAccounting for directions of basetracks\n</h2>\n<p>\n\tEuclidean distance that we used in previous example can be written as:\n\t$$ \\rho(\\text{track}_1, \\text{track}_2)^2 = (x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2,  $$\n\tand can be written shortly as \n\t$$ \\rho(\\text{track}_1, \\text{track}_2)^2 = || \\mathbf{x}_1 - \\mathbf{x}_2 ||^2.  $$ \n\tEach basetrack is described by its position $\\mathbf{x} = (x, y, z)$ \n\tand direction $ \\mathbf{e} = (e_1, e_2, e_3)$. \n\tDirection is completely ignored by previous distance, but plays very important role &mdash; \n\tas we know, basetracks left by same particle, typically have very close directions. \n</p>\n<p>\n\tTo account for both positions and directions of the track we can use the following metric:\n\t$$ \\rho(\\text{track}_1, \\text{track}_2)^2 = || \\mathbf{x}_1 - \\mathbf{x}_2 ||^2 + \\alpha || e_1 - e_2 ||^2. $$\n\tBelow we have the result of clustering with DBSCAN and new metric:\n</p>\n<img src='/images/opera/post/opera_clustering_image3.png' />\n<p>\n\tThe clustering became much more informative: clearly, one can see now parts of the tracks, \n\thowever most of the groups include more than one track, \n\tand some of the outlier basetracks are included in clusters erroneously.\n</p>\n<img src='/images/opera/post/opera-distance2.png' style=\"margin: 20px;\"/>\n<p>\n\tStill, there is some major problem with our distance, in particular, the basetracks \n\tcan be far from each other in euclidean distance and still (quite obviously) belong to a single track or shower.\n\tMetric functions used so far require that 'similar' tracks should be close in euclidean distance, \n\tand thus lost base tracks are a problem.\n</p>\n\n<h2>\n\tAlignment metric\n</h2>\n<p>\n\tTo address named issues I introduced the following <i>alignment distance function</i>\n</p>\n<img src='/images/opera/post/opera-distance3.png' style=\"margin: 20px;\"/>\n<ul>\n\t<li>for a couple of basetracks two planes are considered: one to the left of both tracks and one to the right</li>\n\t<li>for both basetrack intersections are found between planes and basetracks'' direction lines</li>\n\t<li>distances between intersection points $\\text{IP}_1$ and $\\text{IP}_2$ are computed</li>\n\t<li>distance is defined with (strictly speaking, this is not exactly distance, but DBSCAN is fine with this)\n\t\t$$ \\rho(\\text{track}_1, \\text{track}_2 )^2 = \\text{IP}^2_1 + \\text{IP}^2_2, $$\n\t\tso the basetracks are considered close when both IPs are small (which is the case when basetracks are on the same line)\n\t<li>the planes selected so that both tracks are between planes and there is also some additional margin. \n\t\tIt is needed in particular to add some distance to very close basetracks from the same layer, but with different directions\n\t</li>\n</ul>\n<h2>\n\tFinal result\n</h2>\n<p>\n\tThat's how alignment metric + DBSCAN perform:\n</p>\n<img src='/images/opera/post/opera_clustering_image4.png' />\n<p>\n\tSome of the found clusters are shown in details below, \n\tyou can notice that it was able to reconstruct the cluster well even when significant part of the shower \n\tis missing in the data (dark blue cluster of two parts is a single shower) or when tracks left by particles have missing basetracks in the middle\n</p>\n<img src='/images/opera/post/opera_clustering_selected.png' />\n<h2>\n\tTake-home message\n</h2>\n<p>\n\tWhen data analysis approach you use relies on distance \n\t(that's almost all the clustering algorithms, but also neighbours-based methods and KDE), \n\tbe sure to choose distance function wisely (using your prior knowledge about the problem), because this affects result very significantly.\n</p>\n<h2>\n\tReferences\n</h2>\n<ul>\n\t<li>\n\t\tDemonstrations of clustering: <a href='https://www.naftaliharris.com/blog/visualizing-k-means-clustering/'>k-means</a> \n\t\tand  <a href='https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/'>DBSCAN</a> by Naftali Harris\n\t</li>\n\t<li>\n\t\t<a href='http://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html'>Awesome machine learning demonstrations </a>\n\t</li>\n\t<li>\n\t\t<a href='https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf'>DBSCAN paper</a>\n\t</li>\n\t<li>\n\t\t<a href='https://www.toptal.com/machine-learning/clustering-algorithms'>Blogpost with animations of clustering algorithms</a>\t\tby Lovro Iliassich.\n\t</li>\n</ul>",
  "pubDate": "Mon, 10 Jul 2017 00:00:00 +0000",
  "link": "https://arogozhnikov.github.io/2017/07/10/opera-clustering.html",
  "guid": "https://arogozhnikov.github.io/2017/07/10/opera-clustering.html",
  "category": [
    "machine learning",
    "OPERA",
    "particle physics"
  ]
}