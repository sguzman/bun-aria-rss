{
  "title": "Building SAGA optimization for Dask arrays",
  "link": "",
  "updated": "2018-08-07T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/08/07/incremental-saga",
  "content": "<p><em>This work is supported by <a href=\"https://www.ethz.ch/en.html\">ETH Zurich</a>, <a href=\"http://anaconda.com\">Anaconda\nInc</a>, and the <a href=\"https://bids.berkeley.edu/\">Berkeley Institute for Data\nScience</a></em></p>\n\n<p>At a recent Scikit-learn/Scikit-image/Dask sprint at BIDS, <a href=\"http://fa.bianp.net\">Fabian Pedregosa</a> (a\nmachine learning researcher and Scikit-learn developer) and Matthew\nRocklin (Dask core developer) sat down together to develop an implementation of the incremental optimization algorithm\n<a href=\"https://arxiv.org/pdf/1407.0202.pdf\">SAGA</a> on parallel Dask datasets. The result is a sequential algorithm that can be run on any dask array, and so allows the data to be stored on disk or even distributed among different machines.</p>\n\n<p>It was interesting both to see how the algorithm performed and also to see\nthe ease and challenges to run a research algorithm on a Dask distributed dataset.</p>\n\n<h3 id=\"start\">Start</h3>\n\n<p>We started with an initial implementation that Fabian had written for Numpy\narrays using Numba. The following code solves an optimization problem of the form</p>\n\n\\[min_x \\sum_{i=1}^n f(a_i^t x, b_i)\\]\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numba</span> <span class=\"kn\">import</span> <span class=\"n\">njit</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model.sag</span> <span class=\"kn\">import</span> <span class=\"n\">get_auto_step_size</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.utils.extmath</span> <span class=\"kn\">import</span> <span class=\"n\">row_norms</span>\n\n<span class=\"o\">@</span><span class=\"n\">njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">deriv_logistic</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"c1\"># derivative of logistic loss\n</span>    <span class=\"c1\"># same as in lightning (with minus sign)\n</span>    <span class=\"n\">p</span> <span class=\"o\">*=</span> <span class=\"n\">y</span>\n    <span class=\"k\">if</span> <span class=\"n\">p</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">phi</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">p</span><span class=\"p\">))</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">exp_t</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">)</span>\n        <span class=\"n\">phi</span> <span class=\"o\">=</span> <span class=\"n\">exp_t</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mf\">1.</span> <span class=\"o\">+</span> <span class=\"n\">exp_t</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">phi</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">y</span>\n\n<span class=\"o\">@</span><span class=\"n\">njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">SAGA</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n  <span class=\"s\">\"\"\"\n  SAGA algorithm\n\n  A : n_samples x n_features numpy array\n  b : n_samples numpy array with values -1 or 1\n  \"\"\"</span>\n\n    <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">n_features</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">memory_gradient</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"p\">)</span>\n    <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>  <span class=\"c1\"># vector of coefficients\n</span>    <span class=\"n\">step_size</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span> <span class=\"o\">*</span> <span class=\"n\">get_auto_step_size</span><span class=\"p\">(</span><span class=\"n\">row_norms</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">squared</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">(),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s\">'log'</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"c1\"># sample randomly\n</span>        <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">memory_gradient</span><span class=\"p\">.</span><span class=\"n\">size</span><span class=\"p\">)</span>\n        <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># .. inner iteration ..\n</span>        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">idx</span><span class=\"p\">:</span>\n            <span class=\"n\">grad_i</span> <span class=\"o\">=</span> <span class=\"n\">deriv_logistic</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"n\">b</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n\n            <span class=\"c1\"># .. update coefficients ..\n</span>            <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">grad_i</span> <span class=\"o\">-</span> <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"n\">x</span> <span class=\"o\">-=</span> <span class=\"n\">step_size</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">delta</span> <span class=\"o\">+</span> <span class=\"n\">gradient_average</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># .. update memory terms ..\n</span>            <span class=\"n\">gradient_average</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">grad_i</span> <span class=\"o\">-</span> <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">n_samples</span>\n            <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">grad_i</span>\n\n        <span class=\"c1\"># monitor convergence\n</span>        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">'gradient norm:'</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">linalg</span><span class=\"p\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gradient_average</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n</code></pre></div></div>\n\n<p>This implementation is a simplified version of the <a href=\"https://github.com/openopt/copt/blob/master/copt/randomized.py\">SAGA\nimplementation</a>\nthat Fabian uses regularly as part of his research, and that assumes that  $f$ is the <a href=\"https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss\">logistic loss</a>, i.e., $f(z) = \\log(1 + \\exp(-z))$. It can be used to solve problems with other values of $f$ by overwriting the function <code>deriv_logistic</code>.</p>\n\n<p>We wanted to apply it across a parallel Dask array by applying it to each chunk of the Dask array, a smaller Numpy array, one at a time, carrying along a set of parameters along the way.</p>\n\n<h3 id=\"development-process\">Development Process</h3>\n\n<p>In order to better understand the challenges of writing Dask algorithms, Fabian\ndid most of the actual coding to start.  Fabian is good example of a researcher who\nknows how to program well and how to design ML algorithms, but has no direct\nexposure to the Dask library.  This was an educational opportunity both for\nFabian and for Matt.  Fabian learned how to use Dask, and Matt learned how to\nintroduce Dask to researchers like Fabian.</p>\n\n<h3 id=\"step-1-build-a-sequential-algorithm-with-pure-functions\">Step 1: Build a sequential algorithm with pure functions</h3>\n\n<p>To start we actually didn’t use Dask at all, instead, Fabian modified his implementation in a few ways:</p>\n\n<ol>\n  <li>It should operate over a list of Numpy arrays. A list of Numpy arrays is similar to a Dask array, but simpler.</li>\n  <li>It should separate blocks of logic into separate functions, these will\neventually become tasks, so they should be sizable chunks of work. In this\ncase, this led to the creating of the function <code class=\"language-plaintext highlighter-rouge\">_chunk_saga</code> that\nperforms an iteration of the SAGA algorithm on a subset of the data.</li>\n  <li>These functions should not modify their inputs, nor should they depend on\nglobal state.  All information that those functions require (like\nthe parameters that we’re learning in our algorithm) should be\nexplicitly provided as inputs.</li>\n</ol>\n\n<p>These requested modifications affect performance a bit, we end up making more\ncopies of the parameters and more copies of intermediate state.  In terms of\nprogramming difficulty this took a bit of time (around a couple hours) but is a\nstraightforward task that Fabian didn’t seem to find challenging or foreign.</p>\n\n<p>These changes resulted in the following code:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">numba</span> <span class=\"kn\">import</span> <span class=\"n\">njit</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.utils.extmath</span> <span class=\"kn\">import</span> <span class=\"n\">row_norms</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model.sag</span> <span class=\"kn\">import</span> <span class=\"n\">get_auto_step_size</span>\n\n\n<span class=\"o\">@</span><span class=\"n\">njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">_chunk_saga</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">f_deriv</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradient</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Make explicit copies of inputs\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n    <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">gradient_average</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n    <span class=\"n\">memory_gradient</span> <span class=\"o\">=</span> <span class=\"n\">memory_gradient</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Sample randomly\n</span>    <span class=\"n\">idx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">memory_gradient</span><span class=\"p\">.</span><span class=\"n\">size</span><span class=\"p\">)</span>\n    <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># .. inner iteration ..\n</span>    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">idx</span><span class=\"p\">:</span>\n        <span class=\"n\">grad_i</span> <span class=\"o\">=</span> <span class=\"n\">f_deriv</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"n\">b</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># .. update coefficients ..\n</span>        <span class=\"n\">delta</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">grad_i</span> <span class=\"o\">-</span> <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        <span class=\"n\">x</span> <span class=\"o\">-=</span> <span class=\"n\">step_size</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">delta</span> <span class=\"o\">+</span> <span class=\"n\">gradient_average</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># .. update memory terms ..\n</span>        <span class=\"n\">gradient_average</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">grad_i</span> <span class=\"o\">-</span> <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">n_samples</span>\n        <span class=\"n\">memory_gradient</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">grad_i</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradient</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">full_saga</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">callback</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n  <span class=\"s\">\"\"\"\n  data: list of (A, b), where A is a n_samples x n_features\n  numpy array and b is a n_samples numpy array\n  \"\"\"</span>\n    <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n        <span class=\"n\">n_samples</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">n_features</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">memory_gradients</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">]</span>\n    <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>\n\n    <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">get_auto_step_size</span><span class=\"p\">(</span><span class=\"n\">row_norms</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">squared</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">(),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s\">'log'</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">]</span>\n    <span class=\"n\">step_size</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">steps</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">_chunk_saga</span><span class=\"p\">(</span>\n                    <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">deriv_logistic</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span>\n                    <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">callback</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">callback</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n</code></pre></div></div>\n\n<h3 id=\"step-2-apply-daskdelayed\">Step 2: Apply dask.delayed</h3>\n\n<p>Once functions neither modified their inputs nor relied on global state we went\nover a <a href=\"https://mybinder.org/v2/gh/dask/dask-examples/master?filepath=delayed.ipynb\">dask.delayed example</a>,\nand then applied the <code class=\"language-plaintext highlighter-rouge\">@dask.delayed</code> decorator to the functions that\nFabian had written.  Fabian did this at first in about five minutes and to our\nmutual surprise, things actually worked</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">nout</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>                               <span class=\"c1\"># &lt;&lt;&lt;---- New\n</span><span class=\"o\">@</span><span class=\"n\">njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">_chunk_saga</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">f_deriv</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradient</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">full_saga</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">callback</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n        <span class=\"n\">n_samples</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">data</span><span class=\"p\">)</span>                      <span class=\"c1\"># &lt;&lt;&lt;---- New\n</span>\n    <span class=\"p\">...</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">_chunk_saga</span><span class=\"p\">(</span>\n                    <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">deriv_logistic</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span>\n                    <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">)</span>\n        <span class=\"n\">cb</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">callback</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span>        <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>\n        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">cb</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">cb</span><span class=\"p\">)</span>                 <span class=\"c1\"># &lt;&lt;&lt;---- New\n</span>        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">cb</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>However, they didn’t work <em>that well</em>.  When we took a look at the dask\ndashboard we find that there is a lot of dead space, a sign that we’re still\ndoing a lot of computation on the client side.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/saga-1.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/saga-1.png\" width=\"90%\" />\n</a></p>\n\n<h3 id=\"step-3-diagnose-and-add-more-daskdelayed-calls\">Step 3: Diagnose and add more dask.delayed calls</h3>\n\n<p>While things worked, they were also fairly slow.  If you notice the\ndashboard plot above you’ll see that there is plenty of white in between\ncolored rectangles.  This shows that there are long periods where none of the\nworkers is doing any work.</p>\n\n<p>This is a common sign that we’re mixing work between the workers (which shows\nup on the dashbaord) and the client.  The solution to this is usually more\ntargetted use of dask.delayed.  Dask delayed is trivial to start using, but\ndoes require some experience to use well.  It’s important to keep track of\nwhich operations and variables are delayed and which aren’t.  There is some\ncost to mixing between them.</p>\n\n<p>At this point Matt stepped in and added delayed in a few more places and the\ndashboard plot started looking cleaner.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">nout</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>                               <span class=\"c1\"># &lt;&lt;&lt;---- New\n</span><span class=\"o\">@</span><span class=\"n\">njit</span>\n<span class=\"k\">def</span> <span class=\"nf\">_chunk_saga</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">f_deriv</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradient</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">):</span>\n    <span class=\"p\">...</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">full_saga</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">callback</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">:</span>\n        <span class=\"n\">n_samples</span> <span class=\"o\">+=</span> <span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">n_features</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">data</span><span class=\"p\">)</span>                      <span class=\"c1\"># &lt;&lt;&lt;---- New\n</span>    <span class=\"n\">memory_gradients</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">)(</span><span class=\"n\">A</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n                        <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">]</span>         <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>    <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">)(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>  <span class=\"c1\">#  Changed\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">)(</span><span class=\"n\">n_features</span><span class=\"p\">)</span>          <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>\n    <span class=\"n\">steps</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">get_auto_step_size</span><span class=\"p\">)(</span>\n                <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">row_norms</span><span class=\"p\">)(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">squared</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">).</span><span class=\"nb\">max</span><span class=\"p\">(),</span>\n                <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"s\">'log'</span><span class=\"p\">,</span> <span class=\"bp\">False</span><span class=\"p\">)</span>\n             <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"p\">]</span>                    <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>    <span class=\"n\">step_size</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span> <span class=\"o\">*</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nb\">min</span><span class=\"p\">)(</span><span class=\"n\">steps</span><span class=\"p\">)</span>   <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>\n    <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">gradient_average</span> <span class=\"o\">=</span> <span class=\"n\">_chunk_saga</span><span class=\"p\">(</span>\n                    <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">deriv_logistic</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span>\n                    <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">)</span>\n        <span class=\"n\">cb</span> <span class=\"o\">=</span> <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">callback</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"p\">)</span>        <span class=\"c1\"># &lt;&lt;&lt;---- Changed\n</span>        <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">,</span> <span class=\"n\">cb</span> <span class=\"o\">=</span> \\\n            <span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">memory_gradients</span><span class=\"p\">,</span> <span class=\"n\">gradient_average</span><span class=\"p\">,</span> <span class=\"n\">step_size</span><span class=\"p\">,</span> <span class=\"n\">cb</span><span class=\"p\">)</span>  <span class=\"c1\"># New\n</span>        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">cb</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">())</span>                         <span class=\"c1\"># &lt;&lt;&lt;---- changed\n</span>\n    <span class=\"k\">return</span> <span class=\"n\">x</span>\n</code></pre></div></div>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/saga-2.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/saga-2.png\" width=\"90%\" />\n</a></p>\n\n<p>From a dask perspective this now looks good.  We see that one <code class=\"language-plaintext highlighter-rouge\">partial_fit</code>\ncall is active at any given time with no large horizontal gaps between\n<code class=\"language-plaintext highlighter-rouge\">partial_fit</code> calls. We’re not getting any parallelism (this is just a\nsequential algorithm) but we don’t have much dead space.  The model seems to\njump between the various workers, processing on a chunk of data before moving\non to new data.</p>\n\n<h3 id=\"step-4-profile\">Step 4: Profile</h3>\n\n<p>The dashboard image above gives confidence that our algorithm is operating as\nit should.  The block-sequential nature of the algorithm comes out cleanly, and\nthe gaps between tasks are very short.</p>\n\n<p>However, when we look at the profile plot of the computation across all of our\ncores (Dask constantly runs a profiler on all threads on all workers to get\nthis information) we see that most of our time is spent compiling Numba code.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/saga-profile.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/saga-profile.png\" width=\"100%\" />\n</a></p>\n\n<p>We started a conversation for this on the <a href=\"https://github.com/numba/numba/issues/3026\">numba issue\ntracker</a> which has since been\nresolved.  That same computation over the same time now looks like this:</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/saga-3.png\">\n  <img src=\"https://mrocklin.github.io/blog/images/saga-3.png\" width=\"90%\" />\n</a></p>\n\n<p>The tasks, which used to take seconds, now take tens of milliseconds, so we can\nprocess through many more chunks in the same amount of time.</p>\n\n<h3 id=\"future-work\">Future Work</h3>\n\n<p>This was a useful experience to build an interesting algorithm.  Most of the\nwork above took place in an afternoon.  We came away from this activity\nwith a few tasks of our own:</p>\n\n<ol>\n  <li>Build a normal Scikit-Learn style estimator class for this algorithm\nso that people can use it without thinking too much about delayed objects,\nand can instead just use dask arrays or dataframes</li>\n  <li>Integrate some of Fabian’s research on this algorithm that improves performance with\n<a href=\"https://arxiv.org/pdf/1707.06468.pdf\">sparse data and in multi-threaded environments</a>.</li>\n  <li>Think about how to improve the learning experience so that dask.delayed can\nteach new users how to use it correctly</li>\n</ol>\n\n<h3 id=\"links\">Links</h3>\n\n<ul>\n  <li><a href=\"https://gist.github.com/5282dcf47505e2a1d214fd15c7da0ec3\">Notebooks for different stages of SAGA+Dask implementation</a></li>\n  <li><a href=\"https://github.com/scisprints/2018_05_sklearn_skimage_dask\">Scikit-Learn/Image + Dask Sprint issue tracker</a></li>\n  <li><a href=\"https://github.com/scisprints/2018_05_sklearn_skimage_dask\">Paper on SAGA algorithm</a></li>\n  <li><a href=\"https://github.com/openopt/copt/blob/master/copt/randomized.py\">Fabian’s more fully featured non-Dask SAGA implementation</a></li>\n  <li><a href=\"https://github.com/numba/numba/issues/3026\">Numba issue on repeated deserialization</a></li>\n</ul>"
}