{
  "id": "tag:blogger.com,1999:blog-15045980.post-7086735822667839051",
  "published": "2020-12-16T09:32:00.001-08:00",
  "updated": "2020-12-16T09:32:43.777-08:00",
  "category": [
    "",
    ""
  ],
  "title": "Test Flakiness - One of the main challenges of automated testing",
  "content": "By&nbsp;George Pirocanac<div><br /></div><div><br /></div><div>Dealing with test flakiness is a critical skill in testing because automated tests that do not provide a consistent signal will slow down the entire development process. If you haven’t encountered flaky tests, this article is a must-read as it first tries to systematically outline the causes for flaky tests. If you have encountered flaky tests, see how many fall into the areas listed.</div><div><br /></div><div><br /></div><div><div>A follow-up article will talk about dealing with each of the causes.</div></div><div><br /></div><div><br /></div><div>Over the years I’ve seen a lot of reasons for flaky tests, but rather than review them one by one, let’s group the sources of flakiness by the components under which tests are run:</div><div><ul style=\"text-align: left;\"><li>The tests themselves</li><li>The test-running framework</li><li>The application or system under Test (SUT) and the services and libraries that the SUT and testing framework depend upon</li><li>The OS and hardware that the SUT and testing framework depend upon</li></ul><div><br /></div></div><div><div>This is illustrated below. Figure 1 first shows the hardware/software stack that supports an application or system under test. At the lowest level is the hardware. The next level up is the operating system followed by the libraries that provide an interface to the system. At the highest level, is the middleware, the layer that provides application specific interfaces.</div></div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-oo5rt4SbFJc/X9ozDMCNMpI/AAAAAAAAAdU/iBuYZjMwN50y8qLuxzPmZDE9lvDN7P7UQCLcBGAsYHQ/s460/Test%2BFlakiness%2B-%2BFigure%2B1%2B%25281%2529.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"460\" data-original-width=\"439\" height=\"346\" src=\"https://1.bp.blogspot.com/-oo5rt4SbFJc/X9ozDMCNMpI/AAAAAAAAAdU/iBuYZjMwN50y8qLuxzPmZDE9lvDN7P7UQCLcBGAsYHQ/w330-h346/Test%2BFlakiness%2B-%2BFigure%2B1%2B%25281%2529.jpg\" width=\"330\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /><br /></div><div>In a distributed system, however, each of the services of the application and the services it depends upon can reside on a different hardware / software stack as can the test running service. This is illustrated in Figure 2 as the full test running environment.</div><div><br /></div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Olp-05IL-ro/X9o0buW7izI/AAAAAAAAAdo/jKZQNlo3-V4CpYeXPYl2cBnL3Ksm9EGowCLcBGAsYHQ/s726/Test%2BFlakiness%2B-%2BFigure%2B2%2B%25281%2529.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"520\" data-original-width=\"726\" height=\"392\" src=\"https://1.bp.blogspot.com/-Olp-05IL-ro/X9o0buW7izI/AAAAAAAAAdo/jKZQNlo3-V4CpYeXPYl2cBnL3Ksm9EGowCLcBGAsYHQ/w548-h392/Test%2BFlakiness%2B-%2BFigure%2B2%2B%25281%2529.jpg\" width=\"548\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div><br /></div><div><div>As discussed above, each of these components is a potential area for flakiness.</div></div><div><br /></div><div><br /></div><div><h3 style=\"text-align: left;\">The tests themselves</h3></div><div><br /></div><div><div>The tests themselves can introduce flakiness. Typical causes include:</div></div><div><ul style=\"text-align: left;\"><li>Improper initialization or cleanup.</li><li>Invalid assumptions about the state of test data.</li><li>Invalid assumptions about the state of the system. An example can be the system time.</li><li>Dependencies on the timing of the application.</li><li>Dependencies on the order in which the tests are run. (Similar to the first case above.)</li></ul></div><div><br /></div><div><br /></div><h3 style=\"text-align: left;\">The test-running framework</h3><div><br /></div><div>An unreliable test-running framework can introduce flakiness. Typical causes include:</div><div><br /></div><div><ul style=\"text-align: left;\"><li>Failure to allocate enough resources for the system under test thus causing it to fail coming up.&nbsp;</li><li>Improper scheduling of the tests so they “collide” and cause each other to fail.</li><li>Insufficient system resources to satisfy the test requirements.</li></ul></div><div><br /></div><h3 style=\"text-align: left;\">The application or system under test and the services and libraries that the SUT and testing framework depend upon</h3><div><br /></div><div><div>Of course, the application itself (or the system under test) could be the source of flakiness. An application can also have numerous dependencies on other services, and each of those services can have their own dependencies. In this chain, each of the services can introduce flakiness. Typical causes include:</div></div><div><ul style=\"text-align: left;\"><li>Race conditions.</li><li>Uninitialized variables.</li><li>Being slow to respond or being unresponsive to the stimuli from the tests.</li><li>Memory leaks.</li><li>Oversubscription of resources.</li><li>Changes to the application (or dependent services) happening at a different pace than those to the corresponding tests.</li></ul></div><div><br /></div><div>Testing environments are called <i><a href=\"https://testing.googleblog.com/2012/10/hermetic-servers.html\">hermetic</a></i> when they contain everything that is needed to run the tests (i.e. no external dependencies like servers running in production). Hermetic environments, in general, are less likely to be flaky.</div><div><br /></div><h3 style=\"text-align: left;\">The OS and hardware that the SUT and testing framework depend upon</h3><div><br /></div><div><br /></div><div>Finally, the underlying hardware and operating system can be the source of test flakiness. Typical causes include:</div><div><ul style=\"text-align: left;\"><li>Networking failures or instability.</li><li>Disk errors.</li><li>Resources being consumed by other tasks/services not related to the tests being run.</li></ul></div><div><br /></div><div><div>As can be seen from the wide variety of failures, having low flakiness in automated testing can be quite a challenge. This article has both outlined the areas and the types of flakiness that can occur in those areas, so it can serve as a cheat sheet when triaging flaky tests.</div><div><br /></div><div><br /></div><div>In the follow-up of this blog we’ll look at ways of addressing these issues.</div></div><div><br /></div><h3 style=\"text-align: left;\"><br /></h3><h3 style=\"text-align: left;\">References</h3><div><div><ul style=\"text-align: left;\"><li><a href=\"https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html\">Where do our flaky tests come from?</a></li><li><a href=\"https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html\">Flaky Tests at Google and How We Mitigate Them</a></li><li><a href=\"https://testing.googleblog.com/2009/06/my-selenium-tests-arent-stable.html\">My Selenium Tests Aren't Stable!</a></li><li><a href=\"https://testing.googleblog.com/2008/04/tott-avoiding-flakey-tests.html\">TotT: Avoiding Flakey Tests</a></li></ul></div></div><div><br /></div><div><br /></div><div><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google Testing Bloggers",
    "uri": "http://www.blogger.com/profile/03153388556673050910",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 9
}