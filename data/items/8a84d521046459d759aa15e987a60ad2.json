{
  "title": "Performance Impact of Parallel Disk Access",
  "link": "",
  "published": "2020-08-24T00:00:00+00:00",
  "updated": "2020-08-24T00:00:00+00:00",
  "id": "https://pkolaczk.github.io/disk-parallelism",
  "content": "<p>One of the well-known ways of speeding up a data processing task is partitioning the data into smaller\nchunks and processing the chunks in parallel. Let’s assume we can partition the task easily, or the input data is already \npartitioned into separate files which all reside on a single storage device. Let’s also assume the algorithm we run on those\ndata is simple enough so that the computation time is not a bottleneck. How much performance can we gain by reading the files in parallel? \nCan we lose any?</p>\n\n<!--more-->\n\n<script>\nvar colors = [\"rgba(230,140,35,0.8)\", \"rgba(130,20,0,0.8)\"]\n\nfunction makeBarChart(id, labels, data) {   \n    var ctx = document.getElementById(id).getContext('2d');\n    var datasets = [];\n    var colorIndex = 0;\n    for (var series in data) {        \n        datasets.push({\n            label: series,\n            backgroundColor: colors[colorIndex],        \n            barPercentage: 0.6,\n            data: data[series],\n        });\n        colorIndex++;\n    }\n\n    new Chart(ctx, {\n        type: 'horizontalBar',\n        data: {\n            labels: labels,\n            datasets: datasets,\n        },\n        options: {\n            maintainAspectRatio: false,\n            legend: { display: Object.keys(data).length  > 1 },\n            scales: {\n                yAxes: [{\n                    scaleLabel: { \n                        display: true,\n                        labelString: \"# threads\",\n                    },\n                }],\n                xAxes: [{\n                    scaleLabel: { \n                        display: true,\n                        labelString: \"time [s]\",\n                    },\n                    ticks: {\n                        beginAtZero: true\n                    }\n                }]\n            }    \n        }\n    });\n}    \n\nfunction makeBarChartDeferred(id, labels, data) {\n    document.addEventListener('readystatechange', event => {\n        if (event.target.readyState === \"complete\") {\n            makeBarChart(id, labels, data);\n        }\n    });      \n}\n\n</script>\n\n<p>While working on <a href=\"https://github.com/pkolaczk/fclones\">fclones</a> duplicate file finder,\nI’ve put a lot of effort into making it as fast as possible by leveraging capabilities of modern hardware.\nThat’s why I designed my program in a way that all data processing stages can be easily parallelized. \nThe newest version at the moment of writing this post (0.7.0) allows to set thread pools for random I/O \nand sequential I/O separately, and can adapt the settings to different types of storage devices.</p>\n\n<p>In this blog post I’m presenting the results of a few experiments I’ve made separately on SSD and HDD.\nAll the experiments were perfomed on either a Dell Precision 5520 laptop with a 4-core Xeon and a 512 NVMe SSD, from 2016, \nrunning Ubuntu Linux 20.04, or an older Dell M4600 with a 7200 RPM Toshiba HDD running Mint Linux 19.03.</p>\n\n<h1 id=\"ssd--metadata-and-random-reads\">SSD – Metadata and Random Reads</h1>\n<p>The most time-consuming part of the job is actually reading\nthe data from disk into memory in order to compute hashes. The number of files is typically large (thousands or even millions) \nand the problem of computing their hashes is embarrassingly parallel. \nThe first thing my duplicate finder does is scanning directory tree and fetching file metadata like file lenghts and inode identifiers. \nThis process issues a lot of random I/O requests. As expected, the performance gains from multithreading were huge, \nwhich is illustrated in Fig. 1.</p>\n\n<div class=\"figure\">\n    <div style=\"height:12.5em\"><canvas id=\"scanPerfSsd\"></canvas></div>\n    <script>\n    makeBarChartDeferred(\"scanPerfSsd\", \n        [1, 2, 4, 8, 16, 32],\n        {\"time\": [40.38, 19.18, 9.85, 5.74, 4.155, 3.64]});\n    </script>\n    <span class=\"caption\"> Fig.1: Time to fetch metadata of ~1.5 million file entries on an SSD</span>\n</div>\n\n<p>In the next stage, the files matching by size are compared by hashes of their initial 4 kB block. This involves a lot of random I/O as well – \nfor each file, <code class=\"language-plaintext highlighter-rouge\">fclones</code> opens it, reads the first 4 kB of data, computes the hash and closes the file, then moves to the next file. \nSSDs are great at random I/O, and high parallelism level leads to big wins here as well (Fig. 2). It was surprising to me\nthat even 64 threads, which are far more than the number of CPU cores (4 physical, 8 virtual), still improved the performance.\nI guess that with requests of such a small size to such a fast storage, you need to submit really many of them to keep \nthe SSD busy.</p>\n\n<div class=\"figure\">\n    <div style=\"height:14em\"><canvas id=\"prefixHashPerfSsd\"></canvas></div>\n    <script>\n    makeBarChartDeferred(\"prefixHashPerfSsd\", \n        [1, 2, 4, 8, 16, 32, 64], \n        {\"time\": [198, 88.5, 40.1, 23.0, 10.75, 6.69, 5.43]});\n    </script>\n    <span class=\"caption\"> Fig.2: Time to hash initial blocks of ~1.2 million files on an SSD</span>\n</div>\n\n<p>Let’s look at <code class=\"language-plaintext highlighter-rouge\">iostat</code>. With only 1 thread, <code class=\"language-plaintext highlighter-rouge\">iostat</code> reports CPU to be mostly idle, but\nthe SSD utilization is at 100%.</p>\n\n<pre>\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n           2,39    0,00    5,03    5,03    0,00   87,55\n\nDevice            r/s     rMB/s   rrqm/s  %rrqm r_await rareq-sz   aqu-sz  %util\nnvme0n1       5458,00     21,32     0,00   0,00    0,11     4,00     0,00 100,00\n</pre>\n\n<p>Does it mean the SSD is already at its 100% performance? No, because \n<code class=\"language-plaintext highlighter-rouge\">%util</code> is calculated as the ratio of <em>wall clock time</em> the device is serving requests. \nThis doesn’t account for effects of submitting multiple requests at the same time.\nIt looks like my SSD is very happy to receive more load. With 64 threads,\n<code class=\"language-plaintext highlighter-rouge\">%util</code> is still at 100%, but the served read request rate went up by over 40 times:</p>\n\n<pre>\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n          28,46    0,00   66,92    4,62    0,00    0,00\n\nDevice            r/s     rMB/s   rrqm/s  %rrqm r_await rareq-sz   aqu-sz  %util\nnvme0n1     223974,00    874,90     0,00   0,00    0,17     4,00     0,00 100,00\n</pre>\n\n<p>BTW: why the average queue size <code class=\"language-plaintext highlighter-rouge\">aqu-sz</code> remains 0,00 even under 64 threads remain a mystery to me. \nFeel free to drop any clues in the comments.</p>\n\n<p>How do I known the CPU is not the main bottleneck here then? The CPU load numbers given by <code class=\"language-plaintext highlighter-rouge\">iostat</code> are pretty high, aren’t they?\nI measured how much time it takes to do the task when all the data were cached, by running it again, without prior dropping caches. \nWhen all cached, the metadata scanning took 1.5 s and the partial hashing took 1.7 s. This is still\nsignificantly faster than when physical reads were involved, so nope, \nI/O is still the major bottleneck, even with 64 threads.</p>\n\n<h1 id=\"ssd--sequential-reads\">SSD – Sequential Reads</h1>\n\n<p>And what about the sequential I/O reads? Does parallelizing the sequential I/O improve speed as well?\nIt looks like it does, although not by as much as for random I/O (Fig. 3).\nThe last stage of <code class=\"language-plaintext highlighter-rouge\">fclones</code> algorithm is hashing full files – in this experiment the files were mostly JPG and RAW images, \nabout 10 MB large on average. Gains seem to hit a plateau a bit earlier – after 8 threads. In this case the operating \nsystem has an opportunity to prefetch data, so\nit can keep the SSD busy even when my application is not asking for data for a while.</p>\n\n<div class=\"figure\">\n    <div style=\"height:14em\"><canvas id=\"fullHashPerfSsd\"></canvas></div>\n    <script>\n    makeBarChartDeferred(\"fullHashPerfSsd\", \n            [1, 2, 4, 8, 16, 32, 64],\n            {\"time\": [74.3, 33.74, 20.1, 16.75, 15.45, 15.20, 15.15]});\n    </script>\n    <span class=\"caption\"> Fig. 3: Time to hash 21.6 GB of data read from an SSD in function of number of threads</span>\n</div>\n\n<h1 id=\"hdd--random-reads\">HDD – Random Reads</h1>\n<p>Contrary to an SSD, a spinning drive has a large seek-latency and it can serve I/O requests\nat much lower rate. Hence, we can definitely expect random I/O to be much slower on an HDD than on an SSD. \nBut can we expect any performance gains from reading in parallel? \nMy initial thought was there shouldn’t be any visible gains, because a single HDD can only serve \na single read request at a given time, then it has to reposition the heads to “jump” to another file, and \nthis looks very “sequentially” in principle. Having a large number of requests piled up in the queue \nshouldn’t change anything: the HDD would handle them in a sequence anyways. \nAn HDD is also slow enough that even a single fast thread should keep it fully busy with at \nleast one request ready to serve at any time.</p>\n\n<p>I was wrong. It turns out that for small, random I/O requests there are noticeable gains from parallelism \neven on an HDD (Fig. 4). But this happens for a different reason than on SSD. \nThe seek latency depends heavily on the <em>order</em> of the I/O requests. If the process submits more\nI/O requests from multiple threads, the operating system can <em>reorder</em> them by physical data location, thus minimizing\nthe distance the HDD heads have to travel.</p>\n\n<div class=\"figure\">\n    <div style=\"height:14em\"><canvas id=\"partialHashPerfHdd\"></canvas></div>\n    <script>\n    makeBarChartDeferred(\"partialHashPerfHdd\", \n            [1, 2, 4, 8, 16, 64, 256],\n            {\"time\": [681.92, 358.34, 316.53, 276.68, 245.06, 225.99, 227.40]});\n    </script>\n    <span class=\"caption\"> Fig.4: Time to hash initial blocks of 46,165 files on a 7200 RPM HDD</span>\n</div>\n\n<h1 id=\"hdd--sequential-reads\">HDD – Sequential Reads</h1>\n<p>Unfortunately, when reading larger chunks of data sequentially, using multi-threding actually hurts the throughput (Fig. 5).\nThis is because the operating system interleaves the I/O requests coming from different threads and the HDD would have\nto reposition the heads frequently jumping from one file to another. How much throughput is lost depends heavily on the operating\nsystem and its configuration, but generally I’d expect this to be a factor 2x-10x.</p>\n\n<div class=\"figure\">\n    <div style=\"height:18em\"><canvas id=\"fullHashPerfHdd\"></canvas></div>\n    <script>\n    makeBarChartDeferred(\"fullHashPerfHdd\", \n            [1, 2, 4, 8, 16, 64], \n            {\"fadvise\": [24.131, 54.59, 48.44, 45.114, 42.54, 42.102], \n             \"no fadvise\": [24.193, 67.835, 51.237, 53.45, 53.99, 52.24]});\n    </script>\n    <span class=\"caption\"> Fig.5: Time to hash 1.7 GB of data on a 7200 RPM HDD</span>\n</div>\n\n<p>One way of solving this problem in an application is to not allow many threads to contend for the same HDD device at the OS level, and \ninstead make the application take some control over the I/O request scheduling by itself.\nYou can use a dedicated single thread to handle all I/O to a single spinning drive (this is what <code class=\"language-plaintext highlighter-rouge\">fclones</code> does since version 0.7.0),\nor guard I/O operations by a critical section (mutex) associated with each HDD and locked at a granularity coarse enough that\nseek time doesn’t matter. I don’t recommend making the whole application single-threaded, because that would disallow\nissuing parallel requests to multiple devices and it wouldn’t allow the gains outlined above.</p>\n\n<p>Additionally, many operating systems allow to tell the kernel that the application will be reading the file data sequentially. \nFor example in Linux, after opening the file, just call <a href=\"https://man7.org/linux/man-pages/man2/posix_fadvise.2.html\"><code class=\"language-plaintext highlighter-rouge\">posix_fadvise</code></a> with <code class=\"language-plaintext highlighter-rouge\">POSIX_FADV_SEQUENTIAL</code>:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">use</span> <span class=\"nn\">std</span><span class=\"p\">::</span><span class=\"nn\">fs</span><span class=\"p\">::</span><span class=\"o\">*</span><span class=\"p\">;</span>\n<span class=\"k\">use</span> <span class=\"nn\">nix</span><span class=\"p\">::</span><span class=\"nn\">fcntl</span><span class=\"p\">::</span><span class=\"o\">*</span><span class=\"p\">;</span>\n<span class=\"k\">let</span> <span class=\"n\">file</span> <span class=\"o\">=</span> <span class=\"nn\">File</span><span class=\"p\">::</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"s\">\"foo.txt\"</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n<span class=\"k\">let</span> <span class=\"n\">errno</span> <span class=\"o\">=</span> <span class=\"nf\">posix_fadvise</span><span class=\"p\">(</span><span class=\"n\">file</span><span class=\"nf\">.as_raw_fd</span><span class=\"p\">(),</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nn\">PosixFadviseAdvice</span><span class=\"p\">::</span><span class=\"n\">POSIX_FADV_SEQUENTIAL</span><span class=\"p\">)</span><span class=\"o\">?</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Internally this option increases the size of the read-ahead buffer, so the system can fetch data in larger chunks, \npotentially reducing the number of seeks. The effects of this flag are clearly visible and it improves performance of parallel access, \nbut it is not strong enough to reduce the seek overhead to zero. Interestingly, I haven’t observed any effects \nof this flag on single-threaded throughput in my test, but YMMV.</p>\n\n<h1 id=\"conclusions\">Conclusions</h1>\n<ul>\n  <li>Random I/O and reading metadata benefits from parallelism on both types of drives: SSD and HDD</li>\n  <li>SSDs generally benefit from parallelism much more than HDDs</li>\n  <li>Parallel access to HDD when reading large chunks of data sequentially can deteriorate performance</li>\n  <li>Calling <code class=\"language-plaintext highlighter-rouge\">posix_fadvise</code> to inform the system about sequential access pattern improves read throughput slightly\nwhen sharing the device between multiple threads on Linux</li>\n</ul>",
  "author": {
    "name": "Piotr Kołaczkowski"
  },
  "summary": "One of the well-known ways of speeding up a data processing task is partitioning the data into smaller chunks and processing the chunks in parallel. Let’s assume we can partition the task easily, or the input data is already partitioned into separate files which all reside on a single storage device. Let’s also assume the algorithm we run on those data is simple enough so that the computation time is not a bottleneck. How much performance can we gain by reading the files in parallel? Can we lose any?"
}