{
  "title": "Calculus on Computational Graphs: Backpropagation",
  "link": "http://colah.github.io/posts/2015-08-Backprop/index.html",
  "description": "\n<p>Backpropagation is the key algorithm that makes training deep models computationally tractable. For modern neural networks, it can make training with gradient descent as much as ten million times faster, relative to a naive implementation. That’s the difference between a model taking a week to train and taking 200,000 years.</p>\n<p>Beyond its use in deep learning, backpropagation is a powerful computational tool in many other areas, ranging from weather forecasting to analyzing numerical stability – it just goes by different names. In fact, the algorithm has been reinvented at least dozens of times in different fields (see <a href=\"http://www.math.uiuc.edu/documenta/vol-ismp/52_griewank-andreas-b.pdf\">Griewank (2010)</a>). The general, application independent, name is “reverse-mode differentiation.”</p>\n<p>Fundamentally, it’s a technique for calculating derivatives quickly. And it’s an essential trick to have in your bag, not only in deep learning, but in a wide variety of numerical computing situations.</p>\n\n<p><a href=\"http://colah.github.io/posts/2015-08-Backprop/index.html\">Read more.</a></p>\n",
  "pubDate": "Mon, 31 Aug 2015 00:00:00 UTC",
  "guid": "http://colah.github.io/posts/2015-08-Backprop/index.html"
}