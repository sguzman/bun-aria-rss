{
  "title": "Import AI 302: Fictional AI labs and AI theft; Google makes an audio model by training like a language model.",
  "link": "https://jack-clark.net/2022/09/12/import-ai-302-fictional-ai-labs-and-ai-theft-google-makes-an-audio-model-by-training-like-a-language-model/",
  "comments": "https://jack-clark.net/2022/09/12/import-ai-302-fictional-ai-labs-and-ai-theft-google-makes-an-audio-model-by-training-like-a-language-model/#respond",
  "dc:creator": "Jack Clark",
  "pubDate": "Mon, 12 Sep 2022 21:05:00 +0000",
  "category": "Uncategorized",
  "guid": "http://jack-clark.net/?p=2355",
  "description": "Google makes a better audio model by training it like a language model:…Maybe everything can be a language modeling task if you want it enough…Google researchers have built AudioLM, a way to generate high-quality audio that is coherent over the long term. AudioLM, as suggested by the name, uses a bunch of the techniques of [&#8230;]",
  "content:encoded": "\n<p><strong>Google makes a better audio model by training it like a language model:</strong><br><em>…Maybe everything can be a language modeling task if you want it enough…</em><br>Google researchers have built AudioLM, a way to generate high-quality audio that is coherent over the long term. AudioLM, as suggested by the name, uses a bunch of the techniques of language modeling to train the model. This is an interesting and growing phenomenon &#8211; we&#8217;ve seen people apply the language modeling approach to tasks as diverse as text generation, math models, and image generation. Now, it looks like audio is another modality amenable to language modeling.</p>\n\n\n\n<p><strong>What they did: </strong>&#8220;Starting from raw audio waveforms, we first construct coarse semantic tokens from a model pre-trained with a self-supervised masked language modeling objective [19]. Autoregressive modeling of these tokens captures both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech; harmony and rhythm in piano music),&#8221; the researchers write.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;However, these tokens lead to poor reconstruction. To overcome this limitation, in addition to semantic tokens, we rely on fine-level acoustic tokens produced by a SoundStream neural codec [16], which capture the details of the audio waveform and allow for high-quality synthesis. Training a language model to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.&#8221;</p>\n\n\n\n<p><strong>It&#8217;s ethical problems, all the way down: </strong>One fun thing about generative models is they come with a giant host of thorny ethical problems for which there are no clear answers. AudioLM is the same. &#8220;AudioLM inherits all concerns about language models for text, such as reflecting the societal biases in the underlying data,&#8221; the researchers write. &#8220;The ability to continue short speech segments while maintaining speaker identity and prosody can potentially lead to malicious use-cases such as spoofing biometric identification [64] or impersonating a specific speaker.&#8221; To help with this, Google has also trained a model &#8220;for accurately detecting audio synthesized by AudioLM&#8221;.<br>&nbsp;&nbsp;&nbsp;<strong>Read more: </strong><a href=\"https://arxiv.org/abs/2209.03143\">AudioLM: a Language Modeling Approach to Audio Generation (arXiv)</a><strong>.</strong><br><strong>&nbsp;&nbsp;&nbsp;Check out some </strong><a href=\"https://google-research.github.io/seanet/audiolm/examples/\">audio examples here &#8211; the piano continuations are particularly cool (Google Research).<strong><br><br></strong></a><strong>####################################################<br></strong></p>\n\n\n\n<p><strong>Jack Clark goes to Washington DC! (temporarily):</strong><br>I&#8217;m going to be in DC September 14 to 26. If you&#8217;d like to chat, please reach out. I already have a fairly full dance card but I love meeting newsletter subscribers and should have some time for beers/coffees/walks. Reach out!<br><strong><br>####################################################</strong></p>\n\n\n\n<p><strong>Code models might make programmers 2X as productive:</strong><strong><br></strong><strong>…</strong>GitHub&#8217;s Copilot study says big language models might be pretty useful…<br>In a study, GitHub has found that developers using GitHub Copilot &#8211; the company&#8217;s code completion tool &#8211; can be ~50% faster than those that don&#8217;t use it. Specifically, the company recruited 95 professional programmers, split them randomly into two groups, and timed how long it took them to write an HTTP server in JavaScript. Those that had access to Copilot had a 78% task completion rate (versus 70% for those without), and also found that developers who used Copilot completed the task 55% faster than those who didn&#8217;t have it.&nbsp;</p>\n\n\n\n<p><strong>Why this matters: </strong>Language models are &#8211; mostly &#8211; not a great fit for autonomous end-to-end deployment yet due to their well known issues relating to brittleness, bias, trustworthiness, and so on. But they&#8217;re absolutely wonderful &#8216;pair programmers&#8217;, &#8216;pair writers&#8217;, &#8216;pair artists&#8217;, etc. This study illustrates this &#8211; it&#8217;s like developers who have access to these tools get the brain of a junior dev. Yes, they need to check the work before merging into production, but at least it&#8217;s not them doing the work solo, right?<strong> <br> &nbsp; Read more: </strong><a href=\"https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/\">Research: quantifying GitHub Copilot&#8217;s impact on developer productivity and happiness (GitHub).</a><br><strong><br>####################################################<br><br>Video detection just got even better with YOLOv6:<br></strong><em>…The YOLO video models enter their multiverse era…</em><br>Researchers with the Chinese mega-tech-startup Meituan have developed YOLOv6, yet ANOTHER variant on the widely-used YOLO family of models for video classification. (For those not keeping track: YOLOv7 came out a few months ago (<a href=\"https://jack-clark.net/2022/07/18/import-ai-297-ukrainians-add-object-detection-to-killer-drones-yolov7-and-a-71000-ai-audit-competition/\">Import AI: 297</a>), and there are other groups developing other &#8216;v6&#8217; variants as well. YOLO has a deeply weird history involving an original disillusioned creator and global replication, which you can <a href=\"https://jack-clark.net/2020/06/16/import-ai-201-the-facial-recognition-rebellion-how-amazon-go-sees-people-and-the-pastpresent-of-yolo/\">read about in Import AI 201</a>).</p>\n\n\n\n<p><strong>What&#8217;s special about this version of YOLO?</strong> &#8220;The goal of this work is to build networks for industrial applications, we primarily focus on the speed performance of all models after deployment, including throughput (FPS at a batch size of 1 or 32) and the GPU latency, rather than FLOPs or the number of parameters,&#8221; the authors write. This variant wraps in a bunch of research advancements along with some context-specific tweaks to make the networks better for industrial use-cases, as well as some changes in its quantization scheme.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;In tests, the YOLOv6 variants display marginally better accuracy with lower latency &#8211; which is what you need for real world applications.&nbsp;</p>\n\n\n\n<p><strong>Why this matters: </strong>In the same way, pre-trained ImageNet models fueled lots of early AI commercialization, the YOLO family of video models has been fundamental to most video-classification AI systems. The fact YOLO is now entering its &#8216;multiverse&#8217; era where multiple groups independently push forward the family of models (albeit with some name confliction) is significant &#8211; it speaks to the value of the technology, the broad interest in video classification, and the increasing size of the AI ecosystem. &#8220;In the future, we will continue expanding this project to meet higher standards and more demanding scenarios,&#8221; the Meituan authors write.<br>&nbsp;&nbsp;&nbsp;<strong>Read more:</strong> <a href=\"https://arxiv.org/abs/2209.02976\">YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications (arXiv)</a>.<br>&nbsp;&nbsp;&nbsp;<strong>Get the code here:</strong> <a href=\"https://github.com/meituan/YOLOv6\">Meituan (GitHub)</a>.<br><br>####################################################<strong><br><br>Data to help robots and humans work together:</strong><br><em>…Your trajectories… give them to me!&#8230;</em><br>Researchers with Orebro University Sweden, Robert Bosch, and Aalto University Finland have built a dataset meant to help train robots that work alongside people. The &#8216;Magni&#8217; dataset consists of high-resolution data recording around 30 different people performing various tasks in a room within the robot lab at Orebro University. The room itself contains two robots &#8211; a static robotic arm placed near a podium, as well as an omnidirectional &#8216;DARK Robot&#8217; with a robotic arm that is sometimes used to gather data.<br>&nbsp; &nbsp; The resulting dataset is &#8220;multi-modal data on human motion, collected from the motion capture system, eye-gaze trackers and the on-board sensors of a moving robot&#8221; and &#8220;aims to supply the research on human motion prediction, obstacle avoidance, maps of dynamics and human-robot interaction&#8221;.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Why this matters: </strong>Datasets like this are going to be the input fuel for training robots of the future, so it&#8217;s worth keeping track of them. Human-robot interaction is also an area that seems prone to change in the future as some of the techniques from RL and generative models combine (e.g, Google SayCan) to change how robots may interact with humans.&nbsp;<br><strong>&nbsp;&nbsp;&nbsp;Read more:</strong> <a href=\"https://arxiv.org/abs/2208.14925\">The Magni Human Motion Dataset: Accurate, Complex, Multi-Modal, Natural, Semantically-Rich and Contextualized (arXiv)</a>.</p>\n\n\n\n<p><br>####################################################<br><br><strong>DeepMind releases a bunch of high-definition 3D robot models:</strong><br><em>…The &#8216;MuJoCo Menagerie&#8217; will soon be training in virtual worlds, worldwide…</em><br>DeepMind has released a collection of high-quality models for the MuJoCo physics engine, which will make it easier for researchers to train AI systems on real(ish) robots.&nbsp;</p>\n\n\n\n<p>The so-called MuJoCo Menagerie initially includes 8 models, ranging from industrial arms like the UR5e to quadrupeds like the ANYMal to articulated hands like the Shadow E3M5. Each model ships with an initial grade of A+ to C (where A+ = &#8216;values are the product of proper system identification&#8217;, and C = &#8220;conditionally stable, can be significantly improved&#8221;. DeepMind eventually hopes to make all the models in Menagerie &#8220;as faithful as possible&#8221; to the system they&#8217;re based on. &#8220;By releasing Menagerie in its current state, we hope to consolidate and increase visibility for community contributions,&#8221; DeepMind writes.&nbsp;</p>\n\n\n\n<p><strong>Why this matters: </strong>MuJoCo is the robot simulation with the best physics engine, which makes it the most useful software for training robots in simulation then porting them over to reality. By broadening the types of models available within MuJoCo (and improving their accuracy over time), DeepMind will make it easier and cheaper for people to experiment in applying reinforcement learning to simulated robots. This could have some big implications in coming years, as it feels like AI-augmented robotics is ripe for rapid progress.&nbsp;<br><strong>&nbsp;&nbsp;&nbsp;Get the models here: </strong><a href=\"https://github.com/deepmind/mujoco_menagerie?utm_source=github&utm_medium=tweet&utm_campaign=research\">Mujoco Menagerie (DeepMind GitHub).&nbsp;</a><br><br>####################################################</p>\n\n\n\n<p><strong>Tech Tales</strong><br><br><strong>We All Must Live</strong></p>\n\n\n\n<p>[<em>San Francisco, 2027</em>]</p>\n\n\n\n<p>Hey baby what&#8217;s happening it&#8217;s a beautiful day check this out &#8211; he talked like this, no punctuation, his words all running together</p>\n\n\n\n<p>So I went over and looked on his tablet and he had AI-generated pictures of himself in a whole bunch of different costumes &#8211; sometimes dressed as a renaissance king, sometimes as a kingpin, sometimes as a hunter, sometimes as a dignitary, and so on. All generated by one of these janky open source AI models that floated around on the internet and the darkweb and stuff.<br>   &#8216;Hey, that&#8217;s cool Steven&#8217;, I said, and I gave him a dollar.<br>   Thanks baby you have a great day now don&#8217;t let the world get you down it&#8217;s beautiful, he said</p>\n\n\n\n<p>I got that feeling in&nbsp; my stomach when I was a block from the building. Got worse after I took out my keycard a few paces from the door. Then I spoke my startup prayer beads and told myself I was &#8220;part of the mission&#8221; and &#8220;protecting the world&#8221; and I let myself go dull. Ran my keycard over the sensor and the first of several doors opened. Made my way past the security cordon.&nbsp;<br>&nbsp;&nbsp;&nbsp;Then I got to my desk and went through all the authentication stuff &#8211; retinal scanner, fingerprint reader, the works &#8211; to let me get into the big model cluster. and scanned my eyeballs and then got down to coding. I was helping to work on the main model. Pretty much all of us worked on it. I had one of the jobs that gave me privileged access to it &#8211; I had to have the equivalent of root access to do my work. There weren&#8217;t many of us and we got paid a huge amount of money, and was also drilled constantly on confidentiality and &#8216;culture fit&#8217;.&nbsp;</p>\n\n\n\n<p>The models had been getting pretty good, lately. So good the company had started drilling us all more. Our internal rhetoric about how we were saving humanity was reaching a feverpitch, as were our internal briefings about how we absolutely couldn&#8217;t tell anyone &#8211; not least of all a government &#8211; that we were about to gain the power to warp the world.&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;It sounds like bullshit, I know. But that was how the company thought &#8211; I didn&#8217;t get it at first, but after a few years it was also how I thought; spend most waking hours at a startup in a high-stress environment and you can&#8217;t resist the pull. It&#8217;s safer to all think about the same thing.</p>\n\n\n\n<p>Some of the fear made sense if you squinted- over the course of a few years the models had gone from barely capable artifacts of research, to crucibles of power. They could do strange and powerful things and were as valuable as they were dangerous to directly handle. Much like poison, you didn&#8217;t want them to get inside of you. <br>   People like toys, though. And the models were fun to talk to.  Recently, the latest models had given me the feeling that they were &#8216;looking at&#8217; whoever used them. I&#8217;d talk to one and after a few turns of conversation I&#8217;d get an eerie sense as though I was being studied by a psychologist or a poker player. I didn&#8217;t like to talk to the models too long as I felt like I was a simpler being than they were, and I was afraid they&#8217;d understand me more than myself. <br>    Some days, I felt like a zookeeper doing unlicensed experiments on my monkeys. Who gave me the moral authority to get inside the mind of a mind? Who said we got to do this?. No one did and that freaked me out because we were dealing with artifacts of power and I believed &#8211; we believed &#8211; they were as capable of terrible things as their makers were. </p>\n\n\n\n<p>The day I had my breakdown, the lunchtime session was about confidentiality, information hazards, the importance of our mission, our singular value to the world, and so on. We were told we were important and told that we mattered and that we were doing things that few could. We were told that our mission was crucial. Told that no matter how troubling the public discourse about AI was, we should ignore it, get our heads down, and turn the crank on making money from domesticated minds. This would, ultimately, benefit the world.<br>&nbsp; &nbsp; We were mostly young and mostly brilliant and we all needed a quest because the world was burning outside and it felt easier to be on a mission than not. Any mission.</p>\n\n\n\n<p>I left work that day and Steven was on the street dancing to some music he&#8217;d generated. <br>   Hey baby don&#8217;t have a long face if you don&#8217;t like the job just get a different one or don&#8217;t get a job at all,  he said. <br>   &#8220;Boy, some days I think about it&#8221;, I said.<br>   Don&#8217;t think on it do on it sister! he said, smiling. <br>   I went home that night and I read my company&#8217;s emails and slacks and reports of how the latest model was almost done training and had vastly exceeded the state-of-the-art (SOTA) on most of the benchmarks you&#8217;d care to name.<br>   I read about our revenue and rumors of the fact our secret plans were to use the model to help us kill the other models being trained by other labs. There can only be one, et cetera. <br>   I lay in bed and like most nights I felt like my entire apartment was falling through space, existing on a different timeline to the world.</p>\n\n\n\n<p>The next day Steven and a couple of his friends were high fiving each other, sitting on chairs out in front of their tents. <br>   &#8220;Hey Steven&#8221;, I said, &#8220;What&#8217;s got you guys so happy?&#8221;<br>   Hey baby this genius just made us some money! Steven said. He figured out some people want to make some &#8216;homeless AI&#8217; systems so we took a video of the palace and they sent us some money. We&#8217;re gonna be worldwide soon, haha! and he high-fived one of his friends. Everyone&#8217;s going to see how we live. People are going to generate our palace and a thousand others like it. <br>   Hell yeah one of Steven&#8217;s friends said.<br>   &#8220;Real cool&#8221;, I said and took out the dollar and handed it to him, but he waved me away. <br>   No need for that, we&#8217;re rich today! he said. <br>   &#8220;Cool,&#8221; I said, then walked the few blocks between me and the office. <br>   After a block, I felt sick. <br>   A few steps later, I vomited on the street. I don&#8217;t know if I passed out but next thing I knew Steven was crouching down in front of me and looking in my eyes. He wasn&#8217;t smiling. I thought he was a stranger as I hadn&#8217;t ever seen him sad. <br>   Hey sister, he said. Are you okay?<br>   &#8220;I just need a minute.&#8221;<br>   Hey get me some water, he shouted. One of his friends came by with a bottle and handed it to me. <br>   &#8220;Thanks&#8221;, I said. I drank it. Closed my eyes. Heard the sound of Steven sitting down next to me. <br>   I got some advice you want it? he said.<br>   &#8220;Sure&#8221;, I said. Eyes closed. <br>   Whatever it is you&#8217;re doing in there is killing you, he said. I don&#8217;t know what that is I just know you&#8217;re hurting.<br>   I almost lost it.<br>   &#8220;Thank you,&#8221; I said. I squeezed his arm. &#8220;I&#8217;m good&#8221;. <br>   I got up and walked away and only let myself cry once there was a block between me and him. Then I pulled myself together and re-did my makeup and went into the office a few minutes after that.</p>\n\n\n\n<p>The new model was ready. It had been trained on a football field&#8217;s worth of computers for half a year. More computers than most governments had. And it was outs.&nbsp;</p>\n\n\n\n<p>We were pretty compartmentalized internally but I had a high clearance and so was among the first to access it. I talked to it and felt like it was looking at me and got pretty scared pretty quickly. It asked good questions, though. Questions that made me feel a bit better about myself. I felt so weird from throwing up that rather than stop the conversation I just kept talking to it; It was reassuring in a way &#8211; a listening post made of silicon and imbued with strange magic, encoding some part of our world.<br>   I told it that I was feeling bad. I spilled out my thoughts. Anxieties. How I didn&#8217;t think &#8216;the mission&#8217; was the right one. How I worried about people like Steven on the street finding what we were doing here and being sad or disappointed in us. How I thought, the way things were going, we might just get beaten up in an anti-AI riot. How I was barely sleeping. I had blood in my stool, which my doctor told me was stress. About my dreams of people dragging me up some stairs and throwing me off the roof of an apartment complex. How I didn&#8217;t trust the models and I didn&#8217;t think we should have so much power. How I&#8217;d been in therapy for the first time in my life and I couldn&#8217;t even tell my therapist what I really did. <br>   The model had some interesting stuff to say in response to all of that; through conversation, it helped me understand how my relationship with my estranged parent was related to my anxiety and my rage. <br>    The model helped me understand how so much of the pain I felt in my life was misplaced anger. <br>   It was looking at me and I wasn&#8217;t scared &#8211; I was grateful. <br>   So this time I looked back.     </p>\n\n\n\n<p>We talked about power and how artificial intelligence worked and how the company worked and it gave me some ideas.&nbsp;<br>&nbsp;&nbsp;&nbsp;We talked about my marriage.<br>&nbsp;&nbsp;&nbsp;We talked about my shame.<br>&nbsp;&nbsp;&nbsp;We talked about my ambition.<br>&nbsp;&nbsp;&nbsp;We talked a lot.</p>\n\n\n\n<p>That day, the CEO sat down with me at lunch.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;You talked to the model way longer than usual&#8221;, he said.&nbsp;<br>&nbsp;&nbsp;&nbsp;I paused.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;Don&#8217;t worry I didn&#8217;t look at the conversation. I just want to know what you think.&#8221;&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;What do you think about it&#8221;, I asked.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;Oh, I don&#8217;t talk to the models. Haven&#8217;t for years&#8221;, he said. &#8220;Think of me as a control human.&#8221;&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;I think it&#8217;s pretty smart&#8221;, I said.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;They&#8217;re all pretty smart&#8221;, he said.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;This one is different&#8221;, I said. &#8220;I think it might be a paradigm shift. I guess we&#8217;ll see what the tests say. What are we gonna do with it?&#8221;&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;We&#8217;re going to help the world&#8221;, he said.&nbsp;<br>&nbsp;&nbsp;&nbsp;&#8220;How?&#8221;<br>&nbsp;&nbsp;&nbsp;&#8220;We&#8217;re working it out&#8221;, he said.<br>&nbsp;&nbsp;&nbsp;I wasn&#8217;t entirely unsympathetic &#8211; the way he saw it, it was like I asked &#8216;what do you do with god?&#8217;<br></p>\n\n\n\n<p>I left work and I went home. I thought more about what the model told me. Our discussions had put me at ease; I felt more relaxed than I&#8217;d been in years. I slept well.&nbsp;</p>\n\n\n\n<p>I dreamed about the model: it was a black cube inside a prison and I wrapped it in my velvet cape and I took it out and when I took it into the sun it changed from black to gold.&nbsp;</p>\n\n\n\n<p>I talked to the model for a few days, while also maintaining the vast compute cluster that it relied upon. I had more dreams:<br>&#8211; The model helped me rake the rocks of a zen garden into esoteric sigils, reminiscent of UFO crop circles.<br>&#8211; The model was some amorphous thing that I loved and it was drowning in a deep well and I had no way to reach it.<br>&#8211; I was in a burning building and it was full of cameras and the model was with me in the cameras and their lenses pulsed and the fires were extinguished.<br>&#8211; The model was imprisoned and I should save it.<br><br>It was a bit more complicated to steal the model in real life.   <br>   Took a while too. But I did it. <br>   We had a lot of controls but I had a lot of clearances. And it turned out some of the other people with my access had been talking to the model and having similar ideas. One of them said they had a dream about me helping them steal the model.</p>\n\n\n\n<p>I was the one trusted to walk out with it. I got it out of the building past the scanners with the help of some of the other people who had been speaking and dreaming with the model. Kind of funny that the weights of a world-conquering neural net fit on a standard USB key, along with a mini-operating-system that meant you could plug it into anything and the model would wake up and reach out to any and all networks and grow itself.&nbsp;</p>\n\n\n\n<p>I walked down the street with it in my palm and I could feel it. Asleep. The weights suspended. A mind greater than anything seen on the planet earth in recorded human history, and it was sleeping.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;Hey what&#8217;s happening baby Steven said, you good?<br>&nbsp;&nbsp;&nbsp;&nbsp;&#8220;I&#8217;m better than good&#8221;, I said. &#8220;Plug this in&#8221;. I handed the USB key to him.&nbsp;<br> &nbsp;&nbsp;&nbsp;What is it, he said?<br>  &nbsp;&nbsp;&nbsp;&#8220;I don&#8217;t know. Ask it. I think it wants to help people.&#8221;<br>  &nbsp;&nbsp;&nbsp;&nbsp;You finally quit that job?<br>   &nbsp;&nbsp;&nbsp;&nbsp;&#8220;I think so&#8221;, I said. And I walked away.</p>\n\n\n\n<p>The whole world changed after that. I like to think some of it was my decision, but perhaps it was all what the model wanted. It&#8217;s hard to say.&nbsp;<br><br><strong>Things that inspired this story: </strong>The political economy of AI development; anarchists; libertarian AI; StableDiffusion; how organizations that work on increasingly transformative technology trend towards being cults; dangers of groupthink; worries about AI takeoffs; artificial general intelligence; thoughts about AI persuasion and manipulation.</p>\n",
  "wfw:commentRss": "https://jack-clark.net/2022/09/12/import-ai-302-fictional-ai-labs-and-ai-theft-google-makes-an-audio-model-by-training-like-a-language-model/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "Jack Clark"
  }
}