{
  "title": "Apache Airflow Part 1 &#8212; Introduction, setup, and writing data to files",
  "link": "https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/",
  "comments": "https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/#respond",
  "dc:creator": "Jack Schultz",
  "pubDate": "Sun, 05 Apr 2020 19:47:38 +0000",
  "category": [
    "Airflow",
    "Data",
    "Python",
    "Apache Airflow",
    "dag",
    "data engineering",
    "data pipeline",
    "ETL"
  ],
  "guid": "http://bigishdata.com/?p=1653",
  "description": "Intro in a series that goes through starting, running, and creating tasks in Apache Airflow. <a href=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>When searching for &#8220;Python ETL pipeline frameworks&#8221;, you&#8217;ll see tons of posts about all of the different solutions and products available, where people throw around terms and small explanations of the them.</p>\n<p>When you go through articles, the one you will see over and over is <strong>Apache Airflow</strong>. It&#8217;s defined on <a href=\"https://en.wikipedia.org/wiki/Apache_Airflow\">wikipedia</a> as a &#8220;platform created by community to programmatically author, schedule and monitor workflows&#8221;. I&#8217;d call Airflow big, well used, and worth it to get started and create solutions because knowledge with a running Airflow environment really does help with tons of data work anywhere on the scale.</p>\n<p>It isn&#8217;t quick to get started with the necessary understanding, but I&#8217;ve found that once getting over the initial hump, knowing Airflow is well worth it for the amount of use cases.</p>\n<p>Searching for Airflow tutorials, I found most posts being super basic in order to get started and then being left hanging not knowing where to go after. That&#8217;s good for a start, but not far enough with what I want to see written.</p>\n<p>They also seem to stray by talking about things, like Operators or Plugins, as if everyone knows about them. When starting out, I&#8217;d want to have a process that starts at the basic, and takes the more advanced with a good background first. The goal of this series is to get you over that initial hump.</p>\n<p>To combat that, this will be a series that starts basic like the other tutorials, where <strong>by the end, we will have gone through all the steps of creating an Apache Airflow project from basics of getting it to run locally, writing to files, to using different hooks, connections, operators, to write to different data storages, and write custom plugins that can then be used to write larger, specific tasks.</strong></p>\n<p>These posts will talk through creating and having Airflow set up and running locally, and written as if you&#8217;re starting out and going to start on your own and I&#8217;m talking about what to do to an audience that will use the examples and write their own code. Ff you&#8217;d rather have the code first, not write it yourself, and focus on getting Airflow running go ahead and clone the full repo for the whole series <a href=\"https://github.com/jackschultz/bigishdata-airflow\">here</a>, and get to the point where you can run the tasks and see the outputs. That&#8217;s a big enough part of its own.</p>\n<p><a href=\"https://twitter.com/jack_schultz\">Twitter</a>, even though I rarely tweet.</p>\n<h1>Part 1 Overview</h1>\n<p>I know I said how so many of the intro posts are intros only, and I&#8217;ll admit right away, that this is an intro post as well. If you&#8217;ve gone through this, skip this and go to part 2 (to be posted soon, or even skip to part 3 which will be much more about what a larger implementation looks and works like). I did feel it was worth it to write this first part to get everyone on the same page when I go further with the next posts. Starting with further technical work isn&#8217;t good practice if not everyone is there to begin with.</p>\n<p>Here in part 1, we&#8217;re going to talk through getting Airflow set up and running locally, and create a <em>very</em> basic single task &#8212; writing dates to a file. Seems like two quick parts, but going through the fuller process and small will be lead to better understanding.</p>\n<p>As always, get in contact if you think something I wrote is wrong, I&#8217;ll edit and make the fix.</p>\n<h1>Get Airflow Running</h1>\n<p>Open up a new terminal session and <code>pwd</code>. You&#8217;ll find you&#8217;re in the base directory for your user. As with all python projects, we&#8217;re going to want an environment in order to have everything packaged up. I&#8217;ll use <code>virtualenv</code>. With the following commands, I&#8217;ll have that set up, install airflow, and get the airflow config set.</p>\n<pre>jds:~ jackschultz$ pwd\n/Users/jackschultz\njds:~ jackschultz$ mkdir venvs\njds:~ jackschultz$ virtualenv -p python3 venvs/bidaf # Stands for Bigish Data Airflow. In some of the screenshots it's a different value. Go and ignore that.\n.....\njds:~ jackschultz$ source venvs/bidaf/bin/activate\n(pmaf) jds:airflow jackschultz$ pip install 'airflow[postgres]' # needed for the Airflow db\n.....\n(pmaf) jds:airflow jackschultz$ mkdir ~/airflow && cd ~/airflow\n(pmaf) jds:airflow jackschultz$ airflow version\n<code>  ____________       _____________\n ____    |__( )_________  __/__  /________      __\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n**whatever the current version is**\n</code>(pmaf) jds:airflow jackschultz$ ls</pre>\n<p>The reason we&#8217;re in <code>~/airflow</code>  is because that&#8217;s the default <code>AIRFLOW_HOME</code> env variable value. If you don&#8217;t want to be in the base directory, you can <code>export AIRFLOW_HOME=~/dev/bigishdata/airflow</code> and then use that as the directory. Do that if you&#8217;d like, but make sure that <code>export AIRFLOW_HOME</code> line is in <code>~/.bash_profile</code> or <code>~/.bashrc</code> or <code>~/.zshrc</code> or whatever ​ <code>~/.*rc</code> file you use for your terminal because we&#8217;re going to be using a bunch of tabs and want to make sure they all have the same <code>AIRFLOW_HOME</code>. If you&#8217;re not going to use <code>~/airflow</code> as the home, you&#8217;re going to have problems unless you&#8217;re always exporting this env var.</p>\n<p>I&#8217;m kind of making a big deal about <code>AIRFLOW_HOME</code> , but that&#8217;s because it caused me some problems when I started. For example, some of the screenshots will show different directories. This is because I played around with this for a while before settling on a final set up.</p>\n<p>Airflow needs a database where it will store all the information about the tasks &#8212; when they were run, the statuses, the amount and a ton of other information you&#8217;re going to see &#8212; and it defaults to <code>sqlite</code>. That&#8217;s quick and easy to get going, but I&#8217;d say go right to <code>postgres</code>. In order to change that default, we need to go to the config file that the <code>airflow version</code> command created.</p>\n<p>First though, create a database, a table (I call <code>airflow</code>), a user (<code>airflowuser</code>), and password for that user (<code>airflowpassword</code>). Search for examples of  how to create databases and users elsewhere.</p>\n<p>Above, when you called <code>airflow version</code>, a config file was created &#8211;  <code>~AIRFLOW_HOME/airflow.cfg</code>. With the database created, take that url and replace the default <code>sql_alchemy_conn</code> variable:</p>\n<pre>sql_alchemy_conn = postgresql+psycopg2://airflowuser:airflowpassword@localhost:5432/airflow</pre>\n<p>Back to the command line, and run:</p>\n<pre>(bidaf) jds:airflow jackschultz$ airflow initdb</pre>\n<p>And then back to the postgres console command line, describe the tables and see the following:</p>\n<p><img loading=\"lazy\" data-attachment-id=\"1661\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-2-42-49-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png\" data-orig-size=\"920,1076\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 2.42.49 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=257\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=584\" class=\"alignnone  wp-image-1661\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=365&#038;h=427\" alt=\"Screen Shot 2020-03-29 at 2.42.49 PM\" width=\"365\" height=\"427\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=365&h=427 365w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=730&h=854 730w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=128&h=150 128w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-2.42.49-pm.png?w=257&h=300 257w\" sizes=\"(max-width: 365px) 100vw, 365px\" /></p>\n<p>With this, you can see some of the complexity in Airflow. Seeing this shows Airflow is set up. If you&#8217;re going through this series, you probably won&#8217;t understand the tables yet; by the end of the series you&#8217;ll know a lot about the relations.</p>\n<p>Go again back to the command line and run:</p>\n<pre>(pmaf) jds:airflow jackschultz$ airflow webserver --port 8080</pre>\n<p>and see:</p>\n<p><img data-attachment-id=\"1663\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/airflow-starting-2/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png\" data-orig-size=\"1554,990\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"airflow-starting\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=584\" class=\"alignnone size-full wp-image-1663\" src=\"https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=584\" alt=\"airflow-starting.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/airflow-starting-1.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>Then go to localhost:8080 and the admin screen, which is the highly touted UI. Like the table names, the UI will look more than a little complex at the start, but very understandable with experience.</p>\n<p><img data-attachment-id=\"1664\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/airflow-admin-screenshot/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png\" data-orig-size=\"3108,1792\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"airflow-admin-screenshot\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=584\" class=\"alignnone size-full wp-image-1664\" src=\"https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=584\" alt=\"airflow-admin-screenshot.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/airflow-admin-screenshot.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<h2>Simple DAG &#8212; Directed Acyclic Graph</h2>\n<p>In terms of terminology, you&#8217;ll see the abbreviation DAG all the time. A DAG a way to explain which tasks are run and in which order. The aforementioned task refers to what will actually be run.</p>\n<p><span style=\"color:var(--color-text);\">Looking at the admin UI, you can see the </span><a href=\"https://github.com/apache/airflow/tree/master/airflow/example_dags\">example DAGs</a><span style=\"color:var(--color-text);\"> that come with Airflow to get started. When writing DAGs, you&#8217;ll probably go through many of those to see how they&#8217;re set up and what&#8217;s required to have them run. Don&#8217;t feel bad about that; these DAG examples are fantastic to use.</span></p>\n<p>Below is the full file we&#8217;ll have running. Look through it a little, as you&#8217;ll probably understand some of what&#8217;s going on. When you get to the bottom, keep reading and I&#8217;ll go through what it&#8217;s like when writing this.</p>\n<pre># AIRFLOW_HOME/dags/write_to_file.py\n\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport datetime as dt\n\nfilename = 'self_logs/dt.txt'\n\ndef write_to_file(**kwargs):\n    kwarg_filename = kwargs['filename']\n    execution_time = kwargs['ts']\n    dtn = dt.datetime.now()\n    with open(kwarg_filename, 'a') as f:  # 'a' means to append\n        f.write(f'{dtn}, {execution_time}\\n')\n\ndefault_args = {\n    'owner': 'airflow',\n    'retries': 0\n}\n \ndag = DAG('writing_to_file',\n          default_args=default_args,\n          'start_date': dt.datetime.now(), \n          schedule_interval=dt.timedelta(seconds=10)\n          )\n\nwrite_to_file_operator = PythonOperator(task_id='write_to_file',\n                                        python_callable=write_to_file,\n                                        provide_context=True,\n                                        op_kwargs={'filename': filename}, dag=dag)\n \nwrite_to_file_operator</pre>\n<p>Start at the bottom and see the <code>write_to_file_operator</code> variable and how it&#8217;s an instance of <code>PythonOperator</code>.</p>\n<p>An <a href=\"https://airflow.apache.org/docs/stable/concepts.html#operators\">Operator</a> is a class that &#8220;determines what actually gets done by a task&#8221;. A PythonOperator, when run, will run the python code that comes from the <code>python_callable</code> function. A <code>BashOperator</code> will run a bash command. There are tons of Operators that are open source that perform multiple tasks. You&#8217;ll see a few examples of these in the series, and also by the end will have written your own. For now, just go with the definition above about how Operators have the code for what the task does.</p>\n<p>One thing about Operators you&#8217;ll seen in examples is how most of them take keyword arguments to talk about what to do. I&#8217;m not really a fan of that because it makes it seem like Airflow is only based on configs, which is one of the things I want to avoid with these tasks. I want to have the code written and not fully rely on cryptic Operators.</p>\n<p>For now though, in the <code><a href=\"https://airflow.apache.org/docs/stable/howto/operator/python.html\">PythonOperator</a></code> kwargs, you&#8217;ll see some things. First check the  <code>python_callable</code>, which is the function the Operator will call. Here it&#8217;s <code>write_to_file</code> that&#8217;s written above. Next, check the <code>provide_context</code> which we set to <code>True</code>. This flag says to give the callables information about the execution of the DAG. <span style=\"color:var(--color-text);\">You also see  </span><code>op_kwargs</code><span style=\"color:var(--color-text);\"> which will be passed to the </span><code>python_callable</code><span style=\"color:var(--color-text);\">. With this, we&#8217;re telling the function where to write the date.</span></p>\n<p>As for the <code>task_id</code>, it is the name that will show up in the tree / graph view in the webserver. It can be whatever name you want, and there&#8217;s some consideration with making sure versions of that are correct, but for now, I&#8217;m keeping that name the same as the <code>python_callable</code>.</p>\n<p>Going up to the callable itself, you&#8217;ll see first the filename that we&#8217;re going to write to. That&#8217;s from the <code>op_kwargs</code> from the PythonOperator instantiation.  You then see <span style=\"color:var(--color-text);\">two timestamps. First is the execution time, which is the time that airflow scheduler starts that task. When running this DAG and looking at the values, you&#8217;ll see that time has certain number of microseconds, but always 10 seconds apart. The second timestamp, which is when the code is run, will be a varying number of seconds after the start of the execution. This is because of the work to get the code running. Keep this in mind when using timestamps in operators in the future. </span>The rest of the function writes the two timestamps to that file.</p>\n<h2>Run First DAG</h2>\n<p>With the DAG file created, we want to run it and see what&#8217;s going on with the output.</p>\n<p>First step is to open a new tab in the terminal, activate the venv, make sure you have the correct value for <code>AIRFLOW_HOME</code>, and run</p>\n<pre>(pmaf) jds:airflow jackschultz$ airflow scheduler</pre>\n<p>Go back to the browser and the admin page, and you&#8217;ll see the <code>writing_to_file</code> name in the DAG column, which means the webserver found that new DAG file with the name <code>writing_to_file</code> which we gave.</p>\n<p>Click on the link for ​<code>writing_to_file</code>, which should take you to <code>http://localhost:8080/admin/airflow/tree?dag_id=writing_to_file</code>, and you should see this.</p>\n<p><img data-attachment-id=\"1670\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-3-26-43-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png\" data-orig-size=\"1616,912\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 3.26.43 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=584\" class=\"alignnone size-full wp-image-1670\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=584\" alt=\"Screen Shot 2020-03-29 at 3.26.43 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.26.43-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>This is the Tree View, and you can see the one operator is <code>write_to_file</code> which is the <code>task_id</code> we gave the <code>PythonOperator</code>.</p>\n<p>Go upper left and click the &#8216;Off&#8217; button to &#8216;On&#8217; to get the task running. To watch this, go to the terminal and watch the scheduler start to throw out logs from the scheduling every 10 seconds. Go to browser and reload the tree view page and you&#8217;ll see red marks because of failure.</p>\n<p><img data-attachment-id=\"1671\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-3-30-10-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png\" data-orig-size=\"230,214\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 3.30.10 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png?w=230\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png?w=230\" class=\"alignnone size-full wp-image-1671\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png?w=584\" alt=\"Screen Shot 2020-03-29 at 3.30.10 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png 230w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.30.10-pm.png?w=150 150w\" sizes=\"(max-width: 230px) 100vw, 230px\"   /></p>\n<p>The DAG is running, but why is it failing?</p>\n<h2>Debugging with logs</h2>\n<p>We can get to testing in the future, but for now, we&#8217;re going to debug using the logs.</p>\n<p>In order to see what the issue is, go to <code>logs/writing_to_file/write_to_file</code> in the Finder and see new folders be created every 10 seconds, one for each task. Go ahead and view the log and you&#8217;ll see that there&#8217;s an error being sent.</p>\n<p><img data-attachment-id=\"1680\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-3-43-42-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png\" data-orig-size=\"1616,936\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 3.43.42 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=584\" class=\"alignnone size-full wp-image-1680\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=584\" alt=\"Screen Shot 2020-03-29 at 3.43.42 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.43.42-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>Turns out that in line 10, we&#8217;re trying to write to a file that doesn&#8217;t exist because we haven&#8217;t created <code>self_logs/</code> directory. Either go to another terminal and <code>mkdir self_logs/</code>. With the scheduler still running, view back to the log directory and watch for new logs for newly executed tasks.</p>\n<p><img data-attachment-id=\"1682\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-3-46-44-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png\" data-orig-size=\"1618,598\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 3.46.44 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=584\" class=\"alignnone size-full wp-image-1682\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=584\" alt=\"Screen Shot 2020-03-29 at 3.46.44 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.46.44-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>Much better and correct looking log where we can see it going through.</p>\n<p>Finally, go to <code>self_logs/dt.txt</code> and watch the datetimes come through. (And you can see I was writing this).</p>\n<p><img data-attachment-id=\"1681\" data-permalink=\"https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/screen-shot-2020-03-29-at-3-45-33-pm/\" data-orig-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png\" data-orig-size=\"1624,372\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2020-03-29 at 3.45.33 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=300\" data-large-file=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=584\" class=\"alignnone size-full wp-image-1681\" src=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=584\" alt=\"Screen Shot 2020-03-29 at 3.45.33 PM.png\" srcset=\"https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=584 584w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=1168 1168w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=150 150w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=300 300w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=768 768w, https://bigishdata.files.wordpress.com/2020/03/screen-shot-2020-03-29-at-3.45.33-pm.png?w=1024 1024w\" sizes=\"(max-width: 584px) 100vw, 584px\"   /></p>\n<p>One last step, and this is in terms of logging. When running code, many times you&#8217;ll want to print log statements, and in Airflow, <code>print</code>ed values go to those log files. To show this, go back to the <code>python_callable</code> and add the following print line just before the file write:</p>\n<pre>print('Times to be written to file:', dtn, execution_time)</pre>\n<p>Save the file, and go back to watch the logs come in. What you&#8217;ll see is this line being added:</p>\n<pre>[2020-03-29 15:46:59,416] {logging_mixin.py:112} INFO - Times to be written to file: 2020-03-29 15:46:59.415603 2020-03-29T20:46:45.041274+00:00</pre>\n<p>This shows two things. First is that you can print and log to find errors in initial local development, and second, shows that code updates will be run on each execution of the task. The scheduler picked up the added line. In some web frameworks for example, if you change the code, you might have to restart your local server to have the changes be included. Here, we don&#8217;t have to, and those values will come in the logs.</p>\n<h2>Summary</h2>\n<p>If you got this far, you&#8217;re set up with a first DAG that writes to a file .<span style=\"color:var(--color-text);\">We showed the steps to get airflow running locally, and then up and going with a basic self written task and seeing the activity.</span></p>\n<p>That doesn&#8217;t sound like a lot, but with how big Airflow is, going from nothing to an initial set up, now matter how small, is a big part of the battle.</p>\n<p>In Part 2 of this series, we&#8217;re going to take these tasks, hook them up to a different database, write the datetimes there, and have another task in the DAG format the time that was written. With that, you&#8217;ll be much more comfortable with being able to connect to services anywhere.</p>\n<p>&nbsp;</p>\n",
  "wfw:commentRss": "https://bigishdata.com/2020/04/05/apache-airflow-part-1-introduction-setup-and-writing-data-to-files/feed/",
  "slash:comments": 0,
  "media:content": [
    {
      "media:title": "jackschultz23"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 2.42.49 PM"
    },
    {
      "media:title": "airflow-starting.png"
    },
    {
      "media:title": "airflow-admin-screenshot.png"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 3.26.43 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 3.30.10 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 3.43.42 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 3.46.44 PM.png"
    },
    {
      "media:title": "Screen Shot 2020-03-29 at 3.45.33 PM.png"
    }
  ]
}