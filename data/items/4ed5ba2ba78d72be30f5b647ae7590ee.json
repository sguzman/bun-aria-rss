{
  "title": "Structural Equation Modeling",
  "link": "https://www.mathematica-journal.com/2020/12/28/structural-equation-modeling/",
  "comments": "https://www.mathematica-journal.com/2020/12/28/structural-equation-modeling/#respond",
  "pubDate": "Mon, 28 Dec 2020 23:39:03 +0000",
  "dc:creator": "Todd Akers",
  "category": "Volume 22",
  "guid": "https://www.mathematica-journal.com/?p=60651",
  "description": "https://doi.org/10.3888/tmj.22-5 Structural equational modeling is a very popular statistical technique in the social sciences, as it is very flexible and includes factor analysis, path analysis and others as special cases. While usually done with specialized programs, the same can be achieved in Mathematica, which has the benefit of allowing control of any aspect of the [&#8230;]",
  "content:encoded": "<div>\n<div class=\"DOIReference\"><span style=\"font-size: xx-small;\"><a href=\"https://doi.org/10.3888/tmj.22-5\">https://doi.org/10.3888/tmj.22-5</a></span></div>\n<div>\n<p>Structural equational modeling is a very popular statistical technique in the social sciences, as it is very flexible and includes factor analysis, path analysis and others as special cases. While usually done with specialized programs, the same can be achieved in Mathematica, which has the benefit of allowing control of any aspect of the calculation. Moreover, a second, more flexible, approach to calculating these models is described that is conceptually much easier yet potentially more powerful. This second approach is used to describe a solution of the attenuation problem of regression.<span id=\"more-60651\"></span></p>\n<h3>The SEM Method</h3>\n<p>Linear structural equation modeling (SEM) is a technique that has found widespread use in many sciences in the last decades. An early foundational work is Bollen [<a href=\"#Bollen\">1</a>]; a more recent overview is provided by Hoyle [<a href=\"#Hoyle\">2</a>]. The basic idea is to model the linear structure of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_1.gif\" alt=\"\" width=\"6\" height=\"12\" /> observed variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_2.gif\" alt=\"\" width=\"49\" height=\"12\" /> of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_3.gif\" alt=\"\" width=\"6\" height=\"12\" /> cases (observations, subjects) by linear equations that may involve latent variables. These variables are not measured directly but inferred from the observed variables by their linear relation to the observed variables.</p>\n<p>Many commercial programs (including LISREL, Amos, Mplus) and free ones (including lavaan, sem, OpenMX) have been developed to carry out the estimation procedure. From my perspective, the R package lavaan [<a href=\"#Gana\">3</a>, <a href=\"#Rosseel\">4</a>] by Yves Rosseel is the most reliable and convenient one among the free programs. I use it as the gold standard to judge results of my own code.</p>\n<p>This article first gives a quick overview of the standard SEM theory, then shows how to perform the calculations in Mathematica. In the last section, a second approach is discussed.</p>\n<h4>The Standard Example</h4>\n<p>There is a standard example due to Bollen that is also used in the lavaan manual. The dataset consists of observations of 11 manifest variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_4.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_5.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_6.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_7.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_8.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_9.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_10.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_11.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_12.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_13.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_14.gif\" alt=\"\" width=\"12\" height=\"12\" />. SEM models are usually depicted graphically. In the lavaan documentation, this is displayed as in Figure 1.</p>\n<p><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/1.gif\" alt=\"\" width=\"342\" height=\"283\" /></p>\n<p class=\"NumberedFigureCaption\"><strong>Figure 1.</strong> Bollen<span class=\"special-character CloseCurlyQuote\">’</span>s democracy model (image from lavaan documentation [<a href=\"#Rosseel\">4</a>]).</p>\n<p>The variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_15.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_16.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_17.gif\" alt=\"\" width=\"12\" height=\"12\" /> are observed variables that measure the construct of industrialization in 1960, which is described by the latent variable <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_18.gif\" alt=\"\" width=\"27\" height=\"12\" />. This means that the level of industrialization is assumed to be representable by one number for each country, but this number cannot be measured directly; it has to be inferred from its linear relation to gross national product <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_19.gif\" alt=\"\" width=\"12\" height=\"12\" />, energy consumption per capita <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_20.gif\" alt=\"\" width=\"12\" height=\"12\" /> and share of industrial workers <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_21.gif\" alt=\"\" width=\"12\" height=\"12\" />. Next, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_22.gif\" alt=\"\" width=\"33\" height=\"12\" /> and <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_23.gif\" alt=\"\" width=\"33\" height=\"12\" /> are the democracy levels in 1960 and 1965, measured by <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_24.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_25.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_26.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_27.gif\" alt=\"\" width=\"12\" height=\"12\" /> and <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_28.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_29.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_30.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_31.gif\" alt=\"\" width=\"12\" height=\"12\" /> (these indicators are freedom of the press, etc.). The data matrix consists of these 11 numbers for each of 75 countries (cases). The data is delivered with the lavaan package for R. The aim of estimating the model is twofold. First, the weights of the linear connections (represented in the picture by arrows) are estimated. These arrows encode linear equations by the rule that all arrows that end in a variable indicate a linear combination that yields the value of this variable plus some error term variable. To bring this mysterious language down to earth, here are the equations represented in Figure 1:</p>\n<p><img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_32.gif\" alt=\"\" width=\"88\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_33.gif\" alt=\"\" width=\"57\" height=\"12\" />,<br />\n<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_34.gif\" alt=\"\" width=\"95\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_35.gif\" alt=\"\" width=\"70\" height=\"12\" />,<br />\n<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_36.gif\" alt=\"\" width=\"95\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_37.gif\" alt=\"\" width=\"70\" height=\"12\" />,<br />\n<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_38.gif\" alt=\"\" width=\"119\" height=\"12\" />,<br />\n<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_39.gif\" alt=\"\" width=\"183\" height=\"12\" />.</p>\n<p>The variable <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_40.gif\" alt=\"\" width=\"27\" height=\"12\" /> is called an exogenous latent variable because no arrow ends there. It has no associated error variable. However, its manifest (measured) indicator variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_41.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_42.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_43.gif\" alt=\"\" width=\"12\" height=\"12\" /> have associated error variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_44.gif\" alt=\"\" width=\"9\" height=\"12\" /> (they are called <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_45.gif\" alt=\"\" width=\"10\" height=\"12\" /> in [<a href=\"#Bollen\">1</a>]). The indicator variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_46.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_47.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_48.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_49.gif\" alt=\"\" width=\"12\" height=\"12\" /> and <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_50.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_51.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_52.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_53.gif\" alt=\"\" width=\"12\" height=\"12\" /> of the two endogenous latent variables (those latent variables where arrows end) have error variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_54.gif\" alt=\"\" width=\"9\" height=\"12\" /> (called <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_55.gif\" alt=\"\" width=\"10\" height=\"12\" /> in [<a href=\"#Bollen\">1</a>]). The equations that relate latent and manifest variables define the measurement part of the model. The two equations (coming from three arrows) between the latent variables are the structure model, usually of most interest. Fitting the model to the data gives estimates for the weights of the arrows, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_56.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_57.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_58.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_59.gif\" alt=\"\" width=\"11\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_60.gif\" alt=\"\" width=\"14\" height=\"12\" /> <span class=\"special-character Ellipsis\">…</span>. The second goal of SEM modeling is to check how well the structure of the model fits the data; that is, SEM is also a hypothesis-testing method.</p>\n<p>The equations given do not yet identify all variables. Assume we have a solution of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_61.gif\" alt=\"\" width=\"88\" height=\"12\" />; then for any number <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_62.gif\" alt=\"\" width=\"6\" height=\"12\" />, the numbers <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_63.gif\" alt=\"\" width=\"53\" height=\"12\" /> and <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_64.gif\" alt=\"\" width=\"90\" height=\"12\" /> would be solutions, too. To avoid this problem, we either fix the variance of the latent variables to be 1 or we fix some of the weights to be 1. This is the default in lavaan and we adopt it here, hence <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_65.gif\" alt=\"\" width=\"31\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_66.gif\" alt=\"\" width=\"32\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_67.gif\" alt=\"\" width=\"32\" height=\"12\" />.</p>\n<h3>The Standard Way of Estimating SEM</h3>\n<p>Ever since SEM<span class=\"special-character CloseCurlyQuote\">’</span>s invention, SEM models are estimated by calculating the model<span class=\"special-character CloseCurlyQuote\">’</span>s covariance matrix. From the data, we get the empirical covariance matrix <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_68.gif\" alt=\"\" width=\"6\" height=\"12\" />. On the other hand, from the model, we can calculate a theoretical covariance matrix <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_69.gif\" alt=\"\" width=\"7\" height=\"12\" /> between the observed variables. (<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_70.gif\" alt=\"\" width=\"7\" height=\"12\" /> depends on the model and thus on the parameters.) For example, one entry in this matrix would be <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_71.gif\" alt=\"\" width=\"238\" height=\"12\" />. Using linearity and other properties of the covariance, this boils down to a matrix with entries that are polynomials in the model parameters and the covariances and variances between latent variables and error variables. However, without further assumptions, this gives a lot of covariances (e.g. <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_72.gif\" alt=\"\" width=\"54\" height=\"12\" />) that are not determined by the model and hence must be estimated. As this usually leads to too much freedom, the broad assumption is that most error variables are uncorrelated. Only some covariances between error variables are not assumed to be 0; those are marked in the diagram by two-headed arrows between the observed variables. For every pair of observed variables, we calculate the covariance by using the above given model equation as replacement rules and applies linearity and independence assumptions. In the end, we get a covariance matrix <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_73.gif\" alt=\"\" width=\"7\" height=\"12\" /> that depends on the model parameters <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_74.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_75.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_76.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_77.gif\" alt=\"\" width=\"11\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_78.gif\" alt=\"\" width=\"11\" height=\"12\" />, <span class=\"special-character Ellipsis\">…</span> and on the variances of the latent variables and the covariances of error variables that are not assumed to be 0. Details can be found in Bollen [<a href=\"#Bollen\">1</a>].</p>\n<p>To fit the empirical and the theoretical covariance matrix, we have to choose these parameters to minimize some distance function. The three most common are uniform least-square, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_79.gif\" alt=\"\" width=\"105\" height=\"23\" />, generalized least-square, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_80.gif\" alt=\"\" width=\"129\" height=\"23\" /> (<em>I</em> is the identity matrix), and maximum likelihood, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_81.gif\" alt=\"\" width=\"209\" height=\"18\" /> (here <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_82.gif\" alt=\"\" width=\"6\" height=\"12\" /> is the number of manifest variables).</p>\n<p>Now we are in the position to define a Mathematica function that performs SEM. First, we define the helper function <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_83.gif\" alt=\"\" width=\"108\" height=\"14\" /> that gets all variables contained in an expression in such a way that, for example, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_84.gif\" alt=\"\" width=\"28\" height=\"13\" /> counts as one variable.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_1.gif\" alt=\"\" width=\"580\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_2.gif\" alt=\"\" width=\"261\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_3.gif\" alt=\"\" width=\"194\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_4.gif\" alt=\"\" width=\"574\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_5.gif\" alt=\"\" width=\"357\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_6.gif\" alt=\"\" width=\"574\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_7.gif\" alt=\"\" width=\"575\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_8.gif\" alt=\"\" width=\"480\" height=\"140\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_9.gif\" alt=\"\" width=\"274\" height=\"169\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_10.gif\" alt=\"\" width=\"259\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_11.gif\" alt=\"\" width=\"411\" height=\"14\" /></p>\n<p>Here is an example.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_12.gif\" alt=\"\" width=\"196\" height=\"14\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_1.gif\" alt=\"\" width=\"60\" height=\"13\" /></p>\n<p>The method will be explained with Bollen<span class=\"special-character CloseCurlyQuote\">’</span>s democracy dataset, so first, we need to load this dataset. The file bollen.csv contains headers (the names of the variables are saved in the list <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_85.gif\" alt=\"\" width=\"101\" height=\"13\" />) and a first column numbering the cases, which is dropped.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_13.gif\" alt=\"\" width=\"580\" height=\"107\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_2.gif\" alt=\"\" width=\"278\" height=\"14\" /></p>\n<p>The data has 75 rows.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_14.gif\" alt=\"\" width=\"153\" height=\"13\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_3.gif\" alt=\"\" width=\"53\" height=\"13\" /></p>\n<p>Here is the first row of 11 numbers.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_15.gif\" alt=\"\" width=\"114\" height=\"13\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_4.gif\" alt=\"\" width=\"496\" height=\"13\" /></p>\n<p>The model itself has to be specified as a list of replacement rules that mirror the model equations discussed.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_16.gif\" alt=\"\" width=\"234\" height=\"263\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_17.gif\" alt=\"\" width=\"490\" height=\"13\" /></p>\n<p>The code for the estimation function <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_86.gif\" alt=\"\" width=\"22\" height=\"13\" /> includes some utilities. For example, it defines its own covariance and variance functions that take into account which variables are assumed to be uncorrelated. The input of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_87.gif\" alt=\"\" width=\"22\" height=\"13\" /> is the data matrix <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_88.gif\" alt=\"\" width=\"29\" height=\"13\" />, a matrix of numerical values, one row per case. The structural equations <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_89.gif\" alt=\"\" width=\"72\" height=\"13\" /> are given in the format detailed in the previous section, <span class=\"special-character OpenCurlyDoubleQuote\">“</span>The Standard Example.<span class=\"special-character CloseCurlyDoubleQuote\">”</span> Moreover, the function needs:</p>\n<p><span class=\"special-character Bullet\">•</span> the lists of free parameters, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_90.gif\" alt=\"\" width=\"72\" height=\"14\" /> (e.g. path weights)</p>\n<p><span class=\"special-character Bullet\">•</span> endogenous latent variables, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_91.gif\" alt=\"\" width=\"180\" height=\"14\" /></p>\n<p><span class=\"special-character Bullet\">•</span> exogenous latent variables, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_92.gif\" alt=\"\" width=\"173\" height=\"14\" /></p>\n<p><span class=\"special-character Bullet\">•</span> the list of error variables of latent variables, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_93.gif\" alt=\"\" width=\"223\" height=\"13\" /></p>\n<p><span class=\"special-character Bullet\">•</span> errors of exogenous manifest variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_94.gif\" alt=\"\" width=\"245\" height=\"14\" /></p>\n<p><span class=\"special-character Bullet\">•</span> errors of endogenous manifest variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_95.gif\" alt=\"\" width=\"252\" height=\"14\" /></p>\n<p><span class=\"special-character Bullet\">•</span> a list <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_96.gif\" alt=\"\" width=\"72\" height=\"13\" /> of pairs of error variables specifying which error variables are allowed to be correlated</p>\n<p>The code after defining <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_97.gif\" alt=\"\" width=\"7\" height=\"13\" /> can be omitted on a first reading; it is only needed to calculate some fit indices (if required by the option <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_98.gif\" alt=\"\" width=\"29\" height=\"13\" />, which asks to do the fit index (FI) calculation; similarly, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_99.gif\" alt=\"\" width=\"29\" height=\"13\" /> asks to do the maximum likelihood estimation). The estimation is done at the end of the function.</p>\n<p>The goal of the first half of the <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_100.gif\" alt=\"\" width=\"22\" height=\"13\" /> program is the definition of the covariance function <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_101.gif\" alt=\"\" width=\"22\" height=\"13\" /> that takes into account the SEM assumptions: that most error variables are uncorrelated (except those specified to be correlated), leaving variances of latent variables as symbolic entities to be estimated.</p>\n<p>This function is then used to calculate the model implied covariance matrix <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_102.gif\" alt=\"\" width=\"7\" height=\"13\" />. Applying the model equation rules repeatedly gives a matrix that depends only on parameters, variances of latent variables and error variables and some allowed covariances of error variables. The code from the line defining <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_103.gif\" alt=\"\" width=\"14\" height=\"13\" /> (the degree of freedom) onward is only important for getting fit indices. If we are only interested in estimating the model parameters, the next interesting lines are where <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_104.gif\" alt=\"\" width=\"79\" height=\"13\" /> is applied to estimate the model. As described in the introduction, there are several strategies to measure deviation of covariance matrices; for example, the definition of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_105.gif\" alt=\"\" width=\"22\" height=\"13\" /> is a straightforward coding for minimizing <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_106.gif\" alt=\"\" width=\"134\" height=\"23\" />.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_18.gif\" alt=\"\" width=\"419\" height=\"30\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_19.gif\" alt=\"\" width=\"580\" height=\"1254\" /></p>\n<p>Let us run the code on Bollen&#8217;s model in a simplified version where no correlation of error variables is assumed. This may take several minutes.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_20.gif\" alt=\"\" width=\"580\" height=\"44\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_5.gif\" alt=\"\" width=\"580\" height=\"366\" /></p>\n<p>The result combines parameter, variance and covariance estimations according to the various estimating strategies. To judge how well the model fits the data, you can set the option <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_107.gif\" alt=\"\" width=\"29\" height=\"13\" /> to some fit indices:</p>\n<p><span class=\"special-character Bullet\">•</span> RMSEA is the root square mean error</p>\n<p><span class=\"special-character Bullet\">•</span> CFI is the comparable fit index</p>\n<p><span class=\"special-character Bullet\">•</span> TLI is the Tucker<span class=\"special-character Dash\">–</span>Lewis fit index</p>\n<p><span class=\"special-character Bullet\">•</span> NFI is the normed fit index</p>\n<p>RMSEA should be less than 0.1 or better, less than 0.05, and the last three should all be greater than 0.9 or 0.95 for good model fit.</p>\n<p>The results of estimating using the three different methods differ somewhat. This is not a bug of our program; lavaan determines the same numbers up to several decimal places. There are results in the literature about which methods are equivalent under which conditions. For these fit indices to be interpretable, we need to assume that the data is multivariate normally distributed. If this assumption is violated, then we should judge model fit by other indices, which is beyond the scope of this article; however, they could be calculated based on the current approach as well. The book edited by Hoyle [<a href=\"#Hoyle\">2</a>] gives some information on these methods.</p>\n<p>For the original model that allows some covariances between error variables, the runtime gets worse, especially for maximum likelihood estimation. Hence, this is turned off in the following code.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_21.gif\" alt=\"\" width=\"580\" height=\"44\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_6.gif\" alt=\"\" width=\"580\" height=\"304\" /></p>\n<p>The results of both models are exactly the same as calculated with lavaan.</p>\n<h3>An Alternative Approach: Case-based Estimation</h3>\n<p>When I first learned about SEM, I was puzzled by the many notions (e.g. exogenous, endogenous) and the assumptions needed. For example, I felt that correlation of error variables should be calculated by the estimation algorithm and not be set at will when specifying the model. However, these difficulties seem to play no large role in practice and there are thousands of research papers (mainly) in the social sciences that use these methods with great success. Yet, there are some reasons why the standard approach to SEM via covariance matrices can be criticized (a more detailed discussion is given in [<a href=\"#Oldenburg\">5</a>]). Traditional SEM:</p>\n<p><span class=\"special-character Bullet\">•</span> is well suited only for linear models (there are some nonlinear extensions, but they have not yet become mainstream)</p>\n<p><span class=\"special-character Bullet\">•</span> does not give estimates of the values of latent variables for each case (Bayesian variants can do this)</p>\n<p><span class=\"special-character Bullet\">•</span> requires the covariance matrix of observed data to be nonsingular; however, improving measurement methods in <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_108.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_109.gif\" alt=\"\" width=\"12\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_110.gif\" alt=\"\" width=\"12\" height=\"12\" />, for example, may result in highly correlated measures of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_111.gif\" alt=\"\" width=\"27\" height=\"12\" /> (in the extreme case with identical vectors of measured values) and hence their covariance matrix will be almost singular</p>\n<p><span class=\"special-character Bullet\">•</span> has resulting estimations for parameters that depend a lot on the estimation method used</p>\n<p><span class=\"special-character Bullet\">•</span> forbids certain linear models that are not identified in this approach, even though the model itself is sensible and well defined (e.g. the number of covariances of error variables allowed to be nonzero is limited, although in practice there may be correlations)</p>\n<p>You may then wonder why the covariance matrix<span class=\"special-character Dash\">–</span>based approach is so popular. I suppose that more than 40 years ago, computers were not powerful enough to deal with a full dataset, so that the information reduction by calculating the correlation matrix was essential. Since then, many powerful programs have been developed and research has been carried out that gave a good understanding of conditions under which the method works well. Moreover, the psychometric community reached a consensus on how model fit should be judged and thus studies using this method faced no problem being published.</p>\n<p>After this discussion of pros and cons, it is time to present the following case-based approach to SEM estimation that is very easy (one may even call it naive) to implement but is also very flexible and with today&#8217;s computing power, it is feasible in many real-world situations.</p>\n<p>Hence, I propose to do SEM case-based by least-square optimization of the defects of the equations. Assume we have <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_112.gif\" alt=\"\" width=\"6\" height=\"12\" /> observations (cases) of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_113.gif\" alt=\"\" width=\"6\" height=\"12\" /> variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_114.gif\" alt=\"\" width=\"39\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_115.gif\" alt=\"\" width=\"43\" height=\"12\" />. A general equational model consists of <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_116.gif\" alt=\"\" width=\"9\" height=\"12\" /> equations <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_117.gif\" alt=\"\" width=\"107\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_118.gif\" alt=\"\" width=\"49\" height=\"12\" />, which involve the data, latent variables <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_119.gif\" alt=\"\" width=\"11\" height=\"12\" />, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_120.gif\" alt=\"\" width=\"52\" height=\"12\" />, and parameters <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_121.gif\" alt=\"\" width=\"9\" height=\"12\" />. Then the latent variables and the parameters are estimated by minimizing <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_122.gif\" alt=\"\" width=\"157\" height=\"20\" />.</p>\n<p>Another twist is needed to get the best results, however. The above objective function gives all equations the same weight. However, it turned out (by working with simulated data where it is clear which parameters should be found) that we get better results by multiplying by a factor that gives the equations different weights, that is, <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_123.gif\" alt=\"\" width=\"169\" height=\"20\" />. The factor <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_124.gif\" alt=\"\" width=\"10\" height=\"12\" /> can be modified by an option in the code that follows. Best results are obtained for <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_125.gif\" alt=\"\" width=\"42\" height=\"12\" />, where <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_126.gif\" alt=\"\" width=\"6\" height=\"12\" /> is the number of latent variables in <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_127.gif\" alt=\"\" width=\"11\" height=\"12\" />. The idea behind this choice is that an equation that involves only one latent variable links this variable directly to the manifest data and thus should have a high weight. In contrast, equations with many latent variables are not so close to the manifest observations and are thus are more hypothetical, so they should have a lower weight.</p>\n<p>The model equations are not formulated as rules as for the first SEM, but as equations with the name of the error variable attached to each equation. Moreover, the dataset is not normalized, so there are nonzero intercepts in the linear equations. In the first approach this had no consequences, because such additive values are eliminated by calculating the covariance matrix, but in the SEM2 approach, intercepts must be modeled explicitly (and we have the benefit of getting estimates for them as well).</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_22n.gif\" alt=\"\" width=\"282\" height=\"233\" /></p>\n<p>The function SEM2 that carries out the model estimation takes as input <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_128.gif\" alt=\"\" width=\"29\" height=\"13\" /> and the names of the manifest (<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_129.gif\" alt=\"\" width=\"58\" height=\"13\" />) and latent variables (<img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_130.gif\" alt=\"\" width=\"108\" height=\"13\" />). At the technical heart of the function is the subroutine <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_131.gif\" alt=\"\" width=\"14\" height=\"13\" />. This function takes an equation involving latent variables (e.g. <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_132.gif\" alt=\"\" width=\"68\" height=\"13\" /> <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_133.gif\" alt=\"\" width=\"63\" height=\"13\" />) and adds to the objective function <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_134.gif\" alt=\"\" width=\"101\" height=\"14\" /> the appropriate term for each case (i.e. with values from the data replacing the names of manifest variables):</p>\n<p><img style=\"vertical-align: middle;\" title=\"(dem60[1]-(b1 ind60[1]+u1[1]))^2+ (dem60[2]-(b1 ind60[2]+u1[2]))^2+ ...+ (dem60[n]-(b1 ind60[n]+u1[n]))^2\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Image_2.gif\" alt=\"(dem60[1]-(b1 ind60[1]+u1[1]))^2+ (dem60[2]-(b1 ind60[2]+u1[2]))^2+ ...+ (dem60[n]-(b1 ind60[n]+u1[n]))^2\" width=\"239\" height=\"68\" /></p>\n<p>There is one option.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_23.gif\" alt=\"\" width=\"283\" height=\"14\" /></p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_24.gif\" alt=\"\" width=\"580\" height=\"475\" /></p>\n<p>This code estimates Bollen<span class=\"special-character CloseCurlyQuote\">’</span>s model.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_25.gif\" alt=\"\" width=\"538\" height=\"29\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_7.gif\" alt=\"\" width=\"659\" height=\"69\" /></p>\n<p>As mentioned, there is a version that weights equations according to the number of latent variables they have.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_26.gif\" alt=\"\" width=\"531\" height=\"46\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_8.gif\" alt=\"\" width=\"659\" height=\"69\" /></p>\n<p>The results for the estimates differ from what is calculated in the traditional covariance matrix<span class=\"special-character Dash\">–</span>based approach given for <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_135.gif\" alt=\"\" width=\"22\" height=\"13\" />. A simulation study that compares the two approaches [<a href=\"#Oldenburg\">5</a>] showed that in many situations the case-based approach gives better results, especially when the assumption of independent errors is violated. Moreover, the case-based approach is easily applied to nonlinear equations. However, in certain situations it may be necessary to perform the minimization with higher accuracy than provided by standard hardware floating-point numbers.</p>\n<h4>Application to Measurement Error</h4>\n<p>In standard linear regression <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_136.gif\" alt=\"\" width=\"22\" height=\"12\" />, one assumes that the independent variables are measured exactly, while the dependent variable has an error that is ideally normally distributed. If the independent variables are measured with error too, standard linear regression underestimates the regression coefficient. This is the famous attenuation problem and I will show how to solve it. Let us first simulate a dataset with error on both variables.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_27.gif\" alt=\"\" width=\"379\" height=\"195\" /></p>\n<p>Then linear regression underestimates the slope, which should be 0.5.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_28.gif\" alt=\"\" width=\"296\" height=\"14\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_9.gif\" alt=\"\" width=\"216\" height=\"27\" /></p>\n<p>When using case-based modeling, several strategies are possible. We may use one or two latent variables for the true values. As the true dependent variable is just <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_137.gif\" alt=\"\" width=\"46\" height=\"12\" />, the following code uses just one latent variable. Another twist is that the equations are divided by the empirical standard deviations to put them on an equal footing.</p>\n<p class=\"input\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Input_29.gif\" alt=\"\" width=\"237\" height=\"123\" /></p>\n<p class=\"output\"><img src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Output_10.gif\" alt=\"\" width=\"58\" height=\"13\" /></p>\n<p>This example shows both the power of this method and the responsibility of the modeler to set up sensible equations. If we are sure that the errors are uncorrelated, we may add <img style=\"vertical-align: middle;\" src=\"https://content.wolfram.com/uploads/sites/19/2020/12/Oldenburg_Math_138.gif\" alt=\"\" width=\"184\" height=\"13\" /> as another constraint to further improve the estimate. This may also be done automatically with an extended version of SEM2, which will be published when its development is completed.</p>\n<h3>Summary</h3>\n<p>Two methods for the estimation of structural equational models are presented. One uses the traditional covariance matrix<span class=\"special-character Dash\">–</span>based approach and is therefore restricted to linear equations, while the other approach is more general but not yet established in practice. Estimating the models is rather easy in Mathematica, but the numerical problems that arise can be demanding. The new case-based approach is very flexible and promising in certain situations where the standard approach shows limitations.</p>\n<h3>Conclusion</h3>\n<p>Case-based calculation of SEM looks very promising given the numerical power of today<span class=\"special-character CloseCurlyQuote\">’</span>s computers and might give insight in situations where the restrictions of the traditional approach urge researchers into making assumptions that may not be warranted.</p>\n<h3>Acknowledgments</h3>\n<p>It is my pleasure to thank Ed Merkle and Yves Rosseel for many explanations of SEM.</p>\n<h3>References</h3>\n<table class=\"ReferenceTable\">\n<tbody>\n<tr>\n<td class=\"Reference\"><a name=\"Bollen\"></a>[1]</td>\n<td>K. A. Bollen, <em>Structural Equations with Latent Variables</em>, New York: Wiley, 1989.</td>\n</tr>\n<tr>\n<td class=\"Reference\"><a name=\"Hoyle\"></a>[2]</td>\n<td>R. H. Hoyle (ed.), <em>Handbook of Structural Equation Modeling</em>, New York: Guilford Press, 2012.</td>\n</tr>\n<tr>\n<td class=\"Reference\"><a name=\"Gana\"></a>[3]</td>\n<td>K. Gana and G. Broc, <em>Structural Equation Modeling with lavaan</em>, Hoboken: John Wiley & Sons, 2019.</td>\n</tr>\n<tr>\n<td class=\"Reference\"><a name=\"Rosseel\"></a>[4]</td>\n<td>Y. Rosseel. <span class=\"special-character OpenCurlyDoubleQuote\">“</span>lavaan.<span class=\"special-character CloseCurlyDoubleQuote\">”</span> (Aug 25, 2019) <a href=\"https://lavaan.ugent.be/\" target=\"_blank\">https://lavaan.ugent.be</a>.</td>\n</tr>\n<tr>\n<td class=\"Reference\"><a name=\"Oldenburg\"></a>[5]</td>\n<td>R. Oldenburg, <span class=\"special-character OpenCurlyDoubleQuote\">“</span>Case-based vs. Covariance-based SEM,<span class=\"special-character CloseCurlyDoubleQuote\">”</span> forthcoming.</td>\n</tr>\n<tr>\n<td id=\"1099737697\" class=\"DOIReference\" colspan=\"2\"><a name=\"1099737697\"></a>R. Oldenburg, <span class=\"special-character OpenCurlyDoubleQuote\">“</span>Structural Equation Modeling,<span class=\"special-character CloseCurlyDoubleQuote\">”</span> <em>The Mathematica Journal</em>, 2020. <a href=\"https://doi.org/10.3888/tmj.22-5\">https://doi.org/10.3888/tmj.22–5</a>.</td>\n</tr>\n</tbody>\n</table>\n<h3 class=\"SectionAboutAuthor\">About the Author</h3>\n<p class=\"TextAboutAuthor\">Reinhard Oldenburg has studied physics and mathematics and received a PhD in algebra. He has been a high-school teacher and now holds a professorship in Mathematics Education at Augsburg University. His research interests are computer algebra, the logic of elementary algebra and real-world applications.</p>\n<p class=\"TextAboutAuthor\"><strong>Reinhard Oldenburg</strong><br />\n<em>Augsburg University<br />\nMathematics Department<br />\nUniversit<span class=\"special-character ADoubleDot\">ä</span>tsstra<span class=\"special-character SZ\">ß</span>e 14<br />\n86159 Augsburg, Germany<br />\n</em><em><a href=\"mailto:reinhard.oldenburg@math.uni-augsburg.de\">reinhard.oldenburg@math.uni-augsburg.de</a></em></p>\n</div>\n</div>\n",
  "wfw:commentRss": "https://www.mathematica-journal.com/2020/12/28/structural-equation-modeling/feed/",
  "slash:comments": 0
}