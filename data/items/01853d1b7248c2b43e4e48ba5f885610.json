{
  "title": "Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)",
  "link": "https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881?source=rss-55388a733bf9------2",
  "guid": "https://medium.com/p/c17d97a0e881",
  "category": [
    "machine-learning",
    "statistics",
    "data-science"
  ],
  "dc:creator": "Ahmed El Deeb",
  "pubDate": "Mon, 22 Jun 2015 08:32:10 GMT",
  "atom:updated": "2015-06-23T00:17:35.849Z",
  "content:encoded": "<p>Anyone that ever had to train a machine learning model had to go through some parameter sweeping (a.k.a. <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization\">hyper-parameter optimization</a>) to find a sweet spot for algorithm parameters. For random forests the parameters in need of optimization could be the number of trees in the model and the number of features considered at each split, for a neural network, there is the learning rate, the number of hidden layers, the number of hidden units in each layer, and several other parameters.</p><p>Hyper-parameter optimization requires the use (and maybe the abuse) of a validation set on which you can’t trust your performance metrics anymore. In this sense it is like a second phase of learning, or an extension to the learning algorithm itself. The performance metric (or the objective function) can be visualized as a heat-map in the n-dimensional parameter-space or as a surface in an n+1-dimensional space (the dimension n+1 being the value of that objective function). The bumpier this surface is (the more local minima and saddle points it has), the harder it becomes to optimize these parameters. Here are a couple of illustrations for two such surfaces defined by two parameters, the first one is mostly well behaved:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/368/1*fOCJ_mAi_-MNb7dfrXjW5A.gif\" /></figure><p>While the second one is more bumpy and riddled with several local minima:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/588/1*v-LBGd9GV0wZBJI3Vyqddw.jpeg\" /></figure><p>The most common method at selecting algorithm parameters is by far the ubiquitous grid-search. In fact, the word “parameter sweep” actually refers to performing a grid search but has also become synonymous with performing parameter optimization. Grid-search is performed by simply picking a list of values for each parameter, and trying out all possible combinations of these values. This might look methodical and exhaustive. But in truth <strong>even a random search of the parameter space can be MUCH more effective than a grid search!</strong></p><p>This amazing <a href=\"http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">paper</a> by Bergstra et al. claims that a random search of the parameter space is guaranteed to be more effective than grid search (and quite competitive in comparison with more sophisticated techniques).</p><p>Surprising, ha? Why should random search be better than the much more robust-looking grid-search? Here is why:</p><p>The idea is that in most cases the bumpy surface of the objective function is not as bumpy in all dimensions. Some parameters have much less effect on the cost function than others, if the importance of each parameter is known, this can be encoded in the number of values picked for each parameter in the grid-search. But that’s not typically the case, and anyway, just using random search allows the exploration of more values for each parameter, given the same amount of trials:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZTlQm_WRcrNqL-nLnx6GJA.png\" /></figure><p>(The beautiful illustration is taken from the same <a href=\"http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf\">paper</a> referenced above)</p><p><strong>More elaborate ways of optimizing algorithm hyper-parameters exist, in fact whole start-ups have been built around the idea (</strong><a href=\"http://techcrunch.com/2015/06/17/twitter-acquires-machine-learning-startup-whetlab/\"><strong>one of them recently acquired by twitter</strong></a><strong>). A </strong><a href=\"https://github.com/HIPS/Spearmint\"><strong>couple</strong></a><strong> of </strong><a href=\"https://github.com/hyperopt/hyperopt\"><strong>libraries</strong></a><strong> and </strong><a href=\"http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf\"><strong>several</strong></a><strong> </strong><a href=\"http://www.cs.ubc.ca/labs/beta/Projects/SMAC/papers/11-LION5-SMAC.pdf\"><strong>research</strong></a><strong> </strong><a href=\"http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf\"><strong>papers</strong></a><strong> tackle the problem, but for me, random sweeps are good enough for now.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c17d97a0e881\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881\">Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)</a> was originally published in <a href=\"https://medium.com/rants-on-machine-learning\">Rants on Machine Learning</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}