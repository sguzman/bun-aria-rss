{
  "title": "Weekly Review: 11/18/2017",
  "link": "https://codesachin.wordpress.com/2017/11/19/weekly-review-11-18-2017/",
  "comments": "https://codesachin.wordpress.com/2017/11/19/weekly-review-11-18-2017/#respond",
  "dc:creator": "srjoglekar246",
  "pubDate": "Sat, 18 Nov 2017 20:26:40 +0000",
  "category": [
    "Machine Learning/AI",
    "Programming",
    "adversarial",
    "baseline",
    "flatbuffer",
    "machine learning",
    "neural network",
    "tensorflow lite"
  ],
  "guid": "http://codesachin.wordpress.com/?p=912",
  "description": "I finished the Motion Planning course from Robotics this week. It was expected, since the material was quite in line with data structures and algorithms that I have studied during my undergrad. The next one, Mobility, seems to be a notch tougher than Aerial Robotics, mainly because of the focus on calculus and physics (neither &#8230; <a href=\"https://codesachin.wordpress.com/2017/11/19/weekly-review-11-18-2017/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Weekly Review: 11/18/2017</span> <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>I finished the <strong>Motion Planning</strong> course from <a href=\"https://www.coursera.org/specializations/robotics\">Robotics</a> this week. It was expected, since the material was quite in line with data structures and algorithms that I have studied during my undergrad. The next one, <a href=\"https://www.coursera.org/learn/robotics-mobility/home/welcome\">Mobility</a>, seems to be a notch tougher than Aerial Robotics, mainly because of the focus on calculus and physics (neither of which I have touched heavily in years).</p>\n<p>Heres the articles this week:</p>\n<p><strong><a href=\"https://medium.com/@karpathy/software-2-0-a64152b37c35\">Neural Networks: Software 2.0</a></strong></p>\n<p>In this article from Medium, the Director of AI at Tesla gives a fresh perspective on NNs. He refers to the set of <em>weights</em> in a Neural Network as a <em>program</em> which is learnt, as opposed to coded in by a human. This line of thought is justified by the fact that many decisions in Robotics, Search, etc. are taken by parametric ML systems. He also compares it to traditional &#8216;Software 1.0&#8217;, and points out the benefits of each.</p>\n<p><strong><a href=\"http://smerity.com/articles/2017/baselines_need_love.html\">Baselines in Machine Learning</a></strong></p>\n<p>In this article, a senior Research Scientist from Salesforce points out that we need to pay greater attention to baselines in Machine Learning. A <em>baseline</em> is any meaningful &#8216;benchmark&#8217; algorithm that you would compare your algorithm against. The actual reference point would depend on your task &#8211; random/stratified systems for classification, state-of-the-art CNNs for image processing, etc. Read Neal&#8217;s answer to <a href=\"https://www.quora.com/What-does-baseline-mean-in-machine-learning\">this Quora question</a> for a deeper understanding.</p>\n<p>The article ends with a couple of helpful tips, such as:</p>\n<ol>\n<li>Use meaningful baselines, instead of using very crude code. The better your baseline, the more meaningful your results.</li>\n<li>Start off with optimizing the baseline itself. Tune the weights, etc. if you have to &#8211; this gives you a good base to start your work on.</li>\n</ol>\n<p><a href=\"https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html?m=1\">TensorFlow Lite</a></p>\n<p>TensorFlow Lite is now in the Developer Preview mode. It is a light-weight platform for inference (not training) using ML models on mobile/embedded devices. Google calls it an &#8216;evolution of TensorFlow mobile&#8217;. While the latter is still the system you should use in production, TensorFlow lite appears to perform better on many benchmarks (<a href=\"https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html?m=1\">Differences here</a>). Some of the major plus-points of this new platform are smaller binaries, and support for custom ML-focussed hardware accelerators via the <a href=\"https://developer.android.com/ndk/guides/neuralnetworks/index.html\">Android Neural Networks API</a>.</p>\n<p><strong><a href=\"https://google.github.io/flatbuffers/\">Flatbuffers</a></strong></p>\n<p>Reading up on Tensorflow Lite also brought me to Flatbuffers, which are a &#8216;liter&#8217; version of Protobufs. Flatbuffer is a data serialization libraryÂ  for performance-critical applications. Flatbuffers provide the benefits of a smaller memory footprint and lesser generated code, mainly due to skipping of the parsing/unpacking step. Heres the <a href=\"https://github.com/google/flatbuffers\">Github repo</a>.</p>\n<p><strong><a href=\"https://blog.ycombinator.com/how-adversarial-attacks-work/\">Adversarial Attacks</a></strong></p>\n<p>This YCombinator article gives a nice overview of Adversarial attacks on ML models &#8211; attacks that provide &#8216;noisy&#8217; data inputs to intelligent systems, in order to get a &#8216;wrong&#8217; output. The author points out how Gradient descent can be used to sort-of reverse engineer spurious noise, in order to get data &#8216;misclassified&#8217; by a neural network. The article also shows examples of such faulty inputs, and they are surprisingly indistinguishable from the original data!</p>\n<p>&nbsp;</p>\n",
  "wfw:commentRss": "https://codesachin.wordpress.com/2017/11/19/weekly-review-11-18-2017/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "srjoglekar246"
  }
}