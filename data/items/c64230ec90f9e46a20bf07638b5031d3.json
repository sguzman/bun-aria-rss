{
  "title": "Beyond Binary: Ternary and One-hot Neurons",
  "link": "",
  "published": "2017-02-08T00:00:00-05:00",
  "updated": "2017-02-08T00:00:00-05:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2017-02-08:/beyond-binary-ternary-and-one-hot-neurons.html",
  "summary": "While playing with some applications of binary neurons, I found myself wanting to use explicit activations that go beyond a simple yes/no decision. For example, we might want our neural network to make a choice between several categories (in the form of a one-hot vector) or we might want it to make a choice between ordered categories (e.g., a scale of 1 to 10). It's rather easy to extend the straight-through estimator to work well on both of these cases, and I thought I would share my work in this post. I share code for implementing ternary and one-hot neurons in Tensorflow, and show that they can learn to solve MNIST.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\" type=\"text/javascript\"></script>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>While playing with some applications of binary neurons, I found myself wanting to use explicit activations that go beyond a simple yes/no decision. For example, we might want our neural network to make a choice between several categories (in the form of a one-hot vector) or we might want it to make a choice between ordered categories (e.g., a scale of 1 to 10). It’s rather easy to extend the straight-through estimator to work well on both of these cases, and I thought I would share my work in this post. I share code for implementing ternary and one-hot neurons in Tensorflow, and show that they can learn to solve MNIST.</p>\n<p>This is a follow-up post to <a href=\"http://r2rt.com/binary-stochastic-neurons-in-tensorflow.html\">Binary Stochastic Neurons in Tensorflow</a>, and assumes familiarity with binary neurons and the straight-through estimator discussed therein.</p>\n<p><strong>Note Feb. 8, 2017</strong>: I haven’t had a chance to read either of these two papers that I came across after writing this post (they look like they are related to the straight-through softmax activation (I think the first offers up an even better estimator… to be decided)):</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1611.01144\">Categorical Reparameterization with Gumbel-Softmax</a></li>\n<li><a href=\"https://arxiv.org/abs/1611.00712\">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</a></li>\n</ul>\n<p>I plan to update this post once I get around to reading them.</p>\n<p><strong>IPython Notebook</strong>: This post is also available as an IPython notebook <a href=\"https://gist.github.com/spitis/4d600aa019a5689953b1b61a8ddb074d\">here</a>.</p>\n<h2 id=\"general-n-ary-neurons\">General n-ary neurons</h2>\n<p>Whereas the binary neuron outputs 0 or 1, a ternary neuron might output -1, 0, or 1 (or alternatively, 0, 1 or 2). Similarly, we could create an arbitrary n-ary neuron that outputs ordered categories, such as a scale from 1 to 10. Actually, but for the activation function, the code for all of these neurons is the same as the code for the binary neuron: we either round the real output of the activation function to the nearest integer (deterministic), or use its decimal portion to sample either the integer below or the integer above from a bernoulli distribution (stochastic). Note that this means stochastic choices are made only between two adjacent categories, and never across all categories. On the backward pass, we use the straight-through estimator, which means that we replace the gradient of rounding (deterministic) or sampling (stochastic) with the identity. If our activation function is threshholded to [0, 1], this results in a binary neuron, but if it is threshholded to [-1,1], we can output three ordered values.</p>\n<p>We might be tempted to use tanh, which is threshholded to [-1, 1], to create ternary neurons. Picking the right activation, however, is a bit trickier than finding a function that has the correct range. The standard tanh is not very good here because its slope near 0 is close to 1, so that a neuron outputting 0 will tend to get pushed away from 0. Instead, we want something that looks like a soft set of stairs (where each step is a sigmoid), so that the neuron can learn to consistently output intermediate values.</p>\n<p>With such an activation, a neuron outputting 0 will have a small slope on the backward pass, so that many mistakes (in the same direction) will be required to move it towards 1 or -1.</p>\n<p>For the ternary case, the following function works well:</p>\n<p><span class=\"math display\">\\[f(x) = 1.5\\tanh(x) + 0.5\\tanh(-3x).\\]</span></p>\n<p>Here is its plot drawn by Wolfram Alpha:</p>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_ternary_activation.png\" alt=\"Ternary activation function\" /><figcaption>Ternary activation function</figcaption>\n</figure>\n<p>This activation function works well because its slope goes to 0 as it approaches each of the integers in the output range (note that the binary neuron has this property too). Here’s the plot of the derivative for reference:</p>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_ternary_derivative.png\" alt=\"Ternary activation derivative\" /><figcaption>Ternary activation derivative</figcaption>\n</figure>\n<p>The above ternary function is implemented below. I’ve been more interested in the one-hot activations, so I haven’t figured out how to make slope annealing work for this ternary neuron, or a general formula for n-ary neurons. If the mathematically-inclined reader would like to leave some ideas in the comments, that would be much appreciated.</p>\n<h2 id=\"one-hot-neurons\">One-hot neurons</h2>\n<p>N-ary neurons are cool, but restrictive. Binary neurons are restricted to yes/no decisions, and ternary+ neurons express the prior that the output categories are linearly ordered. As an example to illustrate this “ordering” point, consider the categories “dislike”, “neutral” and “like”. There is a natural order between these categories (“like” is closer to “neutral” than to “dislike”). Most categories, however, do not have a natural ordering. For example, there is no natural order between Boston, New York and Toronto. To create a neuron that can decided between unordered categories, we would like it to output a one-hot vector like [1, 0, 0] (Boston) or [0, 0, 1] (Toronto). Luckily, the straight through estimator extends nicely to this scenario.</p>\n<p>We define a d-dimensional one-hot neuron, <span class=\"math inline\">\\(N_d: \\mathbb{R}^n \\to \\mathbb{R}^d\\)</span> as follows. Given an input, <span class=\"math inline\">\\(x \\in \\mathbb{R}^n\\)</span>, we perform the following steps:</p>\n<ol type=\"1\">\n<li><p>Compute a d-dimensional vector of logits, <span class=\"math inline\">\\(z = Wx + b\\)</span>, where <span class=\"math inline\">\\(W \\in \\mathbb{R}^{n \\times d}\\)</span> and <span class=\"math inline\">\\(b \\in \\mathbb{R}^d\\)</span>.</p></li>\n<li><p>Compute softmax activations, <span class=\"math inline\">\\(\\hat{y} = \\text{softmax}_\\tau(z)\\)</span>, whose <span class=\"math inline\">\\(i\\)</span>-th component is</p>\n<p><span class=\"math display\">\\[\\hat{y}_i = \\frac{\\exp(z_i /\\tau)}{\\sum_{k=1}^d \\exp(z_k / \\tau)}\\]</span></p>\n<p>where <span class=\"math inline\">\\(z_i\\)</span> is the <span class=\"math inline\">\\(i\\)</span>-th component of <span class=\"math inline\">\\(z\\)</span>, and <span class=\"math inline\">\\(\\tau \\in (0, \\infty)\\)</span> is the temperature (used for annealing).</p></li>\n<li><p>Next, we either sample a one-hot vector according to the distribution defined by <span class=\"math inline\">\\(\\hat{y}\\)</span> (stochastic), or simply use the maximum value of <span class=\"math inline\">\\(\\hat{y}\\)</span> to determine the one-hot output (deterministic). The result is the output of our neuron, <span class=\"math inline\">\\(y\\)</span>. A formal definition of both of these operations is a bit too ugly for this blog post, but this should be fairly straightforward.</p></li>\n<li><p>Neither sampling from nor using the maximum of <span class=\"math inline\">\\(\\hat{y}\\)</span> have useful (non-zero) gradients. But we can use the straight-through estimator and replace their gradient on the backward pass with the identity. As in the binary case, this leads to a learnable softmax activation.</p></li>\n</ol>\n<p>This definition of one-hot neurons allows for temperature-annealing to be used (note that whereas slope is increased during annealing, temperature is decreased during annealing), which we test below.</p>\n<h2 id=\"implementation-in-tensorflow\">Implementation in Tensorflow</h2>\n<h4 id=\"imports-and-helper-functions\">Imports and helper functions</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np, tensorflow <span class=\"im\">as</span> tf, matplotlib.pyplot <span class=\"im\">as</span> plt, seaborn <span class=\"im\">as</span> sns\n<span class=\"im\">from</span> tensorflow.examples.tutorials.mnist <span class=\"im\">import</span> input_data\n<span class=\"op\">%</span>matplotlib inline\nsns.<span class=\"bu\">set</span>(color_codes<span class=\"op\">=</span><span class=\"va\">True</span>)\nmnist <span class=\"op\">=</span> input_data.read_data_sets(<span class=\"st\">&#39;MNIST_data&#39;</span>, one_hot<span class=\"op\">=</span><span class=\"va\">True</span>)\n<span class=\"im\">from</span> tensorflow.python.framework <span class=\"im\">import</span> ops\n<span class=\"im\">from</span> collections <span class=\"im\">import</span> Counter\n\n<span class=\"kw\">def</span> reset_graph():\n    <span class=\"cf\">if</span> <span class=\"st\">&#39;sess&#39;</span> <span class=\"kw\">in</span> <span class=\"bu\">globals</span>() <span class=\"kw\">and</span> sess:\n        sess.close()\n    tf.reset_default_graph()\n\n<span class=\"kw\">def</span> layer_linear(inputs, shape, scope<span class=\"op\">=</span><span class=\"st\">&#39;linear_layer&#39;</span>):\n    <span class=\"cf\">with</span> tf.variable_scope(scope):\n        w <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;w&#39;</span>,shape)\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>,shape[<span class=\"op\">-</span><span class=\"dv\">1</span>:])\n    <span class=\"cf\">return</span> tf.matmul(inputs,w) <span class=\"op\">+</span> b\n\n<span class=\"kw\">def</span> layer_softmax(inputs, shape, scope<span class=\"op\">=</span><span class=\"st\">&#39;softmax_layer&#39;</span>):\n    <span class=\"cf\">with</span> tf.variable_scope(scope):\n        w <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;w&#39;</span>,shape)\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>,shape[<span class=\"op\">-</span><span class=\"dv\">1</span>:])\n    <span class=\"cf\">return</span> tf.nn.softmax(tf.matmul(inputs,w) <span class=\"op\">+</span> b)\n\n<span class=\"kw\">def</span> compute_accuracy(y, pred):\n    correct <span class=\"op\">=</span> tf.equal(tf.argmax(y,<span class=\"dv\">1</span>), tf.argmax(pred,<span class=\"dv\">1</span>))\n    <span class=\"cf\">return</span> tf.reduce_mean(tf.cast(correct, tf.float32))\n\n<span class=\"kw\">def</span> plot_n(data_and_labels, lower_y <span class=\"op\">=</span> <span class=\"fl\">0.</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;Learning Curves&quot;</span>):\n    fig, ax <span class=\"op\">=</span> plt.subplots()\n    <span class=\"cf\">for</span> data, label <span class=\"kw\">in</span> data_and_labels:\n        ax.plot(<span class=\"bu\">range</span>(<span class=\"dv\">0</span>,<span class=\"bu\">len</span>(data)<span class=\"op\">*</span><span class=\"dv\">100</span>,<span class=\"dv\">100</span>),data, label<span class=\"op\">=</span>label)\n    ax.set_xlabel(<span class=\"st\">&#39;Training steps&#39;</span>)\n    ax.set_ylabel(<span class=\"st\">&#39;Accuracy&#39;</span>)\n    ax.set_ylim([lower_y,<span class=\"dv\">1</span>])\n    ax.set_title(title)\n    ax.legend(loc<span class=\"op\">=</span><span class=\"dv\">4</span>)\n    plt.show()</code></pre></div>\n<h4 id=\"functions-for-ternary-and-n-ary-neurons\">Functions for ternary and n-ary neurons</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> st_round(x):\n    <span class=\"co\">&quot;&quot;&quot;Rounds a tensor using the straight through estimator for the gradient.&quot;&quot;&quot;</span>\n    g <span class=\"op\">=</span> tf.get_default_graph()\n\n    <span class=\"cf\">with</span> ops.name_scope(<span class=\"st\">&quot;StRound&quot;</span>) <span class=\"im\">as</span> name:\n        <span class=\"cf\">with</span> g.gradient_override_map({<span class=\"st\">&quot;Round&quot;</span>: <span class=\"st\">&quot;Identity&quot;</span>}):\n            <span class=\"cf\">return</span> tf.<span class=\"bu\">round</span>(x, name<span class=\"op\">=</span>name)</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> sample_closest_ints(x):\n    <span class=\"co\">&quot;&quot;&quot;If x is a float, then samples floor(x) with probability x - floor(x), and ceil(x) with</span>\n<span class=\"co\">    probability ceil(x) - x, using the straight through estimator for the gradient.</span>\n\n<span class=\"co\">    E.g.,:</span>\n<span class=\"co\">    if x is 0.6, sample_closest_ints(x) will be 1 with probability 0.6, and 0 otherwise,</span>\n<span class=\"co\">    and the gradient will be pass-through (identity).</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    <span class=\"cf\">with</span> ops.name_scope(<span class=\"st\">&quot;SampleClosestInts&quot;</span>) <span class=\"im\">as</span> name:\n        <span class=\"cf\">with</span> tf.get_default_graph().gradient_override_map({<span class=\"st\">&quot;Ceil&quot;</span>: <span class=\"st\">&quot;Identity&quot;</span>,<span class=\"st\">&quot;Sub&quot;</span>: <span class=\"st\">&quot;SampleClosestInts&quot;</span>}):\n            <span class=\"cf\">return</span> tf.ceil(x <span class=\"op\">-</span> tf.random_uniform(tf.shape(x)), name<span class=\"op\">=</span>name)\n\n<span class=\"at\">@ops.RegisterGradient</span>(<span class=\"st\">&quot;SampleClosestInts&quot;</span>)\n<span class=\"kw\">def</span> sample_closest_ints_grad(op, grad):\n    <span class=\"cf\">return</span> [grad, tf.zeros(tf.shape(op.inputs[<span class=\"dv\">1</span>]))]</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> binary_activation(x, slope_tensor):\n    <span class=\"cf\">return</span> tf.cond(tf.equal(<span class=\"fl\">1.</span>, slope_tensor),\n                <span class=\"kw\">lambda</span>: tf.sigmoid(x),\n                <span class=\"kw\">lambda</span>: tf.sigmoid(slope_tensor <span class=\"op\">*</span> x))\n\n<span class=\"kw\">def</span> ternary_activation(x, slope_tensor <span class=\"op\">=</span> <span class=\"va\">None</span>, alpha <span class=\"op\">=</span> <span class=\"dv\">1</span>):\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Does not support slope annealing (slope_tensor is ignored)</span>\n<span class=\"co\">    Wolfram Alpha plot:</span>\n<span class=\"co\">    https://www.wolframalpha.com/input/?i=plot+(1.5*tanh(x)+%2B+0.5(tanh(-(3-1e-2)*x))),+x%3D+-2+to+2</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    <span class=\"cf\">return</span> <span class=\"fl\">1.5</span><span class=\"op\">*</span>tf.tanh(alpha<span class=\"op\">*</span>x) <span class=\"op\">+</span> <span class=\"fl\">0.5</span><span class=\"op\">*</span>(tf.tanh(<span class=\"op\">-</span>(<span class=\"dv\">3</span><span class=\"op\">/</span>alpha)<span class=\"op\">*</span>x))</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> n_ary_activation(x, activation<span class=\"op\">=</span>binary_activation, slope_tensor<span class=\"op\">=</span><span class=\"va\">None</span>, stochastic_tensor<span class=\"op\">=</span><span class=\"va\">None</span>):\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    n-ary activation for creating binary and ternary neurons (and n-ary neurons, if you can</span>\n<span class=\"co\">    create the right activation function). Given a tensor and an activation, it applies the</span>\n<span class=\"co\">    activation to the tensor, and then either samples the results (if stochastic tensor is</span>\n<span class=\"co\">    true), or rounds the results (if stochastic_tensor is false) to the closest integer</span>\n<span class=\"co\">    values. The default activation is a sigmoid (when slope_tensor = 1), which results in a</span>\n<span class=\"co\">    binary neuron, as in http://r2rt.com/binary-stochastic-neurons-in-tensorflow.html.</span>\n<span class=\"co\">    Uses the straight through estimator during backprop. See https://arxiv.org/abs/1308.3432.</span>\n\n<span class=\"co\">    Arguments:</span>\n<span class=\"co\">    * x: the pre-activation / logit tensor</span>\n<span class=\"co\">    * activation: sigmoid, hard sigmoid, or n-ary activation</span>\n<span class=\"co\">    * slope_tensor: slope adjusts the slope of the activation function, for purposes of the</span>\n<span class=\"co\">        Slope Annealing Trick (see http://arxiv.org/abs/1609.01704)</span>\n<span class=\"co\">    * stochastic_tensor: whether to sample the closest integer, or round to it.</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    <span class=\"cf\">if</span> slope_tensor <span class=\"kw\">is</span> <span class=\"va\">None</span>:\n        slope_tensor <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.0</span>)\n    <span class=\"cf\">if</span> stochastic_tensor <span class=\"kw\">is</span> <span class=\"va\">None</span>:\n        stochastic_tensor <span class=\"op\">=</span> tf.constant(<span class=\"va\">True</span>)\n\n    p <span class=\"op\">=</span> activation(x, slope_tensor)\n\n    <span class=\"cf\">return</span> tf.cond(stochastic_tensor,\n        <span class=\"kw\">lambda</span>: sample_closest_ints(p),\n        <span class=\"kw\">lambda</span>: st_round(p))</code></pre></div>\n<h4 id=\"functions-to-make-a-layer-of-one-hot-neurons\">Functions to make a layer of one-hot neurons</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> st_sampled_softmax(logits):\n    <span class=\"co\">&quot;&quot;&quot;Takes logits and samples a one-hot vector according to them, using the straight</span>\n<span class=\"co\">    through estimator on the backward pass.&quot;&quot;&quot;</span>\n    <span class=\"cf\">with</span> ops.name_scope(<span class=\"st\">&quot;STSampledSoftmax&quot;</span>) <span class=\"im\">as</span> name:\n        probs <span class=\"op\">=</span> tf.nn.softmax(logits)\n        onehot_dims <span class=\"op\">=</span> logits.get_shape().as_list()[<span class=\"dv\">1</span>]\n        res <span class=\"op\">=</span> tf.one_hot(tf.squeeze(tf.multinomial(logits, <span class=\"dv\">1</span>), <span class=\"dv\">1</span>), onehot_dims, <span class=\"fl\">1.0</span>, <span class=\"fl\">0.0</span>)\n        <span class=\"cf\">with</span> tf.get_default_graph().gradient_override_map({<span class=\"st\">&#39;Ceil&#39;</span>: <span class=\"st\">&#39;Identity&#39;</span>, <span class=\"st\">&#39;Mul&#39;</span>: <span class=\"st\">&#39;STMul&#39;</span>}):\n            <span class=\"cf\">return</span> tf.ceil(res<span class=\"op\">*</span>probs)\n\n<span class=\"kw\">def</span> st_hardmax_softmax(logits):\n    <span class=\"co\">&quot;&quot;&quot;Takes logits and creates a one-hot vector with a 1 in the position of the maximum</span>\n<span class=\"co\">    logit, using the straight through estimator on the backward pass.&quot;&quot;&quot;</span>\n    <span class=\"cf\">with</span> ops.name_scope(<span class=\"st\">&quot;STHardmaxSoftmax&quot;</span>) <span class=\"im\">as</span> name:\n        probs <span class=\"op\">=</span> tf.nn.softmax(logits)\n        onehot_dims <span class=\"op\">=</span> logits.get_shape().as_list()[<span class=\"dv\">1</span>]\n        res <span class=\"op\">=</span> tf.one_hot(tf.argmax(probs, <span class=\"dv\">1</span>), onehot_dims, <span class=\"fl\">1.0</span>, <span class=\"fl\">0.0</span>)\n        <span class=\"cf\">with</span> tf.get_default_graph().gradient_override_map({<span class=\"st\">&#39;Ceil&#39;</span>: <span class=\"st\">&#39;Identity&#39;</span>, <span class=\"st\">&#39;Mul&#39;</span>: <span class=\"st\">&#39;STMul&#39;</span>}):\n            <span class=\"cf\">return</span> tf.ceil(res<span class=\"op\">*</span>probs)\n\n<span class=\"at\">@ops.RegisterGradient</span>(<span class=\"st\">&quot;STMul&quot;</span>)\n<span class=\"kw\">def</span> st_mul(op, grad):\n    <span class=\"co\">&quot;&quot;&quot;Straight-through replacement for Mul gradient (does not support broadcasting).&quot;&quot;&quot;</span>\n    <span class=\"cf\">return</span> [grad, grad]\n\n<span class=\"kw\">def</span> layer_hard_softmax(x, shape, onehot_dims, temperature_tensor<span class=\"op\">=</span><span class=\"va\">None</span>, stochastic_tensor<span class=\"op\">=</span><span class=\"va\">None</span>,\n                       scope<span class=\"op\">=</span><span class=\"st\">&#39;hard_softmax_layer&#39;</span>):\n    <span class=\"co\">&quot;&quot;&quot;</span>\n<span class=\"co\">    Creates a layer of one-hot neurons. Note that the neurons are flattened before returning,</span>\n<span class=\"co\">    so that the shape of the layer needs to be a multiple of the dimension of the one-hot outputs.</span>\n\n<span class=\"co\">    Arguments:</span>\n<span class=\"co\">    * x: the layer inputs / previous layer</span>\n<span class=\"co\">    * shape: the tuple of [size_previous, layer_size]. Layer_size must be a multiple of onehot_dims,</span>\n<span class=\"co\">        since each neuron&#39;s output is flattened (i.e., the number of neurons will only be</span>\n<span class=\"co\">        layer_size / onehot_dims)</span>\n<span class=\"co\">    * onehot_dims: the size of each neuron&#39;s output</span>\n<span class=\"co\">    * temperature_tensor: the temperature for the softmax</span>\n<span class=\"co\">    * stochastic_tensor: whether the one hot outputs are sampled from the softmax distribution</span>\n<span class=\"co\">        (stochastic - recommended for training), or chosen according to its maximal element</span>\n<span class=\"co\">        (deterministic - recommended for inference)</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    <span class=\"cf\">assert</span>(<span class=\"bu\">len</span>(shape) <span class=\"op\">==</span> <span class=\"dv\">2</span>)\n    <span class=\"cf\">assert</span>(shape[<span class=\"dv\">1</span>] <span class=\"op\">%</span> onehot_dims <span class=\"op\">==</span> <span class=\"dv\">0</span>)\n    <span class=\"cf\">if</span> temperature_tensor <span class=\"kw\">is</span> <span class=\"va\">None</span>:\n        temperature_tensor <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.</span>)\n    <span class=\"cf\">if</span> stochastic_tensor <span class=\"kw\">is</span> <span class=\"va\">None</span>:\n        stochastic_tensor <span class=\"op\">=</span> tf.constant(<span class=\"va\">True</span>)\n\n    <span class=\"cf\">with</span> tf.variable_scope(scope):\n        w <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;w&#39;</span>,shape)\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>,shape[<span class=\"op\">-</span><span class=\"dv\">1</span>:])\n    logits <span class=\"op\">=</span> tf.reshape((tf.matmul(x, w) <span class=\"op\">+</span> b) <span class=\"op\">/</span> temperature_tensor,\n                        [<span class=\"op\">-</span><span class=\"dv\">1</span>, onehot_dims])\n\n    <span class=\"cf\">return</span> tf.cond(stochastic_tensor,\n                <span class=\"kw\">lambda</span>: tf.reshape(st_sampled_softmax(logits), [<span class=\"op\">-</span><span class=\"dv\">1</span>, shape[<span class=\"dv\">1</span>]]),\n                <span class=\"kw\">lambda</span>: tf.reshape(st_hardmax_softmax(logits), [<span class=\"op\">-</span><span class=\"dv\">1</span>, shape[<span class=\"dv\">1</span>]]))</code></pre></div>\n<h4 id=\"function-to-build-graph-for-mnist-classifier\">Function to build graph for MNIST classifier</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">100</span>],\n                     lr <span class=\"op\">=</span> <span class=\"fl\">0.5</span>,\n                     activation<span class=\"op\">=</span>binary_activation,\n                     onehot_dims <span class=\"op\">=</span> <span class=\"dv\">0</span>):\n    reset_graph()\n\n    <span class=\"co\">&quot;&quot;&quot;Placeholders&quot;&quot;&quot;</span>\n    x <span class=\"op\">=</span> tf.placeholder(tf.float32, [<span class=\"va\">None</span>, <span class=\"dv\">784</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;x_placeholder&#39;</span>)\n    y <span class=\"op\">=</span> tf.placeholder(tf.float32, [<span class=\"va\">None</span>, <span class=\"dv\">10</span>], name<span class=\"op\">=</span><span class=\"st\">&#39;y_placeholder&#39;</span>)\n    stochastic_tensor <span class=\"op\">=</span> tf.constant(<span class=\"va\">True</span>)\n    slope_tensor <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.0</span>)\n    temperature_tensor <span class=\"op\">=</span> <span class=\"dv\">1</span> <span class=\"op\">/</span> slope_tensor\n\n    layers <span class=\"op\">=</span> {<span class=\"dv\">0</span>: x}\n    num_hidden_layers <span class=\"op\">=</span> <span class=\"bu\">len</span>(hidden_dims)\n    dims <span class=\"op\">=</span> [<span class=\"dv\">784</span>] <span class=\"op\">+</span> hidden_dims\n\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1</span>, num_hidden_layers<span class=\"op\">+</span><span class=\"dv\">1</span>):\n        <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&quot;layer_&quot;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(i)):\n            <span class=\"cf\">if</span> onehot_dims:\n                layers[i] <span class=\"op\">=</span> layer_hard_softmax(layers[i<span class=\"dv\">-1</span>],\n                                              dims[i<span class=\"dv\">-1</span>:i<span class=\"op\">+</span><span class=\"dv\">1</span>],\n                                              onehot_dims,\n                                              temperature_tensor,\n                                              stochastic_tensor)\n            <span class=\"cf\">else</span>:\n                pre_activations <span class=\"op\">=</span> layer_linear(layers[i<span class=\"dv\">-1</span>], dims[i<span class=\"dv\">-1</span>:i<span class=\"op\">+</span><span class=\"dv\">1</span>], scope<span class=\"op\">=</span><span class=\"st\">&#39;layer_&#39;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(i))\n                <span class=\"cf\">if</span> activation <span class=\"kw\">is</span> tf.tanh <span class=\"kw\">or</span> activation <span class=\"kw\">is</span> tf.sigmoid:\n                    layers[i] <span class=\"op\">=</span> activation(pre_activations)\n                <span class=\"cf\">else</span>:\n                    layers[i] <span class=\"op\">=</span> n_ary_activation(pre_activations,\n                                               activation,\n                                               slope_tensor,\n                                               stochastic_tensor)\n\n    final_hidden_layer <span class=\"op\">=</span> layers[num_hidden_layers]\n    preds <span class=\"op\">=</span> layer_softmax(final_hidden_layer, [dims[<span class=\"op\">-</span><span class=\"dv\">1</span>], <span class=\"dv\">10</span>])\n    loss <span class=\"op\">=</span> <span class=\"op\">-</span>tf.reduce_mean(y <span class=\"op\">*</span> tf.log(preds), reduction_indices<span class=\"op\">=</span><span class=\"dv\">1</span>)\n    ts <span class=\"op\">=</span> tf.train.GradientDescentOptimizer(lr).minimize(loss)\n\n    accuracy <span class=\"op\">=</span> compute_accuracy(y, preds)\n\n    <span class=\"cf\">return</span> <span class=\"bu\">dict</span>(\n        x<span class=\"op\">=</span>x,\n        y<span class=\"op\">=</span>y,\n        stochastic<span class=\"op\">=</span>stochastic_tensor,\n        slope<span class=\"op\">=</span>slope_tensor,\n        final_hidden_layer <span class=\"op\">=</span> final_hidden_layer,\n        loss<span class=\"op\">=</span>loss,\n        ts<span class=\"op\">=</span>ts,\n        accuracy<span class=\"op\">=</span>accuracy,\n        init_op<span class=\"op\">=</span>tf.global_variables_initializer()\n    )</code></pre></div>\n<h4 id=\"function-to-train-the-classifier\">Function to train the classifier</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> train_classifier(<span class=\"op\">\\</span>\n        hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">100</span>,<span class=\"dv\">100</span>],\n        activation<span class=\"op\">=</span>binary_activation,\n        onehot_dims <span class=\"op\">=</span> <span class=\"dv\">0</span>,\n        stochastic_train<span class=\"op\">=</span><span class=\"va\">True</span>,\n        stochastic_eval<span class=\"op\">=</span><span class=\"va\">True</span>,\n        slope_annealing_rate<span class=\"op\">=</span><span class=\"va\">None</span>,\n        epochs<span class=\"op\">=</span><span class=\"dv\">20</span>,\n        lr<span class=\"op\">=</span><span class=\"fl\">0.5</span>,\n        verbose<span class=\"op\">=</span><span class=\"va\">True</span>,\n        label<span class=\"op\">=</span><span class=\"va\">None</span>):\n    g <span class=\"op\">=</span> build_classifier(hidden_dims<span class=\"op\">=</span>hidden_dims, lr<span class=\"op\">=</span>lr,\n        activation<span class=\"op\">=</span>activation, onehot_dims<span class=\"op\">=</span>onehot_dims)\n\n    <span class=\"cf\">with</span> tf.Session() <span class=\"im\">as</span> sess:\n        sess.run(g[<span class=\"st\">&#39;init_op&#39;</span>])\n        slope <span class=\"op\">=</span> <span class=\"dv\">1</span>\n        res_tr, res_val, sample_layers <span class=\"op\">=</span> [], [], []\n        <span class=\"cf\">for</span> epoch <span class=\"kw\">in</span> <span class=\"bu\">range</span>(epochs):\n            feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: mnist.validation.images,\n                       g[<span class=\"st\">&#39;y&#39;</span>]: mnist.validation.labels,\n                       g[<span class=\"st\">&#39;stochastic&#39;</span>]: stochastic_eval,\n                       g[<span class=\"st\">&#39;slope&#39;</span>]: slope}\n            acc, final_hidden <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;accuracy&#39;</span>], g[<span class=\"st\">&#39;final_hidden_layer&#39;</span>]],\n                                            feed_dict<span class=\"op\">=</span>feed_dict)\n            sample_layers.append(final_hidden)\n            <span class=\"cf\">if</span> verbose:\n                <span class=\"bu\">print</span>(<span class=\"st\">&quot;Epoch&quot;</span>, epoch, acc)\n            <span class=\"cf\">else</span>:\n                <span class=\"bu\">print</span>(<span class=\"st\">&#39;.&#39;</span>, end<span class=\"op\">=</span><span class=\"st\">&#39;&#39;</span>)\n\n            accuracy <span class=\"op\">=</span> <span class=\"dv\">0</span>\n            <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"dv\">1</span>,<span class=\"dv\">1001</span>):\n                x, y <span class=\"op\">=</span> mnist.train.next_batch(<span class=\"dv\">50</span>)\n                feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: x, g[<span class=\"st\">&#39;y&#39;</span>]: y, g[<span class=\"st\">&#39;stochastic&#39;</span>]: stochastic_train}\n                acc, _ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;accuracy&#39;</span>],g[<span class=\"st\">&#39;ts&#39;</span>]], feed_dict<span class=\"op\">=</span>feed_dict)\n                accuracy <span class=\"op\">+=</span> acc\n                <span class=\"cf\">if</span> i <span class=\"op\">%</span> <span class=\"dv\">100</span> <span class=\"op\">==</span> <span class=\"dv\">0</span> <span class=\"kw\">and</span> i <span class=\"op\">&gt;</span> <span class=\"dv\">0</span>:\n                    res_tr.append(accuracy<span class=\"op\">/</span><span class=\"dv\">100</span>)\n                    accuracy <span class=\"op\">=</span> <span class=\"dv\">0</span>\n                    feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: mnist.validation.images,\n                               g[<span class=\"st\">&#39;y&#39;</span>]: mnist.validation.labels,\n                               g[<span class=\"st\">&#39;stochastic&#39;</span>]: stochastic_eval,\n                               g[<span class=\"st\">&#39;slope&#39;</span>]: slope}\n                    res_val.append(sess.run(g[<span class=\"st\">&#39;accuracy&#39;</span>], feed_dict<span class=\"op\">=</span>feed_dict))\n\n            <span class=\"cf\">if</span> slope_annealing_rate <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n                slope <span class=\"op\">=</span> slope<span class=\"op\">*</span>slope_annealing_rate\n                <span class=\"cf\">if</span> verbose:\n                    <span class=\"bu\">print</span>(<span class=\"st\">&quot;Annealed slope:&quot;</span>, slope, <span class=\"st\">&quot;| Annealed temperature:&quot;</span>, <span class=\"dv\">1</span><span class=\"op\">/</span> slope)\n\n        feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: mnist.validation.images, g[<span class=\"st\">&#39;y&#39;</span>]: mnist.validation.labels,\n                   g[<span class=\"st\">&#39;stochastic&#39;</span>]: stochastic_eval, g[<span class=\"st\">&#39;slope&#39;</span>]: slope}\n        <span class=\"bu\">print</span>(<span class=\"st\">&quot;</span><span class=\"ch\">\\n</span><span class=\"st\">Final epoch, epoch&quot;</span>, epoch<span class=\"op\">+</span><span class=\"dv\">1</span>, <span class=\"st\">&quot;:&quot;</span>, sess.run(g[<span class=\"st\">&#39;accuracy&#39;</span>], feed_dict<span class=\"op\">=</span>feed_dict))\n\n        <span class=\"cf\">if</span> label <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n            <span class=\"cf\">return</span> (res_tr, label <span class=\"op\">+</span> <span class=\"st\">&quot; - Training&quot;</span>), (res_val, label <span class=\"op\">+</span> <span class=\"st\">&quot; - Validation&quot;</span>)\n        <span class=\"cf\">else</span>:\n            <span class=\"cf\">return</span> [(res_tr, <span class=\"st\">&quot;Training&quot;</span>), (res_val, <span class=\"st\">&quot;Validation&quot;</span>)], sample_layers</code></pre></div>\n<h3 id=\"experiments\">Experiments</h3>\n<p>Let us now demonstrate that the above neurons train on MNIST, and can achieve reasonably good results. I’m not hunting for hyperparameters here, so the below may not be optimal. All training is done over 20 epochs with a learning rate of 0.1.</p>\n<h4 id=\"tanh-baseline-real-valued-activations\">Tanh baseline (real-valued activations)</h4>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">res, _ <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">100</span>], activation<span class=\"op\">=</span>tf.tanh, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\nplot_n(res, lower_y<span class=\"op\">=</span><span class=\"fl\">0.8</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;Tanh Baseline&quot;</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.9772</code></pre>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_21_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h4 id=\"binary-neurons-with-slope-annealing-baseline\">Binary neurons with slope annealing baseline</h4>\n<p>More results on binary neurons available in my prior <a href=\"http://r2rt.com/binary-stochastic-neurons-in-tensorflow.html\">post</a>.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">res, _ <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">100</span>], activation<span class=\"op\">=</span>binary_activation, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      slope_annealing_rate<span class=\"op\">=</span><span class=\"fl\">1.1</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\nplot_n(res, lower_y<span class=\"op\">=</span><span class=\"fl\">0.8</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;Binary Stochastic w/ Slope Annealing Baseline&quot;</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.9732</code></pre>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_23_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h4 id=\"ternary-neurons\">Ternary neurons</h4>\n<p>As noted above, I didn’t spend the time to figure out a good way to slope-anneal the ternary activation, so the below is not slope-annealed.</p>\n<p>The interesting thing to note is that these neurons are more expressive than the binary neurons, and so they not only get closer to the tanh baseline, but they also offer less regularization / overfit more quickly.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">res, sample_layers <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">100</span>], activation<span class=\"op\">=</span>ternary_activation, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\nplot_n(res, lower_y<span class=\"op\">=</span><span class=\"fl\">0.8</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;Ternary Stochastic&quot;</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.9764</code></pre>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_26_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<p>If you’re curious as to the distribution of outputs, here it is:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">c <span class=\"op\">=</span> Counter(np.reshape(sample_layers[<span class=\"op\">-</span><span class=\"dv\">1</span>], [<span class=\"op\">-</span><span class=\"dv\">1</span>]))\ng <span class=\"op\">=</span> sns.barplot(x <span class=\"op\">=</span> <span class=\"bu\">list</span>(c.keys()), y <span class=\"op\">=</span> <span class=\"bu\">list</span>(c.values()))\nsns.plt.title(<span class=\"st\">&#39;Distribution of ternary outputs on MNIST&#39;</span>)</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_28_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h4 id=\"one-hot-neurons-1\">One-hot neurons</h4>\n<p>First, let’s take a look at what the layer activations look like, in case my description above wasn’t super clear. We’ll pull out the last layer of a network that uses 100 5-dimensional one-hot neurons, and then “unflatten” it into shape [100, 5].</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">res, sample_layers <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">500</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">5</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.9794</code></pre>\n<p>Here is what the activations of the first 10 neurons of the first sample in the validation set look like. As discussed, each neuron outputs a 5D one-hot vector:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">np.reshape(sample_layers[<span class=\"dv\">0</span>][<span class=\"dv\">0</span>], [<span class=\"dv\">100</span>, <span class=\"dv\">5</span>])[:<span class=\"dv\">10</span>]</code></pre></div>\n<pre><code>array([[ 0.,  0.,  0.,  0.,  1.],\n       [ 0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.]], dtype=float32)</code></pre>\n<h4 id=\"one-hot-neurons-temperature-annealing\">One-hot neurons: temperature annealing</h4>\n<p>Now, let’s test some 5D neurons to see if temperature annealing does anything. Looks like not really:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">_, res1 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">500</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">5</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                      label<span class=\"op\">=</span> <span class=\"st\">&quot;No temperature annealing&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n_, res2 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">500</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">5</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      slope_annealing_rate<span class=\"op\">=</span><span class=\"fl\">1.2</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                      label<span class=\"op\">=</span> <span class=\"st\">&quot;Annealing rate 1.2&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n_, res3 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">500</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">5</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>,\n                                      slope_annealing_rate<span class=\"op\">=</span><span class=\"fl\">1.5</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                      label<span class=\"op\">=</span> <span class=\"st\">&quot;Annealing rate 1.5&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\nplot_n([res1] <span class=\"op\">+</span> [res2] <span class=\"op\">+</span> [res3], lower_y<span class=\"op\">=</span><span class=\"fl\">0.8</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;5D one-hot neurons, temperature annealing (validation)&quot;</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.981\n....................\nFinal epoch, epoch 20 : 0.9796\n....................\nFinal epoch, epoch 20 : 0.9798</code></pre>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_34_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h4 id=\"one-hot-neurons-number-of-dimensions\">One-hot neurons: number of dimensions</h4>\n<p>We can also easily change with the number of dimensions of each neuron. I keep the layer size constant in terms of the number of neurons, but make them progressively more expressive and see what happens (because layers are flattened, it appears that the layer size is growing, but the number of neurons stays the same). Looks like not much happens, but maybe this has to do with the simplicity of the dataset. Query whether more expressive one-hot neurons would make a difference on a harder dataset. Note: I plotted the training curves locally, and they show the same result (I would have thought higher dimensional neurons would fit the training data better, but apparently not – perhaps due to stochasticity).</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">_, res1 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">300</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">3</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                      label<span class=\"op\">=</span> <span class=\"st\">&quot;3 dimensions&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n_, res2 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">500</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">5</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                      label<span class=\"op\">=</span> <span class=\"st\">&quot;5 dimensions&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n_, res3 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">700</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">7</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                       label<span class=\"op\">=</span> <span class=\"st\">&quot;7 dimensions&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\n_, res4 <span class=\"op\">=</span> train_classifier(hidden_dims<span class=\"op\">=</span>[<span class=\"dv\">1000</span>], onehot_dims<span class=\"op\">=</span><span class=\"dv\">10</span>, epochs<span class=\"op\">=</span><span class=\"dv\">20</span>, lr<span class=\"op\">=</span><span class=\"fl\">0.1</span>, stochastic_eval<span class=\"op\">=</span><span class=\"va\">False</span>,\n                                       label<span class=\"op\">=</span> <span class=\"st\">&quot;10 dimensions&quot;</span>, verbose<span class=\"op\">=</span><span class=\"va\">False</span>)\nplot_n([res1] <span class=\"op\">+</span> [res2] <span class=\"op\">+</span> [res3] <span class=\"op\">+</span> [res4], lower_y<span class=\"op\">=</span><span class=\"fl\">0.8</span>, title<span class=\"op\">=</span><span class=\"st\">&quot;N-dimensional one-hot neurons&quot;</span>)</code></pre></div>\n<pre><code>....................\nFinal epoch, epoch 20 : 0.9772\n....................\nFinal epoch, epoch 20 : 0.98\n....................\nFinal epoch, epoch 20 : 0.981\n....................\nFinal epoch, epoch 20 : 0.9754</code></pre>\n<figure>\n<img src=\"https://r2rt.com/static/images/BB_output_36_1.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>That’s it for now! In this post we saw how we can use the straight-through estimator to create expressive trainable explicit neurons. In particular, we coded up neurons that can represent more than 2 ordered categories (the ternary, or general n-ary neuron), and also neurons that can represent 3 or more unordered categories (the one-hot neuron). Moreover, we showed that they are all competitive with a real-valued tanh baseline on MNIST, and that they provide strong built-in regularization.</p>\n<p>I discuss any applications in this post, but I’m hoping to show you something really cool using one-hot neurons in the next one.</p>\n</body>\n</html>"
}