{
  "title": "ICCV 2015, Day 2",
  "link": "",
  "published": "2015-12-15T01:20:00+00:00",
  "updated": "2015-12-15T01:20:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-12-15:/sebastian/blog/iccv-2015-day-2.html",
  "summary": "<p>This article summarizes the second day of the <a href=\"http://pamitc.org/iccv15/\">ICCV\n2015</a> conference, the International Conference on\nComputer Vision.\nA summary of the <a href=\"http://www.nowozin.net/sebastian/blog/iccv-2015-day-1.html\">first day</a> is also available.</p>\n<h2>Awards</h2>\n<p>The following awards were given at ICCV 2015.</p>\n<h3>Achievement awards</h3>\n<ul>\n<li>PAMI Distinguished Researcher â€¦</li></ul>",
  "content": "<p>This article summarizes the second day of the <a href=\"http://pamitc.org/iccv15/\">ICCV\n2015</a> conference, the International Conference on\nComputer Vision.\nA summary of the <a href=\"http://www.nowozin.net/sebastian/blog/iccv-2015-day-1.html\">first day</a> is also available.</p>\n<h2>Awards</h2>\n<p>The following awards were given at ICCV 2015.</p>\n<h3>Achievement awards</h3>\n<ul>\n<li>PAMI Distinguished Researcher Award (1): <strong>Yann LeCun</strong></li>\n<li>PAMI Distinguished Researcher Award (2): <strong>David Lowe</strong></li>\n<li>PAMI Everingham Prize Winner (1): <strong>Andrea Vedaldi</strong> for <a href=\"http://www.vlfeat.org/\">VLFeat</a></li>\n<li>PAMI Everingham Prize Winner (2): <strong>Daniel Scharstein</strong> and <strong>Rick Szeliski</strong> for the <a href=\"http://vision/middlebury.edu/stereo/data/\">Middlebury Datasets</a></li>\n</ul>\n<h2>Paper awards</h2>\n<ul>\n<li>PAMI Helmholtz Prize (1): <strong>David Martin</strong>, <strong>Charles Fowlkes</strong>, <strong>Doron Tal</strong>, and <strong>Jitendra Malik</strong> for their ICCV 2001 paper \"A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics\".</li>\n<li>PAMI Helmholtz Prize (2): <strong>Serge Belongie</strong>, <strong>Jitendra Malik</strong>, and <strong>Jan Puzicha</strong>, for their ICCV 2001 paper \"Matching Shapes\".</li>\n<li>Marr Prize: <strong>Peter Kontschieder</strong>, <strong>Madalina Fiterau</strong>, <strong>Antonio Criminisi</strong>, and <strong>Samual Rota Bulo</strong>, for <a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf\">\"Deep Neural Decision Forests\"</a>.</li>\n<li>Marr Prize honorable mention: <strong>Saining Xie</strong> and <strong>Zhuowen Tu</strong> for <a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf\">\"Holistically-Nested Edge Detection\"</a>.</li>\n</ul>\n<h2>Interesting Papers</h2>\n<p>The above Marr prize winning papers are very nice, but here I also want to\nhighlight three other papers I found interesting today.</p>\n<h3>Fast R-CNN</h3>\n<p>By Ross Girshick.</p>\n<p>Since 2014 the standard object detection pipeline for natural images is the\nR-CNN system which first extracts a set of object proposals then scores them\nusing a convolutional neural network.\nThe two key weaknesses of the approach are: first, the separation between\nproposal generation and scoring, preventing joint training of model parameters;\nand second the separate scoring of each hypothesis which leads to significant\nruntime overhead.\nThis work and the follow-up work (\"Faster R-CNNs\" at NIPS this year) addresses\nboth issues by proposing a joint model that is trained end-to-end, including\nproposal generation, leading to a new state of the art in object detection.</p>\n<p><a href=\"http://github.com/rbgirshick/fast-rcnn\">Code</a>,\n<a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf\">paper</a>.</p>\n<h3>Unsupervised Visual Representation Learning by Context Prediction</h3>\n<p>By Carl Doersch, Abhinav Gupta, and Alexei A. Efros.</p>\n<p>Supervised deep learning needs lots of labeled training data to achieve good performance.\nThis paper investigates whether we can create and train deep neural networks on\nartificial tasks for which we can create large amounts of training data.  In\nparticular, the paper proposes to predict where a certain patch appears within\nthe image.  For this task, an almost infinite amount of training data is easily\ncreated.\nPerhaps surprisingly the resulting network, despite being trained on this\nartificial task, has learned useful representations for real vision tasks such\nas image classification.</p>\n<p><a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf\">Paper</a>.</p>\n<h3>Deep Fried Convnets</h3>\n<p>By Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song,\nand Ziyu Wang.</p>\n<p>In deep convolutional networks the last few densely connected layers have the\nmost parameters and thus most of the required memory during test time and\ntraining.\nThis work proposes to leverage the <em>fastfood</em> kernel approximation to replace\ndensely connected layers with specific efficient and low parameter operations.</p>\n<p>The empirical results are impressive and the fastfood justification is\nplausible, but I wonder if this work may even provide a hint at a more general\napproach to construct efficient neural network architectures by using arbitrary\ndense but efficient matrix operations (FFT, DCT, Walsh-Hadamard, etcetera).</p>\n<p><a href=\"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf\">Paper</a>.</p>",
  "category": ""
}