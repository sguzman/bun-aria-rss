{
  "title": "Anomaly Detection with Wikipedia Page View Data",
  "link": "http://beautifuldata.net/2015/01/anomaly-detection-with-wikipedia-page-view-data/",
  "comments": "http://beautifuldata.net/2015/01/anomaly-detection-with-wikipedia-page-view-data/#comments",
  "dc:creator": "Benedikt Koehler",
  "pubDate": "Fri, 09 Jan 2015 00:02:22 +0000",
  "category": [
    "data science",
    "R",
    "twitter",
    "visualization",
    "wikipedia",
    "network analysis",
    "outliers"
  ],
  "guid": "http://beautifuldata.net/?p=1134",
  "description": "Today, the Twitter engineering team released another very interesting Open Source R package for working with time series data: &#8220;AnomalyDetection&#8220;. This package uses the Seasonal Hybrid ESD (S-H-ESD) algorithm to identify local anomalies (= variations inside seasonal patterns) and global anomalies (= variations that cannot be explained with seasonal patterns). As a kind of warm &#8230; <a href=\"http://beautifuldata.net/2015/01/anomaly-detection-with-wikipedia-page-view-data/\" class=\"more-link\">Continue reading<span class=\"screen-reader-text\"> \"Anomaly Detection with Wikipedia Page View Data\"</span></a>",
  "content:encoded": "<p>Today, the Twitter engineering team released another very interesting <a href=\"https://github.com/twitter/AnomalyDetection\">Open Source R</a> package for working with time series data: &#8220;<a href=\"https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series\">AnomalyDetection</a>&#8220;. This package uses the Seasonal Hybrid ESD (S-H-<a href=\"http://vsp.pnnl.gov/help/Vsample/Rosners_Outlier_Test.htm\">ESD</a>) algorithm to identify local anomalies (= variations inside seasonal patterns) and global anomalies (= variations that cannot be explained with seasonal patterns).</p>\n<p>As a kind of warm up and practical exploration of the new package, here&#8217;s a short example on how to download Wikipedia PageView statistics and mine them for anomalies (inspired by <a href=\"http://beautifuldata.net/2012/11/wikipedia-attention-and-the-us-elections/\">this blog post</a>, where this package wasn&#8217;t available yet):</p>\n<p>First, we install and load the necessary packages:</p>\n<pre class=\"wp-code-highlight prettyprint\">\nlibrary(RJSONIO)\nlibrary(RCurl)\nlibrary(ggplot2)\ninstall.packages(\"devtools\")\ndevtools::install_github(\"twitter/AnomalyDetection\")\nlibrary(AnomalyDetection)\n</pre>\n<p>Then we choose an interesting Wikipedia page and download the last 90 days of PageView statistics:</p>\n<pre class=\"wp-code-highlight prettyprint\">\npage <- \"USA\"\nraw_data <- getURL(paste(\"http://stats.grok.se/json/en/latest90/\", page, sep=\"\"))\ndata <- fromJSON(raw_data)\nviews <- data.frame(timestamp=paste(names(data$daily_views), \" 12:00:00\", sep=\"\"), stringsAsFactors=F)\nviews$count <- data$daily_views\nviews$timestamp <- as.POSIXlt(views$timestamp) # Transform to POSIX datetime\nviews <- views[order(views$timestamp),]\n</pre>\n<p>I also did some pre-processing and transformation of the dates in POSIX datetime format. A first plot shows this pattern:</p>\n<pre class=\"wp-code-highlight prettyprint\">\nggplot(views, aes(timestamp, count)) + geom_line() + scale_x_datetime() + xlab(\"\") + ylab(\"views\")\n</pre>\n<p><img loading=\"lazy\" src=\"http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_views_USA.png\" alt=\"wikipedia_views_USA\" width=\"600\" height=\"350\" class=\"aligncenter size-full wp-image-1137\" srcset=\"http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_views_USA.png 600w, http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_views_USA-300x175.png 300w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px\" /></p>\n<p>Now, let&#8217;s look for anomalies. The usual way would be to feed a dataframe with a date-time and a value column into the AnomalyDetection function AnomalyDetectionTs(). But in this case, this doesn&#8217;t work because our data is much too coarse. It doesn&#8217;t seem to work with data on days. So, we use the more generic function AnomalyDetectionVec() that just needs the values and some definition of a period. In this case, the period is 7 (= 7 days for one week):</p>\n<pre class=\"wp-code-highlight prettyprint\">\nres = AnomalyDetectionVec(views$count, max_anoms=0.05, direction=&#039;both&#039;, plot=TRUE, period=7)\nres$plot\n</pre>\n<p><img loading=\"lazy\" src=\"http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_anomalies_usa.png\" alt=\"wikipedia_anomalies_usa\" width=\"600\" height=\"350\" class=\"aligncenter size-full wp-image-1139\" srcset=\"http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_anomalies_usa.png 600w, http://beautifuldata.net/wp-content/uploads/2015/01/wikipedia_anomalies_usa-300x175.png 300w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px\" /></p>\n<p>In our case, the algorithm has discovered 4 anomalies. The first on October 30 2014 being an exceptionally high value overall, the second is a very high Sunday, the third a high value overall and the forth a high Saturday (normally, this day is also quite weak).</p>\n",
  "wfw:commentRss": "http://beautifuldata.net/2015/01/anomaly-detection-with-wikipedia-page-view-data/feed/",
  "slash:comments": 4
}