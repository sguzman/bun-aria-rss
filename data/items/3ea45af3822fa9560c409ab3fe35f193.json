{
  "title": "Weekly Review: 11/11/2017",
  "link": "https://codesachin.wordpress.com/2017/11/12/weekly-review-11-04-2017/",
  "comments": "https://codesachin.wordpress.com/2017/11/12/weekly-review-11-04-2017/#respond",
  "dc:creator": "srjoglekar246",
  "pubDate": "Sat, 11 Nov 2017 22:13:37 +0000",
  "category": [
    "Machine Learning/AI",
    "Programming",
    "automl",
    "cnn",
    "gans",
    "google",
    "graphcore",
    "tangent"
  ],
  "guid": "http://codesachin.wordpress.com/?p=851",
  "description": "The Motion Planning course is going faster than I expected. I completed 2 weeks within 5 days. Thats good I guess, since it means I might get to the Capstone project before I take a vacation to India. Heres the stuff from this week: Graphcore and the Intelligent Processing Unit (IPU) Graphcore aims to disrupt &#8230; <a href=\"https://codesachin.wordpress.com/2017/11/12/weekly-review-11-04-2017/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Weekly Review: 11/11/2017</span> <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>The Motion Planning course is going faster than I expected. I completed 2 weeks within 5 days. Thats good I guess, since it means I might get to the Capstone project before I take a vacation to India.</p>\n<p>Heres the stuff from this week:</p>\n<p><strong>Graphcore and the Intelligent Processing Unit (IPU)</strong></p>\n<p><a href=\"https://www.graphcore.ai\">Graphcore</a> aims to disrupt the world of ML-focussed computing devices. In an <a href=\"https://www.graphcore.ai/posts/what-does-machine-learning-look-like\">interesting blog post</a>, they visualize neuron connections in different CNN architectures, and talk about how they compare to the human brain.</p>\n<p>If you are curious about how IPUs differ from CPUs and GPUs, <a href=\"https://www.nextplatform.com/2017/03/09/early-look-startup-graphcores-deep-learning-chip/\">this NextPlatform article</a> gives a few hints: mind you, IPUs are yet to be &#8216;released&#8217;, so theres no concrete information out yet. If you want to brush up on why memory is so important for neural network training (more than inference), <a href=\"https://www.graphcore.ai/posts/why-is-so-much-memory-needed-for-deep-neural-networks\">this</a> is a good place to start.</p>\n<p><strong>Overview of Different CNN architectures</strong></p>\n<p><a href=\"http://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/\">This article</a> on the CV-Tricks blog gives a high-level overview of the major CNN architectures so far: AlexNet, VGG, Inception, ResNets, etc. Its a good place to go for reference if you ever happen to forget what one of them did differently.</p>\n<p>On that note, <a href=\"https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\">this blog post</a> by Adit Deshpande goes into the &#8216;Brief History of Deep Learning&#8217;, marking out all the main research papers of importance.</p>\n<p><strong>Meta-learning and AutoML</strong></p>\n<p>The New York Times posted an <a href=\"https://www.nytimes.com/2017/11/05/technology/machine-learning-artificial-intelligence-ai.html\">article</a> about AI systems that can build other AI systems, thus leading to what they call &#8216;Meta-learning&#8217; (Learning how to learn/build systems that learn).</p>\n<p>Google has been dabbling in meta-learning with a project called AutoML. <a href=\"https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html\">AutoML</a> basically consists of a &#8216;Generator&#8217; network that comes up with various NN architectures, which are then evaluated by a &#8216;Scorer&#8217; that trains them and computes their accuracy. The gradients with respect to these scores are passed back to the Generator, in order to improve the output architectures. <a href=\"https://arxiv.org/abs/1611.01578\">This</a> is their original paper, in case you want to take a look.</p>\n<p>The AutoML team recently wrote <a href=\"https://research.googleblog.com/2017/11/automl-for-large-scale-image.html\">another post</a> about large-scale object detection using their algorithms.</p>\n<p><strong>Tangent</strong></p>\n<p>People from Google <a href=\"https://research.googleblog.com/2017/11/tangent-source-to-source-debuggable.html\">recently open-sourced</a> their library for computing gradients of Python functions. Tangent works directly on your Python code(rather than view it as a black-box), and comes up with a derivative function to compute its gradient. This is useful in cases where you might want to debug how/why some NN architecture is not getting trained the way it&#8217;s supposed to. Here&#8217;s their <a href=\"https://github.com/google/tangent\">Github repo.</a></p>\n<p><strong>Reconstructing films with Neural Network</strong></p>\n<p><a href=\"https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe\">This blog post</a> talks about the use of <a href=\"https://en.wikipedia.org/wiki/Autoencoder\">Autoencoders</a> and <a href=\"https://en.wikipedia.org/wiki/Generative_adversarial_network\">GANs</a> to reconstruct films using NNs trained on them. They also venture into reconstructing films using NNs trained on other stylish films (like <em>A Scanner Darkly</em>). The results are pretty interesting.</p>\n",
  "wfw:commentRss": "https://codesachin.wordpress.com/2017/11/12/weekly-review-11-04-2017/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "srjoglekar246"
  }
}