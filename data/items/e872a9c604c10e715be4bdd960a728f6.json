{
  "title": "Jump-Diffusion Langevin Dynamics for Multimodal Posterior Sampling. (arXiv:2211.01774v1 [stat.ML])",
  "link": "http://arxiv.org/abs/2211.01774",
  "description": "<p>Bayesian methods of sampling from a posterior distribution are becoming\nincreasingly popular due to their ability to precisely display the uncertainty\nof a model fit. Classical methods based on iterative random sampling and\nposterior evaluation such as Metropolis-Hastings are known to have desirable\nlong run mixing properties, however are slow to converge. Gradient based\nmethods, such as Langevin Dynamics (and its stochastic gradient counterpart)\nexhibit favorable dimension-dependence and fast mixing times for log-concave,\nand \"close\" to log-concave distributions, however also have long escape times\nfrom local minimizers. Many contemporary applications such as Bayesian Neural\nNetworks are both high-dimensional and highly multimodal. In this paper we\ninvestigate the performance of a hybrid Metropolis and Langevin sampling method\nakin to Jump Diffusion on a range of synthetic and real data, indicating that\ncareful calibration of mixing sampling jumps with gradient based chains\nsignificantly outperforms both pure gradient-based or sampling based schemes.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/stat/1/au:+Guidolin_J/0/1/0/all/0/1\">Jacopo Guidolin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kungurtsev_V/0/1/0/all/0/1\">Vyacheslav Kungurtsev</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kuzelka_O/0/1/0/all/0/1\">Ond&#x159;ej Ku&#x17e;elka</a>"
}