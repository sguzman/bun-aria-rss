{
  "title": "Thoughts on Mutual Information: More Estimators",
  "link": "http://artem.sobolev.name/posts/2019-08-10-thoughts-on-mutual-information-more-estimators.html",
  "description": "<p>In this post I’d like to show how Self-Normalized Importance Sampling (<a href=\"/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html\">IWHVI</a> and IWAE) and Annealed Importance Sampling can be used to give (sometimes sandwich) bounds on the MI in many different cases.</p>\n<!--more-->\n<p><a href=\"https://en.wikipedia.org/wiki/Mutual_information\">Mutual Information</a> (MI) is an important concept from the Information Theory that captures the idea of information one random variable <span class=\"math inline\">\\(X\\)</span> carries about the r.v. <span class=\"math inline\">\\(Z\\)</span> and is usually denoted <span class=\"math inline\">\\(I(X, Z)\\)</span>, however in order to emphasize the underlying joint distribution I’ll be using a non-standard notation <span class=\"math inline\">\\(\\text{MI}[p(x,z)]\\)</span>. Formally the MI has the following definition:</p>\n<p><span class=\"math display\">\\[\n\\text{MI}[p(x,z)]\n:= \\E_{p(x, z)} \\log \\frac{p(x, z)}{p(x) p(z)}\n= \\E_{p(x, z)} \\log \\frac{p(x \\mid z)}{p(x)}\n\\]</span></p>\n<p>Having such a nice information-measuring interpretation, MI is a natural objective and/or metric in many problems in Machine Learning. One <a href=\"https://arxiv.org/abs/1802.04874\">particular application</a> is evaluatiuon of encoder-decoder-like architectures as MI quantifies amount of information contained in the code. In particular, in Variational Autoencoders a good decoder should have high <span class=\"math inline\">\\(\\text{MI}[p(x,z)]\\)</span>, meaning the code is very useful for generations. A good encoder though… has to keep balance between providing enough information to the decoder, while not deviating too much from the prior by, for example, encoding redundant / unnecessary information. <a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a></p>\n<h2 id=\"mi-estimation\">MI Estimation</h2>\n<p>In general estimating the MI is intractable as it requires knowing the log marginal density <span class=\"math inline\">\\(\\log p(x)\\)</span> or the intractable log posterior <span class=\"math inline\">\\(\\log p(z|x)\\)</span>. However, there are many efficient variational bounds which can be employed to give tractable lower or upper bounds on the MI. Many existing bounds are reviewed in the <a href=\"https://arxiv.org/abs/1905.06922\">On Variational Bounds of Mutual Information</a> paper.</p>\n<p>It is important to take into account what we know about the joint distribution of <span class=\"math inline\">\\(x\\)</span> and <span class=\"math inline\">\\(z\\)</span>. We’ll consider several nested “layers” of decreasing complexity:</p>\n<ol style=\"list-style-type: decimal\">\n<li><strong>Blackbox</strong> case: Distributions <span class=\"math inline\">\\(p(x, z)\\)</span> we can only sample from, but don’t know any densities. This way we can form Monte Carlo estimates after some learning and surprisingly we can give some lower bounds already in this case.</li>\n<li><strong>Known conditional</strong> case: Distributions <span class=\"math inline\">\\(p(x, z)\\)</span> we can sample from and know one conditional distribution, say, <span class=\"math inline\">\\(p(x|z)\\)</span>.</li>\n<li><strong>Known marginal</strong> case: Distributions <span class=\"math inline\">\\(p(x, z)\\)</span> we can sample from and know one marginal distribution, say, <span class=\"math inline\">\\(p(z)\\)</span>.</li>\n<li><strong>Known joint</strong> case: Distributions <span class=\"math inline\">\\(p(x, z)\\)</span> we can sample from and know both a marginal and a conditional distributions, say, <span class=\"math inline\">\\(p(z)\\)</span> and <span class=\"math inline\">\\(p(x|z)\\)</span>.</li>\n<li><strong>Known everything</strong> case: Distributions <span class=\"math inline\">\\(p(x, z)\\)</span> which we can sample from and know all conditionals and marginals. This is a trivial case and doesn’t require any bounds. The MI can be estimated using Monte Carlo directly, so we’ll omit it from the discussion.</li>\n</ol>\n<h2 id=\"bounds-based-on-self-normalized-importance-sampling\">Bounds based on Self-Normalized Importance Sampling</h2>\n<p>Self-Normalized Importance Sampling (SNIS) has been shown (see my previous posts) to give both lower and upper bounds on the marginal log-likelihood:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\text{IWAE}:&\n\\quad\\quad\\quad\n\\log p(x) \\ge \\E_{q(z_{1:K}|x)} \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x,z_k)}{q(z_k|x)}\n\\\\\n\\text{IWHVI}:&\n\\quad\\quad\\quad\n\\log p(x) \\le \\E_{p(z_{0}|x)} \\E_{q(z_{1:K}|x)} \\log \\frac{1}{K+1} \\sum_{k=0}^K \\frac{p(x,z_k)}{q(z_k|x)}\n\\end{align*}\n\\]</span></p>\n<p>We use such bounds to give sandwich bound on the intractable entropy term in the MI. Another useful insight is that</p>\n<p><span class=\"math display\">\\[\n\\omega(z_{0:K}|x) := \\frac{\\hat\\rho(x, z_0) \\tau(z_{1:K}|x)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{\\hat\\rho(x, z_k)}{\\tau(z_k|x)}}\n\\]</span></p>\n<p>is a distribution (a valid pdf, to be precise) for almost any (the only condition is the same as in the standard importance sampling) unnormalized distribution <span class=\"math inline\">\\(\\hat\\rho(x, z)\\)</span> (a joint distribution over <span class=\"math inline\">\\(z\\)</span> and <span class=\"math inline\">\\(x\\)</span>) and a normalized distribution <span class=\"math inline\">\\(\\tau(z|x)\\)</span> (a distribution over <span class=\"math inline\">\\(z\\)</span> possibly conditioned on <span class=\"math inline\">\\(x\\)</span>). The fact that <span class=\"math inline\">\\(\\omega(z_{0:K}|x)\\)</span> is a valid distribution allows us to consider the following KL divergence:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n0\n\\le\n\\text{KL}(p(x, z_0) \\tau(z_{1:K}|x) \\mid\\mid p(x) \\omega(z_{0:K}|x))\n=\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{p(x, z_0) \\tau(z_{1:K}|x)}{p(x) \\omega(z_{0:K}|x)}\n\\end{align*}\n\\]</span></p>\n<p>Which gives the following lower bound on the MI: <span class=\"math display\">\\[\n\\begin{align*}\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{p(x | z_0)}{p(x)}\n& \\ge\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{\\omega(z_{0:K}|x)}{p(z_0) \\tau(z_{1:K}|x)} \\\\\n& =\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{\\hat\\rho(x, z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{\\hat\\rho(x, z_k)}{\\tau(z_k|x)}}\n-\n\\E_{p(z)} \\log p(z)\n\\end{align*}\n\\]</span></p>\n<p>Equivalently, by reparametrizing the bound in terms of <span class=\"math inline\">\\(\\hat\\varrho(x|z) = \\hat\\rho(x, z) / p(z)\\)</span> we have <span class=\"math display\">\\[\n\\begin{align*}\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{p(x | z_0)}{p(x)}\n\\ge\n\\E_{p(x, z_0)}\n\\E_{\\tau(z_{1:K}|x)}\n\\log\n\\frac{\\hat\\varrho(x | z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\hat\\varrho(x | z_k) \\frac{p(z_k)}{\\tau(z_k|x)}}\n\\end{align*}\n\\]</span></p>\n<p>These lower bounds work for any <span class=\"math inline\">\\(\\rho\\)</span> and <span class=\"math inline\">\\(\\varrho\\)</span>, and the optimal choices are <span class=\"math inline\">\\(p(x, z)\\)</span> and <span class=\"math inline\">\\(p(x|z)\\)</span>, correspondingly.</p>\n<h3 id=\"known-joint\">Known Joint</h3>\n<p>First, we’ll consider the easiest case – when we know the joint distribution in the form of a prior + conditional. A prominent example of this class is VAEs decoders, which is defined by some prior in the latent space <span class=\"math inline\">\\(p(z)\\)</span> and a decoder <span class=\"math inline\">\\(p_\\theta(x|z)\\)</span> that uses a neural network to generate a distribution over observations <span class=\"math inline\">\\(x\\)</span> for a particular <span class=\"math inline\">\\(z\\)</span>. Computing the MI of the decoder is arguably a natural way to measure the extent of posterior collapse<a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>, since MI can be expressed in the following form, which essentially measures the true posterior’s deviation from the prior:</p>\n<p><span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n:= \\E_{p(x, z)} \\log \\frac{p(z|x)}{p(z)}\n= \\E_{p(x)} D_{KL}(p(z|x) \\mid\\mid p(z))\n\\]</span></p>\n<p>However the MI as introduced above requires knowing marginal <span class=\"math inline\">\\(p(x)\\)</span>, which is intractable. Luckily, the <a href=\"/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html\">Multisample Variational Bounds</a> allow us to give efficient variational sandwich bounds on MI (where <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span> is an encoder with known density<a href=\"#fn3\" class=\"footnoteRef\" id=\"fnref3\"><sup>3</sup></a>):</p>\n<p><span class=\"math display\">\\[\n\\E_{\\substack{p_\\theta(x, z_0) \\\\ q_\\phi(z_{1:K}|x)} }\n\\log \\frac{p_\\theta(x|z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)}}\n\\le\n\\text{MI}[p_\\theta(x, z)]\n%= \\mathbb{E}_{p_\\theta(x, z)} \\left[ \\log p_\\theta(x|z) - \\log p_\\theta(x) \\right]\n\\le\n\\E_{\\substack{p_\\theta(x, z_0) \\\\ q_\\phi(z_{1:K}|x)} }\n\\log \\frac{p_\\theta(x|z_0)}{\\frac{1}{K} \\sum_{k=1}^K \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)}}\n\\]</span></p>\n<h3 id=\"known-conditional\">Known Conditional</h3>\n<p>However, we might not have any of marginal densities in the closed form. For example, this is the case if we want to estimate the amount of information the encoder <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span> puts into the code <span class=\"math inline\">\\(z\\)</span>. Since it only defines the conditional distribution, we pair it with some data-generating process <span class=\"math inline\">\\(p(x)\\)</span> which defines the population we’d like to evaluate the encoder over.</p>\n<p>Direct application of the aforementioned bounds leads to (where <span class=\"math inline\">\\(\\tau_\\eta(x_k|z)\\)</span> is our variational inverse distribution) <span class=\"math display\">\\[\n\\E_{\\substack{p(x_0) \\\\ q_\\phi(z | x_0) \\\\ \\tau_\\eta(x_{1:K}|z)}}\n\\!\\!\\!\n\\log \\frac{q_\\phi(z|x_0)}{\\frac{1}{K+1} \\sum\\limits_{k=0}^K \\frac{q_\\phi(z|x_k) p(x_k)}{\\tau_\\eta(x_k|z)}}\n\\le\n\\text{MI}[q_\\phi(z | x) p(x)]\n\\le\n\\!\\!\\!\n\\E_{\\substack{p(x_0) \\\\ q_\\phi(z | x_0) \\\\ \\tau_\\eta(x_{1:K}|z)}}\n\\!\\!\\!\n\\log \\frac{q_\\phi(z|x_0)}{\\frac{1}{K} \\sum\\limits_{k=1}^K \\frac{q_\\phi(z|x_k) p(x_k)}{\\tau_\\eta(x_k|z)}}\n\\]</span></p>\n<p>However, we might not have an access to the density <span class=\"math inline\">\\(p(x)\\)</span>. In this case one can resort to SIVI-like bounds by setting <span class=\"math inline\">\\(\\tau_\\eta(x|z) = p(x)\\)</span> and arrive to the following bounds: <span class=\"math display\">\\[\n\\E_{\\substack{p(x_{0:K}) \\\\ q_\\phi(z | x_0)}}\n\\log \\frac{q_\\phi(z|x_0)}{\\frac{1}{K+1} \\sum\\limits_{k=0}^K q_\\phi(z|x_k)}\n\\le\n\\text{MI}[q_\\phi(z | x) p(x)]\n\\le\n\\E_{\\substack{p(x_{0:K}) \\\\ q_\\phi(z | x_0)}}\n\\log \\frac{q_\\phi(z|x_0)}{\\frac{1}{K} \\sum\\limits_{k=1}^K q_\\phi(z|x_k)}\n\\]</span></p>\n<p>These bounds are known as special case of <a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE bounds</a> and are much worse due to uninformed proposal <span class=\"math inline\">\\(\\tau\\)</span>.</p>\n<h3 id=\"known-prior\">Known Prior</h3>\n<p>Sometimes we use complex implicit models as decoders and don’t have closed-form densities for <span class=\"math inline\">\\(p(x|z)\\)</span>. The most popular instance of such models is Generative Adversarial Networks, which is similar to VAE with an exception of not having a well-defined decoder’s density <span class=\"math inline\">\\(p(x|z)\\)</span> (but the prior <span class=\"math inline\">\\(p(z)\\)</span> is typically simple and known). In some sense, this density is degenerate: <span class=\"math inline\">\\(p(x|z) = \\delta(x - f(z))\\)</span> where <span class=\"math inline\">\\(f\\)</span> is generator’s neural network. Unfortunately, we cannot use such density in the IWHVI bounds. Are we doomed then? Turns out, not quite so. Even in this case it’s still possible to give an efficient multisample variational lower bound: <span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n\\ge\n\\E_{p(x, z_0)}\n\\E_{q_\\phi(z_{1:K}|x)}\n\\log \\frac{\\hat\\rho_\\eta(x|z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\hat\\rho_\\eta(x|z_k) \\frac{p(z_k)}{q_\\phi(z_k|x)}}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\hat\\rho_\\eta(x|z) = \\exp(h_\\eta(x|z))\\)</span> is any non-normalized energy-based model that essentially estimates the unknown density <span class=\"math inline\">\\(p(x|z)\\)</span>.</p>\n<p>I am not aware of any good <em>upper</em> bounds and if they are possible. I would think the answer is negative due to hardness of upper-bounding the crossentropy in the black-box case.</p>\n<h3 id=\"known-nothing\">Known Nothing</h3>\n<p>Staying in the realm of implicit models, let’s assume we trained an implicit inference model (think of GANs with encoders) and would like estimate the MI much like in the case of VAE’s encoder in the “known conditional” section. Denoting our inference model <span class=\"math inline\">\\(q(z|x)\\)</span> and the data-generating process <span class=\"math inline\">\\(p(x)\\)</span> (both densities are unknown to us, but we can sample from them), we can adapt the previous section’s lower bound by choosing the proposal <span class=\"math inline\">\\(\\tau_\\eta(x|z) = p(x)\\)</span> <span class=\"math display\">\\[\n\\text{MI}[q(z | x) p(x)]\n\\ge\n\\E_{p(x_{0:K})}\n\\E_{q(z|x_0)}\n\\log \\frac{\\hat\\rho_\\eta(z|x_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\hat\\rho_\\eta(z|x_k)}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\hat\\rho_\\eta(z|x) = \\exp(h_\\eta(z|x))\\)</span> is again a non-normalized energy-based model that estimates the unknown density <span class=\"math inline\">\\(q(z|x)\\)</span>.</p>\n<p>While convenient in its wide applicability, this bound is known to be very loose in cases when the true MI is high. We’ll discuss drawbacks and limitations in the next post on the topic.</p>\n<h2 id=\"bounds-based-on-annealed-importance-sampling\">Bounds based on Annealed Importance Sampling</h2>\n<p>SNIS is not the only way to obtain variational sandwich bounds on log marginal likelihood. Another widely known and powerful approach is <a href=\"https://arxiv.org/abs/1511.02543\">Annealed Importance Sampling</a> (AIS)</p>\n<p>AIS uses two distributions, called forward and backward: <span class=\"math display\">\\[\nq_\\rightarrow(z_{1:T} | x) = q(z_1|x) \\mathcal{T}_2(z_2 \\mid z_1, x) \\cdots  \\mathcal{T}_{T}(z_{T} \\mid z_{T-1}, x)\n\\\\\nq_\\leftarrow(z_{T:1} | x) = p(z_{T}|x) \\mathcal{T}_{T}(z_{T-1} \\mid z_{T}, x) \\cdots  \\mathcal{T}_{2}(z_{1} \\mid z_{2}, x)\n\\\\\nq_\\leftarrow(x, z_{T:1}) = p(x, z_{T}) \\mathcal{T}_{T}(z_{T-1} \\mid z_{T}, x) \\cdots  \\mathcal{T}_{2}(z_{1} \\mid z_{2}, x)\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\mathcal{T}_t\\)</span> is a transition operator that is designed to be invariant to <span class=\"math inline\">\\(p_t(z|x) \\propto q(z|x)^{1-\\beta_t} p(x, z)^{\\beta_t}\\)</span> and <span class=\"math inline\">\\(\\beta_{1:T+1}\\)</span> is a monotonically increasing sequence s.t. <span class=\"math inline\">\\(\\beta_1 = 0\\)</span> and <span class=\"math inline\">\\(\\beta_{T+1} = 1\\)</span>. That is, in the <em>forward distribution</em> <span class=\"math inline\">\\(q_\\rightarrow(z_{1:T} | x)\\)</span> one starts with a sample <span class=\"math inline\">\\(z_1\\)</span> from some proposal <span class=\"math inline\">\\(q(z|x)\\)</span> and then transforms it into a sample from <span class=\"math inline\">\\(p_2(z|x)\\)</span> using the <span class=\"math inline\">\\(\\mathcal{T}_t\\)</span> transition operator (typically a MCMC kernel). The sample <span class=\"math inline\">\\(z_2\\)</span> is then analogously transformed into <span class=\"math inline\">\\(z_3\\)</span> amd so on. The <em>backward distribution</em> <span class=\"math inline\">\\(q_\\rightarrow(z_{T:1} | x)\\)</span> is similar except it starts with the true posterior sample <span class=\"math inline\">\\(z_T \\sim p(z|x)\\)</span> and then sequentially transforms it into a sample from the proposal <span class=\"math inline\">\\(z_1 \\sim q(z|x)\\)</span>.</p>\n<p>Then one defines the importance weight</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\nw(z_{1:T} \\mid x)\n&=\n\\frac{q_\\leftarrow(z_{T:1} | x)}{q_\\rightarrow(z_{1:T} | x)}\n=\n\\frac{\\hat{p}_2(z_1 \\mid x)} {q(z_1|x)}\n\\frac{\\hat{p}_3(z_2 \\mid x)} {\\hat{p}_2(z_2 \\mid x)}\n\\cdots\n\\frac{p(x, z_{T})} {\\hat{p}_{T}(z_{T} \\mid x)} \\\\\n&=\n% \\left( \\tfrac{p(x, z)}{q(z|x)} \\right)^{\\beta_t} q(z|x)\n\\frac{ \\left( \\tfrac{p(x, z_1)}{q(z_1|x)} \\right)^{\\beta_2} q(z_1|x) }{q(z_1|x)}\n\\frac{ \\left( \\tfrac{p(x, z_2)}{q(z_2|x)} \\right)^{\\beta_3} q(z_2|x) }{ \\left( \\tfrac{p(x, z_2)}{q(z_2|x)} \\right)^{\\beta_2} q(z_2|x) }\n\\cdots\n\\frac{ \\left( \\tfrac{p(x, z_{T})}{q(z_{T}|x)} \\right)^{\\beta_{T+1}} q(z_{T}|x) }{ \\left( \\tfrac{p(x, z_{T})}{q(z_{T}|x)} \\right)^{\\beta_{T}} q(z_{T}|x) } \\\\\n&=\n\\left( \\tfrac{p(x, z_1)}{q(z_1|x)} \\right)^{\\beta_2 - \\beta_1}\n\\left( \\tfrac{p(x, z_2)}{q(z_2|x)} \\right)^{\\beta_3 - \\beta_2}\n\\cdots\n\\left(\\tfrac{p(x, z_{T})}{q(z_{T}|x)} \\right)^{\\beta_{T+1} - \\beta_{T}} \\\\\n\\end{align*}\n\\]</span> Where the second identity is due to <span class=\"math inline\">\\(\\mathcal{T}_t\\)</span> satisfying the detailed balance equation. Then one can show that <span class=\"math display\">\\[\n\\E_{q_\\rightarrow(z_{1:T} | x)} w(z_{1:T} \\mid x) = p(x)\n\\quad\\Rightarrow\\quad\n\\E_{q_\\rightarrow(z_{1:T} | x)} \\log w(z_{1:T} \\mid x) \\le \\log p(x),\n\\\\\n\\E_{q_\\leftarrow(z_{T:1} | x)} \\frac{1}{w(z_{1:T} \\mid x)} = \\frac{1}{p(x)}\n\\quad\\Rightarrow\\quad\n\\E_{q_\\leftarrow(z_{T:1} | x)} \\log w(z_{1:T} \\mid x) \\ge \\log p(x)\n\\]</span></p>\n<p>Which gives us another set of sandwich bounds, which we can use to sandwich bound the MI: <span class=\"math display\">\\[\n\\boxed{\n\\E_{q_\\leftarrow(x, z_{T:1} | x)}\n% \\E_{p_\\theta(x, z_T)}\n% \\E_{ \\mathcal{T}_{T}(z_{T-1} \\mid z_{T}, x) \\cdots  \\mathcal{T}_{2}(z_{1} \\mid z_{2}, x) }\n\\log \\frac{p_\\theta(x|z_T)}{ w(z_{1:T} \\mid x) }\n\\le\n\\text{MI}[p_\\theta(x, z)]\n\\le\n\\E_{p_\\theta(x, z_0)}\n\\E_{q_\\rightarrow(z_{1:T} | x)}\n\\log \\frac{p_\\theta(x|z_0)}{ w(z_{1:T} \\mid x) }\n}\n\\]</span></p>\n<p>One can also come up with a “decoder-free” version of the bound in a similar fashion to what we’ve done above. First, introduce the following distributions: <span class=\"math display\">\\[\n\\gamma_\\rightarrow(z_{1:T} | x) = q(z_1|x) \\mathcal{K}_2(z_2 \\mid z_1, x) \\cdots \\mathcal{K}_{T}(z_{T} \\mid z_{T-1}, x)\n\\\\\n\\gamma_\\leftarrow(x, z_{T:1}) = p(x, z_{T}) \\mathcal{K}_{T}(z_{T-1} \\mid z_{T}, x) \\cdots \\mathcal{K}_{2}(z_{1} \\mid z_{2}, x)\n\\]</span> Where <span class=\"math inline\">\\(\\mathcal{K}_t\\)</span> is now tailored to <span class=\"math inline\">\\(\\kappa_t(z|x) \\propto q(z|x)^{1-\\beta_t} \\left( \\hat\\varrho(x|z) p(z) \\right)^{\\beta_t}\\)</span> for certain unnormalized <span class=\"math inline\">\\(\\hat\\varrho(x|z)\\)</span></p>\n<p>Now consider <span class=\"math display\">\\[\n\\begin{align*}\n0\n& \\le\n\\text{KL}\\left(\n\\gamma_\\leftarrow(x, z_{T:1})\n\\mid\\mid\np(x) \\gamma_\\rightarrow(z_{1:T} | x)\n\\right)\n=\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log \\frac{\\gamma_\\leftarrow(x, z_{T:1})}{p(x) \\gamma_\\rightarrow(z_{1:T} | x)} \\\\\n& =\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\left[\n\\frac{p(x, z_{T})}{p(x) q(z_1|x) }\n\\frac{\\mathcal{K}_{2}(z_{1} \\mid z_{2}, x)}{\\mathcal{K}_2(z_2 \\mid z_1, x)}\n \\cdots \n\\frac{ \\mathcal{K}_{T}(z_{T-1} \\mid z_{T}, x) }{\\mathcal{K}_{T}(z_{T} \\mid z_{T-1}, x)}\n\\right] \\\\\n& =\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\left[\n\\frac{p(x, z_{T})}{p(x) q(z_1|x) }\n\\frac{\\kappa_2(z_1|x)}{\\kappa_2(z_2|x)}\n \\cdots \n\\frac{ \\kappa_T(z_{T-1}|x) }{\\kappa_T(z_{T}|x)}\n\\right] \\\\\n& =\n% q(z|x) \\left( \\hat\\varrho(x|z) \\frac{p(z)}{q(z|x)} \\right)^{\\beta_t}\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\left[\n\\tfrac{p(x, z_{T})}{p(x) q(z_1|x) }\n\\tfrac{ q(z_1|x) \\left( \\frac{\\hat\\varrho(x|z_1) p(z_1)}{q(z_1|x)} \\right)^{\\beta_2} }{ q(z_2|x) \\left( \\frac{\\hat\\varrho(x|z_2) p(z_2)}{q(z_2|x)} \\right)^{\\beta_2} }\n \\cdots \n\\tfrac{ q(z_{T-1}|x) \\left( \\frac{\\hat\\varrho(x|z_{T-1}) p(z_{T-1})}{q(z_{T-1}|x)} \\right)^{\\beta_T} }{ q(z_T|x) \\left( \\frac{\\hat\\varrho(x|z_T) p(z_T)}{q(z_T|x)} \\right)^{\\beta_T} }\n\\right]  \\\\\n& =\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\left[\n\\frac{p(x, z_{T})}{p(x)}\n\\frac{1}{p(z_T) \\hat\\varrho(x|z_T)}\n\\prod_{t=1}^T\n\\left( \\frac{\\hat\\varrho(x|z_t) p(z_t)}{q(z_t|x)} \\right)^{\\beta_{t+1}-\\beta_t}\n\\right]\n\\end{align*}\n\\]</span> Hence <span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n\\ge\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\frac{\\hat\\varrho(x|z_T)}{\\prod_{t=1}^T\n\\left( \\frac{\\hat\\varrho(x|z_t) p(z_t)}{q(z_t|x)} \\right)^{\\beta_{t+1}-\\beta_t}}\n\\]</span></p>\n<p>Now we can again reparametrize this formula in terms of <span class=\"math inline\">\\(\\hat\\rho(x, z) = \\hat\\varrho(x|z) p(z)\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n\\ge\n\\mathbb{E}_{\\gamma_\\leftarrow(x, z_{T:1})}\n\\log\n\\frac{\\hat\\rho(x, z_T)}{\\prod_{t=1}^T\n\\left( \\frac{\\hat\\rho(x, z_t)}{q(z_t|x)} \\right)^{\\beta_{t+1}-\\beta_t}}\n- \\mathbb{E}_{p(z)} \\log p(z)\n\\]</span></p>\n<p>It’s tempting to simply put <span class=\"math inline\">\\(q(z|x) = p(z)\\)</span> to obtain a blackbox AIS-based analogue of the InfoNCE bound, however notice that <span class=\"math inline\">\\(\\gamma_\\leftarrow(x, z_{T:1})\\)</span> relies on an MCMC kernel that gradually transforms a sample <span class=\"math inline\">\\(z_T \\sim p(z_T|x)\\)</span> into <span class=\"math inline\">\\(z_1 \\sim p(z_1)\\)</span> and thus needs to know this density. Thus I don’t think one can use AIS in the blackbox mode.</p>\n<p>Finally, note while the bound is valid for any <span class=\"math inline\">\\(\\hat\\rho\\)</span> and <span class=\"math inline\">\\(\\hat\\varrho\\)</span>, it’s not quite differentiable w.r.t. their parameters as any proper MCMC method would require an accept-reject step which is not differentiable. Thus if one seeks to learn <span class=\"math inline\">\\(\\hat\\rho\\)</span> or <span class=\"math inline\">\\(\\hat\\varrho\\)</span>, an alternative objective should be used. Luckily, the SNIS-based lower bound with small <span class=\"math inline\">\\(K\\)</span> would work just fine.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>I presented two different ways to give bounds on the MI in cases of variable complexity. The SNIS-based approach seem to be somewhat novel (the special case of InfoNCE has been already known), and is applicable in many different problems. The AIS-based one is based on well-known sandwich bounds on the log marginal likelihoods, but I haven’t seen it being applied to the problem of MI estimation. The reason might be that it’s more restrictive that the SNIS-based estimations: AIS only works for continuous variables, requires complicated MCMC to function and does not seem to allow blackbox estimators. On the positive side of things, AIS-based estimator should perform much better in high-dimensional problems with large MI, especially if one uses gradient-based kernels like HMC.</p>\n<p>Next, I’ll share some of my thoughts on drawbacks of these (and some other) bounds, in particular in light of the notorious “Formal Limitations” paper.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>See <a href=\"http://approximateinference.org/accepted/HoffmanJohnson2016.pdf\">ELBO surgery: yet another way to carve up the variational evidence lower bound</a><a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>Oftentimes <span class=\"math inline\">\\(D_{KL}(q_\\phi(z|x) \\mid\\mid p(z))\\)</span> is used to measure the extent of the so called posterior collapse. One can argue this is an indirect metric as it does not use <em>decoder at all</em> and relies on the encoder’s ability to approximate the true posterior better than the prior. Moreover, high <span class=\"math inline\">\\(D_{KL}(q_\\phi(z|x) \\mid\\mid p(z))\\)</span> is likely to be an indicator of poor encoder <span class=\"math inline\">\\(q(z|x)\\)</span>, whereas the Mutual Information is monotonic in decoder’s quality.<a href=\"#fnref2\">↩</a></p></li>\n<li id=\"fn3\"><p>As I have argued in the IWHVI paper, it might be beneficial to fit two different encoders for lower and upper bounds, correspondingly.<a href=\"#fnref3\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Sat, 10 Aug 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-08-10-thoughts-on-mutual-information-more-estimators.html",
  "dc:creator": "Artem"
}