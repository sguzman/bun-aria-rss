{
  "title": "Probability-Dependent Gradient Decay in Large Margin Softmax. (arXiv:2210.17145v1 [stat.ML] CROSS LISTED)",
  "link": "http://arxiv.org/abs/2210.17145",
  "description": "<p>In the past few years, Softmax has become a common component in neural\nnetwork frameworks. In this paper, a gradient decay hyperparameter is\nintroduced in Softmax to control the probability-dependent gradient decay rate\nduring training. By following the theoretical analysis and empirical results of\na variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we\nfind that the generalization performance depends significantly on the gradient\ndecay rate as the confidence probability rises, i.e., the gradient decreases\nconvexly or concavely as the sample probability increases. Moreover,\noptimization with the small gradient decay shows a similar curriculum learning\nsequence where hard samples are in the spotlight only after easy samples are\nconvinced sufficiently, and well-separated samples gain a higher gradient to\nreduce intra-class distance. Based on the analysis results, we can provide\nevidence that the large margin Softmax will affect the local Lipschitz\nconstraint of the loss function by regulating the probability-dependent\ngradient decay rate. This paper provides a new perspective and understanding of\nthe relationship among concepts of large margin Softmax, local Lipschitz\nconstraint and curriculum learning by analyzing the gradient decay rate.\nBesides, we propose a warm-up strategy to dynamically adjust Softmax loss in\ntraining, where the gradient decay rate increases from over-small to speed up\nthe convergence rate.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1\">Siyuan Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xie_L/0/1/0/all/0/1\">Linbo Xie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"
}