{
  "title": "Federated Learning and Differential Privacy",
  "link": "https://datasciencevademecum.com/2019/12/12/federated-learning-and-differential-privacy/",
  "dc:creator": "Gianmario",
  "pubDate": "Thu, 12 Dec 2019 14:02:00 +0000",
  "category": [
    "Differential Privacy",
    "Ethics",
    "Federated Learning",
    "Machine Learning",
    "Privacy",
    "data gravity",
    "data sharing",
    "distributed learning",
    "machine learning on edge"
  ],
  "guid": "https://datasciencevademecum.wordpress.com/?p=2332",
  "description": "<p>Federated Learning refers to machine learning (ML) techniques that can train algorithms across multiple decentralized machines holding different local data samples — all without exchanging data.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2019/12/12/federated-learning-and-differential-privacy/\">Federated Learning and Differential Privacy</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "content:encoded": "\n<p><em>This piece is part of a series on 2019 trends in the AI and Machine Learning industry. You can read my full thoughts on the past year in </em><a href=\"https://www.helixa.ai/blog/ai-trends-2019\"><em>this summary I wrote for the Helixa blog</em></a><em>, which also includes links to the other in-depth pieces in this series.</em></p>\n\n\n\n<p><em>&#8212;&#8212;&#8212;-</em></p>\n\n\n\n<p>“Federated Learning” is a new term for many of us, and it looks like the dawn of a new AI epoch.</p>\n\n\n\n<p>Federated Learning refers to machine learning (ML) techniques that can train algorithms across multiple decentralized machines holding different local data samples — all without exchanging data. This new approach is different from traditional centralized or distributed training because there is no assumption that the local data samples are identically distributed.&nbsp;</p>\n\n\n\n<p>Federated Learning aims to mitigate the problem of <a href=\"https://datagravitas.com/2010/12/07/data-gravity-in-the-clouds/\">Data Gravity</a>, defined by Dave McCrory as “the ability of bodies of data to attract applications, services, and other data.”</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"468\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seAPvqqYnvmoz1How4x5wKQ.png?resize=624%2C468&#038;ssl=1\" alt=\"\" class=\"wp-image-2383\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seAPvqqYnvmoz1How4x5wKQ.png?w=624&ssl=1 624w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seAPvqqYnvmoz1How4x5wKQ.png?resize=300%2C225&ssl=1 300w\" sizes=\"(max-width: 624px) 100vw, 624px\" data-recalc-dims=\"1\" /></figure>\n\n\n\n<p>In order to scale with the radical increase in devices, we need to move computation closer to the data generation.&nbsp;</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p><em>Federated Learning is about centralizing models on decentralized data.</em></p></blockquote>\n\n\n\n<p>The major use case is edge computing, where issues of data privacy, security, and network traffic make it expensive and difficult to quickly collect and process data in the cloud. This is especially relevant to IoT devices and smartphones.</p>\n\n\n\n<p>The TensorFlow team has recently released <a href=\"https://www.tensorflow.org/federated\">TFF</a>, an extension to support Federated Learning natively into TensorFlow.<br></p>\n\n\n\n<p><a href=\"https://conferences.oreilly.com/artificial-intelligence/ai-eu/public/schedule/speaker/332591\">Alex Ingerman</a>, Product Manager at Google, presented an interesting use case of TFF: <a href=\"https://ai.google/research/pubs/pub47586/\">Federated Learning for Mobile Keyboard Prediction.</a> GoogleBoard was trained this way, with typing data from millions of smartphone devices, in order to improve keyboard prediction without infringing on the privacy consumers expect with their personal conversations.<br></p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"417\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seMbabhd7CVZmqLCmwStoDg.png?resize=624%2C417&#038;ssl=1\" alt=\"\" class=\"wp-image-2384\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seMbabhd7CVZmqLCmwStoDg.png?w=624&ssl=1 624w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/seMbabhd7CVZmqLCmwStoDg.png?resize=300%2C200&ssl=1 300w\" sizes=\"(max-width: 624px) 100vw, 624px\" data-recalc-dims=\"1\" /></figure>\n\n\n\n<p>For an easy to understand visualization of how Federated Learning works in practice, you can read <a href=\"https://federated.withgoogle.com/\">this online comic illustration</a> by Google AI.</p>\n\n\n\n<p>The main privacy principles to respect are:</p>\n\n\n\n<ol><li>Only access aggregated anonymized reports from devices (e.g. model updates), no raw data</li><li>Use focused collection, which only reports the minimum needed</li><li>Never persist per-device report</li><li>Utilize federated model averaging to generate a global model</li><li>Don’t memorize individual reports during training (overfitting on a single data sample)</li></ol>\n\n\n\n<p>Here is where <a href=\"https://medium.com/georgian-impact-blog/a-brief-introduction-to-differential-privacy-eacf8722283b\">Differential Privacy</a> (DP) comes in place. DP relates to those systems for publicly sharing information about a dataset, describing patterns of groups within the dataset while withholding information about individuals. An algorithm is differentially private if it is robust to <a href=\"https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf\">Membership Inference Attacks</a>, which means that it is impossible for an external observer to tell from the output of the model whether a particular individual&#8217;s information was used in the computation.</p>\n\n\n\n<p>In the specific case of training machine learning models on different data sets, DP mostly consists of two injections:</p>\n\n\n\n<ul><li>Adding noise to the model parameters</li><li>Clipping the maximum model parameters updates (helpful for privacy principle #5)</li></ul>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"325\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/scjmYVkEaTs8rpSiCd5QT0g.png?resize=624%2C325&#038;ssl=1\" alt=\"\" class=\"wp-image-2385\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/scjmYVkEaTs8rpSiCd5QT0g.png?w=624&ssl=1 624w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/scjmYVkEaTs8rpSiCd5QT0g.png?resize=300%2C156&ssl=1 300w\" sizes=\"(max-width: 624px) 100vw, 624px\" data-recalc-dims=\"1\" /></figure>\n\n\n\n<p>In addition to the primary use case of edge devices, Federated Learning and Differential Privacy can boost model performances for the case where different clients want to collaborate without sharing raw data with each other. An example is a <a href=\"https://arxiv.org/pdf/1811.04911.pdf\">framework</a> developed by Georgian Partners and Bluecore Technologies leveraging the findings explained in <a href=\"https://arxiv.org/abs/1606.04722\">Bolt-on differential privacy for scalable stochastic gradient descent-based analytics</a>.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"624\" height=\"252\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/sNH8ZLr8Nwf5LPVdPBfKr3w.png?resize=624%2C252&#038;ssl=1\" alt=\"\" class=\"wp-image-2386\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/sNH8ZLr8Nwf5LPVdPBfKr3w.png?w=624&ssl=1 624w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/04/sNH8ZLr8Nwf5LPVdPBfKr3w.png?resize=300%2C121&ssl=1 300w\" sizes=\"(max-width: 624px) 100vw, 624px\" data-recalc-dims=\"1\" /></figure>\n\n\n\n<p>When you consider recent privacy regulations like GDPR and increased public awareness of data privacy issues, federated learning has the potential to sidestep one of the largest problems in the industry: to help society to securely cooperate for the common good.&nbsp;</p>\n\n\n\n<p>Read this use case of <a href=\"https://www.hpcwire.com/2019/10/17/federated-learning-applied-to-cancer-research/\">Federated Learning applied to Cancer Research</a>.</p>\n\n\n\n<p>&#8212;&#8212;&#8212;-</p>\n\n\n\n<p><em>For curious, AI-focused professionals who want to innovate responsibly, Helixa derives complex human insights through ethical and intentional machine learning. We do this more effectively than anyone else by aggregating multiple data sources, prioritizing consumer privacy, and returning results in seconds. Visit </em><a href=\"http://www.helixa.ai\"><em>www.helixa.ai</em></a><em> to learn more.</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2019/12/12/federated-learning-and-differential-privacy/\">Federated Learning and Differential Privacy</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "post-id": 2335
}