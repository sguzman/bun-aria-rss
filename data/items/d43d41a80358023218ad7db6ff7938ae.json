{
  "title": "Neural Samplers and Hierarchical Variational Inference",
  "link": "http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html",
  "description": "<p>This post sets background for the upcoming post on my work on more efficient use of neural samplers for Variational Inference.</p>\n<!--more-->\n<h2 id=\"variational-inference\">Variational Inference</h2>\n<p>At the core of <em>Bayesian Inference</em> lies the well-known Bayes’ theorem, relating our prior beliefs <span class=\"math inline\">\\(p(z)\\)</span> with those obtained after observing some data <span class=\"math inline\">\\(x\\)</span>:</p>\n<p><span class=\"math display\">\\[\np(z|x)\n=\n\\frac{p(x|z) p(z)}{p(x)}\n=\n\\frac{p(x|z) p(z)}{\\int p(x, z) dz}\n\\]</span></p>\n<p>However, in most practical cases the denominator <span class=\"math inline\">\\(p(x)\\)</span> requires intractable integration. Thus the field of Approximate Bayesian Inference seeks to efficiently approximate this posterior. For example, MCMC-based methods essentially use sample-based empirical distribution as an approximation.</p>\n<p>In problems of <em>learning</em> latent variable models (for example, <a href=\"/posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html\">VAEs</a>) we seek to do maximum likelihood learning for some hierarchical model <span class=\"math inline\">\\(p_\\theta(x) = \\int p_\\theta(x, z) dz\\)</span>, but computing the integral is intractable and latent variables <span class=\"math inline\">\\(z\\)</span> are not observed.</p>\n<p><a href=\"/posts/2016-07-01-neural-variational-inference-classical-theory.html\">Variational Inference</a> is a method that gained a lot of popularity recently, especially due to its scalability. It nicely allows for simultaneous inference (finding the posterior approximate) and learning (optimizing parameters of the model) by means of the <em>evidence lower bound</em> (ELBO) on the <em>marginal log-likelihood</em> <span class=\"math inline\">\\(\\log p_\\theta(x)\\)</span>, obtained by applying importance sampling followed by Jensen’s inequality:</p>\n<p><span class=\"math display\">\\[\n\\log p_\\theta(x)\n= \\log \\mathbb{E}_{p_\\theta(z)} p_\\theta(x|z)\n= \\log \\mathbb{E}_{q_\\phi(z|x)} \\frac{p_\\theta(x, z)}{q_\\phi(z|x)}\n\\ge \\mathbb{E}_{q_\\phi(z|x)} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)}\n=: \\mathcal{L}\n\\]</span></p>\n<p>This lower bound should be maximized w.r.t. both <span class=\"math inline\">\\(\\phi\\)</span> (variational parameters) and <span class=\"math inline\">\\(\\theta\\)</span> (model parameters). To better understand the effect of such optimization, it’s helpful to consider the gap between the marginal log-likelihood and the bound. It’s easy to show that this gap is equal to some Kullback-Leibler (KL) divergence:</p>\n<p><span class=\"math display\">\\[\n\\log p_\\theta(x) - \\mathbb{E}_{q(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\n=\nD_{KL}(q_\\phi(z|x) \\mid\\mid p_\\theta(z|x))\n\\]</span></p>\n<p>Now it’s easy to see that maximizing the ELBO w.r.t. <span class=\"math inline\">\\(\\phi\\)</span> tightens the bound and performs approximate inference – <span class=\"math inline\">\\(q(z|x)\\)</span> becomes closer to the true posterior <span class=\"math inline\">\\(p(z|x)\\)</span> as measured by the KL divergence. While we hope that maximizig the bound w.r.t. <span class=\"math inline\">\\(\\theta\\)</span> increases marginal log-likelihood <span class=\"math inline\">\\(\\log p_\\theta(x)\\)</span>, this is obstructed by the KL-divergence. In a more realistic setting maximizing the ELBO is equivalent to maximizing the marginal log-likelihood regularized with the <span class=\"math inline\">\\(D_{KL}(q_\\phi(z|x) \\mid\\mid p_\\theta(z|x))\\)</span>, except there’s no hyperparameter to control the strength of this regularization. This regularization prevents the true posterior <span class=\"math inline\">\\(p_\\theta(z|x)\\)</span> from deviating too much from the variational distribution <span class=\"math inline\">\\(q(z|x)\\)</span>, which is not bad, as you’d know that the true posterior has somewhat simple form, but on the other hand it prevents us from learning powerful and expressive models <span class=\"math inline\">\\(p_\\theta(x) = \\int p_\\theta(x|z) p_\\theta(z) dz\\)</span>. Therefore if we’re after expressive models <span class=\"math inline\">\\(p_\\theta(x)\\)</span>, we probably should minimize such regularization effect, for example, by means of more expressive variational approximations.</p>\n<p>Intuitively, tighter the bound – lesser the regularizational effect is. And it’s relatively easy to obtain a tighter bound: <span class=\"math display\">\\[\n\\begin{align*}\n\\log p_\\theta(x)\n&= \\log \\mathbb{E}_{p_\\theta(z)} p_\\theta(x|z)\n= \\log \\mathbb{E}_{q_\\phi(z_{1:K}|x)} \\frac{1}{K} \\sum_{k=1}^K \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)} \\\\\n&\\ge \\mathbb{E}_{q_\\phi(z_{1:K}|x)} \\log\\left( \\frac{1}{K} \\sum_{k=1}^K \\frac{p_\\theta(x, z_k)}{q_\\phi(z_k|x)} \\right)\n=: \\mathcal{L}_K\n\\ge \\mathcal{L}\n\\end{align*}\n\\]</span> That is, by simply taking several samples to estimate the marginal likelihood <span class=\"math inline\">\\(p_\\theta(x)\\)</span> under the logarithm, we made the bound tighter. Such bounds usually are called <a href=\"/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html\">IWAE bounds</a> (for <a href=\"https://arxiv.org/abs/1509.00519\">Importance Weighted Autoencoders paper</a> they were first introduced in), but we’ll be calling these bounds <em>Multisample Variational Lower Bounds</em>. Such bounds <a href=\"https://arxiv.org/abs/1808.09034\">were shown</a> to correspond to using more expressive proposal distributions and are very powerful, but require multiple evaluations of the decoder <span class=\"math inline\">\\(p_\\theta(x|z)\\)</span>, which might be very expensive for complex models, for example, when applying <a href=\"https://arxiv.org/abs/1802.02032\">VAEs to dialogue modelling</a>.</p>\n<p>An alternative direction is to use more expressive family of variational distributions <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span>. Moreover, with the explosion of Deep Learning we actually know one family of models that have empirically demonstrated terrific approximation capabilities – Neural Networks. We therefore will consider so called Neural Samplers as generators of approximate posterior <span class=\"math inline\">\\(q(z|x)\\)</span> samples. A <em>Neural Sampler</em> is simply a neural network that is trained to take some simple (say, Gaussian) random variable <span class=\"math inline\">\\(\\psi \\sim q(\\psi|x)\\)</span> and transform it into <span class=\"math inline\">\\(z\\)</span> that has the properties we seek. Canonical examples are GANs and VAEs and we’ll get back to them later in the discussion.</p>\n<p>And using neural nets is not a new idea. There’s been a lot of research along this direction, which we might roughly classify into 3 directions based on how they deal with the intractable <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span> term:</p>\n<ul>\n<li>Flows</li>\n<li>Estimates</li>\n<li>Bounds</li>\n</ul>\n<p>I’ll briefly cover the first two and then discuss the last one, which is of central relevance to this post.</p>\n<h3 id=\"flows\">Flows</h3>\n<p>So called Flow models appeared on the radar with the publication of the <a href=\"https://arxiv.org/abs/1505.05770\">Normalizing Flows paper</a>, and then quickly exploded into a hot topic of research. At the moment there exist dozens of works on all kinds of flows. The basic idea is that if the Neural Net defining the sampler is invertible, then by computing its Jacobian (the determinant of the Jacobi matrix) we can analytically find the density <span class=\"math inline\">\\(q(z|x)\\)</span>. Flows further restrict the samplers to have efficiently computable Jacobians. For further reading refer to <a href=\"http://akosiorek.github.io/ml/2018/04/03/norm_flows.html\">Adam Kosiorek’s post</a>.</p>\n<p>Flows were shown to be very powerful, they even managed to model the high-dimensional data directly, as was shown by OpenAI researchers with <a href=\"https://openai.com/blog/glow/\">Glow model</a>. However, Flow-based model require a neural network specially designed to be invertible and have easy-to-compute Jacobian. Such restrictions might lead to inefficiency in parameter usage, requiring much more parameters and compute compared to simpler methods. The aforementioned Glow uses a lot of parameters and compute to learn modestly hi-res images.</p>\n<h3 id=\"estimates\">Estimates</h3>\n<p>Another direction is to estimate <span class=\"math inline\">\\(q_\\phi(z|x)/p(z)\\)</span> by means of auxiliary models. For example, the <a href=\"http://blog.shakirm.com/2018/01/machine-learning-trick-of-the-day-7-density-ratio-trick/\">Density Ratio Trick</a> lying at the heart of many GANs say that if you have an optimal discriminator <span class=\"math inline\">\\(D^*(z, x)\\)</span> discerning samples from <span class=\"math inline\">\\(q(z|x)\\)</span> from those from <span class=\"math inline\">\\(p(z)\\)</span> (for the given <span class=\"math inline\">\\(x\\)</span>), then the following is true:</p>\n<p><span class=\"math display\">\\[\n\\frac{D^*(z, x)}{1 - D^*(z, x)} = \\frac{q(z|x)}{p(z)}\n\\]</span></p>\n<p>In practice we do not have the optimal classifier, so instead we train auxiliary model to perform such classification. A particularly successful approach along this direction is the <a href=\"https://avg.is.tuebingen.mpg.de/publications/mescheder2017arxiv\">Adversarial Variational Bayes</a>. Biggest advantage of this method is the lack of any restrictions on the Neural Sampler (except the standard requirement of differentiability). The disadvantage is that it loses all bound guarantees and inherits a lot of stability issues from GANs.</p>\n<h2 id=\"bounds-and-hierarchical-variational-inference\">Bounds and Hierarchical Variational Inference</h2>\n<p>Arguably, the most natural approach to employing Neural Samplers as variational approximations is to give an efficient lower bound on the ELBO. In particular, we’d like to give a variational lower bound on the intractable term <span class=\"math inline\">\\(\\log \\tfrac{1}{q_\\phi(z|x)}\\)</span>.</p>\n<p>You can notice that for the Neural Sampler as described above the marginal density <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span> has the form of <span class=\"math inline\">\\(q_\\phi(z|x) = \\int q_\\phi(z|x, \\psi) q_\\phi(\\psi|x) d\\psi\\)</span>, very similr to that of VAE itself! Indeed, the Neural Sampler is a latent variable model like the VAE itself, except its conditioned on <span class=\"math inline\">\\(x\\)</span>. Great – you might think – we’ll just reuse the bounds we have derived above, problem solved, right? Well, no. The problem is that we need to give a lower bound on <strong>negative</strong> marginal log-density, or equivalently, an upper bound on the marginal log-density.</p>\n<p>But first we need to figure out one important question: what is <span class=\"math inline\">\\(q_\\phi(z|x, \\psi)\\)</span>? In case of the GAN-like procedure we could say that this density is degenerate: <span class=\"math inline\">\\(q_\\phi(z|\\psi, x) = \\delta(z - f_\\phi(\\psi, x))\\)</span> where <span class=\"math inline\">\\(f_\\phi\\)</span> is the neural network that generates <span class=\"math inline\">\\(z\\)</span> from <span class=\"math inline\">\\(\\psi\\)</span>. While the estimation-based approach is fine with this since it doesn’t work with densities directly, for the bounds, however, we need <span class=\"math inline\">\\(q_\\phi(z|x, \\psi)\\)</span> to be a well-defined density, so from now on we’ll assume it’s some proper density, not the delta function<a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a>.</p>\n<p>Luckily, one can use the following identity</p>\n<p><span class=\"math display\">\\[\n\\mathbb{E}_{q_\\phi(\\psi|z, x)} \\frac{\\tau_\\eta(\\psi|z, x)}{q_\\phi(z, \\psi|x)}\n=\n\\frac{1}{q_\\phi(z|x)}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\tau(\\psi|z, x)\\)</span> is arbitrary density we’ll be calling <em>auxiliary variational distribution</em>. Then, by applying logarithm and the Jensen’s inequality, we obtain a much needed variational upper bound:</p>\n<p><span class=\"math display\">\\[\n\\log q_\\phi(z|x)\n\\le\n\\mathbb{E}_{q_\\phi(\\psi|z, x)} \\log \\frac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)}\n:=\n\\mathcal{U}\n\\]</span></p>\n<p>Except – oops – it needs a sample from the true inverse model <span class=\"math inline\">\\(q_\\phi(\\psi|z, x)\\)</span>, which in general is not any easier to obtain than to calculate the <span class=\"math inline\">\\(\\log q_\\phi(z)\\)</span> in the first place. Bummer? No – turns out, we can use the fact that samples <span class=\"math inline\">\\(z\\)</span> are coming from the same hierarchical process <span class=\"math inline\">\\(q_\\phi(z, \\psi|x)\\)</span>! Indeed, since we’re interested in the <span class=\"math inline\">\\(\\log q_\\phi(z)\\)</span> averaged over all <span class=\"math inline\">\\(z|x\\)</span>: <span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{E}_{q_\\phi(z|x)}\n\\log q_\\phi(z|x)\n&\\le\n\\mathbb{E}_{q_\\phi(z|x)}\n\\mathbb{E}_{q_\\phi(\\psi|z, x)} \\log \\frac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)} \\\\\n& =\n\\mathbb{E}_{q_\\phi(z, \\psi|x)} \\log \\frac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)} \\\\\n&=\n\\mathbb{E}_{q_\\phi(\\psi|x)}\n\\mathbb{E}_{q_\\phi(z|\\psi, x)} \\log \\frac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)}\n\\end{align*}\n\\]</span></p>\n<p>These algebraic manipulations show that if we sampled <span class=\"math inline\">\\(z\\)</span> through a hierarchical scheme, then <span class=\"math inline\">\\(\\psi\\)</span> used to generate this <span class=\"math inline\">\\(z\\)</span> can be thought of as a free posterior sample<a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>. This leads to the following lower bound on the ELBO, introduced in <a href=\"https://arxiv.org/abs/1511.02386\">Hierarchical Variational Models</a> paper:</p>\n<p><span class=\"math display\">\\[\n\\log p_\\theta(x)\n\\ge\n\\mathcal{L}\n\\ge \\mathbb{E}_{q_\\phi(z, \\psi|x)} \\log \\frac{p_\\theta(x, z)}{ \\tfrac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)} }\n\\]</span> Interestingly, this bound admits another interpretation. Indeed, it can be equivalently represented as <span class=\"math display\">\\[\n\\log p_\\theta(x)\n\\ge \\mathbb{E}_{q_\\phi(z, \\psi|x)} \\log \\frac{p_\\theta(x, z) \\tau_\\eta(\\psi|z, x)}{q_\\phi(z, \\psi|x) }\n\\]</span> Which is just ELBO for an extended model where the latent code <span class=\"math inline\">\\(z\\)</span> as extended with <span class=\"math inline\">\\(\\psi\\)</span>, and since there was not <span class=\"math inline\">\\(\\psi\\)</span> in the original model <span class=\"math inline\">\\(p_\\theta(x, z)\\)</span>, we extended the model as well with <span class=\"math inline\">\\(\\tau_\\eta(\\psi|z, x)\\)</span>. This view has been investigated in the <a href=\"https://arxiv.org/abs/1602.05473\">Auxiliary Deep Generative Models</a> paper.</p>\n<p>Let’s now return to the variational upper bound <span class=\"math inline\">\\(\\mathcal{U}\\)</span>. Can we give a multisample variational upper bound on <span class=\"math inline\">\\(\\log q_\\phi(z|x)\\)</span> similar to IWAE? Well, following the same logic, we can arrive to the following:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\log \\frac{1}{q_\\phi(z|x)}\n& =\n\\log\n\\mathbb{E}_{q_\\phi(\\psi_{1:K}|z, x)} \\frac{1}{K} \\sum_{k=1}^K \\frac{\\tau_\\eta(\\psi_k|z, x)}{q_\\phi(z, \\psi_k|x)} \\\\\n&\\ge\n\\mathbb{E}_{q_\\phi(\\psi_{1:K}|z, x)} \n\\log \\frac{1}{K} \\sum_{k=1}^K \\frac{\\tau_\\eta(\\psi_k|z, x)}{q_\\phi(z, \\psi_k|x)}\n\\end{align*}\n\\]</span> <span class=\"math display\">\\[\n\\log q_\\phi(z|x)\n\\le\n\\mathbb{E}_{q_\\phi(\\psi_{1:K}|z, x)} \n\\log \\frac{1}{\\frac{1}{K} \\sum_{k=1}^K \\frac{\\tau_\\eta(\\psi_k|z, x)}{q_\\phi(z, \\psi_k|x)}}\n\\]</span></p>\n<p>However, this bound – Variational Harmonic Mean Estimator – is no good as it uses <span class=\"math inline\">\\(K\\)</span> samples from the true inverse model <span class=\"math inline\">\\(q_\\phi(\\psi|x,z)\\)</span> whereas we can have only one free sample. The rest have to be obtained through expensive MCMC sampling and that doesn’t scale well. Interestingly, this estimator was already presented in the original VAE paper (though buried in the Appendix D), but discarded as too unstable.</p>\n<h3 id=\"why-multisample-variational-upper-bound\">Why multisample variational upper bound?</h3>\n<p>The gap between the ELBO and it’s tractable lower bound can be shown to be <span class=\"math display\">\\[\n\\mathcal{L}\n-\n\\mathbb{E}_{q_\\phi(z, \\psi|x)} \\log \\frac{p_\\theta(x, z)}{ \\tfrac{q_\\phi(z, \\psi|x)}{\\tau_\\eta(\\psi|z, x)} }\n=\nD_{KL}(q_\\phi(\\psi|x,z) \\mid\\mid \\tau_\\eta(\\psi|x,z))\n\\]</span> So since we’ll be using some simple <span class=\"math inline\">\\(\\tau_eta(\\psi|x,z)\\)</span>, we’ll be restricting the true inverse model <span class=\"math inline\">\\(q_\\phi(\\psi|x,z)\\)</span> to also be somewhat simple, limiting the expressivity of <span class=\"math inline\">\\(q(z|x)\\)</span>, thus limiting the expressivity <span class=\"math inline\">\\(p(z|x)\\)</span>… Looks like we ended up with where we started, right? Well, not quite so, as we might have gained more than lost by moving the simple distribution from <span class=\"math inline\">\\(q(z|x)\\)</span> to <span class=\"math inline\">\\(\\tau(\\psi|x,z)\\)</span>, but still not quite satisfying. So having a multisample upper bound would allow us to give tighter bounds (which don’t suffer from the regularization that much) and not invoke any additional model’s decoder <span class=\"math inline\">\\(p_\\theta(x|z)\\)</span> evaluations (see the Variational Harmonic Mean Estimator above as example).</p>\n<p>So… Are there efficient multisample variational upper bounds? A year ago you might have thought the answer is “Probably no”, until… [To be continued]</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>The problem is that delta function is not a real function, but a generalized function, and a special case has to be taken to deal with them.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>This is not a new result, see <a href=\"https://arxiv.org/abs/1511.02543\">Grosse et al.</a>, section 4.2, paragraph on “simulated data”.<a href=\"#fnref2\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Fri, 26 Apr 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html",
  "dc:creator": "Artem"
}