{
  "title": "Estimating Discrete Entropy, Part 3",
  "link": "",
  "published": "2015-03-07T16:00:00+00:00",
  "updated": "2015-03-07T16:00:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-03-07:/sebastian/blog/estimating-discrete-entropy-part-3.html",
  "summary": "<p>In the last two parts (<a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html\">part one</a>,\n<a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html\">part two</a>) we looked at the problem of\nentropy estimation and several popular estimators.</p>\n<p>In this final article we will take a look at two Bayesian approaches to the\nproblem.</p>\n<h2>Bayesian Estimator due â€¦</h2>",
  "content": "<p>In the last two parts (<a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html\">part one</a>,\n<a href=\"http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html\">part two</a>) we looked at the problem of\nentropy estimation and several popular estimators.</p>\n<p>In this final article we will take a look at two Bayesian approaches to the\nproblem.</p>\n<h2>Bayesian Estimator due to Wolpert and Wolf</h2>\n<p>The first Bayesian approach to entropy estimation was proposed by David\nWolpert and David Wolf in 1995 in their paper \"Estimating functions of\nprobability distributions from a finite set of samples\", published in Physical\nReview E, Vol. 52, No. 6, 1995,\n<a href=\"http://journals.aps.org/pre/abstract/10.1103/PhysRevE.52.6841\">publisher link</a>, and\na longer <a href=\"http://www.santafe.edu/media/workingpapers/93-07-046.pdf\">tech report from\n1993</a>.</p>\n<p>The idea is simple and elegant Bayesian reasoning: specify a model relating\nthe known observations to the unknown quantity, then compute the posterior\ndistribution over the entropy given the observations.</p>\n<p>The model is the following <a href=\"http://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution\">Dirichlet-Multinomial\nmodel</a>,\nassuming a given non-negative vector <span class=\"math\">\\(\\mathbb{\\alpha} \\in \\mathbb{R}^K_+\\)</span>,</p>\n<ol>\n<li><span class=\"math\">\\(\\mathbb{p} \\sim \\textrm{Dirichlet}(\\mathbb{\\alpha})\\)</span>,</li>\n<li><span class=\"math\">\\(x_i \\sim \\textrm{Categorical}(\\mathbb{p})\\)</span>, <span class=\"math\">\\(i=1,\\dots,n\\)</span>, iid.</li>\n</ol>\n<p>If we define, for each bin <span class=\"math\">\\(k \\in \\{1,2,\\dots,K\\}\\)</span> the count</p>\n<div class=\"math\">$$n_k = \\sum_{i=1}^n 1_{\\{x_i = k\\}},$$</div>\n<p>so that <span class=\"math\">\\((n_1,n_2,\\dots,n_K)\\)</span> is a histogram over <span class=\"math\">\\(K\\)</span> outcomes, which is\ndistributed according to a multinomial distribution.\nThen, due to <a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\"><em>conjugacy</em></a>, the\nposterior over the unknown distribution <span class=\"math\">\\(\\mathbb{p}\\)</span> is again a Dirichlet\ndistribution and given as</p>\n<div class=\"math\">$$P(\\mathbb{p} | x_1,\\dots,x_n) = \\textrm{Dirichlet}(\\alpha_1 + n_1, \\dots,\n    \\alpha_K + n_K).$$</div>\n<p>We can now attempt to compute the squared-error optimal point estimate of the\nentropy under this posterior.  One of the main contributions of Wolpert and\nWolf is to provide a family of results that enable moment computations of the\nShannon entropy under the Dirichlet distribution.</p>\n<p>In particular, with <span class=\"math\">\\(n = \\sum_{k=1}^K n_k\\)</span> and <span class=\"math\">\\(\\alpha = \\sum_{k=1}^K\n\\alpha_k\\)</span>, they provide the posterior mean of the entropy as</p>\n<div class=\"math\">$$\\hat{H}_{\\textrm{Bayes}} = \\mathbb{E}[H(\\mathbb{p}) | n_1,\\dots,n_K] =\n    \\psi(n + \\alpha + 1)\n    - \\sum_{k=1}^K \\frac{n_k+\\alpha_k}{n+\\alpha} \\psi(n_k + \\alpha_k + 1),$$</div>\n<p>where <span class=\"math\">\\(\\psi\\)</span> is the <a href=\"http://en.wikipedia.org/wiki/Digamma_function\">digamma\nfunction</a>.\nThis expression is efficient to compute, and similarly the second moment and\nhence the variance of <span class=\"math\">\\(H(p)\\)</span> under the posterior can be computed efficiently.</p>\n<p>The only open question is how to select the prior vector of <span class=\"math\">\\(\\mathbb{\\alpha}\\)</span>.\nIn absence of further information about the distribution we can assume\nsymmetry.\nThen there are four common options,</p>\n<ol>\n<li><span class=\"math\">\\(\\alpha_k = 1\\)</span>, due to Bayes in 1763 and Laplace in 1812.</li>\n<li><span class=\"math\">\\(\\alpha_k = 1/K\\)</span>, due to\n<a href=\"http://www.actuaries.org.uk/system/files/documents/pdf/0285-0334.pdf\">Perks in 1947</a>.</li>\n<li><span class=\"math\">\\(\\alpha_k = 1/2\\)</span>, due to Jeffreys in 1946 and 1961.</li>\n<li><span class=\"math\">\\(\\alpha_k = 0\\)</span>, due to Haldane in 1948.  This yields an <a href=\"http://en.wikipedia.org/wiki/Prior_probability#Improper_priors\">improper\nprior</a>.</li>\n</ol>\n<p>It may not be clear which choice is the best, but I found an interesting\ndiscussion in a paper by <a href=\"http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewFile/3292/3802\">de Campos and\nBenavoli</a>.\nFurther down in this article we will be better equiped to assess the above\nchoices.</p>\n<p>Independent of the choice of the prior parameter Wolpert and Wolf are very\noptimistic about their model and highlight the advantages that come from the\nBayesian approach:</p>\n<blockquote>\n<p>\"One of the strength of Bayesian analysis is its power for dealing with such\nsmall-data cases.  In particular, not only are Bayesian estimators in many\nrespects more 'reasonable' than non-Bayesian estimators for small data, they\nalso naturally provide error bars to govern one's use of their results.\n...\nIn addition, the Bayesian formalism automatically tells you when it is unsure\nof its estimate, through its error bars.\"</p>\n</blockquote>\n<p>Also, on the empirical performance they comment,</p>\n<blockquote>\n<p>\"... for all N the Bayes estimator has a smaller mean-squared error than the\nfrequency-counts estimator.\"</p>\n</blockquote>\n<p>And indeed, also asymptotically the prior has support for every possible\ndistribution, so consistency of the estimated entropy is guaranteed as\n<span class=\"math\">\\(n\\to\\infty\\)</span>.</p>\n<p>All good then?</p>\n<p>Here is the comparison of the squared error and bias of various Bayes\nestimators with different choices of prior <span class=\"math\">\\(\\alpha\\)</span>.  The plot shows, like in\nthe previous article, the performance when evaluated on data generated from a\n<em>different</em> Dirichlet prior.  Each value on the x-axis is a different\ngenerating distribution, but the prior of the estimator remains fixed.</p>\n<p><img alt=\"RMSE and bias experiments for different alpha\nhyperparameters\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-exp.svg\"></p>\n<p>While all of the Bayes estimators perform better than the plugin estimator,\noverall they all fare quite badly:\nthere is a low error and bias only at the matching <span class=\"math\">\\(\\alpha\\)</span> value, but they\ndeteriorate quickly at different values of <span class=\"math\">\\(\\alpha\\)</span>.</p>\n<p>How can this be the case?</p>\n<h2>Nemenman-Shafee-Bialek</h2>\n<p>In 2002 <a href=\"http://arxiv.org/abs/physics/0108025\">Nemenman, Shafee, and Bialek</a>\nrecognized that the innocent looking Dirichlet-Multinomial model implies a\nvery concentrated prior belief over the entropy of the distribution:</p>\n<blockquote>\n<p>\"Thus a seemingly innocent choice of the prior ... leads to a disaster:\nfixing <span class=\"math\">\\(\\alpha\\)</span> specifies the entropy almost uniquely. Furthermore, the\nsituation persists even after we observe some data: until the distribution\nis well sampled, our estimate of the entropy is dominated by the prior!\"</p>\n</blockquote>\n<h3>The Implied Beliefs over the Entropy</h3>\n<p>The following experiment visualizes this: each of the following histograms\nshows the <em>implied prior</em> over <span class=\"math\">\\(H(\\mathbb{p})\\)</span>.\nTo create each histogram, I fixed <span class=\"math\">\\(K\\)</span> and <span class=\"math\">\\(\\alpha\\)</span> and take 1,000,000 samples\nof distributions <span class=\"math\">\\(\\mathbb{p}\\)</span>, then record its entropy.\nIn each histogram plot the x-axis covers exactly the full range over\npossible entropies.</p>\n<p><img alt=\"Induced prior on the entropy, K=2\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-prior-K2.svg\"></p>\n<p>For the case <span class=\"math\">\\(K=2\\)</span> everything looks fine: the implied prior spreads well over\nthe entire range of possible entropies.\nBut look what happens for <span class=\"math\">\\(K=10\\)</span> and <span class=\"math\">\\(K=100\\)</span>:</p>\n<p><img alt=\"Induced prior on the entropy, K=10\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-prior-K10.svg\">\n<img alt=\"Induced prior on the entropy, K=100\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-prior-K100.svg\"></p>\n<p>Here, the implied prior clearly concentrates sharply.  (The least possible\nconcentration of the entropy can be achieved using Perks choice of <span class=\"math\">\\(\\alpha =\n1/K\\)</span>.)\nIn fact, there is no choice of <span class=\"math\">\\(\\alpha\\)</span> for which the prior belief over the\nvery quantity to be estimated does not concentrate as <span class=\"math\">\\(K \\to \\infty\\)</span>.\nIf we have no reason to believe that the entropy really is in the range where\nthe prior dictates it should be, then this is a bad prior.</p>\n<p>How did Nemenman, Shafee, and Bialek solve this problem?</p>\n<h2>NSB estimator</h2>\n<p>They construct a mixture-of-Dirichlet prior by defining a hyperprior on\n<span class=\"math\">\\(\\alpha\\)</span> itself.  The hyperprior <span class=\"math\">\\(P(\\alpha)\\)</span> is chosen such that</p>\n<div class=\"math\">$$P(\\alpha) \\propto \\frac{\\textrm{d} \\mathbb{E}[H|\\alpha]}{\\textrm{d} \\alpha}.$$</div>\n<p>Let us take a look at how this can be derived.\nNemenman and collaborators first show that under the Dirichlet-Multinomial\nmodel the expected entropy is a strictly monotonic continuous function in\n<span class=\"math\">\\(\\alpha\\)</span>, and therefore it is invertible.\nLet us define the shorthand <span class=\"math\">\\(g^{-1}(\\alpha) := \\mathbb{E}[H|\\alpha]\\)</span> as the function\nthat takes <span class=\"math\">\\(\\alpha\\)</span> to the expected entropy.\nNow, by the <a href=\"http://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables\">transformation formula for random\nvairables</a>,\nwe have the induced density</p>\n<div class=\"math\">$$P_{\\alpha}(\\alpha) = P_H(g^{-1}(\\alpha))\n    \\cdot \\left|\\frac{\\textrm{d} g^{-1}(\\alpha)}{\\textrm{d} \\alpha}\\right|.$$</div>\n<p>If we assume that <span class=\"math\">\\(P(H|\\alpha)\\)</span> is highly concentrated (at least for large <span class=\"math\">\\(K\\)</span>\nin the above plots, this holds), then\n<span class=\"math\">\\(P_H(g^{-1}(\\alpha)) \\approx P(H|\\alpha)\\)</span>, and we want this density to be\nconstant.  Hence, we have</p>\n<div class=\"math\">$$P_{\\alpha}(\\alpha) \\propto\n    \\left|\\frac{\\textrm{d} g^{-1}(\\alpha)}{\\textrm{d} \\alpha}\\right|.$$</div>\n<p>Because the right hand side is positive, with <span class=\"math\">\\(g^{-1}(\\alpha) =\n\\mathbb{E}[H|\\alpha]\\)</span> this yields exactly the original expression above.\nThis expression has an analytic solution which, properly normalized is</p>\n<div class=\"math\">$$P(\\alpha) = \\frac{1}{\\log K} \\left(K \\psi_1(K \\alpha + 1)\n    - \\psi_1(\\alpha+1)\\right),$$</div>\n<p>where <span class=\"math\">\\(\\psi_1\\)</span> is the <a href=\"http://en.wikipedia.org/wiki/Trigamma_function\">trigamma\nfunction</a>.</p>\n<p><img alt=\"NSB Dirichlet-mixture prior for entropy estimation, K=2,10, and\n100\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-nsb.svg\"></p>\n<p>Let us look at the <em>implied prior</em> of the entropy when using the NSB prior.\nThey are much more uniform now:</p>\n<p><img alt=\"Implied entropy beliefs when using the NSB Dirichlet-mixture prior, K=2,10, and\n100\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-nsb-entropy-hist.svg\"></p>\n<p>This uniformity results in the NSB estimator having excellent robustness\nproperties and small bias.  It is probably the best general purpose discrete\nentropy estimator available.\nOne drawback however is the increased computational cost: in order to compute\nthe estimator we need to solve a 1D integral numerically over <span class=\"math\">\\(\\alpha\\)</span>.\nEach pointwise evaluation of the integrand function corresponds to computing\n<span class=\"math\">\\(\\hat{H}_{\\textrm{Bayes}}\\)</span> for a fixed value of <span class=\"math\">\\(\\alpha\\)</span>.\nHigh accuracy requires several hundred such evaluations, and this may be\nprohibitively expensive in some applications (for example, decision tree\ninduction). </p>\n<h3>Addendum: Undersampled Regime</h3>\n<p>After a comment from Ilya Nemenman on the previous version of this article, I\nalso did an experiment in the undersampled regime (<span class=\"math\">\\(N &lt; K\\)</span>), where we observe\nfewer outcomes than there are bins.  I am glad I did perform this experiment!</p>\n<p>I select <span class=\"math\">\\(N=100\\)</span> and <span class=\"math\">\\(K=2000\\)</span>, with <span class=\"math\">\\(500\\)</span> replicates and compare the same\nmethods as in the second part of the article.  The results are as follows.</p>\n<p><img alt=\"RMSE and bias experiments for undersampled\nregime\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-undersampled-exp.svg\"></p>\n<p>Almost all estimators perform very poorly in this setting, with the naive\nMiller correction even being off the chart.\nOnly the NSB and the Hausser-Strimmer estimator can be considered usable in\nthis severely undersampled regime, with clear preference towards the NSB\nestimator.</p>\n<p><a href=\"http://www.physics.emory.edu/home/people/faculty/nemenman-ilya.html\">Ilya\nNemenman</a>,\nthe inventor of the NSB estimator, was kind enough to share his feedback on\nthese experiments with me and to allow me to post them here:</p>\n<blockquote>\n<p>I am glad to hear that NSB estimator did well on this test. It's also not\nsurprising that HS estimator did rather well too -- in some sense, it's a\nfrequentist version of NSB. Both NSB and HS perform shrinking towards the\nuniform distribution (infinite pseudocounts or \"alpha\" in your notation),\nand then they lift the shrinkage as <span class=\"math\">\\(N\\)</span> grows. However, HS shrinks much\nstronger than NSB does. As a result, HS performs very well for large entropy\n(large alpha) distributions, and worse for lower entropies. It's probably\npossible to set up a frequentist shrinkage estimator that would shrink\ntowards entropy being half of the maximum value, or shrink towards the\nmaximum value, but less strongly than HS â€” I think that such an estimator\nwould do better over the whole range of alpha. In practice, the strong\nshrinkage imposed by HS becomes problematic when the alphabet size is very\nlarge, say <span class=\"math\">\\(2^{150}\\)</span>, which is what one gets when one takes a 30ms long\nspike train and discretizes it at 0.2 ms resolution (yes spike = 1, no spike\n=0). We had numbers like this in our 2008 PLoS Comp Bio paper. With entropy\nof <span class=\"math\">\\(\\approx 15\\)</span> bits, alphabet size of <span class=\"math\">\\(2^{150}\\)</span>, and 100-1000 samples, NSB\nmay work (more on this below), and HS will shrink towards 150 bits, and will\nlikely overestimate. One way to see this problem is to realize that, in your\ncomparison plots, once you use <span class=\"math\">\\(\\alpha &gt; 1\\)</span>, the entropy\nis nearly the maximum possible entropy. This is why HS works well there, but\nfails for <span class=\"math\">\\(\\alpha \\ll 1\\)</span>, where the entropy is substantially smaller than the\nmaximum. If you were to replot the data putting the true entropy of the\nanalyzed probability distribution (rather than alpha) on the horizontal\naxis, this will be visible, I think.</p>\n</blockquote>\n<p>He continues,</p>\n<blockquote>\n<p>A key point for both NSB and HS is that both may work in the regime of\n<span class=\"math\">\\(N \\sim \\sqrt{K}\\)</span> (better yet, <span class=\"math\">\\(\\sim \\sqrt{2^{H/2}}\\)</span>). On the contrary,\nmost other estimators you analyzed work well only up to <span class=\"math\">\\(N \\sim 2^H\\)</span> (unless\nI am missing something important). This is because NSB and HS require not\ngood sampling of the underlying distribution, but coincidences in the data\nonly. They estimate entropy, effectively, by inverting the usual birthday\nparadox, and using the frequency of coincidences to measure the diversity of\ndata. One can illustrate this by pushing <span class=\"math\">\\(K\\)</span> to even larger values in your\nlast plot, 10000 or even more, if you limit yourself to smaller alpha.</p>\n</blockquote>\n<p>These comments are very insightful and show that my earlier discussion and\nresults were, in a way, limited to the simple case where we have a reasonable\nnumber of samples per bin.  The case Ilya considers in his work is the\nseverely undersampled regime.</p>\n<p>One difficulty in producing the plot he suggests that plots the entropy of the\ndistribution along the x-axis is that it would require an additional binning\noperation along that axis, so I have not produced this plot yet.</p>\n<h3>Reference Prior, Anyone?</h3>\n<p>I wonder whether the NSB prior is a simplification of a full <a href=\"http://projecteuclid.org/euclid.aos/1236693154\">reference\nprior</a>\ntreatment.  This is not exactly the standard setting of reference priors\nbecause we are interested in a function (the entropy) of our random variables,\nso there is an additional indirection.  But I believe it could work as\nfollows: find in the space of all priors on <span class=\"math\">\\(\\alpha\\)</span> the prior that maximizes\nthe KL divergence between implied entropy prior and entropy posterior.</p>\n<p>Using the numerical method suggested in the paper above, I obtained a\nnumerical reference prior (with one additional ABC approximation for the\nlikelihood) for <span class=\"math\">\\(K=2\\)</span> and this closely matches the NSB prior.</p>\n<p><img alt=\"Numerically obtained reference prior for K=2\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-3-refprior.svg\"></p>\n<p>(Interestingly, I recently discovered this work\non <a href=\"http://projecteuclid.org/euclid.ba/1422556416\">overall objective priors</a>\nin which their <em>hierarchical reference prior</em> approach for the\nDirichlet-Multinomial model yields an analytic proper prior which is very\nsimilar to the NSB and numerical reference priors.)</p>\n<h3>Further Reading</h3>\n<p>As you hopefully have noticed, the problem of discrete entropy estimation is\nquite rich and still actively being worked on.\nCurrent work focuses on the case where the distribution is countably infinite.\nFor example, the probability distribution of English words in popular usage is\nan example: there are infinitely many possible words, but a total lexical\norder implies a countable discrete distribution.</p>\n<p>A great up-to-date overview of discrete entropy estimation, including a\nsummary of the current work on the countable infinite case is given in this\n<a href=\"https://memming.wordpress.com/2014/02/09/a-guide-to-discrete-entropy-estimators/\">article</a>\nby <a href=\"http://www.memming.com/\">Memming Park</a>.</p>\n<p>To a general introduction to the difficulties of the entropy estimation\nproblem, this\n<a href=\"http://stat.columbia.edu/~liam/research/pubs/info_est-nc.pdf\">2003 paper</a> by\n<a href=\"http://www.stat.columbia.edu/~liam/\">Liam Paninski</a> is still the best entry\npoint.\nAnother <a href=\"http://arxiv.org/abs/cond-mat/0403192\">very nice overview on entropy\nestimation</a> is due to Thomas Sch&uuml;rmann\nin 2004.\nTo me the best introduction to the family of Bayesian estimators is <a href=\"http://arxiv.org/abs/1302.0328\">(Archer,\nPark, Pillow, 2013)</a>.</p>\n<p>If you wonder why I care about entropy estimation, then, <a href=\"http://www.nowozin.net/sebastian/papers/nowozin2012infogain.pdf\">my ICML 2012\npaper</a> was\nthe application that originally led me to consider the problem.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://www.memming.com/\">Il Memming Park</a>,\n<a href=\"http://ei.is.tuebingen.mpg.de/person/jpeters\">Jonas Peters</a>, and\n<a href=\"http://www.physics.emory.edu/home/people/faculty/nemenman-ilya.html\">Ilya Nemenman</a>\nfor reading a draft version of the article and providing very helpful feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}