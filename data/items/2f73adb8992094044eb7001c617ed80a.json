{
  "id": "tag:blogger.com,1999:blog-15418143.post-6794382104242630346",
  "published": "2015-04-08T14:05:00.001-05:00",
  "updated": "2018-04-17T03:42:50.037-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "Deep Learning vs Probabilistic Graphical Models vs Logic",
  "content": "Today, let's take a look at three paradigms<b>&nbsp;</b>that have shaped the field of Artificial Intelligence in the last 50 years: <b>Logic</b>, <b>Probabilistic Methods</b>, and <b>Deep Learning</b>. The empirical, \"data-driven\", or big-data / deep-learning ideology triumphs today, but that wasn't always the case. Some of the earliest approaches to AI were based on Logic, and the transition from logic to data-driven methods has been heavily influenced by probabilistic thinking, something we will be investigating in this blog post.<br /><br />Let's take a look back Logic and Probabilistic Graphical Models and make some predictions on where the field of AI and Machine Learning is likely to go in the near future. We will proceed in chronological order.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-b_Ux2LXhPyk/VSVdqYcp6-I/AAAAAAAAN8M/eBJ2ln-6nDU/s1600/probgraphmods.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"225\" src=\"https://3.bp.blogspot.com/-b_Ux2LXhPyk/VSVdqYcp6-I/AAAAAAAAN8M/eBJ2ln-6nDU/s1600/probgraphmods.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Image from Coursera's <a href=\"http://online.stanford.edu/pgm-fa12\">Probabilistic Graphical Models</a> course</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><h3><b>1. Logic and Algorithms (Common-sense \"Thinking\" Machines)</b></h3><br />A lot of early work on Artificial Intelligence was concerned with Logic, Automated Theorem Proving, and manipulating symbols. It should not be a surprise that John McCarthy's seminal 1959 paper on AI had the title \"Programs with common sense.\"<br /><br />If we peek inside one of most popular AI textbooks, namely \"Artificial Intelligence: A Modern Approach,\" we immediately notice that the beginning of the book is devoted to <b>search, constraint satisfaction problems, first-order logic, and planning</b>. The third edition's cover (pictured below) looks like a big chess board (because being good at chess used to be a sign of human intelligence), features a picture of Alan Turing (the father of computing theory) as well as a picture of Aristotle (one of the greatest classical philosophers which had quite a lot to say about intelligence).<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-BCOXrp3r-SI/VSVYsgEm-eI/AAAAAAAAN74/J2Lx0ssKvsA/s1600/cover2.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"320\" src=\"https://1.bp.blogspot.com/-BCOXrp3r-SI/VSVYsgEm-eI/AAAAAAAAN74/J2Lx0ssKvsA/s1600/cover2.jpg\" width=\"246\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">The cover of&nbsp;<a href=\"http://aima.cs.berkeley.edu/\">AIMA</a>, the canonical AI text for undergraduate CS students</div><br />Unfortunately, logic-based AI brushes the perception problem under the rug, and I've argued quite some time ago that understanding how perception works is really the key to unlocking the secrets of intelligence. Perception is one of those things which is easy for humans and immensely difficult for machines. (To read more see my 2011 blog post, <a href=\"http://www.computervisionblog.com/2011/03/computer-vision-is-artificial.html\">Computer Vision is Artificial Intelligence</a>). Logic is pure and traditional chess-playing bots are very algorithmic and search-y, but the real world is ugly, dirty, and ridden with uncertainty.<br /><br /><b>I think most contemporary AI researchers agree that Logic-based AI is dead.</b>&nbsp;The kind of world where everything can be perfectly observed, a world with no measurement error, is not the world of robotics and big-data. &nbsp;We live in the era of machine learning, and numerical techniques triumph over first-order logic. &nbsp;As of 2015, I pity the fool who prefers Modus Ponens over Gradient Descent.<br /><br />Logic is great for the classroom and I suspect that once enough perception problems become \"essentially solved\" that we will see a resurgence in Logic. &nbsp;And while there will be plenty of open perception problems in the future, there will be scenarios where the community can stop worrying about perception and start revisiting these classical ideas. Perhaps in 2020.<br /><br /><b>Further reading:</b>&nbsp;<a href=\"http://plato.stanford.edu/entries/logic-ai/\">Logic and Artificial Intelligence from the Stanford Encyclopedia of Philosophy</a><br /><br /><h3><b>2. Probability, Statistics, and Graphical Models (\"Measuring\" Machines)</b></h3><br />Probabilistic methods in Artificial Intelligence came out of the need to deal with uncertainty. The middle part of the Artificial Intelligence a Modern Approach textbook is called \"Uncertain Knowledge and Reasoning\" and is a great introduction to these methods. &nbsp;If you're picking up AIMA for the first time, I recommend you start with this section. And if you're a student starting out with AI, do yourself a favor and don't skimp on the math.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-TKWy6zvuPo4/VSVcc67N3YI/AAAAAAAAN8E/eSdTsH46L1A/s1600/HamburgerDensity4.gif\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"226\" src=\"https://3.bp.blogspot.com/-TKWy6zvuPo4/VSVcc67N3YI/AAAAAAAAN8E/eSdTsH46L1A/s1600/HamburgerDensity4.gif\" width=\"320\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://onlinecourses.science.psu.edu/stat414/node/97\">Intro to PDFs</a> from Penn State's&nbsp;Probability Theory and Mathematical Statistics course</div><br />When most people think about probabilistic methods they think of counting. &nbsp;In laymen's terms, it's fair to think of probabilistic methods as fancy counting methods. &nbsp;Let's briefly take a look at what used to be the two competing methods for thinking probabilistically.<br /><br /><b>Frequentist methods</b> are very empirical -- these methods are data-driven and make inferences purely from data. &nbsp;<b>Bayesian methods</b> are more sophisticated and combine data-driven likelihoods with magical priors. &nbsp;These priors often come from first principles or \"intuitions\" and the Bayesian approach is great for combining heuristics with data to make cleverer algorithms -- a nice mix of the rationalist and empiricist world views.<br /><br />What is perhaps more exciting that then Frequentist vs. Bayesian flamewar is something known as Probabilistic Graphical Models. &nbsp;This class of techniques comes from computer science, and even though Machine Learning is now a strong component of a CS and a Statistics degree, the true power of statistics only comes when it is married with computation.<br /><br /><b>Probabilistic Graphical Models are a marriage of Graph Theory with Probabilistic Methods</b> and they were all the rage among Machine Learning researchers in the mid-2000s. Variational methods, Gibbs Sampling, and Belief Propagation were being pounded into the brains of CMU graduate students when I was in graduate school (2005-2011) and provided us with a superb mental framework for thinking about machine learning problems. I learned most of what I know about Graphical Models from <a href=\"http://homes.cs.washington.edu/~guestrin/\">Carlos Guestrin</a> and <a href=\"http://jonathan-huang.org/\">Jonathan Huang</a>.&nbsp;Carlos Guestrin is now the CEO of GraphLab, Inc (now known as <a href=\"https://dato.com/\">Dato</a>) which is a company that builds large-scale products for machine learning on graphs and Jonathan Huang is a senior research scientist at Google.<br /><br />The video below is a high-level overview of GraphLab, but it serves a very nice overview of \"graphical thinking\" and how it fits into the modern data scientist's tool-belt. Carlos is an excellent lecturer and his presentation is less about the company's product and more about ways of thinking about next-generation machine learning systems.<br /><br /><center> <iframe allow=\"autoplay; encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"252\" src=\"https://www.youtube.com/embed/VliM9-tB2VE\" width=\"448\"></iframe> </center><center> A Computational Introduction to Probabilistic Graphical Models</center><center> by GraphLab, Inc CEO Prof. Carlos Guestrin (Video Link updated 4/17/2018)</center><br />If you think that deep learning is going to solve all of your machine learning problems, you should really take a look at the above video. &nbsp;If you're building recommender systems, an analytics platform for healthcare data, designing a new trading algorithm, or building the next generation search engine, Graphical Models are the perfect place to start.<br /><br /><b>Further reading:</b><br /><a href=\"http://en.wikipedia.org/wiki/Belief_propagation\">Belief Propagation Algorithm Wikipedia Page</a><br /><a href=\"http://www.cs.berkeley.edu/~jordan/papers/variational-intro.pdf\">An Introduction to Variational Methods for Graphical Models</a> by Michael Jordan et al.<br /><a href=\"http://www.cs.berkeley.edu/~jordan/\">Michael Jordan's webpage</a> (one of the titans of inference and graphical models)<br /><br /><h3><b>3. Deep Learning and Machine Learning (Data-Driven Machines)</b></h3>Machine Learning is about learning from examples and today's state-of-the-art recognition techniques require a lot of training data, a deep neural network, and patience. <b>Deep Learning emphasizes the network architecture of today's most successful machine learning approaches.</b> &nbsp;These methods are based on \"deep\" multi-layer neural networks with many hidden layers. NOTE: I'd like to emphasize that using deep architectures (as of 2015) is not new. &nbsp;Just check out the following \"deep\" architecture from 1998.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-eAFL2rN9cm0/VRjBiJiecrI/AAAAAAAAN4E/2Q7LthaoLEY/s1600/lenet.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"155\" src=\"https://4.bp.blogspot.com/-eAFL2rN9cm0/VRjBiJiecrI/AAAAAAAAN4E/2Q7LthaoLEY/s400/lenet.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">LeNet-5 Figure From Yann LeCun's seminal \"<span style=\"background-color: white;\">Gradient-based learning</span></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><span style=\"background-color: white;\">applied to document recognition\"</span>&nbsp;paper.</div><br />When you take a look at <a href=\"http://deeplearning.net/tutorial/lenet.html\">modern guide about LeNet</a>, it comes with the following disclaimer:<br /><br />\"To run this example on a GPU, you need a good GPU. It needs at least 1GB of GPU RAM. More may be required if your monitor is connected to the GPU.<br /><br />When the GPU is connected to the monitor, there is a limit of a few seconds for each GPU function call. This is needed as current GPUs can’t be used for the monitor while performing computations. Without this limit, the screen would freeze for too long and make it look as if the computer froze. This example hits this limit with medium-quality GPUs. When the GPU isn’t connected to a monitor, there is no time limit. You can lower the batch size to fix the timeout problem.\"<br /><br />It really <b>makes me wonder how Yann was able to get <i>anything </i>out of his deep model back in 1998</b>. Perhaps it's not surprising that it took another decade for the rest of us to get the memo.<br /><br />UPDATE: Yann pointed out (via a Facebook comment) that the ConvNet work dates back to 1989. \"It had about 400K connections and took about 3 weeks to train on the USPS dataset (8000 training examples) on a SUN4 machine.\" -- LeCun<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-O60saG4CJs4/VSX5HjoXr4I/AAAAAAAAN8o/smmT02ecM_k/s1600/1989net.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"367\" src=\"https://4.bp.blogspot.com/-O60saG4CJs4/VSX5HjoXr4I/AAAAAAAAN8o/smmT02ecM_k/s400/1989net.jpg\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\">A Deep Network from Yann's work at Bell Labs from 1989</a></div><br /><br />NOTE: At roughly the same time (~1998) two crazy guys in California were trying to cache the entire internet inside the computers in their garage (they started some funny-sounding company which starts with a G). I don't know how they did it, but I guess sometimes to win big you have to <a href=\"http://paulgraham.com/ds.html\">do things that don't scale</a>. Eventually, the world will catch up.<br /><br /><b>Further reading:</b><br /><span style=\"background-color: white;\">Y.&nbsp;LeCun, L.&nbsp;Bottou, Y.&nbsp;Bengio, and P.&nbsp;Haffner. <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\">Gradient-based learning applied to document recognition</a>.&nbsp;</span><cite style=\"background-color: white;\">Proceedings of the IEEE</cite><span style=\"background-color: white;\">, November 1998.</span><br /><span style=\"background-color: white;\"><br /></span> <span style=\"background-color: white;\">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and L. D. Jackel: <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\">Backpropagation Applied to Handwritten Zip Code Recognition</a>, Neural Computation, 1(4):541-551, Winter 1989</span><br /><span style=\"background-color: white;\"><br /></span><span style=\"background-color: white;\"><b>Deep Learning code:</b> Modern <a href=\"http://deeplearning.net/tutorial/lenet.html\">LeNet implementation in Theano </a>and docs.</span><br /><span style=\"background-color: white;\"><br /></span><br /><h3><span style=\"background-color: white;\"><b>Conclusion</b></span></h3><span style=\"background-color: white;\">I don't see traditional first-order logic making a comeback anytime soon. And while there is a lot of hype behind deep learning, distributed systems and \"graphical thinking\" is likely to make a much more profound impact on data science than heavily optimized CNNs. There is no reason why deep learning can't be combined with a GraphLab-style architecture, and some of the new exciting machine learning work in the next decade is likely to be a marriage of these two philosophies.</span><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div>You can also check out a relevant post from last month:<br /><a href=\"http://www.computervisionblog.com/2015/03/deep-learning-vs-machine-learning-vs.html\">Deep Learning vs Machine Learning vs Pattern Recognition</a><br /><br /><a class=\"hn-link\" href=\"javascript:window.location=%22http://news.ycombinator.com/submitlink?u=%22+encodeURIComponent(document.location)+%22&amp;t=%22+encodeURIComponent(document.title)\">Discuss on Hacker News</a>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 9
}