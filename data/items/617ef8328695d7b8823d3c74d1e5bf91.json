{
  "title": "Autoregressive Models in Deep Learning â€” A Brief Survey",
  "link": "",
  "id": "https://www.georgeho.org/deep-autoregressive-models/",
  "updated": "2019-03-09T00:00:00Z",
  "published": "2019-03-09T00:00:00Z",
  "content": "<p>My current project involves working with deep autoregressive models: a class of\nremarkable neural networks that aren&rsquo;t usually seen on a first pass through deep\nlearning. These notes are a quick write-up of my reading and research: I assume\nbasic familiarity with deep learning, and aim to highlight general trends and\nsimilarities across autoregressive models, instead of commenting on individual\narchitectures.</p>\n<p><strong>tldr:</strong> <em>Deep autoregressive models are sequence models, yet feed-forward\n(i.e. not recurrent); generative models, yet supervised. They are a compelling\nalternative to RNNs for sequential data, and GANs for generation tasks.</em></p>\n<h2 id=\"deep-autoregressive-models\">Deep Autoregressive Models</h2>\n<p>To be explicit (at the expense of redundancy), this blog post is about <em>deep\nautoregressive generative sequence models</em>. That&rsquo;s quite a mouthful of jargon\n(and two of those words are actually unnecessary), so let&rsquo;s unpack that.</p>\n<ol>\n<li>\n<p>Deep</p>\n<ul>\n<li>Well, these papers are using TensorFlow or PyTorch&hellip; so they must be\n&ldquo;deep&rdquo; ðŸ˜‰</li>\n<li>You would think this word is unnecessary, but it&rsquo;s actually not!\nAutoregressive linear models like\n<a href=\"https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model\">ARMA</a>\nor\n<a href=\"https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity\">ARCH</a>\nhave been used in statistics, econometrics and financial modelling for\nages.</li>\n</ul>\n</li>\n<li>\n<p>Autoregressive</p>\n<ul>\n<li>\n<p><a href=\"https://deepgenerativemodels.github.io/notes/autoregressive/\">Stanford has a good\nintroduction</a>\nto autoregressive models, but I think a good way to explain these models is\nto compare them to recurrent neural networks (RNNs), which are far more\nwell-known.</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/rnn-unrolled.png\"><img src=\"https://www.georgeho.org/assets/images/rnn-unrolled.png\" alt=\"Recurrent neural network (RNN) block diagram, both rolled and unrolled\"></a>\n<figcaption>Obligatory RNN diagram. Source: <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Chris Olah</a>.</figcaption>\n</figure>\n<ul>\n<li>\n<p>Like an RNN, an autoregressive model&rsquo;s output $h_t$ at time $t$\ndepends on not just $x_t$, but also $x$&rsquo;s from previous time steps.\nHowever, <em>unlike</em> an RNN, the previous $x$&rsquo;s are not provided via some\nhidden state: they are given as just another input to the model.</p>\n</li>\n<li>\n<p>The following animation of Google DeepMind&rsquo;s WaveNet illustrates this\nwell: the $t$th output is generated in a <em>feed-forward</em> fashion from\nseveral input $x$ values.<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup></p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/wavenet-animation.gif\"><img src=\"https://www.georgeho.org/assets/images/wavenet-animation.gif\" alt=\"WaveNet animation\"></a>\n<figcaption>WaveNet animation. Source: <a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\">Google DeepMind</a>.</figcaption>\n</figure>\n</li>\n<li>\n<p>Put simply, <strong>an autoregressive model is merely a feed-forward model which\npredicts future values from past values.</strong></p>\n</li>\n<li>\n<p>I&rsquo;ll explain this more later, but it&rsquo;s worth saying now: autoregressive\nmodels offer a compelling bargain. You can have stable, parallel and\neasy-to-optimize training, faster inference computations, and completely\ndo away with the fickleness of <a href=\"https://en.wikipedia.org/wiki/Backpropagation_through_time\">truncated backpropagation through\ntime</a>, if you\nare willing to accept a model that (by design) <em>cannot have</em> infinite\nmemory. There is <a href=\"http://www.offconvex.org/2018/07/27/approximating-recurrent/\">recent\nresearch</a> to\nsuggest that this is a worthwhile tradeoff.</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Generative</p>\n<ul>\n<li>Informally, a generative model is one that can generate new data after\nlearning from the dataset.</li>\n<li>More formally, a generative model models the joint distribution $P(X, Y)$\nof the observation $X$ and the target $Y$. Contrast this to a\ndiscriminative model that models the conditional distribution $P(Y|X)$.</li>\n<li>GANs and VAEs are two families of popular generative models.</li>\n<li>This is unnecessary word #1: any autoregressive model can be run\nsequentially to generate a new sequence! Start with your seed $x_1, x_2,\n&hellip;, x_k$ and predict $x_{k+1}$. Then use $x_2, x_3, &hellip;, x_{k+1}$ to\npredict $x_{k+2}$, and so on.</li>\n</ul>\n</li>\n<li>\n<p>Sequence model</p>\n<ul>\n<li>Fairly self explanatory: a model that deals with sequential data, whether\nit is mapping sequences to scalars (e.g. language models), or mapping\nsequences to sequences (e.g. machine translation models).</li>\n<li>Although sequence models are designed for sequential data (duh), there has\nbeen success at applying them to non-sequential data. For example,\nPixelCNN (discussed below) can generate entire images, even though images\nare not sequential in nature: the model generates a pixel at a time, in\nsequence!<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup></li>\n<li>Notice that an autoregressive model must be a sequence model, so it&rsquo;s\nredundant to further describe these models as sequential (which makes this\nunnecessary word #2).</li>\n</ul>\n</li>\n</ol>\n<p>A good distinction is that &ldquo;generative&rdquo; and &ldquo;sequential&rdquo; describe <em>what</em> these\nmodels do, or what kind of data they deal with. &ldquo;Autoregressive&rdquo; describes <em>how</em>\nthese models do what they do: i.e. they describe properties of the network or\nits architecture.</p>\n<h2 id=\"some-architectures-and-applications\">Some Architectures and Applications</h2>\n<p>Deep autoregressive models have seen a good degree of success: below is a list\nof some of examples. Each architecture merits exposition and discussion, but\nunfortunately there isn&rsquo;t enough space here to devote to do any of them justice.</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1601.06759\">PixelCNN by Google DeepMind</a> was probably\nthe first deep autoregressive model, and the progenitor of most of the other\nmodels below. Ironically, the authors spend the bulk of the paper discussing a\nrecurrent model, PixelRNN, and consider PixelCNN as a &ldquo;workaround&rdquo; to avoid\nexcessive computation. However, PixelCNN is probably this paper&rsquo;s most lasting\ncontribution.</li>\n<li><a href=\"https://arxiv.org/abs/1701.05517\">PixelCNN++ by OpenAI</a> is, unsurprisingly,\nPixelCNN but with various improvements.</li>\n<li><a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\">WaveNet by Google\nDeepMind</a> is\nheavily inspired by PixelCNN, and models raw audio, not just encoded music.\nThey had to pull <a href=\"https://en.wikipedia.org/wiki/%CE%9C-law_algorithm\">a neat trick from telecommunications/signals\nprocessing</a> in order to\ncope with the sheer size of audio (high-quality audio involves at least 16-bit\nprecision samples, which means a 65,536-way-softmax per time step!)</li>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Transformer, a.k.a. <em>the &ldquo;attention is all you need&rdquo; model</em> by Google\nBrain</a> is now a mainstay of NLP, performing\nvery well at many NLP tasks and being incorporated into subsequent models like\n<a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">BERT</a>.</li>\n</ul>\n<p>These models have also found applications: for example, <a href=\"https://arxiv.org/abs/1610.10099\">Google DeepMind&rsquo;s\nByteNet can perform neural machine translation (in linear\ntime!)</a> and <a href=\"https://arxiv.org/abs/1610.00527\">Google DeepMind&rsquo;s Video Pixel\nNetwork can model video</a>.<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup></p>\n<h2 id=\"some-thoughts-and-observations\">Some Thoughts and Observations</h2>\n<ol>\n<li>\n<p>Given previous values $x_1, x_2, &hellip;, x_t$, these models do not output a\n<em>value</em> for $x_{t+1}$, they output the <em>predictive probability\ndistribution</em> $P(x_{t+1} | x_1, x_2, &hellip;, x_t)$ for $x_{t+1}$.</p>\n<ul>\n<li>If the $x$&rsquo;s are discrete, then you can do this by outputting an $N$-way\nsoftmaxxed tensor, where $N$ is the number of discrete classes. This is\nwhat the original PixelCNN did, but gets problematic when $N$ is large\n(e.g. in the case of WaveNet, where $N = 2^{16}$).</li>\n<li>If the $x$&rsquo;s are continuous, you can model the probability distribution\nitself as the sum of basis functions, and having the model output the\nparameters of these basis functions. This massively reduces the memory\nfootprint of the model, and was an important contribution of PixelCNN++.</li>\n<li>Theoretically you could have an autoregressive model that <em>doesn&rsquo;t</em> model\nthe conditional distribution&hellip; but most recent models do.</li>\n</ul>\n</li>\n<li>\n<p>Autoregressive models are supervised.</p>\n<ul>\n<li>With the success and hype of GANs and VAEs, it is easy to assume that all\ngenerative models are unsupervised: this is not true!</li>\n<li>This means that that training is stable and highly parallelizable, that it\nis straightfoward to tune hyperparameters, and that inference is\ncomputationally inexpensive. We can also break out all the good stuff from\nML-101: train-valid-test splits, cross validation, loss metrics, etc. These\nare all things that we lose when we resort to e.g. GANs.</li>\n</ul>\n</li>\n<li>\n<p>Autoregressive models work on both continuous and discrete data.</p>\n<ul>\n<li>Autoregressive sequential models have worked for audio (WaveNet), images\n(PixelCNN++) and text (Transformer): these models are very flexible in the\nkind of data that they can model.</li>\n<li>Contrast this to GANs, which (as far as I&rsquo;m aware) cannot model discrete\ndata.</li>\n</ul>\n</li>\n<li>\n<p>Autoregressive models are very amenable to conditioning.</p>\n<ul>\n<li>There are many options for conditioning! You can condition on both discrete\nand continuous variables; you can condition at multiple time scales; you can\neven condition on latent embeddings or the outputs of other neural networks.</li>\n<li>There is one ostensible problem with using autoregressive models as\ngenerative models: you can only condition on your data&rsquo;s labels. I.e.\nunlike a GAN, you cannot condition on random noise and expect the model to\nshape the noise space into a semantically (stylistically) meaningful latent\nspace.</li>\n<li>Google DeepMind followed up their original PixelRNN paper with <a href=\"https://arxiv.org/abs/1606.05328\">another\npaper</a> that describes one way to overcome\nthis problem. Briefly: to condition, they incorporate the latent vector into\nthe PixelCNN&rsquo;s activation functions; to produce/learn the latent vectors,\nthey use a convolutional encoder; and to generate an image given a latent\nvector, they replace the traditional deconvolutional decoder with a\nconditional PixelCNN.</li>\n<li>WaveNet goes even futher and employs &ldquo;global&rdquo; and &ldquo;local&rdquo; conditioning (both\nare achieved by incorporating the latent vectors into WaveNet&rsquo;s activation\nfunctions). The authors devise a battery of conditioning schemes to capture\nspeaker identity, linguistic features of input text, music genre, musical\ninstrument, etc.</li>\n</ul>\n</li>\n<li>\n<p>Generating output sequences of variable length is not straightforward.</p>\n<ul>\n<li>Neither WaveNet nor PixelCNN needed to worry about a variable output length:\nboth audio and images are comprised of a fixed number of outputs (i.e. audio\nis just $N$ samples, and images are just $N^2$ pixels).</li>\n<li>Text, on the other hand, is different: sentences can be of variable length.\nOne would think that this is a nail in a coffin, but thankfully text is\ndiscrete: the standard trick is to have a &ldquo;stop token&rdquo; that indicates that\nthe sentence is finished (i.e. model a full stop as its own token).</li>\n<li>As far as I am aware, there is no prior literature on having both problems:\na variable-length output of continuous values.</li>\n</ul>\n</li>\n<li>\n<p>Autoregressive models can model multiple time scales</p>\n<ul>\n<li>\n<p>In the case of music, there are important patterns to model at multiple\ntime scales: individual musical notes drive correlations between audio\nsamples at the millisecond scale, and music exhibits rhythmic patterns\nover the course of minutes. This is well illustrated by the following\nanimation:</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/audio-animation.gif\"><img src=\"https://www.georgeho.org/assets/images/audio-animation.gif\" alt=\"Audio at multiple time scales\"></a>\n<figcaption>Audio exhibits patterns at multiple time scales. Source: <a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\">Google DeepMind</a>.</figcaption>\n</figure>\n</li>\n<li>\n<p>There are two main ways model many patterns at many different time scales:\neither make the receptive field of your model <em>extremely</em> wide (e.g.\nthrough dilated convolutions), or condition your model on a subsampled\nversion of your generated output, which is in turn produced by an\nunconditioned model.</p>\n<ul>\n<li>Google DeepMind composes an unconditional PixelRNN with one or more\nconditional PixelRNNs to form a so-called &ldquo;multi-scale&rdquo; PixelRNN: the\nfirst PixelRNN generates a lower-resolution image that conditions the\nsubsequent PixelRNNs.</li>\n<li>WaveNet employs a different technique and calls them &ldquo;context stacks&rdquo;.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>How the hell can any of this stuff work?</p>\n<ul>\n<li>\n<p>RNNs are theoretically more expressive and powerful than autoregressive\nmodels. However, recent work suggests that such infinite-horizon memory is\nseldom achieved in practice.</p>\n</li>\n<li>\n<p>To quote <a href=\"http://www.offconvex.org/2018/07/27/approximating-recurrent/\">John Miller at the Berkeley AI Research\nlab</a>:</p>\n<blockquote>\n<p><strong>Recurrent models trained in practice are effectively feed-forward.</strong>\nThis could happen either because truncated backpropagation through time\ncannot learn patterns significantly longer than $k$ steps, or, more\nprovocatively, because models <em>trainable by gradient descent</em> cannot have\nlong-term memory.</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>There&rsquo;s actually a lot more nuance than meets the eye in this animation,\nbut all I&rsquo;m trying to illustrate is the feed-forward nature of autoregressive\nmodels.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>I personally think it&rsquo;s breathtakingly that machines can do this. Imagine\nyour phone keyboard&rsquo;s word suggestions (those are autoregressive!) spitting\nout an entire novel. Or imagine weaving a sweater but you had to choose the\ncolor of every stitch, in order, in advance.&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\">\n<p>In case you haven&rsquo;t noticed, Google DeepMind seemed to have had an\ninfatuation with autoregressive models back in 2016.&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}