{
  "title": "Estimating Discrete Entropy, Part 1",
  "link": "",
  "published": "2015-02-07T14:00:00+00:00",
  "updated": "2015-02-07T14:00:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-02-07:/sebastian/blog/estimating-discrete-entropy-part-1.html",
  "summary": "<p>Estimation of the\n<a href=\"http://en.wikipedia.org/wiki/Entropy_%28information_theory%29\">entropy</a> of a\nrandom variable is an important problem that has many applications.\nIf you can estimate entropy accurately, you can also estimate <a href=\"http://en.wikipedia.org/wiki/Mutual_information\">mutual\ninformation</a>, which allows\nyou to find dependent random variables in large data sets â€¦</p>",
  "content": "<p>Estimation of the\n<a href=\"http://en.wikipedia.org/wiki/Entropy_%28information_theory%29\">entropy</a> of a\nrandom variable is an important problem that has many applications.\nIf you can estimate entropy accurately, you can also estimate <a href=\"http://en.wikipedia.org/wiki/Mutual_information\">mutual\ninformation</a>, which allows\nyou to find dependent random variables in large data sets.\nThere are <a href=\"http://en.wikipedia.org/wiki/Mutual_information#Applications_of_mutual_information\">numerous\napplications</a>.</p>\n<p>The setting of discrete entropy estimation with a finite number of outcomes is\nas follows.\nThere is an unknown categorical distribution over <span class=\"math\">\\(K \\geq 2\\)</span> different\noutcomes, defined by means of a probability vector\n<span class=\"math\">\\(\\mathbb{p} = (p_1,p_2,\\dots,p_K)\\)</span>, such that <span class=\"math\">\\(p_k \\geq 0\\)</span> and\n<span class=\"math\">\\(\\sum_k p_k = 1\\)</span>.\nWe are interested in the quantity</p>\n<div class=\"math\">\\begin{equation}\nH(\\mathbb{p}) = -\\sum_{k=1}^K p_k \\log p_k,\\label{eqn:Hdiscrete}\n\\end{equation}</div>\n<p>where <span class=\"math\">\\(0 \\log 0 = 0\\)</span> by convention.</p>\n<p>Because the probability vector is unknown to us we cannot directly use\n<span class=\"math\">\\((\\ref{eqn:Hdiscrete})\\)</span>.\nInstead we assume that we observe <span class=\"math\">\\(n\\)</span> samples <span class=\"math\">\\(X_i\\)</span>, <span class=\"math\">\\(i=1,\\dots,n\\)</span>, from the\ncategorical distribution in order to estimate <span class=\"math\">\\(H(\\mathbb{p})\\)</span>.</p>\n<h3>Naive Plugin Estimator of the Discrete Entropy</h3>\n<p>The naive plugin estimator uses the frequency estimates of the categorical\nprobabilities in the expression for the true entropy, that is,</p>\n<div class=\"math\">\\begin{equation}\n\\hat{H}_N = - \\sum_{k=1}^K \\hat{p}_k \\log \\hat{p}_k,\\label{Hplugin1}\n\\end{equation}</div>\n<p>where <span class=\"math\">\\(\\hat{p}_k = h_k / n\\)</span> are the maximum likelihood estimates of each\nprobability <span class=\"math\">\\(p_k\\)</span>, and <span class=\"math\">\\(h_k = \\sum_{i=1}^n 1_{\\{X_i = k\\}}\\)</span> is simply the\nhistogram over outcomes.\nThe form <span class=\"math\">\\((\\ref{Hplugin1})\\)</span> is equivalent to the simpler form</p>\n<div class=\"math\">$$\\hat{H}_N = \\log n - \\frac{1}{n} \\sum_{k=1}^K h_k \\log h_k.$$</div>\n<h3>Problems of the Naive Plugin Estimator</h3>\n<p>It has long been known, due to <a href=\"http://epubs.siam.org/doi/abs/10.1137/1104033\">(Basharin,\n1959)</a> and <a href=\"http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA020217\">(Harris,\n1975)</a> that the estimator\n<span class=\"math\">\\((\\ref{Hplugin1})\\)</span> underestimates the true entropy <span class=\"math\">\\((\\ref{eqn:Hdiscrete})\\)</span>.\nIn fact, we have for any distribution specified by <span class=\"math\">\\(\\mathbb{p}\\)</span> that</p>\n<div class=\"math\">$$H(\\mathbb{p}) - \\mathbb{E}[\\hat{H}_N] =\n    \\frac{K-1}{2n}\n    - \\frac{1}{12 n^2} \\left(1-\\sum_k^{K} \\frac{1}{p_k}\\right)\n    + O(n^{-3}) \\geq 0,$$</div>\n<p>so that most often the true entropy is at least as large as what <span class=\"math\">\\(\\hat{H}_N\\)</span>\nclaims it is.\nWhy is this the case?  There is a simple explanation illustrated by\nthe following figure and description.</p>\n<p><img alt=\"xlogx bias explanation\" src=\"http://www.nowozin.net/sebastian/blog/images/entropy-estimation-1-bias.svg\"></p>\n<p>Let us only consider a single bin <span class=\"math\">\\(k\\)</span> with true probability <span class=\"math\">\\(p_k\\)</span>.\nIf we would know <span class=\"math\">\\(p_k\\)</span> exactly, the contribution this bin makes to the true\nentropy of the distribution is <span class=\"math\">\\(-p_k \\log p_k\\)</span>.\nWe do not know <span class=\"math\">\\(p_k\\)</span> and instead estimate it using its frequency estimate\n<span class=\"math\">\\(\\hat{p}_k = h_k / n\\)</span>.  The marginal distribution of <span class=\"math\">\\(\\hat{p}_k\\)</span> is a\n<a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\">Binomial distribution</a>.</p>\n<p>I have shown an empirical histogram of 50,000 samples from a\n<span class=\"math\">\\(\\textrm{Binomial}(1000,p_k)\\)</span> distribution in red, where <span class=\"math\">\\(p_k=0.27\\)</span> in this\ncase.  As you can see, there is significant sampling variance about the true\n<span class=\"math\">\\(p_k\\)</span>, despite having seen 1,000 samples.  It is however exactly centered at\n<span class=\"math\">\\(p_k\\)</span> because <span class=\"math\">\\(\\hat{p}_k\\)</span> is an <em>unbiased</em> estimate of <span class=\"math\">\\(p_k\\)</span>, that is we have\n<span class=\"math\">\\(\\mathbb{E} \\hat{p}_k = p_k\\)</span>.  It also is <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation\">approximately\nnormally\ndistributed</a>,\nas can be clearly seen in the Gaussian shape of the red histogram.</p>\n<p>When we now evaluate the function <span class=\"math\">\\(f(x) = -x \\log x\\)</span> we evaluate it at the\nslightly wrong place <span class=\"math\">\\(\\hat{p}_k\\)</span> instead of the true place <span class=\"math\">\\(p_k\\)</span>.\nBecause <span class=\"math\">\\(f\\)</span> is concave in this case, the famous <a href=\"http://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen's\ninequality</a> tells us that</p>\n<div class=\"math\">$$H = \\sum_k f(p_k) = \\sum_k f(\\mathbb{E} \\hat{p}_k) \\geq \\sum_k \\mathbb{E}\nf(\\hat{p}_k) = \\mathbb{E} \\sum_k f(\\hat{p}_k) = \\mathbb{E} H_N,$$</div>\n<p>so that for each <span class=\"math\">\\(p_k\\)</span> the contribution to the entropy is underestimated on\naverage.  (This does not imply that each particular finite sample estimate is\nbelow the true entropy however.)</p>\n<p>In the next part we will take a look at some improved estimators of the\ndiscrete entropy.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://www.memming.com/\">Il Memming Park</a> and\n<a href=\"http://ei.is.tuebingen.mpg.de/person/jpeters\">Jonas Peters</a>\nfor reading a draft version of the article and providing feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}