{
  "title": "Working with email content",
  "link": "",
  "updated": "2013-02-14T22:23:00-05:00",
  "id": "http://beckerfuffle.com/blog/2013/02/14/working-with-email-content",
  "content": "<p>When it comes to <a href=\"http://goo.gl/F2i6l\" title=\"Wikipedia: Tokenization\">tokenization</a>, email content presents some unique challenges. Some messages have a plain text version, some have a HTML version, and some have both. Before you can do cool things with this data like <a href=\"http://goo.gl/X0vQ\" title=\"Wikipedia: NLP\">natural language processing</a> or <a href=\"http://goo.gl/X9l0z\" title=\"Wikipedia: Predictive Analytics\">predictive analysis</a>, you have to convert the data into a uniform format (sometimes referred to as <a href=\"http://goo.gl/bMqGP\">scrubbing</a>) prior to tokenization. In my case, I wanted all of my data to be plain text.</p>\n\n<p>If you have a plain text version of the email, it is probably safe to use it without scrubbing. However if you encounter an e-mail without a plain text version, you&rsquo;ll need to convert the HTML version to text. <a href=\"http://goo.gl/wnRiJu\" title=\"search \"python convert html to text\"\">Searching the web</a>, you&rsquo;re likely to find a myriad of solutions for converting HTML to text. Python is my language of choice, and a few suggestions I found used <a href=\"http://goo.gl/xBDJZ\" title=\"lxml website\">lxml</a>, <a href=\"http://goo.gl/YUOe\" title=\"BeautifulSoup website\">BeautifulSoup</a>, and <a href=\"http://goo.gl/JGYNk\" title=\"Natural Language Toolkit website\">nltk</a> to convert HTML to text.</p>\n\n<h3>lxml and soupparser, an exercise in futility</h3>\n\n<p>lxml has a <a href=\"http://lxml.de/api/lxml.html.clean.Cleaner-class.html\" title=\"lxml cleaner class\">Cleaner</a> class which &ldquo;cleans the document of each of the possible offending elements.&rdquo; The biggest problem with using lxml is it doesn&rsquo;t handle malformed HTML gracefully. To handle these edge cases, you can use the lxml <a href=\"http://lxml.de/elementsoup.html\">soupparser</a> to parse malformed HTML. While in most cases this will work without error, it doesn&rsquo;t produce the desired results for all input. For example, in the following case soupparser will produce an empty HTML document even though there is clearly text in the data:</p>\n\n<figure class='code'><figcaption><span></span></figcaption><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\"><span class='line-number'>1</span>\n<span class='line-number'>2</span>\n<span class='line-number'>3</span>\n<span class='line-number'>4</span>\n<span class='line-number'>5</span>\n<span class='line-number'>6</span>\n<span class='line-number'>7</span>\n<span class='line-number'>8</span>\n</pre></td><td class='code'><pre><code class='python'><span class='line'><span class=\"kn\">from</span> <span class=\"nn\">lxml.etree</span> <span class=\"kn\">import</span> <span class=\"n\">tostring</span>\n</span><span class='line'><span class=\"kn\">from</span> <span class=\"nn\">lxml.html.soupparser</span> <span class=\"kn\">import</span> <span class=\"n\">fromstring</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"s\">'</form all my text is at the end of this malformed html'</span>\n</span><span class='line'><span class=\"n\">root</span> <span class=\"o\">=</span> <span class=\"n\">fromstring</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n</span><span class='line'><span class=\"k\">print</span> <span class=\"n\">tostring</span><span class=\"p\">(</span><span class=\"n\">root</span><span class=\"p\">)</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"s\">'<html/>'</span>\n</span></code></pre></td></tr></table></div></figure>\n\n\n<p>This is because the default HTML parser used by BeautifulSoup is the built-in HTMLParser. As the BeautifulSoup4 docs point out, in older versions (before 2.7.3 or 3.2.2) &ldquo;Pythonâ€™s built-in HTML parser is just not very good&rdquo;. Now there are work arounds to this. If you&rsquo;re using BeautifulSoup4, you can <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\" title=\"bs4 3rd party parser documentation\">specify a different parser</a> to use, which will provide better results. This all seems like a lot of work to convert HTML to text, isn&rsquo;t there a better way?</p>\n\n<h3>nltk.util.clean_html is full of win!</h3>\n\n<p>Enter <a href=\"http://nltk.org/book/ch03.html#dealing-with-html\" title=\"nltk.util.clean_html\">nltk.util.clean_html</a>. clean_html uses regular expressions to strip HTML tags from text. This approach helps avoid the issues found with lxml and BeautifulSoup. Looking at our previous example, we don&rsquo;t lose our text data using clean_html:</p>\n\n<p><img src=\"http://cdn.memegenerator.net/instances/400x/34904914.jpg\" alt=\"NLTK is full of win!\" /></p>\n\n<figure class='code'><figcaption><span></span></figcaption><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\"><span class='line-number'>1</span>\n<span class='line-number'>2</span>\n<span class='line-number'>3</span>\n<span class='line-number'>4</span>\n<span class='line-number'>5</span>\n<span class='line-number'>6</span>\n</pre></td><td class='code'><pre><code class='python'><span class='line'><span class=\"kn\">from</span> <span class=\"nn\">nltk</span> <span class=\"kn\">import</span> <span class=\"n\">clean_html</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"s\">'</form all my text is at the end of this malformed html'</span>\n</span><span class='line'><span class=\"k\">print</span> <span class=\"n\">clean_html</span><span class=\"p\">(</span><span class=\"n\">root</span><span class=\"p\">)</span>\n</span><span class='line'>\n</span><span class='line'><span class=\"s\">'</form all my text is at the end of this malformed html'</span>\n</span></code></pre></td></tr></table></div></figure>\n\n\n<p>Looking at <a href=\"http://nltk.org/_modules/nltk/util.html#clean_html\" title=\"clean_html source code\">the implementation</a>, it almost seems too simple. There are six regular expressions, and that&rsquo;s it! I&rsquo;ve tested all three solutions against several million real e-mail messages, and in all cases nltk provided the best results.</p>\n\n<p>Sifting through all the possible solutions for converting HTML to text and testing each of them was pretty time consuming. If your goal is to scrub the HTML for further analysis, nltk clean_html is definitely the way to go!</p>\n"
}