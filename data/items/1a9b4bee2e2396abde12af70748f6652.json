{
  "title": "Benchmarks for Mass Matrix Adaptation",
  "link": "",
  "id": "https://www.georgeho.org/mass-matrix-benchmarks/",
  "updated": "2019-12-14T00:00:00Z",
  "published": "2019-12-14T00:00:00Z",
  "content": "<p>I was lucky enough to be invited to attend the <a href=\"https://gradientretreat.com/\">Gradient\nRetreat</a> earlier this month. It was an entire week\non a beautiful island with some amazingly intelligent Bayesians, and no demands\non my time other than the self-set (and admittedly vague) goal of contributing\nto probabilistic programming in some way.</p>\n<p>I initially tried to implement mass matrix adaptation in Tensorflow Probability,\nbut I quickly readjusted my goals to something more achievable: running some\nbenchmarks with tuning in Hamiltonian Monte Carlo (HMC).</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/galiano.jpg\"><img src=\"https://www.georgeho.org/assets/images/galiano.jpg\" alt=\"A view of a forest on Galiano Island\"></a>\n<a href=\"https://www.georgeho.org/assets/images/galiano2.jpg\"><img src=\"https://www.georgeho.org/assets/images/galiano2.jpg\" alt=\"The view from a bluff on Galiano Island\"></a>\n<figcaption>Pictures from Galiano Island.</figcaption>\n</figure>\n<p>A quick rundown for those unfamiliar: <em>tuning</em> is what happens before sampling,\nduring which the goal is not to actually draw samples, but to <em>prepare</em> to draw\nsamples<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>. For HMC and its variants, this means estimating HMC parameters such\nas the step size, integration time and mass matrix<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup>, the last of which is\nbasically the covariance matrix of the model parameters. Because my life is\nfinite (and I assume everybody else&rsquo;s is too), I limited myself to mass matrix\nadaptation.</p>\n<p>(If you&rsquo;re still uncertain about the details of tuning or mass matrix\nadaptation, check out <a href=\"https://colcarroll.github.io/hmc_tuning_talk/\">Colin Carroll&rsquo;s essay on HMC\ntuning</a> or the <a href=\"https://mc-stan.org/docs/2_20/reference-manual/hmc-algorithm-parameters.html\">Stan reference\nmanual on HMC\nparameters</a>:\nI don&rsquo;t explain many more concepts in the rest of this post.)</p>\n<p>The interesting thing about tuning is that there are no rules: there are no\nasymptotic guarantees we can rely on and no mathematical results to which we can\nturn for enlightened inspiration. The only thing we care about is obtaining a\ndecent estimate of the mass matrix, and preferably quickly.</p>\n<p>Accompanying this lack of understanding of mass matrix adaptation is an\ncommensurate lack of (apparent) scientific inquiry â€” there is scant literature\nto look to, and for open source developers, there is little prior art to draw\nfrom when writing new implementations of HMC!</p>\n<p>So I decided to do some empirical legwork and benchmark various methods of mass\nmatrix adaptation. Here are the questions I was interested in answering:</p>\n<ol>\n<li>Is the assumption that the mass matrix is diagonal (in other words, assume\nthat all parameters are uncorrelated) a good assumption to make? What are\nthe implications of this assumption for the tuning time, and the number of\neffective samples per second?</li>\n<li>Does the tuning schedule (i.e. the sizes of the adaptation windows) make a\nbig difference? Specifically, should we have a schedule of constant\nadaptation windows, or an &ldquo;expanding schedule&rdquo; of exponentially growing\nadaptation windows?</li>\n<li>Besides assuming the mass matrix is diagonal, are there any other ways of\nsimplifying mass matrix adaptation? For example, could we approximate the\nmass matrix as low rank?</li>\n</ol>\n<p>I benchmarked five different mass matrix adaptation methods:</p>\n<ol>\n<li>A diagonal mass matrix (<code>diag</code>)</li>\n<li>A full (a.k.a. dense) mass matrix (<code>full</code>)</li>\n<li>A diagonal mass matrix adapted on an expanding schedule (<code>diag_exp</code>)</li>\n<li>A full mass matrix adapted on an expanding schedule (<code>diag_exp</code>)</li>\n<li>A low-rank approximation to the mass matrix using <a href=\"https://github.com/aseyboldt/covadapt\">Adrian Seyboldt&rsquo;s <code>covadapt</code> library</a>.</li>\n</ol>\n<p>I benchmarked these adaptation methods against six models:</p>\n<ol>\n<li>A 100-dimensional multivariate normal with a non-diagonal covariance matrix (<code>mvnormal</code>)</li>\n<li>A 100-dimensional multivariate normal with a low-rank covariance matrix (<code>lrnormal</code>)</li>\n<li>A <a href=\"https://docs.pymc.io/notebooks/stochastic_volatility.html\">stochastic volatility model</a> (<code>stoch_vol</code>)</li>\n<li>The <a href=\"https://docs.pymc.io/notebooks/Diagnosing_biased_Inference_with_Divergences.html#The-Eight-Schools-Model\">eight schools model</a> (<code>eight</code>)</li>\n<li>The <a href=\"https://docs.pymc.io/notebooks/hierarchical_partial_pooling.html\">PyMC3 baseball model</a> (<code>baseball</code>)</li>\n<li>A <a href=\"https://docs.pymc.io/notebooks/GP-SparseApprox.html#Examples\">sparse Gaussian process approximation</a> (<code>gp</code>)</li>\n</ol>\n<p>Without further ado, the main results are shown below. Afterwards, I make some\ngeneral observations on the benchmarks, and finally I describe various\nshortcomings of my experimental setup (which, if I were more optimistic, I would\ncall &ldquo;directions for further work&rdquo;).</p>\n<h3 id=\"tuning-times\">Tuning Times</h3>\n<p>This tabulates the tuning time, in seconds, of each adaptation method for each\nmodel. Lower is better. The lowest tuning time for each model is shown in bold\nitalics.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\"></th>\n<th style=\"text-align:right\"><strong><code>mvnormal</code></strong></th>\n<th style=\"text-align:right\"><strong><code>lrnormal</code></strong></th>\n<th style=\"text-align:right\"><strong><code>stoch_vol</code></strong></th>\n<th style=\"text-align:right\"><strong><code>gp</code></strong></th>\n<th style=\"text-align:right\"><strong><code>eight</code></strong></th>\n<th style=\"text-align:right\"><strong><code>baseball</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><strong><code>diag</code></strong></td>\n<td style=\"text-align:right\">365.34</td>\n<td style=\"text-align:right\">340.10</td>\n<td style=\"text-align:right\">239.59</td>\n<td style=\"text-align:right\">18.47</td>\n<td style=\"text-align:right\">2.92</td>\n<td style=\"text-align:right\">5.32</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>full</code></strong></td>\n<td style=\"text-align:right\"><em><strong>8.29</strong></em></td>\n<td style=\"text-align:right\">364.07</td>\n<td style=\"text-align:right\">904.95</td>\n<td style=\"text-align:right\"><em><strong>14.24</strong></em></td>\n<td style=\"text-align:right\"><em><strong>2.91</strong></em></td>\n<td style=\"text-align:right\"><em><strong>4.93</strong></em></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>diag_exp</code></strong></td>\n<td style=\"text-align:right\">358.50</td>\n<td style=\"text-align:right\">360.91</td>\n<td style=\"text-align:right\"><em><strong>219.65</strong></em></td>\n<td style=\"text-align:right\">16.25</td>\n<td style=\"text-align:right\">3.05</td>\n<td style=\"text-align:right\">5.08</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>full_exp</code></strong></td>\n<td style=\"text-align:right\">8.46</td>\n<td style=\"text-align:right\">142.20</td>\n<td style=\"text-align:right\">686.58</td>\n<td style=\"text-align:right\">14.87</td>\n<td style=\"text-align:right\">3.21</td>\n<td style=\"text-align:right\">6.04</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>covadapt</code></strong></td>\n<td style=\"text-align:right\">386.13</td>\n<td style=\"text-align:right\"><em><strong>89.92</strong></em></td>\n<td style=\"text-align:right\">398.08</td>\n<td style=\"text-align:right\">N/A</td>\n<td style=\"text-align:right\">N/A</td>\n<td style=\"text-align:right\">N/A</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"effective-samples-per-second\">Effective Samples per Second</h3>\n<p>This tabulates the number of effective samples drawn by each adaptation method\nfor each model. Higher is better. The highest numbers of effective samples per\nsecond is shown in bold italics.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\"></th>\n<th style=\"text-align:right\"><strong><code>mvnormal</code></strong></th>\n<th style=\"text-align:right\"><strong><code>lrnormal</code></strong></th>\n<th style=\"text-align:right\"><strong><code>stoch_vol</code></strong></th>\n<th style=\"text-align:right\"><strong><code>gp</code></strong></th>\n<th style=\"text-align:right\"><strong><code>eight</code></strong></th>\n<th style=\"text-align:right\"><strong><code>baseball</code></strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"><strong><code>diag</code></strong></td>\n<td style=\"text-align:right\">0.02</td>\n<td style=\"text-align:right\">1.55</td>\n<td style=\"text-align:right\"><em><strong>11.22</strong></em></td>\n<td style=\"text-align:right\">65.36</td>\n<td style=\"text-align:right\">761.82</td>\n<td style=\"text-align:right\">455.23</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>full</code></strong></td>\n<td style=\"text-align:right\">1.73</td>\n<td style=\"text-align:right\">0.01</td>\n<td style=\"text-align:right\">6.71</td>\n<td style=\"text-align:right\"><em><strong>106.30</strong></em></td>\n<td style=\"text-align:right\"><em><strong>840.77</strong></em></td>\n<td style=\"text-align:right\"><em><strong>495.93</strong></em></td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>diag_exp</code></strong></td>\n<td style=\"text-align:right\">0.02</td>\n<td style=\"text-align:right\">1.51</td>\n<td style=\"text-align:right\">9.79</td>\n<td style=\"text-align:right\">59.89</td>\n<td style=\"text-align:right\">640.90</td>\n<td style=\"text-align:right\">336.71</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>full_exp</code></strong></td>\n<td style=\"text-align:right\"><em><strong>1,799.11</strong></em></td>\n<td style=\"text-align:right\"><em><strong>1,753.65</strong></em></td>\n<td style=\"text-align:right\">0.16</td>\n<td style=\"text-align:right\">101.99</td>\n<td style=\"text-align:right\">618.28</td>\n<td style=\"text-align:right\">360.14</td>\n</tr>\n<tr>\n<td style=\"text-align:left\"><strong><code>covadapt</code></strong></td>\n<td style=\"text-align:right\">0.02</td>\n<td style=\"text-align:right\">693.87</td>\n<td style=\"text-align:right\">5.71</td>\n<td style=\"text-align:right\">N/A</td>\n<td style=\"text-align:right\">N/A</td>\n<td style=\"text-align:right\">N/A</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"observations\">Observations</h2>\n<blockquote>\n<p><strong>tldr:</strong> As is typical with these sorts of things, no one adaptation method\nuniformly outperforms the others.</p>\n</blockquote>\n<ul>\n<li>A full mass matrix can provide significant improvements over a diagonal mass\nmatrix for both the tuning time and the number of effective samples per\nsecond. This improvement can sometimes go up to two orders of magnitude!\n<ul>\n<li>This is most noticeable in the <code>mvnormal</code> model, with heavily correlated\nparameters.</li>\n<li>Happily, my benchmarks are not the only instance of full mass matrices\noutperforming diagonal ones: <a href=\"https://dfm.io/posts/pymc3-mass-matrix/\">Dan Foreman-Mackey demonstrated something\nsimilar in one of his blog posts</a>.</li>\n<li>However, in models with less extreme correlations among parameters, this\nadvantage shrinks significantly (although it doesn&rsquo;t go away entirely).\nFull matrices can also take longer to tune. You can see this in the baseball\nor eight schools model.</li>\n<li>Nevertheless, full mass matrices never seem to perform egregiously <em>worse</em>\nthan diagonal mass matrices. This makes sense theoretically: a full mass\nmatrix can be estimated to be diagonal (at the cost of a quadratic memory\nrequirement as opposed to linear), but not vice versa.</li>\n</ul>\n</li>\n<li>Having an expanding schedule for tuning can sometimes give better performance,\nbut nowhere near as significant as the difference between diagonal and full\nmatrices. This difference is most noticeable for the <code>mvnormal</code> and <code>lrnormal</code>\nmodels (probably because these models have a constant covariance matrix and so\nmore careful estimates using expanding windows can provide much better\nsampling).</li>\n<li>I suspect the number of effective samples per second for a full mass matrix on\nthe <code>lrnormal</code> model (0.01 effective samples per second) is a mistake (or\nsome other computational fluke): it looks way too low to be reasonable.</li>\n<li>I&rsquo;m also surprised that <code>full_exp</code> does really badly (in terms of effective\nsamples per second) on the <code>stoch_vol</code> model, despite <code>full</code> doing decently\nwell! This is either a fluke, or a really interesting phenomenon to dig in to.</li>\n<li><code>covadapt</code> seems to run into some numerical difficulties? While running these\nbenchmarks I ran into an inscrutable and non-reproducible\n<a href=\"https://stackoverflow.com/q/18436667\"><code>ArpackError</code></a> from SciPy.</li>\n</ul>\n<h2 id=\"experimental-setup\">Experimental Setup</h2>\n<ul>\n<li>All samplers were run for 2000 tuning steps and 1000 sampling steps. This is\nunusually high, but is necessary for <code>covadapt</code> to work well, and I wanted to\nuse the same number of iterations across all the benchmarks.</li>\n<li>My expanding schedule is as follows: the first adaptation window is 100\niterations, and each subsequent window is 1.005 times the previous window.\nThese numbers give 20 updates within 2000 iterations, while maintaining an\nexponentially increasing adaptation window size.</li>\n<li>I didn&rsquo;t run <code>covadapt</code> for models with fewer than 100 model parameters.\nWith so few parameters, there&rsquo;s no need to approximate a mass matrix as\nlow-rank: you can just estimate the full mass matrix!</li>\n<li>I set <code>target_accept</code> (a.k.a. <code>adapt_delta</code> to Stan users) to 0.9 to make all\ndivergences go away.</li>\n<li>All of these numbers were collected by sampling once per model per adaptation\nmethod (yes only once, sorry) in PyMC3, running on my MacBook Pro.</li>\n</ul>\n<h2 id=\"shortcomings\">Shortcomings</h2>\n<ul>\n<li>In some sense comparing tuning times is not a fair comparison: it&rsquo;s possible\nthat some mass matrix estimates converge quicker than others, and so comparing\ntheir tuning times is essentially penalizing these methods for converging\nfaster than others.</li>\n<li>It&rsquo;s also possible that my expanding schedule for the adaptation windows just\nsucks! There&rsquo;s no reason why the first window needs to be 100 iterations, or\nwhy 1.005 should be a good multiplier. It looks like Stan <a href=\"https://github.com/stan-dev/stan/blob/736311d88e99b997f5b902409752fb29d6ec0def/src/stan/mcmc/windowed_adaptation.hpp#L95\">doubles their\nadaptation window\nsizes</a>\nduring warmup.</li>\n<li>These benchmarks are done only for very basic toy models: I should test more\nextensively on more models that people in The Real Worldâ„¢ use.</li>\n<li>If you are interested in taking these benchmarks further (or perhaps just want\nto fact-check me on my results), the code is <a href=\"https://github.com/eigenfoo/mass-matrix-benchmarks\">sitting in this GitHub\nrepository</a><sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup>.</li>\n</ul>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>It&rsquo;s good to point out that mass matrix adaptation is to make sampling\nmore efficient, not more valid. Theoretically, any mass matrix would work,\nbut a good one (i.e. a good estimate of the covariance matrix of the model\nparameters) could sample orders of magnitudes more efficiently.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>â€¦uh, <em><em>sweats and looks around nervously for differential geometers</em></em>\nmore formally called the <em>metric</em>â€¦&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\">\n<p>There are some violin plots lying around in the notebook, a relic from a\ntime when I thought that I would have the patience to run each model and\nadaptation method multiple times.&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}