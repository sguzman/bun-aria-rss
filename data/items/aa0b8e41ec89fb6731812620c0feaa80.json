{
  "id": "tag:blogger.com,1999:blog-15418143.post-5039329766928752607",
  "published": "2014-01-07T18:45:00.003-05:00",
  "updated": "2014-01-14T02:10:34.445-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "Tracking points in a live camera feed: A behind-the-scenes look at the VMX Project webapp",
  "content": "In our <a href=\"http://vision.ai/\">computer vision startup, vision.ai</a>,&nbsp;we're using open-source tools to create a one-of-a-kind&nbsp;object recognition experience. &nbsp;Our goal is to make state-of-the-art visual object recognition as easy as waving an object in front of your laptop's or smartphone's camera. &nbsp;We've made a webapp and programming environment called VMX that allows you to teach your computer about objects without any advanced programming, nor any bulky software installations -- you'll finally be able to put your computer's new visual reasoning abilities to good use. &nbsp;Today's blog post is about some of the underlying technology that we used to build the VMX prototype. &nbsp;(To learn about the entire project and how you can help, please visit <a href=\"http://www.kickstarter.com/projects/visionai/vmx-project-computer-vision-for-everyone\">VMX Project on Kickstarter</a>.)<br /><br />The VMX project utilizes many different programming languages and technologies. &nbsp;Many of the behind-the-scenes machine learning algorithms have been developed in our lab, but to make a good product it takes more than just robust backed algorithms. &nbsp;On the front-end, the two key open source (MIT licensed) projects we rely on are <a href=\"http://angularjs.org/\">AngularJS</a> and <a href=\"http://inspirit.github.io/jsfeat/\">JSFeat</a>. AngularJS is an open-source JavaScript framework, maintained by Google, that assists with running single-page applications. &nbsp;Today's focus will be on JSFeat, the Javascript Computer Vision Library we use inside the front-end webapp. &nbsp;What is JSFeat? &nbsp;Quoting <a href=\"http://twitter.com/inspirit\">Eugene Zatepyakin</a>, the author of JSFeat,&nbsp;\"The project aim is to explore JS/HTML5 possibilities using modern &amp; state-of-art computer vision algorithms.\"<br /><br /><b>We use the JSFeat library to track points inside the video stream.</b> &nbsp;Below is a YouTube video of our webapp in action, where we enabled the \"debug display\" to show you what is happening to tracked points behind the scenes. &nbsp;The <b>blue points are being tracked</b> inside the browser, the <b>green box is the output of our object detection service</b> (already trained on my face), and the <b>black box is the interpolated result</b> which integrates the backend service and the frontend tracker.<br /><br /><div style=\"text-align: center;\"><br /></div><div style=\"text-align: center;\"><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"//www.youtube.com/embed/Pf7mKlj73As\" width=\"560\"></iframe> </div><div style=\"text-align: center;\"><br /></div>The tracker calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids. &nbsp;The algorithm basically looks at two consecutive video frames and determines how points move by using a straightforward least-squares optimization method. The Lucas-Kanade algorithm is a classic in the computer vision community -- to learn more see the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method\">Lucas-Kanade Wikipedia page</a>&nbsp;or take a graduate level computer vision course. Alternatively, if you find me on the street and ask nicely, I <i>might</i> give you an impromptu lecture on optical flow.<br /><br />Instead of using interest points, in our prototype video we used a regularly spaced grid of points covering the entire video stream. &nbsp;This grid gets re-initialized every N seconds. &nbsp;It avoids the extra expense of finding interest points inside every frame. &nbsp;NOTE: inside our vision.ai computer vision lab, we are incessantly experimenting with better ways of integrating point tracks with strong object detector results. &nbsp;What you're seeing is just an early snapshot of the technology in action.<br /><br />To play with a Lucas-Kanade tracker, take a look at the JSFeat demo page which runs a point tracker directly inside your browser. &nbsp;You'll have to click on points, one at a time. &nbsp;You'll need Google Chrome or Firefox (just like our VMX project), and this will give you a good sense of what using VMX is going to be like once it is available.<br /><br /><div style=\"text-align: center;\">Try the&nbsp;<a href=\"http://inspirit.github.io/jsfeat/sample_oflow_lk.html\">JSFeat Optical Flow Demo</a>!</div><br />To summarize, there are lots of great computer vision tools out there, but none of these tools can give you a comprehensive object recognition system which requires little-to-none programming experience. &nbsp;<b>There is a lot of work needed to put together appropriate machine learning algorithms, object detection libraries, web services, trackers, video codecs, etc</b>. &nbsp;Luckily, the team at vision.ai loves both code and machine learning. &nbsp;In addition, having spent the last 10 years of my life working as a research in Computer Vision doesn't hurt. <br /><br />Getting a PhD in Computer Vision and learning how all of these technologies work is a truly amazing experience. &nbsp;<b>I encourage many students to undertake this 6+ year journey and learn all about computer vision.</b> &nbsp;But I know the PhD path is not for everybody. &nbsp;That's why we've built VMX. &nbsp;So the rest of you can enjoy the power of industrial-grade computer vision algorithms and the ease of intuitive web-based interfaces, without the expertise needed to piece together many different technologies. &nbsp;The number of applications of computer vision tech is astounding and it is a shame that such technology hasn't been delivered with such a lower barrier-to-entry earlier.<br /><br />With VMX, <b>we're excited that the world is going to experience visual object recognition the way it was meant to be experienced.</b> &nbsp;But for that to happen, we still need <i>your</i> support. &nbsp;Check out our <a href=\"http://www.kickstarter.com/projects/visionai/vmx-project-computer-vision-for-everyone\">VMX Project on Kickstarter</a> (the page has lots of additional VMX in action videos), and help spread the word.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://www.kickstarter.com/projects/visionai/vmx-project-computer-vision-for-everyone\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://2.bp.blogspot.com/-hjbZkvxmWSM/UsyM6suKwkI/AAAAAAAANWw/42HCtxyI4qo/s400/input_formats-01.png\" height=\"91\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://www.kickstarter.com/projects/visionai/vmx-project-computer-vision-for-everyone\">VMX Project: Computer Vision for Everyone</a></div><br /><br /><br />",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 2
}