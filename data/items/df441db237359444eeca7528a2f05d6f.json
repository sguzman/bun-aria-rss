{
  "title": "2-D random walks are special",
  "link": "",
  "published": "2020-04-10T00:00:00-07:00",
  "updated": "2020-04-10T00:00:00-07:00",
  "author": {
    "name": "Dustin McIntosh"
  },
  "id": "tag:efavdb.com,2020-04-10:/random-walk-scaling",
  "summary": "<p>Here, we examine the statistics behind discrete random walks on square lattices in <span class=\"math\">\\(M\\)</span> dimensions, with focus on two metrics (see figure below for an example in 2-D): 1. <span class=\"math\">\\(R\\)</span>, the final distance traveled from origin (measured by the Euclidean norm) and 2. <span class=\"math\">\\(N_{unique}\\)</span>, the number of unique locations …</p>",
  "content": "<p>Here, we examine the statistics behind discrete random walks on square lattices in <span class=\"math\">\\(M\\)</span> dimensions, with focus on two metrics (see figure below for an example in 2-D): 1. <span class=\"math\">\\(R\\)</span>, the final distance traveled from origin (measured by the Euclidean norm) and 2. <span class=\"math\">\\(N_{unique}\\)</span>, the number of unique locations visited on the&nbsp;lattice.</p>\n<p align=\"center\">\n     <img src=\"images/example_rw.png\">\n</p>\n\n<p>We envision a single random walker on an <span class=\"math\">\\(M\\)</span>-D lattice and allow it to wander randomly throughout the lattice, taking <span class=\"math\">\\(N\\)</span> steps. We’ll examine how the distributions of <span class=\"math\">\\(R\\)</span> and <span class=\"math\">\\(N_{unique}\\)</span> vary with <span class=\"math\">\\(M\\)</span> and <span class=\"math\">\\(N\\)</span>; we&#8217;ll show that their averages, <span class=\"math\">\\(\\langle R \\rangle\\)</span> and <span class=\"math\">\\(\\langle N_{unique} \\rangle\\)</span>, and their standard deviations, <span class=\"math\">\\(\\sigma_R\\)</span> and <span class=\"math\">\\(\\sigma_{N_{unique}}\\)</span>, scale as power laws with <span class=\"math\">\\(N\\)</span>. The dependence of the exponents and scaling factors on <span class=\"math\">\\(M\\)</span> is interesting and can be only partially reconciled with&nbsp;theory.</p>\n<p>A simple simulation of random walks is easy to write in python for arbitrary dimensions (see <a\nhref=\"https://colab.research.google.com/drive/13GYlaTvO-Wu_3ep_Pa0mRZo-CYelDFmf\">this colab notebook</a>, <a href=\"https://github.com/dustinmcintosh/random-walks\">github</a>).</p>\n<p>Here’s a look at the distribution of our two metrics for <span class=\"math\">\\(N = 1000\\)</span> for a few different&nbsp;dimensionalities:</p>\n<p align=\"center\">\n     <img src=\"images/unique_locations_visited_1000.png\">\n</p>\n\n<p>Let’s make some high-level sense of these&nbsp;results:</p>\n<ul>\n<li>\n<p><span class=\"math\">\\(\\langle R \\rangle\\)</span> depends only weakly on <span class=\"math\">\\(M\\)</span> while <span class=\"math\">\\(\\langle N_{unique} \\rangle\\)</span> clearly increases with <span class=\"math\">\\(M\\)</span>. Both of these results make sense: In 1-D, the walker always has an equal chance to step further away from the origin or closer to it.  It also always has at least a 50% chance of backtracking to a position it has already visited. As you add dimensions, it becomes less likely to step immediately closer or further from the origin and more likely to wander in an orthogonal direction, increasing the distance from the origin by roughly the same amount independent of <em>which</em> orthogonal direction, while also visiting completely new parts of the&nbsp;lattice.</p>\n</li>\n<li>\n<p>In 1-D, if you take an even number of steps, <span class=\"math\">\\(R\\)</span> is always an even integer, so the distribution of <span class=\"math\">\\(R\\)</span> appears “stripe-y” above. <span class=\"math\">\\(R(M=1)\\)</span> can be understood as something like a <a href=\"https://en.wikipedia.org/wiki/Folded_normal_distribution\">folded normal distribution</a> for large <span class=\"math\">\\(N\\)</span>.</p>\n</li>\n<li>\n<p>As <span class=\"math\">\\(M\\)</span> gets larger, the distribution of <span class=\"math\">\\(R\\)</span> tightens up. We are essentially taking <span class=\"math\">\\(M\\)</span> 1-D random walks, each with <span class=\"math\">\\(\\sim N/M\\)</span> steps and adding the results in quadrature. The result of this is that our walker is less likely to be very close to the origin and simultaneously less likely to wander too far afield as it is more likely to have traveled a little bit in many orthogonal&nbsp;dimensions.</p>\n</li>\n<li>\n<p>The width of the <span class=\"math\">\\(N_{unique}\\)</span> distribution, <span class=\"math\">\\(\\sigma_{N_{unique}}\\)</span>, depends in an interesting way on <span class=\"math\">\\(M\\)</span>, increasing from 1-D to 2-D and thereafter decreasing. This seems because the distribution is centered most distant from the extremes of 0 or <span class=\"math\">\\(N\\)</span>.</p>\n</li>\n</ul>\n<p>Let’s take a look at the means of our two metrics as a function of <span class=\"math\">\\(N\\)</span> for various <span class=\"math\">\\(M\\)</span> (apologies for using a gif here, but it helps with 1. keeping this post concise and 2. comparing the plots as <span class=\"math\">\\(M\\)</span> changes incrementally.  If you prefer static images, I’ve included them at <a href=\"https://github.com/dustinmcintosh/random-walks\">github</a>):</p>\n<p align=\"center\">\n     <img src=\"images/R_and_Nu_vs_n.gif\">\n</p>\n\n<p>Note, this is a <a href=\"https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\">log-log plot</a>; power laws show up as straight lines.  One of the first things you’ll notice is that the variation in dependence on <span class=\"math\">\\(N\\)</span> across different <span class=\"math\">\\(M\\)</span> is fairly banal for <span class=\"math\">\\(R\\)</span>, but much more interesting for <span class=\"math\">\\(N_{unique}\\)</span>.  In fact, it is a well-known result (see, e.g., <a href=\"https://math.stackexchange.com/questions/103142/expected-value-of-random-walk\">here</a>) that <span class=\"math\">\\(\\langle R \\rangle \\sim N^\\alpha\\)</span> with <span class=\"math\">\\(\\alpha = 0.5\\)</span> for all <span class=\"math\">\\(M\\)</span>.  <span class=\"math\">\\(\\langle N_{unique} \\rangle\\)</span>, on the other hand, seems similar to <span class=\"math\">\\(\\langle R \\rangle\\)</span> in 1-D, but the dependence on <span class=\"math\">\\(N\\)</span> increases dramatically thereafter and approaches <span class=\"math\">\\(\\langle N_{unique} \\rangle \\approx N^\\beta\\)</span> with <span class=\"math\">\\(\\beta=1\\)</span> at higher dimensions.  If you look closely, you’ll note that 2-D is particularly special (more on that&nbsp;shortly).</p>\n<p>We can also look at the widths (standard deviations) of the two distributions (again, you’ll note that 2-D is particularly&nbsp;interesting):</p>\n<p align=\"center\">\n     <img src=\"images/std_vs_N.gif\">\n</p>\n\n<p>First, note that all 4 of the quantities describing the distributions of <span class=\"math\">\\(R\\)</span> and <span class=\"math\">\\(N_{unique}\\)</span> plotted above are well-described by power laws. Thus, we can extract eight parameters to describe them (four exponents and four scaling factors) as a function of <span class=\"math\">\\(M\\)</span>:</p>\n<div class=\"math\">\\begin{eqnarray}\n\\langle R \\rangle &amp;\\approx&amp; R_0 * N^\\alpha \\\\\n\\langle N_{unique} \\rangle &amp;\\approx&amp; N_0 * N^\\beta \\\\\n\\sigma_R &amp;\\approx&amp; \\sigma_{R,0} * N^\\gamma \\\\\n\\sigma_{N_{unique}} &amp;\\approx&amp; \\sigma_{N_{unique},0} * N^\\delta \\\\\n\\end{eqnarray}</div>\n<p align=\"center\">\n     <img src=\"images/exponents_scaling_vs_M.png\">\n</p>\n\n<p>I’ve plotted all the theoretical results that I could find for these parameters as lines&nbsp;above:</p>\n<ul>\n<li>\n<p>As mentioned, <span class=\"math\">\\(\\alpha = 0.5\\)</span> for all <span class=\"math\">\\(M\\)</span>. (blue line top&nbsp;plot)</p>\n</li>\n<li>\n<p><span class=\"math\">\\(R_0\\)</span> gives <span class=\"math\">\\(\\langle R \\rangle\\)</span> its weak dependence on <span class=\"math\">\\(M\\)</span>, varying according to an elegant ratio of Gamma functions as described <a href=\"https://math.stackexchange.com/questions/103142/expected-value-of-random-walk\">here</a>.  (blue dash-dot line bottom&nbsp;plot)</p>\n</li>\n<li>\n<p>It has been shown that <span class=\"math\">\\(\\beta = 0.5\\)</span> for <span class=\"math\">\\(M=1\\)</span> and <span class=\"math\">\\(\\beta = 1\\)</span> for all <span class=\"math\">\\(M \\geq 3\\)</span>.  (orange dash and line in the top plot)  I found these theoretical results in this <a href=\"https://www.osti.gov/servlets/purl/4637387\">wonderful paper</a>, which you should really glance at, if not read, just to see an interesting piece of history - it’s by George H. Vineyard in 1963 (no so long ago for such a fundamental math problem!) at Brookhaven National Lab for the <span class=\"caps\">US</span> Atomic Energy Commission. Written on a typewriter, here is a sample equation, complete with hand-written scribbles to indicate the vectors and a clearly corrected typo on the first&nbsp;cosine:</p>\n</li>\n</ul>\n<p align=\"center\">\n     <img src=\"images/Vineyard_pic.png\">\n</p>\n\n<ul>\n<li>In the same paper, Vineyard derives that <span class=\"math\">\\(N_{unique, 0} = \\sqrt{8/\\pi}\\)</span> in 1-D and <span class=\"math\">\\(N_{unique, 0} \\approx 0.659462670\\)</span> (yes with all those sig. figs.) in 3-D as derived from evaluating Watson’s Integrals (which, I take it, is what the integrals in the image above are called). (orange dashes in bottom&nbsp;plot)</li>\n</ul>\n<p>The most interesting thing about all this, to me, is that there is no known theoretical result for <span class=\"math\">\\(\\beta\\)</span> in 2-D. From our data, we get <span class=\"math\">\\(\\beta = 0.87 \\pm 0.02\\)</span>.  An interesting phenomenological argument for <span class=\"math\">\\(\\beta\\)</span>’s dependence on <span class=\"math\">\\(M\\)</span>: The random walker spends most of it’s time roaming around within a distance of <span class=\"math\">\\(\\langle R \\rangle \\sim N^{\\alpha}\\)</span> from the origin (where <span class=\"math\">\\(\\alpha=1/2\\)</span>, independent of <span class=\"math\">\\(M\\)</span>), as that is where it is going to end up.  In 1-D, there are only <span class=\"math\">\\(\\sim 2 N^{1/2}\\)</span> sites within this distance, so the number of unique sites visited scales with <span class=\"math\">\\(N^{1/2}\\)</span>. In 3-D, there are <span class=\"math\">\\(\\sim \\frac{4}{3} \\pi N^{3/2}\\)</span> sites in the sphere of radius <span class=\"math\">\\(N^{1/2}\\)</span>, which is <span class=\"math\">\\(\\gg N\\)</span>, so the walker explores <span class=\"math\">\\(\\sim N\\)</span> sites (as it can visit no more than it walks), similar for larger <span class=\"math\">\\(M\\)</span>.</p>\n<p><strong>This is what makes 2-D random walks special</strong>: the number of sites within <span class=\"math\">\\(N^{1/2}\\)</span> distance of the origin is <span class=\"math\">\\(\\sim N\\)</span>, scaling as the number of steps taken - so, the scaling arguments above no longer apply, leaving us somewhere between <span class=\"math\">\\(1/2 &lt; \\beta &lt; 1\\)</span>.</p>\n<p>A few final notes on the exponents and scaling factors that I don’t understand in terms of&nbsp;theory:</p>\n<ul>\n<li>We don’t get exactly <span class=\"math\">\\(\\beta=1\\)</span> from the data in 3-D above, but that may be because some of our <span class=\"math\">\\(N\\)</span> are not quite large&nbsp;enough.</li>\n<li>I have not seen any theory for <span class=\"math\">\\(\\sigma_{R,0}\\)</span>, <span class=\"math\">\\(\\sigma_{N_{unique},0}\\)</span>, <span class=\"math\">\\(\\gamma\\)</span>, or <span class=\"math\">\\(\\delta\\)</span>.</li>\n<li><span class=\"math\">\\(\\gamma\\)</span> appears to also be independent of <span class=\"math\">\\(M\\)</span> at 0.5. Surely this isn&#8217;t hard to&nbsp;derive?</li>\n<li><span class=\"math\">\\(\\delta\\)</span> increases dramatically from 1-D to 2-D (again, 2-D appears the most interesting case) and slowly settles back down to 0.5 at large <span class=\"math\">\\(M\\)</span>.</li>\n<li><span class=\"math\">\\(\\sigma_{N_{unique},0}\\)</span> has a strange effect for <span class=\"math\">\\(M=2\\)</span> also, dipping before going back up and then finally decaying towards zero for <span class=\"math\">\\(M&gt;4\\)</span>.</li>\n<li>For large <span class=\"math\">\\(M\\)</span>, the data indicate that <span class=\"math\">\\(\\sigma_{R,0} \\approx \\sigma_{N_{unique},0}\\)</span> and <span class=\"math\">\\(\\gamma \\approx \\delta \\approx 0.5\\)</span>, indicating that <span class=\"math\">\\(\\sigma_{R} \\approx \\sigma_{N_{unique}}\\)</span>.  Further, not shown here, but in <a href=\"https://github.com/dustinmcintosh/random-walks/blob/master/figures/scaling_of_std_dev_vs_m.png\">github</a>,  <span class=\"math\">\\(\\sigma_{R,0} \\approx \\sigma_{N_{unique},0} \\sim \\sqrt{1/M}\\)</span>. Or, in total, <span class=\"math\">\\(\\sigma_{R} \\approx \\sigma_{N_{unique}} \\sim \\sqrt{N/M}\\)</span> for large <span class=\"math\">\\(M\\)</span> and <span class=\"math\">\\(N\\)</span>.</li>\n</ul>\n<p>I really enjoyed working on this post from end-to-end: writing the random walker class, finding a great python jackknife function from astropy (see the <a href=\"https://colab.research.google.com/drive/13GYlaTvO-Wu_3ep_Pa0mRZo-CYelDFmf\">colab notebook</a>), hacking my way through matplotlib, discovering the amazing Vineyard paper, and just generally exploring this incredibly rich problem that is so easy to state yet difficult to theoretically solve.  I’ll also note that the 2-D results are of particular interest to me given their applicability to the <a href=\"https://www.efavdb.com/world-wandering-dudes\">World Wandering Dudes</a> project that I have been working&nbsp;on.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": [
    "",
    "",
    ""
  ]
}