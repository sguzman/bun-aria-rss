{
  "title": "Hacking Language Models",
  "link": "https://malicious.life/hacking-language-models/",
  "pubDate": "Mon, 05 Sep 2022 15:01:56 +0000",
  "guid": "https://malicious.life/?p=1783",
  "comments": "https://malicious.life/hacking-language-models/#respond",
  "wfw:commentRss": "https://malicious.life/hacking-language-models/feed/",
  "slash:comments": 0,
  "category": "Uncategorized",
  "description": "Language models are everywhere today: they run in the background of Google Translate and other translation tools; they help operate voice assistants like Alexa or Siri; and most interestingly, they are available via several experiential projects trying to emulate natural conversations, such as OpenAI’s GPT-3 and Google’s LaMDA. Can these models be hacked to gain access to the sensitive information  they learned from their training data?",
  "content:encoded": "\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<p>Language models are everywhere today: they run in the background of Google Translate and other translation tools; they help operate voice assistants like Alexa or Siri; and most interestingly, they are available via several experiential projects trying to emulate natural conversations, such as OpenAI’s GPT-3 and Google’s LaMDA. Can these models be hacked to gain access to the sensitive information they learned from their training data? </p>\n\n\n\n<p></p>\n",
  "enclosure": "",
  "itunes:subtitle": "Language models are everywhere today: they run in the background of Google Translate and other translation tools; they help operate voice assistants like Alexa or Siri; and most interestingly, they are available via several experiential projects trying...",
  "itunes:summary": "Language models are everywhere today: they run in the background of Google Translate and other translation tools; they help operate voice assistants like Alexa or Siri; and most interestingly, they are available via several experiential projects trying to emulate natural conversations, such as OpenAI’s GPT-3 and Google’s LaMDA. Can these models be hacked to gain access to the sensitive information  they learned from their training data?",
  "itunes:author": "Cybereason",
  "itunes:image": "",
  "itunes:duration": "27:31"
}