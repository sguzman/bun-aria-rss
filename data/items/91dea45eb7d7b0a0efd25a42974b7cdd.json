{
  "title": "How I selected my starting word for Wordle using simulations and R",
  "link": "https://datascienceplus.com/how-i-selected-my-starting-word-for-wordle-using-simulations-and-r/",
  "comments": "https://datascienceplus.com/how-i-selected-my-starting-word-for-wordle-using-simulations-and-r/#respond",
  "dc:creator": "Bernardo Lares",
  "pubDate": "Thu, 24 Feb 2022 02:19:34 +0000",
  "category": [
    "Basic Statistics",
    "Data Visualisation",
    "lares"
  ],
  "guid": "https://datascienceplus.com/?p=31978",
  "description": "<div style=\"border-top: 1px solid; font-size: 14px;text-align: center; border-bottom: 1px solid; padding: 5px 2px;\"><a href=\"https://datascienceplus.com/posting-from-r-markdown-to-datascienceplus/\">Are you interested in guest posting? Publish at DataScience+  via your RStudio editor.</a></div><h2>Category</h2><ul><li><a href=\"https://datascienceplus.com/category/basic-statistics/\" rel=\"bookmark\" title=\"Permanent Link toBasic Statistics\">Basic Statistics</a></li></ul><h2>Tags</h2><ul><li><a href=\"https://datascienceplus.com/tag/data-visualisation/\" rel=\"bookmark\" title=\"Permanent Link toData Visualisation\">Data Visualisation</a></li><li><a href=\"https://datascienceplus.com/tag/lares/\" rel=\"bookmark\" title=\"Permanent Link tolares\">lares</a></li><li><a href=\"https://datascienceplus.com/tag/rstats/\" rel=\"bookmark\" title=\"Permanent Link toR Programming\">R Programming</a></li></ul>Wordle has been around for some time now and I think it&#8217;s not quite necessary to explain what it is and how to play it. (If I lost you there, please read more about it (and play) here). I&#8217;m not the best for wording games, thus I don&#8217;t find them quite entertaining. But, thinking of [&#8230;]<strong><p>Related Post</p></strong><ul><li><a href=\"https://datascienceplus.com/an-r-alternative-to-pairs-for-omics-qc/\" rel=\"bookmark\" title=\"Permanent Link toAn R alternative to pairs for -omics QC\">An R alternative to pairs for -omics QC</a></li><li><a href=\"https://datascienceplus.com/ditch-p-values-use-bootstrap-confidence-intervals-instead/\" rel=\"bookmark\" title=\"Permanent Link toDitch p-values. Use Bootstrap confidence intervals instead\">Ditch p-values. Use Bootstrap confidence intervals instead</a></li><li><a href=\"https://datascienceplus.com/introduction-for-decision-tree/\" rel=\"bookmark\" title=\"Permanent Link toIntroduction for Decision Tree\">Introduction for Decision Tree</a></li><li><a href=\"https://datascienceplus.com/correlation-vs-pps-in-python/\" rel=\"bookmark\" title=\"Permanent Link toCorrelation vs PPS in Python\">Correlation vs PPS in Python</a></li><li><a href=\"https://datascienceplus.com/earthquake-analysis-4-4-cluster-analysis/\" rel=\"bookmark\" title=\"Permanent Link toEarthquake Analysis (4/4): Cluster Analysis\">Earthquake Analysis (4/4): Cluster Analysis</a></li></ul>",
  "content:encoded": "<div style=\"border-top: 1px solid; font-size: 14px;text-align: center; border-bottom: 1px solid; padding: 5px 2px;\"><a href=\"https://datascienceplus.com/posting-from-r-markdown-to-datascienceplus/\">Are you interested in guest posting? Publish at DataScience+  via your RStudio editor.</a></div><h2>Category</h2><ul><li><a href=\"https://datascienceplus.com/category/basic-statistics/\" rel=\"bookmark\" title=\"Permanent Link toBasic Statistics\">Basic Statistics</a></li></ul><h2>Tags</h2><ul><li><a href=\"https://datascienceplus.com/tag/data-visualisation/\" rel=\"bookmark\" title=\"Permanent Link toData Visualisation\">Data Visualisation</a></li><li><a href=\"https://datascienceplus.com/tag/lares/\" rel=\"bookmark\" title=\"Permanent Link tolares\">lares</a></li><li><a href=\"https://datascienceplus.com/tag/rstats/\" rel=\"bookmark\" title=\"Permanent Link toR Programming\">R Programming</a></li></ul><p><strong>Wordle</strong> has been around for some time now and I think it&#8217;s not quite necessary to explain what it is and how to play it. (If I lost you there, please read more about it (and play) <a href=\"https://www.nytimes.com/games/wordle/index.html\" rel=\"noopener\" target=\"_blank\">here</a>). I&#8217;m not the best for wording games, thus I don&#8217;t find them quite entertaining. But, thinking of algorithmic ways to find puzzle solutions faster using R is! That&#8217;s why I started to think of random ideas regarding Wordle: for people who have played a lot, what&#8217;s their guessing word distribution look like? Are there better or worse words to start with? Are there significant more relevant letters that would be useful to guess your first try?</p>\n<p>I&#8217;ve seen some people answer similar questions after I started thinking on the matter, especially on most frequent letters by position (<a href=\"https://rviews.rstudio.com/2022/02/21/wordle-data-analysis/\" rel=\"noopener\" target=\"_blank\">post</a>), and a way to play and replicate the game using R (<a href=\"https://blog.ephorie.de/wordle-solve-wordle-with-r\" rel=\"noopener\" target=\"_blank\">post</a>).</p>\n<p>Keep in mind <strong>the &#8220;winner starting word&#8221; you find depends</strong> on:<br />\n1. the words you pick to evaluate as possible best words,<br />\n2. the words you are trying to predict and test toward,<br />\n3. the valid words randomly picked to guess, based on the set of seeds picked to select those words</p>\n<p>Yet, it gives us a winner solution.</p>\n<h3>1. Install and load <code>lares</code></h3>\n<p>Run <code>install.packages(\"lares\")</code> and then, load it with <code>library(\"lares\")</code>. No more packages needed from now on.</p>\n<h3>2. Select your starting point</h3>\n<p>Let&#8217;s pick some &#8220;good words&#8221; to see which ones are the best. I won&#8217;t get into details but I set a couple of rules based on letter by position frequency to set these initial words. There are 48 words that comply with these filters. I&#8217;m using some <a href=\"https://laresbernardo.github.io/lares/reference/scrabble.html\" rel=\"noopener\" target=\"_blank\">old Scrabble functions</a> I previously developed for the library for this purpose.</p>\n<pre>\nsome_words <- scrabble_words(\n  language = \"en\", # for English words\n  tiles = \"ESAORILTNU\", # 67% cumulative letters\n  force_n = 5, # comply Wordle universe of 5 letter words\n  force_start = \"s\", # 16% words start with S\n  force_str = \"e\")$word # most frequent letter\nsort(toupper(some_words))\n<em> [1] \"SAINE\" \"SALET\" \"SALUE\" \"SANER\" \"SAUTE\" \"SENOR\" \"SENTI\" ...\n</em>\n</pre>\n<p>We could sample some of them, manually pick those we like most, or simply use all if you&#8217;re patient enough to run the simulations:<br />\n<code>best_words <- toupper(some_words)</code></p>\n<p>Now, I picked 10 random words we are going to guess, starting from the &#8220;best words&#8221; we picked. The more we use, the better it&#8217;ll represent the universe of words (which is about 13K 5 letter words).</p>\n<pre>\nset.seed(2) # For reproducibility (same results)\ntest_words <- toupper(sample(wordle_dictionary(\"en\"), 10)); test_words # Words to guess randomly picked\n<em>  [1] \"BOZOS\" \"RESET\" \"TROWS\" \"SALTS\" \"JIBES\" \"YIRKS\" \"DURST\" \"SITES\" \"PENGO\" \"GARRE\"\n</em>\n</pre>\n<p>Finally, how many different random picking scenarios we&#8217;d like to test on each combination of best words and test words:<br />\n<code>seeds <- 20 # Number of different random picks to check distribution</code></p>\n<p>So basically expect for 480 iterations in total if you use these same settings. For me, it took about 30 minutes using my personal computer.<br />\n<code>print(length(best_words) * length(test_words), seeds)</code></p>\n<h3>3. Run simulations</h3>\n<p>Now that we already set our starting point, let&#8217;s run the simulations. </p>\n<pre>\nresults <- temp <- NULL\ntic(\"wordle_loop\")\nfor (word in best_words) {\n  cat(sprintf(\"- Seed word: %s [%s/%s]\\n\", word, which(word == best_words), length(best_words)))\n  for (objective in test_words) {\n    cat(sprintf(\"Guess %s [%s/%s] <= %s simulations: \", objective, which(objective == test_words), length(test_words), seeds))\n    temp <- wordle_simulation(word, objective, seed = 1:seeds, print = FALSE, quiet = TRUE)\n    results[[word]][[objective]] <- temp\n    cat(signif(mean(sapply(temp, function(x) x$iters)), 4), \"\\n\")\n  }\n}\ntoc(\"wordle_loop\")\n<em>- Seed word: SALUE [1/48]\nGuess BOZOS [1/10] <= 20 simulations: 6.95 \nGuess RESET [2/10] <= 20 simulations: 5.5 \nGuess TROWS [3/10] <= 20 simulations: 6.4 \nGuess SALTS [4/10] <= 20 simulations: 4.7 \nGuess JIBES [5/10] <= 20 simulations: 7.05 \nGuess YIRKS [6/10] <= 20 simulations: 6.55 \nGuess DURST [7/10] <= 20 simulations: 5.05 \nGuess SITES [8/10] <= 20 simulations: 6.95 \nGuess PENGO [9/10] <= 20 simulations: 5.3 \nGuess GARRE [10/10] <= 20 simulations: 6.05\n\n- Seed word: SILEN [2/48]\nGuess BOZOS [1/10] <= 20 simulations: 6.45 \n...\n</em>\n</pre>\n<h3>4. Gather results and check winners</h3>\n<p>Let&#8217;s check the sorted results to see the best and worst words.</p>\n<pre>\nbest_means <- lapply(results, function(a) sapply(a,function(x) mean(sapply(x, function(x) x$iters))))\nsort(sapply(best_means, mean))\n<em>STIRE SOARE SNARE STARE STORE STALE STEAL SNORE\n5.205 5.240 5.325 5.325 5.365 5.380 5.385 5.400 ...\n</em>\n</pre>\n<pre>\nhist(sapply(best_means, mean), breaks = 30)\n</pre>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM-1024x765.png\" alt=\"\" width=\"1024\" height=\"765\" class=\"aligncenter size-large wp-image-31985\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM-1024x765.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM-490x366.png 490w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM-768x574.png 768w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.22.28-PM.png 1494w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></p>\n<p>There&#8217;s a small range on these convergence means: <strong>(5.205, 5.890)</strong>. That means that all these words are similarly good or bad compared with each other. But, notice we can actually easily pick or split the best from the worst words with this methodology. </p>\n<p>To understand this point a bit more, let&#8217;s study a random benchmark: I picked 25 random words (not selected with the &#8220;best&#8221; words criteria) as my new <code>best_words <- toupper(sample(some_words, 25))</code>. Then, re-ran all the code with the same parameters and test words, for a total of 250 iterations, and got the following distribution. (Note: it took 18 minutes this time)</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM-1024x771.png\" alt=\"\" width=\"1024\" height=\"771\" class=\"aligncenter size-large wp-image-31986\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM-1024x771.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM-490x369.png 490w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM-768x579.png 768w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-1.54.04-PM.png 1468w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></p>\n<p>And theory confirmed. We did pick our first best words correctly given that the results given random words are really worse. Now the range covers a convergence mean of <strong>(5.700, 6.585)</strong>. Notice that the best random words are not quite within the best words range but for a few lucky cases. And the best of the best words converge ~1.4 guesses before the worst of the random words. So we can actually do something about it and use better words to start our game!</p>\n<h2>Final comments, asks, and considerations</h2>\n<p>&#8211; There are other <code>wordle_*</code> functions in the package you can use as dictionary, to actually play, and to run simulations. <a href=\"https://laresbernardo.github.io/lares/reference/wordle.html\" rel=\"noopener\" target=\"_blank\">Check out the nice colored results it prints</a>. In the examples there is a small demo on how you can play with a random un-known word without limits.<br />\n<a href=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-2.22.40-PM.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-2.22.40-PM.png\" alt=\"\" width=\"766\" height=\"462\" class=\"aligncenter size-full wp-image-31994\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-2.22.40-PM.png 766w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-2.22.40-PM-490x296.png 490w\" sizes=\"(max-width: 766px) 100vw, 766px\" /></a><br />\n&#8211; You probably won&#8217;t have a great difference on using one of the best picked words unless you play thousands of times: it&#8217;s more a matter of big numbers (and being smart on picking the right available words when guessing).<br />\n&#8211; You could try to re-run this analysis with a wider range of best words or test words to see if they can be improved. Note that the best words are the ones that converge sooner, thus lower iterations means.<br />\n&#8211; Expect to have these <code>wordle_*</code> functions updated in CRAN in a month or so (with nicer plots as well).<br />\n&#8211; Now, from the second input word onwards, it&#8217;s up to your picking skills. Feel free to check which words are left available using <a href=\"https://laresbernardo.github.io/lares/reference/scrabble.html\" rel=\"noopener\" target=\"_blank\">the <code>scrabble_words()</code> function</a>.</p>\n<h2>Bonus benchmarks with different &#8220;best words&#8221; criteria</h2>\n<p>I ran one more experiment, using parallel calculations on 24 cores (which returned results in ~10min for 96 words and 40 iterations scenario), and got to the same conclusions as before, regardless of some outliers. FYI: &#8220;ESAORILT&#8221; are the most frequent letters, in order.<br />\n1) (all) 24 words containing E, starting with S, using any of &#8220;ESAORILT&#8221; letters<br />\n2) 96 good words containing E, using any of &#8220;ESAORILT&#8221; letters<br />\n3) 96 random words</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM-1024x612.png\" alt=\"\" width=\"1024\" height=\"612\" class=\"aligncenter size-large wp-image-32006\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM-1024x612.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM-490x293.png 490w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM-768x459.png 768w, https://datascienceplus.com/wp-content/uploads/2022/02/Screen-Shot-2022-02-23-at-6.24.13-PM.png 1500w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><br />\n<i>If you are wondering what those lower words are (remember the randomness factor), you&#8217;d get SOARE, ALERT, and RAILE. And the worst ones? LOGOI, KEVEL, and YAFFS.</i></p>\n<strong><p>Related Post</p></strong><ul><li><a href=\"https://datascienceplus.com/an-r-alternative-to-pairs-for-omics-qc/\" rel=\"bookmark\" title=\"Permanent Link toAn R alternative to pairs for -omics QC\">An R alternative to pairs for -omics QC</a></li><li><a href=\"https://datascienceplus.com/ditch-p-values-use-bootstrap-confidence-intervals-instead/\" rel=\"bookmark\" title=\"Permanent Link toDitch p-values. Use Bootstrap confidence intervals instead\">Ditch p-values. Use Bootstrap confidence intervals instead</a></li><li><a href=\"https://datascienceplus.com/introduction-for-decision-tree/\" rel=\"bookmark\" title=\"Permanent Link toIntroduction for Decision Tree\">Introduction for Decision Tree</a></li><li><a href=\"https://datascienceplus.com/correlation-vs-pps-in-python/\" rel=\"bookmark\" title=\"Permanent Link toCorrelation vs PPS in Python\">Correlation vs PPS in Python</a></li><li><a href=\"https://datascienceplus.com/earthquake-analysis-4-4-cluster-analysis/\" rel=\"bookmark\" title=\"Permanent Link toEarthquake Analysis (4/4): Cluster Analysis\">Earthquake Analysis (4/4): Cluster Analysis</a></li></ul>",
  "wfw:commentRss": "https://datascienceplus.com/how-i-selected-my-starting-word-for-wordle-using-simulations-and-r/feed/",
  "slash:comments": 0
}