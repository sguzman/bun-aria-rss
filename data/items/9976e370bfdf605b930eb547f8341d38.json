{
  "title": "Torch: bleeding edge DNN research",
  "description": "<h2 id=\"torch\">Torch</h2>\n\n<p>You can find some background for this post here: <a href=\"/2015/05/04/Deep-Learning-with-Python.html\">Deep Learning with Python</a>!</p>\n\n<p>Torch has its strengths and its weaknesses. According to a wonderful write-up by <a href=\"https://plus.google.com/+TomaszMalisiewicz/posts\">Tomasz Malisiewicz</a> titled <a href=\"http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html\">Deep down the rabbit hole: CVPR 2015 and beyond</a>:</p>\n\n<blockquote>\n  <p>Caffe is much more popular that Torch, but when talking to some power users of Deep Learning (like <a href=\"https://plus.google.com/100209651993563042175\">+Andrej Karpathy</a> and other DeepMind scientists), a certain group of experts seems to be migrating from Caffe to Torch.</p>\n</blockquote>\n\n<p>I read somewhere else that Caffe : Torch :: Applications : Research. If you want to do serious research in deep learning, I would suggest using Torch given the level of current interest in the ecosystem, as well as Torch’s flexibility and platform. Facebook AI and Google DeepMind use Torch.</p>\n\n<p>The adept reader may be thinking, “wait a second… I thought this was a series of blog posts about doing deep learning with Python… but Torch is all Lua”, and yes, you are right, Torch is not a Python tool (though some parts of Torch have Python bindings): Torch, from a user’s perspective, is mostly Lua.</p>\n\n<p>For someone well-acquainted with Python, Lua isn’t so different. If doing deep learning is more important to you than what language you use, use Torch. If using Python is more important than using all of the potential horsepower available to you at your fingertips, then don’t use Torch, but just know that – as of this blog post – it’s increasingly the research tool of choice.</p>\n\n<h2 id=\"prerequisites\">Prerequisites</h2>\n\n<p>The documentation for Torch is great. They make installation as easy as a few lines of bash! Here’s a link to <a href=\"http://torch.ch\">Torch</a>.</p>\n\n<p>Let’s get started. Assuming you don’t mind installing torch into <code class=\"language-plaintext highlighter-rouge\">~/torch</code>, you can just use the following bash commands to get started.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>curl <span class=\"nt\">-sk</span> https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash\n<span class=\"nv\">$ </span>git clone https://github.com/torch/distro.git ~/torch <span class=\"nt\">--recursive</span>\n<span class=\"nv\">$ </span><span class=\"nb\">cd</span> ~/torch<span class=\"p\">;</span> ./install.sh\n</code></pre></div></div>\n\n<p>Now Torch, LuaJIT, LuaRocks (package manager akin to <code class=\"language-plaintext highlighter-rouge\">pip</code>), and some packages (installed via LuaRocks) are installed. Type <code class=\"language-plaintext highlighter-rouge\">th</code> to use the REPL, and in the REPL, you can type <code class=\"language-plaintext highlighter-rouge\">os.exit()</code> to quit.</p>\n\n<p>At the end you’ll be prompted to add Torch to your PATH environment variable. Type ‘yes’ to complete everything. Just do a quick <code class=\"language-plaintext highlighter-rouge\">source ~/.bashrc</code> to update your environment.</p>\n\n<p>Head to Torch’s <a href=\"http://torch.ch/docs/getting-started.html\">getting started</a> page for more.</p>\n\n<p>Now we’re ready to begin. Fasten your seltbelt: some crazy s*** is about to go down.</p>\n\n<h2 id=\"extending-torch-to-generate-joycean-prose\">Extending Torch to generate Joycean prose</h2>\n\n<p>What the… what does that even mean?</p>\n\n<p>I’ll show you.</p>\n\n<p>Andrej Karpathy’s very detailed and extremely interesting blog post, <em><a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a></em>, goes through several examples that harness code that he very kindly open-sourced <a href=\"https://github.com/karpathy/char-rnn\">here</a>, to implement a “multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for training/sampling from character-level language models.”</p>\n\n<p>To do that, <code class=\"language-plaintext highlighter-rouge\">git clone</code> the repo wherever you like. Then:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>luarocks <span class=\"nb\">install </span>nngraph \n<span class=\"nv\">$ </span>luarocks <span class=\"nb\">install </span>optim\n<span class=\"nv\">$ </span>luarocks <span class=\"nb\">install </span>cutorch <span class=\"c\"># for GPU use</span>\n<span class=\"nv\">$ </span>luarocks <span class=\"nb\">install </span>cunn    <span class=\"c\"># for GPU use</span>\n</code></pre></div></div>\n\n<p>OK. Awesome. Now that we’ve got that out of the way, let’s get some data:</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><img src=\"/assets/big_data.png\" alt=\"Big Data\" /></p>\n\n<table>\n  <tbody>\n    <tr>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Lulz. Let’s skip the big data for now and just start with something small: the full text of my favorite novel, James Joyce’s 1922 masterpiece, Ulysses. Full text available <a href=\"https://www.gutenberg.org/files/4300\">here</a> via one of the files types of your choosing (I chose .txt for this project). In a text editor, I removed the beginning/end of the file what I considered to be unreflective of Joyce, namely, the Project Gutenberg boilerplate :-). My file begins, famously:</p>\n\n<blockquote>\n  <p>– I –</p>\n\n  <p>Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of\nlather on which a mirror and a razor lay crossed.</p>\n</blockquote>\n\n<p>And ends, famously:</p>\n\n<blockquote>\n  <p>yes I said yes I will Yes.</p>\n\n  <p>Trieste-Zurich-Paris 1914-1921</p>\n</blockquote>\n\n<p>Sweet. Now navigate to your <code class=\"language-plaintext highlighter-rouge\">char-rnn</code> directory, and move your Ulysses text file to <code class=\"language-plaintext highlighter-rouge\">&lt;path&gt;/&lt;to&gt;/char-rnn/data/ulysses/input.txt</code> (obviously doing <code class=\"language-plaintext highlighter-rouge\">mkdir &lt;path&gt;/&lt;to&gt;/char-rnn/data/ulysses</code> first).</p>\n\n<p>If you want to look at some settings, you can type <code class=\"language-plaintext highlighter-rouge\">th train.lua -help</code>, otherwise, let’s start training on Ulysses.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>th train.lua <span class=\"nt\">-data_dir</span> data/ulysses <span class=\"nt\">-gpuid</span> <span class=\"nt\">-1</span> <span class=\"c\"># this goes against your CPU</span>\n<span class=\"nv\">$ </span>th train.lua <span class=\"nt\">-data_dir</span> data/ulysses \t\t<span class=\"c\"># this goes against your GPU</span>\n</code></pre></div></div>\n\n<p>If sucessful, you should see some output like this:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>1897/28850 <span class=\"o\">(</span>epoch 3.288<span class=\"o\">)</span>, train_loss <span class=\"o\">=</span> 1.76713313, grad/param norm <span class=\"o\">=</span> 9.8094e-02, <span class=\"nb\">time</span>/batch <span class=\"o\">=</span> 0.21s\n</code></pre></div></div>\n\n<p>Let’s sample from a training checkpoint and see what kind of text we generate.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">## When sampling, be consistent w.r.t. whether or not you trained/are training with your CPU or GPU.</span>\n<span class=\"nv\">$ </span>th sample.lua cv/some_checkpoint.t7 <span class=\"nt\">-gpuid</span> <span class=\"nt\">-1</span> <span class=\"c\"># if you trained against your CPU</span>\n<span class=\"nv\">$ </span>th sample.lua cv/some_checkpoint.t7 \t\t\t<span class=\"c\"># if you trained against your GPU</span>\n</code></pre></div></div>\n\n<p>Note that there are some markdown-esque characters in Project Gutenburg files, denoting common fomatting styles such as underline, <em>italics</em>, etc.</p>\n\n<p>Here’s our first sample:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>th sample.lua cv/lm_lstm_epoch1.73_1.9188.t7\n</code></pre></div></div>\n\n<blockquote>\n  <p>nned posted, his bind. He are so had. Mishing not eumal dy saye\ngap, Jesmotition, Hefleston foum his isence, Bloom, the peptlemer and callidant or yame of cersainitien. With Redellosy\nWisco oum for con. Maldrear sailly of exchochened liaty that in throum munders anutetoH icatiped _Koumban of falles aroumedupacelly)_ Jal. Noceping fer scontactrents?</p>\n\n  <p>–Comanen, felliits.\n Shourd comentlaned\non or whal onverfoul of wappen in that blinking awdactire of like\na bancaserable with m. Joy, E! I,, dlodnce good thet? Stubre he owald few of cloum. THy and more of\nthe\nvarss spewing how. What?</p>\n\n  <p>ut the brom in Bock Murigens, what earte up  vore.\nHerrom Goloonhy\ncrarks of the time he burth for me fleeterelfs him.</p>\n\n  <p>–Claper I saum. Learked of thit?</p>\n\n  <p>–On a silthing by smolled in Dra0 Comes beard.</p>\n\n  <p>Bliest _Ceven te moune, Frambly, sears have the druck, turt, some, Manch Cire u’blow. I house I west and yes? I’res\nbabladgow. Jneess of combolast and meeye._ Maloraga_.</p>\n\n  <p>You cipet dought\nwho\nca\nsumper herd claused. Lyformselting tumper. Ithere.</p>\n\n  <p>He your after urot!\nSwort up he siblar cappitites. Quains_ life to a sude her coucting then\nfeose, it wattersing thinsarding oot\nof\nDostle_)_ Who one sporial\nsp. Butnen it the sapined by Gulleruust pursan, Muss? Mome Ponain’s. Jesoliy, _10 Excudis amored he’s yel. _Thobe and pricty.</p>\n\n  <p>I movery’s to.</p>\n\n  <p>moned her have coman 4Dakit them man her are to yeard took to Detrarn yound more, Woundackel. And the bgoinalius Parman’s bushove bifferly,\nlarging toost)_ Goine of the nothing any suppencede_ lictedy groveby)_</p>\n\n  <p>_(Werrical mentovatubaly alking flames of conson\nis was diys.\nHat, they’ke of dest jegcises corsay:</p>\n\n  <p>_Wemstan’s naks 107rearminal gruttell and here to gusrouted or shunonil on that the to in temhord beasing hay Lovely, Mn Purninat.</p>\n\n  <p>U telloss aster. Dewained. Setherades)_ Hishur hand, Drisim, Hell Twander thack\n_Dousfuar prosy, doneson. Mound deatingsed, that pibst it on melughands and smul I make to enel in the comuty he and butterelan\nand</p>\n</blockquote>\n\n<p>I recognize some words in there! Wow, this almost looks like Finnegans Wake<sup id=\"fnref:0\" role=\"doc-noteref\"><a href=\"#fn:0\" class=\"footnote\">1</a></sup> and we’re just getting started. Or perhaps some form of primordial English/Anglo-Saxon. Leopold Bloom is in there, “Jesmotition” seems like a play on “Jesuit”, and it’s occasionally formatting italics properly in markdown with two <code class=\"language-plaintext highlighter-rouge\">_</code> (though it did not close its parentheses properly… yet).</p>\n\n<p>Here’s the best sample, after 50 training epochs. Ulysses is about ~1.5mb in size, so fairly small. Smaller than Karpathy’s Shakespeare contactenated training/validation file of ~4.4mb. The lowest validation error that was captured was after 17 epochs (after which it started to slightly overfit… the default parameters are small!).</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>th sample.lua data/ulysses/cv/lm_lstm_epoch17.33_1.5834.t7 <span class=\"nt\">-temperature</span> 1\n</code></pre></div></div>\n\n<blockquote>\n  <p>Power, Kinch, an, his dead reformed, for the churches hand.</p>\n\n  <p>They were namebox: a kitchen and perhage his sight on his canes deep any outwas, life\nstands. Clesser. A fellows her last firing. And beneather to him,\nthey give me 39: then he was brilliging bying. A lair unde for paper so\nfresh strangy gallous flashing at the crassies and\nthit about a son of their God’s kind. His arm.</p>\n\n  <p>She curaces you much interracied that common of yours. Passenear and he toteher. There\nand in I live for near them it spouched hers.</p>\n\n  <p>Becual left, her wall.</p>\n\n  <p>He is Lounata, the curtor, white hoaryses that gave Coimband, looked by\na hum, he\nwouldn distraction of Drwaphur, the drinken causing out for everybody holy\ngloriamed and stone.</p>\n\n  <p>Died’s patther pleaser, tomberful jung bless that on the door and\ngrunting for Pletic laudancy, signorian doing to the would. One a hard\nhe avaded him explaid, music hazebrakes vala oberous inquisition,\nand ruges grauts with special pupped letters in which      Buck Poile starts were up to them\nupon his great gizzard exchbumminesses:\nthe ebit passed pounds. Insaces. Molly, fallonerly, box to intertails.</p>\n\n  <p>Bloom works. Quick! Pollyman. An a lot it was seeming, mide, says, up and the rare borns at\nLeopolters! Cilleynan’s face. Childs hell my milk by their\ndoubt in thy last, unhall sit attracted with source\n     The door of Kildan\nand the followed their stowabout over that of three constant\ntrousantly Vinisis Henry Doysed and let up to a man with hands in surresses afraid quarts to here over\nsomeware as cup to a whie yellow accept thicks answer to me.</p>\n\n  <p>Hopping, solanist, cheying and they all differend and wears, widewpackquellen\ncumanstress, greets. Chrails her droken looked musicles reading and reason descorning\nfor the by Bloomford, swelling a scrarsuit by breed we mouth,\nthe past much turned by Borne.</p>\n\n  <p>Makers hear than, Moormar there, the first porter filsions.</p>\n\n  <p>What player well happened the last. A field stones,\nhalling shutualar of anylings, Abbo</p>\n</blockquote>\n\n<p>Wow, that’s stunning. “Brillig” makes me think of Jabberwocky, and there are some humorous gems in there. Notice how real sentence structures are starting to take shape and notice the references to multiple characters, including their nicknames. And the various portmonteaus and <em>jeu de mots</em>. Remember that this model trains at the character level and at the start of training didn’t know a thing about English or anything else relating to the structure of language or prose. Fascinating.</p>\n\n<p>How did we get this?</p>\n\n<p>One of the command-line arguments used for sampling is called <em>temperature</em>, whose flag is <code class=\"language-plaintext highlighter-rouge\">-t</code>. This can be a float between 0 and 1 (must be strictly &gt; 0). To give some intuition, temperature is in a sense the amount of creative license you are giving to your trained RNN while sampling from it. A temperature of 1 allows for the most creative license. This will give perhaps the most interesting results, but said results may not resemble (depending on the size of your network, the size of your data, and how it fits) something that is as readable as something sampled at a lower temperature. Conversely, lower temperature samples are more conservative: sampling at lower temperatures is more likely to result in samples that behave very nicely, yet they can even be <em>boring</em>. In some cases extremely low temperatures will cause self-repeating loops. Here is a very sad example where I have set the temperature = 0.1 (sad because Ulysses is so characteristically original and unrepetitive).</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>th sample.lua data/ulysses/cv/lm_lstm_epoch17.33_1.5834.t7 <span class=\"nt\">-temperature</span> 0.1\n</code></pre></div></div>\n\n<blockquote>\n  <p>nears, the street of the\nconstinction of the same of the same of the course of the street of the\nconstinical constinion of the constituting the sun and the priest of the\nconstinions of the best bearded the stage of the same of the same\nbright the state of the stage of the barrels and the barrels and\nthe street of the constituting the same of the stage of the same of the\nconstinction of the street of the course of the constituting the stairs\nof the course of the street of the barrels and the\nstates of the street of the same of the course of the course of the\nbell discussion of the course of the stage of the street of the\ncourse of the street of the barrels and the street of the stage of the\nconstinction of the course of the same of the course of the constituting the\nsecond and the same of the course of the same of the course of the\nlast of the street of the constable of the constituted the stairs of the\ncourse of the course of the last setter of the course of the same thing\nand the street of the course of the street of the course of the face of the\nconstinction of the course of the same of the stage of the street of the\nconstinction of the course of the barrels and the steps of the same of the\nconstinions of the course of the constituting the sea and the stage of the\nlast stranger of the stage of the street of the street and the street of the\nconstinction of the course of the street of the same of the state of the\nconstinical sure of the course of the same of the course of the stage of the\nconstinction of the same breath of the police of the same the same of the\nconstruction of the same of the course of the course of the last the\nstandance of the course of the street of the same of the darkers\nof the constable of the construction of the same of the street of the\nbellaman with the street of the course of the same of the course of the stairs\nof the course of the street of the course of the beauty of the\nconstruction of the constituting the same states of the course of</p>\n</blockquote>\n\n<p>Indeed this isn’t quite as fun. Notice how conservative our RNN has become w.r.t. sampling new words and structures. It’s not perfectly repetitive, but it sure reads like it. Remember, this is being sampled character-by-character.</p>\n\n<h2 id=\"what-to-do-next\">What to do next</h2>\n\n<p>You may be inspired to mashup your favorite authors. You may be inspired to train an RNN on Finnegans Wake.<sup id=\"fnref:2\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\">2</a></sup> <em>Klikkaklakkaklaskaklopatzklatschabattacreppycrottygraddaghsemmihsammihnouithappluddyappladdypkonpkot</em>. You may be inspired to mashup texts in more than one language. You may be inspired to train an RNN on code, or perhaps sheet music. Perhaps you may be inspired to train one on audio or video files.<sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\">3</a></sup></p>\n\n<p>Try training on a larger corpus. Try increasing the size of your network layers, as well as the number of layers. To paraphrase <a href=\"http://www.andrewng.org/\">Andrew Ng</a>, if your training error is too high, then add more rocket fuel (data), and if your test error is high, then add more rockets (i.e., increase the size of your deep neural network).</p>\n\n<p>In this post, we have only explored a single deep learning model: a recurrent neural network with a long short term memory. Torch is extremely flexible and can be used for (as far as I know) neural networks topologies represented by arbitrary directed acyclic graphs (DAGs) – though bi-directional RNNs and other valid DNN architectures seem to violate the DAG requirement – I need to learn more. If you’re a graph theory nerd like I am, that is pretty cool. In fact, that should be its own blog post.</p>\n\n<p>Another thing you can do is do a deep dive into the literature! Here is an excellent technical <a href=\"http://www.cs.toronto.edu/~graves/preprint.pdf\">primer</a><sup id=\"fnref:3\" role=\"doc-noteref\"><a href=\"#fn:3\" class=\"footnote\">4</a></sup>.</p>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:0\" role=\"doc-endnote\">\n      <p>To be fair, Finnegans Wake is more readable than this, but when I think of Finnegans Wake in the back of my mind, it is almost no different than this first example. Let’s see how successive Epochs bring our RNN closer to Ulysses. Next I’ll train an RNN on Finnegans Wake itself. Yikes! <a href=\"#fnref:0\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:2\" role=\"doc-endnote\">\n      <p>You are a brave, brave soul. Finnegans Wake itself looks like the output of an RNN with high temperature. <a href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:1\" role=\"doc-endnote\">\n      <p>Note: I am attempting to train on audio files right now and am seeing mixed results, which I think is due to the character-level training that this code is performing, and how that syncs up against very specific file types such as MIDI (especially multi-channel). <a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:3\" role=\"doc-endnote\">\n      <p>Graves, Alex. Supervised sequence labelling with recurrent neural networks. Vol. 385. Heidelberg: Springer, 2012. <a href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>",
  "pubDate": "Sun, 28 Jun 2015 11:49:40 +0000",
  "link": "http://korbonits.github.io/2015/06/28/Torch-bleeding-edge-DNN-research.html",
  "guid": "http://korbonits.github.io/2015/06/28/Torch-bleeding-edge-DNN-research.html"
}