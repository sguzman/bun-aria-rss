{
  "title": "Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR",
  "link": "https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/",
  "comments": "https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/#comments",
  "dc:creator": "Gianmario",
  "pubDate": "Thu, 21 May 2020 15:14:00 +0000",
  "category": [
    "Amazon EC2",
    "Big Data",
    "Cloud",
    "Machine Learning",
    "Natural Language Processing",
    "Python",
    "Spark",
    "Uncategorized",
    "distributed",
    "embeddings",
    "emr",
    "nlp",
    "tensorflow",
    "universalsentenceencoder"
  ],
  "guid": "https://datasciencevademecum.com/?p=2434",
  "description": "<p>Tensorflow HUB makes available a variety of pre-trained models ready to use for inference. A very powerful model is the (Multilingual) Universal Sentence Encoder that allows embedding bodies of text written in any language into a common numerical vector representation. Embedding text is a very powerful natural language processing (NLP) technique for extracting features from &#8230; <a href=\"https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR</span></a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/\">Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "content:encoded": "\n<p>Tensorflow HUB makes available a variety of pre-trained models ready to use for inference. A very powerful model is the <a href=\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\">(Multilingual) Universal Sentence Encoder </a>that allows embedding bodies of text written in any language into a common numerical vector representation.</p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/Universal-Sentence-Encoder.png?w=660&#038;ssl=1\" alt=\"Universal Sentence Encoder | Learn OpenCV\" data-recalc-dims=\"1\"/><figcaption>Universal Sentence Encoder </figcaption></figure>\n\n\n\n<p>Embedding text is a very powerful natural language processing (NLP) technique for extracting features from text fields. Those features can be used for training other models or for data analysis takes such as clustering documents or search engines based on word semantics.</p>\n\n\n\n<p>Unfortunately, if we have billions of text data to encode it might take several days to run on a single machine. <br>In this tutorial, I will show how to leverage Spark. In particular, we will use the AWS-managed Elastic MapReduce (EMR) service to apply the sentence encoder to a large dataset and complete it in a matter of a couple of hours.</p>\n\n\n\n<h2>Configuration</h2>\n\n\n\n<h3>EMR Cluster</h3>\n\n\n\n<p>In this example, we would assume a cluster of a Master node (r4.4xlarge) and 50 core nodes (r4.2xlarge spot instances). The cluster will have a total of 400 cores and ~3TB of theoretical memory. In practice, each executor will be limited by YARN to a maximum memory of ~52GB.</p>\n\n\n\n<p>If it is not affordable to spin a cluster with a lot of nodes, the total memory size of the cluster should not be a bottleneck as the Spark lazy execution mode would not require the whole dataset to be loaded in memory at the same time.</p>\n\n\n\n<p>In order to take full advantage of the EMR cluster resources we can conveniently use the property &#8220;maximizeResourceAllocation&#8221;. Moreover we also need to configure livy to not timeout our session (not required for spark-submit jobs).</p>\n\n\n\n<p>We can achieve both by specifying the following configuration:</p>\n\n\n\n<div class=\"is-layout-flow wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<pre class=\"wp-block-code\"><code>&#91;{\"classification\":\"spark\",\"properties\":{\"maximizeResourceAllocation\":\"true\"}},{\"classification\":\"livy-conf\",\"properties\":{\"livy.server.session.timeout-check\":\"false\"}}]</code></pre>\n</div></div>\n\n\n\n<p>I would also recommend to choose a recent release of emr-5.X and including at least the following software packages: Hadoop 2.8.5, Ganglia 3.7.2, Spark 2.4.4, Livy 0.6.0.</p>\n\n\n\n<p>Add the open-to-the-world security groups for the Master and Core nodes (this will be required to access the Spark UI and Ganglia in case the cluster is deployed in a VPC).</p>\n\n\n\n<h3>Spark session</h3>\n\n\n\n<p>Create an EMR Notebook and connect it to the previously created cluster. Before to create the session we need to tune some memory configurations.</p>\n\n\n\n<p>Since most of the computation and memory will be used by the python processes we need to change the memory balance between the JVM and python processes:</p>\n\n\n\n<figure class=\"wp-block-image\"><img decoding=\"async\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/gHMwv.jpg?w=660&#038;ssl=1\" alt=\"PySpark Internals\" data-recalc-dims=\"1\"/><figcaption>Source: <a href=\"https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals\">PySpark Internals</a></figcaption></figure>\n\n\n\n<p>The specified executor memory will only account for the JVM but not for the memory required by external processes, in our case TensorFlow.</p>\n\n\n\n<p>We need to tweak both the spark.yarn.executor.memoryOverhead to something greater than 10% of the spark.executor.memory as well as the allocated spark.python.worker.memory to avoid unnecessary disk spilling.</p>\n\n\n\n<p>We will start by configuring those YARN parameters before to start the Livy session by running in a notebook cell:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>%%configure -f\n{ \"conf\":{\n\"spark.pyspark.python\": \"python3\",\n\"spark.pyspark.virtualenv.enabled\": \"true\",\n\"spark.pyspark.virtualenv.type\":\"native\",\n\"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n\"spark.executor.memory\": \"50g\",\n\"spark.yarn.executor.memoryOverhead\": \"12000\",\n\"spark.python.worker.memory\": \"10g\"    \n}}</code></pre>\n\n\n\n<p>Out of 61GB available, we allocated 10GB for the python workers, 50GB to the JVM of which 12GB of overhead.</p>\n\n\n\n<p>More details on configuration tuning in <a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/\">Best practices for successfully managing memory for Apache Spark applications on Amazon EMR</a>.</p>\n\n\n\n<p>Now can create a session executing a cell containing the spark context object:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"202\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/1_image.png?resize=660%2C202&#038;ssl=1\" alt=\"\" class=\"wp-image-2435\" data-recalc-dims=\"1\"/><figcaption>Livy info widget</figcaption></figure>\n\n\n\n<h3>Dependency management</h3>\n\n\n\n<p>AWS did a good job of making it easier to install libraries at runtime without having to write custom bootstrap actions or AMIs. We can install packages in both the master and core nodes using the <a href=\"https://aws.amazon.com/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/\">install_pypi_package API</a>:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>for package in &#91;\"pandas==0.25\", \"tensorflow==2.1.0\", \"tensorflow_text==2.1.1\", \"tensorflow-hub==0.7.0\"]:\n    sc.install_pypi_package(package)\n\nsc.list_packages()</code></pre>\n\n\n\n<p>It will install the provided packages and print out the list of installed packages in the python 3.6 virtual env.</p>\n\n\n\n<p>Note: In Hadoop 3.0 (EMR 6.x) it should be possible to deploy a Spark cluster in Docker containers but I have not tried yet.</p>\n\n\n\n<h2>Embedding job</h2>\n\n\n\n<p>We need to have data loaded as a Spark DataFrame with a key column and a text column.</p>\n\n\n\n<p>The embedding job would conceptually do the followings:</p>\n\n\n\n<ol><li>download the TensorFlow multilingual universal sentence encoder model</li><li>slice the data partitions into chunks of text documents</li><li>embed each chunk in a NumPy matrix</li><li>convert the matrix into a list of spark.sql.Row objects</li></ol>\n\n\n\n<pre class=\"wp-block-code\"><code>muse_columns = &#91;f\"muse_{(format(x, '03'))}\" for x in range(512)]\n\ndef get_embedding_batch(batch, model, id_col, text_col, muse_columns):\n    \n    rows = &#91;row for row in batch if row&#91;text_col] is not None and len(row&#91;text_col].split(\" \")) >=3]\n    if len(rows) == 0:\n        return &#91;]\n    \n    from pyspark.sql import Row\n    \n    EmbeddingRow = Row(id_col, *muse_columns)\n    \n    keys = &#91;x&#91;id_col] for x in rows]\n    text = &#91;x&#91;text_col] for x in rows]\n    \n    embedding_mat = model(text).numpy()\n    return &#91;EmbeddingRow(keys&#91;i], *embedding_mat&#91;i, :].reshape(-1).tolist()) for i in range(len(keys))]\n\n\ndef chunks(iterable, n=10):\n    from itertools import chain, islice\n    iterator = iter(iterable)\n    for first in iterator:\n        yield chain(&#91;first], islice(iterator, n - 1))\n\n\ndef get_embedding_batch_gen(batch, \n                            id_col, \n                            text_col, \n                            muse_columns=muse_columns,\n                            chunk_size=1000):\n    import tensorflow_hub as hub\n    import tensorflow_text\n    \n    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n    chunk_iter = chunks(batch, n=chunk_size)\n    \n    for chunk in chunk_iter:\n        for row in get_embedding_batch(batch=chunk, model=model, id_col=id_col, \n                                       text_col=text_col, muse_columns=muse_columns):\n            yield row</code></pre>\n\n\n\n<p>A few gotchas:</p>\n\n\n\n<ul><li>The model was downloaded and instantiated only once; alternatively, we could have used Spark native broadcast variables.</li><li>In order to make the model working at run-time, we first had to import tensorflow_text in each executor</li><li>We transformed an iterable of Row objects to an iterable of Row objects by only materializing one chunk of 1000 rows per time.</li><li>We discarded any sentence with less than 3 tokens.</li><li>The numpy float32 type is not compatible with the Spark DoubleType; thus, it must be converted into a float first.</li></ul>\n\n\n\n<h3>Toy example</h3>\n\n\n\n<p>Let&#8217;s try this code with a small data sample:</p>\n\n\n\n<pre class=\"wp-block-code\"><code>english_sentences = &#91;\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\nitalian_sentences = &#91;\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\njapanese_sentences = &#91;\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]\n\nsentences = english_sentences + italian_sentences + japanese_sentences</code></pre>\n\n\n\n<p>Now we can run inference in batches using the mapPartitions API and then convert the results in a Spark DataFrame containing the key column and the 512 muse embedding columns.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StringType, FloatType\nfrom pyspark.sql import Row\nfrom functools import partial\nsentences = &#91;Row(id=i, text=sentence) for i, sentence in enumerate(sentences)]\n\nsentence_embeddings_rdd = sc.parallelize(sentences).mapPartitions(partial(get_embedding_batch_gen, id_col='id', text_col='text'))\n\nschema = StructType(&#91;StructField('id', StringType(), False)] + &#91;StructField(col, FloatType(), False) \n                                                                for col in muse_columns])\nsentence_embeddings_df = sqlContext.createDataFrame(sentence_embeddings_rdd, schema)</code></pre>\n\n\n\n<p>In the above example we manually specify the schema to avoid the slowdown of the dynamic schema inference.</p>\n\n\n\n<h3>Partitioning</h3>\n\n\n\n<p>The toy example should work straight away because of the data sample is very tiny. If we have to make it scale for very large datasets we want to neither hit OutOfMemory errors nor store the output in many thousands of small parts.<br>We may want to resize the partitions of the sentences RDD (order of tens of thousands) before to map them and coalesce the embedding data frame to something reasonably small (a few hundred partitions) just before to write to the storage layer and reduce the number of output files.</p>\n\n\n\n<p>Please note that the chunk size is slicing the partition to make tensors not too large when doing the inference but they don&#8217;t guarantee that the executor would not hold the whole partition in memory.</p>\n\n\n\n<p>E.g. A dataset of 1billion text documents can be divided into 5k partitions of 200k documents each which mean each partition will contain roughly 200 sequential chunks. The output should be saved as parquet files of 400 parts.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>sentence_embeddings_rdd = large_text_corpus_rdd.repartition(5000).mapPartitions(embedding_generator_function)\nlarge_text_corpus_df = sqlContext.createDataFrame(large_text_corpus_rdd, schema)\nlarge_text_corpus_df.coalesce(400).write.option(\"compression\", \"snappy\").parquet(output_path)</code></pre>\n\n\n\n<p>And that&#8217;s it, you can monitor the Spark job and eventually access the embedding divided into 400 almost equally sized parts in parquet format and compressed with snappy.</p>\n\n\n\n<h2>Monitoring</h2>\n\n\n\n<p>The main tool for monitoring a Spark job are its UI and Ganglia. </p>\n\n\n\n<h3>Spark UI</h3>\n\n\n\n<p>If we execute %info in the Jupyter notebook to get the list of current and past Livy sessions. From the widget, you also get the links to the Spark UI and the master hostname (from where you can access Ganglia at http://master_hostname/ganglia/). We will need to access those servers through a proxy in case the cluster was deployed in a private network.</p>\n\n\n\n<p>From the Spark UI, we would expect a computation graph like the following:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"1005\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/1_image-2.png?resize=660%2C1005&#038;ssl=1\" alt=\"\" class=\"wp-image-2439\" data-recalc-dims=\"1\"/><figcaption>DAG Visualization of the Spark job</figcaption></figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"108\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/1_image-3.png?resize=660%2C108&#038;ssl=1\" alt=\"\" class=\"wp-image-2440\" data-recalc-dims=\"1\"/><figcaption>Stages of the Spark job</figcaption></figure>\n\n\n\n<p>We can observe the two levels of repartitioning: stage 32 repartition the data before the model inference and stage 33 would repartition before the write operation.</p>\n\n\n\n<h3>Ganglia</h3>\n\n\n\n<p>If you open the Ganglia UI , and did everything correctly, you should expect to see something like this:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"751\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/05/1_image-4.png?resize=660%2C751&#038;ssl=1\" alt=\"\" class=\"wp-image-2457\" data-recalc-dims=\"1\"/><figcaption>Ganglia overview of the cluster</figcaption></figure>\n\n\n\n<p>If you experience a large imbalance between memory usage and CPU usage you may want to change the instance types to a more computation-optimized family rather than the r4. </p>\n\n\n\n<p>The execution time of each task was approximately 20minutes for 80k text sentences each and considering 8 tasks would be executed in concurrency within the same executor.</p>\n\n\n\n<h2>Conclusions</h2>\n\n\n\n<p>This approach can be adapted for running model inference with any machine learning library at any scale. The usage of EMR with spot instances will make it fast and cheap. We used the EMR notebook for convenience but you can wrap the same logic into a spark-submit job and use bootstrap actions for installing packages.</p>\n\n\n\n<p>In addition to storing the embeddings in the form of a data frame, you could also extend the code for storing the raw tensors of each partition and load them into TensorBoard for efficient 3-dimensional visualization.</p>\n\n\n\n<p>If instead, you are looking to ways to run TensorFlow in distributed mode on top of Spark you have to use a different architecture like explained in the article <a href=\"https://towardsdatascience.com/scaling-up-with-distributed-tensorflow-on-spark-afc3655d8f95\">Scaling up with Distributed Tensorflow on Spark</a>.</p>\n\n\n\n<p>Please leave your comments and subscribe to stay up-to-date to the next tutorials.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/\">Embedding billions of text documents using Tensorflow Universal Sentence Encoder and Spark EMR</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "wfw:commentRss": "https://datasciencevademecum.com/2020/05/21/embedding-billions-of-text-documents-using-tensorflow-universal-sentence-encoder-on-top-of-spark-emr/feed/",
  "slash:comments": 2,
  "post-id": 2434
}