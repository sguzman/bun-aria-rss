{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-4364275298594755861",
  "published": "2022-10-04T11:43:00.005-07:00",
  "updated": "2022-10-22T19:51:19.210-07:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "Large Motion Frame Interpolation",
  "content": "<span class=\"byline-author\">Posted by Fitsum Reda and Janne Kontkanen, Google Research</span> <p><a href=\"https://en.wikipedia.org/wiki/Motion_interpolation\">Frame interpolation</a> is the process of synthesizing in-between images from a given set of images. The technique is often used for <a href=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Frame_rate%23Frame_rate_up-conversion&amp;sa=D&amp;source=docs&amp;ust=1664322285856563&amp;usg=AOvVaw1YyWoW7xD9EqEtOSAjIKSD\">temporal up-sampling</a> to increase the refresh rate of videos or to create slow motion effects. Nowadays, with digital cameras and smartphones, we often take several photos within a few seconds to capture the best picture. Interpolating between these “near-duplicate” photos can lead to engaging videos that reveal scene motion, often delivering an even more pleasing sense of the moment than the original photos.  </p><a name='more'></a>  <p>Frame interpolation between consecutive video frames, which often have small motion, has been studied extensively. Unlike videos, however, the temporal spacing between near-duplicate photos can be several seconds, with commensurately large in-between motion, which is a major failing point of existing frame interpolation methods. Recent methods attempt to handle large motion by training on datasets with <a href=\"https://arxiv.org/abs/2103.16206\">extreme motion</a>, albeit with limited effectiveness on <a href=\"https://arxiv.org/abs/2108.06815\">smaller motions</a>. </p> <p>In “<a href=\"https://arxiv.org/pdf/2202.04901.pdf\">FILM: Frame Interpolation for Large Motion</a>”, published at <a href=\"https://eccv2022.ecva.net/\">ECCV 2022</a>, we present a method to create high quality slow-motion videos from near-duplicate photos. FILM is a new neural network architecture that achieves state-of-the-art results in large motion, while also handling smaller motions well. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s1920/image3.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1080\" data-original-width=\"1920\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRIrNGJ_ULWX3iM63cdI0H6CJx9200rUy_yxYJbT15kNRlAkgiZ4o_8NfWyfZ2GnTusotAQXxMjVkhh0zvOW92meJkwRniD2Fh6DyvZukZDOo_ZujCcsos37LOeE9rpZlyh2VNFYVMa5WapuSpCxdTJ9UWVBLZvRdlup7ACXPlq_zhR6tyE8KoUpsg0g/s16000/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">FILM interpolating between two near-duplicate photos to create a slow motion video.</td></tr></tbody></table><h2>FILM Model Overview</h2><p>The FILM model takes two images as input and outputs a middle image. At inference time, we recursively invoke the model to output in-between images. FILM has three components: (1) A feature extractor that summarizes each input image with deep multi-scale (<a href=\"https://arxiv.org/abs/1612.03144\">pyramid</a>) features; (2) a bi-directional motion estimator that computes pixel-wise motion (i.e., flows) at each pyramid level; and (3) a fusion module that outputs the final interpolated image. We train FILM on regular video frame triplets, with the middle frame serving as the ground-truth for supervision. </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s530/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"530\" data-original-width=\"319\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiblix_Ee8FHA04AR4SV3sL5MhDduduPcIDc22CjitRMyqzZqrZh7_sAt32LsNSkjljYp1gr-tjKsZsOq7oMOkjSJbm8kqUYp750Jjiboa2dnRVT4QKR1Pi6fNc4Km-C6K-atgssRffENi7OfNAO7S4-YZsKdkpMZTSCJM5ztk5ACQDBuhorhwD-f3_Ng/s16000/image4.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A standard<a href=\"https://arxiv.org/abs/1612.03144\"> feature pyramid</a> extraction on two input images. Features are processed at each level by a series of convolutions, which are then downsampled to half the spatial resolution and passed as input to the deeper level.</td></tr></tbody></table> <h2>Scale-Agnostic Feature Extraction </h2><p>Large motion is typically handled with hierarchical motion estimation using multi-resolution feature pyramids (shown above). However, this method struggles with small and fast-moving objects because they can disappear at the deepest pyramid levels. In addition, there are far fewer available pixels to derive supervision at the deepest level. </p> <p>To overcome these limitations, we adopt a <a href=\"https://augmentedperception.github.io/pixelfusion/\">feature extractor</a> that shares weights across scales to create a “scale-agnostic” feature pyramid. This feature extractor (1) allows the use of a shared motion estimator across pyramid levels (next section) by equating large motion at shallow levels with small motion at deeper levels, and (2) creates a compact network with fewer weights.  </p> <p>Specifically, given two input images, we first create an image pyramid by successively downsampling each image. Next, we use a shared <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1\">U-Net</a> convolutional encoder to extract a smaller feature pyramid from each image pyramid level (columns in the figure below). As the third and final step, we construct a scale-agnostic feature pyramid by horizontally concatenating features from different convolution layers that have the same spatial dimensions. Note that from the third level onwards, the feature stack is constructed with the same set of shared convolution weights (shown in the same color). This ensures that all features are similar, which allows us to continue to share weights in the subsequent motion estimator. The figure below depicts this process using four pyramid levels, but in practice, we use seven. </p> <div style=\"line-height:40%;\">    <br></div><h2>Bi-directional Flow Estimation </h2><p>After feature extraction, FILM performs pyramid-based residual flow estimation to compute the flows from the yet-to-be-predicted middle image to the two inputs. The flow estimation is done once for each input, starting from the deepest level, using a stack of convolutions. We estimate the flow at a given level by adding a residual correction to the upsampled estimate from the next deeper level. This approach takes the following as its input: (1) the features from the first input at that level, and (2) the features of the second input after it is warped with the upsampled estimate. The same convolution weights are shared across all levels, except for the two finest levels. </p> <p>Shared weights allow the interpretation of small motions at deeper levels to be the same as large motions at shallow levels, boosting the number of pixels available for large motion supervision. Additionally, shared weights not only enable the training of powerful models that may reach a higher <a href=\"https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\">peak signal-to-noise ratio</a> (PSNR), but are also needed to enable models to fit into GPU memory for practical applications.</p>  <table align=\"center\" cellpadding=\"2\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s768/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"768\" data-original-width=\"579\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgcbs3LhMChr0VhdiLdluPnPIopUdLH6_Oq-iRNjcirN6flBWD-hmOjvEPduIym1aW5fO_gPOlkP_LwXxPqYWpVq2Yg0RxJ0rHY4BpjdDN7RLJg6emzHvaZBVFkeGtDs8nn9poGdmX6Om-4kP5E1KR2k3J-N_ERqGK0LRFugWnEbIamBGeatWPLdq1cww/s16000/image1.png\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s768/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"768\" data-original-width=\"571\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZjPZaKydxStrnc2i8NAsN7UUl2qYFSe2GcI5-ZBxt1RCe-4xoxI3gB8KjmoRh9y2YXg_kDL4OEwXAQpKpRXBhQ50fSuhIO6JCj1GkvOCkw6Bu3y__bBJFCH28V6u9cK27mAEsEXQcwrBWKZ_ZnNrNU0Vex5ZlQOCoUEwrWPRR-R76rdAQmaqUCAIXLA/s16000/image4.png\" /></a></td></tr></tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">The impact of weight sharing on image quality. <b>Left</b>: no sharing, <b>Right</b>: sharing. For this ablation we used a smaller version of our model (called FILM-med in the <a href=\"https://arxiv.org/pdf/2202.04901.pdf\">paper</a>) because the full model without weight sharing would diverge as the regularization benefit of weight sharing was lost.</td></tr></tbody></table> <h2>Fusion and Frame Generation</h2><p>Once the bi-directional flows are estimated, we warp the two feature pyramids into alignment. We obtain a concatenated feature pyramid by stacking, at each pyramid level, the two aligned feature maps, the bi-directional flows and the input images. Finally, a <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28#Bib1\">U-Net</a> decoder synthesizes the interpolated output image from the aligned and stacked feature pyramid.  </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s1600/image9.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1000\" data-original-width=\"1600\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0SmdFNEGohaKfz2VoKw15mX92A_62xLRmPB7DAMWZqRGFdV9EYkTQpGZo6gIgTHstm2Bxd_suOyFZtAf6o6MF3esTx6jJ_gWf4-1E7EoHnDPx7RkgaozqKBSBja8uKgHkbXUVV4uZ3FdPFjwicqqI7rhGNGPtDomn1o4lYdiMZe8HtFnV9cjF42FjMA/s16000/image9.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">FILM Architecture. <b>FEATURE EXTRACTION</b>: we extract scale-agnostic features. The features with matching colors are extracted using shared weights. <b>FLOW ESTIMATION</b>: we compute bi-directional flows using shared weights across the deeper pyramid levels and warp the features into alignment. <b>FUSION</b>: A U-Net decoder outputs the final interpolated frame.</td></tr></tbody></table> <h2>Loss Functions </h2><p>During training, we supervise FILM by combining three losses. First, we use the <a href=\"https://en.wikipedia.org/wiki/Least_absolute_deviations\">absolute L1</a> difference between the predicted and ground-truth frames to capture the motion between input images. However, this produces blurry images when used alone. Second, we use <a href=\"https://arxiv.org/abs/1603.08155\">perceptual loss</a> to improve image fidelity. This minimizes the L1 difference between the <a href=\"https://www.image-net.org/\">ImageNet</a> pre-trained <a href=\"https://arxiv.org/abs/1409.1556\">VGG-19</a> features extracted from the predicted and ground truth frames. Third, we use <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\">Style loss</a> to minimize the L2 difference between the <a href=\"https://en.wikipedia.org/wiki/Gram_matrix\">Gram matrix</a> of the ImageNet pre-trained VGG-19 features. The Style loss enables the network to produce sharp images and realistic <a href=\"https://en.wikipedia.org/wiki/Inpainting\">inpaintings</a> of large pre-occluded regions. Finally, the losses are combined with weights empirically selected such that each loss contributes equally to the total loss. </p>  <p>Shown below, the combined loss greatly improves sharpness and image fidelity when compared to training FILM with L1 loss and VGG losses. The combined loss maintains the sharpness of the tree leaves.  </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s1629/image8.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"640\" data-original-width=\"1629\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmVUdzOeQQv4NG9pIYl0zzzg0b39-R1XT_1-r7EXNwBx4SaAZuFqkJ9MdS-HTwSVDaUEIkZwqhoSR7qA5jhHk2gX076TkrRKBt9G4MJqKpHuh4o6ChvuKJep_ew7uPDEZw_Y0DlVApCDWaZHsLubJfd8PI7Hp2aiBTWgekdGd3vjNwzWAk1i9mXGIp1Q/s16000/image8.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">FILM’s combined loss functions. L1 loss (<b>left</b>), L1 plus VGG loss (<b>middle</b>), and Style loss (<b>right</b>), showing significant sharpness improvements (green box).</td></tr></tbody></table> <h2>Image and Video Results</h2><p>We evaluate FILM on an internal near-duplicate photos dataset that exhibits large scene motion. Additionally, we compare FILM to recent frame interpolation methods: <a href=\"https://arxiv.org/abs/2003.05534\">SoftSplat</a> and <a href=\"https://arxiv.org/abs/2108.06815\">ABME</a>. FILM performs favorably when interpolating across large motion. Even in the presence of motion as large as 100 pixels, FILM generates sharp images consistent with the inputs.</p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s1897/image6.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"909\" data-original-width=\"1897\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnJ4WEQIhreXe_RRfbGtfyETixhFV36yKFC9LU0RnixWuYVhiBELkVkxThn04uLzpUFBljXR3arlME9wX_wSXdIa1s8aEX3LVjrQG7xL4ao9UJVQYDZS7xzPjucXX_gS627x2oNo6s4XX3xAKEjLQBcxiUJx0TNMQM0Gf48QDLITpstfo3GdDbukbpIg/s16000/image6.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Frame interpolation with SoftSplat (<b>left</b>), ABME (<b>middle</b>) and FILM (<b>right</b>) showing favorable image quality and temporal consistency. </td></tr></tbody></table> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s1896/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"909\" data-original-width=\"1896\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhwFw0mLZbtSiumwE8BQfqmGmElkNFqdeVFsFdeZIO6jG12pHvnfompGPb2J6XZmCgrpbYN76Q4b7VXmK3Jk8csJ_23JdvI75QjGYjM5XRcKA0-7JyGQFF2zUsBaztihQYAJ7gJJCkRrZVi2lXey8KfoTakpqbk9UeC3YteI8uzP5wSZDCInnmRHv10cg/s16000/image3.gif\" /></a></td></tr></tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s1174/image5.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"756\" data-original-width=\"1174\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh30uP1TzIx05mFEbwmhDa0LVzvLWGI5Y0PD4mUjLd5o4XbjryGniowokFDuFBBgqe2vYPsokkEajRrq_0PNbQt21nKkoitX0YwqWp2gzFR4VbDSK-A1jWhoObZmw9CbZY6WY8ymj4P5f78kcewYIPgSE4IVOBFSmzOJ-glnLsMzC9ZzhPOW7RECIsT_A/s16000/image5.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Large motion interpolation. <b>Top</b>: 64x slow motion video. <b>Bottom</b> (left to right): The two input images blended, SoftSplat interpolation, ABME interpolation, and FILM interpolation. FILM captures the dog’s face while maintaining the background details.</td></tr></tbody></table> <h2>Conclusion</h2><p>We introduce FILM, a large motion frame interpolation neural network. At its core, FILM adopts a scale-agnostic feature pyramid that shares weights across scales, which allows us to build a “scale-agnostic” bi-directional motion estimator that learns from frames with normal motion and generalizes well to frames with large motion. To handle wide disocclusions caused by large scene motion, we supervise FILM by matching the Gram matrix of ImageNet pre-trained VGG-19 features, which results in realistic inpainting and crisp images. FILM performs favorably on large motion, while also handling small and medium motions well, and generates temporally smooth high quality videos. </p> <div style=\"line-height:40%;\">    <br></div><h2>Try It Out Yourself</h2><p>You can try out FILM on your photos using the <a href=\"https://film-net.github.io\">source code</a>, which is now publicly available. </p> <div style=\"line-height:40%;\">    <br></div><h2>Acknowledgements</h2><p><i>We would like to thank Eric Tabellion, Deqing Sun, Caroline Pantofaru, Brian Curless for their contributions. We thank Marc Comino Trinidad for his contributions on the scale-agnostic feature extractor, Orly Liba and Charles Herrmann for feedback on the text, Jamie Aspinall for the imagery in the paper, Dominik Kaeser, Yael Pritch, Michael Nechyba, William T. Freeman, David Salesin, Catherine Wah, and Ira Kemelmacher-Shlizerman for support. Thanks to Tom Small for creating the animated diagram in this post.</i>  </p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}