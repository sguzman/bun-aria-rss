{
  "title": "Finding the K in K-Means Clustering",
  "link": "https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/",
  "comments": "https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/#comments",
  "dc:creator": "datasciencelab",
  "pubDate": "Fri, 27 Dec 2013 11:37:54 +0000",
  "category": [
    "Experiments",
    "clustering",
    "gap statistic",
    "k-means",
    "python"
  ],
  "guid": "http://datasciencelab.wordpress.com/?p=354",
  "description": "A couple of weeks ago, here at The Data Science Lab we showed how Lloyd&#8217;s algorithm can be used to cluster points using k-means with a simple python implementation. We also produced interesting visualizations of the Voronoi tessellation induced by the clustering. At the end of the post we hinted at some of the shortcomings [&#8230;]",
  "content:encoded": "<p>A couple of weeks ago, here at The Data Science Lab we showed <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">how Lloyd&#8217;s algorithm can be used to cluster points using k-means</a> with a simple python implementation. We also produced <a href=\"https://datasciencelab.files.wordpress.com/2013/12/p_n2000_k15_.gif\">interesting visualizations</a> of the Voronoi tessellation induced by the clustering. At the end of the post we hinted at some of the shortcomings of this clustering procedure. The basic k-means is an extremely simple and efficient algorithm. However, it assumes prior knowledge of the data in order to choose the appropriate <em>K</em>. Other disadvantages are the sensitivity of the final clusters to the selection of the initial centroids and the fact that the algorithm can produce empty clusters. In today&#8217;s post, and <a href=\"https://twitter.com/EmilStenstrom/status/412351119281491968\">by popular request</a>, we are going to have a look at the first question, namely how to find the appropriate <em>K</em> to use in the k-means clustering procedure.</p>\n<h3>Meaning and purpose of clustering, and the elbow method</h3>\n<p>Clustering consist of grouping objects in sets, such that objects within a cluster are as similar as possible, whereas objects from different clusters are as dissimilar as possible. Thus, the optimal clustering is somehow subjective and dependent on the characteristic used for determining similarities, as well as on the level of detail required from the partitions. For the purpose of our <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">clustering experiment</a> we use clusters derived from <a href=\"http://en.wikipedia.org/wiki/Normal_distribution\">Gaussian distributions</a>, i.e. globular in nature, and look only at the usual definition of <a href=\"http://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean distance</a> between points in a two-dimensional space to determine intra- and inter-cluster similarity.</p>\n<p>The following measure represents the sum of intra-cluster distances between points in a given cluster <img src=\"https://s0.wp.com/latex.php?latex=C_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_k\" class=\"latex\" /> containing <img src=\"https://s0.wp.com/latex.php?latex=n_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=n_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=n_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"n_k\" class=\"latex\" /> points:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D_k+%3D+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%5Csum_%7B%5Cmathrm%7Bx%7D_j+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmathrm%7Bx%7D_j+%7C%7C%5E2+%3D+2+n_k+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmu_k+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D_k+%3D+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%5Csum_%7B%5Cmathrm%7Bx%7D_j+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmathrm%7Bx%7D_j+%7C%7C%5E2+%3D+2+n_k+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmu_k+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D_k+%3D+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%5Csum_%7B%5Cmathrm%7Bx%7D_j+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmathrm%7Bx%7D_j+%7C%7C%5E2+%3D+2+n_k+%5Csum_%7B%5Cmathrm%7Bx%7D_i+%5Cin+C_k%7D+%7C%7C%5Cmathrm%7Bx%7D_i+-+%5Cmu_k+%7C%7C%5E2&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle D_k = &#92;sum_{&#92;mathrm{x}_i &#92;in C_k} &#92;sum_{&#92;mathrm{x}_j &#92;in C_k} ||&#92;mathrm{x}_i - &#92;mathrm{x}_j ||^2 = 2 n_k &#92;sum_{&#92;mathrm{x}_i &#92;in C_k} ||&#92;mathrm{x}_i - &#92;mu_k ||^2\" class=\"latex\" />.</p>\n<p>Adding the normalized intra-cluster sums of squares gives a measure of the compactness of our clustering:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+W_k+%3D+%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B1%7D%7B2n_k%7D+D_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+W_k+%3D+%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B1%7D%7B2n_k%7D+D_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+W_k+%3D+%5Csum_%7Bk%3D1%7D%5EK+%5Cfrac%7B1%7D%7B2n_k%7D+D_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle W_k = &#92;sum_{k=1}^K &#92;frac{1}{2n_k} D_k\" class=\"latex\" />.</p>\n<p>This variance quantity <img src=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"W_k\" class=\"latex\" /> is the basis of a naive procedure to determine the optimal number of clusters: the <a href=\"http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method\">elbow method</a>.<a href=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg\"><img loading=\"lazy\" data-attachment-id=\"429\" data-permalink=\"https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/dataclustering_elbowcriterion/\" data-orig-file=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg\" data-orig-size=\"381,305\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"DataClustering_ElbowCriterion\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg?w=300\" data-large-file=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg?w=381\" class=\"alignright size-medium wp-image-429\" alt=\"DataClustering_ElbowCriterion\" src=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg?w=300&#038;h=240\" width=\"300\" height=\"240\" srcset=\"https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg?w=300&h=240 300w, https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg?w=150&h=120 150w, https://datasciencelab.files.wordpress.com/2013/12/dataclustering_elbowcriterion.jpg 381w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<blockquote><p>If you graph the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters are chosen at this point, hence the &#8220;elbow criterion&#8221;.</p></blockquote>\n<p>But as Wikipedia promptly explains, this &#8220;elbow&#8221; cannot always be unambiguously identified. In this post we will show a more sophisticated method that provides a statistical procedure to formalize the &#8220;elbow&#8221; heuristic.</p>\n<h3>The gap statistic</h3>\n<p>The gap statistic was developed by Stanford researchers <a href=\"http://www.stanford.edu/~hastie/Papers/gap.pdf\">Tibshirani, Walther and Hastie in their 2001 paper</a>. The idea behind their approach was to find a way to standardize the comparison of <img src=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log W_k\" class=\"latex\" /> with a null reference distribution of the data, i.e. a distribution with no obvious clustering. Their estimate for the optimal number of clusters <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> is the value for which <img src=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log W_k\" class=\"latex\" /> falls the farthest below this reference curve. This information is contained in the following formula for the gap statistic:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BGap%7D_n%28k%29+%3D+E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BGap%7D_n%28k%29+%3D+E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BGap%7D_n%28k%29+%3D+E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle &#92;mathrm{Gap}_n(k) = E_n^*&#92;{&#92;log W_k&#92;} - &#92;log W_k\" class=\"latex\" />.</p>\n<p>The reference datasets are in our case generated by sampling uniformly from the original dataset&#8217;s bounding box (see green box in the upper right plot of the figures below). To obtain the estimate <img src=\"https://s0.wp.com/latex.php?latex=E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=E_n%5E%2A%5C%7B%5Clog+W_k%5C%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"E_n^*&#92;{&#92;log W_k&#92;}\" class=\"latex\" /> we compute the average of <img src=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B\" class=\"latex\" /> copies <img src=\"https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log W^*_k\" class=\"latex\" /> for <img src=\"https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B=10\" class=\"latex\" />, each of which is generated with a Monte Carlo sample from the reference distribution. Those <img src=\"https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+W%5E%2A_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log W^*_k\" class=\"latex\" /> from the <img src=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B\" class=\"latex\" /> Monte Carlo replicates exhibit a standard deviation <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{sd}(k)\" class=\"latex\" /> which, accounting for the simulation error, is turned into the quantity</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle s_k = &#92;sqrt{1+1/B}&#92;,&#92;mathrm{sd}(k)\" class=\"latex\" />.</p>\n<p>Finally, the optimal number of clusters <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> is the smallest <img src=\"https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k\" class=\"latex\" /> such that <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{Gap}(k) &#92;geq &#92;mathrm{Gap}(k+1) - s_{k+1}\" class=\"latex\" />.</p>\n<h3>A Python implementation of the algorithm</h3>\n<p>The computation of the gap statistic involves the following steps (<a href=\"http://www.stanford.edu/~hastie/Papers/gap.pdf\">see original paper</a>):</p>\n<ul>\n<li>Cluster the observed data, varying the number of clusters from <img src=\"https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k = 1, ..., k_{&#92;mathrm{max}}\" class=\"latex\" />, and compute the corresponding <img src=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"W_k\" class=\"latex\" />.</li>\n<li>Generate <img src=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B\" class=\"latex\" /> reference data sets and cluster each of them with varying number of clusters <img src=\"https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k+%3D+1%2C+...%2C+k_%7B%5Cmathrm%7Bmax%7D%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k = 1, ..., k_{&#92;mathrm{max}}\" class=\"latex\" />. Compute the estimated gap statistic <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%3D+%281%2FB%29+%5Csum_%7Bb%3D1%7D%5EB+%5Clog+W%5E%2A_%7Bkb%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%3D+%281%2FB%29+%5Csum_%7Bb%3D1%7D%5EB+%5Clog+W%5E%2A_%7Bkb%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%3D+%281%2FB%29+%5Csum_%7Bb%3D1%7D%5EB+%5Clog+W%5E%2A_%7Bkb%7D+-+%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{Gap}(k) = (1/B) &#92;sum_{b=1}^B &#92;log W^*_{kb} - &#92;log W_k\" class=\"latex\" />.</li>\n<li>With <img src=\"https://s0.wp.com/latex.php?latex=%5Cbar%7Bw%7D+%3D+%281%2FB%29+%5Csum_b+%5Clog+W%5E%2A_%7Bkb%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cbar%7Bw%7D+%3D+%281%2FB%29+%5Csum_b+%5Clog+W%5E%2A_%7Bkb%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbar%7Bw%7D+%3D+%281%2FB%29+%5Csum_b+%5Clog+W%5E%2A_%7Bkb%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;bar{w} = (1/B) &#92;sum_b &#92;log W^*_{kb}\" class=\"latex\" />, compute the standard deviation <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29+%3D+%5B%281%2FB%29+%5Csum_b+%28%5Clog+W%5E%2A_%7Bkb%7D+-+%5Cbar%7Bw%7D%29%5E2%5D%5E%7B1%2F2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29+%3D+%5B%281%2FB%29+%5Csum_b+%28%5Clog+W%5E%2A_%7Bkb%7D+-+%5Cbar%7Bw%7D%29%5E2%5D%5E%7B1%2F2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bsd%7D%28k%29+%3D+%5B%281%2FB%29+%5Csum_b+%28%5Clog+W%5E%2A_%7Bkb%7D+-+%5Cbar%7Bw%7D%29%5E2%5D%5E%7B1%2F2%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{sd}(k) = [(1/B) &#92;sum_b (&#92;log W^*_{kb} - &#92;bar{w})^2]^{1/2}\" class=\"latex\" /> and define <img src=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+s_k+%3D+%5Csqrt%7B1%2B1%2FB%7D%5C%2C%5Cmathrm%7Bsd%7D%28k%29&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;displaystyle s_k = &#92;sqrt{1+1/B}&#92;,&#92;mathrm{sd}(k)\" class=\"latex\" />.</li>\n<li>Choose the number of clusters as the smallest <img src=\"https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"k\" class=\"latex\" /> such that <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathrm%7BGap%7D%28k%29+%5Cgeq+%5Cmathrm%7BGap%7D%28k%2B1%29+-+s_%7Bk%2B1%7D&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathrm{Gap}(k) &#92;geq &#92;mathrm{Gap}(k+1) - s_{k+1}\" class=\"latex\" />.</li>\n</ul>\n<p>Our python implementation makes use of the <code>find_centers(X, K)</code> function defined in <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">this post</a>. The quantity <img src=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"W_k\" class=\"latex\" /> is computed as follows:</p>\n<pre class=\"brush: python; title: ; notranslate\">\ndef Wk(mu, clusters):\n    K = len(mu)\n    return sum([np.linalg.norm(mu[i]-c)**2/(2*len(c)) \\\n               for i in range(K) for c in clusters[i]])\n</pre>\n<p>The gap statistic is implemented in the following code snapshot. Note that we use <img src=\"https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%3D10&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B=10\" class=\"latex\" /> for the reference datasets and we span values of <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> from 1 to 9.</p>\n<pre class=\"brush: python; title: ; notranslate\">\ndef bounding_box(X):\n    xmin, xmax = min(X,key=lambda a:a[0])[0], max(X,key=lambda a:a[0])[0]\n    ymin, ymax = min(X,key=lambda a:a[1])[1], max(X,key=lambda a:a[1])[1]\n    return (xmin,xmax), (ymin,ymax)\n\ndef gap_statistic(X):\n    (xmin,xmax), (ymin,ymax) = bounding_box(X)\n    # Dispersion for real distribution\n    ks = range(1,10)\n    Wks = zeros(len(ks))\n    Wkbs = zeros(len(ks))\n    sk = zeros(len(ks))\n    for indk, k in enumerate(ks):\n        mu, clusters = find_centers(X,k)\n        Wks[indk] = np.log(Wk(mu, clusters))\n        # Create B reference datasets\n        B = 10\n        BWkbs = zeros(B)\n        for i in range(B):\n            Xb = []\n            for n in range(len(X)):\n                Xb.append([random.uniform(xmin,xmax),\n                          random.uniform(ymin,ymax)])\n            Xb = np.array(Xb)\n            mu, clusters = find_centers(Xb,k)\n            BWkbs[i] = np.log(Wk(mu, clusters))\n        Wkbs[indk] = sum(BWkbs)/B\n        sk[indk] = np.sqrt(sum((BWkbs-Wkbs[indk])**2)/B)\n    sk = sk*np.sqrt(1+1/B)\n    return(ks, Wks, Wkbs, sk)\n</pre>\n<h3>Finding the K</h3>\n<p>We shall now apply our algorithm to diverse distributions and see how it performs. Using the <code>init_board_gauss(N, k)</code> function defined in <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">our previous post</a>, we produce an ensemble of 200 data points normally distributed around 3 centers and run the gap statistic on them.</p>\n<pre class=\"brush: python; title: ; notranslate\">\nX = init_board_gauss(200,3)\nks, logWks, logWkbs, sk = gap_statistic(X)\n</pre>\n<p>The following plot gives an idea of what is happening:<br />\n<a href=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n200_k3.png\"><img src=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n200_k3.png?w=657&#038;h=1024\" alt=\"Fig_N200_K3\" width=\"657\" height=\"1024\" /></a></p>\n<p>The upper left plot shows the target distribution with 3 clusters. On the right is its bounding box and one Monte Carlo sample drawn from a uniform reference distribution within that rectangle. In the middle left we see the plot of <img src=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"W_k\" class=\"latex\" /> that is used to determine <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> with the elbow method. Indeed a knee-like feature is observed at <img src=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=3\" class=\"latex\" />, however the gap statistic is a better way of formalizing this phenomenon. On the right is the comparison of <img src=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog+W_k&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log W_k\" class=\"latex\" /> for the original and averaged reference distributions. Finally, the bottom plots show the gap quantity on the left, with a clear peak at the correct <img src=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=3\" class=\"latex\" /> and the criteria for choosing it on the right. The correct <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> is the smallest for which the quantity plotted in blue bars becomes positive. The optimal number is correctly guessed by the algorithm as <img src=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D3&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=3\" class=\"latex\" />.</p>\n<p>Let us now have a look at another example with 400 points around 5 clusters:<br />\n<a href=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n400_k5.png\"><img alt=\"Fig_N400_K5\" src=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n400_k5.png?w=657&#038;h=1024\" width=\"657\" height=\"1024\" /></a></p>\n<p>In this case, the elbow method would not have been conclusive, however the gap statistic correctly shows a peak in the gap at <img src=\"https://s0.wp.com/latex.php?latex=K%3D5&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D5&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D5&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=5\" class=\"latex\" /> and the bar plot changes sign at the same correct value.</p>\n<p>Similarly, we can study what happens when the data points are clustered around a single centroid:<br />\n<a href=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n100_k1.png\"><img alt=\"Fig_N100_K1\" src=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n100_k1.png?w=657&#038;h=1024\" width=\"657\" height=\"1024\" /></a></p>\n<p>It is clear in the above figures that the original and the reference distributions in the middle right plot follow the same decay law, so that no abrupt fall-off of the blue curve with respect to the red one is observed at any <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" />. The bar plot shows positive values for the entire <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> range. We conclude that <img src=\"https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=1\" class=\"latex\" /> is the correct clustering.</p>\n<p>Finally, let us have a look at a uniform, non-clustered distribution of 200 points, generated with the <code>init_board(N)</code> function defined in <a href=\"https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\">our previous post</a>:<br />\n<a href=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n200_k1.png\"><img alt=\"Fig_N200_K1\" src=\"https://datasciencelab.files.wordpress.com/2013/12/fig_n200_k1.png?w=657&#038;h=1024\" width=\"657\" height=\"1024\" /></a></p>\n<p>In this case, the algorithm also guesses <img src=\"https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K%3D1&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K=1\" class=\"latex\" /> correctly, and it is clear from the middle right plot that both the original and the reference distributions follow exactly the same decay law, since they are essentially different samples from the same uniform distribution on [-1,1] x [-1,1]. The gap curve on the bottom left oscillates between local maxima and minima, indicating certain structures within the original distribution originated by statistical fluctuations.</p>\n<h3>Table-top data experiment take-away message</h3>\n<p>The estimation of the optimal number of clusters within a set of data points is a very important problem, as most clustering algorithms need that parameter as input in order to group the data. Many methods have been proposed to find the proper <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" />, among which the &#8220;elbow&#8221; method offers a very clear and naive solution based on intra-cluster variance. The gap statistic, proposed by Tobshirani <em>et al.</em> formalizes this approach and offers an easy-to-implement algorithm that successfully finds the correct <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"K\" class=\"latex\" /> in the case of globular, Gaussian-distributed, mildly disjoint data distributions.</p>\n<p><strong>Update:</strong> For a proper initialization of the centroids at the start of the k-means algorithm, we implement the <a href=\"https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/\">improved k-means++ seeding procedure</a>. </p>\n<p><strong>Update:</strong> For a comparison of this approach with an alternative method for finding the K in k-means clustering, read <a href=\"https://datasciencelab.wordpress.com/2014/01/21/selection-of-k-in-k-means-clustering-reloaded/\" title=\"Selection of K in K-means Clustering, Reloaded\">this article</a>. </p>\n",
  "wfw:commentRss": "https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/feed/",
  "slash:comments": 42,
  "media:content": [
    {
      "media:title": "datasciencelab"
    },
    {
      "media:title": "DataClustering_ElbowCriterion"
    },
    {
      "media:title": "Fig_N200_K3"
    },
    {
      "media:title": "Fig_N400_K5"
    },
    {
      "media:title": "Fig_N100_K1"
    },
    {
      "media:title": "Fig_N200_K1"
    }
  ]
}