{
  "title": "Topic Modeling and Latent Dirichlet Allocation (LDA)",
  "link": "https://datascienceplus.com/topic-modeling-and-latent-dirichlet-allocation-lda/",
  "comments": "https://datascienceplus.com/topic-modeling-and-latent-dirichlet-allocation-lda/#respond",
  "dc:creator": "Enes Zvornicanin",
  "pubDate": "Sun, 30 Jan 2022 16:26:45 +0000",
  "category": [
    "Introduction",
    "Machine Learning",
    "NLP"
  ],
  "guid": "https://datascienceplus.com/?p=31815",
  "description": "<div style=\"border-top: 1px solid; font-size: 14px;text-align: center; border-bottom: 1px solid; padding: 5px 2px;\"><a href=\"https://datascienceplus.com/posting-from-r-markdown-to-datascienceplus/\">Are you interested in guest posting? Publish at DataScience+  via your RStudio editor.</a></div><h2>Category</h2><ul><li><a href=\"https://datascienceplus.com/category/introduction/\" rel=\"bookmark\" title=\"Permanent Link toIntroduction\">Introduction</a></li></ul><h2>Tags</h2><ul><li><a href=\"https://datascienceplus.com/tag/machine-learning/\" rel=\"bookmark\" title=\"Permanent Link toMachine Learning\">Machine Learning</a></li><li><a href=\"https://datascienceplus.com/tag/nlp/\" rel=\"bookmark\" title=\"Permanent Link toNLP\">NLP</a></li><li><a href=\"https://datascienceplus.com/tag/python/\" rel=\"bookmark\" title=\"Permanent Link toPython\">Python</a></li></ul>Topic Modeling Topic modeling is a natural language processing (NLP) technique for determining the topics in a document. Also, we can use it to discover patterns of words in a collection of documents. By analyzing the frequency of words and phrases in the documents, it&#8217;s able to determine the probability of a word or phrase [&#8230;]<strong><p>Related Post</p></strong><ul><li><a href=\"https://datascienceplus.com/how-to-import-two-modules-with-same-function-name-in-python/\" rel=\"bookmark\" title=\"Permanent Link toHow to import two modules with same function name in Python\">How to import two modules with same function name in Python</a></li><li><a href=\"https://datascienceplus.com/text-generation-with-python/\" rel=\"bookmark\" title=\"Permanent Link toText Generation with Python\">Text Generation with Python</a></li><li><a href=\"https://datascienceplus.com/merging-datasets-with-tidyverse/\" rel=\"bookmark\" title=\"Permanent Link toMerging Datasets with Tidyverse\">Merging Datasets with Tidyverse</a></li><li><a href=\"https://datascienceplus.com/5-easy-steps-to-kickstart-a-career-in-data-science-by-learning-python/\" rel=\"bookmark\" title=\"Permanent Link to5 Easy Steps to Kickstart a Career in Data Science by Learning Python\">5 Easy Steps to Kickstart a Career in Data Science by Learning Python</a></li><li><a href=\"https://datascienceplus.com/top-open-source-tools-for-natural-language-processing-in-python/\" rel=\"bookmark\" title=\"Permanent Link toTop open source tools for natural language processing in Python\">Top open source tools for natural language processing in Python</a></li></ul>",
  "content:encoded": "<div style=\"border-top: 1px solid; font-size: 14px;text-align: center; border-bottom: 1px solid; padding: 5px 2px;\"><a href=\"https://datascienceplus.com/posting-from-r-markdown-to-datascienceplus/\">Are you interested in guest posting? Publish at DataScience+  via your RStudio editor.</a></div><h2>Category</h2><ul><li><a href=\"https://datascienceplus.com/category/introduction/\" rel=\"bookmark\" title=\"Permanent Link toIntroduction\">Introduction</a></li></ul><h2>Tags</h2><ul><li><a href=\"https://datascienceplus.com/tag/machine-learning/\" rel=\"bookmark\" title=\"Permanent Link toMachine Learning\">Machine Learning</a></li><li><a href=\"https://datascienceplus.com/tag/nlp/\" rel=\"bookmark\" title=\"Permanent Link toNLP\">NLP</a></li><li><a href=\"https://datascienceplus.com/tag/python/\" rel=\"bookmark\" title=\"Permanent Link toPython\">Python</a></li></ul><h2>Topic Modeling</h2>\n<p><b>Topic modeling is a natural language processing (NLP) technique for determining the topics in a document. Also, we can use it to discover patterns of words in a collection of documents.</b> By analyzing the frequency of words and phrases in the documents, it&#8217;s able to determine the probability of a word or phrase belonging to a certain topic and cluster documents based on their similarity or closeness.</p>\n<p>Firstly, topic modeling starts with a large corpus of text and reduces it to a much smaller number of topics. Topics are found by analyzing the relationship between words in the corpus. Also, topic modeling finds which words frequently co-occur with others and how often they appear together.</p>\n<p>The model tries to find clusters of words that co-occur more frequently than they would otherwise expect due to chance alone. This gives a rough idea about topics in the document and where they rank on its hierarchy of importance.<br />\nThe current methods for extraction of topic models include Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), Probabilistic Latent Semantic Analysis (PLSA), and Non-Negative Matrix Factorization (NMF). <b>In this article, we&#8217;ll focus on Latent Dirichlet Allocation (LDA).</b></p>\n<p>The reason topic modeling is useful is that it allows the user to not only explore what&#8217;s inside their corpus (documents) but also build new connections between topics they weren&#8217;t even aware of. <b>Some applications of topic modeling also include text summarization,  recommender systems, spam filters, and similar.</b></p>\n<h2>Latent Dirichlet Allocation (LDA)</h2>\n<p><b>Latent Dirichlet Allocation (LDA) is an unsupervised clustering technique that is commonly used for text analysis. It&#8217;s a type of topic modeling in which words are represented as topics, and documents are represented as a collection of these word topics.</b></p>\n<p>For this purpose, we&#8217;ll describe the LDA through topic modeling. Thus, let&#8217;s imagine that we have a collection of documents or articles. Each document has a topic such as computer science, physics, biology, etc. Also, some of the articles might have multiple topics. The problem is that we have only articles but not their topics and we would like to have an algorithm that is able to sort documents into topics.</p>\n<h3>Sampling Topics</h3>\n<p>We can imagine that LDA will place documents in the space according to the document topics. For example, in our case with topics computer science, physics, and biology, LDA will put documents into a triangle where corners are the topics. We can see this in the image below where each orange circle represents one document.</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/01/triangle.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/01/triangle-490x366.png\" alt=\"\" width=\"490\" height=\"366\" class=\"alignnone size-medium wp-image-31817\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/01/triangle-490x366.png 490w, https://datascienceplus.com/wp-content/uploads/2022/01/triangle.png 533w\" sizes=\"(max-width: 490px) 100vw, 490px\" /></a></p>\n<p>As we&#8217;ve said, some documents might have several topics and an example of that is the document between computer science and biology in the image above. For instance, it&#8217;s possible if the document is about biotechnology. <b>In probability and statistics, such kind of distribution is called Dirichlet distribution and it&#8217;s controlled by the parameter \\(\\alpha\\).</b></p>\n<p>For example, \\(\\alpha=1\\) indicates that samples are more evenly distributed over the space, \\(\\alpha>1\\) means that samples are gathering in the middle, and \\(\\alpha<1\\) indicates that samples tend towards corners. Also, parameter \\(\\alpha\\) is usually a \\(k\\)-dimensional vector, where each component corresponds to each corner or topic in our case. We can observe this behavior in the image below.</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/01/dirichlet.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/01/dirichlet-490x142.png\" alt=\"\" width=\"490\" height=\"366\" class=\"alignnone size-large wp-image-31819\" /></a></p>\n<p>Next, if we consider the document about biotechnology that we mentioned above, it might consist of 50% computer science, 45% biology, and 5% physics. Generally, we can define this distribution of topics over a document as multinomial distribution with parameter \\(\\theta\\). Accordingly, the \\(\\theta\\) parameter is a \\(k\\)-dimensional vector of probabilities, which must sum to 1. After that, we sample from the multinomial distribution \\(N\\) different topics. In order to understand this process, we can observe the image below.</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/01/topics.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/01/topics-490x173.png\" alt=\"\" width=\"490\" height=\"173\" class=\"alignnone size-medium wp-image-31820\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/01/topics-490x173.png 490w, https://datascienceplus.com/wp-content/uploads/2022/01/topics-1024x361.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/01/topics-768x270.png 768w, https://datascienceplus.com/wp-content/uploads/2022/01/topics.png 1170w\" sizes=\"(max-width: 490px) 100vw, 490px\" /></a></p>\n<h3>Sampling Words</h3>\n<p>After picking \\(N\\) different topics, we would also need to sample words. For that purpose, we&#8217;ll also use Dirichlet and multinomial distribution.  The second Dirichlet distribution, defined with parameter \\(\\beta\\), maps topics in word space. For instance, the corners of a triangle, tetrahedron in case of 4 dimensions or simplex for \\(n\\) dimensions, might be now words such as algorithm, genetic, velocity, or similar.</p>\n<p>Instead of documents, now we are placing topics into this space. For example, the topic of computer science is closer to the word algorithm rather than to the word genetic, and the multinomial distribution of words for this topic might consist of 75% algorithm, 15% genetic, and 10% velocity. Similarly, we can define multinomial distributions for topic biology as 10% algorithm, 85% genetic, and 5% velocity, and topic physics as 20% algorithm, 5% genetic, and 75% velocity.</p>\n<p>Also, after defining multinomial distributions for topics, we&#8217;ll sample words from those distributions corresponding to each topic sampled in the first step. This process can be more easily understood through the illustration below.</p>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/01/lda.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/01/lda-490x147.png\" alt=\"\" width=\"490\" height=\"147\" class=\"alignnone size-medium wp-image-31821\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/01/lda-490x147.png 490w, https://datascienceplus.com/wp-content/uploads/2022/01/lda-1024x307.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/01/lda-768x231.png 768w, https://datascienceplus.com/wp-content/uploads/2022/01/lda.png 1512w\" sizes=\"(max-width: 490px) 100vw, 490px\" /></a></p>\n<p>For example, if we consider a blue circle that represents a computer science topic, this topic has its own distribution by words that we&#8217;re using. Next, following the same order of sampled topics, for each topic, we select one word based on the topics distribution. For instance, the first topic above is computer science, and based on probability 0.75, 0.15, and 0.1 for words algorithm, genetic, and velocity respectively, we selected the word algorithm. Following this process, LDA creates a new document.</p>\n<h3>LDA Definition</h3>\n<p>In this way, for each input document, we create a new one. After all, we want to maximize the probability of creating the same document and the whole process above is mathematically defined as</p>\n<p>$$<br />\nP(\\boldsymbol{W}, \\boldsymbol{Z}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\alpha, \\beta) = \\prod_{i = 1}^{M}P(\\theta_{j}; \\alpha)\\prod_{i = 1}^{K}P(\\phi; \\beta)\\prod_{t = 1}^{N}P(Z_{j, t} | \\theta_{j})P(W_{j, t} | \\phi z_{j, t}),<br />\n$$</p>\n<p>\\(\\alpha\\) and \\(\\beta\\) define Dirichlet distributions, \\(\\theta\\) and \\(\\phi\\) define multinomial distributions, \\(\\boldsymbol{Z}\\) is the vector with topics of all words in all documents, \\(\\boldsymbol{W}\\) is the vector with all words in all documents, \\(M\\) number of documents, \\(K\\) number of topics and \\(N\\) number of words.<br />\n<b>The whole process of training or maximizing probability can be done using Gibbs sampling where the general idea is to make each document and each word as monochromatic as possible. Basically, it means we want that each document have as few as possible articles and each word belongs to as few as possible topics.</b></p>\n<h2>Example</h2>\n<p>In this example, we’ll use the 20 newsgroups text dataset. The 20 newsgroups dataset comprises around 12000 newsgroups posts on 20 topics. Let&#8217;s load the data and all the needed packages.</p>\n<pre>\nimport pandas as pd\nimport re\nimport numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom gensim import corpora, models\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models.coherencemodel import CoherenceModel\nimport pyLDAvis.gensim\n</pre>\n<pre>\nnewsgroups_train = fetch_20newsgroups(subset='train')\n\ndf = pd.DataFrame({'post': newsgroups_train['data'], 'target': newsgroups_train['target']})\ndf['target_names'] = df['target'].apply(lambda t: newsgroups_train['target_names'][t])\ndf.head()\n<em> \tpost \ttarget \ttarget_names\n0 \tFrom: lerxst@wam.umd.edu (where's my thing)\\nS... \t7 \trec.autos\n1 \tFrom: guykuo@carson.u.washington.edu (Guy Kuo)... \t4 \tcomp.sys.mac.hardware\n2 \tFrom: twillis@ec.ecn.purdue.edu (Thomas E Will... \t4 \tcomp.sys.mac.hardware\n3 \tFrom: jgreen@amber (Joe Green)\\nSubject: Re: W... \t1 \tcomp.graphics\n4 \tFrom: jcm@head-cfa.harvard.edu (Jonathan McDow... \t14 \tsci.space\n</em>\n</pre>\n<p>As a text preprocessing step, we&#8217;ll first remove URLs, HTML tags, emails, and non-alpha characters. After that, we&#8217;ll lemmatize it and remove stopwords.</p>\n<pre>\ndef remove_urls(text):\n    \" removes urls\"\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n    \ndef remove_html(text):\n    \" removes html tags\"\n    html_pattern = re.compile('')\n    return html_pattern.sub(r'', text)\n\ndef remove_emails(text):\n    email_pattern = re.compile('\\S*@\\S*\\s?')\n    return email_pattern.sub(r'', text)\n\ndef remove_new_line(text):\n    return re.sub('\\s+', ' ', text)\n\ndef remove_non_alpha(text):\n    return re.sub(\"[^A-Za-z]+\", ' ', str(text))\n\ndef preprocess_text(text):\n    t = remove_urls(text)\n    t = remove_html(t)\n    t = remove_emails(t)\n    t = remove_new_line(t)\n    t = remove_non_alpha(t)\n    return t\n\ndef lemmatize_words(text, lemmatizer):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndef remove_stopwords(text, stopwords):\n    return \" \".join([word for word in str(text).split() if word not in stopwords])\n\n\ndf['post_preprocessed'] = df['post'].apply(preprocess_text).str.lower()\n\nprint('lemming...')\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\ndf['post_final'] = df['post_preprocessed'].apply(lambda post: lemmatize_words(post, lemmatizer))\n\nprint('remove stopwors...')\n\nnltk.download('stopwords')\nswords = set(stopwords.words('english'))\n\ndf['post_final'] = df['post_preprocessed'].apply(lambda post: remove_stopwords(post, swords))\ndf.head()\n<em>\n\tpost \ttarget \ttarget_names \tpost_preprocessed \tpost_final\n0 \tFrom: lerxst@wam.umd.edu (where's my thing)\\nS... \t7 \trec.autos \tfrom where s my thing subject what car is this... \tthing subject car nntp posting host rac wam um...\n1 \tFrom: guykuo@carson.u.washington.edu (Guy Kuo)... \t4 \tcomp.sys.mac.hardware \tfrom guy kuo subject si clock poll final call ... \tguy kuo subject si clock poll final call summa...\n2 \tFrom: twillis@ec.ecn.purdue.edu (Thomas E Will... \t4 \tcomp.sys.mac.hardware \tfrom thomas e willis subject pb questions orga... \tthomas e willis subject pb questions organizat...\n3 \tFrom: jgreen@amber (Joe Green)\\nSubject: Re: W... \t1 \tcomp.graphics \tfrom joe green subject re weitek p organizatio... \tjoe green subject weitek p organization harris...\n4 \tFrom: jcm@head-cfa.harvard.edu (Jonathan McDow... \t14 \tsci.space \tfrom jonathan mcdowell subject re shuttle laun... \tjonathan mcdowell subject shuttle launch quest...\n</em>\n</pre>\n<p>Next, we&#8217;ll make the dictionary and corpus. Also, as a corpus, we can use only term frequency or TF-IDF.</p>\n<pre>\nposts = [x.split(' ') for x in df['post_final']]\nid2word = corpora.Dictionary(posts)\ncorpus_tf = [id2word.doc2bow(text) for text in posts]\nprint(corpus_tf[0])\n<em>[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 5), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1)]\n</em>\n</pre>\n<pre>\ntfidf = models.TfidfModel(corpus_tf)\ncorpus_tfidf = tfidf[corpus_tf]\nprint(corpus_tfidf[0])\n<em>[(0, 0.11498876048525103), (1, 0.09718324368673029), (2, 0.10251459464215813), (3, 0.22950467156922127), (4, 0.11224887707193924), (5, 0.1722981301822569), (6, 0.07530011969613486), (7, 0.4309484809469165), (8, 0.08877590143625969), (9, 0.04578068195160004), (10, 0.07090803901993002), (11, 0.1222656727768876), (12, 0.14524649469964415), (13, 0.05251249361530128), (14, 0.0989263305425191), (15, 0.04078267609390185), (16, 0.11756371552272524), (17, 0.17436169259993298), (18, 0.10155337594190954), (19, 0.20948825386578207), (20, 0.09695491629716278), (21, 0.024520714650907785), (22, 0.12964907508803875), (23, 0.08179595178219969), (24, 0.035633159058452026), (25, 0.11020678338364179), (26, 0.24952108927266048), (27, 9.459268363417395e-05), (28, 0.10776183582290975), (29, 0.07547376776331942), (30, 0.06670829980433708), (31, 0.062106577059591), (32, 0.13626396477950442), (33, 0.10453869332078215), (34, 0.07661054771383646), (35, 0.17037224424255862), (36, 0.024905114157890113), (37, 0.0011640619492058468), (38, 0.12139841280668175), (39, 0.054717960920777436), (40, 0.02308905209371841), (41, 0.13459748784234876), (42, 0.20608696405865523), (43, 0.056503689640334795), (44, 0.09456465243547033), (45, 0.09876981207502786), (46, 0.12006279504111743), (47, 0.08461773880033642), (48, 0.13486864088205006), (49, 0.13432885719305454), (51, 0.24952108927266048), (52, 0.05421309514981315), (53, 0.064793199454388), (54, 0.16160262905222716), (55, 0.027057268862720633), (56, 0.1954679598913907), (57, 0.09504085428857881), (58, 0.105116264304804), (59, 0.06248175923527969)]\n\n</em>\n</pre>\n<p>After that, we test both corpus using the LDA model. We’ll measure their performance using coherence score UMass as a more commonly used CV score might not give good results. More about coherence scores can be found in <a href=\"https://www.baeldung.com/cs/topic-modeling-coherence-score#1-cv-coherence-score\">this article</a>.</p>\n<p>In order to see keywords for each topic, we use method `show_topics`. Basically, it shows the topic index and weightage of each keyword.</p>\n<pre>\nmodel = LdaMulticore(corpus=corpus_tf,id2word = id2word, num_topics = 20,\n                     alpha=.1, eta=0.1, random_state = 0)\n\ncoherence = CoherenceModel(model = model, texts = posts, dictionary = id2word, coherence = 'u_mass')\n\nprint(coherence.get_coherence())\nprint(model.show_topics())\n<em>-1.6040665431701946\n[(6, '0.010*\"god\" + 0.008*\"people\" + 0.007*\"one\" + 0.006*\"would\" + 0.006*\"subject\" + 0.005*\"lines\" + 0.004*\"article\" + 0.004*\"writes\" + 0.004*\"organization\" + 0.004*\"may\"'),\n (12, '0.008*\"subject\" + 0.008*\"lines\" + 0.008*\"organization\" + 0.006*\"article\" + 0.006*\"writes\" + 0.006*\"would\" + 0.005*\"one\" + 0.005*\"x\" + 0.004*\"university\" + 0.004*\"posting\"'),\n...\n...\n</em>\n</pre>\n<h3>Visualize the topics-keywords</h3>\n<p>After we built the LDA model, the next step is to visualize results using pyLDAvis package. It&#8217;s an interactive chart that shows topics and keywords. </p>\n<p>On the left side, topics are represented as circles. The larger the circle, the more prevalent is that topic. A good topic model will have big, non-overlapping circles scattered throughout the chart instead of being clustered in one quadrant.</p>\n<p>On the right side, we can observe the most relevant keywords from the selected topic.</p>\n<pre>\nlda_display = pyLDAvis.gensim.prepare(model, corpus_tf, id2word, sort_topics = False)\npyLDAvis.display(lda_display)\n</pre>\n<p><a href=\"https://datascienceplus.com/wp-content/uploads/2022/01/ldavis.png\"><img src=\"https://datascienceplus.com/wp-content/uploads/2022/01/ldavis-490x287.png\" alt=\"\" width=\"490\" height=\"287\" class=\"alignnone size-medium wp-image-31838\" srcset=\"https://datascienceplus.com/wp-content/uploads/2022/01/ldavis-490x287.png 490w, https://datascienceplus.com/wp-content/uploads/2022/01/ldavis-1024x599.png 1024w, https://datascienceplus.com/wp-content/uploads/2022/01/ldavis-768x449.png 768w, https://datascienceplus.com/wp-content/uploads/2022/01/ldavis.png 1186w\" sizes=\"(max-width: 490px) 100vw, 490px\" /></a></p>\n<p>In the end, dominant topic and contribution percent of that topic is extracted. </p>\n<pre>\ndata_dict = {'dominant_topic':[], 'perc_contribution':[], 'topic_keywords':[]}\n\nfor i, row in enumerate(model[corpus_tf]):\n    #print(i)\n    row = sorted(row, key=lambda x: x[1], reverse=True)\n    #print(row)\n    for j, (topic_num, prop_topic) in enumerate(row):\n        wp = model.show_topic(topic_num)\n        topic_keywords = \", \".join([word for word, prop in wp])\n        data_dict['dominant_topic'].append(int(topic_num))\n        data_dict['perc_contribution'].append(round(prop_topic, 3))\n        data_dict['topic_keywords'].append(topic_keywords)\n        #print(topic_keywords)\n        break\n\ndf_topics = pd.DataFrame(data_dict)\n\ncontents = pd.Series(posts)\n\ndf_topics['post'] = df['post']\ndf_topics.head()\n</pre>\n<h3>Further work</h3>\n<p><b>This tutorial represents only the theoretical background and baseline model for topic modeling and the LDA algorithm. Thus, the presented results might not be the best possible, and a lot of space is left for improving them. For instance, we might try using different text preprocessing methods, tune LDA hyperparameters, such as the number of topics, alpha, beta, try different coherence metrics, and similar.</b> </p>\n<h2>References</h2>\n<ul>\n<li>Latent Dirichlet Allocation (Part 1 and 2), <a href=\"https://www.youtube.com/watch?v=T05t-SqKArY\">https://www.youtube.com/watch?v=T05t-SqKArY</a></li>\n<li>Topic Modeling With Gensim (Python), <a href=\"https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\">https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/</a></li>\n<li>When Coherence Score is Good or Bad in Topic Modeling?, <a href=\"https://www.baeldung.com/cs/topic-modeling-coherence-score\">https://www.baeldung.com/cs/topic-modeling-coherence-score</a></li>\n<li>Topic modeling guide (GSDM,LDA,LSI), <a href=\"https://www.kaggle.com/ptfrwrd/topic-modeling-guide-gsdm-lda-lsi\">https://www.kaggle.com/ptfrwrd/topic-modeling-guide-gsdm-lda-lsi</a></li>\n<li>Beginners Guide to Topic Modeling in Python, <a href=\"https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\">https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/</a></li>\n</ul>\n<strong><p>Related Post</p></strong><ul><li><a href=\"https://datascienceplus.com/how-to-import-two-modules-with-same-function-name-in-python/\" rel=\"bookmark\" title=\"Permanent Link toHow to import two modules with same function name in Python\">How to import two modules with same function name in Python</a></li><li><a href=\"https://datascienceplus.com/text-generation-with-python/\" rel=\"bookmark\" title=\"Permanent Link toText Generation with Python\">Text Generation with Python</a></li><li><a href=\"https://datascienceplus.com/merging-datasets-with-tidyverse/\" rel=\"bookmark\" title=\"Permanent Link toMerging Datasets with Tidyverse\">Merging Datasets with Tidyverse</a></li><li><a href=\"https://datascienceplus.com/5-easy-steps-to-kickstart-a-career-in-data-science-by-learning-python/\" rel=\"bookmark\" title=\"Permanent Link to5 Easy Steps to Kickstart a Career in Data Science by Learning Python\">5 Easy Steps to Kickstart a Career in Data Science by Learning Python</a></li><li><a href=\"https://datascienceplus.com/top-open-source-tools-for-natural-language-processing-in-python/\" rel=\"bookmark\" title=\"Permanent Link toTop open source tools for natural language processing in Python\">Top open source tools for natural language processing in Python</a></li></ul>",
  "wfw:commentRss": "https://datascienceplus.com/topic-modeling-and-latent-dirichlet-allocation-lda/feed/",
  "slash:comments": 0
}