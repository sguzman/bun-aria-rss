{
  "title": "Dask on HPC - Initial Work",
  "link": "",
  "updated": "2017-09-18T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/09/18/pangeo-1",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc.</a> and the <a href=\"https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=504780\">NSF\nEarthCube</a> program.</em></p>\n\n<p>We <a href=\"http://blogs.ei.columbia.edu/2017/09/13/pangeo-project-will-improve-access-to-climate-data/\">recently\nannounced</a>\na collaboration between the <a href=\"https://ncar.ucar.edu/\">National Center for Atmospheric Research\n(NCAR)</a>, <a href=\"http://www.ldeo.columbia.edu/\">Columbia\nUniversity</a>, and Anaconda Inc to accelerate the\nanalysis of atmospheric and oceanographic data on high performance computers\n(HPC) with XArray and Dask.  The <a href=\"https://figshare.com/articles/Pangeo_NSF_Earthcube_Proposal/5361094\">full\ntext</a> of\nthe proposed work is <a href=\"https://figshare.com/articles/Pangeo_NSF_Earthcube_Proposal/5361094\">available\nhere</a>.  We\nare very grateful to the NSF EarthCube program for funding this work, which\nfeels particularly relevant today in the wake (and continued threat) of the\nmajor storms Harvey, Irma, and Jose.</p>\n\n<p>This is a collaboration of academic scientists (Columbia), infrastructure\nstewards (NCAR), and software developers (Anaconda and Columbia and NCAR) to\nscale current workflows with XArray and Jupyter onto big-iron HPC systems and\npeta-scale datasets.  In the first week after the grant closed a few of us\nfocused on the quickest path to get science groups up and running with XArray,\nDask, and Jupyter on these HPC systems.  This blogpost details what we achieved\nand some of the new challenges that we’ve found in that first week.  We hope to\nfollow this blogpost with many more to come in the future.\nToday we cover the following topics:</p>\n\n<ol>\n  <li>Deploying Dask with MPI</li>\n  <li>Interactive deployments on a batch job scheduler, in this case PBS</li>\n  <li>The virtues of JupyterLab in a remote system</li>\n  <li>Network performance and 3GB/s infiniband</li>\n  <li>Modernizing XArray’s interactions with Dask’s distributed scheduler</li>\n</ol>\n\n<p>A video walkthrough deploying Dask on XArray on an HPC system is available <a href=\"https://www.youtube.com/watch?v=7i5m78DSr34\">on\nYouTube</a> and instructions for\natmospheric scientists with access to the <a href=\"https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne\">Cheyenne\nSupercomputer</a>\nis available\n<a href=\"https://github.com/pangeo-data/pangeo-discussion/wiki/Getting-Started-with-Dask-on-Cheyenne\">here</a>.</p>\n\n<p>Now lets start with technical issues:</p>\n\n<h2 id=\"deploying-dask-with-mpi\">Deploying Dask with MPI</h2>\n\n<p>HPC systems use job schedulers like SGE, SLURM, PBS, LSF, and others.  Dask\nhas been deployed on all of these systems before either by academic groups or\nfinancial companies.  However every time we do this it’s a little different and\ngenerally tailored to a particular cluster.</p>\n\n<p>We wanted to make something more general.  This started out as a <a href=\"https://github.com/dask/distributed/issues/1260\">GitHub issue\non PBS scripts</a> that tried to\nmake a simple common template that people could copy-and-modify.\nUnfortunately, there were significant challenges with this.  HPC systems and\ntheir job schedulers seem to focus and easily support only two common use\ncases:</p>\n\n<ol>\n  <li>Embarrassingly parallel “run this script 1000 times” jobs.  This is too\nsimple for what we have to do.</li>\n  <li><a href=\"https://en.wikipedia.org/wiki/Message_Passing_Interface\">MPI</a> jobs.  This\nseemed like overkill, but is the approach that we ended up taking.</li>\n</ol>\n\n<p>Deploying dask is somewhere between these two.  It falls into the master-slave\npattern (or perhaps more appropriately coordinator-workers).  We ended up\nbuilding an <a href=\"http://mpi4py.readthedocs.io/en/stable/\">MPI4Py</a> program that\nlaunches Dask.  MPI is well supported, and more importantly consistently\nsupported, by all HPC job schedulers so depending on MPI provides a level of\nstability across machines.  Now dask.distributed ships with a new <code class=\"language-plaintext highlighter-rouge\">dask-mpi</code>\nexecutable:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>mpirun --np 4 dask-mpi\n</code></pre></div></div>\n\n<p>To be clear, Dask isn’t using MPI for inter-process communication.  It’s still\nusing TCP.  We’re just using MPI to launch a scheduler and several workers and\nhook them all together.  In pseudocode the <code class=\"language-plaintext highlighter-rouge\">dask-mpi</code> executable looks\nsomething like this:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">mpi4py</span> <span class=\"kn\">import</span> <span class=\"n\">MPI</span>\n<span class=\"n\">comm</span> <span class=\"o\">=</span> <span class=\"n\">MPI</span><span class=\"p\">.</span><span class=\"n\">COMM_WORLD</span>\n<span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">comm</span><span class=\"p\">.</span><span class=\"n\">Get_rank</span><span class=\"p\">()</span>\n\n<span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n    <span class=\"n\">start_dask_scheduler</span><span class=\"p\">()</span>\n<span class=\"k\">else</span><span class=\"p\">:</span>\n    <span class=\"n\">start_dask_worker</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Socially this is useful because <em>every</em> cluster management team knows how to\nsupport MPI, so anyone with access to such a cluster has someone they can ask\nfor help.  We’ve successfully translated the question “How do I start Dask?” to\nthe question “How do I run this MPI program?” which is a question that the\ntechnical staff at supercomputer facilities are generally much better equipped\nto handle.</p>\n\n<h2 id=\"working-interactively-on-a-batch-scheduler\">Working Interactively on a Batch Scheduler</h2>\n\n<p>Our collaboration is focused on interactive analysis of big datasets.  This\nmeans that people expect to open up Jupyter notebooks, connect to clusters\nof many machines, and compute on those machines while they sit at their\ncomputer.</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/pangeo-dask-client.png\" width=\"60%\" /></p>\n\n<p>Unfortunately most job schedulers were designed for batch scheduling.  They\nwill try to run your job quickly, but don’t mind waiting for a few hours for a\nnice set of machines on the super computer to open up.  As you ask for more\ntime and more machines, waiting times can increase drastically.  For most MPI\njobs this is fine because people aren’t expecting to get a result right away\nand they’re certainly not interacting with the program, but in our case we\nreally do want some results right away, even if they’re only part of what we\nasked for.</p>\n\n<p>Handling this problem long term will require both technical work and policy\ndecisions.  In the short term we take advantage of two facts:</p>\n\n<ol>\n  <li>Many small jobs can start more quickly than a few large ones.  These take\nadvantage of holes in the schedule that are too small to be used by larger\njobs.</li>\n  <li>Dask doesn’t need to be started all at once.  Workers can come and go.</li>\n</ol>\n\n<p>And so I find that if I ask for several single machine jobs I can easily cobble\ntogether a sizable cluster that starts very quickly.  In practice this looks\nlike the following:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ qsub start-dask.sh      # only ask for one machine\n$ qsub add-one-worker.sh  # ask for one more machine\n$ qsub add-one-worker.sh  # ask for one more machine\n$ qsub add-one-worker.sh  # ask for one more machine\n$ qsub add-one-worker.sh  # ask for one more machine\n$ qsub add-one-worker.sh  # ask for one more machine\n$ qsub add-one-worker.sh  # ask for one more machine\n</code></pre></div></div>\n\n<p>Our main job has a wall time of about an hour.  The workers have shorter wall\ntimes.  They can come and go as needed throughout the computation as our\ncomputational needs change.</p>\n\n<h2 id=\"jupyter-lab-and-web-frontends\">Jupyter Lab and Web Frontends</h2>\n\n<p>Our scientific collaborators enjoy building Jupyter notebooks of their work.\nThis allows them to manage their code, scientific thoughts, and visual outputs\nall at once and for them serves as an artifact that they can share with their\nscientific teams and collaborators.  To help them with this we start a Jupyter\nserver on the same machine in their allocation that is running the Dask\nscheduler.  We then provide them with SSH-tunneling lines that they can\ncopy-and-paste to get access to the Jupyter server from their personal\ncomputer.</p>\n\n<p>We’ve been using the new Jupyter Lab rather than the classic notebook.  This is\nespecially convenient for us because it provides much of the interactive\nexperience that they lost by not working on their local machine.  They get a\nfile browser, terminals, easy visualization of textfiles and so on without\nhaving to repeatedly SSH into the HPC system.  We get all of this functionality\non a single connection and with an intuitive Jupyter interface.</p>\n\n<p>For now we give them a script to set all of this up.  It starts Jupyter Lab\nusing Dask and then prints out the SSH-tunneling line.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">scheduler_file</span><span class=\"o\">=</span><span class=\"s\">'scheduler.json'</span><span class=\"p\">)</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">socket</span>\n<span class=\"n\">host</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">run_on_scheduler</span><span class=\"p\">(</span><span class=\"n\">socket</span><span class=\"p\">.</span><span class=\"n\">gethostname</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">start_jlab</span><span class=\"p\">(</span><span class=\"n\">dask_scheduler</span><span class=\"p\">):</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">subprocess</span>\n    <span class=\"n\">proc</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">Popen</span><span class=\"p\">([</span><span class=\"s\">'jupyter'</span><span class=\"p\">,</span> <span class=\"s\">'lab'</span><span class=\"p\">,</span> <span class=\"s\">'--ip'</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"s\">'--no-browser'</span><span class=\"p\">])</span>\n    <span class=\"n\">dask_scheduler</span><span class=\"p\">.</span><span class=\"n\">jlab_proc</span> <span class=\"o\">=</span> <span class=\"n\">proc</span>\n\n<span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">run_on_scheduler</span><span class=\"p\">(</span><span class=\"n\">start_jlab</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"ssh -N -L 8787:%s:8787 -L 8888:%s:8888 -L 8789:%s:8789 cheyenne.ucar.edu\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"p\">,</span> <span class=\"n\">host</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<p>Long term we would like to switch to an entirely point-and-click interface\n(perhaps something like JupyterHub) but this will requires additional thinking\nabout deploying distributed resources along with the Jupyter server instance.</p>\n\n<h2 id=\"network-performance-on-infiniband\">Network Performance on Infiniband</h2>\n\n<p>The intended computations move several terabytes across the cluster.\nOn this cluster Dask gets about 1GB/s simultaneous read/write network bandwidth\nper machine using the high-speed Infiniband network.  For any commodity or\ncloud-based system this is <em>very fast</em> (about 10x faster than what I observe on\nAmazon).  However for a super-computer this is only about 30% of what’s\npossible (see <a href=\"https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne\">hardware specs</a>).</p>\n\n<p>I suspect that this is due to byte-handling in Tornado, the networking library\nthat Dask uses under the hood.  The following image shows the diagnostic\ndashboard for one worker after a communication-heavy workload.  We see 1GB/s\nfor both read and write.  We also see 100% CPU usage.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/pangeo-network.png\"><img src=\"https://mrocklin.github.io/blog/images/pangeo-network.png\" width=\"70%\" /></a></p>\n\n<p>Network performance is a big question for HPC users looking at Dask.  If we can\nget near MPI bandwidth then that may help to reduce concerns for this\nperformance-oriented community.</p>\n\n<ul>\n  <li><a href=\"https://github.com/pangeo-data/pangeo-discussion/issues/6\">Github issue for this project</a></li>\n  <li><a href=\"https://github.com/tornadoweb/tornado/issues/2147\">Github issue for Tornado</a></li>\n</ul>\n\n<p><a href=\"https://stackoverflow.com/questions/43881157/how-do-i-use-an-infiniband-network-with-dask\"><em>How do I use Infiniband network with Dask?</em></a></p>\n\n<h2 id=\"xarray-and-daskdistributed\">XArray and Dask.distributed</h2>\n\n<p>XArray was the first major project to use Dask internally.  This early\nintegration was critical to prove out Dask’s internals with user feedback.\nHowever it also means that some parts of XArray were designed well before some\nof the newer parts of Dask, notably the asynchronous distributed scheduling\nfeatures.</p>\n\n<p>XArray can still use Dask on a distributed cluster, but only with the subset of\nfeatures that are also available with the single machine scheduler.  This means\nthat persisting data in distributed RAM, parallel debugging, publishing shared\ndatasets, and so on all require significantly more work today with XArray than\nthey should.</p>\n\n<p>To address this we plan to update XArray to follow a newly proposed <a href=\"https://github.com/dask/dask/pull/1068#issuecomment-326591640\">Dask\ninterface</a>.\nThis is complex enough to handle all Dask scheduling features, but light weight\nenough not to actually require any dependence on the Dask library itself.\n(Work by <a href=\"http://jcrist.github.io/\">Jim Crist</a>.)</p>\n\n<p>We will also eventually need to look at reducing overhead for inspecting\nseveral NetCDF files, but we haven’t yet run into this, so I plan to wait.</p>\n\n<h2 id=\"future-work\">Future Work</h2>\n\n<p>We think we’re at a decent point for scientific users to start playing with the\nsystem.  We have a <a href=\"https://github.com/pangeo-data/pangeo-discussion/wiki/Getting-Started-with-Dask-on-Cheyenne\">Getting Started with Dask on Cheyenne</a>\nwiki page that our first set of guinea pig users have successfully run through\nwithout much trouble.  We’ve also identified a number of issues that the\nsoftware developers can work on while the scientific teams spin up.</p>\n\n<ol>\n  <li><a href=\"https://github.com/tornadoweb/tornado/issues/2147\">Zero copy Tornado writes</a> to <a href=\"https://github.com/pangeo-data/pangeo-discussion/issues/6\">improve network bandwidth</a></li>\n  <li><a href=\"https://github.com/pangeo-data/pangeo-discussion/issues/5\">Enable Dask.distributed features in XArray</a> by <a href=\"https://github.com/dask/dask/pull/1068\">formalizing dask’s expected interface</a></li>\n  <li><a href=\"https://github.com/pangeo-data/pangeo-discussion/issues/8\">Dynamic deployments on batch job schedulers</a></li>\n</ol>\n\n<p>We would love to engage other collaborators throughout this process.  If you or\nyour group work on related problems we would love to hear from you.  This grant\nisn’t just about serving the scientific needs of researchers at Columbia and\nNCAR, but about building long-term systems that can benefit the entire\natmospheric and oceanographic community.  Please engage on the\n<a href=\"https://github.com/pangeo-data/pangeo-discussion/issues\">Pangeo GitHub issue tracker</a>.</p>"
}