{
  "title": "Floating-Point Formats and Deep Learning",
  "link": "",
  "id": "https://www.georgeho.org/floating-point-deep-learning/",
  "updated": "2020-07-26T00:00:00Z",
  "published": "2020-07-26T00:00:00Z",
  "content": "<p>Floating-point formats are not the most glamorous or (frankly) the important\nconsideration when working with deep learning models: if your model isn&rsquo;t working well,\nthen your floating-point format certainly isn&rsquo;t going to save you! However, past a\ncertain point of model complexity/model size/training time, your choice of\nfloating-point format can have a significant impact on your model training times and\neven performance.</p>\n<p>Here&rsquo;s how the rest of this post is structured:</p>\n<ol>\n<li><a href=\"#floating-point-in-_my_-deep-learning\">Why should you, a deep learning practitioner,\ncare</a> about what floating-point format your\nmodel uses?</li>\n<li><a href=\"#floating-point-formats\">What even <em>is</em> floating-point</a>, especially these new\nfloating-point formats made specifically for deep learning?</li>\n<li><a href=\"#advice-for-practitioners\">What practical advice is there</a> on using floating-point\nformats for deep learning?</li>\n</ol>\n<h2 id=\"floating-point-in-_my_-deep-learning\">Floating-Point? In <em>My</em> Deep Learning?</h2>\n<p><a href=\"https://knowyourmeme.com/photos/6052-its-more-likely-than-you-think\">It&rsquo;s more likely than you\nthink!</a></p>\n<p>It&rsquo;s been known for quite some time that <a href=\"https://arxiv.org/abs/1502.02551\">deep neural networks can\ntolerate</a> <a href=\"https://arxiv.org/abs/1412.7024\">lower numerical\nprecision</a>. High-precision calculations turn out not\nto be that useful in training or inferencing neural networks: the additional precision\nconfers no benefit while being slower and less memory-efficient.</p>\n<p>Surprisingly, some models can even reach a higher accuracy with lower precision, which\nrecent research attributes to the <a href=\"https://arxiv.org/abs/1809.00095\">regularization effects from the lower\nprecision</a>.</p>\n<p>Finally (and this is speculation on my part â€” I haven&rsquo;t seen any experiments or papers\ncorroborating this), it&rsquo;s possible that certain complicated models <em>cannot converge</em>\nunless you use an appropriately precise format. There&rsquo;s a drift between the analytical\ngradient update and what the actual backward propagation looks like: the lower the\nprecision, the bigger the drift. I&rsquo;d expect that deep learning is particularly\nsusceptible to an issue here because there&rsquo;s a lot of multiplications, divisions and\nreduction operations.</p>\n<h2 id=\"floating-point-formats\">Floating-Point Formats</h2>\n<p>Let&rsquo;s take a quick look at three floating-point formats for deep learning. There are a\nlot more floating-point formats, but only a few have gained traction: floating-point\nformats require the appropriate hardware and firmware support, which restricts the\nintroduction and adoption of new formats.</p>\n<p>For a quick overview, Grigory Sapunov wrote a great <a href=\"https://medium.com/@moocaholic/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407\">run-down of various floating-point\nformats for deep\nlearning</a>.</p>\n<h3 id=\"ieee-floating-point-formats\">IEEE floating-point formats</h3>\n<p>These floating-point formats are probably what most people think of when someone says\n&ldquo;floating-point&rdquo;. The IEEE standard 754 sets out several formats, but for the purposes\nof deep learning we are only interested three:\n<a href=\"https://en.wikipedia.org/wiki/Half-precision_floating-point_format\">FP16</a>,\n<a href=\"https://en.wikipedia.org/wiki/Single-precision_floating-point_format\">FP32</a> and\n<a href=\"https://en.wikipedia.org/wiki/Double-precision_floating-point_format\">FP64</a> (a.k.a.\nhalf-, single- and double-precision floating-point formats)<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>.</p>\n<p>Let&rsquo;s take FP32 as an example. Each FP32 number is a sequence of 32 bits,\n$b_{31} b_{30} &hellip; b_{0}$. Altogether, this sequence represents the real number</p>\n<p>$$ (-1)^{b_{31}} \\cdot 2^{(b_{30} b_{29} &hellip; b_{23}) - 127} \\cdot (1.b_{22} b_{21} &hellip; b_{0})_2 $$</p>\n<p>Here, $b_{31}$ (the <em>sign bit</em>) determines the sign of the represented value.</p>\n<p>$b_{30}$ through $b_{23}$ determine the magnitude or scale of the represented value\n(notice that a change in any of these bits drastically changes the size of the\nrepresented value). These bits are called the <em>exponent</em> or <em>scale bits</em>.</p>\n<p>Finally, $b_{22}$ through $b_{0}$ determine the precise value of the represented\nvalue. These bits are called the <em>mantissa</em> or <em>precision bits</em>.</p>\n<p>Obviously, the more bits you have, the more you can do. Here&rsquo;s how the three formats\nbreak down:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\"></th>\n<th style=\"text-align:right\">Sign Bits</th>\n<th style=\"text-align:right\">Exponent (Scale) Bits</th>\n<th style=\"text-align:right\">Mantissa (Precision) Bits</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">FP16</td>\n<td style=\"text-align:right\">1</td>\n<td style=\"text-align:right\">5</td>\n<td style=\"text-align:right\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FP32</td>\n<td style=\"text-align:right\">1</td>\n<td style=\"text-align:right\">8</td>\n<td style=\"text-align:right\">23</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">FP64</td>\n<td style=\"text-align:right\">1</td>\n<td style=\"text-align:right\">11</td>\n<td style=\"text-align:right\">53</td>\n</tr>\n</tbody>\n</table>\n<p>There are some details that I&rsquo;m leaving out here (e.g. how to represent NaNs, positive\nand negative infinities), but this is largely how floating point numbers work. A lot\nmore detail can be found on the <a href=\"https://en.wikipedia.org/wiki/Floating-point_arithmetic#IEEE_754:_floating_point_in_modern_computers\">Wikipedia\npage</a>\nand of course the <a href=\"https://ieeexplore.ieee.org/document/8766229\">latest revision of the IEEE standard\n754</a> itself.</p>\n<p>FP32 and FP64 are widely supported by both software (C/C++, PyTorch, TensorFlow) and\nhardware (x86 CPUs and most NVIDIA/AMD GPUs).</p>\n<p>FP16, on the other hand, is not as widely supported in software (you need to use <a href=\"http://half.sourceforge.net/\">a\nspecial library</a> to use them in C/C++). However, since\ndeep learning is trending towards favoring FP16 over FP32, it has found support in the\nmain deep learning frameworks (e.g. <code>tf.float16</code> and <code>torch.float16</code>). In terms of\nhardware, FP16 is not supported in x86 CPUs as a distinct type, but is well-supported on\nmodern GPUs.</p>\n<h3 id=\"google-bfloat16\">Google BFloat16</h3>\n<p>BFloat16 (a.k.a. the Brain Floating-Point Format, after Google Brain) is basically the\nsame as FP16, but 3 mantissa bits become exponent bits (i.e. bfloat16 trades 3 bits'\nworth of precision for scale).</p>\n<figure class=\"align-center\">\n<img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/bfloat16.png\" alt=\"Diagram illustrating the number and type of bits in bfloat16.\">\n<figcaption>The number and type of bits in bfloat16. Source: <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus\">Google Cloud blog</a>.</figcaption>\n</figure>\n<p>When it comes to deep learning, there are generally three &ldquo;flavors&rdquo; of values: weights,\nactivations and gradients. Google suggests storing weights and gradients in FP32, and\nstoring activations in bfloat16. However, in particularly gracious circumstances,\nweights can be stored in bfloat16 without a significant performance degradation.</p>\n<p>You can read a lot more on the <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus\">Google Cloud\nblog</a>,\nand <a href=\"https://arxiv.org/abs/1905.12322\">this paper by Intel and Facebook studying the bfloat16\nformat</a>.</p>\n<p>In terms of software support, bfloat16 is not supported in C/C++, but is supported in\nTensorFlow (<a href=\"https://www.tensorflow.org/api_docs/python/tf#bfloat16\"><code>tf.bfloat16</code></a>) and\nPyTorch (<a href=\"https://www.tensorflow.org/api_docs/python/tf#bfloat16\"><code>torch.bfloat16</code></a>).</p>\n<p>In terms of hardware support, it is supported by <a href=\"https://en.wikipedia.org/wiki/Cooper_Lake_(microarchitecture)\">some modern\nCPUS</a>, but the real\nsupport comes out in GPUs and ASICs. At the time of writing, bfloat16 is supported by\nthe NVIDIA A100 (the first GPU to support it!), and <a href=\"https://www.techpowerup.com/260344/future-amd-gpu-architecture-to-implement-bfloat16-hardware\">will be supported in future AMD\nGPUs</a>.\nAnd of course, it is supported by Google TPU v2/v3.</p>\n<h3 id=\"nvidia-tensorfloat\">NVIDIA TensorFloat</h3>\n<p>Strictly speaking, this isn&rsquo;t really its own floating-point format, just an overzealous\nbranding of the technique that NVIDIA developed to train in mixed precision on their\nTensor Core hardware<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup>.</p>\n<p>An NVIDIA TensorFloat (a.k.a. TF32) is just a 32-bit float that drops 13 precision bits\nin order to execute on Tensor Cores. Thus, it has the precision of FP16 (10 bits), with\nthe range of FP32 (8 bits). However, if you&rsquo;re not using Tensor Cores, it&rsquo;s just a\n32-bit float; if you&rsquo;re only thinking about storage, it&rsquo;s just a 32-bit float.</p>\n<figure class=\"align-center\">\n<img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/tensorfloat32.png\" alt=\"Diagram illustrating the number and type of bits in an NVIDIA TensorFloat\">\n<figcaption>The number and type of bits in an NVIDIA TensorFloat. Source: <a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">NVIDIA blog</a>.</figcaption>\n</figure>\n<p>One distinct advantage of TF32 is that they&rsquo;re kind of like FP32. To quote from the\nNVIDIA developer blog,</p>\n<blockquote>\n<p>Applications using NVIDIA libraries enable users to harness the benefits of TF32 with no\ncode change required. TF32 Tensor Cores operate on FP32 inputs and produce results in\nFP32. Non-matrix operations continue to use FP32.</p>\n</blockquote>\n<p>You can read more about TF32 <a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">on the NVIDIA\nblog</a>, and\nabout its hardware support in the Ampere architecture on <a href=\"https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\">the NVIDIA developer\nblog</a>.</p>\n<p>TF32 is not in the C/C++ standard at all, but is supported in <a href=\"https://developer.nvidia.com/blog/cuda-11-features-revealed/\">CUDA\n11</a>.</p>\n<p>Hardware-wise, the NVIDIA A100 is the first GPU (and, at the time of writing, the only\ndevice) supporting TF32.</p>\n<h2 id=\"advice-for-practitioners\">Advice for Practitioners</h2>\n<p>The first thing to say is that floating-point formats are <em>by no means</em> the most\nimportant consideration for your deep learning model â€” not even close. Floating-point\nformats will most likely only make a difference for very large or complex models, for\nwhich fitting the model on GPU memory is a challenge, or for which training times are\nexcruciatingly long.</p>\n<p>The second thing to say is that any practical advice has to be heavily dependent on what\nhardware you have available to you.</p>\n<h3 id=\"automatic-mixed-precision-amp-training--a-good-default\">Automatic mixed precision (AMP) training â€” a good default</h3>\n<p>Most deep learning stacks support mixed-precision training, which is a pretty good\ndefault option to reap some of the benefits of low-precision training, while still\nreasonably avoiding underflow and overflow problems.</p>\n<p>TensorFlow supports <a href=\"https://www.tensorflow.org/guide/mixed_precision\">mixed-precision training\nnatively</a>, whereas the <a href=\"https://github.com/NVIDIA/apex\">NVIDIA Apex\nlibrary</a> makes automatic mixed precision training\navailable in PyTorch. To get started, take a look at NVIDIA&rsquo;s <a href=\"https://developer.nvidia.com/automatic-mixed-precision\">developer guide for\nAMP</a>, and <a href=\"https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html\">documentation for\ntraining in mixed\nprecision</a>.</p>\n<p>It&rsquo;s worth going over the gist of mixed precision training. There are basically two main\ntricks:</p>\n<ol>\n<li><em>Loss scaling:</em> multiply the loss by some large number, and divide the gradient\nupdates by this same large number. This avoids the loss underflowing (i.e. clamping\nto zero because of the finite precision) in FP16, while still maintaining faithful\nbackward propagation.</li>\n<li><em>FP32 master copy of weights</em>: store the weights themselves in FP32, but cast them to\nFP16 before doing the forward and backward propagation (to reap the performance\nbenefits). During the weight update, the FP16 gradients are cast to FP32 to update\nthe master copy.</li>\n</ol>\n<p>You can read more about these techniques in <a href=\"https://arxiv.org/abs/1710.03740\">this paper by NVIDIA and Baidu\nResearch</a>, or on the accompanying <a href=\"https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/\">blog post by\nNVIDIA</a>.</p>\n<h3 id=\"alternative-floating-point-formats--make-sure-itll-be-worth-it\">Alternative floating-point formats â€” make sure it&rsquo;ll be worth it</h3>\n<p>If you&rsquo;ve already trained your model in mixed precision, it might not be worth the time\nor effort to port your code to take advantage of an alternative floating-point format\nand bleeding edge hardware.</p>\n<p>However, if you choose to go that route, make sure your use case really demands it.\nPerhaps you can&rsquo;t scale up your model without using bfloat16, or you really need to cut\ndown on training times.</p>\n<p>Unfortunately, I don&rsquo;t have a well-informed opinion on how bfloat16 stacks up against\nTF32, so &ldquo;do your homework&rdquo; is all I can advise. However, since the NVIDIA A100s only\njust (at the time of writing) dropped into the market, it&rsquo;ll be interesting to see what\nthe machine learning community thinks of the various low precision options available.</p>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>Technically speaking, there are <a href=\"https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format\">quadruple-</a> and <a href=\"https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format\">octuple-precision</a> floating-point formats, but those are pretty rarely used, and certainly unheard of in deep learning.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>A Tensor Core is essentially a mixed-precision FP16/FP32 core, which NVIDIA has optimized for deep learning applications.&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}