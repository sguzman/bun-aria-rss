{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-3606260649484710280",
  "published": "2022-10-25T12:52:00.000-07:00",
  "updated": "2022-10-25T12:52:17.444-07:00",
  "category": [
    "",
    "",
    "",
    ""
  ],
  "title": "Open Images V7 — Now Featuring Point Labels",
  "content": "<span class=\"byline-author\">Posted by Rodrigo Benenson, Research Scientist, Google Research</span> <p><a href=\"https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html\">Open Images</a> is a computer vision dataset covering ~9 million images with labels spanning thousands of object categories. Researchers around the world use Open Images to train and evaluate computer vision models. Since the <a href=\"https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html\">initial release</a> of Open Images in 2016, which included image-level labels covering 6k categories, we have provided multiple updates to enrich annotations and expand the potential use cases of the dataset. Through several releases, we have added <a href=\"https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html\">image-level labels for over 20k categories</a> on all images and <a href=\"https://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html\">bounding box annotations</a>, <a href=\"https://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html\">visual relations</a>, <a href=\"https://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html\">instance segmentations</a>, and <a href=\"https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html\">localized narratives</a> (synchronized voice, mouse trace, and text caption) on a subset of 1.9M images.  </p><a name='more'></a>  <p>Today, we are happy to announce the release of <a href=\"https://storage.googleapis.com/openimages/web/index.html\">Open Images V7</a>, which expands the Open Images dataset even further with a new annotation type called <em>point-level labels</em> and includes a new all-in-one visualization tool that allows a better exploration of the rich data available. </p> <div style=\"line-height: 40%;\">    <br /></div><h2>Point Labels</h2><p>The main strategy used to collect the new point-level label annotations leveraged suggestions from a machine learning (ML) model and human verification. First, the ML model selected points of interest and asked a yes or no question, e.g., “is this point on a pumpkin?”. Then, human annotators spent an average of 1.1 seconds answering the yes or no questions. We aggregated the answers from different annotators over the same question and assigned a final “yes”, “no”, or “unsure” label to each annotated point. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s474/image3.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"396\" data-original-width=\"474\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVHhZfSUBgnAKpUO9qhA0jtxKym_bRLFaZR-zpVaCab_t1-tFYVjslsZf9OcQpfOPppcKqWNKh78ojdYsF0lDilnTfN_GfOytHRPT6gA1ho7uRMmFOoTPeW1XM19eekxXxI3dwk2JAJHKrk7Jp2QNpmNTfS0hrNVTOPM5QU3FYxEXGWWN9G-odpMLMuw/s16000/image3.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Illustration of the annotations interface. <br />(<a href=\"https://c2.staticflickr.com/9/8329/8140854650_2a93ae7105_z.jpg\">Image</a> by <a href=\"https://www.flickr.com/people/lenore-m/\">Lenore Edman</a>, under <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0 license</a>)</td></tr></tbody></table> <p>For each annotated image, we provide a collection of points, each with a “yes” or “no” label for a given class. These points provide sparse information that can be used for the semantic segmentation task. We collected a total of 38.6M new point annotations (12.4M with “yes” labels) that cover 5.8 thousand classes and 1.4M images. </p> <p>By focusing on point labels, we expanded the number of images annotated and categories covered. We also concentrated the efforts of our annotators on efficiently collecting useful information. Compared to our instance segmentation, the new points include 16x more classes and cover more images. The new points also cover 9x more classes than our box annotations. Compared to existing segmentation datasets, like <a href=\"https://paperswithcode.com/dataset/pascal-voc\">PASCAL VOC</a>, <a href=\"https://cocodataset.org/#home\">COCO</a>, <a href=\"https://www.cityscapes-dataset.com/\">Cityscapes</a>, <a href=\"https://www.lvisdataset.org/\">LVIS</a>, or <a href=\"https://groups.csail.mit.edu/vision/datasets/ADE20K/\">ADE20K</a>, our annotations cover more classes and more images than previous work. The new point label annotations are the first type of annotation in Open Images that provides localization information for both things (countable objects, like cars, cats, and catamarans), and stuff categories (uncountable objects like grass, granite, and gravel). Overall, the newly collected data is roughly equivalent to two years of human annotation effort. </p> <p>Our initial experiments show that this type of sparse data is suitable for both training and evaluating segmentation models. Training a model directly on sparse data allows us to reach comparable quality to training on dense annotations. Similarly, we show that one can directly compute the traditional semantic segmentation <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">intersection-over-union</a> (IoU) metric over sparse data. The ranking across different methods is preserved, and the sparse IoU values are an accurate estimate of its dense version. See our <a href=\"https://storage.googleapis.com/openimages/web_v7/2022_pointillism_arxiv.pdf\">paper</a> for more details.  </p> <p>Below, we show four example images with their point-level labels, illustrating the rich and diverse information these annotations provide. Circles ⭘ are “yes” labels, and squares <b>☐</b> are “no” labels. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s1338/image1.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1309\" data-original-width=\"1338\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi54uBkZ9pta9G1pdLBtSLeUd1ll64NbfCMt7TFj-auHmlA_4FED5zmAIXjU7yXHmGeun1OaUtUEhmo0sDT5qB9Dn259wal1HM0fnu5fzM_pjrvhyTUfiN0_brbZuzt4LkJnkLlMm6_1P_GX8zzkLqvvJRm1hsbdPgvwJ7xfi2bo-BVJiC5wWmBP1yhTA/s16000/image1.jpg\" /></a></td>  <td><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s1527/image7.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1252\" data-original-width=\"1527\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdzdC9OBwyNDBCF7E0isRWF2vFusLpNYaBM-lXS34mUJ5AS-97GFS1vd-eOkuyy_CQMtV4Hv0NuQ4IXScJYiAroQn1CDIQurxDbEF5UrP-5ImLKcGGCWbrHLOgR_BxOfR3_foNiIpSUFy4f8AiuE5FlIbOo2GvQhdMwybLSnxhNZrVQLk1DeHcHwlOwg/s16000/image7.png\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s1650/image6.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1650\" data-original-width=\"1544\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlvNblnDZa_eOUixtNRquY1E_cwQrgcWRVFqIaXQravN-HjmeMTjVDbZkRunVQUQ9iQCkXq_KnOh5LrmVJbKoGZH87qI9BeaJjgnw5tNpCao-o0pPuTLmAfkTKRKPt5T5b4L8bGxQYP6fG4CyzFIxFkYyNJdohcdWMBZaPvx3JEiDAa_Zczz4e8yyd4g/s16000/image6.png\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s1684/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1650\" data-original-width=\"1684\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQMEhuUdYlNl4rf8RrlDg7IVznnwk3nfg2WgzEXSAMYOAhJuhFWpLGOsbTgG_5gYfMcA5JHfUJtncic3w450mWwtivUmGfoIDN3-LeYnQ9GlNvC3X2LB5xcqA456nnxLWAx1bvExBhyMiqCvhLzBzCxsaU64MbMcYorqmGTp2k2hsxYoS08DaTpmInUA/s16000/image4.png\" /></a></td>  </tr></tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Four example images with point-level labels. <br />Images by <a href=\"https://c7.staticflickr.com/5/4088/5070100626_57e898dfdf_z.jpg\">Richie</a> <a href=\"https://www.flickr.com/people/puroticorico/\">Diesterheft</a>, <a href=\"https://c3.staticflickr.com/4/3928/15385724218_0a4b86d2f9_o.jpg\">John AM</a> <a href=\"https://www.flickr.com/people/nuevajam/\">Nueva</a>, <a href=\"https://farm8.staticflickr.com/4073/4793785065_bd8509a087_o.jpg\">Sarah</a> <a href=\"https://www.flickr.com/people/sackerman519/\">Ackerman</a>, and <a href=\"https://c5.staticflickr.com/5/4101/4908677641_458e9a060f_o.jpg\">C</a> <a href=\"https://www.flickr.com/people/madmarlin_/\">Thomas</a>, all under <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0 license.</a></td></tr></tbody></table>  <div style=\"line-height: 40%;\">    <br /></div><p></p><h2>New Visualizers</h2><p>In addition to the new data release, we also expanded <a href=\"https://storage.googleapis.com/openimages/web/visualizer/index.html\">the available visualizations</a> of the Open Images annotations. The Open Images website now includes dedicated visualizers to explore the localized narratives annotations, the new point-level annotations, and a new all-in-one view. This new all-in-one view is available for the subset of 1.9M densely annotated images and allows one to explore the rich annotations that Open Images has accumulated over seven releases. On average these images have annotations for 6.7 image-labels (classes), 8.3 boxes, 1.7 relations, 1.5 masks, 0.4 localized narratives and 34.8 point-labels per image. </p> <p>Below, we show two example images with various annotations in the all-in-one visualizer. The figures show the image-level labels, bounding boxes, box relations, instance masks, localized narrative mouse trace and caption, and point-level labels. The <b>+</b> classes have positive annotations (of any kind), while <b>–</b> classes have only negative annotations (image-level or point-level).  </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s1571/image9.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1439\" data-original-width=\"1571\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjm9yNIIz238CuHM_7zLK8RWs5H4tF8Kt6WPu5Pdjk9bdKeOGL89nvDUwIaaIIrSPcL-G0eEye9cbz6vpwIX0Fnntwq-WnPY0Cxz_hvX6-tHTPNQUtUnWQDNxnANcaARn0ZWpfjp3OXVUq6cm4IFMhCaas5tuZ2Uh_gToAsHdXbbjF4T4c90M3Swyj_FQ/s16000/image9.png\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s1433/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1433\" data-original-width=\"1132\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjNCFmwrp7dp8JxSO4qVJHVBbnEQugQPVT_b80AYWsqKGdg1to0u_eLf5K1_BBQFt8bqH7b2NsTSit9Jq4UYe_-23NQ-uBiPplHhIqU7fMKA5a3msPDEpmgi9yaqaTQJBlV6Z3yfmI7lFTqdbpIqlzNi_NxB2kGlERRI1fS10iDug-v_OxAmWez_Cedgw/s16000/image2.png\" /></a></td>  </tr></tbody></table> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Two example images with various annotations in the all-in-one visualizer. <br />Images by <a href=\"https://c1.staticflickr.com/4/3835/15014236762_04033666d5_z.jpg\">Jason</a> <a href=\"https://www.flickr.com/people/jasonparis/\">Paris</a>, and <a href=\"https://c7.staticflickr.com/5/4015/4359099851_701ccfd762_z.jpg\">Rubén</a> <a href=\"https://www.flickr.com/people/vike/\">Vique</a>, all under <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0 license.</a></td></tr></tbody></table>   <div style=\"line-height: 40%;\">    <br /></div><h2>Conclusion</h2><p>We hope that this new data release will enable computer vision research to cover ever more diverse and challenging scenarios. As the quality of automated semantic segmentation models improves over common classes, we want to move towards the long tail of visual concepts, and sparse point annotations are a step in that direction. More and more works are exploring how to use such sparse annotations (e.g., as <a href=\"https://openreview.net/forum?id=wt6QxYgddsl\">supervision</a> for <a href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Pointly-Supervised_Instance_Segmentation_CVPR_2022_paper.html\">instance</a> <a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880599.pdf\">segmentation</a> or <a href=\"https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_With_ICCVW_2021_paper.html\">semantic segmentation</a>), and Open Images V7 contributes to this research direction. We are looking forward to seeing what you will build next. </p> <div style=\"line-height: 40%;\">    <br /></div><h2>Acknowledgements</h2><p><i>Thanks to <a href=\"https://sites.google.com/corp/view/vittoferrari\">Vittorio Ferrari</a>, <a href=\"https://jponttuset.cat/\">Jordi Pont-Tuset</a>, <a href=\"https://akuznetso.github.io/\">Alina Kuznetsova</a>, Ashlesha Sadras, and the annotators team for their support creating this new data release.</i></p><p></p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}