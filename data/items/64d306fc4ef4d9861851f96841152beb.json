{
  "title": "November finds in #arxiv and NIPS 2013",
  "link": "https://mirror2image.wordpress.com/2013/12/01/november-finds-in-arxiv-and-nips-2013/",
  "dc:creator": "mirror2image",
  "pubDate": "Sun, 01 Dec 2013 13:56:01 +0000",
  "category": [
    "arxiv",
    "compressed sensing",
    "Deep Learning",
    "machine learning",
    "Math",
    "nonlinear optimization"
  ],
  "guid": "http://mirror2image.wordpress.com/?p=1592",
  "description": "This is &#8220;find in arxiv&#8221; reposts form my G+ stream for November. NIPS 2013 Accelerating Stochastic Gradient Descent using Predictive Variance Reduction Stochastic gradient (SGD) is the major tool for Deep Learning. However if you look at the plot of cost function over iteration for   SGD you will see that after quite fast descent it [&#8230;]",
  "content:encoded": "<p>This is &#8220;find in arxiv&#8221; reposts form <a title=\"Googleplus\" href=\"https://plus.google.com/u/0/+SergeyTen/posts\">my G+ stream</a> for November.</p>\n<p><b>NIPS 2013</b></p>\n<p><i>Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</i></p>\n<p><i></i>Stochastic gradient (SGD) is the major tool for Deep Learning. However if you look at the plot of cost function over iteration for   SGD you will see that after quite fast descent it becoming extremely slow, and error decrease could even become non-monotonous.  Author explain by necessity of trade of between the step size and variance of random factor &#8211; more precision require smaller variance but that mean smaller descent step and slower convergence. &#8220;Predictive variance&#8221; author suggest to mitigate problem is the same old &#8220;adding back the noise&#8221; trick, used for example in Split Bregman. Worth reading IMHO.</p>\n<p><a href=\"http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction\" rel=\"nofollow\">http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction</a></p>\n<p><i>Predicting Parameters in Deep Learning</i><br />\nOutput of the first layer of ConvNet is quite smooth, and that could be used for dimensionality reduction, using some dictionary, learned or fixed(just some simple kernel). For ConvNet predicting 75% of parameters with fixed dictionary have negligible effect on accuracy.<br />\n<a href=\"http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning\" rel=\"nofollow\">http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning</a></p>\n<p><i>Learning a Deep Compact Image Representation for Visual Tracking</i><br />\nApplication of ADMM (Alternating Direction Method of Multipliers, of which Split Bregman again one of the prominent examples) to denoising autoencoder with sparsity.<br />\n<a href=\"http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking\" rel=\"nofollow\">http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking</a></p>\n<p><i>Deep Neural Networks for Object Detection</i><br />\nPeople from Google are playing with Alex Krizhevsky&#8217;s ConvNet<br />\n<a href=\"http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection\" rel=\"nofollow\">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection</a></p>\n<p><b>&#8211;arxiv (last)&#8211;</b></p>\n<p><i>Are all training examples equally valuable?</i><br />\nIt&#8217;s intuitively obvious that some training sample are making training process worse. The question is &#8211; how to find wich sample should be removed from training? Kind of removing outliers. Authors define &#8220;training value&#8221; for each sample of binary classifier.<br />\n<a href=\"http://arxiv.org/pdf/1311.7080\" rel=\"nofollow\">http://arxiv.org/pdf/1311.7080</a></p>\n<p><i>Finding sparse solutions of systems of polynomial equations via group-sparsity optimization</i><br />\nFinding sparse solution of polynomial system with lifting method.<br />\nI still not completely understand why quite weak structure constraint is enough for found approximation to be solution with high probability. It would be obviously precise  for binary 0-1 solution, but why for general sparse?<br />\n<a href=\"http://arxiv.org/abs/1311.5871\" rel=\"nofollow\">http://arxiv.org/abs/1311.5871</a></p>\n<p><i>Semi-Supervised Sparse Coding</i><br />\nBig dataset with small amount of labeled samples &#8211; what to do? Use unlabeled samples for sparse representation. And train labeled samples in sparse representation.<br />\n<a href=\"http://arxiv.org/abs/1311.6834\" rel=\"nofollow\">http://arxiv.org/abs/1311.6834</a><br />\nFrom the same author, similar theme &#8211; Cross-Domain Sparse Coding<br />\nTwo domain training &#8211; use cross domain data representation to map all the samples from both source and target domains to a data representation space with a common distribution across domains.<br />\n<a href=\"http://arxiv.org/abs/1311.7080\" rel=\"nofollow\">http://arxiv.org/abs/1311.7080</a></p>\n<p><i>Robust Low-rank Tensor Recovery: Models and Algorithms</i><br />\nMore of tensor decomposition with trace norm<br />\n<a href=\"http://arxiv.org/abs/1311.6182\" rel=\"nofollow\">http://arxiv.org/abs/1311.6182</a></p>\n<p><i>Complexity of Inexact Proximal Newton methods</i><br />\nApplication of Proximal Newton (BFGS) to subset of coordinates each step &#8211; active set coordinate descent.<br />\n<a href=\"http://arxiv.org/pdf/1311.6547\" rel=\"nofollow\">http://arxiv.org/pdf/1311.6547</a></p>\n<p><i>Computational Complexity of Smooth Differential Equations</i><br />\nPolynomial-memory complexity of ordinary differential equations.<br />\n<a href=\"http://arxiv.org/abs/1311.5414\" rel=\"nofollow\">http://arxiv.org/abs/1311.5414</a></p>\n<p><strong>&#8211;arxiv (2)&#8211;</strong></p>\n<p><b>Deep Learning</b></p>\n<p><i>Visualizing and Understanding Convolutional Neural Networks</i><br />\nThis is exploration of Alex Krizhevsky&#8217;s  ConvNet<br />\n( <a href=\"https://code.google.com/p/cuda-convnet/\" rel=\"nofollow\">https://code.google.com/p/cuda-convnet/</a> )<br />\nusing &#8220;deconvnet&#8221; approach &#8211; using deconvolution on output of each layer and visualizing it. Results looks interesting &#8211; strting from level 3 it&#8217;s something like thersholded edge enchantment, or sketch. Also there are evidences supporting &#8220;learn once use everywhere&#8221; approach &#8211; convnet trained on ImageNet is also effective on other datasets<br />\n<a href=\"http://arxiv.org/abs/1311.2901\" rel=\"nofollow\">http://arxiv.org/abs/1311.2901</a></p>\n<p><i>Unsupervised Learning of Invariant Representations in Hierarchical Architectures</i><br />\nAnother paper on why and how deep learning works.<br />\nAttempt to build theoretical framework for invariant features in deep learning. Interesting result &#8211; Gabor wavelets are optimal filters for simultaneous scale and translation invariance. Relations to sparsity and scattering transform<br />\n<a href=\"http://arxiv.org/abs/1311.4158\" rel=\"nofollow\">http://arxiv.org/abs/1311.4158</a></p>\n<p><b>Computer Vision</b><br />\n<i>An Experimental Comparison of Trust Region and Level Sets</i><br />\nTrust regions method for energy-based segmentation.<br />\nTrust region is one of the most important tools in optimization, especially non-convex.<br />\n<a href=\"http://en.wikipedia.org/wiki/Trust_region\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Trust_region</a><br />\n<a href=\"http://arxiv.org/abs/1311.2102\" rel=\"nofollow\">http://arxiv.org/abs/1311.2102</a></p>\n<p><i>Blind Deconvolution with Re-weighted Sparsity Promotion</i><br />\nUsing reweighted L2 norm for sparsity in blind deconvolution<br />\n<a href=\"http://arxiv.org/abs/1311.4029\" rel=\"nofollow\">http://arxiv.org/abs/1311.4029</a></p>\n<p><b>Optimization</b></p>\n<p><i>Online universal gradient methods</i><br />\nabout  Nesterov&#8217;s universal gradient method (<br />\n<a href=\"http://www.optimization-online.org/DB_FILE/2013/04/3833.pdf\" rel=\"nofollow\">http://www.optimization-online.org/DB_FILE/2013/04/3833.pdf</a> )<br />\nIt use Bregman distance and related to ADMM.<br />\nThe paper is application of universal gradient method  to online learning and give bound on regret function.<br />\n<a href=\"http://arxiv.org/abs/1311.3832\" rel=\"nofollow\">http://arxiv.org/abs/1311.3832</a></p>\n<p><b>CS</b><br />\n<i>A Component Lasso</i><br />\nApproximate covariance matrix with block-diagonal matrix and apply Lasso to each block separately<br />\n<a href=\"http://arxiv.org/abs/1311.4472\" rel=\"nofollow\">http://arxiv.org/abs/1311.4472</a></p>\n<p>_FuSSO: Functional Shrinkage and Selection Operator<br />\nLasso in functional space with some orthogonal basis_<br />\n<a href=\"http://arxiv.org/abs/1311.2234\" rel=\"nofollow\">http://arxiv.org/abs/1311.2234</a></p>\n<p><i>Non-Convex Compressed Sensing Using Partial Support Information</i><br />\nMore of Lp norm for sparse recovery. Reweighted this time.<br />\n<a href=\"http://arxiv.org/abs/1311.3773\" rel=\"nofollow\">http://arxiv.org/abs/1311.3773</a></p>\n<p><strong>&#8211;arxiv  (1)&#8211;</strong></p>\n<p><b>Optimization, CS</b><br />\nScalable Frames and Convex Geometry<br />\nFrame theory is a basis(pun intended) of wavelets theory, compressed sening and overcomplete dictionaries in ML<br />\n<a href=\"http://en.wikipedia.org/wiki/Frame_of_a_vector_space\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Frame_of_a_vector_space</a><br />\nHere is a discussion how to make &#8220;tight frame&#8221;<br />\n<a href=\"http://en.wikipedia.org/wiki/Frame_of_a_vector_space#Tight_frames\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Frame_of_a_vector_space#Tight_frames</a><br />\nfrom an ordinary frame by scaling <i>m</i> of its components<br />\nInteresting geometric insight provided &#8211; to do it <i>m</i>components of frame should make &#8220;blunt cone&#8221;<br />\n<a href=\"http://en.wikipedia.org/wiki/Convex_cone#Blunt_and_pointed_cones\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Convex_cone#Blunt_and_pointed_cones</a><br />\n<a href=\"http://arxiv.org/abs/1310.8107\" rel=\"nofollow\">http://arxiv.org/abs/1310.8107</a></p>\n<p>Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization<br />\nSome bounds for convergence of dictionary learning. Converge id initial error is O(1/s^2), s- sparcity level<br />\n<a href=\"http://arxiv.org/abs/1310.7991\" rel=\"nofollow\">http://arxiv.org/abs/1310.7991</a></p>\n<p><b>Robust estimators</b><br />\nRobustness of ℓ1 minimization against sparse outliers and its implications in Statistics and Signal Recovery<br />\nThis is another exploration of L1 estimator. It happens (contrary to common sense) that L1 is not especially robust from &#8220;breakdown point&#8221; point of view if there is no constraint of noise. However it practical usefulness can be explained that it&#8217;s very robust to sparse noise<br />\n<a href=\"http://arxiv.org/abs/1310.7637\" rel=\"nofollow\">http://arxiv.org/abs/1310.7637</a></p>\n<p><b>Numerical</b><br />\nLocal Fourier Analysis of Multigrid Methods with Polynomial Smoothers and Aggressive coarsening<br />\nOverrelaxaction with Chebyshev weights on the fine grid, with convergence analysis.<br />\n<a href=\"http://arxiv.org/abs/1310.8385\" rel=\"nofollow\">http://arxiv.org/abs/1310.8385</a></p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}