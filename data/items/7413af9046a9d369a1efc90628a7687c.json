{
  "title": "Introduction to OpenAI Scholars 2020",
  "link": "",
  "published": "2020-02-14T09:00:00-08:00",
  "updated": "2020-02-14T09:00:00-08:00",
  "author": {
    "name": "Cathy Yeh"
  },
  "id": "tag:efavdb.com,2020-02-14:/openai-scholars-intro",
  "summary": "<p>Two weeks ago, I started at the <a href=\"https://openai.com/blog/openai-scholars-spring-2020/\">OpenAI Scholars</a> program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in …</p>",
  "content": "<p>Two weeks ago, I started at the <a href=\"https://openai.com/blog/openai-scholars-spring-2020/\">OpenAI Scholars</a> program, which provides the opportunity to study and work full time on a project in an area of deep learning over 4 months.  I’m having a blast!  It’s been a joy focusing 100% on learning and challenging myself in an atmosphere full of friendly intellectual energy and&nbsp;drive.</p>\n<p>My mentor is Jerry Tworek, an OpenAI research scientist who works on reinforcement learning (<span class=\"caps\">RL</span>) in robotics, and I’ve also chosen to focus on <span class=\"caps\">RL</span> during the program.  I constructed a <a href=\"https://docs.google.com/document/d/1MlM5bxMqqiUIig5I6Y28fegvbqokjuvS2llVd2dIIRE/edit?usp=sharing\">syllabus</a> that will definitely evolve over time, but I’ll try to keep it up-to-date to serve as a useful record for myself and a guide for others who might be interested in a similar course of&nbsp;study.</p>\n<p>Some casual notes from the last two&nbsp;weeks:</p>\n<p>(1) There are manifold benefits to working on a topic that is in my mentor’s area of expertise.  For example, I’ve already benefited from Jerry’s intuition around hyperparameter tuning and debugging <span class=\"caps\">RL</span>-specific problems, as well as his guidance on major concepts I should focus on in my first month, namely, model-free <span class=\"caps\">RL</span> divided broadly into Q-Learning and Policy&nbsp;Gradients.</p>\n<p>(2) <strong>Weights <span class=\"amp\">&amp;</span> Biases</strong> at <a href=\"wandb.com\">wandb.com</a> is a fantastic free tool for tracking machine learning experiments that many people use at OpenAI.  It was staggeringly simple to integrate wandb with my training script &#8212; both for local runs and in the cloud!  Just ~4 extra lines of code, and logged metrics automagically appear in my wandb dashboard, with auto-generated plots grouped by experiment name, saved artifacts,&nbsp;etc.</p>\n<p>Here&#8217;s an example of a <a href=\"https://app.wandb.ai/frangipane/dqn?workspace=user-frangipane\">dashboard</a> tracking experiments for my first attempt at implementing a deep <span class=\"caps\">RL</span> algorithm from scratch (<span class=\"caps\">DQN</span>, or Deep Q learning).  The script that is generating the experiments is still a work in progress, but you can see how few lines were required to integrate with wandb <a href=\"https://github.com/frangipane/reinforcement-learning/blob/master/DQN/dqn.py\">here</a>.  Stay tuned for a blog post about <span class=\"caps\">DQN</span> itself in the&nbsp;future!</p>\n<p>(3) I&#8217;ve found it very helpful to parallelize reading Sutton and Barto&#8217;s <a href=\"http://incompleteideas.net/book/RLbook2018.pdf\">Reinforcement Learning: An Introduction</a>, <em>the</em> classic text on <span class=\"caps\">RL</span>, with watching David Silver&#8217;s pedagogical online <a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">lectures</a>.  Silver&#8217;s lectures follow the book closely for the first few chapters, then start condensing several chapters per lecture beginning around lecture 4 or 5 &#8212; helpful since I&#8217;m aiming to ramp up on <span class=\"caps\">RL</span> over a short period of time!  Silver also supplements with insightful explanations and material that aren&#8217;t covered in the book, e.g. insights about the convergence properties of some <span class=\"caps\">RL</span>&nbsp;algorithms.</p>\n<p>Note, Silver contributed to the work on Deep Q Learning applied to Atari that generated a lot of interest in deep <span class=\"caps\">RL</span> beginning in 2013, leading to a <a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\">publication</a> in Nature in 2015, so his lecture 6 on Value Function Approximation (<a href=\"https://www.youtube.com/watch?v=UoPei5o4fps\">video</a>, <a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf\">slides</a>) is a perfect accompaniment to reading the&nbsp;paper.</p>",
  "category": [
    "",
    "",
    "",
    "",
    ""
  ]
}