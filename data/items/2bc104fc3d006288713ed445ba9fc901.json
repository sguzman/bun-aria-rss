{
  "title": "Thoughts on Mutual Information: Formal Limitations",
  "link": "http://artem.sobolev.name/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html",
  "description": "<p>This posts continues the discussion started in the <a href=\"/posts/2019-08-10-thoughts-on-mutual-information-more-estimators.html\">Thoughts on Mutual Information: More Estimators</a>. This time we’ll focus on drawbacks and limitations of these bounds.</p>\n<!--more-->\n<p>Let’s start with a elephant in the room: a year ago an interesting preprint has been uploaded to arxiv: <a href=\"https://arxiv.org/abs/1811.04251\">Formal Limitations on the Measurement of Mutual Information</a> in which authors essentially argue that if you don’t know any densities (the hardest case, according to my hierarchy), then any <strong>distribution-free high-confidence lower bound</strong> on the MI would require <span class=\"math inline\">\\(\\exp(\\text{MI})\\)</span> number of samples and thus black-box MI lower bounds should be deemed impractical.</p>\n<h2 id=\"formal-limitations\">Formal Limitations</h2>\n<p>The paper mounts a massive attack on distribution-free lower bounds on the Mutual Information. Not only McAllester and Stratos show that existing bounds are inferior, but also kill any blackbox lower bounds on the KL divergence. The core result that warrants impossibility of cheap and good lower bounds on the Mutual Information is the Theorem 2, which states (in a slightly reformulated notation)</p>\n<blockquote>\n<p><strong>Theorem</strong>: Let <span class=\"math inline\">\\(B\\)</span> be any distribution-free high-confidence lower bound on <span class=\"math inline\">\\(\\mathbb{H}[p(x)]\\)</span> computed from a sample <span class=\"math inline\">\\(x_{1:N} \\sim p(x)\\)</span> More specifically, let <span class=\"math inline\">\\(B(x_{1:N}, \\delta)\\)</span> be any real-valued function of a sample and a confidence parameter <span class=\"math inline\">\\(\\delta\\)</span> such that for any <span class=\"math inline\">\\(p(x)\\)</span>, with probability at least <span class=\"math inline\">\\((1 − \\delta)\\)</span> over a draw of <span class=\"math inline\">\\(x_{1:N}\\)</span> from <span class=\"math inline\">\\(p(x)\\)</span>, we have <span class=\"math display\">\\[\\mathbb{H}[p(x)] \\ge B(x_{1:N}, δ).\\]</span> For any such bound, and for <span class=\"math inline\">\\(N \\ge 50\\)</span> and <span class=\"math inline\">\\(k \\ge 2\\)</span>, with probability at least <span class=\"math inline\">\\(1 − \\delta − 1.01/k\\)</span> over the draw of <span class=\"math inline\">\\(x_{1:N}\\)</span> we have <span class=\"math display\">\\[B(x_{1:N}, \\delta) ≤ \\log(2k N^2)\\]</span></p>\n</blockquote>\n<p>Indeed, since (in discrete case) <span class=\"math inline\">\\(I(X, X) = H(X)\\)</span> <a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a>, any good<a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a> lower bound on the MI would give a good lower bound on the entropy, and the theorem above says there are no such bounds (only those that are either exponentially expensive to compute or are not high-confidence or are not black-box). Authors then argue that one can have good estimators if they forgo the lower bound guarantee and settle with an estimate that is neither a lower or an upper bound. However, this is undesirable in many cases, especially when we’d like to compare two numbers.</p>\n<p>Unfortunately, I found the paper hard to digest and as far as I know, it’s still not published, so probably we should be cautious about the presented result. Nevertheless, I’ll show below that several often used bounds do indeed seem to have this limitation.</p>\n<h3 id=\"the-nguyen-wainwright-jordan-bound\">The Nguyen-Wainwright-Jordan Bound</h3>\n<p>The process we’ve followed so far to derive a lower bound on the MI has been somewhat cumbersome: we first decomposed the MI into some expectation and then used fancy bounds on some of the terms. An alternative and easier approach is to recall that the MI is a certain KL divergence take any off-the-shelf lower bounds on the KL divergence.</p>\n<p>One such lower bound can be obtained using the Fenchel conjugate functions (<a href=\"https://arxiv.org/abs/0809.0853\">Nguyen et al.</a>, alternatively see the <a href=\"https://arxiv.org/abs/1606.00709\">f-GANs</a> paper):</p>\n<p><span class=\"math display\">\\[\nKL(p(x) \\mid\\mid q(x)) \\ge\n\\mathbb{E}_{p(x)} f(x)\n-\n\\mathbb{E}_{q(x)} \\exp(f(x))\n+ 1\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(f(x)\\)</span> (a critic) is any function that takes <span class=\"math inline\">\\(x\\)</span> as input and outputs a scalar. The optimal choice can be shown to be <span class=\"math inline\">\\(f^*(x) = \\ln \\tfrac{p(x)}{q(x)}\\)</span>. And all is nice, except the lurking menace of the <span class=\"math inline\">\\(\\exp\\)</span> term. Consider a Monte Carlo estimate in the case of optimal critic (<span class=\"math inline\">\\(x_{1:N} \\sim p(x), y_{1:M} \\sim q(y)\\)</span>): <span class=\"math display\">\\[\n\\frac{1}{N} \\sum_{n=1}^N \\ln \\frac{p(x_n)}{q(x_n)}\n-\n\\frac{1}{M} \\sum_{m=1}^M \\frac{p(y_m)}{q(y_m)}\n+ 1\n\\]</span> The first term is exactly the Monte Carlo estimate of the KL divergence, while the second (the balancing term as it’s counterweights the first one) in expectation gives 1. However, ratio of densities might take on extremely large values and in general has enormous variance. Indeed, the variance of the balancing term is</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\mathbb{V}_{q(y_{1:M})} \\left[ \\frac{1}{M} \\sum_{m=1}^M \\frac{p(y_m)}{q(y_m)} \\right]\n&=\n\\frac{1}{M} \n\\mathbb{V}_{q(y)} \\left[ \\frac{p(y)}{q(y)} \\right]\n=\n\\frac{1}{M} \n\\E_{q(y)} \\left[ \\left(\\frac{p(y)}{q(y)} \\right)^2 - 1 \\right] \\\\\n&=\n\\frac{1}{M} \n\\E_{p(y)} \\left[ \\frac{p(y)}{q(y)} - 1 \\right]\n=\n\\frac{\\E_{p(y)} \\left[ \\exp \\log \\frac{p(y)}{q(y)} \\right] - 1}{M} \\\\\n& \\ge\n\\frac{\\exp \\E_{p(y)} \\left[ \\log \\frac{p(y)}{q(y)} \\right] - 1}{M}\n=\n\\frac{\\exp\\left(\\text{KL}(p(y) \\mid\\mid q(y))\\right) - 1}{M}\n\\end{align*}\n\\]</span></p>\n<p>So, one can see that indeed, the NWJ bound can’t give us high-confidence few-samples lower bound on any KL, not only the MI. This is because the second term would bias the bound by contributing large zero-mean noise. The only way to drive the magnitude of this noise down is to take more samples, and as the analysis above shows, number of samples should be exponential in the KL (The statement could be made more precise by appealing to the Chebyshev’s inequality).</p>\n<h3 id=\"the-donsker-varadhan-estimator\">The Donsker-Varadhan Estimator</h3>\n<p>Donsker and Varadhan have proposed essentially a tighter bound on the KL divergence, of the form <span class=\"math display\">\\[\nKL(p(x) \\mid\\mid q(x)) \\ge\n\\E_{p(x)} f(x)\n-\n\\log \\E_{q(x)} \\exp(f(x))\n+ 1\n\\]</span> With the same <span class=\"math inline\">\\(f^*(x) = \\ln \\tfrac{p(x)}{q(x)}\\)</span> being an optimal critic. There are two key differences to the previous bound: the first is that it uses a logarithm in front of the balancing term, preventing it from contributing huge variance (but this variance still has to go somewhere, and we’ll see where it goes), and the second (and the most important) is that this bound is no loger amendable to (unbiased) Monte Carlo estimation due to the logarithm outside of the expectation. In practice <a href=\"https://arxiv.org/abs/1801.04062\">people just take an empirical average</a> under the expectation thus obtaining a biased estimate (which in general is neither a lower nor an upper bound):</p>\n<p><span class=\"math display\">\\[\n\\frac{1}{N} \\sum_{n=1}^N \\ln \\frac{p(x_n)}{q(x_n)}\n-\n\\log\n\\frac{1}{M} \\sum_{m=1}^M \\frac{p(y_m)}{q(y_m)}\n\\]</span></p>\n<p>It can be shown that the balancing term now has huge bias and is always negative. It’s also easy to see that the biased converges to 0 as we take more samples <span class=\"math inline\">\\(M\\)</span>, so one might hope that with moderately many samples we’d have some tolerable bias. Well, this doesn’t seem to be the case.</p>\n<p>Take a closer look at the bias of the balancing term <span class=\"math display\">\\[\n\\E_{q(y_{1:M})}\n\\log\n\\frac{1}{M} \\sum_{m=1}^M \\frac{p(y_m)}{q(y_m)}\n\\]</span> It can be seen as an asymptotically unbiased estimate (a lower bound for all finite <span class=\"math inline\">\\(M\\)</span>) of the log-normalizing constant of <span class=\"math inline\">\\(p(y)\\)</span> (which is 1 since it’s already normalized) and is well-studied. In particular, <a href=\"https://arxiv.org/abs/1808.09034\">Domke and Sheldon</a> have shown (Theorem 3) that, essentially, the bias of the balancing term converges to 0 with the following rate:</p>\n<p><span class=\"math display\">\\[\nO\\left(M^{-1} \\mathbb{V}_{q(y)} \\left[ \\frac{p(y)}{q(y)} \\right] \\right)\n\\]</span></p>\n<p>Which 1) shows us where the variance has gone; 2) hints that in order to eliminate the bias we’d again need to take exponential number of samples.I don’t know what happens to the actual variance of the balancing term, but it can only make things worse.</p>\n<h3 id=\"the-contrastive-predictive-coding-bound\">The Contrastive-Predictive-Coding Bound</h3>\n<p>Let’s leave the realm of lower bounds on KL now. Previously I have already presented the InfoNCE bound: <span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n\\ge\n\\E_{p(z_{0:K})}\n\\E_{p(x | z_0)}\n\\log \\frac{p(x|z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K p(x|z_k)}\n\\]</span></p>\n<p>Importantly, this bound does not have access to any marginals of the <span class=\"math inline\">\\(p(x,z)\\)</span> joint. It’s easy to show that this lower bound is upper bounded by <span class=\"math inline\">\\(\\log (K+1)\\)</span>, which confirms the thesis:</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\E_{p(z_{0:K})}\n\\E_{p(x | z_0)}\n\\log \\frac{p(x|z_0)}{\\frac{1}{K+1} \\sum\\limits_{k=0}^K p(x|z_k)}\n& =\n\\log (K+1)\n+\n\\E_{p(z_{0:K})}\n\\E_{p(x | z_0)}\n\\log \\frac{p(x|z_0)}{\\sum\\limits_{k=0}^K p(x|z_k)} \\\\\n& \\le\n\\log (K+1)\n\\end{align*}\n\\]</span> Which is due to the log’s argument being between 0 and 1. So this <span class=\"math inline\">\\(\\log(K+1)\\)</span> upper bound on the lower bound means that if the true MI is much larger than this value, the bound will be very loose.</p>\n<p>Given all these negative results, one might ask themselves if knowing the marginal <span class=\"math inline\">\\(p(z)\\)</span> would do much better. Consider the “known prior” case:</p>\n<p><span class=\"math display\">\\[\n\\text{MI}[p(x, z)]\n\\ge\n\\E_{p(x, z_0)}\n\\E_{q_\\phi(z_{1:K}|x)}\n\\log \\frac{\\hat\\varrho_\\eta(x|z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\hat\\varrho_\\eta(x|z_k) \\frac{p(z_k)}{q_\\phi(z_k|x)}}\n\\]</span></p>\n<p>Then we have <span class=\"math display\">\\[\n\\begin{align*}\n\\E_{\\substack{p(x, z_0) \\\\ q_\\phi(z_{1:K}|x)}}\n&\n\\log \\frac{\\hat\\varrho_\\eta(x|z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\hat\\varrho_\\eta(x|z_k) \\frac{p(z_k)}{q_\\phi(z_k|x)}} \\\\\n& =\n\\log(K+1)\n+\n\\E_{\\substack{p(x, z_0) \\\\ q_\\phi(z_{1:K}|x)}}\n\\left[\n\\log \\frac{\\hat\\varrho_\\eta(x|z_0) \\frac{p(z_0)}{q_\\phi(z_0|x)}}{\\sum_{k=0}^K \\hat\\varrho_\\eta(x|z_k) \\frac{p(z_k)}{q_\\phi(z_k|x)}}\n-\n\\log \\frac{p(z_0)}{q_\\phi(z_0|x)}\n\\right]\n\\\\\n& \\le\n\\log(K+1)\n+\n\\E_{\\substack{p(x, z_0) \\\\ q_\\phi(z_{1:K}|x)}}\n\\log \\frac{q_\\phi(z_0|x) p(x|z_0)}{p(z_0) p(x|z_0)} \\\\\n& =\n\\log(K+1)\n+\n\\E_{p(x, z_0)}\n\\log \\frac{q_\\phi(z|x) p(x|z)}{p(z|x) p(x)} \\\\\n& =\n\\log(K+1)\n+\n\\E_{p(x, z)}\n\\log \\frac{p(x|z)}{p(x)}\n-\n\\E_{p(x, z)}\n\\log \\frac{p(z|x)}{q_\\phi(z|x)} \\\\\n& =\n\\log(K+1)\n+\n\\text{MI}[p(x, z)]\n-\n\\text{KL}(p(x,z) \\mid\\mid q_\\phi(z|x) p(x))\n\\end{align*}\n\\]</span></p>\n<p>Which shows that by choosing <span class=\"math inline\">\\(q_\\phi(z|x) = p(z)\\)</span> we essentially threw the baby out with the bathwater. Yes, <span class=\"math inline\">\\(K\\)</span> still needs to be exponential, but this time not in the original MI, but rather in <span class=\"math inline\">\\(\\text{KL}(p(x,z) \\mid\\mid q_\\phi(z|x) p(x))\\)</span>, which can be made much smaller with a good choice of the variational distribution <span class=\"math inline\">\\(q_\\phi(z|x)\\)</span>.</p>\n<p>Also recall that we can reparametrize the bound in terms of <span class=\"math inline\">\\(\\hat\\rho_\\eta(x, z) = \\hat\\varrho_\\eta(x|z) p(z)\\)</span></p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\n\\text{MI}[p(x, z)]\n& \\ge\n\\E_{p(x, z_0)}\n\\E_{q_\\phi(z_{1:K}|x)}\n\\log \\frac{\\hat\\rho_\\eta(x, z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{\\hat\\rho_\\eta(x, z_k) }{q_\\phi(z_k|x)}}\n-\n\\E_{p(z)}\n\\log p(z) \\\\\n\\text{MI}[p(x, z)]\n& =\n\\E_{p(x, z)} \\log p(z|x) - \\E_{p(z)} \\log p(z)\n\\end{align*}\n\\]</span></p>\n<p>Hence <span class=\"math display\">\\[\n\\E_{p(x, z_0)} \\log p(z_0|x)\n\\ge\n\\E_{p(x, z_0)}\n\\E_{q_\\phi(z_{1:K}|x)}\n\\log \\frac{\\hat\\rho_\\eta(x, z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{\\hat\\rho_\\eta(x, z_k) }{q_\\phi(z_k|x)}}\n\\]</span> Now we can choose the <span class=\"math inline\">\\(p(x)\\)</span> marginal freely. Let <span class=\"math inline\">\\(p(x) = \\delta(x - \\tilde{x})\\)</span>. Then <span class=\"math display\">\\[\n\\E_{p(z_0|\\tilde{x})}\n\\log p(z_0|\\tilde{x})\n\\ge\n\\E_{p(z_0|\\tilde{x})}\n\\E_{q_\\phi(z_{1:K}|\\hat{x})}\n\\log \\frac{\\hat\\rho_\\eta(\\tilde{x}, z_0)}{\\frac{1}{K+1} \\sum_{k=0}^K \\frac{\\hat\\rho_\\eta(\\tilde{x}, z_k) }{q_\\phi(z_k|\\tilde{x})}}\n\\]</span></p>\n<p>So in a sense (and this is indeed how we derived the bound in the first place), this “known prior” lower bound is based on a distribution-free <em>upper</em> bound on the entropy of <span class=\"math inline\">\\(z|x\\)</span> and avoids lower bounding any entropies.</p>\n<h2 id=\"but-why-does-it-work-in-practice\">But why does it work in practice?</h2>\n<p>Despite the negative results above, there’s a lot of empirical evidence of successful applications of all of the distribution-free bounds presented above. So what’s going on? Quite possibly, this was the question folks from the Google Brain have asked themselves in their recent preprint <a href=\"https://arxiv.org/abs/1907.13625\">On Mutual Information Maximization for Representation Learning</a>. In this paper researchers investigated representations obtained by the Mutual Information maximization principle. For example, one finding is that tighter MI estimates surprisingly led to worse performance. Overall, the apparent conclusion of the paper is that MI estimation perspective does not seem to explain the observed behavior. Authors then suggest the metric learning perspective and reinterpret lower bounds on the MI as metric learning objectives.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>All this evidence suggests that estimating the MI is an even harder problem than we used to think. In particular, blackbox MI estimation seems to be intractable in non-toy cases. Luckily, representation learning works nevertheless, probably due to a different phenomena.</p>\n<p>However, for many problems it’d be really nice to have a way to quantify the dependence between <span class=\"math inline\">\\(x\\)</span> and <span class=\"math inline\">\\(z\\)</span>. A possible approach here is to consider different divergences between the joint <span class=\"math inline\">\\(p(x, z)\\)</span> and the product of marginals <span class=\"math inline\">\\(p(x) p(z)\\)</span>. For example, one possible direction is to replace the KL divergence with some other <a href=\"https://en.wikipedia.org/wiki/F-divergence\"><span class=\"math inline\">\\(f\\)</span>-divergence</a> (see <a href=\"https://ieeexplore.ieee.org/document/4455754\">Lautum Information</a>, for example), or, <a href=\"https://en.wikipedia.org/wiki/Wasserstein_metric\">Wasserstein distance</a>. And there’s already some works in this direction: <a href=\"https://arxiv.org/abs/1903.11780\">Wasserstein Dependency Measure for Representation Learning</a> explores, unsurprisingly, the Wasserstein distance, or, <a href=\"https://arxiv.org/abs/1808.06670\">Learning deep representations by mutual information estimation and maximization</a> considers Jensen-Shannon divergence instead of the KL divergence. It’d curious to see some theorems / efficient bounds for these and other divergences.</p>\n<p>Finally, one additional contribution of the Formal Limitations paper is the impossibility or good lower bounds on the KL divergence (supported by the reasoning above). This raises the question: given the whole family of <span class=\"math inline\">\\(f\\)</span>-divergences and their Fenchel conjugate-based blackbox lower bounds, do all of them exhibit such computationally unfavorable behavior? If no, which ones do?</p>\n<p>Thanks to <a href=\"https://twitter.com/poolio\">Ben Poole</a>, <a href=\"https://twitter.com/eeevgen\">Evgenii Egorov</a> and Arseny Kuznetsov for valuable discussions.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>Or, in my notation, <span class=\"math inline\">\\(\\text{MI}[p(x) \\delta(z - x)] = \\mathbb{H}[p(x)]\\)</span>.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>By “good lower bound” I mean distribution-free high-confidence lower bound that uses some decent amount of samples, say, polynomial or even linear in the MI.<a href=\"#fnref2\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Wed, 14 Aug 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-08-14-thoughts-on-mutual-information-formal-limitations.html",
  "dc:creator": "Artem"
}