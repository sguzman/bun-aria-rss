{
  "title": "MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01452",
  "description": "<p>Enabling private inference is crucial for many cloud inference services that\nare based on Transformer models. However, existing private inference solutions\nfor Transformers can increase the inference latency by more than 60x or\nsignificantly compromise the quality of inference results. In this paper, we\ndesign the framework MPCFORMER using secure multi-party computation (MPC) and\nKnowledge Distillation (KD). It can be used in tandem with many specifically\ndesigned MPC-friendly approximations and trained Transformer models. MPCFORMER\nsignificantly speeds up Transformer model inference in MPC settings while\nachieving similar ML performance to the input model. We evaluate MPCFORMER with\nvarious settings in MPC. On the IMDb dataset, we achieve similar performance to\nBERTBASE, while being 5.3x faster. On the GLUE benchmark, we achieve 97%\nperformance of BERTBASE with a 2.2x speedup. We show that MPCFORMER remains\neffective with different trained Transformer weights such as ROBERTABASE and\nlarger models including BERTLarge. In particular, we achieve similar\nperformance to BERTLARGE, while being 5.93x faster on the IMDb dataset.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"
}