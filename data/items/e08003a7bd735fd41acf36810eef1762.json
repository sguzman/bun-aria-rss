{
  "title": "Meaning of conditional probability",
  "link": "https://mirror2image.wordpress.com/2013/11/07/meaning-of-conditional-probability/",
  "dc:creator": "mirror2image",
  "pubDate": "Thu, 07 Nov 2013 11:47:54 +0000",
  "category": [
    "Math",
    "machine learning",
    "Science"
  ],
  "guid": "http://mirror2image.wordpress.com/?p=1133",
  "description": "Conditional probability was always baffling me. Empirical, frequentists meaning is clear, but the abstract definition, originating from Kolmogorov &#8211; what was its mathematical meaning? How it can be derived? It&#8217;s a nontrivial definition and is appearing in the textbooks out the air, without measure theory intuition behind it. Here I mostly follow Chang&Pollard  paper  Conditioning as disintegartion. Beware that [&#8230;]",
  "content:encoded": "<p>Conditional probability was always baffling me. Empirical, frequentists meaning is clear, but the abstract definition, originating from Kolmogorov &#8211; what was its <em>mathematical </em>meaning? How it can be derived? It&#8217;s a nontrivial definition and is appearing in the textbooks out the air, without <a title=\"Measure\" href=\"http://en.wikipedia.org/wiki/Measure_(mathematics)\">measure theory</a> intuition behind it.</p>\n<p>Here I mostly follow Chang&Pollard  paper  <em><a href=\"http://www.stat.yale.edu/~jtc5/papers/ConditioningAsDisintegration.pdf\">Conditioning as disintegartion</a>. </em>Beware that the paper use non-standard notation, but this post follow more common notation, same as in wikipedia.</p>\n<p>Here is example form Chang&Pollard paper:</p>\n<p>Suppose we have distribution on  <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R}^2\" class=\"latex\" /> concentrated in two straight lines <img src=\"https://s0.wp.com/latex.php?latex=L_1&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_1\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=L_2&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_2\" class=\"latex\" /> with respective density <img src=\"https://s0.wp.com/latex.php?latex=g_i%28x%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"g_i(x)\" class=\"latex\" /> and angles with <em>X</em> axis <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha_i&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;alpha_i\" class=\"latex\" />. Observation (<em>X,Y</em>) taken, giving <img src=\"https://s0.wp.com/latex.php?latex=X%3Dx_0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X=x_0\" class=\"latex\" />, what is probability that point lies on the line <img src=\"https://s0.wp.com/latex.php?latex=L_1&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_1\" class=\"latex\" /> ?</p>\n<p>Standard approach would be approximate <img src=\"https://s0.wp.com/latex.php?latex=x_0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"x_0\" class=\"latex\" /> with <img src=\"https://s0.wp.com/latex.php?latex=%5B+x_0%2C+x_0+%2B%5CDelta+%5D+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"[ x_0, x_0 +&#92;Delta ] \" class=\"latex\" /> and take limit with  <img src=\"https://s0.wp.com/latex.php?latex=%5CDelta+%5Cto+0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;Delta &#92;to 0\" class=\"latex\" /></p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cint_%7Bx_0%7D%5E%7Bx_0%2B+%5CDelta%7Dg_1%2Fcos%28%5Calpha_1%29%7D%7B%5Cint_%7Bx_0%7D%5E%7Bx_0%2B+%5CDelta%7Dg_1%2Fcos%28%5Calpha_1%29+%2B+%5Cint_%7Bx_0%7D%5E%7Bx_0%2B+%5CDelta%7Dg_2%2Fcos%28%5Calpha_2%29%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;frac{&#92;int_{x_0}^{x_0+ &#92;Delta}g_1/cos(&#92;alpha_1)}{&#92;int_{x_0}^{x_0+ &#92;Delta}g_1/cos(&#92;alpha_1) + &#92;int_{x_0}^{x_0+ &#92;Delta}g_2/cos(&#92;alpha_2)}\" class=\"latex\" /></p>\n<p>Not only taking this limit is kind of cumbersome, it&#8217;s also not totally obvious that it&#8217;s the same conditional probability that defined in the abstract definition &#8211; we are replacing ratio with limit here.</p>\n<p>Now what is &#8220;correct way&#8221; to define conditional probabilities, especially for distributions?</p>\n<p><a title=\"Disintegration\" href=\"http://en.wikipedia.org/wiki/Disintegration_theorem\">Disintegration</a>!</p>\n<p>For simplicity we will first talk about single scalar random variable, defined on probability space. We will think of random variable <strong><em>X</em> </strong>as function on the sample space. Now condition <img src=\"https://s0.wp.com/latex.php?latex=X%3Dx_0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X=x_0\" class=\"latex\" /> define <a title=\"fiber\" href=\"http://en.wikipedia.org/wiki/Fiber_(mathematics)\">fiber</a> &#8211; inverse image of <img src=\"https://s0.wp.com/latex.php?latex=x_0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"x_0\" class=\"latex\" />.</p>\n<p>Disintegration theorem say that probability measure on the sample space can be decomposed into two measures &#8211; parametric family of measures induced by original probability on each fiber and &#8220;orthogonal&#8221; measure on <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R}\" class=\"latex\" />  &#8211; on the parameter space of the first measure. Here <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R}\" class=\"latex\" /> is the space of values of <strong><em>X </em></strong> and serve as parameter space for measures on fibers. Second measure induced by the inverse image of  the function (random variable) for each measurable set on <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R}\" class=\"latex\" />. This second measure is called <a href=\"http://en.wikipedia.org/wiki/Pushforward_measure\">Pushforward measure</a>. Pushforward measure is just for measurable set on <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{R}\" class=\"latex\" /> (in our case) taking its <strong><em>X  </em></strong>inverse image on sample space and measuring it with μ.</p>\n<p>Fiber is in fact sample space for conditional event, and measure on fiber is our conditional distribution.</p>\n<p>Full statement of the theorem require some term form measure theory. Following wikipedia</p>\n<p>Let P(X) is collection of <a title=\"borel measure\" href=\"http://en.wikipedia.org/wiki/Borel_measure\">Borel </a><a title=\"Probability measure\" href=\"http://en.wikipedia.org/wiki/Probability_measure\">probability measures </a>on X,  P(Y) is collection of  Borel probability measures on Y</p>\n<p>Let Y and X be two <a title=\"Radon space\" href=\"http://en.wikipedia.org/wiki/Radon_space\">Radon space</a>s. Let μ ∈ P(Y), let <img src=\"https://s0.wp.com/latex.php?latex=%5Cpi&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;pi\" class=\"latex\" />: Y → X be a Borel- <a href=\"http://en.wikipedia.org/wiki/Measurable_function\">measurable function</a>, and let ν ∈ P(X) be the <a href=\"http://en.wikipedia.org/wiki/Pushforward_measure\">pushforward measure</a>  <img src=\"https://s0.wp.com/latex.php?latex=%5Cmu+%5Ccirc+%5Cpi%5E%7B-1%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu &#92;circ &#92;pi^{-1}\" class=\"latex\" />.</p>\n<p>* Then there exists a ν-almost everywhere uniquely determined family of probability measures <img src=\"https://s0.wp.com/latex.php?latex=%5Cmu_x&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu_x\" class=\"latex\" />  ⊆  P(Y) such that<br />\n* the function <img src=\"https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cmu_%7Bx%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"x &#92;mapsto &#92;mu_{x}\" class=\"latex\" /> is Borel measurable, in the sense that <img src=\"https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cmu_%7Bx%7D+%28B%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"x &#92;mapsto &#92;mu_{x} (B)\" class=\"latex\" /> is a Borel-measurable function for each Borel-measurable set B ⊆ Y;<br />\n*  <img src=\"https://s0.wp.com/latex.php?latex=%5Cmu_x&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu_x\" class=\"latex\" />  &#8220;lives on&#8221; the  fiber <img src=\"https://s0.wp.com/latex.php?latex=%5Cpi%5E%7B-1%7D%28x%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;pi^{-1}(x)\" class=\"latex\" /> : for ν-almost all x  ∈ X,</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cmu_x%28Y+%5Csetminus+%5Cpi%5E%7B-1%7D+%28x%29%29%3D0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu_x(Y &#92;setminus &#92;pi^{-1} (x))=0\" class=\"latex\" /></p>\n<p>and so <img src=\"https://s0.wp.com/latex.php?latex=%5Cmu_x%28E%29%3D%5Cmu_x%28E+%5Ccap+%5Cpi%5E%7B-1%7D%28x%29%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu_x(E)=&#92;mu_x(E &#92;cap &#92;pi^{-1}(x))\" class=\"latex\" /></p>\n<p>* for every Borel-measurable function <img src=\"https://s0.wp.com/latex.php?latex=f&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"f\" class=\"latex\" /> : Y → [0, ∞],</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cint_%7BY%7D+f%28y%29+%5C%2C+%5Cmathrm%7Bd%7D+%5Cmu+%28y%29+%3D+%5Cint_%7BX%7D+%5Cint_%7B%5Cpi%5E%7B-1%7D+%28x%29%7D+f%28y%29+%5C%2C+%5Cmathrm%7Bd%7D+%5Cmu_%7Bx%7D+%28y%29+%5Cmathrm%7Bd%7D+%5Cnu+%28x%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;int_{Y} f(y) &#92;, &#92;mathrm{d} &#92;mu (y) = &#92;int_{X} &#92;int_{&#92;pi^{-1} (x)} f(y) &#92;, &#92;mathrm{d} &#92;mu_{x} (y) &#92;mathrm{d} &#92;nu (x)\" class=\"latex\" /></p>\n<p>From here for any event <em>E</em> form Y</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cmu+%28E%29+%3D+%5Cint_%7BX%7D+%5Cmu_%7Bx%7D+%5Cleft%28+E+%5Cright%29+%5C%2C+%5Cmathrm%7Bd%7D+%5Cnu+%28x%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu (E) = &#92;int_{X} &#92;mu_{x} &#92;left( E &#92;right) &#92;, &#92;mathrm{d} &#92;nu (x)\" class=\"latex\" /></p>\n<p>This was complete statement of the disintegration theorem.</p>\n<p>Now returning to Chang&Pollard example. For formal derivation I refer you to the original paper, here we will just &#8220;guess&#8221; <img src=\"https://s0.wp.com/latex.php?latex=%5Cmu_x&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mu_x\" class=\"latex\" />, and by uniqueness it give us disintegration. Our conditional distribution for <img src=\"https://s0.wp.com/latex.php?latex=X%3Dx&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X=x\" class=\"latex\" /> will be just point masses on the intersections of lines <img src=\"https://s0.wp.com/latex.php?latex=L_1&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_1\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=L_2&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_2\" class=\"latex\" /> with axis  <img src=\"https://s0.wp.com/latex.php?latex=X%3Dx&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X=x\" class=\"latex\" /><br />\nHere <img src=\"https://s0.wp.com/latex.php?latex=%5Cdelta&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;delta\" class=\"latex\" /> is <a href=\"http://en.wikipedia.org/wiki/Delta_function\">delta function</a> &#8211; point mass.</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=d%5Cmu_x%28y%29+%3D+%5Cfrac%7B%5Cdelta+%28L_1%28+x+%29-y%29+g_1%28+x+%29+%2F+cos%28%5Calpha_1%29+%2B+%5Cdelta+%28L_2%28+x+%29-y%29+g_2%28+x+%29+%2F+cos%28%5Calpha_2%29%7D%7BZ%28x%29%7Ddy+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d&#92;mu_x(y) = &#92;frac{&#92;delta (L_1( x )-y) g_1( x ) / cos(&#92;alpha_1) + &#92;delta (L_2( x )-y) g_2( x ) / cos(&#92;alpha_2)}{Z(x)}dy \" class=\"latex\" /></p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=Z%28x%29+%3D+g_1%28x%29%2Fcos%28%5Calpha_1%29+%2B+g_2%28x%29%2Fcos%28%5Calpha_2%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"Z(x) = g_1(x)/cos(&#92;alpha_1) + g_2(x)/cos(&#92;alpha_2)\" class=\"latex\" /></p>\n<p>and for <img src=\"https://s0.wp.com/latex.php?latex=%5Cnu&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;nu\" class=\"latex\" /><br />\n<img src=\"https://s0.wp.com/latex.php?latex=d%5Cnu+%3D+Z%28x%29dx+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d&#92;nu = Z(x)dx \" class=\"latex\" /></p>\n<p>Our conditional probability that event lies on <img src=\"https://s0.wp.com/latex.php?latex=L_1&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"L_1\" class=\"latex\" /> with condition <img src=\"https://s0.wp.com/latex.php?latex=X+%3D+x_0&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"X = x_0\" class=\"latex\" />, form conditional density <img src=\"https://s0.wp.com/latex.php?latex=d%5Cmu_%7Bx_0%7D%2Fdy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d&#92;mu_{x_0}/dy\" class=\"latex\" /> thus</p>\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7Bg_1%28x_0%29%2Fcos%28%5Calpha_1%29%7D%7Bg_1%28x_0%29%2Fcos%28%5Calpha_1%29+%2B+g_2%28x_0%29%2Fcos%28%5Calpha_2%29%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;frac{g_1(x_0)/cos(&#92;alpha_1)}{g_1(x_0)/cos(&#92;alpha_1) + g_2(x_0)/cos(&#92;alpha_2)}\" class=\"latex\" /></p>\n<p>Another example from Chen&Pollard. It relate to <a href=\"http://en.wikipedia.org/wiki/Sufficient_statistic#Mathematical_definition\">sufficient statistics</a>. Term sufficient statistic used if we have probability distribution depending on some parameter, like in maximum likelihood estimation. Sufficient statistic is some function of sample, if it&#8217;s possible to estimate parameter of distribution from only values of that function in the best possible way &#8211; adding more data form the sample will not give more information about parameter of distribution.<br />\nLet <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D_%7B%5Ctheta%7D&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{P}_{&#92;theta}\" class=\"latex\" /> be uniform distribution on the square <img src=\"https://s0.wp.com/latex.php?latex=%5B0%2C+%5Ctheta%5D%5E2&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"[0, &#92;theta]^2\" class=\"latex\" />. In that case M = max(x, y) is sufficient statistics for <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />. How to show it?<br />\nLet take our function <img src=\"https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cto+m&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"(x,y) &#92;to m\" class=\"latex\" /> , <img src=\"https://s0.wp.com/latex.php?latex=m+%3D+max%28x%2Cy%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"m = max(x,y)\" class=\"latex\" /> and make disintegration.<br />\n<img src=\"https://s0.wp.com/latex.php?latex=d%5Cmu_m%28x%2Cy%29+%3D+s_m%28x%2Cy%29dxdy&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d&#92;mu_m(x,y) = s_m(x,y)dxdy\" class=\"latex\" /> is a uniform distribution on edges where <em>x</em> and <em>y</em> equal <em>m</em> and <img src=\"https://s0.wp.com/latex.php?latex=d%5Cnu+%3D+%282m+%2F+%5Ctheta%5E2+%29dm&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"d&#92;nu = (2m / &#92;theta^2 )dm\" class=\"latex\" /><br />\n<img src=\"https://s0.wp.com/latex.php?latex=s_m%28x%2Cy%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"s_m(x,y)\" class=\"latex\" /> is density of conditional probability <img src=\"https://s0.wp.com/latex.php?latex=P%28%5Ccdot+%7C+M+%3D+m%29+&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"P(&#92;cdot | M = m) \" class=\"latex\" /> and it doesn&#8217;t depend on <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" /><br />\nFor any <img src=\"https://s0.wp.com/latex.php?latex=f+%5C+%5C+%5Cmathbb%7BP%7D_%7B%5Ctheta%7D%28f%7C+m%29+%3D+%5Cint+f+s_m%28x%2Cy%29+%5C+%2C+%5C+%5Cmathbb%7BP%7D_%7B%5Ctheta%2C+m%7D%28%5Ccdot%29+%3D+%5Cmathbb%7BP%7D_%7Bm%7D%28%5Ccdot%29&#038;bg=e6e6e6&#038;fg=333333&#038;s=0&#038;c=20201002\" alt=\"f &#92; &#92; &#92;mathbb{P}_{&#92;theta}(f| m) = &#92;int f s_m(x,y) &#92; , &#92; &#92;mathbb{P}_{&#92;theta, m}(&#92;cdot) = &#92;mathbb{P}_{m}(&#92;cdot)\" class=\"latex\" /> &#8211; means that <em> M </em>is<em> </em> sufficient.</p>\n<p>It seems that in most cases disintegration is not a tool for finding conditional distribution. Instead it can help to guess it and form uniqueness prove that the guess is correct. That correctness could be nontrivial &#8211; there are some paradoxes similar to <a href=\"http://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox\">Borel Paradox</a> in Chang&Pollard paper.</p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}