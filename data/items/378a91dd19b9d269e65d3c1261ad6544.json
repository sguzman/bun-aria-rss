{
  "title": "Neural Variational Inference: Classical Theory",
  "link": "http://artem.sobolev.name/posts/2016-07-01-neural-variational-inference-classical-theory.html",
  "description": "<p>As a member of <a href=\"http://bayesgroup.ru/\">Bayesian methods research group</a> I’m heavily interested in Bayesian approach to machine learning. One of the strengths of this approach is ability to work with hidden (unobserved) variables which are interpretable. This power however comes at a cost of generally intractable exact inference, which limits the scope of solvable problems.</p>\n<p>Another topic which gained lots of momentum in Machine Learning recently is Deep Learning, of course. With Deep Learning we can now build big and complex models that outperform most hand-engineered approaches given lots of data and computational power. The fact that Deep Learning needs a considerable amount of data also requires these methods to be scalable — a really nice property for any algorithm to have, especially in a Big Data epoch.</p>\n<p>Given how appealing both topics are it’s not a surprise there’s been some work to marry these two recently. In this <a href=\"/tags/modern%20variational%20inference%20series.html\">series</a> of blogsposts I’d like to summarize recent advances, particularly in variational inference. This is not meant to be an introductory discussion as prior familiarity with classical topics (Latent variable models, <a href=\"https://en.wikipedia.org/wiki/Variational_Bayesian_methods\">Variational Inference, Mean-field approximation</a>) is required, though I’ll introduce these ideas anyway just to remind it and setup the notation.</p>\n<!--more-->\n<h3>\nLatent Variables Models\n</h3>\n<p>Suppose you have a probabilistic model that’s easy to describe using some auxiliary variables <span class=\"math inline\">\\(Z\\)</span> that you don’t observe directly (or even would like to infer it given the data). One classical example for this setup is Gaussian Mixture Modeling: we have <span class=\"math inline\">\\(K\\)</span> components in a mixture, and <span class=\"math inline\">\\(z_n\\)</span> is a <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a> vector of dimensionality <span class=\"math inline\">\\(K\\)</span> indicating which component an observation <span class=\"math inline\">\\(x_n\\)</span> belongs to. Then, conditioned on <span class=\"math inline\">\\(z_n\\)</span> the distribution of <span class=\"math inline\">\\(x_n\\)</span> is a usual Gaussian distribution: <span class=\"math inline\">\\(p(x_{n} \\mid z_{nk} = 1) = \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)\\)</span> (here whenever I refer to a distribution, you should read it as its density. At least <a href=\"https://en.wikipedia.org/wiki/Generalized_function\">generalized one</a>). Therefore the joint distribution of the model is</p>\n<p><span class=\"math display\">\\[\np(x, z \\mid \\Theta) = \\prod_{n=1}^N \\prod_{k=1}^K \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)^{z_{nk}} \\pi_k^{z_{nk}}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\pi\\)</span> is a probability distribution over <span class=\"math inline\">\\(K\\)</span> outcomes, and <span class=\"math inline\">\\(\\Theta\\)</span> is a set of all model’s parameters (<span class=\"math inline\">\\(\\pi\\)</span>, <span class=\"math inline\">\\(\\mu\\)</span>s and <span class=\"math inline\">\\(\\Sigma\\)</span>s).</p>\n<p>We’d like to do 2 things with the model: first, we obviously need to learn parameters <span class=\"math inline\">\\(\\Theta\\)</span>, and second, we’d like infer these latent variables <span class=\"math inline\">\\(z_n\\)</span> to know which cluster the observation <span class=\"math inline\">\\(x_n\\)</span> belongs to, that is, we need to calculate the distribution of <span class=\"math inline\">\\(z_n\\)</span> conditioned on <span class=\"math inline\">\\(x_n\\)</span>: <span class=\"math inline\">\\(p(z_n \\mid x_n)\\)</span>.</p>\n<p>We want to learn the parameters <span class=\"math inline\">\\(\\Theta\\)</span> as usual by maximizing the log-likelihood. Unfortunately, we don’t know true assignments <span class=\"math inline\">\\(z_n\\)</span>, and marginalizing over it as in <span class=\"math inline\">\\(p(x_n) = \\sum_{k=1}^K \\pi_k p(x_n, z_{nk} = 1)\\)</span> is not a good idea as the resulting optimization problem would lose its convexity. Instead we decompose the log-likelihood as follows:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\log p(x)\n&= \\mathbb{E}_{q(z\\mid x)} \\overbrace{\\log p(x)}^{\\text{const in $z$}}\n= \\mathbb{E}_{q(z\\mid x)} \\log \\frac{p(x, z) q(z\\mid x)}{p(z \\mid x) q(z\\mid x)}  \\\\\n&= \\mathbb{E}_{q(z\\mid x)} \\log \\frac{p(x, z)}{q(z\\mid x)} + D_{KL}(q(z\\mid x) \\mid\\mid p(z \\mid x))\n\\end{align}\n\\]</span></p>\n<p>The second term is a Kullback-Leibler divergence, which is always non-negative, and equals zero iff distributions are equal almost everywhere <span class=\"math inline\">\\(q(z\\mid x) = p(z \\mid x)\\)</span>. Therefore putting <span class=\"math inline\">\\(q(z \\mid x) = p(z \\mid x)\\)</span> eliminates the second term, leaving us with <span class=\"math inline\">\\(\\log p(x) = \\mathbb{E}_{p(z \\mid x)} \\log \\frac{p(x, z)}{p(z \\mid x)}\\)</span>. Therefore all we need to be able to do is to calculate the posterior <span class=\"math inline\">\\(p(z \\mid x)\\)</span>, and maximize the expectation. This is how EM algorithm is derived: at E-step we calculate the posterior <span class=\"math inline\">\\(p(z \\mid x, \\Theta^{\\text{old}})\\)</span>, and at M-step we maximize the expectation <span class=\"math inline\">\\(\\mathbb{E}_{p(z \\mid x, \\Theta^{\\text{old}})} \\log \\frac{p(x, z \\mid \\Theta)}{p(z \\mid x, \\Theta)}\\)</span> with respect to <span class=\"math inline\">\\(\\Theta\\)</span> keeping <span class=\"math inline\">\\(\\Theta^{\\text{old}}\\)</span> fixed.</p>\n<p>Now, all we are left to do is to find the posterior <span class=\"math inline\">\\(p(z \\mid x)\\)</span> which is given by the following deceivingly simple formula knows as a Bayes’ rule.</p>\n<p><span class=\"math display\">\\[\np(z \\mid x) = \\frac{p(x \\mid z) p(z)}{\\int p(x \\mid z) p(z)dz}\n\\]</span></p>\n<p>Of course, there’s no free lunch and computing the denominator is intractable in general case. One <strong>can</strong> compute the posterior exactly when the prior <span class=\"math inline\">\\(p(z)\\)</span> and the likelihood <span class=\"math inline\">\\(p(x \\mid z)\\)</span> are <a href=\"https://en.wikipedia.org/wiki/Conjugate_prior\">conjugate</a> (that is, after multiplying the prior by the likelihood you get the same functional form for <span class=\"math inline\">\\(z\\)</span> as in the prior, thus the posterior comes from the same family as the prior but with different parameters), but many models of practical interest don’t have this property. This is where variational inference comes in.</p>\n<h3>\nVariational Inference and Mean-field\n</h3>\n<p>In a variational inference (VI) framework we approximate the true posterior <span class=\"math inline\">\\(p(z \\mid x)\\)</span> with some other simpler distribution <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span> where <span class=\"math inline\">\\(\\Lambda\\)</span> is a set of (variational) parameters of the (variational) approximation (I may omit <span class=\"math inline\">\\(\\Lambda\\)</span> and <span class=\"math inline\">\\(\\Theta\\)</span> in a “given” clause when it’s convenient, remember, it always could be placed there). One possibility is to divide latent variables <span class=\"math inline\">\\(z\\)</span> in groups and force the groups to be independent. In extreme case each variable gets its own group, assuming independence among all variables <span class=\"math inline\">\\(q(z \\mid x) = \\prod_{d=1}^D q(z_d \\mid x)\\)</span>. If we then set about to find the best approximation to the true posterior in this fully factorized class, we will no longer have the optimal <span class=\"math inline\">\\(q\\)</span> being the true posterior itself, as the true posterior is presumably too complicated to be dealt with in analytic form (which we want from the approximation <span class=\"math inline\">\\(q\\)</span> when we say “simpler distribution”). Therefore we find the optimal <span class=\"math inline\">\\(q(z_i)\\)</span> by minimizing the KL-divergence with the true posterior (<span class=\"math inline\">\\(\\text{const}\\)</span> denotes terms that are constant w.r.t. <span class=\"math inline\">\\(q(z_i)\\)</span>):</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\nD_{KL}(q(z \\mid x) \\mid\\mid p(z \\mid x))\n&= \\mathbb{E}_{q(z_i \\mid x)} \\left[ \\mathbb{E}_{q(z_{-i} \\mid x)} \\log \\frac{q(z_1 \\mid x) \\dots q(z_D \\mid x)}{p(z \\mid x)} \\right] \\\\\n&= \\mathbb{E}_{q(z_i \\mid x)} \\left[ \\log q(z_i \\mid x) - \\underbrace{\\mathbb{E}_{q(z_{-i} \\mid x)} \\log p(z \\mid x)}_{\\log f(z_i \\mid x)} \\right] + \\text{const} \\\\\n&= \\mathbb{E}_{q(z_i \\mid x)} \\left[ \\log \\frac{q(z_i \\mid x)}{\\tfrac{1}{Z} f(z_i \\mid x)} \\right] - \\log Z + \\text{const} \\\\\n&= D_{KL}\\left(q(z_i \\mid x) \\mid\\mid \\tfrac{1}{Z} f(z_i \\mid x)\\right) + \\text{const}\n\\end{align}\n\\]</span></p>\n<p>For many models it’s possible to look into <span class=\"math inline\">\\(\\mathbb{E}_{q(z_{-i} \\mid x)} \\log p(z \\mid x)\\)</span> and immediately recognize logarithm of unnormalized density of some known distribution.</p>\n<p>Another cornerstone of this framework is a notion of <strong>Evidence Lower Bound</strong> (ELBO): recall the decomposition of log-likelihood we derived above. In our current setting we can not compute the right hand side as we can not evaluate the true posterior <span class=\"math inline\">\\(p(z \\mid x)\\)</span>. However, note that the left hand side (that is, the log-likelihood) does not depend on the variational distribution <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span>. Therefore, maximizing the first term of the right hand side w.r.t. variational parameters <span class=\"math inline\">\\(\\Lambda\\)</span> results in minimizing the second term, the KL-divergence with the true posterior. This implies we can ditch the second term, and maximize the first one w.r.t. both model parameters <span class=\"math inline\">\\(\\Theta\\)</span> and variational parameters <span class=\"math inline\">\\(\\Lambda\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\text{ELBO:} \\quad \\mathcal{L}(\\Theta, \\Lambda) := \\mathbb{E}_{q(z \\mid x, \\Lambda)} \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)}\n\\]</span></p>\n<p>Okay, so this covers the basics, but before we go to the neural networks-based methods we need to discuss some general approaches to VI and how to make it scalable. This is what the next blog post is all about.</p>",
  "pubDate": "Fri, 01 Jul 2016 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2016-07-01-neural-variational-inference-classical-theory.html",
  "dc:creator": "Artem"
}