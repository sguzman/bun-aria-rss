{
  "title": "Modeling the secular trend in a cluster randomized trial using very flexible models",
  "link": "https://www.r-bloggers.com/2022/10/modeling-the-secular-trend-in-a-cluster-randomized-trial-using-very-flexible-models/",
  "dc:creator": "Keith Goldfeld",
  "pubDate": "Tue, 01 Nov 2022 00:00:00 +0000",
  "category": "R bloggers",
  "guid": "https://www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/",
  "description": "<div style = \"width:60%; display: inline-block; float:left; \">\n<p>A key challenge - maybe the key challenge - of a stepped wedge clinical trial design is the threat of confounding by time. This is a cross-over design where the unit of randomization is a group or cluster, where each cluster begins in the control st...</p></div>\n<div style = \"width: 40%; display: inline-block; float:right;\"></div>\n<div style=\"clear: both;\"></div>\n<strong>Continue reading</strong>: <a href=\"https://www.r-bloggers.com/2022/10/modeling-the-secular-trend-in-a-cluster-randomized-trial-using-very-flexible-models/\">Modeling the secular trend in a cluster randomized trial using very flexible models</a>",
  "content:encoded": "<!-- \n<div style=\"min-height: 30px;\">\n[social4i size=\"small\" align=\"align-left\"]\n</div>\n-->\n\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 12px;\">\n[This article was first published on  <strong><a href=\"https://www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/\"> ouR data generation</a></strong>, and kindly contributed to <a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers</a>].  (You can report issue about the content on this page <a href=\"https://www.r-bloggers.com/contact-us/\">here</a>)\n<hr>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div>\n\n\n\n<p>A key challenge &#8211; maybe <em>the</em> key challenge &#8211; of a <a href=\"https://www.rdatagen.net/post/alternatives-to-stepped-wedge-designs/\" rel=\"nofollow\" target=\"_blank\">stepped wedge clinical trial design</a> is the threat of confounding by time. This is a cross-over design where the unit of randomization is a group or cluster, where each cluster begins in the control state and transitions to the intervention. It is the transition point that is randomized. Since outcomes could be changing over time regardless of the intervention, it is important to model the time trends when conducting the efficacy analysis. The question is <em>how</em> we choose to model time, and I am going to suggest that we might want to use a very flexible model, such as a cubic spline or a generalized additive model (GAM).</p>\n<p>I am not going to talk more about stepped wedge designs here (if you want more background <a href=\"https://academic.oup.com/ije/article/49/3/1043/5835358\" rel=\"nofollow\" target=\"_blank\">this paper</a> would be a fine place to start), but will briefly describe a flexible way to model time trends. And I am going to simplify a bit to assume that we are talking about a cluster randomized trial (CRT), where clusters are randomized to treatment or control only. Confounding by time is not really an issue here, since treatment and control are implemented in parallel across different clusters, but we still might want to model time to get more efficient estimates of the treatment effect. I will consider the flexible modeling approaches for stepped wedge designs in a future post.</p>\n<div id=\"simulating-the-data-the-data-generation-process\" class=\"section level3\">\n<h3>Simulating the data (the data generation process)</h3>\n<p>As I typically do, I will frame this discussion around a simulated data set, which I will describe in some detail. Before we start, here are the libraries I use to generate and present the data:</p>\n<pre>library(simstudy)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(data.table)\nlibrary(mgcv)\nlibrary(lme4)\nlibrary(splines)</pre>\n<p>The simulated data will include 48 clusters over 20 time periods. 24 will be randomized to the control arm, 24 to the intervention. For each cluster and period, there are 30 individuals. The figure shows the cluster averages at each time point <span class=\"math inline\">\\(k\\)</span> for one randomly generated data set:</p>\n<p><img src=\"https://i1.wp.com/www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/index.en_files/figure-html/figure1-1.png?w=450&#038;ssl=1\" data-recalc-dims=\"1\" /></p>\n<p>The data generation process that underlies this plot is:</p>\n<p><span class=\"math display\">\\[\nY_{ijk} \\sim N(\\mu =100 + b^0_{j} + b^1_{jk} &#8211; 0.1k^2 + 5A_j, \\sigma^2 = 9)\n\\]</span></p>\n<p><span class=\"math inline\">\\(Y_{ijk}\\)</span> is the outcome measurement for individual <span class=\"math inline\">\\(i\\)</span> in cluster <span class=\"math inline\">\\(j\\)</span> at period <span class=\"math inline\">\\(k\\)</span>. In this case, <span class=\"math inline\">\\(k \\in \\{0, \\dots, 19\\}\\)</span>. There is an increasing decline in <span class=\"math inline\">\\(Y\\)</span> over time (based on the quadratic term <span class=\"math inline\">\\(k^2\\)</span>). <span class=\"math inline\">\\(A_j\\)</span> is a treatment indicator for cluster <span class=\"math inline\">\\(j\\)</span>, and <span class=\"math inline\">\\(A \\in \\{0 ,1\\}\\)</span>, and the treatment effect is <span class=\"math inline\">\\(5\\)</span>.</p>\n<p><span class=\"math inline\">\\(b_{0j}\\)</span> is a cluster-level random intercept for cluster <span class=\"math inline\">\\(j\\)</span>, <span class=\"math inline\">\\(b^0_{j} \\sim N(\\mu = 0, \\sigma^2 = 6)\\)</span>. <span class=\"math inline\">\\(b^1_{jk}\\)</span> is a cluster-specific time period effect for each time period <span class=\"math inline\">\\(k\\)</span>; the vector of cluster-time effects <span class=\"math inline\">\\(\\mathbf{b^1_j} \\sim N(0, \\Sigma)\\)</span>, where <span class=\"math inline\">\\(\\Sigma = DRD\\)</span> is a <span class=\"math inline\">\\(20 \\times 20\\)</span> covariance matrix based on a diagonal matrix <span class=\"math inline\">\\(D\\)</span> and an auto-regressive correlation structure <span class=\"math inline\">\\(R\\)</span>:</p>\n<p><span class=\"math display\">\\[ D = 16 * I_{20 \\times 20}\\]</span>\nand</p>\n<p><span class=\"math display\">\\[ R =\\begin{bmatrix}\n1 &#038; \\rho &#038; \\rho^2 &#038; \\dots &#038; \\rho^{19} \\\\\n\\rho &#038; 1 &#038; \\rho &#038; \\dots &#038; \\rho^{18} \\\\\n\\rho^2 &#038; \\rho &#038; 1 &#038; \\dots &#038; \\rho^{17} \\\\\n\\vdots &#038; \\vdots &#038; \\vdots &#038; \\vdots &#038; \\vdots \\\\\n\\rho^{19} &#038; \\rho^{18} &#038; \\rho^{17} &#038; \\dots &#038; 1 \\\\\n\\end{bmatrix}, \\ \\ \\rho = 0.7 \\]</span></p>\n<p>The <code>simstudy</code> definitions establish <span class=\"math inline\">\\(b^0\\)</span>, <span class=\"math inline\">\\(A\\)</span>, <span class=\"math inline\">\\(D\\)</span>, and <span class=\"math inline\">\\(Y\\)</span>. The vector <span class=\"math inline\">\\(\\mathbf{b^1}\\)</span> is created separately in the actual data generation process using <code>addCorGen</code>, using <span class=\"math inline\">\\(\\mu = 0\\)</span> and <span class=\"math inline\">\\(\\sigma^2 = 16\\)</span>. Here are the initial definitions:</p>\n<pre>def <- defData(varname = \"b0\", formula = 0, variance = 6)\ndef <- defData(def, varname = \"A\", formula = \"1;1\", dist = \"trtAssign\")\ndef <- defData(def, varname = \"mu\", formula = 0, dist = \"nonrandom\")\ndef <- defData(def, varname = \"s2\", formula = 16, dist = \"nonrandom\")\n\ndefOut <- defDataAdd(varname = \"y\", \n  formula = \"100 + b0 + b1k - 0.1 * k^2 + 5*A\", \n  variance = 9)</pre>\n<p>I’ve wrapped the data generation process inside a function so that I can use it in a replication study at the end of the post. The function adds a normalized version of time and ensures that the site variable is a factor, both adjustments needed for modeling.</p>\n<pre>s_generate <- function() {\n  \n  d <- genData(48, def, id = \"site\")\n  d <- addPeriods(d, 20, \"site\", perName = \"k\")\n  \n  d <- addCorGen(dtOld = d, idvar = \"site\", nvars = 20, \n                 rho = .7, corstr = \"ar1\",\n                 dist = \"normal\", param1 = \"mu\", param2 = \"s2\", cnames = \"b1k\")\n  \n  d <- genCluster(d, \"timeID\", numIndsVar = 30, level1ID = \"id\")\n  d <- addColumns(defOut, d)\n  \n  d[, normk := (k - min(k))/(max(k) - min(k))]\n  d[, site := as.factor(site)]\n  \n  d[]\n}\n\nset.seed(123)\ndd <- s_generate()</pre>\n</div>\n<div id=\"some-modeling-options\" class=\"section level3\">\n<h3>Some modeling options</h3>\n<p>If we are interested in accounting for the secular (or time) trend when estimating the treatment effect, we have a number of different options. We can assume there is no structure to the pattern of time, we can impose an extreme form of structure, or we can try to find a flexible middle ground.</p>\n<div id=\"time-without-structure\" class=\"section level4\">\n<h4>Time without structure</h4>\n<p>In stepped wedge designs - it is quite common to assume little if no structure in time trends. In the context of a CRT this could be set up by including a time-specific effect for each period <span class=\"math inline\">\\(k\\)</span>, as in this model for an outcome <span class=\"math inline\">\\(Y_{ijk}\\)</span> for individual <span class=\"math inline\">\\(i\\)</span> in group <span class=\"math inline\">\\(j\\)</span>:</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0 + \\gamma_k + \\delta A_j + b_j +e_{ijk}\n\\]</span></p>\n<p>where <span class=\"math inline\">\\(A_j\\)</span> is an indicator for treatment <span class=\"math inline\">\\(j\\)</span>, and is set to 1 if cluster <span class=\"math inline\">\\(j\\)</span> has been randomized to the intervention. <span class=\"math inline\">\\(\\beta_0\\)</span> and <span class=\"math inline\">\\(b_j\\)</span> are the intercept and random intercept, respectively. <span class=\"math inline\">\\(\\delta\\)</span> is the effect size parameter. <span class=\"math inline\">\\(\\gamma_k\\)</span> is the time-specific effect for period <span class=\"math inline\">\\(k\\)</span>. This is a totally reasonable approach to take, but if <span class=\"math inline\">\\(k\\)</span> starts to get quite large, we would need to need estimate large number of parameters (<span class=\"math inline\">\\(K\\)</span> period parameters, to be more precise), which is not always desirable, so we won’t take this approach here.</p>\n</div>\n<div id=\"time-with-over-simplified-structure\" class=\"section level4\">\n<h4>Time with over-simplified structure</h4>\n<p>An alternative approach is to model time in a linear fashion as</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0 + \\gamma k + \\delta A_j  + b_j + e_{ijk}\n\\]</span></p>\n<p>where we have a single parameter <span class=\"math inline\">\\(\\gamma\\)</span> instead of <span class=\"math inline\">\\(K\\)</span> period parameters. Here is an estimate of the treatment effect <span class=\"math inline\">\\(\\delta\\)</span> using a mixed effects model assuming a common linear time trend:</p>\n<pre>linear <- lmer(y ~ A + k + ( 1  | site) , data = dd)\nsummary(linear)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n##   Estimate Std. Error \n##       6.23       0.85</pre>\n<p>The linear model gets around the problem of a large number parameters, but it imposes a very strong assumption that the outcome <span class=\"math inline\">\\(Y\\)</span> changes linearly over time (and in this case at the same rate for each cluster). This is unlikely to be the case. We could fit a quadratic model like</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0  + \\gamma_0 k + \\gamma_1 k^2 + \\delta A_j + b_j  + e_{ijk}\n\\]</span></p>\n<p>but the assumption is still quite strong. We could also fit a mixed effects model with a random slope <span class=\"math inline\">\\(b_{1j}\\)</span> as well:</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0  + \\gamma k + \\delta A_j + b_{0j} + b_{1j} k  + e_{ijk}\n\\]</span></p>\n<p>But, if the temporal trend is not linear, there is no reason to think this would be the best approach.</p>\n</div>\n<div id=\"mixed-effects-model-with-fixed-cubic-spline-and-random-intercept\" class=\"section level4\">\n<h4>Mixed effects model with <em>fixed</em> cubic spline and random intercept</h4>\n<p>We can introduce some flexibility into the model by using a <a href=\"https://datascienceplus.com/cubic-and-smoothing-splines-in-r/\" rel=\"nofollow\" target=\"_blank\">cubic spline</a>, which is constructed using a piece-wise cubic polynomial defined by specific points (knots) along the x-axis.</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0 + cs(k) + \\delta A_j + b_j  + e_{ijk}\n\\]</span></p>\n<p>The cubic spline model is fit in <code>R</code> using the function <code>bs</code> in the <code>splines</code> package. In order to get more stable estimates, I’ve standardized the time measurement before using it in the model. In this case, the effect size estimate and standard error are the same as the linear model.</p>\n<pre>fix_cs <- lmer(y ~ A + bs(normk) + ( 1  | site) , data = dd)\nsummary(fix_cs)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n##   Estimate Std. Error \n##       6.23       0.85</pre>\n</div>\n<div id=\"mixed-effects-model-with-random-cubic-spline\" class=\"section level4\">\n<h4>Mixed effects model with <em>random</em> cubic spline</h4>\n<p>There is no reason to believe that each cluster shares the same time trend, as assumed in the first two models estimated here. So now we introduce additional flexibility by fitting random cubic spline for each cluster.</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0 + \\delta A_j + b_j + cs_j(k) + e_{ijk}\n\\]</span></p>\n<p>The only difference between the fixed cubic spline model estimation is that the <code>bs</code> function appears in random effect portion of the model. The effect size estimate is slightly more biased than the previous estimates but has slightly less uncertainty.</p>\n<pre>ran_cs <- lmer(y ~ A + ( bs(normk) | site) , data = dd)\nsummary(ran_cs)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n##   Estimate Std. Error \n##      6.546      0.799</pre>\n</div>\n<div id=\"generalized-additive-model-with-site-specific-smoothing\" class=\"section level4\">\n<h4>Generalized additive model with site-specific smoothing</h4>\n<p>Another flexible modeling approach is the <a href=\"https://m-clark.github.io/generalized-additive-models/\" rel=\"nofollow\" target=\"_blank\">generalized additive model</a>, which provides potentially even more flexibility than the spline models and can provide protections against over fitting. The underlying flexibility of the GAM is due to the wide range of basis functions that are available for the construction of the curve. I recommend taking a look the <a href=\"https://m-clark.github.io/generalized-additive-models/\" rel=\"nofollow\" target=\"_blank\">link</a> for a nice introduction.</p>\n<p>In this case, the model includes cluster-specific curves <span class=\"math inline\">\\(f_j(k)\\)</span>:</p>\n<p><span class=\"math display\">\\[\nY_{ijk} = \\beta_0 + \\delta A_j + f_j(k) + e_{ijk}\n\\]</span></p>\n<p>We estimate the model using the <code>gamm</code> function in the <code>mgcv</code> package. By setting the <code>bs</code> argument to “fs” in the smoothing function <code>s</code>, we will get estimated cluster-specific curves. “fs” refers to a special smooth factor interaction basis, where the interaction in this case is between site and time <span class=\"math inline\">\\(k\\)</span>.</p>\n<pre>gam <- gamm(y ~ A + s(k, site, bs = \"fs\", k = 5), data = dd, method=\"REML\")\ncbind(summary(gam$gam)$p.coeff, summary(gam$gam)$se)[2,]\n## [1] 6.204 0.863</pre>\n<p>The figure below shows the predicted site-specific curves for each of the estimated models. The rigidity of the linear and fixed cubic spline models is pretty clear. And in at least this particular case, the two flexible methods appear to generate quite similar predicted curves.</p>\n<p><img src=\"https://i2.wp.com/www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/index.en_files/figure-html/figure2-1.png?w=450&#038;ssl=1\" data-recalc-dims=\"1\" /></p>\n<p>The next figure shows the individual-level outcomes and the predicted curves for a small number of sites. It is clear that the curves for the less flexible methods are biased. The similarity of the flexible models is particularly evident here.</p>\n<p><img src=\"https://i1.wp.com/www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/index.en_files/figure-html/figure3-1.png?w=450&#038;ssl=1\" data-recalc-dims=\"1\" /></p>\n</div>\n</div>\n<div id=\"evaluating-bias-and-variance-of-treatment-effect-estimate\" class=\"section level3\">\n<h3>Evaluating bias and variance of treatment effect estimate</h3>\n<p>The results from a single data set are interesting, but we really need to understand how well the models perform over a large number of data sets. How do the model estimates of the true treatment effect (<span class=\"math inline\">\\(\\delta = 5\\)</span>) compare when considering bias, variance, and coverage of the 95% confidence interval?</p>\n<p>The replication process requires generating data and then fitting the models. The data generation uses the data definitions and data generating function provided earlier in the post. In this case, we will use 1000 data sets.</p>\n<pre>replicate <- function(){\n  \n  dd <- s_generate()\n  \n  linear <- lmer(y ~ A + k + ( 1  | site) , data = dd)\n  est.lin <- summary(linear)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n  \n  fix_cs <- lmer(y ~ A + bs(normk) + ( 1  | site) , data = dd)\n  est.fcs <- summary(fix_cs)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n  \n  ran_cs <- lmer(y ~ A + ( bs(normk) | site) , data = dd)\n  est.rcs <- summary(ran_cs)$coefficients[\"A\", c(\"Estimate\", \"Std. Error\")]\n  \n  gam <- gamm(y ~ A + s(k, site, bs = \"fs\", k = 5), data = dd, method=\"REML\")\n  est.gam <- cbind(summary(gam$gam)$p.coeff, summary(gam$gam)$se)[2,]\n  \n  dres <- data.table(t(est.lin), t(est.fcs), t(est.rcs), t(est.gam))\n  setnames(dres, \n    c(\"d.lin\", \"se.lin\", \"d.fcs\", \"se.fcs\", \"d.rcs\", \"se.rcs\", \"d.gam\", \"se.gam\")\n  )\n  \n  dres[]\n}\n\nres <- rbindlist(pblapply(1:1000, function(x) replicate()))</pre>\n<p>Each replication provides the point estimate of the treatment effect as well as the estimate of the standard error. Here is a sampling of the results:</p>\n<pre>res\n##       d.lin se.lin d.fcs se.fcs d.rcs se.rcs d.gam se.gam\n##    1:  4.53  0.847  4.53  0.847  4.73  0.805  4.55  0.850\n##    2:  3.68  0.911  3.68  0.911  3.68  0.873  3.77  0.908\n##    3:  3.34  0.734  3.34  0.734  3.33  0.685  3.36  0.729\n##    4:  4.95  0.690  4.95  0.690  5.07  0.680  4.89  0.688\n##    5:  5.75  0.865  5.75  0.865  5.67  0.776  5.81  0.868\n##   ---                                                    \n##  996:  5.27  1.001  5.27  1.001  5.17  0.974  5.35  1.008\n##  997:  5.94  0.842  5.94  0.842  6.10  0.818  5.89  0.839\n##  998:  4.92  0.910  4.92  0.910  4.91  0.876  4.94  0.916\n##  999:  4.72  0.799  4.72  0.799  4.41  0.696  4.71  0.786\n## 1000:  4.56  0.887  4.56  0.887  4.74  0.852  4.54  0.888</pre>\n<p>The average of the point estimates across all replications provides an estimate of the bias for each model. The four approaches are relatively unbiased, and this includes the less flexible approaches that didn’t seem to do so well at prediction on the individual level. The random cubic spline seems to have slightly less bias:</p>\n<pre>res[, .(lin = mean(d.lin), fcs = mean(d.fcs), rcs = mean(d.rcs), gam = mean(d.gam))] - 5\n##       lin    fcs    rcs    gam\n## 1: 0.0314 0.0314 0.0206 0.0304</pre>\n<p>A comparison of the observed standard errors suggests that the random cubic spline model is slightly more variable than the other three modeling approaches, suggesting a bias-variance trade-off.</p>\n<pre>res[, .(lin = sd(d.lin), fcs = sd(d.fcs), rcs = sd(d.rcs), gam = sd(d.gam))]\n##     lin  fcs   rcs   gam\n## 1: 0.92 0.92 0.941 0.919</pre>\n<p>And while all four methods underestimate the uncertainty, on average, the random cubic spline model most severely underestimated the standard errors:</p>\n<pre>res[, .(lin = mean(se.lin), fcs = mean(se.fcs), rcs = mean(se.rcs), gam = mean(se.gam))]\n##      lin   fcs  rcs   gam\n## 1: 0.907 0.907 0.87 0.908</pre>\n<p>Consistent with the disparities in variance estimates, the random cubic splines did not perform as well with respect to the coverage rates of the 95% confidence intervals:</p>\n<pre>coverage <- function(est, se) {\n  rmin <- est - 1.96 * se\n  rmax <- est + 1.96 * se\n  \n  mean(rmin < 5 &#038; rmax > 5)\n}\n\nres[, .(lin = coverage(d.lin, se.lin), fcs = coverage(d.fcs, se.fcs),\n        rcs = coverage(d.rcs, se.rcs), gam = coverage(d.gam, se.gam))]\n##      lin   fcs   rcs  gam\n## 1: 0.944 0.944 0.924 0.94</pre>\n<p>It will be interesting to see how the bias-variance trade-off plays out in the context of a stepped wedge design, particularly if the outcomes are binary. Will the less flexible methods continue to perform as well as the GAM model, and will the cubic spline model continue to underestimate the standard errors? More to come.</p>\n</div>\n\n<div style=\"border: 1px solid; background: none repeat scroll 0 0 #EDEDED; margin: 1px; font-size: 13px;\">\n<div style=\"text-align: center;\">To <strong>leave a comment</strong> for the author, please follow the link and comment on their blog: <strong><a href=\"https://www.rdatagen.net/post/2022-11-01-modeling-secular-trend-in-crt-using-gam/\"> ouR data generation</a></strong>.</div>\n<hr />\n<a href=\"https://www.r-bloggers.com/\" rel=\"nofollow\">R-bloggers.com</a> offers <strong><a href=\"https://feedburner.google.com/fb/a/mailverify?uri=RBloggers\" rel=\"nofollow\">daily e-mail updates</a></strong> about <a title=\"The R Project for Statistical Computing\" href=\"https://www.r-project.org/\" rel=\"nofollow\">R</a> news and tutorials about <a title=\"R tutorials\" href=\"https://www.r-bloggers.com/how-to-learn-r-2/\" rel=\"nofollow\">learning R</a> and many other topics. <a title=\"Data science jobs\" href=\"https://www.r-users.com/\" rel=\"nofollow\">Click here if you're looking to post or find an R/data-science job</a>.\n\n<hr>Want to share your content on R-bloggers?<a href=\"https://www.r-bloggers.com/add-your-blog/\" rel=\"nofollow\"> click here</a> if you have a blog, or <a href=\"http://r-posts.com/\" rel=\"nofollow\"> here</a> if you don't.\n</div><strong>Continue reading</strong>: <a href=\"https://www.r-bloggers.com/2022/10/modeling-the-secular-trend-in-a-cluster-randomized-trial-using-very-flexible-models/\">Modeling the secular trend in a cluster randomized trial using very flexible models</a>",
  "enclosure": "",
  "post-id": 334104
}