{
  "title": "Deep multimodality models in image search ranking stack",
  "link": "https://blogs.bing.com/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack",
  "pubDate": "Thu, 14 Oct 2021 08:00:00 -0700",
  "dc:creator": "Bing Team",
  "category": [
    "artificial intelligence",
    "bing",
    "Bing Image Search",
    "Bing Visual Search",
    "engineering"
  ],
  "guid": "07c12f5c-9817-4bb9-985d-2fd81ec76f97",
  "description": "Rank Multimodal (RankMM) The RankMM model effectively combines the search paradigms of a text query, page context, and images to aid image and video retrieval. RankMM models are Visual Language (VL) models which take page context into account to improve image and video retrieval performance in a web-scale search engine.",
  "content:encoded": "Microsoft Bing is constantly developing new ways to provide users with more relevant, accurate, and visually appealing results. We innovate, collaborate, and adopt cutting-edge advances in natural language processing, computer vision, and artificial intelligence to inspire and deliver the best experiences at an internet scale across billions of web queries and images.<br />\n<br />\nIn recent years, much ground-breaking work in image and video retrieval, auto-captioning, and visual question answering has ensued because of the development of transformer-based techniques for computer vision and natural language processing. However, applying similar techniques and the same models to a web-scale search engine to retrieve images and video in response to a text query is more complicated. The difficulty arises from searching through billions of images to retrieve relevant and aesthetically pleasing photos efficiently and quickly in the context of specific text search queries.<br />\n<br />\nIn 2019, we <a href=\"https://blogs.bing.com/search-quality-insights/2019/Multi-granularity-matching-for-Bing-Image-Search/\">discussed</a> how Bing's image search continues to evolve toward a more intelligent and precise search engine through multi-granularity matches, enhanced understanding of user queries, images, webpages, and their relationships. The vector-matching, attribute-matching, and Best Representative Query matching techniques assist in resolving a variety of complex cases, such as when users search for objects with specific context or attributes, such as {blonde man with a mustache} or {dance outfits for girls with a rose}<a href=\"#_ftn1\" name=\"_ftnref1\" title=\"\">[1]</a>.<br />\n<br />\nBuilding upon previous innovation, we developed end-to-end multi-modality deep learning models to improve image search results event further.<br />\n&nbsp;\n<h3>Rank Multi Modal (RankMM) Models</h3>\n\n<div>RankMM models are Visual Language (VL) models which take page context into account to improve image and video retrieval performance in a web-scale search engine. RankMM is a deep ranking model leveraging query, image/video, and the associated webpage to predict image/video relevance score. The RankMM model effectively combines the search paradigms of a text query, page context, and images to aid in image and video retrieval. Additionally, it enables different models to adapt to the cascading framework's various layers; models in each layer can use different model structures, training data, and training tasks.<br />\n<br />\nThe cascade framework of highly efficient and cost-effective deep learning models for efficient image retrieval consisting of a recall layer, a ranking layer, and a re-ranking layer. The result set is gradually narrowed through each layer, and the proportion of relevant and irrelevant images and videos gradually increases.<br />\n&nbsp;\n<h3>Search Stack - Cascading Framework</h3>\nThe cascaded framework aims to resolve the complexity of retrieving billions of images and videos with high relevancy and runtime latency needs of a web-scale search engine. The framework has separate components: Recall layer, Ranking layer, and Re-ranking layer. The result set is narrowed sequentially through each layer to retrieve images in the most efficient manner possible.<br />\n<br />\n<img alt=\"image004.png\" src=\"https://blogs.bing.com/getattachment/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack/image004.png.aspx?width=800&height=239\" style=\"width: 800px; height: 239px;\" title=\"image004.png\" />\n<div style=\"text-align: center;\"><em>Overview of the cascade search stack and visual language model in each layer</em></div>\n&nbsp;\n\n<h4>Recall Layer</h4>\nThe Recall layer's goal is to narrow down the result set from billions to thousands quickly. The proportion of positives and negatives in this layer is between O(10<sup>-6</sup>) and O(10<sup>-4</sup>). Traditional search engines use sparse word match, which builds an inverted index from documents and only recalls documents that contain all the query terms. However, in modern systems, dense vector match, using deep learning models to project query terms and documents in the same semantic space to calculate similarity, will return the highest similarity scores documents as search results. We combine the result set from both sparse word match and dense vector match as the input for the next layer in the recall layer for RankMM. For the dense vector match, we use a loosely coupled (LC) model in this layer. In the following section, we will go over the loosely coupled model.<br />\n&nbsp;\n<h4>Ranking Layer</h4>\nFollowing the Recall, the Ranking models will use predicted relevance scores to rank the thousands of images. In this layer, the ratio of positives to negatives is between O(10<sup>-1</sup>) and O(10<sup>0</sup>). The ranking layer in traditional search engine systems is typically a learning-to-rank (LTR) model such as XGBoost<a href=\"#_ftn1\" name=\"_ftnref1\" title=\"\">[2]</a> (Chen and Guestrin, 2016) or LamdaMART<a href=\"#_ftn2\" name=\"_ftnref2\" title=\"\">[3]</a> (Burges, 2010). LTR models rank the document list based on hand-crafted features such as the number of matched terms between query and document, BM25<a href=\"#_ftn3\" name=\"_ftnref3\" title=\"\">[4]</a> (Robertson et al.,2009), etc. To improve the ranking layer's quality, we create two transformer-based models, a loosely coupled (LC) model, and a tightly coupled (TC) model, and incorporate the predicted scores into the LTR model. Given that the recall output is still in the thousands of pixels and the search latency is typically in milliseconds, this approach should not be computationally expensive.<br />\n&nbsp;\n<h4>Re-Ranking Layer</h4>\nThe re-ranking layer reorders hundreds of top results from the previous ranking layer to improve precision. This layer contains a roughly equal number of positives and negatives (10<sup>0</sup>). This layer performs the re-ranking task using a list-wise LTR model<a href=\"#_ftn4\" name=\"_ftnref4\" title=\"\">[5]</a> (Cao et al., 2007). It is distinct from the LTR model used in the ranking layer as its objective is to predict the optimal order of the list, rather than a single query-document pair's relevance score. Since the result set in this layer is much smaller, we can incorporate relatively heavier models to obtain better results.\n\n<div>&nbsp;\n<h3>Model Structure</h3>\nOur VL models take three inputs: a query (Q), a page (P), and an image (I) to determine the degree of similarity between each query and each document. Additional page and image information is extracted from the document for contexts, such as the page title and image caption. We categorize the model structures that are used in our search stack into tightly coupled model (TC) and loosely coupled model (LC).<br />\n<br />\n<img alt=\"image006.png\" src=\"https://blogs.bing.com/getattachment/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack/image006.png.aspx?width=800&height=369\" style=\"width: 800px; height: 369px;\" title=\"image006.png\" /><br />\n<br />\n&nbsp;\n<h4>Tightly Coupled</h4>\nBy combining the three inputs (P, Q, and I), the tightly coupled model (TC) generates a prediction score by learning the joint representation of all input modalities and inherently model their relationship. In a transformer-based structure, the attention layers are used to simulate the interactions of two tokens. The TC model can simulate the Q, P, and I relationships. However, it is typically more expensive at runtime because we must infer this model using all of the Q, P, and I input each time the user submits a new query.<br />\n&nbsp;\n<h4>Loosely Coupled</h4>\nIn contrast to the TC model, in a loosely coupled model (LC), the query and other inputs are fed into two separate models, yielding a query model and a document model. Additionally, the doc model's input can be multi-modal, including both page and image input. The LC model outputs cosine distance between a query embedding vector and a document embedding vector. Thus, the TC model typically outperforms the LC model. We can compute the document embedding vectors for all documents in the search index in advance, store them, and then compute the query embedding vector during runtime, reducing computational cost and search latency. We call it a loosely coupled model because there is no direct interaction between the query and the document.<br />\n<br />\n<img alt=\"image008.png\" src=\"https://blogs.bing.com/getattachment/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack/image008.png.aspx?width=800&height=281\" style=\"width: 800px; height: 281px;\" title=\"image008.png\" /><br />\n<br />\nWe trained the models using a small amount of human-labeled data and large-scale semi-supervised data. First, we collected queries and top 1K image results to form billions of <Q, P, I> triplets. Then, we applied weighted sampling to the whole corpus to generate universal training data and removing the language bias. Next, we generated binary labels by treating the top 10 results as positive samples and the rest as negative. We also produced continuous labels by training a teacher model. For human labeling, we used a stratified sampling approach with each language to select data and graded each <Q, I> pair on three relevance levels, Excellent, Good, and Bad.<br />\n<br />\n<img alt=\"image010.png\" src=\"https://blogs.bing.com/getattachment/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack/image010.png.aspx?width=800&height=187\" style=\"width: 800px; height: 187px;\" title=\"image010.png\" />\n<h3>Results</h3>\nWe examined several different model structures for fusing the three signals, which can be an excellent starting point for future research. Additionally, we apply distinct RankMM models to each layer in a cascaded framework to account for each layer's unique properties. We see quality improvements in each layer because of those models. RankMM models have contributed significantly to improvements in the core quality of image and video search in real-world scenarios over the last year.<br />\n<br />\n<img alt=\"image012.png\" src=\"https://blogs.bing.com/getattachment/search-quality-insights/october-2021/Deep-multimodality-models-in-image-search-ranking-stack/image012.png.aspx?width=800&height=370\" style=\"width: 800px; height: 370px;\" title=\"image012.png\" /><br />\n<br />\nTake a look yourself how deep multimodality models in image search ranking stack helps in improving user experiences and search performance on <a href=\"https://www.bing.com/images/trending?FORM=ILPTRD\">bing.com/images</a> for your search query and in <a href=\"https://www.bing.com/visualsearch?FORM=ILPVIS\">Visual Search</a>.<br />\n<br />\nShare your experiences and provide feedback so that we can continue to improve our models.<br />\n<br />\n-&nbsp;&nbsp;<a href=\"https://www.linkedin.com/in/edward-cui/\">Edward Cui</a>, <a href=\"https://www.linkedin.com/in/lin-su-24a51747/\">Lin Su</a>, Yu Bao, <a href=\"https://www.linkedin.com/in/aaron-zhang-64a51727/\">Aaron Zhang</a>, <a href=\"https://www.linkedin.com/in/ashishjaiman/\">Ashish Jaiman</a>, and The Bing Multimedia team.<br />\n&nbsp;\n<h3>References&nbsp;</h3>\n\n<hr align=\"left\" size=\"1\" width=\"33%\" />\n<div id=\"ftn1\"><a href=\"#_ftnref1\" name=\"_ftn1\" title=\"\">[1]</a> <a href=\"https://blogs.bing.com/search-quality-insights/2019/Multi-granularity-matching-for-Bing-Image-Search/\">Multi-granularity matching for Bing Image Search | Search Quality Insights</a></div>\n\n<div id=\"ftn1\"><a href=\"#_ftnref1\" name=\"_ftn1\" title=\"\">[2]</a> <a href=\"https://arxiv.org/abs/1603.02754\">[1603.02754] XGBoost: A Scalable Tree Boosting System (arxiv.org)</a></div>\n\n<div id=\"ftn2\"><a href=\"#_ftnref2\" name=\"_ftn2\" title=\"\">[3]</a> <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">MSR-TR-2010-82.pdf (microsoft.com)</a></div>\n\n<div id=\"ftn3\"><a href=\"#_ftnref3\" name=\"_ftn3\" title=\"\">[4]</a> <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf\">LearningBM25MSRTechReport.dvi (microsoft.com)</a></div>\n\n<div id=\"ftn4\"><a href=\"#_ftnref4\" name=\"_ftn4\" title=\"\">[5]</a> <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf\">tr-2007-40.pdf (microsoft.com)</a></div>\n</div>\n&nbsp;\n\n<div id=\"ftn1\">&nbsp;</div>\n</div>\n"
}