{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2016-12-18T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/12/18/dask-dev-3",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<p>To increase transparency I’m blogging weekly about the work done on Dask and\nrelated projects during the previous week.  This log covers work done between\n2016-12-11 and 2016-12-18.  Nothing here is ready for production.  This\nblogpost is written in haste, so refined polish should not be expected.</p>\n\n<p>Themes of last week:</p>\n\n<ol>\n  <li>Benchmarking new scheduler and worker on larger systems</li>\n  <li>Kubernetes and Google Container Engine</li>\n  <li>Fastparquet on S3</li>\n</ol>\n\n<h3 id=\"rewriting-load-balancing\">Rewriting Load Balancing</h3>\n\n<p>In the last two weeks we rewrote a significant fraction of the worker and\nscheduler.  This enables future growth, but also resulted in a loss of our load\nbalancing and work stealing algorithms (the old one no longer made sense in the\ncontext of the new system.)  Careful dynamic load balancing is essential to\nrunning atypical workloads (which are surprisingly typical among Dask users) so\nrebuilding this has been all-consuming this week for me personally.</p>\n\n<p>Briefly, Dask initially assigns tasks to workers taking into account the\nexpected runtime of the task, the size and location of the data that the task\nneeds, the duration of other tasks on every worker, and where each piece of data\nsits on all of the workers.  Because the number of tasks can grow into the\nmillions and the number of workers can grow into the thousands, Dask needs to\nfigure out a near-optimal placement in near-constant time, which is hard.\nFurthermore, after the system runs for a while, uncertainties in our estimates\nbuild, and we need to rebalance work from saturated workers to idle workers\nrelatively frequently.  Load balancing intelligently and responsively is\nessential to a satisfying user experience.</p>\n\n<p>We have a decently strong test suite around these behaviors, but it’s hard to\nbe comprehensive on performance-based metrics like this, so there has also been\na lot of benchmarking against real systems to identify new failure modes.\nWe’re doing what we can to create isolated tests for every failure mode that we\nfind to make future rewrites retain good behavior.</p>\n\n<p>Generally working on the Dask distributed scheduler has taught me the\nbrittleness of unit tests.  As we have repeatedly rewritten internals while\nmaintaining the same external API our testing strategy has evolved considerably\naway from fine-grained unit tests to a mixture of behavioral integration tests\nand a very strict runtime validation system.</p>\n\n<p>Rebuilding the load balancing algorithms has been high priority for me\npersonally because these performance issues inhibit current power-users from\nusing the development version on their problems as effectively as with the\nlatest release.  I’m looking forward to seeing load-balancing humming nicely\nagain so that users can return to git-master and so that I can return to\nhandling a broader base of issues.  (Sorry to everyone I’ve been ignoring the\nlast couple of weeks).</p>\n\n<h3 id=\"test-deployments-on-google-container-engine\">Test deployments on Google Container Engine</h3>\n\n<p>I’ve personally started switching over my development cluster from Amazon’s EC2\nto Google’s Container Engine.  Here are some pro’s and con’s from my particular\nperspective.  Many of these probably have more to do with how I use each\nparticular tool rather than intrinsic limitations of the service itself.</p>\n\n<p>In Google’s Favor</p>\n\n<ol>\n  <li>Native and immediate support for Kubernetes and Docker, the combination of\nwhich allows me to more quickly and dynamically create and scale clusters\nfor different experiments.</li>\n  <li>Dynamic scaling from a single node to a hundred nodes and back ten minutes\nlater allows me to more easily run a much larger range of scales.</li>\n  <li>I like being charged by the minute rather than by the hour, especially\ngiven the ability to dynamically scale up</li>\n  <li>Authentication and billing feel simpler</li>\n</ol>\n\n<p>In Amazon’s Favor</p>\n\n<ol>\n  <li>I already have tools to launch Dask on EC2</li>\n  <li>All of my data is on Amazon’s S3</li>\n  <li>I have nice data acquisition tools,\n<a href=\"http://s3fs.readthedocs.io/en/latest/\">s3fs</a>, for S3 based on boto3.\nGoogle doesn’t seem to have a nice Python 3 library for accessing Google\nCloud Storage :(</li>\n</ol>\n\n<p>I’m working from Olivier Grisel’s repository\n<a href=\"https://github.com/ogrisel/docker-distributed\">docker-distributed</a> although\nupdating to newer versions and trying to use as few modifications from naive\ndeployment as possible.  My current branch is\n<a href=\"https://github.com/mrocklin/docker-distributed/tree/update\">here</a>.  I hope to\nhave something more stable for next week.</p>\n\n<h3 id=\"fastparquet-on-s3\">Fastparquet on S3</h3>\n\n<p>We gave fastparquet and Dask.dataframe a spin on some distributed S3 data on\nFriday.  I was surprised that everything seemed to work out of the box.  Martin\nDurant, who built both fastparquet and s3fs has done some nice work to make\nsure that all of the pieces play nicely together.  We ran into some performance\nissues pulling bytes from S3 itself.  I expect that there will be some tweaking\nover the next few weeks.</p>"
}