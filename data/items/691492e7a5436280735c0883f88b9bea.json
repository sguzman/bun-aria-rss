{
  "title": "Illustrated Guide to ROC and AUC",
  "link": "https://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/",
  "comments": "https://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/#comments",
  "pubDate": "Tue, 23 Jun 2015 11:49:55 +0000",
  "dc:creator": "Raffael Vogler",
  "category": [
    "Machine Learning",
    "R"
  ],
  "guid": "http://www.joyofdata.de/blog/?p=3597",
  "description": "(In a past job interview I failed at explaining how to calculate and interprete ROC curves &#8211; so here goes my attempt to fill this knowledge gap.) Think of a regression model mapping a number of features onto a real number &#8230; <a href=\"https://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p style=\"text-align: justify;\"><img class=\"alignright wp-image-3648 size-full\" src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc.png\" alt=\"roc\" width=\"251\" height=\"250\" srcset=\"https://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc.png 251w, https://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc-150x150.png 150w\" sizes=\"(max-width: 251px) 100vw, 251px\" />(In a past job interview I failed at explaining how to <a href=\"https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">calculate and interprete ROC curves </a>&#8211; so here goes my attempt to fill this knowledge gap.) Think of a <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">regression model</a> mapping a number of features onto a real number (potentially a probability). The resulting real number can then be mapped on one of two classes, depending on whether this predicted number is greater or lower than some choosable threshold. Let&#8217;s take for example a logistic regression and <a href=\"http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\" target=\"_blank\" rel=\"noopener noreferrer\">data on the survivorship of the Titanic accident</a> to introduce the relevant concepts which will lead naturally to the ROC (Receiver Operating Characteristic) and its AUC or AUROC (Area Under ROC Curve).</p>\n<p style=\"text-align: justify;\"><span id=\"more-3597\"></span></p>\n<h1>Titanic Data Set and the Logistic Regression Model</h1>\n<p style=\"text-align: justify;\">Every record in the data set represents a passenger &#8211; providing information on her/his age, gender, class, number of siblings/spouses aboard (sibsp), number of parents/children aboard (parch) and, of course, whether s/he survived the accident.</p>\n<p></p><pre class=\"crayon-plain-tag\"># https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/read_and_prepare_titanic_dataset.R\n> df <- read_and_prepare_titanic_dataset(\"~/Downloads/titanic3.csv\")\n> str(df)\n\n'data.frame':\t1046 obs. of  6 variables:\n  $ survived: Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 1 2 1 ...\n$ pclass  : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n$ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2 1 2 1 2 ...\n$ age     : num  29 0.92 2 30 25 48 63 39 53 71 ...\n$ sibsp   : int  0 1 1 1 1 0 1 0 2 0 ...\n$ parch   : int  0 2 2 2 2 0 0 0 0 0 ...</pre><p></p>\n<p style=\"text-align: justify;\">The logistic regression model is tested on batches of 10 cases with a model trained on the remaining N-10 cases &#8211; the test batches form a partition of the data. In short, <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\" target=\"_blank\" rel=\"noopener noreferrer\">Leave-10-out CV</a> has been applied to arrive at more accurate estimation of the out-of-sample error rates.</p>\n<p></p><pre class=\"crayon-plain-tag\"># https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/log_reg.R\n> predictions <- log_reg(df, size=10)\n> str(predictions)\n\n'data.frame':\t1046 obs. of  2 variables:\n $ survived: Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 2 2 1 2 1 2 ...\n $ pred    : num  0.114 0.854 0.176 0.117 0.524 ...</pre><p></p>\n<h1>Distribution of the Predictions</h1>\n<p style=\"text-align: justify;\">Now let&#8217;s first have a look at the distribution of survival and death cases on the predicted survival probabilities.</p>\n<p></p><pre class=\"crayon-plain-tag\"># https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R\n> plot_pred_type_distribution(predictions, 0.7)</pre><p></p>\n<p style=\"text-align: justify;\"><a href=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/06/prediction_type_distribution.png\"><img class=\"alignleft wp-image-3599 size-full\" src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/06/prediction_type_distribution.png\" alt=\"prediction_type_distribution\" width=\"926\" height=\"293\" srcset=\"https://www.joyofdata.de/blog/wp-content/uploads/2015/06/prediction_type_distribution.png 926w, https://www.joyofdata.de/blog/wp-content/uploads/2015/06/prediction_type_distribution-300x95.png 300w, https://www.joyofdata.de/blog/wp-content/uploads/2015/06/prediction_type_distribution-500x158.png 500w\" sizes=\"(max-width: 926px) 100vw, 926px\" /></a>If we consider survival as a positive (1) and death due to the accident as a negative (0) result, then the above plot illustrates the tradeoff we face upon choosing a reasonable threshold. If we increase the threshold the number of false positive (FP) results is lowered, while the number of false negative (FN) results increases.</p>\n<h1 style=\"text-align: justify;\">Receiver Operating Characteristic</h1>\n<p style=\"text-align: justify;\"><span class=\"alignright\"></span>This question of how to balance false positives and false negatives (depending on the cost/consequences of either mistake) arose on a major scale during World War II in context of interpretation of radar signals for identification of enemy air planes. For the purpose of visualizing and quantifying the impact of a threshold on the FP/FN-tradeoff the ROC curve was introduced. The ROC curve is the interpolated curve made of points whose coordinates are functions of the threshold:</p>\n<p><img src=\"http://www.joyofdata.de/blog/wp-content/ql-cache/quicklatex.com-7ac4bb516b31a31148268524821068e1_l3.png\" class=\"ql-img-inline-formula quicklatex-auto-format\" alt=\"&#92;&#116;&#101;&#120;&#116;&#123;&#116;&#104;&#114;&#101;&#115;&#104;&#111;&#108;&#100;&#125;&#32;&#61;&#32;&#92;&#116;&#104;&#101;&#116;&#97;&#32;&#92;&#105;&#110;&#32;&#92;&#109;&#97;&#116;&#104;&#98;&#98;&#123;&#82;&#125;&#32;&#92;&#116;&#101;&#120;&#116;&#123;&#44;&#32;&#104;&#101;&#114;&#101;&#32;&#125;&#32;&#92;&#116;&#104;&#101;&#116;&#97;&#32;&#92;&#105;&#110;&#32;&#091;&#48;&#44;&#49;&#093;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"252\" style=\"vertical-align: -5px;\"/></p>\n<p><img src=\"http://www.joyofdata.de/blog/wp-content/ql-cache/quicklatex.com-7674339e13a8cee308503ea19a9497d8_l3.png\" class=\"ql-img-inline-formula quicklatex-auto-format\" alt=\"&#82;&#79;&#67;&#95;&#120;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#61;&#32;&#70;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#70;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#123;&#70;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#43;&#32;&#84;&#78;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#32;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#70;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#123;&#92;&#35;&#78;&#125;\" title=\"Rendered by QuickLaTeX.com\" height=\"29\" width=\"340\" style=\"vertical-align: -9px;\"/></p>\n<p><img src=\"http://www.joyofdata.de/blog/wp-content/ql-cache/quicklatex.com-e0a5528b94aca1b673b3b611a867be7e_l3.png\" class=\"ql-img-inline-formula quicklatex-auto-format\" alt=\"&#82;&#79;&#67;&#95;&#121;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#61;&#32;&#84;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#84;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#123;&#70;&#78;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#43;&#32;&#84;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#32;&#61;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#84;&#80;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#123;&#92;&#35;&#80;&#125;&#32;&#61;&#32;&#49;&#32;&#45;&#32;&#92;&#102;&#114;&#97;&#99;&#123;&#70;&#78;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#125;&#123;&#92;&#35;&#80;&#125;&#32;&#61;&#32;&#49;&#32;&#45;&#32;&#70;&#78;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"29\" width=\"558\" style=\"vertical-align: -9px;\"/></p>\n<p style=\"text-align: justify;\"><a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Worked_example\" target=\"_blank\" rel=\"noopener noreferrer\">In terms of hypothesis tests</a> where rejecting the null hypothesis is considered a positive result the FPR (false positive rate) corresponds to the Type I error, the FNR (false negative rate) to the Type II error and (1 &#8211; FNR) to the power. So the ROC for above distribution of predictions would be:</p>\n<p></p><pre class=\"crayon-plain-tag\"># https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/calculate_roc.R\nroc <- calculate_roc(predictions, 1, 2, n = 100)\n\n# https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_roc.R\nplot_roc(roc, 0.7, 1, 2)</pre><p></p>\n<h1><a href=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc_and_cost_function.png\"><img class=\" size-full wp-image-3625 aligncenter\" src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc_and_cost_function.png\" alt=\"roc_and_cost_function\" width=\"944\" height=\"498\" srcset=\"https://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc_and_cost_function.png 944w, https://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc_and_cost_function-300x158.png 300w, https://www.joyofdata.de/blog/wp-content/uploads/2015/06/roc_and_cost_function-500x264.png 500w\" sizes=\"(max-width: 944px) 100vw, 944px\" /></a></h1>\n<p style=\"text-align: justify;\">The dashed lines indicate the location of the (FPR, TPR) corresponding to a threshold of 0.7. Note that the low corner (0,0) is associated with a threshold of 1 and the top corner (1,1) with a threshold of 0.</p>\n<p style=\"text-align: justify;\">The cost function and the corresponding coloring of the ROC points illustrate that an optimal FPR and TPR combination is determined by the associated cost. Depending on the use case false negatives might be more costly than false positive or vice versa. Here I assumed a cost of 1 for FP cases and a cost of 2 for FN cases.</p>\n<h1 style=\"text-align: justify;\">Area Under (ROC) Curve</h1>\n<p style=\"text-align: justify;\">The optimal point on the ROC curve is (FPR, TPR) = (0,1). No false positives and all true positives. So the closer we get there the better. The second essential observation is that the curve is by definition monotonically increasing.</p>\n<p style=\"text-align: justify;\"><img src=\"http://www.joyofdata.de/blog/wp-content/ql-cache/quicklatex.com-f3f30d4a2c0b950dab8ab97396515939_l3.png\" class=\"ql-img-inline-formula quicklatex-auto-format\" alt=\"&#70;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;<&#32;&#70;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;'&#41;&#32;&#92;&#105;&#109;&#112;&#108;&#105;&#101;&#115;&#32;&#92;&#116;&#104;&#101;&#116;&#97;&#32;>&#32;&#92;&#116;&#104;&#101;&#116;&#97;'&#32;&#92;&#105;&#109;&#112;&#108;&#105;&#101;&#115;&#32;&#84;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;&#41;&#32;&#92;&#108;&#101;&#113;&#32;&#84;&#80;&#82;&#40;&#92;&#116;&#104;&#101;&#116;&#97;'&#41;\" title=\"Rendered by QuickLaTeX.com\" height=\"18\" width=\"451\" style=\"vertical-align: -4px;\"/></p>\n<p style=\"text-align: justify;\">This inequation can be easily checked by looking at the first plot by mentally pushing the threshold (red line) up and down; it implies the monotonicity. Furthermore any reasonable model&#8217;s ROC is located above the identity line as a point below it would imply a prediction performance worse than random (in that case, simply inverting the predicted classes would bring us to the sunny side of the ROC space).</p>\n<p style=\"text-align: justify;\">All those features combined make it apparently reasonable to summarize the ROC into a single value by calculating the area of the convex shape below the ROC curve &#8211; this is the AUC. The closer the ROC gets to the optimal point of perfect prediction the closer the AUC gets to 1.</p>\n<p></p><pre class=\"crayon-plain-tag\"># AUC for the example\n\n> library(pROC)\n> auc(predictions$survived, predictions$pred)\n\nArea under the curve: 0.8421</pre><p></p>\n<h1>ROC and AUC for Comparison of Classifiers</h1>\n<p style=\"text-align: justify;\"><span class=\"alignleft\"></span>Mainly two reasons are responsible for why an ROC curve is a potentially powerful metric for comparison of different classifiers. One is that the resulting ROC is invariant against class skew of the applied data set &#8211; that means a data set featuring 60% positive labels will yield the same (statistically expected) ROC as a data set featuring 45% positive labels (though this will affect the cost associated with a given point of the ROC). The other is that the ROC is invariant against the evaluated score &#8211; which means that we could compare a model giving non-calibrated scores like a regular linear regression with a logistic regression or a random forest model whose scores can be considered as class probabilities.</p>\n<p style=\"text-align: justify;\">The AUC furthermore offers interesting interpretations:</p>\n<blockquote><p>The AUC has an important statistical property: the AUC of a classifier is equivalent to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. <em>[Fawcett]</em></p>\n<p>&nbsp;</p>\n<p>[The AUC] also has various natural intuitive interpretations, one of which is that it is the average sensitivity of a classifier under the assumption that one is equally likely to choose any value of the specificity — under the assumption of a uniform distribution over specificity. <em>[Hand]</em></p></blockquote>\n<p style=\"text-align: justify;\">As the ROC itself is variable with respect to a given data set it is necessary to average multiple ROCs derived from different data sets to arrive at a good estimation of a classifier&#8217;s true ROC function.</p>\n<p style=\"text-align: justify;\"><p style=\"text-align: center; border: 1px solid #378efd\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/stay-tuned.png\" alt=\"stay-tuned\" style=\"padding-right:30px; height:30px\"/>\n\n<a href=\"https://twitter.com/joyofdata\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/twitter.png\" alt=\"twitter\" height=\"28\" /></a>\n<a href=\"http://feedly.com/i/subscription/feed/http://www.joyofdata.de/blog/feed/\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/feedly.png\" alt=\"feedly\" width=\"30\" height=\"30\" /></a>\n<a href=\"https://github.com/joyofdata\" target=\"new\" style=\"padding-right:20px\">\n<img src=\"http://www.joyofdata.de/blog/wp-content/uploads/2015/05/github.png\" alt=\"github\" width=\"30\" height=\"30\" /></a>\n</p></p>\n<h1>Criticism of the AUC</h1>\n<p style=\"text-align: justify;\">It seems problematic, in the first place, to absolutely measure and compare the performance of classifiers with something as simple as a scalar between 0 and 1. The main fundamental reason of this is that problem specific cost functions hurt the assumption of points in the ROC space being homogenous in that regard and by that comparable across classifiers. This non-uniformity of the cost function causes ambiguities if ROC curves of different classifiers cross and on itself when the ROC curve is compressed into the AUC by means of integration over the false positive rate.</p>\n<blockquote><p>However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p ? P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. <em>[Hand]</em></p></blockquote>\n<p><a href=\"http://www.cs.iastate.edu/~cs573x/Notes/hand-article.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">David J. Hand</a> gives a statistically profound reasoning for the dubiousness of the AUC.</p>\n<h1>Sources</h1>\n<p>[Fawcett]: <a href=\"https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">&#8220;An introduction to ROC analysis&#8221; by Tom Fawcett</a></p>\n<p>[Hand]: <a href=\"http://www.cs.iastate.edu/~cs573x/Notes/hand-article.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">&#8220;Measuring classifier performance: a coherent alternative</a><br />\n<a href=\"http://www.cs.iastate.edu/~cs573x/Notes/hand-article.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">to the area under the ROC curve&#8221; by David J. Hand</a></p>\n<hr />\n<p style=\"text-align: justify;\">(original article published on <a href=\"http://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/\">www.joyofdata.de</a>)</p>\n",
  "wfw:commentRss": "https://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/feed/",
  "slash:comments": 9
}