{
  "title": "Estimating Benchmark Results Uncertainty",
  "link": "",
  "published": "2020-12-22T00:00:00+00:00",
  "updated": "2020-12-22T00:00:00+00:00",
  "id": "https://pkolaczk.github.io/estimating-benchmark-errors",
  "content": "<p>Physicists say that a measurement result given without an error estimate is worthless. This applies\nto benchmarking as well. We not only want to know how performant a computer program or a system is, \nbut we also want to know if we can trust the performance numbers. This article explains how to compute\nuncertainty intervals and how to avoid some traps caused by applying commonly known\nstatistical methods without validating their assumptions first.</p>\n\n<!--more-->\n\n<p>In my previous posts I focused on building a tool that can measure performance of a database system.\nThat simple tool gave a single value as its output. If you ran it multiple times, you’d notice, that even\nwhen measuring the performance of the same server again and again, the results are slightly different \neach time.</p>\n\n<h1 id=\"sources-of-variance\">Sources of Variance</h1>\n<p>There are several sources of variance of performance:</p>\n\n<ul>\n  <li>\n    <p>Unstable hardware performance. Many computers allow dynamic CPU frequency scaling, so their speed of processing\nis not constant. The frequency can be changed in response to load or temperature change. When benchmarking\nyou should beware of these effects, because they can introduce a lot of variance into the results. It is recommended\nto lock CPU frequency to a constant value low enough that it can be maintained for the whole measurement time, \nwithout the risk of thermal-throttling. Storage is also another source of big variance. You typically have\nno control over where the operating system decides to store data on a spinning drive.</p>\n  </li>\n  <li>\n    <p>Complexity of the software layers running the benchmarked program. For example JVM based programs manage memory\nby GC, which kicks in at random points in time. Even modern concurrent GCs impose some performance\npenalty when running, despite not causing a full freeze (pause) of the process threads. \nThere is also a lot of noise introduced by code compilation when the JVM is not yet warmed up. This applies likely to other\nmanaged runtimes as well.</p>\n  </li>\n  <li>\n    <p>Sharing the same hardware between multiple applications or services. If the service you’re benchmarking shares\nresources with other apps, then activity of those other apps will affect the results of your measurement. \nIf possible, try to lock exclusive access to physical devices critical to the performance of \nthe program you’re measuring.</p>\n  </li>\n  <li>\n    <p>External or internal asynchronous signals that need to be handled, e.g. network traffic, device interrupts, etc.\nEven if the benchmark program doesn’t use network or any peripheral devices, the system needs to spend a tiny \namount of computing power to handle them – and that power might be taken from the benchmarked app.</p>\n  </li>\n  <li>\n    <p>Caching effects. You may notice that an disk I/O heavy benchmark may run faster the second time you run it,\nif you don’t evict the page cache before each run. Some caches may be easy to reset, but some may be inaccessible.</p>\n  </li>\n  <li>\n    <p>Finally the benchmarked program itself might not have truly deterministic performance profile. For example a database\nsystem might want to run periodic tasks to cleanup dead data, etc.</p>\n  </li>\n</ul>\n\n<p>There are probably many other reasons that I missed. While it is worth spending some time on minimizing \nthe biggest sources of noise, it may be uneconomical or even outright impossible to get rid of all of the\nnoise. Therefore your results will always have some degree of variance.</p>\n\n<h1 id=\"basic-statistics\">Basic Statistics</h1>\n<p>Because the measured value behaves randomly, it can be modelled by a random variable. \nFor further reference, let’s call it \\(X\\).\nWe can characterize the performance of the system by the statistical distribution\nof \\(X\\). Of course, we never know the true distribution, but we can get some estimate\nof it by measuring (observing) the value \\(N\\) times and applying some exciting maths to them to get a few useful numbers.</p>\n\n<p>Programmers very frequently compute basic satictics such as:</p>\n<ul>\n  <li>Arithmetic mean – estimates of the <em>expected value</em> of \\(X\\)</li>\n  <li>Standard deviation – estimates how much values sampled from \\(X\\) are dispersed around the mean and is useful to judge the stability of the performance. \nThis doesn’t depend on the number of observations \\(N\\), because this is a property of \\(X\\). If you’re getting a high standard deviation of the results, \nyou may want to look at eliminating some of the sources of noise mentioned in the earlier section.</li>\n  <li>Standard error of the mean – estimates how far the mean we estimated from the sample could be from the true expected value of \\(X\\). Typically\nwe should expect this gets smaller the more measurements (more information about the underlying distribution) we have.</li>\n  <li>Percentiles / Histogram – can be used to visualize the distribution and is particularly useful when the distribution is not normal, e.g. in cases\nlike response-time distribution. High percentiles like 99.9% can give a lot of useful information about hiccups in the system caused by e.g. GC, etc.</li>\n</ul>\n\n<h1 id=\"standard-error-of-the-mean\">Standard Error of the Mean</h1>\n<p>The standard error \\(s_{\\bar{X}}\\) of the mean is a useful metric that describes how accurately \nwe estimated the expected value. We can estimate it from the data by using the following formulas:</p>\n\n\\[\\hat{s} = \\sqrt{ \\frac{1}{N - 1} \\sum_{i = 1}^{N} (x_i - \\bar{x})^2 }\\]\n\n\\[\\hat{s}_{\\bar{X}} = \\frac{\\hat{s}}{\\sqrt{N}}\\]\n\n<p>where \\(\\bar{x}\\) denotes the mean of the observations and \\(s\\) denotes the standard deviation from the mean. \nYou don’t have to remember that formula nor code it manually, because probably every statistical library in most programming languages \noffers it out-of-the-box.</p>\n\n<p>Why is the standard error such a useful metric? If we average a large enough number of observations, \nby <a href=\"https://en.wikipedia.org/wiki/Central_limit_theorem\">the Central Limit Theorem</a>, \nthe sample average \\(\\bar{X}\\) will be distributed very closely to the normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\) where\n\\(\\mu \\approx \\bar{x}\\) and \\(\\sigma \\approx \\hat{s}_{\\bar{X}}\\). The really good news is that typically \\(N = 10\\) is already \n“large enough” and also the distribution of \\(X\\) doesn’t need to be normal – it is enough it has finite mean and variance. \nBTW: If your benchmark results have infinite variance, then you may have a bigger problem than calculating the error of the mean.</p>\n\n<p>This conclusion allows us to obtain confidence intervals for \\(\\bar{x}\\):</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>lower bound</th>\n      <th>upper bound</th>\n      <th>probability \\(\\mathrm{E}X\\) lies within the bounds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>\\(\\bar{x} - \\hat{s}_{\\bar{X}}\\)</td>\n      <td>\\(\\bar{x} + \\hat{s}_{\\bar{X}}\\)</td>\n      <td>0.6827</td>\n    </tr>\n    <tr>\n      <td>\\(\\bar{x} - 2 \\hat{s}_{\\bar{X}}\\)</td>\n      <td>\\(\\bar{x} + 2 \\hat{s}_{\\bar{X}}\\)</td>\n      <td>0.9545</td>\n    </tr>\n    <tr>\n      <td>\\(\\bar{x} - 3 \\hat{s}_{\\bar{X}}\\)</td>\n      <td>\\(\\bar{x} + 3 \\hat{s}_{\\bar{X}}\\)</td>\n      <td>0.9973</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The normal distributuion probability function is another nice thing that’s available in most statistical libraries, so\nyou could compute custom confidence intervals for any probability you wish easily.</p>\n\n<h1 id=\"there-is-a-trap\">There is a Trap</h1>\n<p>Computing has this nice property that, unlike in biology, geography, medical or social sciences, it makes it easy and cheap to \nobtain huge samples. Every quadrupling of the sample size makes the confidence interval twice narrower for the same probability,\nbecause there is \\(\\sqrt{N}\\) in the denominator of the formula for the standard error, and the enumerator depends only on the distribution\nof \\(X\\), but not the number of observations. So, theoretically, we could estimate the expected value with any \naccuracy we wish, and we’re only limited by the time we’re allowed to run the test.</p>\n\n<p>Let’s take for example, that we want to compute the average response time of the server. \nWe issue \\(N\\) requests and calculate their mean. The local single node Casssandra server installed\non my development laptop can easily handle ~180k requests per second. Whoa! This makes my \\(N = 180000\\) for a benchmark that takes only a single second.\nI can easily collect 1 million data points in less than 10 seconds. 1 million data points \nshould make the error of the average response time estimate very small.</p>\n\n<p>Indeed, with 99.9% confidence interval and \\(N = 1\\) million, I obtained a result like this:</p>\n\n<pre>\nAvg response time: 2.45 ± 0.01 ms\n</pre>\n\n<p>What a lovely tiny error!\nIf it takes only a few seconds, why not run it a few more times to make really sure the result is correct?</p>\n\n<pre>\nAvg response time: 2.51 ± 0.01 ms\nAvg response time: 2.48 ± 0.01 ms\nAvg response time: 2.57 ± 0.01 ms\nAvg response time: 2.42 ± 0.01 ms\nAvg response time: 2.47 ± 0.01 ms\n</pre>\n\n<p>Wait… did I say <strong>99.9%</strong> confidence interval? \nWhy are my numbers fluctuating so much?! The differences definitely exceed the error interval.</p>\n\n<p>After double checking the implementation and running more tests to verify I’m not just getting those ugly results by an extreme unlikely bad luck, \nI found that…</p>\n\n<h1 id=\"assumptions-matter\">Assumptions Matter</h1>\n<p>There is a nice joke about biologists and statisticians going to a conference by train.</p>\n\n<blockquote>\n  <p>3 biologists and 3 statisticians were going to a scientific conference in the same car compartment.\nThe biologists talk about how lucky they were to get a 5% discount on their train tickets. \nThe statisticians responded: “We’ve paid even less – we’ve bought only one ticket for all three of us!”<br />\n“How so?” – the biologists got confused. “You’ll see”. \nWhen the statisticians notice the ticket controller appear in the car, all of them quickly go\nto a lavatory and close the door. When the controller sees the lavatory is occupied, he knocks \non the door, the statisticians give him the (single) ticket through a small gap below the door.\nBiologists are amazed seeing this.</p>\n\n  <p>After the conference, the biologists buy only one ticket\nfor the return home and want to use the statisticians’ method. \nThey meet with the same 3 statisticians in the compartment and \ntell them their plan. The statisticians respond: “Well, that’s a nice idea, but we’ve already \nimproved our method. See, this time we’ve bought no tickets at all!”. “How so?!”. “You’ll see”.\nWhen the controller appears, the biologists rush to the lavatory. The statisticians follow them\nand knock on the door. The biologists give the ticket they’ve bought, the statisticians take it\nand lock themselves in another lavatory. \nThe conclusion: don’t use statistical methods you don’t understand!</p>\n</blockquote>\n\n<p>The Central Limit Theorem holds if results of each experiment \\(X_i\\) \nhave the same distribution and are independent from each other. Unfortunately often neither \nof these assumption holds in the real world.</p>\n\n<p>Imagine the benchmark code is executed on a computer with a CPU that has some kind of “turbo mode”\ni.e. the operating system boosts its frequency if the CPU is cool, but then lowers it once it heats up.\nWhen running a sequence of experiments on such a machine, the results obtained at the beginning\nof the sequence would likely have different expected value than the results obtained near the end.\nThe longer you run the experiment and the more data points you collect, the lower the mean performance\nwould be, because the initial data points that were collected when the CPU frequency was boosted would \nmatter less relative to all the collected data. The best way of fixing that problem is to run \nthe benchmarks in an environment that doesn’t change performance in response to the load generated \nby the benchmark. However, after turning off any turbo modes and locking the CPU frequency to a \nconstant value, the variability of the results was still a lot larger than what the standard error \npredicted.</p>\n\n<p>Let’s look closer at where the formula for \\(s_{\\bar{x}}\\) is coming from. \nAssume we draw a sample \\(\\{x_1, x_2, ..., x_N\\}\\) from a sequence of random\nvariables \\(\\{X_1, X_2, ..., X_N\\}\\). A sequence of such variables is called a <em>stochastic process</em>.\nLet’s assume variables \\(X_i\\) are eqally distributed and have a finite expected value and variance:</p>\n\n\\[\\mathrm{E}X_1 = \\mathrm{E}X_2 = ... = \\mathrm{E}X_N = \\mathrm{E}X \\tag{1}\\]\n\n\\[\\mathrm{Var}X_1 = \\mathrm{Var}X_2 = ... = \\mathrm{Var}X_N = \\mathrm{Var}X \\tag{2}\\]\n\n<p>Let \\(\\bar{X}\\) be the random variable denoting the mean of all the observations:</p>\n\n\\[\\bar{X} = \\frac{1}{N}\\sum_{i = 1}^{N}X_i\\]\n\n<p>From (1) we can conclude that our mean will have the same expected value as the expected value\nof \\(X\\):</p>\n\n\\[\\mathrm{E}\\bar{X} = \\frac{1}{N}\\sum_{i = 1}^N\\mathrm{E}X_i = \\frac{1}{N}N\\mathrm{E}X = \\mathrm{E}X\\]\n\n<p>What about variance? We can use (2) in a similar way, however one must be careful when taking constants out \nfrom under the variance, because they become squared:</p>\n\n\\[\\mathrm{Var}\\bar{X} = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i = 1}^{N}X_i\\right) = \n  \\frac{1}{N^2}\\mathrm{Var}\\left(\\sum_{i = 1}^{N}X_i\\right)\\]\n\n<p>If we additionally assume that \\(X_i\\) is independent from \\(X_j\\) for \\(i \\neq j\\), the variance of a sum of variables \nis equal to the sum of their variances, hence we obtain:</p>\n\n\\[\\mathrm{Var}\\bar{X} = \\frac{1}{N^2}\\sum_{i = 1}^{N}\\mathrm{Var}X_i = \\frac{N}{N^2}\\mathrm{Var}X = \\frac{1}{N} \\mathrm{Var}X \\tag{3}\\]\n\n<p>Finally, standard deviation \\(s\\) is the square root of the variance:</p>\n\n\\[s_{\\bar{X}} = \\frac{1}{\\sqrt{N}}\\sqrt{\\mathrm{Var}X} = \\frac{s}{\\sqrt{N}}\\]\n\n<p>which is the formula that’s used to compute the standard error we mentioned earlier.</p>\n\n<p>However, if \\({X_i}\\) is not independent from \\({X_j}\\) for any \\(i \\neq j\\), then the formula\n(3) is incorrect, because the variance of a sum is not the sum of variances! The general formula for summing variances is \na bit more complex and contains additional covariance terms:</p>\n\n\\[\\mathrm{Var}\\left(\\sum_{i = 1}^{N}X_i\\right) = \\sum_{i = 1}^N\\sum_{j = 1}^N \\mathrm{Cov}(X_i, X_j) = \n\\sum_{i = 1}^{N}\\mathrm{Var}X_i + 2 \\sum_{i = 2}^N\\sum_{j &lt; i} \\mathrm{Cov(X_i, X_j)}\\]\n\n<p>After dividing by \\(N^2\\) we get:</p>\n\n\\[\\mathrm{Var}\\bar{X} = \\frac{1}{N^2}\\sum_{i = 1}^N\\sum_{j = 1}^N \\mathrm{Cov}(X_i, X_j) = \n\\frac{1}{N}\\left(\\mathrm{Var}X + \\frac{2}{N} \\sum_{i = 2}^N\\sum_{j &lt; i} \\mathrm{Cov(X_i, X_j)}\\right) \\tag{4}\\]\n\n<p>We can see that any dependency between experiments can cause the second term to become non-zero. \nIf variables \\(X_i\\) are positively correlated, the covariance term will be positive and the actual\nvariance of the mean would be larger. In extreme case, if all data points \nwere 100% positively correlated with each other, that is \\(\\mathrm{Cov}(X_i, X_j) = \\mathrm{Var}(X)\\) for all \\(i\\) and \\(j\\), formula (4) would become:</p>\n\n\\[\\mathrm{Var}\\bar{X} = \\mathrm{Var}X\\]\n\n<p>This means dependence between the results makes the <em>effective size of the sample</em> smaller than \\(N\\). \nIn the extreme case it can make effective \\(N\\) equal 1, regardless of the number of the observations we make.</p>\n\n<p>There is a good thought experiment that can explain this phenomenon without formulas.\nImagine you’re periodically checking the outside temperature to compute the yearly average. \nIn experiment A you take a measurement once per second. In experiment B, you take a measurement\nevery 10 ms, so you get 100 more data points than in experiment A. But can you really say \nthe expected error of the yearly average is really \\(\\sqrt{100}\\) times lower in experiment B than in A?\nYou can’t conclude that, because there is an extremely strong correlation between measurements \ntaken with such a high frequency and many of your data points are just exactly the same. \nBy sampling every 10 ms, you artificially increased the number of data points, \nbut due to strong autocorrelation, the amount of real information you collected didn’t really increase (much).</p>\n\n<p>This problem exists in benchmarking as well, although it is probably not as strong as with the weather.\nImagine that during the test, the operating system decides to put the benchmarking process\non hold for a while and perform another unrealated task. Or a JVM decides to start GC. Or Cassandra starts flushing a memtable.\nIf our data point is just a single database request, then\nseveral hundreds requests executed at that time can be affected. This makes response times not truly independent from\neach other.</p>\n\n<h1 id=\"better-estimate-of-variance-of-the-mean\">Better Estimate Of Variance Of The Mean</h1>\n<p>Let’s put formula (4) into practice. We need to find a way to compute these \\(\\mathrm{Cov}(X_i, X_j)\\) terms.\nThe nested sum looks a bit scary though, and estimating covariance from data itself requires \\(O(N)\\) steps, so \naltogether it would be \\(O(N^3)\\).</p>\n\n<p>We can simplify it a bit by adding an assumption that our stochastic process \\(X\\) is <em>weakly stationary</em>, i.e. its basic \nstatistical properties like mean, variance and autocovariance do not change when shifted in time.\nIn this case not only \\(\\mathrm{E}X_i = \\mathrm{E}X_{i + \\tau}\\) and \\(\\mathrm{Var}X_i = \\mathrm{Var}X_{i + \\tau}\\) but also \n\\(\\mathrm{Cov}(X_{i}, X_{j}) = \\mathrm{Cov}(X_{i + \\tau}, X_{j + \\tau})\\) for all values of \\(i\\) and \\(\\tau\\).</p>\n\n<p>For convenience, let’s define autocovariance with lag \\(k\\) as:</p>\n\n\\[\\gamma_X(k) = \\mathrm{Cov}(X_{1}, X_{1 + k}) = \\mathrm{Cov}(X_{2}, X_{2 + k}) = ... = \\mathrm{Cov}(X_{N - k}, X_{N})\\]\n\n\\[\\gamma_X(0) = \\mathrm{Var}X\\]\n\n<p>And now we can rewrite formula (4) into:</p>\n\n\\[\\mathrm{Var}\\bar{X} = \\frac{1}{N}\\left(\\gamma_X(0) + \\frac{2}{N} \\sum_{k = 1}^{N-1} (N - k)\\gamma_X(k)\\right) \\tag{5}\\]\n\n<p>Note the term \\((N - k)\\) that replaces the inner loop in (4). There are \\((N - k)\\) pairs \\((X_i, X_{i+k})\\) in our series \\(X\\).</p>\n\n<p>We can estimate variance and autocovariance from the data:</p>\n\n\\[\\hat{\\gamma}_X(k) = \\frac{1}{N}\\sum_{i = 1}^{N - k}(x_i - \\bar{x})(x_{i+k} - \\bar{x})\\]\n\n<p>Finally, the autocovariance-corrected formula for empirical error of the mean is:</p>\n\n\\[\\hat{s}_{\\bar{X}} = \\sqrt{\\frac{1}{N}\\left(\\hat{\\gamma}_X(0) + \\frac{2}{N} \\sum_{k = 1}^{N-1} (N - k)\\hat{\\gamma}_X(k)\\right)} \\tag{6}\\]\n\n<h1 id=\"another-trap\">Another Trap</h1>\n<p>Recipe (6) has a few problems when transformed directly into code:</p>\n<ul>\n  <li>the complexity is \\(O(N^2)\\), which is practical only for thousands of data points, but probably not for millions</li>\n  <li>autocovariances with higher lag \\(k\\) add a lot of noise because they are computed from fewer data points than autocovariances with low \\(k\\)</li>\n  <li>autocovariances \\(\\hat{\\gamma}\\) are estimated from the same data, and, guess what, they are… autocorrelated! :D</li>\n</ul>\n\n<p>A method that works reasonably well in practice is limiting \\(k\\) to a value much lower than \\(N\\), e.g. \\(\\sqrt{N}\\):</p>\n\n\\[\\hat{s}_{\\bar{X}} = \\sqrt{\\frac{1}{N}\\left(\\hat{\\gamma}_X(0) + \\frac{2}{N} \\sum_{k = 1}^{\\sqrt{N}} (N - k)\\hat{\\gamma}_X(k)\\right)}\\]\n\n<p>This is based on the assumption that data points that are distant in time are much less correlated with each other \nthan points that are close to each other, therefore truncating these higher-lag autocovariances doesn’t remove much useful information. \nThis assumption is not true in general case, however, many stochastic processes we can observe in practice in benchmarking have such property or are “close enough”. \nYou will need a bit of experimentation to find out the best lag cut-off for your system. \nAdditionally, a nice side-effect of limiting the number of autocovariance terms is reducing the asymptotic complexity.</p>\n\n<p>Another, more general method is multiplying autocovariance terms by diminishing weights less than 1, approaching zero when \\(k\\) reaches a hard \nlimit. A list of various weighting schemes is given by <a href=\"#Andrews1991\">[1]</a>. You can also find a broader discussion about applying this formula\nin <a href=\"#Okui2010\">[2]</a>.</p>\n\n<h1 id=\"code\">Code</h1>\n<p>Putting it all together, we can write the following function for estimating error of the mean:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">pub</span> <span class=\"k\">fn</span> <span class=\"nf\">error_of_the_mean</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"o\">&amp;</span><span class=\"p\">[</span><span class=\"nb\">f64</span><span class=\"p\">])</span> <span class=\"k\">-&gt;</span> <span class=\"nb\">f64</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"n\">v</span><span class=\"nf\">.len</span><span class=\"p\">()</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">1</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nn\">f64</span><span class=\"p\">::</span><span class=\"n\">NAN</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">let</span> <span class=\"n\">len</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"nf\">.len</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"nb\">f64</span><span class=\"p\">;</span>\n\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">mean</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n    <span class=\"k\">for</span> <span class=\"o\">&amp;</span><span class=\"n\">x</span> <span class=\"n\">in</span> <span class=\"n\">v</span><span class=\"nf\">.iter</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"n\">mean</span> <span class=\"o\">+=</span> <span class=\"n\">x</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">mean</span> <span class=\"o\">/=</span> <span class=\"n\">len</span><span class=\"p\">;</span>\n\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n    <span class=\"k\">for</span> <span class=\"o\">&amp;</span><span class=\"n\">x</span> <span class=\"n\">in</span> <span class=\"n\">v</span><span class=\"nf\">.iter</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"k\">let</span> <span class=\"n\">diff</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">;</span>\n        <span class=\"n\">var</span> <span class=\"o\">+=</span> <span class=\"n\">diff</span> <span class=\"o\">*</span> <span class=\"n\">diff</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">var</span> <span class=\"o\">/=</span> <span class=\"n\">len</span><span class=\"p\">;</span>\n\n    <span class=\"k\">let</span> <span class=\"n\">bandwidth</span> <span class=\"o\">=</span> <span class=\"n\">len</span><span class=\"nf\">.powf</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">);</span>\n    <span class=\"k\">let</span> <span class=\"n\">max_lag</span> <span class=\"o\">=</span> <span class=\"nf\">min</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"nf\">.len</span><span class=\"p\">(),</span> <span class=\"n\">bandwidth</span><span class=\"nf\">.ceil</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"nb\">usize</span><span class=\"p\">);</span>\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">cov</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n    <span class=\"k\">for</span> <span class=\"n\">lag</span> <span class=\"n\">in</span> <span class=\"mi\">1</span><span class=\"o\">..</span><span class=\"n\">max_lag</span> <span class=\"p\">{</span>\n        <span class=\"k\">let</span> <span class=\"n\">weight</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"n\">lag</span> <span class=\"o\">/</span> <span class=\"n\">len</span><span class=\"p\">;</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"n\">in</span> <span class=\"n\">lag</span><span class=\"o\">..</span><span class=\"n\">v</span><span class=\"nf\">.len</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n            <span class=\"k\">let</span> <span class=\"n\">diff_1</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">;</span>\n            <span class=\"k\">let</span> <span class=\"n\">diff_2</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"n\">lag</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">mean</span><span class=\"p\">;</span>\n            <span class=\"n\">cov</span> <span class=\"o\">+=</span> <span class=\"mf\">2.0</span> <span class=\"o\">*</span> <span class=\"n\">diff_1</span> <span class=\"o\">*</span> <span class=\"n\">diff_2</span> <span class=\"o\">*</span> <span class=\"n\">weight</span><span class=\"p\">;</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">cov</span> <span class=\"o\">/=</span> <span class=\"n\">len</span><span class=\"p\">;</span>\n\n    <span class=\"p\">((</span><span class=\"n\">var</span> <span class=\"o\">+</span> <span class=\"n\">cov</span><span class=\"p\">)</span><span class=\"nf\">.max</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">len</span><span class=\"p\">)</span><span class=\"nf\">.sqrt</span><span class=\"p\">()</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h1 id=\"references\">References</h1>\n<ol>\n  <li><a name=\"Andrews1991\"></a> D. W. K. Andrews, “Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation,” Econometrica, vol. 59, no. 3, pp. 817–858, 1991.</li>\n  <li><a name=\"Okui2010\"></a> R. Okui, “Asymptotically Unbiased Estimation Of Autocovariances And Autocorrelations With Long Panel Data,” Econometric Theory, vol. 26, no. 5, pp. 1263–1304, 2010.</li>\n</ol>",
  "author": {
    "name": "Piotr Kołaczkowski"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "Physicists say that a measurement result given without an error estimate is worthless. This applies to benchmarking as well. We not only want to know how performant a computer program or a system is, but we also want to know if we can trust the performance numbers. This article explains how to compute uncertainty intervals and how to avoid some traps caused by applying commonly known statistical methods without validating their assumptions first."
}