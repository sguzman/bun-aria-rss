{
  "id": "tag:blogger.com,1999:blog-5547907074344788039.post-6865187956066040557",
  "published": "2012-12-31T12:13:00.001-08:00",
  "updated": "2012-12-31T12:13:46.887-08:00",
  "title": "Howto: build a news aggregator (in 100 LOC)",
  "content": "<br /><div class=\"p1\">In this post, I'll cover how to build a crude, simple but working news aggregator in about 100 lines of python (<a href=\"https://github.com/leapingllamas/p-value.info/blob/master/news_aggregator_2012_12_31/news_agg_basic.py\">source code</a>).&nbsp;While a complete news aggregator will comprise of at least three steps:</div><ul class=\"ul1\"><li class=\"li1\"><b>filter</b>: select a subset of stories of interest, i.e. the top news -- e.g., that Zynga's shuttering of several games will be one of the most interesting to Techmeme's readership in the screenshot below</li><li class=\"li1\"><b>aggregate</b>: cluster like documents --- that Techcrunch and GamesIndustry International are both covering the same story</li><li class=\"li1\"><b>choose</b>: choose the best representative story -- that the Techcrunch story should be highlighted over the GamesIndustry International's story,</li></ul><div class=\"p1\">in this article, we will focus on the second step only, aggregation of similar articles.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-xqp3M78pElk/UOHCsw9a1_I/AAAAAAAAAE0/7JDYJNSqHaQ/s1600/techmeme.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"335\" src=\"http://3.bp.blogspot.com/-xqp3M78pElk/UOHCsw9a1_I/AAAAAAAAAE0/7JDYJNSqHaQ/s400/techmeme.jpg\" width=\"400\" /></a></div><div class=\"p2\"><br /></div><div class=\"p1\">Let us state the goal explicitly: given multiple sources of documents, such as RSS news feeds, cluster together similar documents that cover the same material. For instance, both Reuters and Associated Press might cover Hillary Clinton's admission to hospital (which was a major story at time of writing). In the <a href=\"https://news.google.com/\">google news</a> screen shot below we can see that USA Today, Detroit Free Press, Boston Herald and at least 10 other news organizations covered that story. We wish to recognize that these are the same underlying story and cluster or aggregate them.</div><div class=\"p2\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-mH5CamQDxDs/UOHDJjgeA6I/AAAAAAAAAE8/SBI-rXqLZ6w/s1600/googlenews.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"408\" src=\"http://4.bp.blogspot.com/-mH5CamQDxDs/UOHDJjgeA6I/AAAAAAAAAE8/SBI-rXqLZ6w/s640/googlenews.jpg\" width=\"640\" /></a></div><div class=\"p2\"><br /></div><div class=\"p1\">My approach was the following:</div><ul class=\"ul1\"><li class=\"li1\">parse RSS feeds</li><li class=\"li1\">extract keywords using <a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">TF-IDF</a> (i.e., term frequency * inverse document frequency)</li><li class=\"li1\">union those to a set of features that encompasses the whole corpus</li><li class=\"li1\">create a feature vector for each document</li><li class=\"li1\">compute matrix of cosine similarities</li><li class=\"li1\">run hierarchical clustering and plot dendrogram</li><li class=\"li1\">extract final clusters of interest</li></ul><div class=\"p1\">I'm not claiming this is an optimal scheme or will necessarily scale well and I did not focus on vectorized operations. It does, however, produce non-embarassing results in 100 lines of code. Later, we will discuss other approaches including the algorithm versus human debate.</div><div class=\"p2\"><br /></div><div class=\"p1\">Let's build an aggregator. To start, we will need some data sources. I chose a handful of RSS feeds about technology and IT news:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">feeds = [</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://www.sfgate.com/rss/feed/Tech-News-449.php',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://feeds.feedburner.com/TechCrunch/startups',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://news.cnet.com/8300-1001_3-92.xml',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://www.zdnet.com/news/rss.xml',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://www.computerweekly.com/rss/Latest-IT-news.xml',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://feeds.reuters.com/reuters/technologyNews',</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">&nbsp; &nbsp; 'http://www.tweaktown.com/news-feed/'</span></div><div class=\"p1\"><span style=\"color: #6fa8dc; font-family: inherit;\">]</span></div><div class=\"p2\"><br /></div><div class=\"p2\">(We can assume that these source have already mostly accomplished the first news aggregator step: filtering.)</div><div class=\"p2\"><br /></div><div class=\"p1\">Next, we need to parse out the source. I decided to use the document title and description which is a shortish summary. I used the <a href=\"http://code.google.com/p/feedparser/\">feedparser</a> python library and the natural language processing library <a href=\"http://nltk.org/\">NLTK</a>. For each document, I concatenate the title and description, convert to lowercase, and remove one character words. I store those final documents in a corpus and also store the titles to make the final results more interpretable:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import feedparser</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import nltk</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">corpus = []</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">titles=[]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">ct = -1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">for feed in feeds:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; d = feedparser.parse(feed)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; for e in d['entries']:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;words = nltk.wordpunct_tokenize(nltk.clean_html(e['description']))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;words.extend(nltk.wordpunct_tokenize(e['title']))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;lowerwords=[x.lower() for x in words if len(x) &gt; 1]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;ct += 1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;print ct, \"TITLE\",e['title']</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;corpus.append(lowerwords)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;titles.append(e['title'])</span></div><div class=\"p2\"><br /></div><div class=\"p1\">Here is a sample of those data (titles only):</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">0 TITLE Tweeters say the dumbest things</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">1 TITLE IFixit thrives by breaking tech items down</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">2 TITLE Earthcam's New Year's Live 2013</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">3 TITLE Content Fleet offers publishers hot tips</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">...</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">96 TITLE Sony stops shipments of PS2 console in Japan</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">97 TITLE RIM's first patent payment to Nokia: $65 million</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">98 TITLE ZTE aiming for thinner 5-inch 1080p-capable smartphone, Grand S has grand dreams</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">99 TITLE Japanese security firm to take to the sky in 2014, will unleash crime-fighting drones</span></div><div class=\"p2\"><br /></div><div class=\"p1\">We will extract keywords using TF-IDF. I modified the following from <a href=\"http://timtrueman.com/a-quick-foray-into-linear-algebra-and-python-tf-idf/\">this post</a>:</div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import math</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">from operator import itemgetter</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def freq(word, document): return document.count(word)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def wordCount(document): return len(document)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def numDocsContaining(word,documentList):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; count = 0</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; for document in documentList:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; if freq(word,document) &gt; 0:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; count += 1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; return count</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def tf(word, document): return (freq(word,document) / float(wordCount(document)))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def idf(word, documentList): return math.log(len(documentList) / numDocsContaining(word,documentList))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def tfidf(word, document, documentList): return (tf(word,document) * idf(word,documentList))</span></div><div class=\"p1\"><br /></div><div class=\"p1\">I only want to extract a few top keywords from each document and I'll store the set of those keywords, across all documents, in key_word_list:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import operator</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def top_keywords(n,doc,corpus):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; d = {}</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; for word in set(doc):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; d[word] = tfidf(word,doc,corpus)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; sorted_d = sorted(d.iteritems(), key=operator.itemgetter(1))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; sorted_d.reverse()</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; return [w[0] for w in sorted_d[:n]] &nbsp;&nbsp;</span></div><div class=\"p2\"><span style=\"color: #6fa8dc;\"><br /></span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">key_word_list=set()</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">nkeywords=4</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">[[key_word_list.add(x) for x in top_keywords(nkeywords,doc,corpus)] for doc in corpus]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">ct=-1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">for doc in corpus:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;ct+=1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;print ct,\"KEYWORDS\",\" \".join(top_keywords(nkeywords,doc,corpus))</span></div><div class=\"p2\"><br /></div><div class=\"p1\">Here is a sample of those data:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">0 KEYWORDS she tweeted athletes south</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">1 KEYWORDS ifixit repair devincenzi items</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">2 KEYWORDS earthcam live famed sound</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">3 KEYWORDS fleet content tips publishers</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">...</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">96 KEYWORDS playstation sony console ouya</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">97 KEYWORDS payment nokia patent agreement</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">98 KEYWORDS zte grand boots ces</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">99 KEYWORDS drones soon 2014 drone</span></div><div class=\"p2\"><br /></div><div class=\"p1\">Now that we have this superset of keywords, we need to go through each document again and compute TF-IDF for each term. Thus, this will be likely be a sparse vector as most of the entries will be zero.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">feature_vectors=[]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">n=len(corpus)</span></div><div class=\"p2\"><span style=\"color: #6fa8dc;\"><br /></span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">for document in corpus:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; vec=[]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; [vec.append(tfidf(word, document, corpus) if word in document else 0) for word in key_word_list]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; feature_vectors.append(vec)</span></div><div class=\"p2\"><br /></div><div class=\"p1\">With a vector of TF-IDF for each document, we can compare the similarity with each other. I decided to use <a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\">cosine similarity</a> (and yes, this matrix is symmetric and thus the code is far from optimized):</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import numpy</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">mat = numpy.empty((n, n))</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">for i in xrange(0,n):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; for j in xrange(0,n):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;mat[i][j] = nltk.cluster.util.cosine_distance(feature_vectors[i],feature_vectors[j])</span></div><div class=\"p2\"><br /></div><div class=\"p1\">We now have a single similarity number for each pair of documents.</div><div class=\"p2\"><br /></div><div class=\"p1\">The next step is clustering. Initially, the most obvious might be to use k-means or <a href=\"http://www.p-value.info/2012/11/why-k-means-works.html\">k-means++</a> clustering. It is simple, well-known and can work relatively well. As well as getting stuck on local optima, the major problem here, however, is that one has to choose k. This is not always easy. Moreover, I strongly suspect that the characteristics of news changes dramatically. That is, on presidential election day, almost all top stories will be about voting and speculation on the results. That is, there may be very few, very large clusters. On slow news days that story landscape may be very different. As such, I think it would be difficult to choose a constant k across days and the characteristics of the data may change so much that a scheme to choose k dynamically might be tricky.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">I chose a different approach: agglomerative or hierarchical clustering. Here clusters are grown by fusing neighboring documents to form a tree. The structure of that tree may change radically among different days but we can choose a similarity threshold to prune the tree to a set of final clusters. That similarity metric can be constant over days.</div><div class=\"p2\"><br /></div><div class=\"p1\">I used the <a href=\"http://pypi.python.org/pypi/hcluster\">hcluster</a> python library and saved the dendrogram to file:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">from hcluster import linkage, dendrogram</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">t = 0.8</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">Z = linkage(mat, 'single')</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">dendrogram(Z, color_threshold=t)</span></div><div class=\"p2\"><span style=\"color: #6fa8dc;\"><br /></span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">import pylab</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">pylab.savefig( \"hcluster.png\" ,dpi=800)</span></div><div class=\"p2\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-aTUpomMzpWk/UOHGcViQoBI/AAAAAAAAAFM/0fm06MWCPCg/s1600/hcluster.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"480\" src=\"http://4.bp.blogspot.com/-aTUpomMzpWk/UOHGcViQoBI/AAAAAAAAAFM/0fm06MWCPCg/s640/hcluster.png\" width=\"640\" /></a></div><div class=\"p1\"><br /></div><div class=\"p2\"><br /></div><div class=\"p1\">Finally, we need to chose a threshold from which to prune -- how similar is similar enough. 0.8 worked best for my data. There is then a little work to extract the clustered items from the dendrogram and print out the IDs and titles of the final clusters.</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">def extract_clusters(Z,threshold,n):</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;clusters={}</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;ct=n</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;for row in Z:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; if row[2] &lt; threshold:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n1=int(row[0])</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; n2=int(row[1])</span></div><div class=\"p2\"><span style=\"color: #6fa8dc;\"><br /></span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if n1 &gt;= n:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;l1=clusters[n1]&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;del(clusters[n1])&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;l1= [n1]</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp;&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if n2 &gt;= n:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;l2=clusters[n2]&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;del(clusters[n2])&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;l2= [n2] &nbsp; &nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; l1.extend(l2) &nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; clusters[ct] = l1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ct += 1</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; else:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return clusters</span></div><div class=\"p2\"><span style=\"color: #6fa8dc;\"><br /></span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">clusters = extract_clusters(Z,t,n)</span></div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">for key in clusters:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;print \"=============================================\"&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;for id in clusters[key]:</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; &nbsp; &nbsp;print id,titles[id]</span></div><div class=\"p2\"><br /></div><div class=\"p1\">Here then, in combination with the dendrogram above, are my final results:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">35 Huawei linked to plan to sell restricted equipment to Iran</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">65 Exclusive: Huawei partner offered embargoed HP gear to Iran</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">10 UK's Pearson invests in Barnes &amp; Noble's Nook</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">73 Pearson to buy stake in Nook, Barnes &amp; Noble shares up</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">21 Kim Dotcom To Host Mega’s Launch Event At His New Mega Zealand Mansion Next Month</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">94 Kim Dotcom to host Mega's launch event at his New Zealand mega mansion next month</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">90 Data suggests Facebook's Poke has led to online buzz around Snapchat</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">93 Snapchat videos and photos can be saved after all</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">14 Apple CEO Tim Cook paid $4.16 million</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">41 Apple's Tim Cook sees his 2012 pay fall 99 percent</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">8 Netflix CEO gets pay bump after 2012 cut</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">72 Netflix increases CEO Hastings' 2013 salary to $4 million</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">95 Nissan Leaf owners can get their batteries refreshed under new warranty options</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">99 Japanese security firm to take to the sky in 2014, will unleash crime-fighting drones</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">83 Windows RT ported to HTC HD2</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">77 Samsung Galaxy S III extended battery arriving in Germany on January 5</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">92 RumorTT: Samsung Galaxy S IV rumored for April Launch with integrated S Pen</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">75 Unnamed BlackBerry 10 device slides through FCC with AT&amp;T-capable LTE abilities</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">98 ZTE aiming for thinner 5-inch 1080p-capable smartphone, Grand S has grand dreams</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">40 HP confirms: Feds investigating the Autonomy acquisition</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">52 US Department of Justice is investigating allegations of accounting fraud at Autonomy</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">11 HP says gov't investigating troubled Autonomy unit</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">39 Autonomy founder fires back at HP after news of DOJ inquiry</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">55 Top 10 broadband stories of 2012</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">63 BT wins BDUK broadband contract in Norfolk</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">=============================================</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">97 RIM's first patent payment to Nokia: $65 million</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">84 Fujitsu blames weak Windows 8 demand for poor sales</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">87 Cheezburger network raises $5M in funding for LOLcats, FAIL Blog, others</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">79 Samsung has big plans for Silicon Valley with 1.1 million square foot R&amp;D center</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">89 Apple drops patent claim against Samsung Galaxy S III Mini</span></div><div class=\"p2\"><br /></div><div class=\"p1\">The complete output is <a href=\"https://github.com/leapingllamas/p-value.info/blob/master/news_aggregator_2012_12_31/results.txt\">here</a>.&nbsp;</div><div class=\"p1\"><br /></div><div class=\"p1\">To sort the items within a cluster, the final news aggragator step, one could sort by PageRank using code such as <a href=\"http://www.schurpf.com/google-pagerank-python/\">this</a>.</div><div class=\"p1\"><br /></div><div class=\"p1\">We see good cluster around Huawei, Person, Kim Dotcom, Facebook/Snapchat, Tim Cook, and Netflix. All of those seem reasonable. The next is spurious. Next is a broader cluster around smartphones. The next is a good four story cluster around the HP and Autonomy scandal. The next cluster relates to broadband and the documents are related but not really about the same material. Finally, there is a cluster around \"millions\" which cover both dollars and square foot.</div><div class=\"p2\"><br /></div><div class=\"p1\">The results are not great. I'm not going to build a <a href=\"http://techmeme.com/\">techmeme.com</a>&nbsp;competitor from this. However, they are surprisingly respectable for a first pass and for so few lines of code.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">To try to improve results, I tried doing a weighted TF-IDF. That is, I suspect that words in the title are relatively more important than those in the description. Thus, I set a weight 1 for each word in the description and weight title_wt &gt; 1 for each word in the title. When we compute term frequency I sum the weights for the words present rather than counts. The code for this change is <a href=\"https://github.com/leapingllamas/p-value.info/blob/master/news_aggregator_2012_12_31/news_agg_weighted.py\">here</a>.&nbsp;</div><div class=\"p1\"><br /></div><div class=\"p1\">I did not find that this improved results. The dominant parameter by far was the number of keywords from each document (nkeywords=4 above). Increasing this dramatically increases false positives (i.e. larger mixed clusters, such as the million cluster above). Reducing it below 4 led to very few clusters (for a given document similarity threshold). Actually, I was surprised that the optimal value of 4 was so low. I had expected a larger number would have been needed to disambiguate different the articles.</div><div class=\"p2\"><br /></div><div class=\"p1\">A potential improvement that I have not tried is stemming the words. In the sample keyword output above you might have noticed the following:</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">99 KEYWORDS drones soon 2014 drone</span></div><div class=\"p2\"><br /></div><div class=\"p1\">Drones and drone are the same noun. Had I stemmed these it would have increased the TF-IDF of drone and created a space for an additional keyword.</div><div class=\"p2\"><br /></div><div class=\"p1\">I may be able to tweak my current code and improve results. I could try different parameters, try different similarity metrics, different clustering schemes but it is not clear how good the results could be. Could I match a human curator? According to Gabe Rivera, the founder of Techmeme.com, algorithms are not currently a match for humans. His site employs a small team of editors [http://techmeme.com/about] to curate the top news:</div><div class=\"p2\"><br /></div><div class=\"p3\">Our experience leads us to believe that a thoughtful combination of both algorithmic and human editing offers the best means for curating in a space as broad as technology.</div><div class=\"p4\"><br /></div><div class=\"p5\">I somehow started thinking about news aggregators just a few days before <a href=\"http://getprismatic.com/news/home\">prismatic</a> got a <a href=\"http://techcrunch.com/2012/12/05/prismatic/)\">large round of funding</a>. In the days that followed there was some debate about algorithms versus humans. Here is Techcrunch:</div><blockquote class=\"tr_bq\">News aggregation is a glory-less pursuit, where even the companies that do succeed have to grapple with the fact that no algorithm is ever going to be as good as the human brain at judging what news/content will appeal to said brain</blockquote><div class=\"p8\">And Gabe Rivera (from Techmeme.com):&nbsp;</div><blockquote class=\"tr_bq\">\"A lot of people who think they can go all the way with the automated approach fail to realize a news story has become obsolete,\" said Rivera, explaining that an article can be quickly superseded even if it receives a million links or tweets. from&nbsp;<a href=\"http://gigaom.com/2012/11/29/techmeme-founder-give-me-human-editors-and-the-new-york-times/\">gigaom</a>&nbsp;</blockquote><div class=\"p9\">and paraphrasing:</div><blockquote class=\"tr_bq\">Some decisions, after all, are best left to humans.&nbsp;&nbsp;When a news story breaks, for example, or a much-hyped product launches, a wealth of news coverage follows.&nbsp;&nbsp;Which is the best version?&nbsp;&nbsp;While Techmeme’s algorithms often derive this answer from “signals” on the Internet, sometimes surfacing the right story for the audience — perhaps the most comprehensive take or the most nuanced coverage — requires human intervention.&nbsp;<a href=\"http://outbrain.com/\">outbrain.com</a></blockquote><div class=\"p11\">and finally in 2008, <a href=\"http://bub.blicio.us/techmeme-adds-a-human-touch/\">at the time Rivera introduced human editors</a></div><blockquote class=\"tr_bq\">I should note that the experience of introducing direct editing has been a revelation even for us, despite the fact that we planned it. Interacting directly with an automated news engine makes it clear that the human+algorithm combo can curate news far more effectively that the individual human or algorithmic parts. It really feels like the age of the news cyborg has arrived. Our goal is apply this new capability to producing the clearest and most useful tech news overview available</blockquote><div class=\"p12\">(See also <a href=\"http://storify.com/mathewi/gabe-rivera-s-thoughts-on-prismatic\">this thread</a>.)</div><div class=\"p7\"><br /></div><div class=\"p8\">As mentioned earlier, news aggregation is not just about clustering similar stories.&nbsp;There are really three steps: story selection, clustering like documents, and selecting best item from each cluster.&nbsp;This is certainly a set of hard problems. Which stories might be of interest depends on the audience and other contexts.&nbsp;Clustering similar documents is probably the easiest subtask and that is fraught with problems, e.g. that a bag-of-words model does not distinguish millions of dollars versus millions of square feet. (Thus, a n-gram model might be another possible approach.)&nbsp;There is a sea of potential news sources, they have different foci, reputations come and go, quality changes, news organization editors vary and change too. As Gabe mentions, there is a temporal aspect too where there is a window of interest before the news get stale.</div><div class=\"p2\"><br /></div><div class=\"p1\">There is no reason that an algorithm or suite of algorithms could not do this well. There are certainly a lot of social cues now that an algorithm could use to identify trending, interesting stories: the number of likes, shares or retweets; the reputation of those sharing the news; the number or rate of comments on those stories etc. To understand the audience and refine itself, an algorithm could use direct user feedback and again shares / like / retweets of those top stories.&nbsp;However, it remains a hard problem. With the big news sources covering core issues, smaller news operations have to work a different angle and sometimes those are the most interesting. Journalists are not consistent: they might write a blazing article one day and a mediocre the next. Thus, for now the algorithmic base + human editors or a pure crowd-sourced filtering (such as reddit) do seem to produce the best quality results.<br /><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Carl Anderson",
    "uri": "http://www.blogger.com/profile/11930448254473684406",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 17
}