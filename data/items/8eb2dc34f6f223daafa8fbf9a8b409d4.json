{
  "title": "Do Bayesians Overfit?",
  "link": "",
  "published": "2018-07-11T22:30:00+01:00",
  "updated": "2018-07-11T22:30:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2018-07-11:/sebastian/blog/do-bayesians-overfit.html",
  "summary": "<p><em>TLDR:</em> Yes, and there are precise results, although they are not as well known\nas they perhaps should be.</p>\n<p>Over the last few years I had many conversations in which the statement was\nmade that Bayesians methods are generally immune â€¦</p>",
  "content": "<p><em>TLDR:</em> Yes, and there are precise results, although they are not as well known\nas they perhaps should be.</p>\n<p>Over the last few years I had many conversations in which the statement was\nmade that Bayesians methods are generally immune to overfitting, or at least,\nrobust against overfitting, or---everybody would have to agree, right?---it\nclearly is better than maximum aposteriori estimation.</p>\n<p>Various loose arguments in support include the built-in Bayesian version of\nOccam's razor, and the principled treatment of any uncertainty throughout the estimation.\nHowever, over the years it has always bothered me that this argument is only\nmade casually and for many years I was not aware of a formal proof or\ndiscussion except for the well-known result that in case the model is\nwell-specified the Bayes posterior predictive is risk-optimal.</p>\n<p>Until recently!  A colleague pointed me to a book written by\n<a href=\"http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/\">Sumio Watanabe</a>\n(reference and thanks below) and this blog post is the result of the findings\nin this nice book.</p>\n<h1>Overfitting</h1>\n<p>In machine learning, the concept of <em>overfitting</em> is very important in practice.\nIn fact, it is perhaps the most important concept to understand when learning from data.\nMany practices and methods aim squarely at measuring and preventing\noverfitting.  The following are just a few examples:</p>\n<ul>\n<li><em>Regularization</em> limits the <em>capacity</em> of a machine learning model in order to avoid overfitting;</li>\n<li><em>Separating data into a training, validation, and test set</em>, is best practice\n  to assess generalization performance and to avoid overfitting;</li>\n<li><em>Dropout</em>, a regularization scheme for deep neural networks, is popularly\n  used to mitigate overfitting.</li>\n</ul>\n<p>But what is overfitting?  Can we formally define it?</p>\n<h2>Defining Overfitting</h2>\n<p>The most widely used loose definition is the following.</p>\n<blockquote>\n<p>Overfitting is the gap between the performance on the training set and the performance on the test set.</p>\n</blockquote>\n<p>This definition makes a number of assumptions:</p>\n<ol>\n<li>The data is independent and identically distributed and comes separated in a training set and a test set.</li>\n<li>There is a clearly defined performance measure.</li>\n<li>The test set is of sufficient size so that the performance estimation error is negligible.</li>\n</ol>\n<p>For example, in a classification task the performance measure may be the\nclassification error or the softmax-cross-entropy loss (log-loss).</p>\n<p>However, in practice this definition of overfitting can be too strict:\nin many cases we care about minimizing generalization error, not about the difference between generalization error and training error.\nFor deep learning in particular, the training error is often zero for the model that is selected as the one minimizing validation error.\nThe recent paper <a href=\"http://proceedings.mlr.press/v80/belkin18a.html\">(Belkin, Ma, Mandal, \"To Understand Deep Learning We Need to Understand Kernel Learning\", ICML 2018)</a> is studying this phenomenon.</p>\n<p>Is overfitting relevant for Bayesians as well?</p>\n<h2>The Bayesian Case</h2>\n<p>(This paragraph summarizes Bayesian prediction and contains nothing new or controversial.)</p>\n<p>Since <em>de Finetti</em>, a subjective Bayesian measures the performance of any model\nby the predicted likelihood of future observables.\nGiven a sample <span class=\"math\">\\(D_n=(x_1, \\dots, x_n)\\)</span>, generated from some true\ndata-generating distribution <span class=\"math\">\\(x_i \\sim Q\\)</span>, a Bayesian proceeds by setting up a\nmodel <span class=\"math\">\\(P(x|\\theta)\\)</span>, where <span class=\"math\">\\(\\theta\\)</span> are unknown parameters of the model, with\nprior <span class=\"math\">\\(P(\\theta)\\)</span>.\nThe data reveals information about <span class=\"math\">\\(\\theta\\)</span> in the form of a posterior distribution <span class=\"math\">\\(P(\\theta|D_n)\\)</span>.\nThe posterior distribution over parameters is then useful in constructing our best guess of what we will see next, in the form of the\n<em>posterior predictive distribution</em>,</p>\n<div class=\"math\">$$P(x | D_n) = \\int P(x | \\theta) \\, P(\\theta | D_n) \\,\\textrm{d}\\theta.$$</div>\n<p>Note that in particular the only degrees of freedom are in the choice of model <span class=\"math\">\\(P(x|\\theta)\\)</span> and in the prior <span class=\"math\">\\(P(\\theta)\\)</span>.</p>\n<p>How good is <span class=\"math\">\\(P(x|D_n)\\)</span>?\nA Bayesian cares about the predicted likelihood of future observables, which\ncorresponds to the cross-entropy loss, and is also called the <em>Bayesian\ngeneralization loss</em>,</p>\n<div class=\"math\">$$B_g = -\\mathbb{E}_{x \\sim Q}[\\log P(x|D_n)].$$</div>\n<p>Likewise, given our training sample <span class=\"math\">\\(D_n\\)</span>, we can define the <em>Bayesian training\nloss</em>,</p>\n<div class=\"math\">$$B_t = - \\frac{1}{n} \\sum_{i=1}^n \\log P(X_{n+1}=x_i | D_n).$$</div>\n<p>However, the concept of a \"Bayesian training loss'' is unnatural to a Bayesian\nbecause it uses the data twice: first, to construct the posterior predictive\n<span class=\"math\">\\(P(x|D_n)\\)</span>, and then a second time, to evaluate the likelihood on <span class=\"math\">\\(D_n\\)</span>.\nNevertheless, we will see below that the concept, combined with the so called\nGibbs training loss, is a very useful one.</p>\n<p>The question of whether Bayesians overfit is then clearly stated as:</p>\n<div class=\"math\">$$B_t \\ll B_g\\,?$$</div>\n<h1>A Simple Experiment</h1>\n<p>We consider an elementary experiment of sampling data from a Normal distribution with unknown mean.\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\mu &amp; \\sim &amp; \\mathcal{N}(\\mu_0, \\sigma^2_0),\\\\\nx_i &amp; \\sim &amp; \\mathcal{N}(\\mu, \\sigma^2), \\qquad i=1,\\dots,n.\n\\end{eqnarray}</div>\n<p>In this case, exact Bayesian inference is feasible because the <a href=\"https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\">posterior and\nposterior-predictive distributions have a simple closed-form\nsolution</a>, each of which is a\nNormal distributions.</p>\n<p>For varying sample size <span class=\"math\">\\(n\\)</span> we perform 2,000 replicates of generating data\naccording to the above sampling procedure and evaluate the Bayesian\ngeneralization loss and the Bayesian training loss.\nThe following plot shows the average errors over all replicates.</p>\n<p><img alt=\"Bayesian overfitting on a Normal mean\nexperiment\" src=\"http://www.nowozin.net/sebastian/blog/images/bayesian-bias-normal-mean.svg\"></p>\n<p>Clearly <span class=\"math\">\\(B_t &lt; B_g\\)</span>, and there is overfitting.</p>\n<p>What about non-Bayesian estimators, such as MAP estimation and maximum likelihood estimation?</p>\n<h2>Maximum Aposteriori (MAP) and Maximum Likelihood (MLE)</h2>\n<p>Two popular point estimators are the maximum aposteriori estimator (MAP), defined as</p>\n<div class=\"math\">$$\\hat{\\theta}_{\\textrm{map}} = \\textrm{argmax}_{\\theta} P(\\theta | D_n),$$</div>\n<p>and the maximum likelihood estimator (MLE), defined as</p>\n<div class=\"math\">$$\\hat{\\theta}_{\\textrm{mle}} = \\textrm{argmax}_{\\theta} \\sum_{i=1}^n \\log P(x_i|\\theta).$$</div>\n<p>Each of these estimators also has a generalization loss and a training loss.\nIn our experiment the MLE estimator is dominated by the MAP estimator, which is\nin turn dominated by the Bayesian estimate, which is optimal in terms of generalization loss.</p>\n<p><img alt=\"MAP and MLE comparison for the Normal mean experiment\" src=\"http://www.nowozin.net/sebastian/blog/images/bayesian-bias-normal-mean-mapmle.svg\"></p>\n<p>The gap between the MLE generalization error (top line, dotted) and the MAP\ngeneralization error (black dashed line) is due to the use of the informative\nprior about <span class=\"math\">\\(\\mu\\)</span>.\nThe gap between the Bayesian generalization error (black solid line) and the\nMAP generalization error (black dashed line) is due to the Bayesian handling of\nestimation uncertainty.\nIn this simple example the information contained in the prior is more important\nthan the Bayesian treatment of estimation uncertainty.</p>\n<p>Can we estimate <span class=\"math\">\\(B_g\\)</span> except via prediction on hold-out data?</p>\n<h1>WAIC: Widely Applicable Information Criterion</h1>\n<p>It turns out that we can estimate <span class=\"math\">\\(B_g\\)</span> to order <span class=\"math\">\\(O(n^{-2})\\)</span> from just our training set.\nThis is useful because it provides us an estimate of our generalization\nperformance, and hence can be used for model selection and hyperparameter\noptimization.</p>\n<p>The <em>Widely Applicable Information Criterion (WAIC)</em>, invented by <a href=\"http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/\">Sumio\nWatanabe</a>, estimates\nthe Bayesian generalization error,</p>\n<div class=\"math\">$$\\textrm{WAIC} = B_t + 2(G_t - B_t),$$</div>\n<p>where <span class=\"math\">\\(G_t\\)</span> is the <em>Gibbs training loss</em>, defined as the average loss of individual models from the posterior,</p>\n<div class=\"math\">$$G_t = -\\mathbb{E}_{\\theta \\sim P(\\theta|D_n)}\\left[\\frac{1}{n} \\sum_{i=1}^n \\log P(X_{n+1} = x_i|\\theta)\\right].$$</div>\n<p>Due to <a href=\"https://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen's\ninequality</a> we always have\n<span class=\"math\">\\(G_t &gt; B_t\\)</span> so the right hand summand in <span class=\"math\">\\(\\textrm{WAIC}\\)</span> is always positive.\nImportantly, given a training set we can actually evaluate <span class=\"math\">\\(\\textrm{WAIC}\\)</span>, but we cannot evaluate <span class=\"math\">\\(B_g\\)</span>.</p>\n<p>Watanabe showed that</p>\n<div class=\"math\">$$\\mathbb{E}[B_g] = \\mathbb{E}[\\textrm{WAIC}] + O(n^{-2}).$$</div>\n<p>Evaluating the previous experiment we can see that <span class=\"math\">\\(\\textrm{WAIC}\\)</span> indeed accurately estimates <span class=\"math\">\\(B_g\\)</span>.</p>\n<p><img alt=\"Widely applicable information criterion (WAIC): demonstrating good agreement with Bayesian generalization error\" src=\"http://www.nowozin.net/sebastian/blog/images/bayesian-bias-normal-mean-waic1.svg\"></p>\n<p>Even better, Watanabe also showed that <span class=\"math\">\\(\\textrm{WAIC}\\)</span> continues to estimate\nthe Bayesian generalization error accurately in singular models and in case the\nmodel is misspecified.\nHere, <em>singular</em> means that there is not a bijective map between model parameters and distributions.\n<em>Misspecified</em> means that no parameter exists which matches the true data-generating distribution.</p>\n<h2>WAIC with Approximate Posteriors</h2>\n<p>The above definition of <span class=\"math\">\\(\\textrm{WAIC}\\)</span> assumes an exact Bayesian posterior.\nIn practice we may not have the luxury of being able to compute the exact\nposterior, and instead use approximate inference methods such as\n<a href=\"http://www.mcmchandbook.net/\">Markov chain Monte Carlo (MCMC)</a> to get sample\nbased approximations to the posterior,\nor <a href=\"https://arxiv.org/pdf/1711.05597.pdf\">variational Bayes (VB)</a> to get\napproximations within a parametric family of distributions.</p>\n<p>For such approximations WAIC can degenerate considerably.\nFor example, consider a posterior family</p>\n<div class=\"math\">$$\\mathcal{U}_v := \\{ \\mathcal{N}(\\mu, C) \\, | \\, \\mu \\in \\mathbb{R}^d, \\, 0 \\prec C \\prec vI \\},$$</div>\n<p>where a scalar <span class=\"math\">\\(v &gt; 0\\)</span> bounds the eigenvalues of <span class=\"math\">\\(C\\)</span> from above.\nDoing variational Bayes with <span class=\"math\">\\(\\mathcal{U}_{\\epsilon}\\)</span> then corresponds to MAP\nestimation and the difference <span class=\"math\">\\(G_t - B_t\\)</span> will be close to zero, yet <span class=\"math\">\\(B_t\\)</span> can\nbe very small.  In this case, applying the <span class=\"math\">\\(\\textrm{WAIC}\\)</span> equation would\nsuggest that <span class=\"math\">\\(B_g \\approx B_t\\)</span>, so we severely underestimate the Bayesian\ngeneralization loss.</p>\n<p>The same holds true for MCMC: even if <span class=\"math\">\\(\\theta^{(1)}, \\dots, \\theta^{(K)}\\)</span> would be exact samples from\n<span class=\"math\">\\(P(\\theta|D_n)\\)</span> and we approximate the exact posterior by the set of these\nparameters, the estimate of <span class=\"math\">\\(B_t\\)</span> is now too large so <span class=\"math\">\\(G_t - B_t\\)</span> is\nunderestimated.</p>\n<h1>Conclusion</h1>\n<p>Clearly, Bayesians do overfit, just like any other procedure does.</p>\n<p>The following is a list of relevant literature with some comments.</p>\n<ul>\n<li><a href=\"https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A#\">(Sumio Watanabe, \"Algebraic Geometry and Statistical Learning Theory\", Cambridge University Press, 2009)</a>,\n  a monograph summarizing in detail earlier results.  The results are\n  particularly relevant for neural networks (which are singular models) and for Bayesian neural networks.</li>\n<li>For WAIC, see also Section 7.1 in <a href=\"http://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf\">(Sumio Watanabe, \"A Widely Applicable Bayesian Information Criterion\", JMLR, 2013)</a>.</li>\n<li><a href=\"http://stat.columbia.edu/~gelman/research/published/waic_understand3.pdf\">(Gelman, Hwang, Vehtari, \"Understanding predictive information criteria for Bayesian models\", Statistics and Computing, 2013)</a>\n  have good things to say about WAIC when comparing multiple information\n  criteria (AIC, DIC, WAIC), \"<em>WAIC is fully Bayesian (using the posterior\n  distribution rather than a point estimate), gives reasonable results in the\n  examples we have considered here, and has a more-or-less explicit connection to\n  cross-validation</em>\"</li>\n<li>The application of WAIC to select hyperparameters is studied by Watanabe in\n  <a href=\"https://arxiv.org/pdf/1503.07970.pdf\">(Watanabe, \"Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory\", 2015)</a>.</li>\n<li>Can one improve on the Bayesian risk?  Yes, if the model is misspecified.  A not so well-known paper,\n  <a href=\"https://projecteuclid.org/download/pdf_1/euclid.bj/1126126768\">(Fushiki, \"Bootstrap prediction and Bayesian prediction under misspecified models\", Bernoulli, 2005)</a>\n  compares the Bayesian posterior predictive generalization loss with the\n  generalization loss of a so-called <em>Bootstrap prediction</em> posterior, proving\n  that the latter is more efficient asymptotically in the misspecified setting.</li>\n</ul>\n<p><em>Acknowledgements</em>.  I thank <a href=\"https://www.microsoft.com/en-us/research/people/ryoto/\">Ryota\nTomioka</a> for exciting\ndiscussions and for pointing me to Watanabe's book.\nThanks also to <a href=\"https://www.inference.vc/about/\">Ferenc Husz&aacute;r</a> and\n<a href=\"http://cbl.eng.cam.ac.uk/Public/Turner/Turner\">Richard Turner</a> for\nfeedback on a draft of the article and to\n<a href=\"https://yobibyte.github.io/\">Vitaly Kurin</a> and Artem for a correction.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}