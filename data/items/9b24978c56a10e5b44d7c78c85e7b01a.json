{
  "title": "Transformers in Natural Language Processing — A Brief Survey",
  "link": "",
  "id": "https://www.georgeho.org/transformers-in-nlp/",
  "updated": "2020-05-23T00:00:00Z",
  "published": "2020-05-23T00:00:00Z",
  "content": "<p>I&rsquo;ve recently had to learn a lot about natural language processing (NLP), specifically\nTransformer-based NLP models.</p>\n<p>Similar to my previous blog post on <a href=\"https://www.georgeho.org/deep-autoregressive-models/\">deep autoregressive\nmodels</a>, this blog post is a write-up\nof my reading and research: I assume basic familiarity with deep learning, and aim to\nhighlight general trends in deep NLP, instead of commenting on individual architectures\nor systems.</p>\n<p>As a disclaimer, this post is by no means exhaustive and is biased towards\nTransformer-based models, which seem to be the dominant breed of NLP systems (at least,\nat the time of writing).</p>\n<h2 id=\"some-architectures-and-developments\">Some Architectures and Developments</h2>\n<p>Here&rsquo;s an (obviously) abbreviated history of Transformer-based models in NLP<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup> in\n(roughly) chronological order. I also cover some other non-Transformer-based models,\nbecause I think they illuminate the history of NLP.</p>\n<ol>\n<li>\n<p>word2vec and GloVe</p>\n<ul>\n<li>\n<p>These were the first instances of word embeddings pre-trained on large amounts of\nunlabeled text. These word embeddings generalized well to most other tasks (even\nwith limited amounts of labeled data), and usually led to appreciable improvements\nin performance.</p>\n</li>\n<li>\n<p>These ideas were immensely influential and have served NLP extraordinarily well.\nHowever, they suffer from a major limitation. They are <em>shallow</em> representations\nthat can only be used in the first layer of any network: the remainder of the\nnetwork must still be trained from scratch.</p>\n</li>\n<li>\n<p>The main appeal is well illustrated below: each word has its own vector\nrepresentation, and there are linear vector relationships can encode common-sense\nsemantic meanings of words.</p>\n<figure class=\"align-center\">\n<img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/linear-relationships.png\" alt=\"Linear vector relationships in word embeddings\">\n<figcaption>Linear vector relationships in word embeddings. Source: <a href=\"https://www.tensorflow.org/images/linear-relationships.png\">TensorFlow documentation</a>.</figcaption>\n</figure>\n</li>\n<li>\n<p>Further reading</p>\n<ul>\n<li><a href=\"http://arxiv.org/abs/1301.3781\">word2vec: Mikolov et al., Google. January 2013</a>\nand <a href=\"http://arxiv.org/abs/1310.4546\">October 2013</a>.</li>\n<li><a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe: Pennington et al., Stanford CS. EMNLP\n2014.</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Broadly speaking, after word2vec/GloVe and before Transformers, a lot of ink was\nspilled on other different approaches to NLP, including (but certainly not limited\nto)</p>\n<ol>\n<li>Convolutional neural networks</li>\n<li>Recurrent neural networks</li>\n<li>Reinforcement learning approaches</li>\n<li>Memory-augmented deep learning</li>\n</ol>\n<ul>\n<li>Perhaps the most famous of such models is <a href=\"https://allennlp.org/elmo\">ELMo (Embeddings from Language\nModels)</a> by AI2, which learned bidirectional word\nembeddings using LSTMs, and began NLP&rsquo;s fondness of Sesame Street.</li>\n<li>I won&rsquo;t go into much more detail here: partly because not all of these approaches\nhave held up as well as current Transformer-based models, and partly because I have\nplans for my computer that don&rsquo;t involve blogging about recent advances in NLP.</li>\n<li>Here is <a href=\"https://arxiv.org/abs/1708.02709\">a survey paper</a> (and an <a href=\"https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d\">associated blog\npost</a>)\npublished shortly after the Transformer was invented, which summarizes a lot of the\nwork that was being done during this period.</li>\n</ul>\n</li>\n<li>\n<p>Transformer</p>\n<ul>\n<li>\n<p>The authors introduce a feed-forward network architecture, using only attention\nmechanisms and dispensing with convolutions and recurrence entirely (which were not\nuncommon techniques in NLP at the time).</p>\n</li>\n<li>\n<p>It achieved state-of-the-art performance on several tasks, and (perhaps more\nimportantly) was found to generalize very well to other NLP tasks, even with\nlimited data.</p>\n</li>\n<li>\n<p>Since this architecture was the progenitor of so many other NLP models, it&rsquo;s\nworthwhile to dig into the details a bit. The architecture is illustrated below:\nnote that its feed-forward nature and multi-head self attention are critical\naspects of this architecture!</p>\n<figure class=\"align-center\">\n<img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/transformer-block.png\" alt=\"Graphical representation of BERT\">\n<figcaption>Graphical representation of BERT. Source: <a href=\"https://i.pinimg.com/originals/02/95/a3/0295a3be438ae68f604e53fc88c7edb4.png\">Pinterest</a>.</figcaption>\n</figure>\n</li>\n<li>\n<p>Further reading</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Vaswani et al., Google Brain. December 2017.</a></li>\n<li><a href=\"https://jalammar.github.io/illustrated-transformer/\"><em>The Illustrated Transformer</em> blog post</a></li>\n<li><a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\"><em>The Annotated Transformer</em> blog post</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>ULMFiT (Universal Language Model Fine-tuning for Text Classification)</p>\n<ul>\n<li>The authors introduce an effective transfer learning method that can be applied to\nany task in NLP: this paper introduced the idea of general-domain, unsupervised\npre-training, followed by task-specific fine-tuning. They also introduce other\ntechniques that are fairly common in NLP now, such as slanted triangular learning\nrate schedules. (what some researchers now call warm-up).</li>\n<li>Further reading\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1801.06146.pdf\">Howard and Ruder. January 2018.</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>GPT-1 and GPT-2 (Generative Pre-trained Transformers)</p>\n<ul>\n<li>At the risk of peeking ahead, GPT is largely BERT but with Transformer decoder\nblocks, instead of encoder blocks. Note that in doing this, we lose the\nautoregressive/unidirectional nature of the model.</li>\n<li>Arguably the main contribution of GPT-2 is that it demonstrated the value of\ntraining larger Transformer models (a trend that I personally refer to as the\n<em>Embiggening</em>).</li>\n<li>GPT-2 generated some controversy, as OpenAI <a href=\"https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2\">initially refused to open-source the\nmodel</a>,\nciting potential malicious uses, but <a href=\"https://www.theverge.com/2019/11/7/20953040/openai-text-generation-ai-gpt-2-full-model-release-1-5b-parameters\">ended up releasing the model\nlater</a>.</li>\n<li>Further reading\n<ul>\n<li><a href=\"https://openai.com/blog/language-unsupervised/\">Radford et al., OpenAI. June\n2018</a> and <a href=\"https://openai.com/blog/better-language-models/\">February\n2019</a>.</li>\n<li><a href=\"http://jalammar.github.io/illustrated-gpt2/\"><em>The Illustrated GPT-2</em> blog post</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>BERT (Bidirectional Encoder Representations from Transformers)</p>\n<ul>\n<li>\n<p>The authors use the Transformer encoder (and only the encoder) to pre-train deep\nbidirectional representations from unlabeled text. This pre-trained BERT model can\nthen be fine-tuned with just one additional output layer to achieve\nstate-of-the-art performance for many NLP tasks, without substantial task-specific\narchitecture changes, as illustrated below.</p>\n<figure class=\"align-center\">\n<img style=\"float: middle\" src=\"https://www.georgeho.org/assets/images/bert.png\" alt=\"Graphical representation of BERT\">\n<figcaption>Graphical representation of BERT. Source: <a href=\"https://i.pinimg.com/originals/02/95/a3/0295a3be438ae68f604e53fc88c7edb4.png\">Pinterest</a>.</figcaption>\n</figure>\n</li>\n<li>\n<p>BERT was a drastic development in the NLP landscape: it became almost a cliche to\nconclude that BERT performs &ldquo;surprisingly well&rdquo; on whatever task or dataset you\nthrow at it.</p>\n</li>\n<li>\n<p>Further reading</p>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1810.04805.pdf\">Devlin et al., Google AI Language, May 2019.</a></li>\n<li><a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">Accompanying blog post</a></li>\n<li><a href=\"https://jalammar.github.io/illustrated-bert/\"><em>The Illustrated BERT</em> blog post</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>RoBERTa (Robustly Optimized BERT Approach)</p>\n<ul>\n<li>\n<p>The scientific contributions of this paper are best quoted from its abstract:</p>\n<blockquote>\n<p>We find that BERT was significantly under-trained, and can match or exceed the\nperformance of every model published after it. [&hellip;] These results highlight the\nimportance of previously overlooked design choices, and raise questions about the\nsource of recently reported improvements.</p>\n</blockquote>\n</li>\n<li>\n<p>The authors use an identical architecture to BERT, but propose several improvements\nto the training routine, such as changing the dataset and removing the\nnext-sentence-prediction (NSP) pre-training task. Funnily enough, far and away the\nbest thing the authors did to improve BERT was just the most obvious thing: train\nBERT for longer!</p>\n</li>\n<li>\n<p>Further reading:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1907.11692\">Liu et al., Facebook AI. June 2019.</a></li>\n<li><a href=\"https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/\">Accompanying blog post</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>T5 (Text-to-Text Transfer Transformer)</p>\n<ul>\n<li>There are two main contributions of this paper:\n<ol>\n<li>The authors recast all NLP tasks into a text-to-text format: for example,\ninstead of performing a two-way softmax for binary classification, one could\nsimply teach an NLP model to output the tokens &ldquo;spam&rdquo; or &ldquo;ham&rdquo;. This provides a\nunified text-to-text format for all NLP tasks.</li>\n<li>The authors systematically study and compare the effects of pre-training\nobjectives, architectures, unlabeled datasets, transfer approaches, and other\nfactors on dozens of canonical NLP tasks.</li>\n</ol>\n</li>\n<li>This paper (and especially the tables in the appendices!) probably cost the Google\nteam an incredible amount of money, and the authors were very thorough in ablating\nwhat does and doesn&rsquo;t help for a good NLP system.</li>\n<li>Further reading\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1910.10683.pdf\">Raffel et al., Google. October 2019.</a></li>\n<li><a href=\"https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">Accompanying blog post</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"some-thoughts-and-observations\">Some Thoughts and Observations</h2>\n<p>Here I comment on some general trends that I see in Transformer-based models in NLP.</p>\n<ol>\n<li>\n<p>Ever since Google developed the Transformer in 2017, most NLP contributions are not\narchitectural: instead most recent advances have used the Transformer model as-is, or\nusing some subset of the Transformer (e.g. BERT and GPT use exclusively Transformer\nencoder and decoder blocks, respectively). Instead, recent research has focused on\nthe way NLP models are pre-trained or fine-tuned, or creating a new dataset, or\nformulating a new NLP task to measure &ldquo;language understanding&rdquo;, etc.</p>\n<ul>\n<li>I&rsquo;m personally not sure what to make of this development: why did we collectively\nagree that architectural research wasn&rsquo;t worth pursuing anymore?</li>\n<li>But spinning this the other way, we see that Transformers are a <em>fascinating</em>\narchitecture: the model has proven so surprisingly versatile and easy to teach that\nwe are still making meaningful advances with the same architecture. In fact, it is\nstill an open question how and why Transformers perform as well as they do: there\nis an open field of research focusing on answering this question for BERT (since\nBERT has been uniquely successful model) called\n<a href=\"https://huggingface.co/transformers/bertology.html\">BERTology</a>.</li>\n</ul>\n</li>\n<li>\n<p>It was never a question of <em>whether</em> NLP systems would follow computer vision&rsquo;s model\nof fine-tuning pre-trained models (i.e. training a model on ImageNet and then doing\ntask-specific fine-tuning for downstream applications), but rather <em>how</em>.</p>\n<ol>\n<li>What specific task and/or dataset should NLP models be pre-trained on?\n<ul>\n<li>Language modelling has really won out here: BERT was originally published with a\n<em>next-sentence prediction</em> (NSP) pre-training task, which RoBERTa completely did\naway with.</li>\n</ul>\n</li>\n<li>Exactly <em>what</em> is being learnt during pre-training?\n<ul>\n<li>Initially it was a separate vector for each token (i.e. pre-training a shallow\nrepresentation of text), and these days it is an entire network is pre-trained.</li>\n<li>Sebastian Ruder <a href=\"https://thegradient.pub/nlp-imagenet/\">wrote a great article</a>\nthat delves more into this topic.</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>There are (generally speaking) three flavors of Transformer models.</p>\n<ol>\n<li>Autoregressive models</li>\n<li>Autoencoding models</li>\n<li>Sequence-to-sequence models</li>\n</ol>\n<ul>\n<li>Hugging Face does an excellent job of summarizing the differences between these\nthree flavors of models in <a href=\"https://huggingface.co/transformers/summary.html\">their <em>Summary of the\nModels</em></a>, which I&rsquo;ve reproduced\nhere:</li>\n</ul>\n<blockquote>\n<p>Autoregressive models are pretrained on the classic language modeling task: guess\nthe next token having read all the previous ones. They correspond to the decoder of\nthe original transformer model, and a mask is used on top of the full sentence so\nthat the attention heads can only see what was before in the next, and not what’s\nafter. Although those models can be fine-tuned and achieve great results on many\ntasks, the most natural application is text generation. A typical example of such\nmodels is GPT.</p>\n<p>Autoencoding models are pretrained by corrupting the input tokens in some way and\ntrying to reconstruct the original sentence. They correspond to the encoder of the\noriginal transformer model in the sense that they get access to the full inputs\nwithout any mask. Those models usually build a bidirectional representation of the\nwhole sentence. They can be fine-tuned and achieve great results on many tasks such\nas text generation, but their most natural application is sentence classification\nor token classification. A typical example of such models is BERT.</p>\n<p>[&hellip;]</p>\n<p>Sequence-to-sequence models use both the encoder and the decoder of the original\ntransformer, either for translation tasks or by transforming other tasks to\nsequence-to-sequence problems. They can be fine-tuned to many tasks but their most\nnatural applications are translation, summarization and question answering. The\noriginal transformer model is an example of such a model (only for translation), T5\nis an example that can be fine-tuned on other tasks.</p>\n</blockquote>\n</li>\n<li>\n<p>Different NLP models learn different kinds of embeddings, and it&rsquo;s worth\nunderstanding the differences between these various learnt representations.</p>\n<ol>\n<li>Contextual vs non-contextual embeddings\n<ul>\n<li>The first word embeddings (that is, word2vec and GloVe) were <em>non-contextual</em>:\neach word had its own embedding, independent of the words that came before or\nafter it.</li>\n<li>Almost all other embeddings are <em>contextual</em> now: when embedding a token, they\nalso consider the tokens before &amp;/ after it.</li>\n</ul>\n</li>\n<li>Unidirectional vs bidirectional embeddings\n<ul>\n<li>When considering the context of a token, the question is whether you should\nconsider the tokens both before and after it (i.e. bidirectional embeddings), or\njust the tokens that came before (i.e. unidirectional embeddings).</li>\n<li>Unidirectional embeddings make the sense when generating text (i.e. text\ngeneration must be done in the way humans write text: in one direction). On the\nother hand, bidirectional embeddings make sense when performing sentence-level\ntasks such as summarization or rewriting.</li>\n<li>The Transformer was notable in that it had bidirectional encoder blocks and\nunidirectional decoder blocks. That&rsquo;s why BERT [GPT-2] produces bidirectional\n[unidirectional] embeddings, since it&rsquo;s a stack of Transformer encoders\n[decoders].</li>\n<li>Note that the unidirectional/bidirectional distinction is related to whether or\nnot the model is autoregressive: autoregressive models learn unidirectional\nembeddings.</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>Transformer-based models have had an interesting history with scaling.</p>\n<ul>\n<li>This trend probably started when GPT-2 was published: &ldquo;it sounds very dumb and too\neasy, but magical things happen if you make your Transformer model bigger&rdquo;.</li>\n<li>An open question is, how do Transformer models scale (along any dimension of\ninterest)? For example, how much does dataset size or the number of layers or the\nnumber of training iterations matter in the ultimate performance of a Transformer\nmodel? At what point does making your Transformer model &ldquo;bigger&rdquo; (along any\ndimension of interest) provide diminishing returns?</li>\n<li>There is some <a href=\"https://github.com/huggingface/awesome-papers#march-24-2020\">solid\nwork</a> being done to\nanswer this question, and there seems to be good evidence for some fairly\nsurprising conclusions!</li>\n</ul>\n</li>\n</ol>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>Since writing this blog post, there have been several more Transformer-based NLP models published, such as the <a href=\"https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\">Reformer</a> from Google and <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a> from OpenAI. Because I can&rsquo;t possibly keep up with <em>all</em> new Transformer-based models, I won&rsquo;t be writing about them.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}