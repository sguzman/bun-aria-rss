{
  "title": "The Softmax Function Derivative (Part 2)",
  "link": "https://aimatters.wordpress.com/2020/06/14/derivative-of-softmax-layer/",
  "comments": "https://aimatters.wordpress.com/2020/06/14/derivative-of-softmax-layer/#comments",
  "dc:creator": "Stephen Oman",
  "pubDate": "Sun, 14 Jun 2020 16:58:41 +0000",
  "category": [
    "Artificial Intelligence",
    "Example",
    "Machine Intelligence",
    "Programming"
  ],
  "guid": "http://aimatters.wordpress.com/?p=1210",
  "description": "In a previous post, I showed how to calculate the derivative of the Softmax function. This function is widely used in Artificial Neural Networks, typically in final layer in order to estimate the probability that the network&#8217;s input is in one of a number of classes. In this post, I&#8217;ll show how to calculate the [&#8230;]",
  "content:encoded": "\n<p>In a <a href=\"https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\">previous post</a>, I showed how to calculate the derivative of the Softmax function. This function is widely used in Artificial Neural Networks, typically in final layer in order to estimate the probability that the network&#8217;s input is in one of a number of classes.</p>\n\n\n\n<p>In this post, I&#8217;ll show how to calculate the derivative of the whole Softmax Layer rather than just the function itself.</p>\n\n\n\n<p>The Python code is based on the excellent article by Eli Bendersky which can be found <a rel=\"noreferrer noopener\" href=\"https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\" target=\"_blank\">here</a>.</p>\n\n\n\n<h2>The Softmax Layer </h2>\n\n\n\n<p><amp-fit-text layout=\"fixed-height\" min-font-size=\"6\" max-font-size=\"72\" height=\"80\">A Softmax Layer in an Artificial Neural Network is typically composed of two functions. The first is the usual sum of all the weighted inputs to the layer. The output of this is then fed into the Softmax function which will output the probability distribution across the classes we are trying to predict. Here&#8217;s an example with three inputs and five classes:</amp-fit-text></p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img data-attachment-id=\"1216\" data-permalink=\"https://aimatters.wordpress.com/softmaxlayerann/\" data-orig-file=\"https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png\" data-orig-size=\"595,416\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"softmaxlayerann\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png?w=300\" data-large-file=\"https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png?w=595\" src=\"https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png?w=595\" alt=\"\" class=\"wp-image-1216\" srcset=\"https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png 595w, https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png?w=150 150w, https://aimatters.files.wordpress.com/2020/06/softmaxlayerann.png?w=300 300w\" sizes=\"(max-width: 595px) 100vw, 595px\" /></figure></div>\n\n\n\n<p>For a given output <em>z<sub>i</sub></em>, the calculation is very straightforward:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=z_%7Bi%7D%3Dw_%7Bi1%7Dx_%7B1%7D%2Bw_%7Bi2%7Dx_%7B2%7D%2B...%2Bw_%7Bin%7Dx_%7Bn%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z_%7Bi%7D%3Dw_%7Bi1%7Dx_%7B1%7D%2Bw_%7Bi2%7Dx_%7B2%7D%2B...%2Bw_%7Bin%7Dx_%7Bn%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_%7Bi%7D%3Dw_%7Bi1%7Dx_%7B1%7D%2Bw_%7Bi2%7Dx_%7B2%7D%2B...%2Bw_%7Bin%7Dx_%7Bn%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"z_{i}=w_{i1}x_{1}+w_{i2}x_{2}+...+w_{in}x_{n}\" class=\"latex\" /></p>\n\n\n\n<p>We simply multiply each input to the node by it&#8217;s corresponding weight. Expressing this in vector notation gives us the familiar:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Ctextbf%7Bz%7D%3D%5Ctextbf%7Bw%7D%5E%7B%5Ctextbf%7BT%7D%7D%5Ctextbf%7Bx%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ctextbf%7Bz%7D%3D%5Ctextbf%7Bw%7D%5E%7B%5Ctextbf%7BT%7D%7D%5Ctextbf%7Bx%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctextbf%7Bz%7D%3D%5Ctextbf%7Bw%7D%5E%7B%5Ctextbf%7BT%7D%7D%5Ctextbf%7Bx%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;textbf{z}=&#92;textbf{w}^{&#92;textbf{T}}&#92;textbf{x}\" class=\"latex\" /></p>\n\n\n\n<p>The vector <strong>w</strong> is two dimensional so it&#8217;s actually a matrix and we can visualise the formula for our example as follows:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img data-attachment-id=\"1231\" data-permalink=\"https://aimatters.wordpress.com/matrix-sum/\" data-orig-file=\"https://aimatters.files.wordpress.com/2020/06/matrix-sum.png\" data-orig-size=\"355,150\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"matrix-sum\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://aimatters.files.wordpress.com/2020/06/matrix-sum.png?w=300\" data-large-file=\"https://aimatters.files.wordpress.com/2020/06/matrix-sum.png?w=355\" src=\"https://aimatters.files.wordpress.com/2020/06/matrix-sum.png?w=355\" alt=\"\" class=\"wp-image-1231\" srcset=\"https://aimatters.files.wordpress.com/2020/06/matrix-sum.png 355w, https://aimatters.files.wordpress.com/2020/06/matrix-sum.png?w=150 150w, https://aimatters.files.wordpress.com/2020/06/matrix-sum.png?w=300 300w\" sizes=\"(max-width: 355px) 100vw, 355px\" /></figure></div>\n\n\n\n<p>I&#8217;ve already covered the Softmax Function itself in the previous post, so I&#8217;ll just repeat it here for completeness:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Csigma%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D+%7B%5Csum_%7Bj%3D1%7D%5EK+e%5E%7Bz_%7Bj%7D%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=3&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Csigma%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D+%7B%5Csum_%7Bj%3D1%7D%5EK+e%5E%7Bz_%7Bj%7D%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=3&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csigma%28z_%7Bi%7D%29%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D+%7B%5Csum_%7Bj%3D1%7D%5EK+e%5E%7Bz_%7Bj%7D%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=3&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;sigma(z_{i})=&#92;frac{e^{z_{i}}} {&#92;sum_{j=1}^K e^{z_{j}}}\" class=\"latex\" /></p>\n\n\n\n<p>Here&#8217;s the python code for that:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nimport numpy as np\n\n# input vector\nx = np.array(&#91;0.1,0.5,0.4])\n\n# using some hard coded values for the weights\n# rather than random numbers to illustrate how \n# it works\nW = np.array(&#91;&#91;0.1, 0.2, 0.3, 0.4, 0.5],\n             &#91;0.6, 0.7, 0.8, 0.9, 0.1],\n             &#91;0.11, 0.12, 0.13, 0.14, 0.15]])\n\n# Softmax function\ndef softmax(Z):\n    eZ = np.exp(Z)\n    sm = eZ / np.sum(eZ)\n    return sm\n\nZ = np.dot(np.transpose(W), x)\nh = softmax(Z)\nprint(h)\n</pre></div>\n\n\n<p>Which should give us the output <em>h</em> (the hypothesis):</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; gutter: false; title: ; notranslate\">\n&#91;0.19091352 0.20353145 0.21698333 0.23132428 0.15724743]\n</pre></div>\n\n\n<h2>Calculating the Derivative</h2>\n\n\n\n<p>The Softmax layer is a combination of two functions, the summation followed by the Softmax function itself. Mathematically, this is usually written as:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=h+%3D+S%28Z%28x%29%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=h+%3D+S%28Z%28x%29%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=h+%3D+S%28Z%28x%29%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"h = S(Z(x))\" class=\"latex\" /></p>\n\n\n\n<p>The next thing to note is that we will be trying to calculate the change in the hypothesis <em>h</em> with respect to changes in the weights, not the inputs. The overall derivative of the layer that we are looking for is:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=h%27+%3D++%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=h%27+%3D++%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=h%27+%3D++%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"h&#039; =  &#92;frac{d&#92;textbf{S}}{d&#92;textbf{w}}\" class=\"latex\" /></p>\n\n\n\n<p>We can use the differential chain rule to calculate the derivative of the layer as follows:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot++%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot++%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot++%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7Bw%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{d&#92;textbf{S}}{d&#92;textbf{w}} = &#92;frac{d&#92;textbf{S}}{d&#92;textbf{Z}} &#92;cdot  &#92;frac{d&#92;textbf{Z}}{d&#92;textbf{w}}\" class=\"latex\" /></p>\n\n\n\n<p><a href=\"https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\">In the previous post</a>, I showed how to work out dS/dZ and just for completeness, here is a short Python function to carry out the calculation:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ndef sm_dir(S):\n    S_vector = S.reshape(S.shape&#91;0],1)\n    S_matrix = np.tile(S_vector,S.shape&#91;0])\n    S_dir = np.diag(S) - (S_matrix * np.transpose(S_matrix))\n    return S_dir\n\nDS = sm_dir(h)\nprint(DS)\n</pre></div>\n\n\n<p>The output of that function is a matrix as follows:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; gutter: false; title: ; notranslate\">\n&#91;&#91; 0.154465 -0.038856 -0.041425 -0.044162 -0.030020]\n &#91;-0.038856  0.162106 -0.044162 -0.047081 -0.032004]\n &#91;-0.041425 -0.044162 0.1699015 -0.050193 -0.034120]\n &#91;-0.044162 -0.047081 -0.050193  0.177813 -0.036375]\n &#91;-0.030020 -0.032004 -0.034120 -0.036375  0.132520]]\n</pre></div>\n\n\n<h3>Derivative of Z</h3>\n\n\n\n<p>Let&#8217;s next look at the derivative of the function Z() with respect to W, dZ/dW. We are trying to find the change in each of the elements of Z(), <em>z<sub>k</sub></em> when each of the weights <em>w<sub>ij</sub></em> are changed.</p>\n\n\n\n<p>So right away, we are going to need a matrix to hold all of those values. Let&#8217;s assume that the output vector of Z() has K elements. There are (<em>i</em> <img src=\"https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;cdot\" class=\"latex\" /> <em>j</em>) individual weights in W. Therefore, our matrix of derivatives is going to be of dimensions (K, (<em>i</em> <img src=\"https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ccdot&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;cdot\" class=\"latex\" /> <em>j</em>)). Each of the elements of the matrix will be a partial derivative of the output <em>z<sub>k</sub></em> with respect to the particular weight <em>w<sub>ij</sub></em>:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS%7D%7D%7B%5Cdelta%7Bx%7D%7D+%3D+%5Cleft+%5B+%5Cbegin%7Barray%7D%7Bccc%7D+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5C%5C++%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7Bk%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%26+%5Cldots+%5C%5C+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5Cend%7Barray%7D+%5Cright+%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS%7D%7D%7B%5Cdelta%7Bx%7D%7D+%3D+%5Cleft+%5B+%5Cbegin%7Barray%7D%7Bccc%7D+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5C%5C++%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7Bk%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%26+%5Cldots+%5C%5C+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5Cend%7Barray%7D+%5Cright+%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7BS%7D%7D%7B%5Cdelta%7Bx%7D%7D+%3D+%5Cleft+%5B+%5Cbegin%7Barray%7D%7Bccc%7D+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7B1%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5C%5C++%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7Bk%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%26+%5Cldots+%5C%5C+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D%7B%5Cdelta%7Bw_%7B11%7D%7D%7D+%26+%5Cldots+%26+%5Cfrac%7B%5Cdelta%7Bz_%7BK%7D%7D%7D+%7B%5Cdelta%7Bw_%7B53%7D%7D%7D+%5Cend%7Barray%7D+%5Cright+%5D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{S}}{&#92;delta{x}} = &#92;left [ &#92;begin{array}{ccc} &#92;frac{&#92;delta{z_{1}}}{&#92;delta{w_{11}}} & &#92;ldots & &#92;frac{&#92;delta{z_{1}}} {&#92;delta{w_{53}}} &#92;&#92;  &#92;ldots & &#92;frac{&#92;delta{z_{k}}}{&#92;delta{w_{ij}}} & &#92;ldots &#92;&#92; &#92;frac{&#92;delta{z_{K}}}{&#92;delta{w_{11}}} & &#92;ldots & &#92;frac{&#92;delta{z_{K}}} {&#92;delta{w_{53}}} &#92;end{array} &#92;right ]\" class=\"latex\" /></p>\n\n\n\n<p>Taking one of those elements, using our example above, we can see how to work out the derivative:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=z_%7B1%7D+%3D+w_%7B11%7Dx_%7B1%7D+%2B+w_%7B12%7Dx_%7B2%7D+%2B+w_%7B13%7Dx_%7B3%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=z_%7B1%7D+%3D+w_%7B11%7Dx_%7B1%7D+%2B+w_%7B12%7Dx_%7B2%7D+%2B+w_%7B13%7Dx_%7B3%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=z_%7B1%7D+%3D+w_%7B11%7Dx_%7B1%7D+%2B+w_%7B12%7Dx_%7B2%7D+%2B+w_%7B13%7Dx_%7B3%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"z_{1} = w_{11}x_{1} + w_{12}x_{2} + w_{13}x_{3}\" class=\"latex\" /></p>\n\n\n\n<p>None of the other weights are used in<em> z<sub>1</sub></em>. The partial derivative of <em>z<sub>1</sub></em> with respect to <em>w<sub>11</sub></em> is <em>x<sub>1</sub></em>. Likewise, the partial derivative of <em>z<sub>1</sub></em> with respect to <em>w<sub>12</sub></em> is <em>x<sub>2</sub></em>, and with respect to <em>w<sub>13</sub></em> is <em>x<sub>3</sub></em>. The derivative of <em>z<sub>1</sub></em> with respect to the rest of the weights is 0.</p>\n\n\n\n<p>This makes the whole matrix rather simple to derive, since it is mostly zeros. Where the elements are not zero (i.e. where i = k), then the value is <em>x<sub>j</sub></em>. Here is the corresponding Python code to calculate that matrix.</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\n# derivative of the Summation Function Z w.r.t weight matrix W given inputs x\n\ndef z_dir(Z, W, x):\n    dir_matrix = np.zeros((W.shape&#91;0] * W.shape&#91;1], Z.shape&#91;0]))\n    \n    for k in range(0, Z.shape&#91;0]):\n        for i in range(0, W.shape&#91;1]):\n            for j in range(0, W.shape&#91;0]):\n                if i == k:\n                    dir_matrix&#91;(i*W.shape&#91;0]) + j]&#91;k] = x&#91;j]\n    \n    return dir_matrix\n</pre></div>\n\n\n<p>If we use the example above, then the derivative matrix will look like this:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nDZ = z_dir(Z, W, x)\nprint(DZ)\n\n&#91;&#91;0.1 0.  0.  0.  0. ]\n &#91;0.5 0.  0.  0.  0. ]\n &#91;0.4 0.  0.  0.  0. ]\n &#91;0.  0.1 0.  0.  0. ]\n &#91;0.  0.5 0.  0.  0. ]\n &#91;0.  0.4 0.  0.  0. ]\n &#91;0.  0.  0.1 0.  0. ]\n &#91;0.  0.  0.5 0.  0. ]\n &#91;0.  0.  0.4 0.  0. ]\n &#91;0.  0.  0.  0.1 0. ]\n &#91;0.  0.  0.  0.5 0. ]\n &#91;0.  0.  0.  0.4 0. ]\n &#91;0.  0.  0.  0.  0.1]\n &#91;0.  0.  0.  0.  0.5]\n &#91;0.  0.  0.  0.  0.4]]\n</pre></div>\n\n\n<p>Going back to the formula for the derivative of the Softmax Layer:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BW%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot+%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7BW%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BW%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot+%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7BW%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BW%7D%7D+%3D+%5Cfrac%7Bd%5Ctextbf%7BS%7D%7D%7Bd%5Ctextbf%7BZ%7D%7D+%5Ccdot+%5Cfrac%7Bd%5Ctextbf%7BZ%7D%7D%7Bd%5Ctextbf%7BW%7D%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{d&#92;textbf{S}}{d&#92;textbf{W}} = &#92;frac{d&#92;textbf{S}}{d&#92;textbf{Z}} &#92;cdot &#92;frac{d&#92;textbf{Z}}{d&#92;textbf{W}}\" class=\"latex\" /></p>\n\n\n\n<p>We now just take the dot product of both of the derivative matrices to get the derivative for the whole layer:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nDL = np.dot(DS, np.transpose(DZ))\nprint(DL)\n\n&#91;&#91; 0.01544  0.07723  0.06178 -0.00388 -0.01942 -0.01554\n  -0.00414 -0.02071 -0.01657 -0.00441 -0.02208 -0.01766\n  -0.00300 -0.01501 -0.01200]\n &#91;-0.00388 -0.01942 -0.01554  0.01621  0.0810   0.06484\n  -0.00441 -0.02208 -0.01766 -0.00470 -0.02354 -0.01883\n  -0.00320 -0.01600 -0.01280]\n &#91;-0.00414 -0.02071 -0.01657 -0.00441 -0.02208 -0.01766\n   0.01699  0.08495  0.06796 -0.00501 -0.02509 -0.02007\n  -0.00341 -0.01706 -0.01364]\n &#91;-0.00441 -0.02208 -0.01766 -0.00470 -0.02354 -0.01883\n  -0.00501 -0.02509 -0.02007  0.01778  0.08890  0.07112\n  -0.00363 -0.01818 -0.01455]\n &#91;-0.00300 -0.01501 -0.01200 -0.00320 -0.01600 -0.01280\n  -0.00341 -0.01706 -0.01364 -0.00363 -0.01818 -0.01455\n   0.01325  0.06626  0.05300]]\n</pre></div>\n\n\n<h2>Shortcut!</h2>\n\n\n\n<p>While it is instructive to see the matrices being derived explicitly, it is possible to manipulate the formulas to make it easier. Starting with one of the entries in the matrix DL, it looks like this:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+%5Csum_%7Bk%3D1%7D%5EK+D_%7Bk%7DS_%7Bt%7D+%5Ccdot+D_%7Bij%7DZ_%7Bk%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+%5Csum_%7Bk%3D1%7D%5EK+D_%7Bk%7DS_%7Bt%7D+%5Ccdot+D_%7Bij%7DZ_%7Bk%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+%5Csum_%7Bk%3D1%7D%5EK+D_%7Bk%7DS_%7Bt%7D+%5Ccdot+D_%7Bij%7DZ_%7Bk%7D&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{s_{t}}}{&#92;delta{w_{ij}}} = &#92;sum_{k=1}^K D_{k}S_{t} &#92;cdot D_{ij}Z_{k}\" class=\"latex\" /></p>\n\n\n\n<p>Since the matrix dZ/dW is mostly zeros, then we can try to simplify it. dZ/dW is non-zero when <em>i</em> = <em>k</em>, and then it is equal to <em>x<sub>j</sub></em> as we worked out above. So we can simplify the non-zero entries to:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+D_%7Bi%7DS_%7Bt%7Dx_%7Bj%7D++&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+D_%7Bi%7DS_%7Bt%7Dx_%7Bj%7D++&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+D_%7Bi%7DS_%7Bt%7Dx_%7Bj%7D++&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{s_{t}}}{&#92;delta{w_{ij}}} = D_{i}S_{t}x_{j}  \" class=\"latex\" /></p>\n\n\n\n<p>In the previous post, we established that when the indices are the same, then:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=D_%7Bi%7DS_%7Bj%7D+%3D+S_%7Bj%7D%281-S_%7Bi%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=D_%7Bi%7DS_%7Bj%7D+%3D+S_%7Bj%7D%281-S_%7Bi%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_%7Bi%7DS_%7Bj%7D+%3D+S_%7Bj%7D%281-S_%7Bi%7D%29&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"D_{i}S_{j} = S_{j}(1-S_{i})\" class=\"latex\" /></p>\n\n\n\n<p>So:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%281-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{s_{t}}}{&#92;delta{w_{ij}}} = S_{t}(1-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>When the indices are not the same, we use:</p>\n\n\n\n<p><img src=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cdelta%7Bs_%7Bt%7D%7D%7D%7B%5Cdelta%7Bw_%7Bij%7D%7D%7D+%3D+S_%7Bt%7D%280-S_%7Bi%7D%29x_%7Bj%7D+&#038;bg=ffffff&#038;fg=5e5e5e&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;frac{&#92;delta{s_{t}}}{&#92;delta{w_{ij}}} = S_{t}(0-S_{i})x_{j} \" class=\"latex\" /></p>\n\n\n\n<p>What these two formulas show is that it is possible to calculate each of the entries in the derivative matrix by using only the input values X and the Softmax output S, skipping the matrix dot product altogether.</p>\n\n\n\n<p>Here is the Python code corresponding to that:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ndef l_dir_shortcut(W, S, x):\n    dir_matrix = np.zeros((W.shape&#91;0] * W.shape&#91;1], W.shape&#91;1]))\n    \n    for t in range(0, W.shape&#91;1]):\n        for i in range(0, W.shape&#91;1]):\n            for j in range(0, W.shape&#91;0]):\n                dir_matrix&#91;(i*W.shape&#91;0]) + j]&#91;t] = S&#91;t] * ((i==t) - S&#91;i]) * x&#91;j]\n                \n    return dir_matrix\n\nDL_shortcut = np.transpose(l_dir_shortcut(W, h, x))\n</pre></div>\n\n\n<p>To verify that, we can cross check it with the matrix we derived from first principle:</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nprint(DL_shortcut)\n\n&#91;&#91; 0.01544  0.07723  0.06178 -0.00388 -0.01942 -0.01554\n  -0.00414 -0.02071 -0.01657 -0.00441 -0.02208 -0.01766\n  -0.00300 -0.01501 -0.01200]\n &#91;-0.00388 -0.01942 -0.01554  0.01621  0.08105  0.06484\n  -0.00441 -0.02208 -0.01766 -0.00470 -0.02354 -0.01883\n  -0.00320 -0.01600 -0.01280]\n &#91;-0.00414 -0.02071 -0.01657 -0.00441 -0.02208 -0.01766\n   0.01699  0.08495  0.06796 -0.00501 -0.02509 -0.02007\n  -0.00341 -0.01706 -0.01364]\n &#91;-0.00441 -0.02208 -0.01766 -0.00470 -0.02354 -0.01883\n  -0.00501 -0.02509 -0.02007  0.01778  0.08890  0.07112\n  -0.00363 -0.01818 -0.01455]\n &#91;-0.00300 -0.01501 -0.01200 -0.00320 -0.01600 -0.01280\n  -0.00341 -0.01706 -0.01364 -0.00363 -0.01818 -0.01455\n   0.01325  0.06626  0.05300]]\n</pre></div>\n\n\n<p>Lastly, it&#8217;s worth noting that in order to actually modify each of the weights, we need to sum up the individual adjustments in each of the corresponding columns.</p>\n",
  "wfw:commentRss": "https://aimatters.wordpress.com/2020/06/14/derivative-of-softmax-layer/feed/",
  "slash:comments": 1,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "A single neuron"
    },
    {
      "media:title": "stephenoman"
    },
    "",
    ""
  ]
}