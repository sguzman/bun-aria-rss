{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2018-07-08T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/07/08/dask-dev",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc</a></em></p>\n\n<p>To increase transparency I’m trying to blog more often about the current work\ngoing on around Dask and related projects.  Nothing here is ready for\nproduction.  This blogpost is written in haste, so refined polish should not be\nexpected.</p>\n\n<p>Current efforts for June 2018 in Dask and Dask-related projects include\nthe following:</p>\n\n<ol>\n  <li>Yarn Deployment</li>\n  <li>More examples for machine learning</li>\n  <li>Incremental machine learning</li>\n  <li>HPC Deployment configuration</li>\n</ol>\n\n<h3 id=\"yarn-deployment\">Yarn deployment</h3>\n\n<p>Dask developers often get asked <em>How do I deploy Dask on my Hadoop/Spark/Hive\ncluster?</em>.  We haven’t had a very good answer until recently.</p>\n\n<p>Most Hadoop/Spark/Hive clusters are actually <em>Yarn</em> clusters.  Yarn is the most\ncommon cluster manager used by many clusters that are typically used to run\nHadoop/Spark/Hive jobs including any cluster purchased from a vendor like\nCloudera or Hortonworks.  If your application can run on Yarn then it can be a\nfirst class citizen here.</p>\n\n<p>Unfortunately Yarn has really only been accessible through a Java API, and so\nhas been difficult for Dask to interact with.  That’s changing now with a few\nprojects, including:</p>\n\n<ul>\n  <li><a href=\"https://dask-yarn.readthedocs.io\">dask-yarn</a>: an easy way to launch Dask on\nYarn clusters</li>\n  <li><a href=\"https://jcrist.github.io/skein/\">skein</a>: an easy way to launch generic\nservices on Yarn clusters (this is primarily what backs dask-yarn)</li>\n  <li><a href=\"https://conda.github.io/conda-pack/\">conda-pack</a>: an easy way to bundle\ntogether a conda package into a redeployable environment, such as is useful\nwhen launching Python applications on Yarn</li>\n</ul>\n\n<p>This work is all being done by <a href=\"http://jcrist.github.io/\">Jim Crist</a> who is, I\nbelieve, currently writing up a blogpost about the topic at large.  Dask-yarn\nwas soft-released last week though, so people should give it a try and report\nfeedback on the <a href=\"https://github.com/dask/dask-yarn\">dask-yarn issue tracker</a>.\nIf you ever wanted direct help on your cluster, now is the right time because\nJim is working on this actively and is not yet drowned in user requests so\ngenerally has a fair bit of time to investigate particular cases.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask_yarn</span> <span class=\"kn\">import</span> <span class=\"n\">YarnCluster</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>\n\n<span class=\"c1\"># Create a cluster where each worker has two cores and eight GB of memory\n</span><span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">YarnCluster</span><span class=\"p\">(</span><span class=\"n\">environment</span><span class=\"o\">=</span><span class=\"s\">'environment.tar.gz'</span><span class=\"p\">,</span>\n                      <span class=\"n\">worker_vcores</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                      <span class=\"n\">worker_memory</span><span class=\"o\">=</span><span class=\"s\">\"8GB\"</span><span class=\"p\">)</span>\n<span class=\"c1\"># Scale out to ten such workers\n</span><span class=\"n\">cluster</span><span class=\"p\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Connect to the cluster\n</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h3 id=\"more-examples-for-machine-learning\">More examples for machine learning</h3>\n\n<p>Dask maintains a Binder of simple examples that show off various ways to use\nthe project.  This allows people to click a link on the web and quickly be\ntaken to a Jupyter notebook running on the cloud.  It’s a fun way to quickly\nexperience and learn about a new project.</p>\n\n<p>Previously we had a single example for arrays, dataframes, delayed, machine\nlearning, etc.</p>\n\n<p>Now <a href=\"https://stsievert.com/\">Scott Sievert</a> is expanding the examples within\nthe machine learning section.  He has submitted the following two so far:</p>\n\n<ol>\n  <li><a href=\"https://mybinder.org/v2/gh/dask/dask-examples/master?filepath=machine-learning%2Fincremental.ipynb\">Incremental training with Scikit-Learn and large datasets</a></li>\n  <li><a href=\"https://mybinder.org/v2/gh/dask/dask-examples/master?filepath=machine-learning%2Fxgboost.ipynb\">Dask and XGBoost</a></li>\n</ol>\n\n<p>I believe he’s planning on more.  If you use\n<a href=\"http://dask-ml.readthedocs.io/en/latest/\">dask-ml</a> and have recommendations or\nwant to help, you might want to engage in the <a href=\"https://github.com/dask/dask-ml/issues/new\">dask-ml issue\ntracker</a> or <a href=\"https://github.com/dask/dask-examples/issues/new\">dask-examples issue\ntracker</a>.</p>\n\n<h3 id=\"incremental-training\">Incremental training</h3>\n\n<p>The incremental training mentioned as an example above is also new-ish.  This\nis a Scikit-Learn style meta-estimator that wraps around other estimators that\nsupport the <code class=\"language-plaintext highlighter-rouge\">partial_fit</code> method.  It enables training on large datasets in an\nincremental or batchwise fashion.</p>\n\n<h4 id=\"before\">Before</h4>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">SGDClassifier</span>\n\n<span class=\"n\">sgd</span> <span class=\"o\">=</span> <span class=\"n\">SGDClassifier</span><span class=\"p\">(...)</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n\n<span class=\"k\">for</span> <span class=\"n\">filename</span> <span class=\"ow\">in</span> <span class=\"n\">filenames</span><span class=\"p\">:</span>\n    <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"p\">...</span>\n\n    <span class=\"n\">sgd</span><span class=\"p\">.</span><span class=\"n\">partial_fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h4 id=\"after\">After</h4>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">SGDClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask_ml.wrappers</span> <span class=\"kn\">import</span> <span class=\"n\">Incremental</span>\n\n<span class=\"n\">sgd</span> <span class=\"o\">=</span> <span class=\"n\">SGDClassifier</span><span class=\"p\">(...)</span>\n<span class=\"n\">inc</span> <span class=\"o\">=</span> <span class=\"n\">Incremental</span><span class=\"p\">(</span><span class=\"n\">sgd</span><span class=\"p\">)</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">dask.dataframe</span> <span class=\"k\">as</span> <span class=\"n\">dd</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"n\">filenames</span><span class=\"p\">)</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"p\">...</span>\n<span class=\"n\">inc</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h4 id=\"analysis\">Analysis</h4>\n\n<p>From a parallel computing perspective this is a very simple and un-sexy way of\ndoing things.  However my understanding is that it’s also quite pragmatic.  In\na distributed context we leave a lot of possible computation on the table (the\nsolution is inherently sequential) but it’s fun to see the model jump around\nthe cluster as it absorbs various chunks of data and then moves on.</p>\n\n<p><img src=\"https://user-images.githubusercontent.com/1320475/42237033-2bddf11e-7eec-11e8-88c5-5f0ebd2fb4df.png\" width=\"70%\" alt=\"Incremental training with Dask-ML\" /></p>\n\n<p>There’s ongoing work on how best to combine this with other work like pipelines\nand hyper-parameter searches to fill in the extra computation.</p>\n\n<p>This work was primarily done by <a href=\"https://tomaugspurger.github.io/\">Tom Augspurger</a>\nwith help from <a href=\"https://stsievert.com/\">Scott Sievert</a></p>\n\n<h3 id=\"dask-user-stories\">Dask User Stories</h3>\n\n<p>Dask developers are often asked “Who uses Dask?”.  This is a hard question to\nanswer because, even though we’re inundated with thousands of requests for\nhelp from various companies and research groups, it’s never fully clear who\nminds having their information shared with others.</p>\n\n<p>We’re now trying to crowdsource this information in a more explicit way by\nhaving users tell their own stories.  Hopefully this helps other users in their\nfield understand how Dask can help and when it might (or might not) be useful\nto them.</p>\n\n<p>We originally collected this information in a <a href=\"https://goo.gl/forms/JEebEFTOPrWa3P4h1\">Google\nForm</a> but have since then moved it to a\n<a href=\"https://github.com/mrocklin/dask-stories\">Github repository</a>.  Eventually\nwe’ll publish this as a <a href=\"https://github.com/mrocklin/dask-stories/issues/7\">proper web\nsite</a> and include it in our\ndocumentation.</p>\n\n<p>If you use Dask and want to share your story this is a great way to contribute\nto the project.  Arguably Dask needs more help with spreading the word than it\ndoes with technical solutions.</p>\n\n<h3 id=\"hpc-deployments\">HPC Deployments</h3>\n\n<p>The <a href=\"http://dask-jobqueue.readthedocs.io/en/latest/\">Dask Jobqueue</a> package for\ndeploying Dask on traditional HPC machines is nearing another release.  We’ve\nchanged around a lot of the parameters and configuration options in order to\nimprove the onboarding experience for new users.  It has been going very\nsmoothly in recent engagements with new groups, but will mean a breaking\nchange for existing users of the sub-project.</p>"
}