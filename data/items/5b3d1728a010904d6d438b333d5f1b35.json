{
  "id": "tag:blogger.com,1999:blog-4446292666398344382.post-2692193822377299085",
  "published": "2020-07-17T18:33:00.000-07:00",
  "updated": "2020-07-18T14:30:30.677-07:00",
  "category": "",
  "title": "Convex-concave games off the shelf",
  "content": "If you need to solve a convex optimization problem nowadays, you are in great shape.  Any problem of the form $$<br />\\begin{alignat}{2}<br />&\\!\\inf_z & \\qquad & f(z) \\\\<br />& \\text{subject to} & & h(z) = 0 \\\\<br />& & & g(z) \\preceq 0<br />\\end{alignat}<br />$$ where $f$ and $g$ are convex and $h$ is affine can be attacked by several excellent freely available software packages: my current favorite is <a href=\"https://www.cvxpy.org/\">cvxpy</a>, which is a joy to use.  If you have a lot of variables and not a lot of constraints, you can instead solve a dual problem.  It ends up looking like $$<br />\\begin{alignat}{2}<br />&\\!\\sup_{x}  & \\qquad & L(x) \\\\<br />& \\text{subject to} & & \\tilde{h}_x(x) = 0 \\\\<br />& & & \\tilde{g}_x(x) \\preceq 0<br />\\end{alignat}<br />$$ where $L$ is concave, assuming that you get lucky and can analytically eliminate all the primal variables $z$ such that only the dual variables $x$ remain.  But what if you can't eliminate all the primal variables, but only most of them?  You might end up with a problem like $$<br />\\begin{alignat}{2}<br />&\\!\\sup_{x} \\inf_y & \\qquad & L(x, y) \\\\<br />& \\text{subject to} & & \\tilde{h}_x(x) = 0 \\\\<br />& & & \\tilde{g}_x(x) \\preceq 0 \\\\<br />& & & \\tilde{h}_y(y) = 0 \\\\<br />& & & \\tilde{g}_y(y) \\preceq 0<br />\\end{alignat}<br />$$ where $\\tilde{g}_x$ and $\\tilde{g}_y$ are convex, and $\\tilde{h}_x$ and $\\tilde{h}_y$ are affine, $L(x, \\cdot)$ is convex in $y$ given fixed $x$, and $L(\\cdot, y)$ is concave in $x$ given fixed $y$.  It feels like this problem should be easier to solve than the original problem if many primal variables have been analytically eliminated.  Unfortunately, <i>none of my favorite convex optimization toolkits will accept a problem of this form.</i>  This is despite <a href=\"https://web.stanford.edu/class/ee392o/cvxccv.pdf\">the viability of interior-point methods for such problems</a>.  Bummer.<br /><br />One thing I tried was to solve the inner infimum using a standard toolkit, compute the gradient of the solution wrt the outer parameters via the sensitivity map, and then use a first-order method for the outer supremum.  This did not work for me: it works for toy problems but on real problems the outer supremum has very slow convergence suggesting ill-conditioning.  <br /><br />What I need is the power of interior-point methods to handle ill-conditioning via second-order information.  I'm able to achieve this via sequential quadratic minimax programming: first, locally approximate the objective $L(\\lambda, \\mu, y)$ with a quadratic around the current point and linearize the constraints. $$<br />\\begin{alignat}{2}<br />&\\!\\sup_{\\delta x} \\inf_{\\delta y} & \\qquad & \\frac{1}{2} \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\end{array}\\right)^\\top \\left(\\begin{array}{cc} P_{xx} & P_{yx}^\\top \\\\ P_{yx} & P_{yy} \\end{array}\\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\end{array} \\right) + \\left(\\begin{array}{c} q_x \\\\ q_y \\end{array} \\right)^\\top \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\end{array} \\right) \\\\<br />& \\text{subject to} & & \\left(\\begin{array}{cc} A_x & 0 \\\\ 0 & A_y \\end{array} \\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\end{array}\\right) = \\left(\\begin{array}{c} b_x \\\\ b_y \\end{array}\\right) \\\\<br />& & & \\left(\\begin{array}{cc} G_x & 0 \\\\ 0 & G_y \\end{array} \\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\end{array}\\right) \\preceq \\left(\\begin{array}{c} h_x \\\\ h_y \\end{array}\\right) \\\\<br />\\end{alignat}<br />$$ The <a href=\"https://en.wikipedia.org/wiki/Wolfe_duality\">Wolfe dual</a> converts this problem into a standard QP: $$<br />\\begin{alignat}{2}<br />&\\!\\sup_{\\delta x, \\delta y, \\lambda, \\mu} & \\qquad &  \\frac{1}{2} \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\\\ \\lambda \\\\ \\mu \\end{array}\\right)^\\top \\left(\\begin{array}{cccc} P_{xx} & 0 & 0 & 0 \\\\ 0 & -P_{yy} & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\\\ \\lambda \\\\ \\mu \\end{array}\\right) + \\left(\\begin{array}{c} q_x \\\\ 0 \\\\ -b_y \\\\ -h_y \\end{array} \\right)^\\top \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\\\ \\lambda \\\\ \\mu \\end{array} \\right) \\\\<br />& \\text{subject to} & & \\left(\\begin{array}{cc} A_x & 0 & 0 & 0 \\\\ P_{yx} & P_{yy} & A_y^\\top & G_y^\\top \\end{array} \\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\\\ \\lambda \\\\ \\mu \\end{array}\\right) = \\left(\\begin{array}{c} b_x \\\\ -q_y \\end{array}\\right) \\\\<br />& & & \\left(\\begin{array}{cc} G_x & 0 & 0 & 0  \\\\ 0 & 0 & 0 & -I \\end{array} \\right) \\left(\\begin{array}{c} \\delta x \\\\ \\delta y \\\\ \\lambda \\\\ \\mu \\end{array}\\right) \\preceq \\left(\\begin{array}{c} h_x \\\\ 0 \\end{array}\\right) \\\\<br />\\end{alignat}<br />$$ If you solve this for $(\\delta x, \\delta y)$ you get a Newton step for your original problem.  The step acceptance criterion here is tricky: if the iterate is feasible you want to leverage the saddle point condition (see equation (11) of <a href=\"https://arxiv.org/abs/1906.00233\">Essid et. al.</a>).  If the iterate is infeasible more sophistication is required, but fortunately my constraints were actually linear so doing an initial exact inner solve allowed me to iterate while staying feasible.  (Note: if you solve a more general convex problem on each step, you don't need to linearize the $x$ constraints.)<br /><br />YMMV!",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Paul Mineiro",
    "uri": "http://www.blogger.com/profile/05439062526157173163",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 0
}