{
  "title": "Revisiting Grammatical Error Correction Evaluation and Beyond. (arXiv:2211.01635v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01635",
  "description": "<p>Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore\nand BARTScore) have been widely used in several sentence generation tasks\n(e.g., machine translation and text summarization) due to their better\ncorrelation with human judgments over traditional overlap-based methods.\nAlthough PT-based methods have become the de facto standard for training\ngrammatical error correction (GEC) systems, GEC evaluation still does not\nbenefit from pretrained knowledge. This paper takes the first step towards\nunderstanding and improving GEC evaluation with pretraining. We first find that\narbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory\ncorrelation results because of the excessive attention to inessential systems\noutputs (e.g., unchanged parts). To alleviate the limitation, we propose a\nnovel GEC evaluation metric to achieve the best of both worlds, namely PT-M2\nwhich only uses PT-based metrics to score those corrected parts. Experimental\nresults on the CoNLL14 evaluation task show that PT-M2 significantly\noutperforms existing methods, achieving a new state-of-the-art result of 0.949\nPearson correlation. Further analysis reveals that PT-M2 is robust to evaluate\ncompetitive GEC systems. Source code and scripts are freely available at\nhttps://github.com/pygongnlp/PT-M2.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1\">Peiyuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"
}