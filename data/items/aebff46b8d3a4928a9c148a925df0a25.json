{
  "title": "Becoming a Bayesian, Part 3",
  "link": "",
  "published": "2015-05-15T21:00:00+01:00",
  "updated": "2015-05-15T21:00:00+01:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2015-05-15:/sebastian/blog/becoming-a-bayesian-part-3.html",
  "summary": "<p>This post continues the previous post, <a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">part 1</a> and\n<a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-2.html\">part 2</a>,\noutlining my criticism towards a ''naive'' subjective Bayesian viewpoint:</p>\n<ol>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">The consequences of model misspecification</a>.</li>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-2.html\">The ''model first computation last'' approach</a>.</li>\n<li>Denial of methods of classical statistics, in this post â€¦</li></ol>",
  "content": "<p>This post continues the previous post, <a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">part 1</a> and\n<a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-2.html\">part 2</a>,\noutlining my criticism towards a ''naive'' subjective Bayesian viewpoint:</p>\n<ol>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-1.html\">The consequences of model misspecification</a>.</li>\n<li><a href=\"http://www.nowozin.net/sebastian/blog/becoming-a-bayesian-part-2.html\">The ''model first computation last'' approach</a>.</li>\n<li>Denial of methods of classical statistics, in this post.</li>\n</ol>\n<h2>Denial of the Value of Classical Statistics</h2>\n<p>Suppose for the sake of a simple example that our task is to estimate the\nunknown mean <span class=\"math\">\\(\\mu\\)</span> of an unknown probability distribution <span class=\"math\">\\(P\\)</span> with bounded\nsupport over the real line.\nTo this end we receive a sequence of <span class=\"math\">\\(n\\)</span> iid samples\n<span class=\"math\">\\(X_1\\)</span>, <span class=\"math\">\\(X_2\\)</span>, <span class=\"math\">\\(\\dots\\)</span>, <span class=\"math\">\\(X_n\\)</span>.</p>\n<p>Now suppose that <em>after</em> receiving these <span class=\"math\">\\(n\\)</span> samples I do not use the obvious\nsample mean estimator but I take only the first sample <span class=\"math\">\\(X_1\\)</span> and estimate\n<span class=\"math\">\\(\\hat{\\mu} = X_1\\)</span>.\nIs this a good estimator?\nIntuition tells us that it is not, because it ignores part of the useful input\ndata, namely <span class=\"math\">\\(X_i\\)</span> for any <span class=\"math\">\\(i &gt; 1\\)</span>, but how can we formally analyze this?</p>\n<p>From a subjective Bayesian viewpoint the <a href=\"http://projecteuclid.org/download/pdf_1/euclid.lnms/1215466214\">likelihood\nprinciple</a>\ndoes not permit us to ignore evidence which is already available.\nIf we posit a model <span class=\"math\">\\(P(X_i|\\theta)\\)</span> and a prior <span class=\"math\">\\(P(\\theta)\\)</span> we have to work\nwith the posterior\n    </p>\n<div class=\"math\">$$P(\\theta | X_1,\\dots,X_n) \\propto\n        \\prod_{i=1,\\dots,n} P(X_i|\\theta) P(\\theta).$$</div>\n<p>\nTherefore our estimator <span class=\"math\">\\(\\hat{\\mu}=X_1\\)</span> cannot correspond to a Bayesian\nposterior mean of any non-trivial model for all parameters.\nThis is of course a very strict viewpoint and one may object that we\n<em>can</em> talk about properties of the sequence of posteriors <span class=\"math\">\\(P(\\theta | X_1)\\)</span>,\n<span class=\"math\">\\(P(\\theta | X_1, X_2)\\)</span>, etc.\nBut even in this generous view, <em>after</em> observing all samples we are not\npermitted to ignore part of them.  (If you are still not convinced, consider\nthe estimator defined by <span class=\"math\">\\(\\hat{\\mu} = X_2\\)</span> if <span class=\"math\">\\(X_1 &gt; 0\\)</span>, and <span class=\"math\">\\(\\hat{\\mu} = X_3\\)</span>\notherwise.)\nSo Bayesian statistics does not offer us a method to analyze our proposed\nestimator.</p>\n<p>A classical statistician <em>can</em> analyze pretty much arbitrary procedures,\nincluding ones of the silly type <span class=\"math\">\\(\\hat{\\mu}\\)</span> that we proposed.\nThe analysis may be technically difficult or apply only in the asymptotic\nregime but does not rule out any estimator apriori.\nTypical results may take the form of a derivation of the variance or bias of\nthe estimator.\nIn our case we have an\n<a href=\"http://en.wikipedia.org/wiki/Bias_of_an_estimator\"><em>unbiased</em></a>\nestimate of the mean, <span class=\"math\">\\(\\mathbb{E}[\\hat{\\mu}]-\\mu = 0\\)</span>.\nAs for the variance, because we only take the first sample, even as <span class=\"math\">\\(n \\to\n\\infty\\)</span> the variance <span class=\"math\">\\(\\mathbb{V}[\\hat{\\mu}]\\)</span> remains constant, so the\nestimator is\n<a href=\"http://en.wikipedia.org/wiki/Consistent_estimator\"><em>inconsistent</em></a>, a clear\nindication that our <span class=\"math\">\\(\\hat{\\mu}\\)</span> is a bad estimator.</p>\n<p>Another typical result is in the form of a <a href=\"http://en.wikipedia.org/wiki/Confidence_interval\">confidence\ninterval</a> of a parameter of\ninterest.\nOne can argue that confidence intervals are <a href=\"http://en.wikipedia.org/wiki/Credible_interval#Confidence_interval\">not exactly answering the\nquestion of\ninterest</a>\n(that is, whether the parameter really is in the given interval), but if they\nare of interest, one can sometimes <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/rssb.12080/abstract\">obtain them also from a Bayesian\nanalysis</a>.</p>\n<p>There exist cases where existing statistical procedures can be reinterpreted\nfrom a Bayesian viewpoint.  This is achieved by proposing a model and prior\nsuch that inferences under this model and prior exactly or approximately match\nthe answers of the existing procedure or at least have satisfying frequentist\nproperties.\nTwo cases of this are the following:</p>\n<ul>\n<li><a href=\"http://utstat.toronto.edu/pub/reid/research/vaneeden.pdf\">Matching priors</a>,\nwhere in some cases it is possible to establish an exact equivalence for\nsimple parametric models without latent variables.\nOne recent example for even a non-parametric model is the <a href=\"http://arxiv.org/abs/1401.0303\">Good-Turing estimator\nfor the missing mass</a>, where an asymptotic\nequivalence between the classic <a href=\"http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\">Good-Turing\nestimator</a>\nand a Bayesian non-parametric model is established.</li>\n<li><a href=\"http://projecteuclid.org/euclid.aos/1236693154\">Reference priors</a>, a\ngeneralization of the Jeffrey prior, in which the prior is constructed to be\nleast informative.  Here least informative is in the sense that when you\nsample from the prior and consider the resulting posterior using the sample,\nthe divergence to the original prior should be large in expectation; that is,\nsamples from the prior should be able to change your beliefs to the maximum\npossible extend.  When it is possible to derive reference priors, these\ntypically have excellent frequentist robustness properties, and are useful\ndefault prior choices.\nUnfortunately, in models with multiple parameters there is no unique reference\nprior, and generally the set of <a href=\"http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf\">known reference\npriors</a>\nseems to be quite small.\nThis problematic case-by-case state is nicely summarized in this recent work\non <a href=\"http://projecteuclid.org/euclid.ba/1422556416\">overall objective priors</a>.</li>\n</ul>\n<p>Should we care at all about these classic notions of qualities of an\nestimators?\nI have seen <em>Bayesians</em> dismiss properties such as <em>unbiasedness</em>\nand <em>consistency</em> as unimportant, but I cannot understand this stance.\nFor example, an unbiased estimator operating on iid sampled data immediately\nimplies a scalable parallel estimator applicable to the big data setting,\nsimply by separately estimating the quantity of interest, then taking the\naverage of estimates.  This is a practical and useful consequence of the\nunbiasedness property.  Similarly, <em>consistency</em> is at least a guarantee that\nwhen more data is available the qualities of your inferences are improving,\nand this should be of interest to anyone whose goal it is to build systems\nwhich can learn.  (There do exist some results on Bayesian posterior\nconsistency, for a summary see Chapter 20 of\n<a href=\"http://www.stat.purdue.edu/~dasgupta/\">DasGupta's</a>\n<a href=\"http://www.springer.com/mathematics/probability/book/978-0-387-75970-8\">book</a>.)</p>\n<p>Let me summarize.\nBayesian estimators are often superior to alternatives.\nBut the set of procedures yielding Bayesian estimates is strictly smaller than\nthe set of all statistical procedures.\nWe need methods to analyze the larger set, in particular to characterize the\nsubset of useful estimators, where <em>useful</em> is application dependent.</p>\n<p><em>Acknowledgements</em>.  I thank <a href=\"http://www.jancsary.net/\">Jeremy Jancsary</a>,\n<a href=\"http://files.is.tue.mpg.de/pgehler/\">Peter Gehler</a>,\n<a href=\"http://pub.ist.ac.at/~chl/\">Christoph Lampert</a>, and\n<a href=\"http://www.ong-home.my/\">Cheng Soon-Ong</a> for feedback.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}