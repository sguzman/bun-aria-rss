{
  "title": "Animating MCMC with PyMC3 and Matplotlib",
  "link": "",
  "published": "2014-01-02T09:00:00-05:00",
  "updated": "2014-01-02T09:00:00-05:00",
  "author": {
    "name": "Thomas Wiecki"
  },
  "id": "tag:twiecki.io,2014-01-02:/blog/2014/01/02/visualizing-mcmc/",
  "summary": "<p>Here's the deal: I used <a href=\"https://github.com/pymc-devs/pymc\">PyMC3</a>,\n<a href=\"http://matplotlib.org/\">matplotlib</a>, and <a href=\"http://jakevdp.github.io/\">Jake Vanderplas'</a>\n<a href=\"https://github.com/jakevdp/JSAnimation\">JSAnimation</a> to create\njavascript animations of three MCMC sampling algorithms --\n<a href=\"https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\">Metropolis-Hastings</a>, <a href=\"https://en.wikipedia.org/wiki/Slice_sampling\">slice sampling</a> and <a href=\"http://arxiv.org/abs/1111.4246\">NUTS</a>.</p>\n<p>I like visualizations because they provide a good intuition for how\nthe samplers work and what problems they can run into.</p>\n<p>You can download the â€¦</p>",
  "content": "<p>Here's the deal: I used <a href=\"https://github.com/pymc-devs/pymc\">PyMC3</a>,\n<a href=\"http://matplotlib.org/\">matplotlib</a>, and <a href=\"http://jakevdp.github.io/\">Jake Vanderplas'</a>\n<a href=\"https://github.com/jakevdp/JSAnimation\">JSAnimation</a> to create\njavascript animations of three MCMC sampling algorithms --\n<a href=\"https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\">Metropolis-Hastings</a>, <a href=\"https://en.wikipedia.org/wiki/Slice_sampling\">slice sampling</a> and <a href=\"http://arxiv.org/abs/1111.4246\">NUTS</a>.</p>\n<p>I like visualizations because they provide a good intuition for how\nthe samplers work and what problems they can run into.</p>\n<p>You can download the full notebook <a href=\"https://rawgithub.com/twiecki/WhileMyMCMCGentlySamples/master/content/downloads/notebooks/sample_animation.ipynb\">here</a> or <a href=\"http://nbviewer.ipython.org/github/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/sample_animation.ipynb?create=1\">view it in your browser</a>. Note that for this post I used\nvideo embedding due to the size of the animations if they are not\ncompressed. The notebook contains code for both.</p>\n<p>The model is a simple linear model as explained in my <a href=\"https://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/\">previous blog post on Bayesian GLMs</a>. Essentially,\nI generated some data and estimate <code>intercept</code> and <code>slope</code>. In the\nlower left corner is the <em>joint</em> posterior while the plot above shows\nthe <em>trace</em> of the <em>marginal</em> posterior of the <code>intercept</code> while the\nright plot shows the trace of the <em>marginal</em> posterior of the <code>slope</code> parameter. Each\npoint represents a sample drawn from the posterior. At 3 quarters of the way I added a thousand samples to show that they all sample from the posterior eventually.</p>\n<h2>Metropolis-Hastings</h2>\n<p>First, lets see how our old-school Metropolis-Hastings (MH)\nperforms. The code uses matplotlib's handy <code>FuncAnimation</code> (see\n<a href=\"http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\">here</a>\nfor a tutorial), my own animation code, and the <a href=\"https://github.com/pymc-devs/pymc/pull/433\">recently merged iterative sampling function <code>iter_sample()</code></a>.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/23zAmLruZVA\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<p>As you can see, there is quite some correlation between <code>intercept</code>\nand <code>slope</code> -- if we believe in a higher intercept we must also\nbelieve in a lower slope (which makes geometrical sense if you think\nhow lines could fit through the point clouds). This often makes it\ndifficult for the MCMC algorithm to converge (i.e. sample from the\ntrue posterior) as we wittness here.</p>\n<p>The reason MH does not do anything at first is that MH proposes huge\njumps that are not accepted because they are way outside the\nposterior. PyMC then tunes the proposal distribution so that smaller\njumps are proposed. These smaller jumps however lead to the\nrandom-walk behavior you can see which makes sampling inefficient (for\na good intuition about this \"drunken walk\", see\n<a href=\"http://healthyalgorithms.com/2010/03/12/a-useful-metaphor-for-explaining-mcmc/\">here</a>).</p>\n<h2>Slice sampling</h2>\n<p>Lets see how Slice sampling fares.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5mMwKG7rbZQ\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<p>As you can see, slice sampling does a much better job. For one thing,\nthere are no rejections (which is a property of the algorithm). But\nthere's still room for improvement. At the core, slice sampling always\nupdates one random variable at a time while keeping all others\nconstant. This property leads to small steps being taken (imagine\ntrying to move along a diagonal area on the chess board with a Rook)\nand makes sampling from correlated posteriors inefficient.</p>\n<h2>NUTS (Hamiltonian Monte Carlo)</h2>\n<p>NUTS on the other hand is a newer gradient-based sampler that operates\non the joint posterior. Correlations are not a problem because this\nsampler can actually move diagonally as well (more like the Queen). As\nyou can see, it does a much better job at exploring the posterior and\ntakes much wider steps.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Fq_hlq8AfYo\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\n<p>Mesmerizing, ain't it?</p>\n<p>What surprised me about the slice sampling is that if I looked at the\nindividual traces (top and right plot) only, I'd say they hadn't\nconverged. But rather it seems that while the step-size is small,\naveraging samples over a longer run should still provide meaningful\ninference.</p>\n<h2>Where to go from here</h2>\n<p>I was initially setting out to get real-time plotting while sampling\ninto PyMC. What I've shown here just creates an animation after\nsampling has finished. Unfortunately, I don't think it's currently\npossible to do so in the IPython Notebook as it requires embedding of\nHTML for which we need the finished product. If anyone has an idea\nhere that might be a very interesting extension.</p>\n<h2>Further reading</h2>\n<ul>\n<li><a href=\"http://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\">Jake's tutorial on matplotlib animations</a></li>\n<li><a href=\"http://jakevdp.github.io/blog/2013/05/19/a-javascript-viewer-for-matplotlib-animations/\">Jake's blog post on embedding JS animations in the notebook</a></li>\n<li><a href=\"http://healthyalgorithms.com/2011/01/28/mcmc-in-python-pymc-step-methods-and-their-pitfalls/\">Abe Flaxman's much prettier videos on MCMC</a>\n  (Would be nice to replace my crappy plotting code with his -- PRs welcome.)</li>\n</ul>",
  "category": ""
}