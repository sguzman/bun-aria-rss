{
  "title": "Java Art Generation with Neural Style Transfer",
  "link": "http://ramok.tech/2018/02/24/java-art-generation-with-neural-style-transfer/",
  "dc:creator": "Klevis Ramo",
  "pubDate": "Sat, 24 Feb 2018 15:12:18 +0000",
  "category": [
    "Convolutional Neural Network",
    "Machine Learning",
    "Neural Networks",
    "convolution neural network",
    "deep learning",
    "deeplearning4j",
    "java deep learning",
    "java machine learning",
    "java neural network",
    "java style transfer",
    "neural network generate art",
    "neural networks",
    "style transfer"
  ],
  "guid": "http://ramok.tech/?p=1809",
  "description": "In this post we are going to build a deep learning Java Application using deeplearning4j for the purpose of generating art. Beside being an attractive and fascinating topic neural style transfer give great insight in what deep convolution layers are learning. Feel free to run the application and try with your own images. What Is &#8230; <a href=\"http://ramok.tech/2018/02/24/java-art-generation-with-neural-style-transfer/\" class=\"more-link\">Continue reading<span class=\"screen-reader-text\"> \"Java Art Generation with Neural Style Transfer\"</span></a>",
  "content:encoded": "<p>In this post we are going to build a deep learning Java Application using <a href=\"https://deeplearning4j.org/\">deeplearning4j </a>for the purpose of generating art. Beside being an attractive and fascinating topic neural style transfer give great insight in what deep convolution layers are learning. Feel free to run the <a href=\"https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/styletransfer/NeuralStyleTransfer.java\">application</a> and try with your own images.</p>\n<h2>What Is Neural Style Transfer?</h2>\n<p>Neural Style transfer is the process of creating a new image by mixing two images together. Lets suppose we have this two images below:<br />\n<img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1862 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=292%2C219\" alt=\"\" width=\"292\" height=\"219\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?w=800 800w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=300%2C225 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=768%2C576 768w\" sizes=\"(max-width: 292px) 85vw, 292px\" data-recalc-dims=\"1\" /><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1864 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?resize=277%2C220\" alt=\"\" width=\"277\" height=\"220\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?w=640 640w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?resize=300%2C238 300w\" sizes=\"(max-width: 277px) 85vw, 277px\" data-recalc-dims=\"1\" /></p>\n<p>and the generated art image will look like below:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1868\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?resize=400%2C300\" alt=\"\" width=\"400\" height=\"300\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?w=400 400w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 400px) 85vw, 400px\" data-recalc-dims=\"1\" /></p>\n<p>and since we like the art on the right image would like to transfer that style into our own memory photos. Of course we would prefer to save the photos content as much as possible and in same time transform them according to the art image style. This may look like :</p>\n<p>We need to find a way to capture content and style image features so we can mix them together in way that the output will look satisfactory for the eye.</p>\n<p>Deep Convolution Neural Networks like VGG-16 are already in a way capturing this features looking at the fact that they are able to classify/recognize a large variety of images(millions) with quite a high accuracy. We just need to look deeper on neural layers and understand or visualize what they are doing.</p>\n<h2>What are Convolution Networks learning?</h2>\n<p>Already a great paper offers the insight : <a href=\"https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">Zeiler and Fergus, 2013 Visualizing and Understanding Convolutional Networks</a>. They have developed quite a sophisticated way to visualize internal layersby using Deconvolutional Networks and other specific methods. In here we will focus only on the high level intuition of what neural layers are doing.</p>\n<p>Lets first bring into the focus VGG-16 architecture we saw in <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/\">Cat Image Recognition Application</a> :</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1589\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?resize=840%2C469\" alt=\"\" width=\"840\" height=\"469\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?w=1265 1265w, https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?resize=300%2C167 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?resize=768%2C429 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?resize=1024%2C571 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h04_38.jpg?resize=1200%2C670 1200w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px\" data-recalc-dims=\"1\" /></p>\n<p>While training with images lets suppose we pick the first layer and start monitoring some of his units/neurons(9 to 12 usually) activation values. From all activation&#8217;s values lets pick 9 maximum values per each of the chosen units(9-12). For all of this 9 values we will visualize the patch of the images that cause those activation to maximize. In few words the part of image the is making those neurons fire bigger values.</p>\n<p>Since we are just in the first layer the units capture only small part of the images and rather low level features as below:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1817 zoooom alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h41_06.jpg?resize=217%2C216\" alt=\"\" width=\"217\" height=\"216\" data-recalc-dims=\"1\" /><br />\nIt looks like the 1st neuron is interested in diagonal lines while the 3rd and 4th in vertical and diagonal lines and the 8th for sure likes green color. Is noticeable that all this are really small part of images and the layer is rather capturing low level features.</p>\n<p>&nbsp;</p>\n<p>Lets move a bit deeper and choose the 2 layer:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1819 zoooom aligncenter\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h46_51.jpg?resize=407%2C406\" alt=\"\" width=\"407\" height=\"406\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h46_51.jpg?w=233 233w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h46_51.jpg?resize=150%2C150 150w\" sizes=\"(max-width: 407px) 85vw, 407px\" data-recalc-dims=\"1\" /></p>\n<p>This layer neurons start to detect some more features like the second detects thin vertical lines , the 6th and 7th start capturing round shapes and 14th is obsessed with yellow color.</p>\n<p>Deeper into 3rd layer:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-1820 zoooom\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h55_55.jpg?resize=479%2C360\" alt=\"\" width=\"479\" height=\"360\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h55_55.jpg?w=304 304w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h55_55.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 479px) 85vw, 479px\" data-recalc-dims=\"1\" /></p>\n<p>Well this layer for sure starts to detect more interesting stuff like 6th is more activated for round shapes that look like tires, 10th is not easy to explain but likes orange and round shapes while the 11th start even detecting some humans.</p>\n<p>Even Deeper&#8230; into layer 4 and 5:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\" wp-image-1821 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_23.jpg?resize=200%2C496\" alt=\"\" width=\"200\" height=\"496\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_23.jpg?w=150 150w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_23.jpg?resize=121%2C300 121w\" sizes=\"(max-width: 200px) 85vw, 200px\" data-recalc-dims=\"1\" /> <img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1822 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_14.jpg?resize=204%2C508\" alt=\"\" width=\"204\" height=\"508\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_14.jpg?w=153 153w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_21h59_14.jpg?resize=121%2C300 121w\" sizes=\"(max-width: 204px) 85vw, 204px\" data-recalc-dims=\"1\" /></p>\n<p>So deeper we go bigger part of the image neurons are detecting therefore capturing high level features(second neuron on 5 layer is really into dogs) of the image in comparison to low level layers capturing rather small part of image.</p>\n<p>This gives a great intuition on what deep convolutional layers are learning and also coming back to our style transfer we have the insight how to generate art and keep the content from two images. We just need to generate a new image which when feed to neural networks as input generates more or less same activation values as the content(photo) and style(art painting) image.</p>\n<h2>Transfer Learning</h2>\n<p>One great thing about deep learning in general is the fact that is highly portable between application and even different programming languages and frameworks. The reason is simply because what a deep learning algorithm produces is just weights which are simply decimal values and they can be easily transported and imported on different environments.</p>\n<p>Anyway for our case we are going to use VGG-16 architecture pre trained with IMAGENET. Usually VGG-19 is used but unfortunately it results too slow on CPU , maybe on GPU it will be better. Below java code:</p>\n<pre>  private ComputationGraph loadModel() throws IOException {\n        ZooModel zooModel = new VGG16();\n        ComputationGraph vgg16 = (ComputationGraph) zooModel.initPretrained(PretrainedType.IMAGENET);\n        vgg16.initGradientsView();\n        log.info(vgg16.summary());\n        return vgg16;\n    }</pre>\n<h2>Load Images</h2>\n<p>At the beginning we have only <em>content image</em> and <em>styled image</em> so the <em>combined image</em> is rather a noisy image. Loading images is fairly easy task :</p>\n<pre>private static final DataNormalization IMAGE_PRE_PROCESSOR = new VGG16ImagePreProcessor();\nprivate static final NativeImageLoader LOADER = new NativeImageLoader(HEIGHT, WIDTH, CHANNELS);</pre>\n<pre>INDArray content = loadImage(CONTENT_FILE);\n\nINDArray style = loadImage(STYLE_FILE);</pre>\n<pre>private INDArray loadImage(String contentFile) throws IOException {\n    INDArray content = LOADER.asMatrix(new ClassPathResource(contentFile).getFile());\n    IMAGE_PRE_PROCESSOR.transform(content);\n    return content;\n}</pre>\n<p>Please note that after loading the pixels we are normalizing(IMAGE_PRE_PROCESSOR) the pixels with the mean values from all images used during training of VGG-16 with ImageNet dataset. <a href=\"http://ramok.tech/2017/08/26/anomaly-detection-using-octave/#Data_Preparation\">Normalization</a> helps to speed up training and is something it is more or less always done.</p>\n<p>Now is time to generate a noisy image :</p>\n<pre>private INDArray createCombinationImage() throws IOException {\n    INDArray content = LOADER.asMatrix(new ClassPathResource(CONTENT_FILE).getFile());\n    IMAGE_PRE_PROCESSOR.transform(content);\n    INDArray combination = createCombineImageWithRandomPixels();\n    combination.muli(NOISE_RATION).addi(content.muli(1.0 - NOISE_RATION));\n    return combination;\n}</pre>\n<p>As we can see from the code the combined image is not purely noisy but some part of it is taken from content(NOISE_RATION controls the percentage). The idea is taken from this <a href=\"http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style\">tensor flow implementation</a> and it is done for speeding up the training therefore getting good results faster. Anyway the algorithm eventually will produce more or less same results with pure noise images but it will just take longer and more iterations.</p>\n<h2>Content Image Cost Function</h2>\n<p>As we mention earlier we will use intermediate layers activation values produced by a neural network as a metric showing how similar two images are. First lets get those layer activation&#8217;s by doing forward pass for the content and combined image using VGG-16 pretrained model:</p>\n<pre>Map<String, INDArray> activationsContentMap = vgg16FineTune.feedForward(content, true);</pre>\n<pre>Map<String, INDArray> activationsCombMap = vgg16FineTune.feedForward(combination, true);</pre>\n<p>Now per each image we have a <em>map</em> with layer name as key and activation&#8217;s  values on that layer as value. We will choose a deep layer(<a href=\"http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style\"><strong>conv4_2</strong></a>) for our content image cost function because we want to capture as high level as possible features. The reason we choose a deep layer is because we would like the combine image or the generated image to retain the look and shape of content. In same time we choose only one layer because we don&#8217;t want the combine image to  look exactly like content but rather leave some space for the art.</p>\n<p>Once we have activation&#8217;s for chosen layer for both images content and combine is time to compare them together and see how similar they are. In order to measure their similarity we will use their squared difference divided by activation dimensions as described by this <a href=\"https://arxiv.org/pdf/1508.06576.pdf\">paper</a>:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1828\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_23h18_40.jpg?resize=317%2C71\" alt=\"\" width=\"317\" height=\"71\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_23h18_40.jpg?w=317 317w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-22_23h18_40.jpg?resize=300%2C67 300w\" sizes=\"(max-width: 317px) 85vw, 317px\" data-recalc-dims=\"1\" /></p>\n<p style=\"text-align: center;\"><em>F<sup>ij </sup>denotes the combine image layer activation values and P<sup>ij </sup>content image layer activation values. Basically is just the euclidian distance between two activation&#8217;s in particular layer.</em></p>\n<p>What we want is that ideally the difference to be zero. In few words minimize as much as possible the difference between images features on that layer. In this way way we transferred features captured by that layer from content image to combine image.</p>\n<p>The implementation in java of the cost function will look like below:</p>\n<pre>public double contentLoss(INDArray combActivations, INDArray contentActivations) {\n    return sumOfSquaredErrors(contentActivations, combActivations) / (4.0 * (CHANNELS) * (WIDTH) * (HEIGHT));\n}</pre>\n<pre>public double sumOfSquaredErrors(INDArray a, INDArray b) {\n    INDArray diff = a.sub(b); // difference\n    INDArray squares = Transforms.pow(diff, 2); // element-wise squaring\n    return squares.sumNumber().doubleValue();\n}</pre>\n<p>The only non essential <a href=\"http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style\">difference with the mathematical</a> formula from the paper is division of the activation dimension rather than with 2.</p>\n<h2>Style Image Cost Function</h2>\n<p>The approach for style image is quite similar with the content image in way that we will still use neural layers activation&#8217;s values difference as similarity measurement of images. Anyway there some difference with the cost function for style images in how the activation&#8217;s values are processed and calculated.</p>\n<p>Recalling from previous <a href=\"http://ramok.tech/2017/12/13/java-digit-recognizer-with-convolutional-neural-networks/\">convolution layers</a> post and <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/\">cat recognition application</a> a typical convolution operation will results in an output with several channels(3rd dimension) beside height and width(e.x 16 X 20 X 356 , w X h X c). Usually convolution shrinks width and height and increases channels.</p>\n<p>Style is defined as the correlation between each of units across channels in a specific chosen layer. E.x if we have a layer with shape 12X12X32 than if we pick up the 10th channel all 12X12=144 units of the 10th channel will be correlated with all 144 units of each of the other channels like 1,2,3,4,5,6,7,8,9, 11,12&#8230;.32.</p>\n<p>Mathematically this is called more specifically the <em>Gram Matrix(G)</em> and is calculated as the multiplication of the units values across channels in a layer.If values are almost the same than the Gram will output a big value in contrast when they values are completely different. So gram signals captures how related different channels are with each other(like correlation intuition).  From the paper it will look like <a href=\"https://arxiv.org/pdf/1508.06576.pdf\">below</a>:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1845\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_18h17_50.jpg?resize=137%2C57\" alt=\"\" width=\"137\" height=\"57\" data-recalc-dims=\"1\" /></p>\n<p style=\"text-align: center;\"><i>l the chosen layer, k is an index that iterates over channels in a layer, notice k<sub>*</sub> is not iterating because this is the channel we compare with all other channels, i and j are referring to the unit</i></p>\n<p>The implementation in java looks like below:</p>\n<pre>public double styleLoss(INDArray style, INDArray combination) {\n    INDArray s = gramMatrix(style);\n    INDArray c = gramMatrix(combination);\n    int[] shape = style.shape();\n    int N = shape[0];\n    int M = shape[1] * shape[2];\n    return sumOfSquaredErrors(s, c) / (4.0 * (N * N) * (M * M));\n}</pre>\n<pre>public INDArray gramMatrix(INDArray x) {\n    INDArray flattened = flatten(x);\n    INDArray gram = flattened.mmul(flattened.transpose());\n    return gram;\n}</pre>\n<p>Once we have the Gram Matrix we do the same as for the content calculate the Euclidean distance so the squared difference between Gram Matrices of the combine and style images activation&#8217;s values.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1843\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_18h13_55.jpg?resize=219%2C59\" alt=\"\" width=\"219\" height=\"59\" data-recalc-dims=\"1\" /></p>\n<p style=\"text-align: center;\"><i>G<sup>ij</sup><sub>l</sub> is denoting the combine gram values and A<sup>ij</sup><sub>l</sub> the style gram values on specific layer l.</i></p>\n<p>There is one last detail about the style cost function, usually choosing more than one layer gives better results. So for final style cost function we are going to choose 4 layers and add them together:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1841\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_18h10_44.jpg?resize=171%2C54\" alt=\"\" width=\"171\" height=\"54\" data-recalc-dims=\"1\" /></p>\n<p style=\"text-align: center;\"><em>E is just the equation above and w<sup>1 </sup>denotes to a weight per layer so we are controlling the impact or the contribution of each layer. We maybe want lower layer to contribute less than upper layer but still have them.</em></p>\n<p>Finally in java it looks like below:</p>\n<pre>private static final String[] STYLE_LAYERS = new String[]{\n    \"block1_conv1,0.5\",\n    \"block2_conv1,1.0\",\n    \"block3_conv1,1.5\",\n    \"block4_conv2,3.0\",\n    \"block5_conv1,4.0\"\n};</pre>\n<pre>private Double allStyleLayersLoss(Map<String, INDArray> activationsStyleMap, Map<String, INDArray> activationsCombMap) {\n    Double styles = 0.0;\n    for (String styleLayers : STYLE_LAYERS) {\n        String[] split = styleLayers.split(\",\");\n        String styleLayerName = split[0];\n        double weight = Double.parseDouble(split[1]);\n        styles += styleLoss(activationsStyleMap.get(styleLayerName).dup(), activationsCombMap.get(styleLayerName).dup()) * weight;\n    }\n    return styles;\n}</pre>\n<h2>Total Cost Function</h2>\n<p>Total cost measures how fare or different the combine image is from content image features from the selected layer and from features selected from multiple layers on style image. In order to have control over how much we want our combine image to look as content or style two wights are introduced <strong>α </strong>and <strong>β</strong>:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1850\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_21h16_38.jpg?resize=393%2C59\" alt=\"\" width=\"393\" height=\"59\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_21h16_38.jpg?w=393 393w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/2018-02-23_21h16_38.jpg?resize=300%2C45 300w\" sizes=\"(max-width: 393px) 85vw, 393px\" data-recalc-dims=\"1\" /></p>\n<p>Increasing <strong>α </strong>will cause combine image to look more like content while increasing <strong>β </strong>will cause combine image to have more style. Usually we decrease <strong>α </strong>and increase <strong>β. </strong></p>\n<h2>Updating Combine Image</h2>\n<p>By now we have a great way to measure how similar the combine image is with the content and the style. What we need now is to react on the comparison result in order to change the combine image so that next time will have less difference or lower cost function value. Step by step we will change the combine image to become closer and closer to content and style images layers features.</p>\n<p>The amount of change is done by using derivation of the total cost function. The derivation <a href=\"http://ramok.tech/2017/10/30/recommender-system/#Minimize_Cost_Function\">simply gives a direction</a> to go. We than multiply the derivation value by an coefficient α which simply defines how much you want to progress or change. You don&#8217;t want small values as it will take a lot of iteration to improve where bigger values will make the algorithm never converge or producing unstable values(<a href=\"http://ramok.tech/2017/10/30/recommender-system/#Insight_Gradient_Descent\">see here for more</a>).</p>\n<p>If it were <a href=\"https://www.tensorflow.org/\">TensorFlow</a> we will be done by now since it handles the derivation or the cost function automatically for us. Although <a href=\"https://deeplearning4j.org/\">deeplearning4j</a> requires manual calculation of derivation(<a href=\"https://github.com/deeplearning4j/nd4j\">n4dj</a> it offers some autodiff feel free to experiment for automatic derivation) and is not design to work in the way style transfer learning requires , it has all the flexibility and pieces to build the algorithm.</p>\n<p>Thanks to<a href=\"https://github.com/schrum2\"> Jacob Schrum</a> we were able to build derivation implementation in java please find the details on <a href=\"https://deeplearning4j.org/\">deeplearning4j</a> examples on <a href=\"https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/styletransfer/NeuralStyleTransfer.java\">github class implementation</a> originally started at <a href=\"https://github.com/klevis/MM-NEAT/blob/dev/src/main/java/edu/southwestern/util/graphics/NeuralStyleTransfer.java\">MM-NEAT repository</a>.</p>\n<p>The last step is to update the combine image with the derivation value(multiplied by α as well):</p>\n<pre> AdamUpdater adamUpdater = createADAMUpdater();\n        for (int iteration = 0; iteration < ITERATIONS; iteration++) {\n            log.info(\"iteration  \" + iteration);\n            Map<String, INDArray> activationsCombMap = vgg16FineTune.feedForward(combination, true);\n\n            INDArray styleBackProb = backPropagateStyles(vgg16FineTune, activationsStyleGramMap, activationsCombMap);\n\n            INDArray backPropContent = backPropagateContent(vgg16FineTune, activationsContentMap, activationsCombMap);\n\n            INDArray backPropAllValues = backPropContent.muli(ALPHA).addi(styleBackProb.muli(BETA));\n\n            adamUpdater.applyUpdater(backPropAllValues, iteration);\n            combination.subi(backPropAllValues);\n\n            log.info(\"Total Loss: \" + totalLoss(activationsStyleMap, activationsCombMap, activationsContentMap));\n            if (iteration % SAVE_IMAGE_CHECKPOINT == 0) {\n                //save image can be found at target/classes/styletransfer/out\n                saveImage(combination.dup(), iteration);\n            }\n        }</pre>\n<p>So we simply subtract the derivation value from combine images pixels each iteration and the cost function grantees each iteration we come closer to the image we want. In order the algorithm to be effective we update using <a href=\"https://deeplearning4j.org/updater\">ADAM</a> which is simply helps gradient descent to converge more stably. Basically a simpler Updater will work fine as well but it will take slightly more time.</p>\n<p>What we described so fare is Gradient Descent more specifically Stochastic Gradient Descent since we are updating only one sample at time. Usually for transfer learning is used L-BFGS but with <a href=\"https://deeplearning4j.org/\">deeplearning4j</a> will be harder and I didn&#8217;t have an insight how to approach it.</p>\n<h2>Application</h2>\n<p>Originally the case was implemented at <a href=\"https://github.com/klevis/MM-NEAT/blob/dev/src/main/java/edu/southwestern/util/graphics/NeuralStyleTransfer.java\">MM-NEAT</a> together with <a href=\"https://github.com/schrum2\"> Jacob Schrum</a> but later one was contributed to <a href=\"https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/styletransfer/NeuralStyleTransfer.java\">deeplearning4j-examples</a> project so feel free to download from any of the source(from dl4j is slightly refactored).</p>\n<p>Basically the class code can be easily copied and run on different project as it has no other dependencies beside <a href=\"https://deeplearning4j.org/\">deeplearning4j</a> of course.</p>\n<p>Usually to get descent results you need to run minimum 500 iteration but 1000 is more often recommended while 5000 iteration produces really high quality images. Anyway expect to let the algorithm run for couple of hours(3-4) for 1000 iterations.</p>\n<p>There few parameters we can play in order to affect the combine image to what it looks best to us:</p>\n<ul>\n<li>Change loss <strong>α(impacts content) </strong>and <strong>β(impacts style) </strong>values which simply affect how much you want your image to look like content or style. Some values suggested in other implementations are (0.025, 5) , (5,100),(10,40) but anyway feel free to experiment there is plenty of room for optimization.</li>\n<li>Change style layers weights , currently we have bigger values for higher layers and small values for low layers. Anyway there other implementation showing very good results with equal wights e.x we have layers for style so the weights will be all <strong>0.2</strong>. It will be quite interesting to try also increasing low level layers impact and notice how the images is transformed.</li>\n<li>Change content layer or style layers to a lower layers or deeper layers and notice how the image is greatly affected.</li>\n<li>There are other parameters related mostly with the algorithm itself which are worth to consider in the context of speeding up the execution like ADAM beta and beta_2 momentum constants or learning rate.</li>\n</ul>\n<h2>Showcases</h2>\n<p>Please find below some show cases. Feel free to share more show cases since exist many more interesting art mixtures out there.</p>\n<p>1.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1866 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/vlora.jpg?resize=332%2C192\" alt=\"\" width=\"332\" height=\"192\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/vlora.jpg?w=850 850w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/vlora.jpg?resize=300%2C174 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/vlora.jpg?resize=768%2C446 768w\" sizes=\"(max-width: 332px) 85vw, 332px\" data-recalc-dims=\"1\" /> <img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1865 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?resize=257%2C258\" alt=\"\" width=\"257\" height=\"258\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?w=1059 1059w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?resize=150%2C150 150w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?resize=300%2C300 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?resize=768%2C769 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style3.jpg?resize=1022%2C1024 1022w\" sizes=\"(max-width: 257px) 85vw, 257px\" data-recalc-dims=\"1\" /></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1867\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration995.jpg?resize=400%2C300\" alt=\"\" width=\"400\" height=\"300\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration995.jpg?w=400 400w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration995.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 400px) 85vw, 400px\" data-recalc-dims=\"1\" />2. <img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1862 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=292%2C219\" alt=\"\" width=\"292\" height=\"219\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?w=800 800w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=300%2C225 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content4.jpg?resize=768%2C576 768w\" sizes=\"(max-width: 292px) 85vw, 292px\" data-recalc-dims=\"1\" /><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1864 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?resize=277%2C220\" alt=\"\" width=\"277\" height=\"220\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?w=640 640w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style2.jpg?resize=300%2C238 300w\" sizes=\"(max-width: 277px) 85vw, 277px\" data-recalc-dims=\"1\" /></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1868\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?resize=400%2C300\" alt=\"\" width=\"400\" height=\"300\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?w=400 400w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration840.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 400px) 85vw, 400px\" data-recalc-dims=\"1\" /></p>\n<p>3.<img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1811 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content.jpg?resize=291%2C218\" alt=\"\" width=\"291\" height=\"218\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content.jpg?w=800 800w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content.jpg?resize=300%2C225 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/content.jpg?resize=768%2C576 768w\" sizes=\"(max-width: 291px) 85vw, 291px\" data-recalc-dims=\"1\" /> <img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1812 alignnone\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style.jpg?resize=292%2C219\" alt=\"\" width=\"292\" height=\"219\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style.jpg?w=800 800w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style.jpg?resize=300%2C225 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/style.jpg?resize=768%2C576 768w\" sizes=\"(max-width: 292px) 85vw, 292px\" data-recalc-dims=\"1\" /></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-1860\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration545.jpg?resize=352%2C264\" alt=\"\" width=\"352\" height=\"264\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration545.jpg?w=400 400w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/02/iteration545.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 352px) 85vw, 352px\" data-recalc-dims=\"1\" /></p>\n<p>&nbsp;</p>\n<h2>Results & Future Work</h2>\n<ol>\n<li>In general <a href=\"http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style\">style transfer algorithm is slow</a> because each time requires forward pass and several back propagation passes(couple of hours with 800X600 resolution with <a href=\"https://www.tensorflow.org/\">tensorflow</a>). I didn&#8217;t personally perform any comparison of <a href=\"https://deeplearning4j.org/\"><strong>deeplearning4j</strong> </a>with other frameworks like <a href=\"https://www.tensorflow.org/\">TensorFlow</a> but at first look I have the impression that it is slower.Especially if you try to run with high resolution like 800 X 600 it becomes on CPU almost not commutable. Maybe running on GPU will help and probably will do but again I did not try so feel free to suggest an new insights or experiments.</li>\n<li>There is a <a href=\"https://cs.stanford.edu/people/jcjohns/eccv16/\">new paper</a> which suggest a state of the art technique to make style transfer faster. The implementation is so efficient that can be applied also for videos so it will be quite interesting to find a way to implement on java. Please find below few demos and implementation in <a href=\"https://www.tensorflow.org/\">TensorFlow</a>:\n<ol>\n<li><a href=\"https://github.com/lengstrom/fast-style-transfer\">https://github.com/lengstrom/fast-style-transfer</a></li>\n<li><a href=\"https://github.com/jcjohnson/fast-neural-style\">https://github.com/jcjohnson/fast-neural-style</a></li>\n<li><a href=\"https://github.com/hwalsuklee/tensorflow-fast-style-transfer\">https://github.com/hwalsuklee/tensorflow-fast-style-transfer</a></li>\n</ol>\n</li>\n</ol>\n",
  "post-id": 1809
}