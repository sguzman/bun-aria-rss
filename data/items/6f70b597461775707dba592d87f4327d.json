{
  "title": "Marcin Pionnier on finishing 5th in the RTA competition",
  "link": "https://medium.com/kaggle-blog/marcin-pionnier-on-finishing-5th-in-the-rta-competition-3f98580b9de9?source=rss----4b0982ce16a3---4",
  "guid": "https://medium.com/p/3f98580b9de9",
  "category": [
    "kaggle",
    "kaggle-competition"
  ],
  "dc:creator": "Kaggle Team",
  "pubDate": "Mon, 20 Apr 2020 17:12:42 GMT",
  "atom:updated": "2020-04-20T17:12:42.609Z",
  "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/426/0*bZ9qP-OMjM2VUwjJ.png\" /></figure><p>I graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org) — from my perspective it is a very good way to get real data mining experience.</p><p><strong>What I tried</strong></p><p>As far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regression — because the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE 🙂 ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&time interval:<br> — number of minutes from 0:00 hours up to current moment (“now”)<br> — average drive time for given loop&interval<br> — loop times for current moment and some number of historical moments<br> before (the number of time points and the loop varied between the<br> methods)<br> — differences between “neighboring” time moments for the above data:<br> just differences or differences transformed with logistic function<br> (1/1+e^-difference). Use of logistic function gave a jump from public<br> RMSE at about 198 to 189. The idea to use of sigmoid function here was<br> just my intuition inspired by differences distribution.<br> — “saturations” for for each loop (except the 2 first loops at both<br> directions ).</p><p>I introduced the simple (and very naive) model of traffic growth:<br> If the speed at given loop is up to 40 km/h — the saturation is 1;</p><p>If the difference between the previous loop and the given loop is more than 5 km/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData file — after the regression this minimal value was used if predicted value was less than minimum.</p><p>I did not use historical data at all — I found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions).</p><p><strong>What ended up working</strong></p><p>For each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData<br> file:<br> Model 1: For all (61) loops: current + 5 times moments before and 5 simple differences — 675 attributes,<br> Model 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only) — 204 to 404 atrributes,<br> Model 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences,<br> saturations (for current time moment only) 204 to 404 atrributes,<br> Model with least RMSE computed on the train file was selected for particular loop. It is not a very good strategy, however I thought<br> that generally linear regression was resistant to overfitting (it is not true — as the number of variable grows, the more variance can be explained — this is what I have learnt).</p><p>This strategy gave me public RMSE 189.3</p><p>I added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily:<br> Model 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only) — 614 attributes. This turn gave mi 188.6 public result.</p><p>What is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979) , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3):<br> Model 5: like model 3 but also loop times are “sigmoided” not only differences.</p><p><strong>What tools I used</strong></p><p>My solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests). Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread) — at the of the competition i was using computer with 4 processors and 12 GB of RAM — with 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations.</p><p><em>Originally published at blog.kaggle.com on February 17, 2011.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3f98580b9de9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/kaggle-blog/marcin-pionnier-on-finishing-5th-in-the-rta-competition-3f98580b9de9\">Marcin Pionnier on finishing 5th in the RTA competition</a> was originally published in <a href=\"https://medium.com/kaggle-blog\">Kaggle Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}