{
  "title": "Marcin Pionnier on finishing 5th in the RTA competition",
  "link": "https://medium.com/kaggle-blog/marcin-pionnier-on-finishing-5th-in-the-rta-competition-3f98580b9de9?source=rss----4b0982ce16a3---4",
  "guid": "https://medium.com/p/3f98580b9de9",
  "category": [
    "kaggle",
    "kaggle-competition"
  ],
  "dc:creator": "Kaggle Team",
  "pubDate": "Mon, 20 Apr 2020 17:12:42 GMT",
  "atom:updated": "2020-04-20T17:12:42.609Z",
  "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/426/0*bZ9qP-OMjM2VUwjJ.png\" /></figure><p>I graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org)â€Šâ€”â€Šfrom my perspective it is a very good way to get real data mining experience.</p><p><strong>What IÂ tried</strong></p><p>As far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regressionâ€Šâ€”â€Šbecause the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE ğŸ™‚ ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&time interval:<br>â€Šâ€”â€Šnumber of minutes from 0:00 hours up to current moment (â€œnowâ€)<br>â€Šâ€”â€Šaverage drive time for given loop&interval<br>â€Šâ€”â€Šloop times for current moment and some number of historical moments<br> before (the number of time points and the loop varied between the<br> methods)<br>â€Šâ€”â€Šdifferences between â€œneighboringâ€ time moments for the above data:<br> just differences or differences transformed with logistic function<br> (1/1+e^-difference). Use of logistic function gave a jump from public<br> RMSE at about 198 to 189. The idea to use of sigmoid function here was<br> just my intuition inspired by differences distribution.<br>â€Šâ€”â€Šâ€œsaturationsâ€ for for each loop (except the 2 first loops at both<br> directions ).</p><p>I introduced the simple (and very naive) model of traffic growth:<br> If the speed at given loop is up to 40 km/hâ€Šâ€”â€Šthe saturation isÂ 1;</p><p>If the difference between the previous loop and the given loop is more than 5 km/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData fileâ€Šâ€”â€Šafter the regression this minimal value was used if predicted value was less thanÂ minimum.</p><p>I did not use historical data at allâ€Šâ€”â€ŠI found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions).</p><p><strong>What ended upÂ working</strong></p><p>For each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData<br> file:<br> Model 1: For all (61) loops: current + 5 times moments before and 5 simple differencesâ€Šâ€”â€Š675 attributes,<br> Model 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only)â€Šâ€”â€Š204 to 404 atrributes,<br> Model 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences,<br> saturations (for current time moment only) 204 to 404 atrributes,<br> Model with least RMSE computed on the train file was selected for particular loop. It is not a very good strategy, however I thought<br> that generally linear regression was resistant to overfitting (it is not trueâ€Šâ€”â€Šas the number of variable grows, the more variance can be explainedâ€Šâ€”â€Šthis is what I haveÂ learnt).</p><p>This strategy gave me public RMSEÂ 189.3</p><p>I added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily:<br> Model 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only)â€Šâ€”â€Š614 attributes. This turn gave mi 188.6 publicÂ result.</p><p>What is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979)Â , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3):<br> Model 5: like model 3 but also loop times are â€œsigmoidedâ€ not only differences.</p><p><strong>What tools IÂ used</strong></p><p>My solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests). Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread)â€Šâ€”â€Šat the of the competition i was using computer with 4 processors and 12 GB of RAMâ€Šâ€”â€Šwith 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations.</p><p><em>Originally published at blog.kaggle.com on February 17,Â 2011.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3f98580b9de9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/kaggle-blog/marcin-pionnier-on-finishing-5th-in-the-rta-competition-3f98580b9de9\">Marcin Pionnier on finishing 5th in the RTA competition</a> was originally published in <a href=\"https://medium.com/kaggle-blog\">Kaggle Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
}