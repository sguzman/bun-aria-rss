{
  "title": "Automatic Text Summarization: Overview and Challenges",
  "description": "<p>The amount of information that we consume is growing every day. As a consequence, we need mechanisms to compress this growing amount of information. Text summarization is a tool for compressing written text and has been used for ages. At this moment, the amount of information is growing exponentially as</p>",
  "link": "https://www.data-blogger.com/automatic-text-summarization-overview-and-challenges/",
  "guid": "623ecb5bc8ffcd000104c616",
  "category": [
    "Research",
    "Artificial Intelligence",
    "Natural Language Processing"
  ],
  "dc:creator": "Kevin Jacobs",
  "pubDate": "Sat, 26 Mar 2022 08:21:56 GMT",
  "media:content": "",
  "content:encoded": "<img src=\"https://images.unsplash.com/photo-1602722053020-af31042989d5?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tjYXNlfGVufDB8fHx8MTY0ODI4MjYyOQ&ixlib=rb-1.2.1&q=80&w=2000\" alt=\"Automatic Text Summarization: Overview and Challenges\"><p>The amount of information that we consume is growing every day. As a consequence, we need mechanisms to compress this growing amount of information. Text summarization is a tool for compressing written text and has been used for ages. At this moment, the amount of information is growing exponentially as of which it might be helpful to design models that can automatically summarize texts for us.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://kevinjacobs.io/content/images/2022/03/wordcloud.png\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"1024\" height=\"768\"><figcaption>A wordcloud of the Wikipedia article on automatic text summarization.</figcaption></figure><h2 id=\"historical-notes\">Historical Notes</h2><p>Automatic text summarization comes in two flavours: extractive summarization and abstractive summarization. Extractive summarization models take exact phrases from the reference documents and use them as a summary. One of the very first research papers on (extractive) text summarization is the work of Luhn [1]. TextRank [2] (based on the concepts used by the PageRank algorithm) is another widely used extractive summarization model.</p><p>In the era of deep learning, abstractive summarization became a reality. With abstractive summarization, a model generates a text instead of using literal phrases of the reference documents. One of the more recent works on abstractive summarization is PEGASUS [3] (a demo is available at <a href=\"https://huggingface.co/google/pegasus-xsum\">HuggingFace</a>). PEGASUS can summarize the following <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">Wikipedia article</a>:</p><figure class=\"kg-card kg-code-card\"><pre><code>Python is an interpreted high-level general-purpose programming language.\nIts design philosophy emphasizes code readability with its use of significant indentation.\nIts language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\nPython is dynamically-typed and garbage-collected.\nIt supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming.\nIt is often described as a \"batteries included\" language due to its comprehensive standard library.\nGuido van Rossum began working on Python in the late 1980s, as a successor to the ABC programming language, and first released it in 1991 as Python 0.9.0.\nPython 2.0 was released in 2000 and introduced new features, such as list comprehensions and a garbage collection system using reference counting.\nPython 3.0 was released in 2008 and was a major revision of the language that is not completely backward-compatible.\nPython 2 was discontinued with version 2.7.18 in 2020.\nPython consistently ranks as one of the most popular programming languages.</code></pre><figcaption>The input text.</figcaption></figure><p>As output, it then generates the following (abstractive) summary of this text:</p><figure class=\"kg-card kg-code-card\"><pre><code>Python is a programming language developed by Guido van Rossum.</code></pre><figcaption>The generated abstractive summary.</figcaption></figure><p>What I find interesting, is that this exact phrase cannot be found in the reference document and that one model is capable of compressing textual information automatically. However, there are some challenges with abstractive text summarization as well which are explored in the next section.</p><h2 id=\"challenges-for-automatic-text-summarization\">Challenges for Automatic Text Summarization</h2><p>In this section, several challenges for automatic text summarization will be discussed as well as potential research directions.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://images.unsplash.com/photo-1576269076940-31c888bebb3e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGlucHV0JTIwb3V0cHV0fGVufDB8fHx8MTYzMDYxMjkxNQ&ixlib=rb-1.2.1&q=80&w=2000\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"6000\" height=\"4000\" srcset=\"https://images.unsplash.com/photo-1576269076940-31c888bebb3e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGlucHV0JTIwb3V0cHV0fGVufDB8fHx8MTYzMDYxMjkxNQ&ixlib=rb-1.2.1&q=80&w=600 600w, https://images.unsplash.com/photo-1576269076940-31c888bebb3e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGlucHV0JTIwb3V0cHV0fGVufDB8fHx8MTYzMDYxMjkxNQ&ixlib=rb-1.2.1&q=80&w=1000 1000w, https://images.unsplash.com/photo-1576269076940-31c888bebb3e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGlucHV0JTIwb3V0cHV0fGVufDB8fHx8MTYzMDYxMjkxNQ&ixlib=rb-1.2.1&q=80&w=1600 1600w, https://images.unsplash.com/photo-1576269076940-31c888bebb3e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGlucHV0JTIwb3V0cHV0fGVufDB8fHx8MTYzMDYxMjkxNQ&ixlib=rb-1.2.1&q=80&w=2400 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Photo by <a href=\"https://unsplash.com/@thevictorbarrios?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Victor Barrios</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a></figcaption></figure><h3 id=\"controlling-the-outputs\">Controlling the Outputs</h3><p>What are the challenges that we are facing with these kinds of models? Are these models perfect? No, and that directly brings me to the first point: how can we measure the \"quality\" of a summary? In the past, several metrics are developed. Such as ROUGE and BLEU (which roughly measure the amount of overlap between the generated summary and the reference text). But what \"fluency\" (the grammatical and semantical correctness of a text)? And factual correctness? One issue with abstractive models is the generated output might contain words and numbers that are not found in the reference texts. Restricting the vocabulary might be one possible solution for constraining the output [5] which is explained below. Hopefully, more metrics and methods for controlling the outputs will become available.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://images.unsplash.com/photo-1497633762265-9d179a990aa6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tzfGVufDB8fHx8MTYzMDYxMjg3MQ&ixlib=rb-1.2.1&q=80&w=2000\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"6016\" height=\"4000\" srcset=\"https://images.unsplash.com/photo-1497633762265-9d179a990aa6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tzfGVufDB8fHx8MTYzMDYxMjg3MQ&ixlib=rb-1.2.1&q=80&w=600 600w, https://images.unsplash.com/photo-1497633762265-9d179a990aa6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tzfGVufDB8fHx8MTYzMDYxMjg3MQ&ixlib=rb-1.2.1&q=80&w=1000 1000w, https://images.unsplash.com/photo-1497633762265-9d179a990aa6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tzfGVufDB8fHx8MTYzMDYxMjg3MQ&ixlib=rb-1.2.1&q=80&w=1600 1600w, https://images.unsplash.com/photo-1497633762265-9d179a990aa6?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fGJvb2tzfGVufDB8fHx8MTYzMDYxMjg3MQ&ixlib=rb-1.2.1&q=80&w=2400 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Photo by <a href=\"https://unsplash.com/@kimberlyfarmer?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Kimberly Farmer</a> / <a href=\"https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit\">Unsplash</a></figcaption></figure><h3 id=\"multi-document-summarization\">Multi-document Summarization</h3><p>Another challenge is multi-document summarization, in which multiple documents are summarized into a single summary. This task can be complicated further by using documents of different languages as input. The inputs of this task can become large. Most of the abstractive models are based on Transformers [4] which are known to have a quadratic memory requirement with respect to the number of input tokens. In practice, often 512 subword tokens can be used with Transformer-based models which is a troublesome limitation for the multi-document summarization task. Luckily, some models are capable of transforming the quadratic memory requirement to a linear memory requirement, such as the Longformer [6] which is explained below. A larger number of datasets on (multilingual) multi-document summarization together with solutions on decreasing the memory requirement of Transformers might be helpful for multi-document summarization.</p><h3 id=\"other-research-directions\">Other Research Directions</h3><p>Another interesting research direction might be controlling the inputs. What if we can concentrate on only certain aspects of the inputs? Or what if we can combine textual data with image data? Another idea might be to combine text summarization with other NLP subtasks in order to gain more control over the process.</p><h2 id=\"possible-solutions\">Possible Solutions</h2><h3 id=\"nucleus-sampling\">Nucleus Sampling</h3><p>Constraining the vocabulary as Nucleus Sampling does can help in controlling the output. The authors of [5] mention that generated text often is bland, incoherent or stuck in a repetitive loop. The following image shows these undesirable properties:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://kevinjacobs.io/content/images/2022/03/Screenshot-2022-03-11-at-14.46.48.png\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"672\" height=\"133\"><figcaption>As explained in [5], texts can get stuck in repetitive loops (the blue text) or generate incoherent gibberish (the red text).</figcaption></figure><p>To cope with these issues, the authors propose to use Nucleus Sampling: a dynamically sized subset of the vocabulary while predicting the next word, depending on the likelihood of the next word. The authors call this top-p sampling. It is closely related to top-k sampling, in which the top-k vocabulary is used.</p><p>An interesting assumption as explained in the paper is that human-written text does not equal the most probable text. They support this assumption by the fact that people optimize against stating the obvious [7] - which is exactly the opposite of optimizing for the most likely text.</p><p>The following image illustrates how Nucleus Sampling avoids repetitive loops and incoherent texts in a single example in which the algorithms generate text starting from a given sentence:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://kevinjacobs.io/content/images/2022/03/Screenshot-2022-03-11-at-14.47.57.png\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"672\" height=\"666\"><figcaption>Example of different text generation techniques and issues found in the generated texts.</figcaption></figure><p>Thus, techniques like top-k sampling and Nucleus Sampling (top-p sampling) help in generating more coherent and less repetitive texts which are two undesirable properties in text generation and thus also in automatic text summarization.</p><h3 id=\"longformer\">Longformer</h3><p>One issue for multi-document summarization is the quadratic memory requirement of Transformer-based models. One solution is the Longformer [6]. As the authors mention, Transformer-based models use attention on all input tokens:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://kevinjacobs.io/content/images/2022/03/Screenshot-2022-03-11-at-14.48.36.png\" class=\"kg-image\" alt=\"Automatic Text Summarization: Overview and Challenges\" loading=\"lazy\" width=\"732\" height=\"158\"><figcaption>Attention patterns of Transformer-based models (a) versus attention patterns used in the Longformer (b, c and d).</figcaption></figure><p>The idea of the Longformer is to use different attention strategies in order to cope with longer inputs. By incorporating special tokens through the text, one can enable the computation of full attention on these special tokens. Therefore, the memory requirement gets reduced to the number of these special tokens times the number of input tokens. To compare: Transformers work on 512 subword tokens, but the Longformer is evaluated on a dataset where some documents contained 14.5K tokens! Besides the Longformer, there are other solutions as well focussing on reducing the quadratic memory requirement, such as Big Bird [8]. Once we can overcome the quadratic memory cost without sacrificing the quality, multi-document summarization and other tasks related to longer and/or multiple documents can be solved. </p><h2 id=\"conclusion\">Conclusion</h2><p>The amount of (textual) information is growing exponentially as well as the need for automatic text summarization tools. Automatic text summarization is an exciting subfield of natural language processing. Both extractive and abstractive text summarization methods might bring us solutions for keeping up with the growing amount of information. One challenge for automatic text summarization is measuring the quality of generated texts and another challenge is the input length constraint in Transformer-based models.</p><h2 id=\"references\">References</h2><ol><li>Luhn, H. P. (1958). The automatic creation of literature abstracts. <em>IBM Journal of research and development</em>, <em>2</em>(2), 159-165.</li><li>Mihalcea, R., & Tarau, P. (2004, July). Textrank: Bringing order into text. In <em>Proceedings of the 2004 conference on empirical methods in natural language processing</em> (pp. 404-411).</li><li>Zhang, J., Zhao, Y., Saleh, M., & Liu, P. (2020, November). Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In <em>International Conference on Machine Learning</em> (pp. 11328-11339). PMLR.</li><li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</li><li>Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). The curious case of neural text degeneration. <em>arXiv preprint arXiv:1904.09751</em>.</li><li>Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The long-document transformer. <em>arXiv preprint arXiv:2004.05150</em>.</li><li>H Paul Grice. Logic and conversation. In P Cole and J L Morgan (eds.), <em>Speech Acts</em>, volume 3 of <em>Syntax and Semantics</em>, pp. 41&#x2013;58. Academic Press, 1975.</li><li>Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020, July). Big Bird: Transformers for Longer Sequences. In <em>NeurIPS</em>.</li></ol>"
}