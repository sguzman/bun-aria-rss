{
  "title": "Opinion mining on Dutch news articles",
  "description": "<p>In this blog post, I will learn you how you can mine opinions about companies from news articles. I will share how I scraped thousands of news articles in a few minutes and how one could classify the opinion expressed in the titles of the news articles. This information could</p>",
  "link": "https://www.data-blogger.com/opinion-mining-on-dutch-news-articles/",
  "guid": "622cfbd6092f120001a24da9",
  "category": [
    "Information Retrieval",
    "Python"
  ],
  "dc:creator": "Kevin Jacobs",
  "pubDate": "Wed, 20 Jun 2018 00:00:00 GMT",
  "media:content": "",
  "content:encoded": "<img src=\"https://images.unsplash.com/photo-1495020689067-958852a7765e?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fG5ld3NwYXBlcnxlbnwwfHx8fDE2NDcxMTUyNzk&ixlib=rb-1.2.1&q=80&w=2000\" alt=\"Opinion mining on Dutch news articles\"><p>In this blog post, I will learn you how you can mine opinions about companies from news articles. I will share how I scraped thousands of news articles in a few minutes and how one could classify the opinion expressed in the titles of the news articles. This information could be used for example to help with watching competitors of a company or to predict global trends.</p><h2 id=\"what-components-are-needed\">What components are needed?</h2><p>The following components are needed:</p><ul><li>A method for scraping news.</li><li>A method for scraping opinions (ratings, etcetera).</li><li>A model for sentiment for answering the following question: &#x201C;What words/phrases are related to what sentiment?&#x201D;</li></ul><p>First, the news scraper is explained. Then, the opinion scraper and sentiment model are explained and then the ensemble is tested in the real world!</p><h2 id=\"rss-webscraper-for-news-headlines\">RSS webscraper for news headlines</h2><p>I used Scrapy for scraping webpages and RSS. The first thing to do, is to setup the project using Scrapy. This is done by executing the following code:</p><pre><code class=\"language-bash\">scrapy startproject collect</code></pre><p>Then, move into the project folder (as explained in the message when setting up the project) and create a spider pointing at the domain to scrape the news from.</p><p>I ended up with the following RSS spider:</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy import Selector\nfrom scrapy.spiders import XMLFeedSpider\n\nfrom collect.items import NewsItem\n\n\nclass NewsRSSSpider(XMLFeedSpider):\n    name = 'news'\n    allowed_domains = ['news.nl']\n    start_urls = ['https://www.news.nl/sitemap_index.xml']\n    namespaces = [\n        ('ns', 'https://www.sitemaps.org/schemas/sitemap/0.9'),\n        ('news', 'https://www.google.com/schemas/sitemap-news/0.9')\n    ]\n\n    def parse(self, response):\n        self.logger.info('Parsing %s' % response.url)\n\n        selector = Selector(response, type='xml')\n        self._register_namespaces(selector)\n\n        # Extract news items when found\n        nodes = selector.xpath('//news:news')\n        for node in nodes:\n            fields = ['name', 'language', 'genres', 'publication_date', 'title', 'keywords']\n            item = NewsItem()\n            for field in fields:\n                value = node.xpath('news:%s/text()' % field).extract_first()\n                item[field] = value\n            yield item\n\n        # Follow URLs when found\n        nodes = selector.xpath('//ns:loc/text()')\n        urls = [node.extract() for node in nodes]\n        urls = [url for url in urls if url.endswith('.xml')]\n        urls = sorted(urls, reverse=True)\n        for i, url in enumerate(urls):\n            yield scrapy.Request(url, callback=self.parse)</code></pre><p>And the following item:</p><pre><code class=\"language-python\">class NewsItem(scrapy.Item):\n    name = scrapy.Field()\n    language = scrapy.Field()\n    genres = scrapy.Field()\n    publication_date = scrapy.Field()\n    title = scrapy.Field()\n    keywords = scrapy.Field()</code></pre><p>Then, I can execute the spider and export all headlines to a CSV file:</p><pre><code class=\"language-bash\">scrapy crawl news -o news.csv -t csv</code></pre><h2 id=\"html-webscraper-for-dutch-opinions\">HTML webscraper for Dutch opinions</h2><p>Then, I created a webscraper to scrape Dutch opinions. This resulted into the following Python code:</p><pre><code class=\"language-python\"># -*- coding: utf-8 -*-\nimport scrapy\n\nfrom collect.items import ReviewItem\n\n\nclass OpinionSpider(scrapy.Spider):\n    name = 'opinions'\n    allowed_domains = ['opinions.nl']\n    start_urls = ['https://opinions.nl/opinions?page=1']\n\n    def parse(self, response):\n        self.logger.info('Parsing %s' % response.url)\n\n        for node in response.xpath('//li[contains(@class, \"review-preview\")]'):\n            reviewer = node.xpath('div[contains(@class, \"review-reviewer\")]//h3[contains(@class, \"name\")]//text()').extract()\n            reviewer = ''.join(reviewer).strip()\n            feedback = node.xpath('div[contains(@class, \"review-feedback\")]//text()').extract()\n            feedback = ''.join(feedback).strip()\n            rating = node.xpath('div[contains(@class, \"review-scores\")]//span[contains(@class, \"review-rating\")]//text()').extract_first()\n            if rating is None:\n                continue\n            rating = float(rating.strip().replace(',', '.'))\n            yield ReviewItem(user=reviewer, rating=rating, feedback=feedback)\n\n        for node in response.xpath('//a/@href'):\n            url = node.extract()\n            if url.startswith('/opinions?page='):\n                yield scrapy.Request('https://opinions.nl/%s' % url[1:], callback=self.parse)</code></pre><p>And the following item:</p><pre><code class=\"language-python\">class ReviewItem(scrapy.Item):\n    user = scrapy.Field()\n    rating = scrapy.Field()\n    feedback = scrapy.Field()</code></pre><p>I then executed the code and created a CSV file containing thousands of Dutch opinions (opinions.csv).</p><h2 id=\"training-a-sentiment-model\">Training a sentiment model</h2><p>After scraping all the data, the sentiment model can be build. I used <a href=\"https://chainer.org/\">Chainer</a> to create the Neural Network. Then, the opinions.csv is loaded:</p><pre><code class=\"language-python\">import pandas as pd\n\ndf_review = pd.read_csv('opinion.csv')</code></pre><p>After loading the CSV, preprocessing is done. The preprocessing converts the raw text to character identifiers. Also the rating (scale 1 &#x2013; 10) is normalized (from 0.0 to 1.0) such that the model can work with it. Then, this is continuous variable is replaced by its discrete counterpart such that there are three classes: negative sentiment (< 0.25), positive sentiment (> 0.75) and neutral sentiment (between 0.25 and 0.75). Also accents are removed and characters which are not in the specified alphabet are removed. This results into the following preprocessing code:</p><pre><code class=\"language-python\">import re\nfrom unidecode import unidecode\nimport nltk\n\nalphabet = list('abcdefghijklmnopqrstuvwxyz\\'\".,:;?!()\\r\\n&-+ ')\ndf_review.rating = pd.to_numeric(df_review.rating, errors='coerce')\ndf_review = df_review.dropna()\ndf_review = df_review.assign(sentiment=df_review.rating.apply(lambda x: (x - 1.0) / 9.0))\ndf_review.feedback = df_review.feedback.apply(unidecode)\ndf_review = df_review.assign(feedback_lowered=df_review.feedback.str.lower())\ndf_review = df_review.assign(nonalphabet_chars=df_review.feedback_lowered.apply(lambda x: re.sub('[%s]' % re.escape(''.join(alphabet)), '', x)))\ndf_review = df_review[df_review.nonalphabet_chars.apply(len) == 0]\ndf_review = df_review.assign(words=df_review.feedback_lowered.apply(nltk.word_tokenize))\ndf_review = df_review.assign(target=df_review.sentiment.apply(lambda x: 0.0 if x < 0.25 else 0.5 if x <= 0.75 else 1.0))</code></pre><p>Then, the vocabulary (the alphabet) is defined. An &#x201C;UNK&#x201D; marker is added for unknown characters (which will in fact never occur due to the preprocessing). Then, the characters are converted to identifiers:</p><pre><code class=\"language-python\">import numpy as np\n\nwords = sorted(list(set(word for words in df_review.words.values.tolist() for word in words)))\nwords = ['UNK'] + words\nvocab = {word: i for i, word in enumerate(words)}\n\ndef preprocess(text):\n    text = text.lower()\n    words = nltk.word_tokenize(text)\n    return np.asarray([vocab.get(word, vocab.get('UNK')) for word in words], dtype='i')\n\ndf_review = df_review.assign(ids=df_review.feedback.apply(preprocess))\ndf_review</code></pre><h3 id=\"creating-the-dataset\">Creating the dataset</h3><p>The next step is to create an iterator and converter for the data. The iterator is a dataset object in Chainer. The dataset has a __len__() method which computes the number of items in the dataset and the get_example(i) method fetches the ith example. The converter is used to generate batches from the dataset iterator. This results into the following code:</p><pre><code class=\"language-python\">from chainer import dataset\n\nclass DutchSentimentDataset(dataset.DatasetMixin):\n    def __init__(self, df):\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n\n    def get_example(self, i):\n        item = self.df.iloc[i]\n        return {\n            'X': item.ids,\n            't': item.sentiment\n        }\n\ndef converter(batch, device=None):\n    return {\n        'X': [item['X'] for item in batch],\n        't': np.hstack([item['t'] for item in batch]).reshape((-1, 1)),\n    }\n    \nds = DutchSentimentDataset(df_review_balanced[['sentiment', 'ids']])\nbatch = [ds.get_example(_) for _ in range(5)]\nconverter(batch)</code></pre><p>This gives the following output:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2018/06/result-1.png\" class=\"kg-image\" alt=\"Opinion mining on Dutch news articles\" loading=\"lazy\"></figure><p>The result.</p><p>As you can see, this is a batch consisting of 5 items. All the items have negative sentiment (< 0.25). The character identifiers per opinion are shown.</p><h3 id=\"the-model-code-and-training-process\">The model code and training process</h3><p>Now it is time to define the model and train it! This is done with the following Chainer code:</p><pre><code class=\"language-python\">from chainer.iterators import SerialIterator\nimport numpy as np\nimport chainer\nimport chainer.links as L\nimport chainer.functions as F\nfrom chainer import optimizers\nfrom chainer import training\nfrom chainer import report\nfrom chainer.training import extensions\nimport json\n\ndef sequence_embed(embed, xs):\n    x_len = [len(x) for x in xs]\n    x_section = np.cumsum(x_len[:-1])\n    ex = embed(F.concat(xs, axis=0))\n    exs = F.split_axis(ex, x_section, 0)\n    return exs\n\nclass RNNEncoder(chainer.Chain):\n\n    def __init__(self, n_source_vocab, n_emb_units):\n        super(RNNEncoder, self).__init__()\n        with self.init_scope():\n            self.embed = L.EmbedID(n_source_vocab, n_emb_units)\n            self.rnn = L.NStepBiGRU(1, n_emb_units, n_emb_units, 0.5)\n            self.l_out_1 = L.Linear(32)\n            self.l_out_2 = L.Linear(1)\n            \n    def get_hidden_layer(self, xs):\n        exs = sequence_embed(self.embed, xs)\n        hx, _ = self.rnn(None, exs)\n        hx = F.transpose(hx, [1, 0, 2])\n        out_1 = self.l_out_1(hx)\n        out_1 = F.relu(out_1)\n        return out_1\n            \n    def predict(self, xs):\n        out_1 = self.get_hidden_layer(xs)\n        out_2 = self.l_out_2(out_1)\n        out_2 = F.sigmoid(out_2)\n        return out_2\n            \n    def __call__(self, X, t):\n        out = self.predict(X)\n        \n        batch_size = len(X)\n        loss = -F.matmul(F.log(out.T), t) + -F.matmul(F.log(1. - out.T), 1. - t)\n        loss = loss / batch_size\n        \n        report({\n            'loss': loss.data[0, 0]\n        }, self)\n        \n        return loss\n\nmodel = RNNEncoder(len(vocab), 200)\nmax_epoch = 10\n\ntrain_iter = SerialIterator(ds, batch_size=50, repeat=False, shuffle=True)\n\nwith open('vocab.json', 'w') as fp:\n    json.dump(vocab, fp)\n\noptimizer = optimizers.Adam()\noptimizer.setup(model)\nupdater = training.StandardUpdater(train_iter, optimizer, converter=converter)\ntrainer = training.Trainer(updater, (max_epoch, 'epoch'), out='out')\ntrainer.extend(extensions.LogReport(trigger=(1, 'iteration')))\ntrainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/loss', 'elapsed_time'])) #, 'validation/main/loss'\ntrainer.extend(extensions.PlotReport(['main/loss'], 'iteration', file_name='loss.png', trigger=(1, 'iteration')))\ntrainer.extend(extensions.snapshot_object(model, 'model_iter_{.updater.iteration}'), trigger=(50, 'iteration'))\ntrainer.run()</code></pre><p>And now we can train it! The following loss plot was generated:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.data-blogger.com/wp-content/uploads/2018/06/loss.png\" class=\"kg-image\" alt=\"Opinion mining on Dutch news articles\" loading=\"lazy\"></figure><p>Funky loss.</p><p>I run it until there is no more unseen data available (so for 1 epoch). Sadly, I could get better results if I had collected more data. The loss noise can be reduced if I use larger batch sizes.</p><h2 id=\"using-the-model\">Using the model</h2><p>Now the model can compute the sentiment for a given sentence. It can also be used to predict the sentiment for a given news headline. The following sentiment is found using the model (on a sample of a few news headlines):</p><p>0.81 CompanyX neemt specialist in zakelijk CompanyY in zijn geheel over 0.76 CompanyX boekt meer winst op dalende omzet 0.72 Topman ondanks concurrentie 'uitermate tevreden' met jaarcijfers CompanyX 0.29 CompanyX haalt eigen doelstelling voor uitbreiding snel internet niet 0.23 Storing netwerk CompanyX in noorden houdt tot 3.00 uur 's nachts aan</p><p>Translated into English:</p><p>0.81 CompanyX acquires specialist for CompanyY in its entirety 0.76 CompanyX is more profitable on declining sales 0.72 Top executive of CompanyX 'extremely satisfied' with CompanyX annual figures 0.29 CompanyX does not achieve its own objective for fast internet 0.23 Network malfunctioning of CompanyX in the north will last until 3 AM</p><p>As you can see, these results are correct (1.00 means maximum positive sentiment and 0.00 means maximum negative sentiment). However, there are some major drawbacks. The sentence sentiment does not tell you anything about the sentiment per entity. It would be interesting to classify the sentiment per entity.</p><h2 id=\"conclusions-tldr\">Conclusions (TL;DR)</h2><p>With some effort, it is possible to detect sentiment in news articles (in any language). One improvement of the model is to compute sentiment per entity, but that is left as future work.</p>"
}