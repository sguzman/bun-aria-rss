{
  "title": "A Deep Dive into Transformers with TensorFlow and Keras: Part&#160;1",
  "link": "https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/",
  "dc:creator": "Aritra Roy Gosthipaty and Ritwik Raha",
  "pubDate": "Mon, 05 Sep 2022 13:00:00 +0000",
  "category": [
    "Attention",
    "Deep Learning",
    "Transformers",
    "Tutorial",
    "attention",
    "deep learning",
    "transformers",
    "tutorial"
  ],
  "guid": "https://pyimagesearch.com/?p=33846",
  "description": "<p>Table of Contents A Deep Dive into Transformers with TensorFlow and Keras: Part 1 Introduction The Transformer Architecture Encoder Decoder Evolution of Attention Version 0 Version 1 Version 2 Problems Solution Version 3 Version 4 (Cross-Attention) Version 5 (Self-Attention) Version&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/\">A Deep Dive into Transformers with TensorFlow and Keras: Part&nbsp;1</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n",
  "content:encoded": "\n<script src=\"https://fast.wistia.com/embed/medias/qmgs8r5rn1.jsonp\" async=\"\"></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async=\"\"></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_qmgs8r5rn1 videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/qmgs8r5rn1/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\"></div></div></div></div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"TOC\"/>\n\n\n\n<h2><strong>Table of Contents</strong></h2>\n\n\n\n<div class=\"toc\">\n<ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2BPTitle\">A Deep Dive into Transformers with TensorFlow and Keras: Part 1</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Introduction\">Introduction</a></li>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Architecture\">The Transformer Architecture</a></li>\n                <ul>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4Encoder\">Encoder</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4Decoder\">Decoder</a></li>\n                </ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Evolution\">Evolution of Attention</a></li>\n                <ul>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V0\">Version 0</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V1\">Version 1</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V2\">Version 2</a></li>\n                        <ul>\n                            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h5Problems\">Problems</a></li>\n                            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h5Solution\">Solution</a></li>\n                        </ul>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V3\">Version 3</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V4\">Version 4 (Cross-Attention)</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V5\">Version 5 (Self-Attention)</a></li>\n                    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h4V6\">Version 6 (Multi-Head Attention)</a></li>\n                </ul>\n        </ul>\n    <li><a rel=\"noopener\" target=\"_blank\" href=\"#h2Summary\">Summary</a></li>\n        <ul>\n            <li><a rel=\"noopener\" target=\"_blank\" href=\"#h3Citation\">Citation Information</a></li>\n        </ul>\n</ul>\n</div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2BPTitle\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>A Deep Dive into Transformers with TensorFlow and Keras: Part 1</strong></a></h2>\n\n\n\n<p>While we look at gorgeous <a href=\"https://openai.com/blog/dall-e/\" target=\"_blank\" rel=\"noreferrer noopener\">futuristic landscapes generated by AI</a> or use massive models to <a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">write our own tweets</a>, it is important to remember where all this started.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/transformers-part-1-featured.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-1024x575.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34452\" width=\"700\" height=\"393\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured.png?size=126x71&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-300x169.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured.png?size=378x212&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured.png?size=504x283&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured.png?size=630x354&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-768x431.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-1024x575.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-1536x863.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/transformers-part-1-featured-2048x1151.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a></figure></div>\n\n\n<p>Data, matrix multiplications, repeated and scaled with non-linear switches. Maybe that simplifies things a lot, but even today, most architectures boil down to these principles. Even the most complex systems, ideas, and papers can be boiled down to just that:</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p><strong>Data, matrix multiplications, repeated and scaled with non-linear switches.</strong></p></blockquote>\n\n\n\n<p>Over the past few months, we have covered <em>Natural Language Processing (NLP)</em> through our tutorials. We started from the very <strong>history</strong> and <strong>foundation</strong> of NLP and discussed Neural Machine Translation with <strong>attention</strong>.</p>\n\n\n\n<p>Here are all the tutorials chronologically.</p>\n\n\n\n<ul><li><a href=\"https://pyimagesearch.com/2022/06/27/introduction-to-natural-language-processing-nlp/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Introduction to Natural Language Processing (NLP)</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/07/04/introduction-to-the-bag-of-words-bow-model/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Introduction to the Bag-of-Words (BoW) Model</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/07/11/word2vec-a-study-of-embeddings-in-nlp/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Word2Vec: A Study of Embeddings NLP</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/07/18/comparison-between-bagofwords-and-word2vec/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Comparison Between BagofWords and Word2Vec</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/07/25/introduction-to-recurrent-neural-networks-with-keras-and-tensorflow/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Introduction to Recurrent Neural Networks with Keras and TensorFlow</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/08/01/long-short-term-memory-networks/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Long Short-Term Memory Networks</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/08/15/neural-machine-translation/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Neural Machine Translation</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/08/22/neural-machine-translation-with-bahdanaus-attention-using-tensorflow-and-keras/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Neural Machine Translation with Bahdanau’s Attention Using TensorFlow and Keras</strong></a></li><li><a href=\"https://pyimagesearch.com/2022/08/29/neural-machine-translation-with-luongs-attention-using-tensorflow-and-keras/\" target=\"_blank\" rel=\"noreferrer noopener\"><strong>Neural Machine Translation with Luong’s Attention Using TensorFlow and Keras</strong></a></li></ul>\n\n\n\n<p>Now, the progression of NLP, as discussed, tells a story. We begin with <strong>tokens</strong> and then build <strong>representations</strong> of these tokens. We use these representations to find similarities between tokens and <strong>embed</strong> them in a high-dimensional space. The same embeddings are also passed into <strong>sequential models</strong> that can process sequential data. Those models are used to build context and, through an ingenious way, attend to parts of the input sentence that are useful to the output sentence in <strong>translation</strong>.</p>\n\n\n\n<p>Phew! That was a lot of research. <em>We are almost something of a scientist ourselves</em>.</p>\n\n\n\n<p>But what lies ahead? A group of <em>real</em> scientists got together to answer that question and formulate a genius plan (as shown in <strong>Figure 1</strong>) that would shake the field of Deep Learning to its very core.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/board-room-attention.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34457\" width=\"700\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?size=126x90&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention-300x214.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?size=378x270&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?size=504x360&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?size=630x450&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/board-room-attention.png?lossy=1&strip=1&webp=1 700w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 1:</strong> A meme on attention (image by the authors).</figcaption></figure></div>\n\n\n<p>In this tutorial, you will learn about the evolution of the attention mechanism that led to the seminal architecture of Transformers.</p>\n\n\n\n<p>This lesson is the 1st in a 3-part series on <strong>NLP 104</strong>:</p>\n\n\n\n<ol><li><a href=\"https://pyimg.co/8kdj1\" target=\"_blank\" rel=\"noreferrer noopener\"><strong><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 1</em></strong></a><strong> (today’s tutorial)</strong></li><li><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 2</em></li><li><em>A Deep Dive into Transformers with TensorFlow and Keras: Part 3</em></li></ol>\n\n\n\n<p><strong>To learn how the attention mechanism evolved into the Transformer architecture, </strong><strong><em>just keep reading.</em></strong></p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2BPTitle\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>A Deep Dive into Transformers with TensorFlow and Keras: Part 1</strong></a></h2>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Introduction\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Introduction</strong></a></h3>\n\n\n\n<p>In our <a href=\"https://pyimagesearch.com/2022/08/22/neural-machine-translation-with-bahdanaus-attention-using-tensorflow-and-keras/\" target=\"_blank\" rel=\"noreferrer noopener\">previous blog post</a>, we covered Neural Machine Translation models based on Recurrent Neural Network architectures that include an <strong>encoder</strong> and a <strong>decoder</strong>. In addition, to facilitate better learning, we also introduce the <strong>attention module</strong>.</p>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> proposed a simple yet effective change to the Neural Machine Translation models. An excerpt from the paper best describes their proposal.</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p>We propose a new simple network architecture, the <strong>Transformer</strong>, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p></blockquote>\n\n\n\n<p>In today’s tutorial, we will cover the <strong>theory</strong> behind this neural network architecture called the Transformer. We will focus on the following in this tutorial:</p>\n\n\n\n<ul><li>The Transformer Architecture<ul><li>Encoder</li><li>Decoder</li></ul></li><li>Evolution of Attention<ul><li>Version 0</li><li>Version 1</li><li>Version 2</li><li>Version 3</li><li>Version 4 (Cross-Attention)</li><li>Version 5 (Self-Attention)</li><li>Version 6 (Multi-Head Attention)</li></ul></li></ul>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Architecture\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>The Transformer Architecture</strong></a></h3>\n\n\n\n<p>We take a top-down approach in building the intuitions behind the Transformer architecture. Let us first look at the entire architecture and break down individual components later.</p>\n\n\n\n<p>The Transformer consists of two individual modules, namely the <strong>Encoder</strong> and the <strong>Decoder</strong>, as shown in <strong>Figure 2</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/entire-architecture.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34458\" width=\"353\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/entire-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 353px) 100vw, 353px\" /></a><figcaption><strong>Figure 2:</strong> The entire Transformer architecture (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4Encoder\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Encoder</strong></a></h4>\n\n\n\n<p>As shown in <strong>Figure 3</strong>, the <strong>encoder</strong> is a stack of <img src='https://929687.smushcdn.com/2633864/wp-content/latex/8d9/8d9c307cb7f3c4a32822a51922d1ceaa-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='N' title='N' class='latex' /> identical layers. Each layer is composed of <em>two</em> sub-layers.</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p>The first is a <strong>multi-head self-attention mechanism</strong>, and the second is a simple, <strong>position-wise, fully connected feed-forward network</strong>.</p></blockquote>\n\n\n\n<p>The authors also employ residual connections (red lines) and a normalization operation around the two sub-layers.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/ecoder-architecture.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34460\" width=\"353\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/ecoder-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 353px) 100vw, 353px\" /></a><figcaption><strong>Figure 3:</strong> The <strong>encoder</strong> in the Transformer (image by the authors).</figcaption></figure></div>\n\n\n<p>The source tokens are first <strong>embedded</strong> into a high-dimensional space. The input embeddings are added with <strong>positional encoding</strong> (we will cover positional encodings in depth later in the tutorial series). The summed embeddings are then fed into the encoder.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h4Decoder\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Decoder</strong></a></h4>\n\n\n\n<p>As shown in <strong>Figure 4</strong>, the <strong>decoder</strong> is a stack of <img src='https://929687.smushcdn.com/2633864/wp-content/latex/8d9/8d9c307cb7f3c4a32822a51922d1ceaa-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='N' title='N' class='latex' /> identical layers. Each layer is composed of <em>three</em> sub-layers.</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.</p></blockquote>\n\n\n\n<p>The decoder also has residual connections and a normalization operation around the three sub-layers.</p>\n\n\n\n<p>Notice that the first sublayer of the decoder is a <strong>masked</strong> multi-head attention layer instead of a multi-head attention layer.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/decoder-architecture.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-723x1024.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34462\" width=\"353\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture.png?size=126x178&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-212x300.png?lossy=1&strip=1&webp=1 212w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-723x1024.png?lossy=1&strip=1&webp=1 723w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-768x1088.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-1084x1536.png?lossy=1&strip=1&webp=1 1084w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/decoder-architecture-1446x2048.png?lossy=1&strip=1&webp=1 1446w\" sizes=\"(max-width: 353px) 100vw, 353px\" /></a><figcaption><strong>Figure 4:</strong> The <strong>decoder</strong> in the Transformer (image by the authors).</figcaption></figure></div>\n\n\n<p>The target tokens are offset by one. Like the encoder, the tokens are first <strong>embedded</strong> into a high-dimensional space. The embeddings are then added with <strong>positional encodings</strong>. The summed embeddings are then fed into the decoder.</p>\n\n\n\n<blockquote class=\"wp-block-quote\"><p><br>This <strong>masking</strong>, combined with the fact that the target tokens are offset by one position, ensures that the predictions for position <img src='https://929687.smushcdn.com/2633864/wp-content/latex/865/865c0c0b4ab0e063e5caa3387c1a8741-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='i' title='i' class='latex' /> can depend only on the known outputs at positions less than <img src='https://929687.smushcdn.com/2633864/wp-content/latex/865/865c0c0b4ab0e063e5caa3387c1a8741-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='i' title='i' class='latex' />.</p></blockquote>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Evolution\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Evolution of Attention</strong></a></h3>\n\n\n\n<p>The encoder and decoder have been built around a central piece called the Multi-Head Attention module. This piece of the architecture is the formula X that has placed Transformers at the top of the Deep Learning food chain. But Multi-Head Attention (MHA) did not always exist in its present form.</p>\n\n\n\n<p>We have studied a very basic form of attention in the prior blog posts covering the Bahdanau and Luong attentions. However, the journey from the early form of attention to the one that is actually used in the Transformers architecture is long and full of <strong>monstrous</strong> notations.</p>\n\n\n\n<p>But do not fear. Our quest will be to navigate the different versions of attention and counter any problems we might face. At the end of our journey, we shall emerge with an intuitive understanding of how attention works in the Transformer architecture.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h4V0\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 0</strong></a></h4>\n\n\n\n<p>To understand the intuition of attention, we start with an <strong>input</strong> and a <strong>query</strong>. Then, we attend to parts of the input based on the query. So if you have an image of a landscape and someone asks you to decipher the weather there, you would attend to the sky first. The image is the input, while the query is “how is the weather there?”</p>\n\n\n\n<p>In terms of computation, attention is given to parts of the <strong>input matrix</strong> which is similar to the <strong>query vector</strong>. We compute the similarity between the input matrix and the query vector. After we obtain the <strong>similarity score</strong>, we transform the input matrix into an output vector. The output vector is the <strong>weighted summation (or average)</strong> of the input matrix.</p>\n\n\n\n<p>Intuitively the weighted summation (or average) should be richer in representation than the original input matrix. It includes the &#8220;where and what to attend to.&#8221; The diagram of this baseline version (version 0) is shown in <strong>Figure 5</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v0-input-1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1-1024x313.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34535\" width=\"650\" height=\"200\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1.png?size=126x39&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1-300x92.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1.png?size=378x116&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1.png?size=504x155&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1.png?size=630x194&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1-768x235.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1-1024x313.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-input-1.png?lossy=1&strip=1&webp=1 1511w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 0 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> <img src='https://929687.smushcdn.com/2633864/wp-content/latex/6ed/6edd2d8d7e9f15b57987b504fd4dc9a8-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='f_\\text{att}' title='f_\\text{att}' class='latex' />, which is a **feed-forward network**. The feed-forward network takes the query and input, and projects both of them to dimension <img src='https://929687.smushcdn.com/2633864/wp-content/latex/d10/d1045a0cd40aaa2adbfa3492d2a1d8a6-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='D_E' title='D_E' class='latex' />.</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v0-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output-1024x371.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34521\" width=\"650\" height=\"235\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output.png?size=126x46&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output-300x109.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output.png?size=378x137&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output.png?size=504x182&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output.png?size=630x228&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output-768x278.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output-1024x371.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output-1536x556.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v0-output.png?lossy=1&strip=1&webp=1 1757w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 0 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-0.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0-1024x322.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34464\" width=\"700\" height=\"253\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0.png?size=140x51&lossy=1&strip=1&webp=1 140w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0.png?size=280x101&lossy=1&strip=1&webp=1 280w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0.png?size=420x152&lossy=1&strip=1&webp=1 420w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0.png?size=560x202&lossy=1&strip=1&webp=1 560w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-0.png?size=700x253&lossy=1&strip=1&webp=1 700w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 5:</strong> Baseline version of attention with a feed-forward network as a similarity function (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V1\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 1</strong></a></h4>\n\n\n\n<blockquote class=\"wp-block-quote\"><p>The two most commonly used attention functions are <strong>additive attention</strong> and <strong>dot-product (multiplicative) attention</strong>. Additive attention computes the compatibility function using a feed-forward network. </p></blockquote>\n\n\n\n<p>The first change we make to the mechanism is swapping out the feed-forward network with a dot product operation. Turns out that this is highly efficient with reasonably good results. While we use the dot product, notice how the shape of the input vectors now changes to incorporate the dot product. The diagram of version 1 is shown in <strong>Figure 6</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v1-input.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input-1024x289.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34523\" width=\"650\" height=\"183\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input.png?size=126x35&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input-300x85.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input.png?size=378x106&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input.png?size=504x142&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input.png?size=630x177&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input-768x217.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input-1024x289.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-input.png?lossy=1&strip=1&webp=1 1496w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 1 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> Dot Product</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v1-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output-1024x324.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34524\" width=\"650\" height=\"206\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output.png?size=126x40&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output-300x95.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output.png?size=378x120&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output.png?size=504x160&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output.png?size=630x200&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output-768x243.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output-1024x324.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output-1536x486.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v1-output.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 1 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-1024x330.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34475\" width=\"700\" height=\"225\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1.png?size=126x41&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-300x97.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1.png?size=378x122&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1.png?size=504x162&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1.png?size=630x203&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-768x247.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-1024x330.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-1536x494.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-1-2048x659.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 6: </strong>Version 1 with a dot product as a similarity function (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V2\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 2</strong></a></h4>\n\n\n\n<p>In this section, we will discuss a very important concept realized in the paper. The authors propose &#8220;<strong>scaled dot product</strong>&#8221; instead of &#8220;<strong>normal dot product</strong>&#8221; as the similarity function. The scaled dot product is exactly the same as the dot product, but scaled with a factor of <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e4a/e4aed554f3f385d7f60d910d7eca949e-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\displaystyle\\frac{1}{\\sqrt{\\text{dim}}}' title='\\displaystyle\\frac{1}{\\sqrt{\\text{dim}}}' class='latex' />.</p>\n\n\n\n<p>Here let us pose some problems and devise the solutions ourselves. The scaling factor will be hidden inside the solution.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h5Problems\"/>\n\n\n\n<h5><a href=\"#TOC\"><strong>Problems</strong></a></h5>\n\n\n\n<ul><li><strong>Vanishing Gradient Problem:</strong> The weights of a Neural Network update in proportion to the gradient of the loss. The problem is that, in some cases, the gradient will be small, effectively preventing the weight from changing its value at all. This, in turn, prohibits the network from learning any further. This is often referred to as the vanishing gradient problem.</li><li><strong>Unnormalized softmax:</strong> Consider a normal distribution. The softmax of the distribution is heavily dependent on its <strong>standard deviation</strong>. With a huge standard deviation, the softmax will result in a peak with zeros all around. <strong>Figures 7-10</strong> help visualize the problem.</li></ul>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/unnormalized-softmax-1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1-1024x608.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34477\" width=\"650\" height=\"386\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1.png?size=126x75&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1-300x178.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1.png?size=378x224&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1.png?size=504x299&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1.png?size=630x374&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1-768x456.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1-1024x608.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-1.png?lossy=1&strip=1&webp=1 1058w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 7:</strong> Creating a normal distribution with a mean of 0 and a standard deviation of 100 (image by the authors).</figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/hist-1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/hist-1.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34479\" width=\"500\" height=\"323\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/hist-1.png?size=100x65&lossy=1&strip=1&webp=1 100w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/hist-1.png?size=200x129&lossy=1&strip=1&webp=1 200w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/hist-1-300x194.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/hist-1.png?lossy=1&strip=1&webp=1 384w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><figcaption><strong>Figure 8: </strong>The histogram of the normal distribution (image by the authors).</figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/unnormalized-softmax-2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2-1024x450.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34481\" width=\"650\" height=\"286\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2.png?size=126x55&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2-300x132.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2.png?size=378x166&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2.png?size=504x222&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2.png?size=630x277&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2-768x338.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2-1024x450.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-2.png?lossy=1&strip=1&webp=1 1042w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 9:</strong> Plot the softmax of the distribution (image by the authors).</figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/softmax-1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-1.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34483\" width=\"500\" height=\"333\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-1.png?size=100x67&lossy=1&strip=1&webp=1 100w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-1.png?size=200x133&lossy=1&strip=1&webp=1 200w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-1-300x200.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-1.png?lossy=1&strip=1&webp=1 372w\" sizes=\"(max-width: 500px) 100vw, 500px\" /></a><figcaption><strong>Figure 10:</strong> Softmax visualization (image by the authors).</figcaption></figure></div>\n\n\n<ul><li><strong>Unnormalized softmax leading to the vanishing gradient:</strong> Consider if your logits pass through softmax and then we have a loss (cross-entropy). The errors that backpropagate will be dependent on the softmax output.<br><br>Now assume that you have an unnormalized softmax function, as mentioned above. The error corresponding to the <strong>peak</strong> will definitely be back-propagated, while the others (corresponding to zeros in the softmax) will not flow at all. This gives rise to the vanishing gradient problem.</li></ul>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h5Solution\"/>\n\n\n\n<h5><a href=\"#TOC\"><strong>Solution</strong></a></h5>\n\n\n\n<p>To counter the problem of vanishing gradients due to unnormalized softmax, we need to find a way to have a better softmax output.</p>\n\n\n\n<p>It turns out that the standard deviation of a distribution largely influences the softmax output. Let’s create a normal distribution with a standard deviation of 100. We also scale the distribution so that the standard deviation is unity. The code to create the distribution and scale it can be found in <strong>Figure 11</strong>. <strong>Figure 12</strong> visualizes the histograms of the distributions.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/unnormalized-softmax-solution.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution-1024x734.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34486\" width=\"650\" height=\"466\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution.png?size=126x90&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution-300x215.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution.png?size=378x271&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution.png?size=504x361&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution.png?size=630x452&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution-768x551.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution-1024x734.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/unnormalized-softmax-solution.png?lossy=1&strip=1&webp=1 1230w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 11:</strong> Create a normal distribution with a mean of 0 and a standard deviation of 100. Scale the distribution to 1 standard deviation. (image by the authors)</figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/un-softmax-sol-diagram.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram-1024x683.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34488\" width=\"700\" height=\"467\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram.png?size=126x84&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram-300x200.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram.png?size=378x252&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram.png?size=504x336&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram.png?size=630x420&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram-768x512.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram-1024x683.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram-1536x1024.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/un-softmax-sol-diagram.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 12:</strong> Visualize both distributions (image by the authors).</figcaption></figure></div>\n\n\n<p>The histograms of both distributions seem alike. One is the scaled version of the other (look at the <em>x</em>-axis).</p>\n\n\n\n<p>Let’s calculate the softmax of both and visualize them as shown in <strong>Figures 13 and 14</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/softmax-std.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34490\" width=\"600\" height=\"391\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std.png?size=126x82&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-300x196.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std.png?size=378x246&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std.png?size=504x328&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-768x501.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std.png?lossy=1&strip=1&webp=1 938w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Figure 13:</strong> Apply softmax to both distributions (image by the authors).</figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/softmax-std-diagram.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram-1024x683.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34493\" width=\"700\" height=\"467\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram.png?size=126x84&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram-300x200.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram.png?size=378x252&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram.png?size=504x336&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram.png?size=630x420&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram-768x512.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram-1024x683.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram-1536x1024.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/softmax-std-diagram.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 14:</strong> Visualize the softmax of both distributions (image by the authors).</figcaption></figure></div>\n\n\n<p><strong>Scaling</strong> the distribution to unit standard deviation provides a distributed softmax output. This softmax allows the gradients to backpropagate, saving our model from <strong>collapsing</strong>.</p>\n\n\n\n<p><strong>Scaling of the Dot Product</strong></p>\n\n\n\n<p>We came across the <em>vanishing gradient problem</em>, the <em>unnormalized softmax output</em>, and also a way we can <em>counter</em> it. We are yet to understand the relationship between the above-mentioned problems and solutions to that of the scaled dot product proposed by the authors.</p>\n\n\n\n<p>The attention layers consist of a similarity function that takes two vectors and performs a dot product. This dot product is then passed through a softmax to create the attention weights. This recipe is perfect for a vanishing gradient problem. The way to counter the problem is to transform the dot product result into a unit standard deviation distribution.</p>\n\n\n\n<p>Let us assume that we have two independent and randomly distributed variables: <img src='https://929687.smushcdn.com/2633864/wp-content/latex/0cc/0cc175b9c0f1b6a831c399e269772661-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='a' title='a' class='latex' /> and <img src='https://929687.smushcdn.com/2633864/wp-content/latex/92e/92eb5ffee6ae2fec3ad71c777531578f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='b' title='b' class='latex' />, as shown in <strong>Figure 15</strong>. Both vectors have a mean of 0 and a standard deviation of 1. </p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/scaling-dot-product.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product-1024x854.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34495\" width=\"600\" height=\"500\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product.png?size=126x105&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product-300x250.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product.png?size=378x315&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product.png?size=504x420&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product-768x640.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product-1024x854.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/scaling-dot-product.png?lossy=1&strip=1&webp=1 1058w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></a><figcaption><strong>Figure 15:</strong> Build the random variables (image by the authors).</figcaption></figure></div>\n\n\n<p>What is interesting here is that the mean of such a dot product remains to be 0 regardless of the size of the random variables, but the variance and, in turn, the standard deviation are directly proportional to the size of the random variables. To be specific, variance is linearly proportional while standard deviation is proportional with the factor of <img src='https://929687.smushcdn.com/2633864/wp-content/latex/513/513fecd3e58aeeadae6364937dff1a3f-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\sqrt{\\text{dim}}' title='\\sqrt{\\text{dim}}' class='latex' />.</p>\n\n\n\n<p>To prohibit the dot product from a vanishing gradient problem, the authors scale the dot product with the <img src='https://929687.smushcdn.com/2633864/wp-content/latex/e4a/e4aed554f3f385d7f60d910d7eca949e-ffffff-000000-0.png?lossy=1&strip=1&webp=1' alt='\\displaystyle\\frac{1}{\\sqrt{\\text{dim}}}' title='\\displaystyle\\frac{1}{\\sqrt{\\text{dim}}}' class='latex' /> factor. This, in turn, is the scaled dot product that the authors have suggested in the paper. The visualization of Version 2 is shown in <strong>Figure 16</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v2-input.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input-1024x306.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34525\" width=\"650\" height=\"194\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input.png?size=126x38&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input-300x90.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input.png?size=378x113&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input.png?size=504x150&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input.png?size=630x188&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input-768x230.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input-1024x306.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-input.png?lossy=1&strip=1&webp=1 1492w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 2 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> Dot Product</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v2-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output-1024x329.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34526\" width=\"700\" height=\"225\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output.png?size=126x41&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output-300x96.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output.png?size=378x122&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output.png?size=504x162&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output.png?size=630x203&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output-768x247.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output-1024x329.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output-1536x493.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v2-output.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 2 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-1024x330.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34498\" width=\"700\" height=\"226\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2.png?size=126x41&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-300x97.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2.png?size=378x122&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2.png?size=504x163&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2.png?size=630x203&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-768x247.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-1024x330.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-1536x494.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-2-2048x659.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 16:</strong> Version 2 with scaled dot product as a similarity function (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V3\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 3</strong></a></h4>\n\n\n\n<p>Previously we looked at a <strong>single</strong> query vector. Let us scale this implementation to <strong>multiple</strong> query vectors. We calculate the similarities of the input matrix with all the query vectors (query matrix) we have. The visualization of Version 3 is shown in <strong>Figure 17</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v3-input.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input-1024x275.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34527\" width=\"650\" height=\"175\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input.png?size=126x34&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input-300x81.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input.png?size=378x102&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input.png?size=504x136&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input.png?size=630x170&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input-768x206.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input-1024x275.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input-1536x412.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-input.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 3 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> Dot Product</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v3-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output-1024x285.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34528\" width=\"700\" height=\"195\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output.png?size=126x35&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output-300x84.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output.png?size=378x105&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output.png?size=504x140&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output.png?size=630x176&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output-768x214.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output-1024x285.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output-1536x428.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v3-output.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 3 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-1024x415.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34502\" width=\"700\" height=\"284\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3.png?size=126x51&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-300x122.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3.png?size=378x153&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3.png?size=504x204&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3.png?size=630x256&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-768x312.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-1024x415.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-1536x623.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-3-2048x831.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 17:</strong> Version 3 having multiple query vectors (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V4\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 4 (Cross-Attention)</strong></a></h4>\n\n\n\n<p>To build cross-attention, we make some changes. The changes are specific to the input matrix. As we already know, attention needs an input matrix and a query matrix. Suppose we projected the input matrix into a pair of matrices, namely the <strong>key</strong> and <strong>value</strong> matrices.</p>\n\n\n\n<p>The key matrix is attended to with respect to the query matrix. This results in attention weights. Here the value matrix is transformed with the attention weights as opposed to the input matrix transformation, as seen earlier.</p>\n\n\n\n<p>This is done to <strong>decouple</strong> the complexity. The input matrix can now have a better projection that takes care of building attention weights and better output matrices as well. The visualization of Cross Attention is shown in <strong>Figure 18</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v4-input.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input-1024x331.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34529\" width=\"700\" height=\"227\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input.png?size=126x41&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input-300x97.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input.png?size=378x123&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input.png?size=504x163&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input.png?size=630x204&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input-768x248.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input-1024x331.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input-1536x496.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-input.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 4 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> Dot Product</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v4-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output-1024x272.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34530\" width=\"700\" height=\"186\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output.png?size=126x33&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output-300x80.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output.png?size=378x100&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output.png?size=504x134&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output.png?size=630x167&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output-768x204.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output-1024x272.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output-1536x409.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v4-output.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 4 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-1024x497.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34505\" width=\"700\" height=\"340\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4.png?size=126x61&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-300x146.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4.png?size=378x184&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4.png?size=504x245&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4.png?size=630x306&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-768x373.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-1024x497.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-1536x745.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-4-2048x994.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 18: </strong>Version 4, input decoupled into key and value (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V5\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 5 (Self-Attention)</strong></a></h4>\n\n\n\n<p>With cross-attention, we learned that there are three matrices in the attention module: key, value, and query. The key and value matrix are projected versions of the input matrix. What if the query matrix also was projected from the input?</p>\n\n\n\n<p>This results in what we call self-attention. Here the main motivation is to build a richer implementation of self with respect to self. This sounds funny, but it is highly important and forms the basis of the Transformer architecture. The visualization of Self-Attention is shown in <strong>Figure 19</strong>.</p>\n\n\n\n<p><strong>Inputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v5-input.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input-1024x329.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34531\" width=\"700\" height=\"225\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input.png?size=126x41&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input-300x97.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input.png?size=378x122&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input.png?size=504x162&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input.png?size=630x203&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input-768x247.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input-1024x329.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input-1536x494.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-input.png?lossy=1&strip=1&webp=1 1800w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 5 Input</strong></figcaption></figure></div>\n\n\n<p><strong>Similarity function:</strong> Dot Product</p>\n\n\n\n<p><strong>Outputs:</strong></p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/v5-output.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output-1024x294.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34532\" width=\"700\" height=\"201\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output.png?size=126x36&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output-300x86.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output.png?size=378x109&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output.png?size=504x145&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output.png?size=630x181&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output-768x221.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output-1024x294.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output-1536x442.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/v5-output.png?lossy=1&strip=1&webp=1 1760w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Version 5 Output</strong></figcaption></figure></div>\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/evolution-Version-5.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-1024x497.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34508\" width=\"700\" height=\"339\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5.png?size=126x61&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-300x146.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5.png?size=378x183&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5.png?size=504x244&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5.png?size=630x305&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-768x373.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-1024x497.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-1536x745.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/evolution-Version-5-2048x994.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong>Figure 19:</strong> Version 5, self-attention, where input is broken into key, query, and value (image by the authors).</figcaption></figure></div>\n\n\n<hr class=\"wp-block-separator\" id=\"h4V6\"/>\n\n\n\n<h4><a href=\"#TOC\"><strong>Version 6 (Multi-Head Attention)</strong></a></h4>\n\n\n\n<p>This is the last stage of evolution. We have come a long way. We started by building the intuition of attention, and now we will discuss multi-head (self) attention.</p>\n\n\n\n<p>The authors wanted to decouple relations further by introducing multiple heads of attention. This means that the key, value, and query matrices are now split into a number of heads and projected. The individual splits are then passed into a (self) attention module (described above).</p>\n\n\n\n<p>All the splits are then concatenated into a single representation. The visualization of Multi-Head Attention is shown in <strong>Figure 20</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://pyimagesearch.com/wp-content/uploads/2022/09/mha.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-1024x516.png?lossy=1&strip=1&webp=1\" alt=\"\" class=\"wp-image-34511\" width=\"700\" height=\"353\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha.png?size=126x64&lossy=1&strip=1&webp=1 126w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-300x151.png?lossy=1&strip=1&webp=1 300w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha.png?size=378x191&lossy=1&strip=1&webp=1 378w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha.png?size=504x254&lossy=1&strip=1&webp=1 504w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha.png?size=630x318&lossy=1&strip=1&webp=1 630w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-768x387.png?lossy=1&strip=1&webp=1 768w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-1024x516.png?lossy=1&strip=1&webp=1 1024w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-1536x774.png?lossy=1&strip=1&webp=1 1536w, https://929687.smushcdn.com/2633864/wp-content/uploads/2022/09/mha-2048x1032.png?lossy=1&strip=1&webp=1 2048w\" sizes=\"(max-width: 630px) 100vw, 630px\" /></a><figcaption><strong><strong>Figure 20:</strong> </strong>Final version, multi-head attention as devised by <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Vaswani et al.</a> (image by the authors).</figcaption></figure></div>\n\n\n<p>If you have come this far, take a pause and congratulate yourselves. The journey has been long and filled with monstrous notations and <strong>numerous</strong> matrix multiplications. But as promised, we now have an intuitive sense of how Multi-Head Attention evolved. To recap:</p>\n\n\n\n<ul><li>Version 0 started with the baseline, where the similarity function is computed between an input and a query using a feed-forward network.</li><li>Version 1 saw us swap that feed-forward network for a simple dot product.</li><li>Due to problems like vanishing gradients and unnormalized probability distribution, we use a scaled dot product in Version 2.</li><li>In Version 3, we use multiple query vectors rather than just one.</li><li>In Version 4, we build the cross-attention layer by breaking the input vector into key and value matrices.</li><li><em>Whatever is found outside can also be found inside</em>. Thus in Version 5, we obtain the query vector from the input as well, calling this the self-attention layer.</li><li>Version 6 is the last and final form, where we see all relations between query, key, and value being further decoupled by using multiple heads.</li></ul>\n\n\n\n<p>Transformers might have multiple heads, but we have only one, and if it is spinning right now, we do not blame you. Here is an interactive demo to visually recap whatever we have learned thus far.</p>\n\n\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSUbwBhjdqckhZ0gt3n9Ayml7Fk81_nN6bnZ-9aiYgG0pNMfmd6cpUcGbfQgyjfVzomJsXSfZHe7FYp/embed?start=true&loop=true&delayms=60000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\n\n\n\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<div id=\"pitch\" style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>What's next? I recommend <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\">PyImageSearch University</a>.</h3>\n\n\t<script src=\"https://fast.wistia.com/embed/medias/kno0cmko2z.jsonp\" async></script><script src=\"https://fast.wistia.com/assets/external/E-v1.js\" async></script><div class=\"wistia_responsive_padding\" style=\"padding:56.25% 0 0 0;position:relative;\"><div class=\"wistia_responsive_wrapper\" style=\"height:100%;left:0;position:absolute;top:0;width:100%;\"><div class=\"wistia_embed wistia_async_kno0cmko2z videoFoam=true\" style=\"height:100%;position:relative;width:100%\"><div class=\"wistia_swatch\" style=\"height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;\"><img src=\"https://fast.wistia.com/embed/medias/kno0cmko2z/swatch\" style=\"filter:blur(5px);height:100%;object-fit:contain;width:100%;\" alt=\"\" aria-hidden=\"true\" onload=\"this.parentNode.style.opacity=1;\" /></div></div></div></div>\n\n\t<div style=\"margin-top: 32px; margin-bottom: 32px; \">\n\t\t<strong>Course information:</strong><br/>\n\t\t53+ total classes • 57+ hours of on-demand code walkthrough videos • Last updated: October 2022<br/>\n\t\t<span style=\"color: #169FE6;\">★★★★★</span> 4.84 (128 Ratings) • 15,800+ Students Enrolled\n\t</div>\n\n\t<p><strong>I strongly believe that if you had the right teacher you could <em>master</em> computer vision and deep learning.</strong></p>\n\n\t<p>Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?</p>\n\n\t<p>That’s <em>not</em> the case.</p>\n\n\t<p>All you need to master computer vision and deep learning is for someone to explain things to you in <em>simple, intuitive</em> terms. <em>And that’s exactly what I do</em>. My mission is to change education and how complex Artificial Intelligence topics are taught.</p>\n\n\t<p>If you're serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to <em>successfully</em> and <em>confidently</em> apply computer vision to your work, research, and projects. Join me in computer vision mastery.</p>\n\n\t<p><strong>Inside PyImageSearch University you'll find:</strong></p>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ courses</strong> on essential computer vision, deep learning, and OpenCV topics</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>53+ Certificates</strong> of Completion</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>57+ hours</strong> of on-demand video</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Brand new courses released <em>regularly</em></strong>, ensuring you can keep up with state-of-the-art techniques</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Pre-configured Jupyter Notebooks in Google Colab</strong></li>\n\t\t<li style=\"list-style: none;\">&check; Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)</li>\n\t\t<li style=\"list-style: none;\">&check; Access to <strong>centralized code repos for <em>all</em> 450+ tutorials</strong> on PyImageSearch</li>\n\t\t<li style=\"list-style: none;\">&check; <strong> Easy one-click downloads</strong> for code, datasets, pre-trained models, etc.</li>\n\t\t<li style=\"list-style: none;\">&check; <strong>Access</strong> on mobile, laptop, desktop, etc.</li>\n\t</ul>\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next%3F%20I%20recommend\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to join PyImageSearch University</a>\n\t</p>\n</div>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h2Summary\"/>\n\n\n\n<h2><a href=\"#TOC\"><strong>Summary</strong></a></h2>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noreferrer noopener\">Attention is all you need</a>, was published in the year 2017. Since then, it has absolutely revolutionized Deep Learning. Almost all tasks and novel architectures have leveraged Transformers as a whole or in parts. </p>\n\n\n\n<p>The novelty of the architecture stands out when we study the evolution of the attention mechanism rather than singularly focusing on the version used in the paper. </p>\n\n\n\n<p>This tutorial focused on developing this central piece: the Multi-Head Attention layer. In upcoming tutorials, we will learn about the connecting wires (feed-forward layers, positional encoding, and others) that hold the architecture together and also how to code the architecture in TensorFlow and Keras.</p>\n\n\n\n<hr class=\"wp-block-separator\" id=\"h3Citation\"/>\n\n\n\n<h3><a href=\"#TOC\"><strong>Citation Information</strong></a></h3>\n\n\n\n<p><strong>A. R. Gosthipaty and R. Raha. </strong>“A Deep Dive into Transformers with TensorFlow and Keras: Part 1,” <em>PyImageSearch</em>, P. Chugh, S. Huot, K. Kidriavsteva, and A. Thanki, eds., 2022, <a href=\"https://pyimg.co/8kdj1\" target=\"_blank\" rel=\"noreferrer noopener\">https://pyimg.co/8kdj1</a> </p>\n\n\n\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"raw\" data-enlighter-theme=\"classic\" data-enlighter-highlight=\"\" data-enlighter-linenumbers=\"false\" data-enlighter-lineoffset=\"\" data-enlighter-title=\"\" data-enlighter-group=\"\">@incollection{ARG-RR_2022_DDTFK1,\n  author = {Aritra Roy Gosthipaty and Ritwik Raha},\n  title = {A Deep Dive into Transformers with {TensorFlow} and {K}eras: Part 1},\n  booktitle = {PyImageSearch},\n  editor = {Puneet Chugh and Susan Huot and Kseniia Kidriavsteva and Abhishek Thanki},\n  year = {2022},\n  note = {https://pyimg.co/8kdj1},\n}</pre>\n\n\n\n<hr class=\"wp-block-separator has-css-opacity\"/>\n\n\n\n<div style=\"padding: 40px; width: 100%; background-color: #F4F6FA;\">\n\t<h3>Want free GPU credits to train models?</h3>\n\n\t<ul style=\"margin-left: 0px;\">\n\t\t<li style=\"list-style: none;\">We used <a target=\"_blank\" href=\"https://cloud.jarvislabs.ai/\">Jarvislabs.ai</a>, a GPU cloud, for all the experiments.</li>\n\t\t<li style=\"list-style: none;\">We are proud to offer PyImageSearch University students $20 worth of Jarvislabs.ai GPU cloud credits. Join PyImageSearch University and claim your $20 credit <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">here</a>.</li>\n\t</ul>\n\n\n\t<p>In Deep Learning, we need to train Neural Networks. These Neural Networks can be trained on a CPU but take a lot of time. Moreover, sometimes these networks do not even fit (run) on a CPU.</p>\n\n\t<p>To overcome this problem, we use <strong>GPUs</strong>.  The problem is these GPUs are <strong>expensive</strong> and become outdated quickly. </p>\n\n\t<p>GPUs are great because they take your Neural Network and train it quickly.  The problem is that GPUs are expensive, so you don’t want to buy one and use it only occasionally.  Cloud GPUs let you use a GPU and <strong>only pay for the time you are running the GPU</strong>.  It’s a brilliant idea that saves you money.</p>\n\n\t<p><strong>JarvisLabs</strong> provides the best-in-class GPUs, and <strong>PyImageSearch University students</strong> get between 10-50 hours on a world-class GPU (time depends on the specific GPU you select).</p>\n\n\n\t<p>This gives you a chance to <strong>test-drive a monstrously powerful GPU</strong> on any of our tutorials in a jiffy. So join <a target=\"_blank\" href=\"https://pyimagesearch.com/pyimagesearch-university/\">PyImageSearch University</a> today and try it for yourself.</p>\n\n\n\t<p style=\"text-align: center;\">\n\t\t<a target=\"_blank\" class=\"button link\" href=\"https://pyimagesearch.com/pyimagesearch-university/\" style=\"background-color: #6DC713; border-bottom: none;\">Click here to get Jarvislabs credits now</a>\n\t</p>\n</div>\n\n\n\n<p><strong> </strong><strong><div id=\"download-the-code\" class=\"post-cta-wrap\">\n<div class=\"gpd-post-cta\">\n\t<div class=\"gpd-post-cta-content\">\n\t\t\n\n\t\t\t<div class=\"gpd-post-cta-top\">\n\t\t\t\t<div class=\"gpd-post-cta-top-image\"><img src=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1\" alt=\"\" srcset=\"https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1 410w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=126x174&lossy=1&strip=1&webp=1 126w,https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?size=252x348&lossy=1&strip=1&webp=1 252w\" sizes=\"(max-width: 410px) 100vw, 410px\" /></div>\n\t\t\t\t\n\t\t\t\t<div class=\"gpd-post-cta-top-title\"><h4>Join the PyImageSearch Newsletter and Grab My FREE 17-page Resource Guide PDF</h4></div>\n\t\t\t\t<div class=\"gpd-post-cta-top-desc\"><p>Enter your email address below to <strong>join the PyImageSearch Newsletter</strong> and <strong>download my FREE 17-page Resource Guide PDF</strong> on Computer Vision, OpenCV, and Deep Learning.</p></div>\n\n\n\t\t\t</div>\n\n\t\t\t<div class=\"gpd-post-cta-bottom\">\n\t\t\t\t<form class=\"footer-cta\" action=\"https://www.getdrip.com/forms/657075648/submissions\" method=\"post\" target=\"_blank\" data-drip-embedded-form=\"657075648\">\n\t\t\t\t\t<input name=\"fields[email]\" type=\"email\" value=\"\" placeholder=\"Your email address\" class=\"form-control\" />\n\n\t\t\t\t\t<button type=\"submit\">Join the Newsletter!</button>\n\n\t\t\t\t\t<div style=\"display: none;\" aria-hidden=\"true\"><label for=\"website\">Website</label><br /><input type=\"text\" id=\"website\" name=\"website\" tabindex=\"-1\" autocomplete=\"false\" value=\"\" /></div>\n\t\t\t\t</form>\n\t\t\t</div>\n\n\n\t\t\n\t</div>\n\n</div>\n</div></strong></p>\n<p>The post <a rel=\"nofollow\" href=\"https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/\">A Deep Dive into Transformers with TensorFlow and Keras: Part&nbsp;1</a> appeared first on <a rel=\"nofollow\" href=\"https://pyimagesearch.com\">PyImageSearch</a>.</p>\n"
}