{
  "title": "Spark vs. Cassandra (Cyanite) for Metric Timeseries",
  "description": "<p><em>An unsuccessful attempt to scale performance metrics processing by switching from Graphite/R to Spark/Cassandra/Cyanite</em></p>\n\n<p>My typical workflow is to run load tests until the system fails (or discover production failure), extract data from <a href=\"http://graphite.readthedocs.org/\">Graphite</a> (<a href=\"https://github.com/graphite-project/whisper\">Whisper</a> files produced by <a href=\"https://github.com/graphite-project/carbon\">Carbon</a>), import it into <a href=\"http://www.r-project.org/\">R</a>, and then run a battery of statistical tests and visual explorations to find a root cause of the problem. There is a strong need for hi-resolution data in load tests (because systems fail really fast under load) but the tools I use now are not really good at ingesting and processing data with granularity finer than 10 seconds. It’s been a long time since I first tried to escape ‘Graphite world’ but all attempts failed so far and here I’ll tell about another failed attempt.</p>\n\n<p>‘Graphite world’ (<a href=\"https://github.com/graphite-project/carbon\">Carbon</a>), <a href=\"https://github.com/graphite-project/whisper\">Whisper</a>, <a href=\"https://github.com/graphite-project/graphite-web\">Graphite-Web</a>) has a <a href=\"https://github.com/graphite-project/carbon/issues\">lot</a> of <a href=\"https://github.com/graphite-project/graphite-web/issues\">issues</a> and is <a href=\"https://github.com/graphite-project/graphite-web/tags\">moving</a> <a href=\"https://github.com/graphite-project/carbon/tags\">slowly</a>. Feeding metrics with 1s resolution to carbon works for one host but doesn’t work at practical scale. This could be done but needs more hardware than monitored hosts.</p>\n\n<p><a href=\"http://www.r-project.org/\">R</a> has a great number of <a href=\"http://cran.r-project.org/web/views/TimeSeries.html\">timeseries related libraries</a> but it runs on a single core and is limited by available memory on a single box. There are <a href=\"http://cran.r-project.org/web/views/HighPerformanceComputing.html\">packages</a> that allow R to fork several computation processes and move data between them but it’s still limited to a single box.</p>\n\n<p><a href=\"http://cassandra.apache.org/\">Cassandra</a> is said to be <a href=\"http://strataconf.com/big-data-conference-ca-2015/public/schedule/detail/39534\">good</a> at <a href=\"http://www.slideshare.net/jericevans/time-series-data-with-apache-cassandra\">timeseries</a> data. <a href=\"http://spark.apache.org/\">Spark</a> is said to be fast at distributed data processing and there is <a href=\"https://github.com/datastax/spark-cassandra-connector\">spark-cassandra</a> connector. <a href=\"https://github.com/pyr/cyanite/\">Cyanite</a> is a simple app written in <a href=\"http://clojure.org/\">Clojure</a> that ingests metrics in the same format as Carbon does and saves them into <a href=\"https://github.com/pyr/cyanite/blob/master/doc/schema.cql\">simple table</a> in Cassandra. There is a <a href=\"https://github.com/brutasse/graphite-cyanite\">Graphite-Cyanite</a> project that allows to run <a href=\"https://github.com/brutasse/graphite-api\">Graphite-Api</a> on top of Cyanite and use all Graphite-compatible dashboards like <a href=\"http://grafana.org/\">Grafana</a>.</p>\n\n<p>So I wired them all together and tested with one <a href=\"https://collectd.org/\">collectd</a> instance gathering data with 10s interval. It worked OK so I proceeded by adding more hosts and switching to 1s interval. Cyanite ate all cpu and failed. Profiling showed that the default <a href=\"https://github.com/pyr/cyanite/blob/master/src/io/cyanite/path.clj\">in-memory index</a> of metric names is the bottleneck so I installed Elasticsearch and switched Cyanite path store to es-native. This time both Cyanite and Elasticsearch ate all cpu and failed again. It turns out that for every metric update Cyanite tries to send metric name to Elasticsearch <a href=\"https://github.com/pyr/cyanite/issues/96\">(cyanite/#96)</a> which amounts to thousands requests per second. It’s mostly a time waste because metric names don’t change that often. I ended up disabling metric name indexing and data flowed to Cassandra with a little complaints <a href=\"https://github.com/pyr/cyanite/issues/73\">(cyanite/#73)</a>.</p>\n\n<p><a href=\"http://spark.apache.org/\">Spark</a> version was 1.3.1. Released version of <a href=\"https://github.com/datastax/spark-cassandra-connector\">spark-cassandra-connector</a> supported only Spark 1.2 but the master branch was able to work with Spark 1.3.1 (see <a href=\"https://datastax-oss.atlassian.net/browse/SPARKC-98\">SPARKC-98</a> for more details). There are a lot of changes in internal Spark APIs between 1.2 and 1.3 so DataFrames and SQL was not working with connector (<a href=\"https://datastax-oss.atlassian.net/browse/SPARKC-112\">SPARKC-112</a>) but RDD operations were quite usable.</p>\n\n<p>First problem encountered was poor locality of spark jobs caused by missing reverse DNS in my virtual machines where spark and cassandra were running. There is a task <a href=\"https://issues.apache.org/jira/browse/SPARK-5113\">SPARK-5113</a> to help with that but in the mean time if your reverse DNS is broken (as it usually is in intranets) then it’s better to force spark to use only ip addresses for everything (set <code class=\"language-plaintext highlighter-rouge\">SPARK_LOCAL_IP</code>, <code class=\"language-plaintext highlighter-rouge\">SPARK_PUBLIC_DNS</code>, <code class=\"language-plaintext highlighter-rouge\">SPARK_LOCAL_HOSTNAME</code> to an ip address in <code class=\"language-plaintext highlighter-rouge\">spark-env.sh</code>)</p>\n\n<p>Then there was not enough job parallelism. Number of partitions was equal to number of Cassandra hosts (replication factor was 1). Data in <a href=\"https://github.com/pyr/cyanite/blob/master/doc/schema.cql\">cyanite table</a> is partitioned by metric name and there were about 1500 unique names. Default connector setting <code class=\"language-plaintext highlighter-rouge\">spark.cassandra.input.split.size</code> was too large and I lowered it to <code class=\"language-plaintext highlighter-rouge\">5000</code>.</p>\n\n<p>Then I noticed quite high number of context switches and cpu usage of Cassandra process. There were too many queries executed by connector so I increased <code class=\"language-plaintext highlighter-rouge\">spark.cassandra.input.page.row.size</code> to <code class=\"language-plaintext highlighter-rouge\">10000</code>. Each datapoint is a single number and at 1s resolution 10000 points equals to about 3 hours of data for a single metric.</p>\n\n<p>These parameters (split size and page row size) are better to set individually per table because number of partitions, rows per partition, cells per row, cell size are different. Setting these parameters manually in Scala seems ugly because you can’t provide one implicit for ReadConf and not provide for other parameters:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>val rdd = sc.cassandraTable(\"metric\", \"metric\")(CassandraConnector(sc.getConf), \n                                                ReadConf(5000, 10000), \n                                                implicitly[ClassTag[CassandraRow]], \n                                                implicitly[RowReaderFactory[CassandraRow]], \n                                                implicitly[ValidRDDType[CassandraRow]])\n</code></pre></div></div>\n\n<p>Some kind of auto-tuning based on optimal data size per job might be better than global one-size-fit-all defaults.</p>\n\n<p>Job speed increased but Cassandra cpu usage remained quite high. <a href=\"https://perf.wiki.kernel.org/index.php/Tutorial#Live_analysis_with_perf_top\"><code class=\"language-plaintext highlighter-rouge\">perf top</code></a> profiler showed LZ4 decompression routines so I tried disabling sstable compression. Compression level was about 0.3 and increasing data size 3 times was not a big deal. It didn’t help at all. Cassandra was still hogging cpu. <a href=\"http://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html\">JMC</a> profiler showed that Cassandra was reading sstables (quite expected behavior). That prompted me to check if the storage format was good enough for the data.</p>\n\n<p>It takes about 100 bytes to store 1 timestamp and 1 value of type double without compression and about 30 bytes with compression enabled in the <a href=\"https://github.com/pyr/cyanite/blob/master/doc/schema.cql\">table</a>. It’s a lot compared to <a href=\"http://graphite.readthedocs.org/en/latest/whisper.html\">whisper format</a> which takes 12 bytes for that. For each data cell cassandra stores column name and write timestamp. Column names are identical for the majority of cells stored and compression takes care of that on disk but memory overhead is still high (see discussion at <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4175\">CASSANDRA-4175</a>). Saving another timestamp for data cell which has a timestamp inside seems redundant. While it’s possible to set cell write time by <code class=\"language-plaintext highlighter-rouge\">insert ... using timestamp ...</code> this internal timestamp can’t be used for sorting and filtering data.</p>\n\n<p>After all of that I arrived to the point when reading 51000000 datapoints took about 40 seconds on 6 cores. That data was produced by 4 hosts with collectd reporting with 1 second interval for about 10 hours. Number of unique metric names was 1500. Data size on disk with compression enabled was 1.3Gb total.</p>\n\n<p>With the data ready to be processed in Spark RDD I got stuck on what’s next. Usually I’d run positive-only-diff on known counters, filter out flat metrics, calculate different percentiles on moving windows and run Tukey Method to find spikes. But Java/Scala toolbox for time series is almost empty. There is nothing like <a href=\"http://cran.r-project.org/web/packages/xts/index.html\">xts</a>, <a href=\"http://ggplot2.org/\">ggplot2</a>, <a href=\"http://cran.r-project.org/web/packages/TSclust/index.html\">TSclust</a>, <a href=\"https://github.com/robjhyndman/forecast\">forecast</a>. The only thing that I found is a recently started <a href=\"https://github.com/cloudera/spark-timeseries\">spark-timeseries</a></p>\n\n<p>There are tools available beyond Java/Scala ecosystem at the price of cpu cycles spent on another round of data serialization/deserialization (first one is moving data from Cassandra into Spark). <a href=\"http://spark.apache.org/docs/latest/api/python/index.html\">PySpark</a> could give access to <a href=\"http://pandas.pydata.org/\">Pandas</a> and <a href=\"http://ipython.org/\">IPython</a> (with <a href=\"https://issues.apache.org/jira/browse/SPARK-4897\">python 3 support</a> in upcoming Spark 1.4). SparkR is <a href=\"https://issues.apache.org/jira/browse/SPARK-5654\">going to be released</a> in Spark 1.4 too.</p>\n\n<p>I took a several days break between my experiments and then I found that none of my queries work anymore. They either failed or returned empty results. Cassandra was dying with out of memory errors. One log file had a warning about too many tombstones being scanned. The problem was caused by data expiration because I set 1 week TTL and forgot about that. Cassandra <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_about_deletes_c.html\">doesn’t really delete data</a> but places special records (tombstones) to mark it as deleted. Large delete operations cause <a href=\"https://lostechies.com/ryansvihla/2014/10/20/domain-modeling-around-deletes-or-using-cassandra-as-a-queue-even-when-you-know-better/\">known problems</a>. Consequences could be reduced by decreasing <code class=\"language-plaintext highlighter-rouge\">gc_grace_seconds</code> and switching to <a href=\"http://www.datastax.com/dev/blog/datetieredcompactionstrategy\">DateTieredCompactionStrategy</a>.</p>\n\n<p>It turns out that using single Cassandra cell for a single timeseries value is not that efficient both from storage overhead and from cpu cycles lost on doing extra housekeeping. Buffering incoming data and packing several timestamp-value pairs into a single Cassandra cell would be better.</p>",
  "pubDate": "Mon, 11 May 2015 00:00:00 +0000",
  "link": "https://mabrek.github.io/blog/spark-cassandra-timeseries/",
  "guid": "https://mabrek.github.io/blog/spark-cassandra-timeseries/"
}