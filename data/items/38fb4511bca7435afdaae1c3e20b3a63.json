{
  "title": "Denoising Dirty Documents &#8211; Part 10",
  "link": "https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/",
  "comments": "https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/#comments",
  "dc:creator": "Colin Priest",
  "pubDate": "Sun, 01 Nov 2015 08:57:30 +0000",
  "category": [
    "Convolutional Neural Networks",
    "Deep Learning",
    "Image Processing",
    "Kaggle",
    "Machine Learning",
    "Python"
  ],
  "guid": "http://colinpriest.com/?p=562",
  "description": "In my last blog, I explained how to take advantage of an information leakage regarding the repeated backgrounds in Kaggle&#8217;s &#8230;<p><a href=\"https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a></p>",
  "content:encoded": "<p>In my <a href=\"https://colinpriest.com/2015/10/15/denoising-dirty-documents-part-9/\" target=\"_blank\">last blog</a>, I explained how to take advantage of an information leakage regarding the repeated backgrounds in Kaggle&#8217;s Denoising Dirty Documents competition. The result of that process was that we had done a fairly good job of removing the background. But the score from doing this was not good enough to get a good placing. We need to do some processing on the image to improve the score.</p>\n<p>Today we will use an approach that does not require me to do any feature engineering &#8211; <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\">convolutional neural networks</a>, which are neural networks where the first few layers repeatedly apply the same weights across overlapping regions of the input data. One intuitive way of thinking about this is that it is like applying an edge detection filter (much like I described <a href=\"https://colinpriest.com/2015/08/21/denoising-dirty-documents-part-4/\">here</a>) where the algorithm finds the appropriate weights for several different edge filters.</p>\n<p><img src=\"https://i0.wp.com/deeplearning.net/tutorial/_images/mylenet.png\" alt=\"_images/mylenet.png\" /></p>\n<p>I&#8217;m told that convolutional neural networks are inspired by how vision works in the natural world. So if I test whether convolutional neural networks work well, am I giving them a robot eye test?</p>\n<p><a href=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg\"><img loading=\"lazy\" data-attachment-id=\"576\" data-permalink=\"https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/robot-eye-test/\" data-orig-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg\" data-orig-size=\"591,591\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"1\"}\" data-image-title=\"robot eye test\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg?w=300\" data-large-file=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg?w=529\" class=\"alignnone size-medium wp-image-576\" src=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg?w=300\" alt=\"robot eye test\" width=\"300\" height=\"300\" srcset=\"https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg?w=300 300w, https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg?w=150 150w, https://colinpriestdotcom.files.wordpress.com/2015/11/robot-eye-test.jpg 591w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></a></p>\n<p>While I am comfortable coding in R, there is little support for convolutional neural networks in R, and I had to code this in Python, using the <a href=\"http://deeplearning.net/software/theano/\" target=\"_blank\">Theano</a> library. The reason that I chose Theano is because neural network model fitting can be quite time consuming, and Theano supports GPU based processing, which can be orders of magnitude faster than CPU based calculations. To simply the code, I am using Daniel Nouri&#8217;s <a href=\"https://github.com/dnouri/nolearn\" target=\"_blank\">nolearn</a> library, which sits over the <a href=\"https://github.com/Lasagne/Lasagne\" target=\"_blank\">lasagne</a> library, which sits over the Theano library. This is the first time I have coded in Python and the first time I have used convolutional neural networks, so it was a good learning experience.</p>\n<p>Since my PCs don&#8217;t have top of the line graphics cards with GPU processing support, I decided to run my code in a cloud on a virtual machine with GPU support. And since I didn&#8217;t want go through the effort of setting up a Linux machine and installing all of the libraries and compilers, I used <a href=\"https://www.dominodatalab.com/\" target=\"_blank\">Domino Data Labs</a> to host my analysis. You can find my project <a href=\"https://app.dominodatalab.com/colinpriest/cnn_leakage\" target=\"_blank\">here</a>.</p>\n<p>Before I progress to coding the model, I have to set up the environment.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nimport os\nimport shutil\n \ndef setup_theano():\n\tdestfile = \"/home/ubuntu/.theanorc\"\n\topen(destfile, 'a').close()\n\tshutil.copyfile(\"/mnt/.theanorc\", destfile)\n \n\tprint \"Finished setting up Theano\"\n</pre>\n<p>The Python script shown above creates a function that copies the Theano settings file into the appropriate folder in the virtual machine, so that Theano knows to use GPU processing rather than CPU processing. This function gets called from my main script.</p>\n<p>In order to reduce the number of calculations, I used a network architecture that inputs an image and outputs an image. The <a href=\"https://www.kaggle.com/c/denoising-dirty-documents/forums/t/16298/background-removal-deep-convolutional-networks-1-232\" target=\"_blank\">suggestion</a> for this architecture comes from <a href=\"https://www.kaggle.com/ironbar\" target=\"_blank\">ironbar</a>, a great guy who placed third in the competition. This is unlike all of the examples I found online, which have just one or two outputs, usually because the online example is a classification problem identifying objects appearing within the image. But there are two issues with this architecture:</p>\n<ol>\n<li>it doesn&#8217;t allow for fully connected layers before the output, and</li>\n<li>the target images are different sizes.</li>\n</ol>\n<p>I chose to ignore the first problem, although if I had time I would have tried out a more traditional architecture that included fully connected layers but which only models one target pixel at a time.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\ndef image_matrix(img):\n \"\"\"\n The output value has shape (<number of pixels>, <number of rows>, <number of columns>)\n \"\"\"\n # 420 x 540 or 258 x 540?\n if img.shape[0] == 258:\n return (img[0:258, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540))\n if img.shape[0] == 420:\n result = []\n result.append((img[0:258, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540)))\n result.append((img[162:420, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540)))\n result = np.vstack(result).astype('float32').reshape((2, 1, 258, 540))\n return result\n</pre>\n<p>For the second problem, I used the script shown above to split the larger images into two smaller images that were the same size as the other small images in the data, thereby standardising the output dimensions.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\ndef load_train_set(file_list):\n xs = []\n ys = []\n for fname in file_list:\n x = image_matrix(load_image(os.path.join('./train_foreground/', fname)))\n y = image_matrix(load_image(os.path.join('./train_cleaned/', fname)))\n for i in range(0, x.shape[0]):\n xs.append(x[i, :, :, :].reshape((1, 1, 258, 540)))\n ys.append(y[i, :, :, :].reshape((1, 1, 258, 540)))\n return np.vstack(xs), np.vstack(ys)\n</pre>\n<p>Theano uses tensors (multi-dimensional matrices) to store the training data and outputs. The first dimension is the index of the training data item. The second dimension is the colourspace information e.g. RGB would be 3 dimensions. Since our images are greyscale, this dimension has a size of only 1. The remaining dimensions are the dimensions of the input data / output data. The script shown above reshapes the data to meet this criteria.<br />\nThe nolearn library simplifes the process of defining the architecture of a neural network. I used 3 hidden convolutional layers, each with 25 image filters. The script below shows how this was achieved.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\nnet2 = NeuralNet(\nlayers = [\n('input', layers.InputLayer),\n('conv1', layers.Conv2DLayer),\n('conv2', layers.Conv2DLayer),\n('conv3', layers.Conv2DLayer),\n('output', layers.FeaturePoolLayer),\n],\n#layer parameters:\ninput_shape = (None, 1, 258, 540),\nconv1_num_filters = 25, conv1_filter_size = (7, 7), conv1_pad = 'same',\nconv2_num_filters = 25, conv2_filter_size = (7, 7), conv2_pad = 'same',\nconv3_num_filters = 25, conv3_filter_size = (7, 7), conv3_pad = 'same',\noutput_pool_size = 25,\noutput_pool_function = T.sum,\ny_tensor_type=T.tensor4,\n\n#optimization parameters:\nupdate = nesterov_momentum,\nupdate_learning_rate = 0.005,\nupdate_momentum = 0.9,\nregression = True,\nmax_epochs = 200,\nverbose = 1,\nbatch_iterator_train=BatchIterator(batch_size=25),\non_epoch_finished=[EarlyStopping(patience=20),],\ntrain_split=TrainSplit(eval_size=0.25)\n)\n</pre>\n<p>Due to the unique nature of the problem versus the online examples, my first attempt at this script was not successful. Here are the key changes that I needed to make:</p>\n<ul>\n<li>set <strong>y_tensor_type=T.tensor4</strong> because the target is 2 dimensional</li>\n<li>your graphics card almost certainly doesn&#8217;t have enough RAM to process all of the images at once, so you need to use a batch iterator and experiment to find a suitable batch size e.g. <strong>batch_iterator_train=BatchIterator(batch_size=25)</strong></li>\n</ul>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\n\nplot_loss(net2)\nplt.savefig(\"./results/plotloss.png\")\n\n</pre>\n<p>I also wanted to plot the loss across the iterations. So I added the two lines above, giving me the graph below.</p>\n<p><img src=\"https://app.dominodatalab.com/colinpriest/cnn_leakage/raw/f9964bb023d2f7d33673ed5689e4b87e1a29a852/results/plotloss.png?inline=true\" alt=\"\" /></p>\n<p>During the first several iterations, the neural network is balancing out the weights so that the pixels are the correct magnitude, and after that the serious work of image processing begins.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\n\nplot_conv_weights(net2.layers_[1], figsize=(4, 4))\nplt.savefig(\"./results/convweights.png\")\n\n</pre>\n<p>I wanted to see what some of the convolutional filters looked like. So I added the two lines shown above, giving me the set of images below.</p>\n<p><img src=\"https://app.dominodatalab.com/colinpriest/cnn_leakage/raw/f9964bb023d2f7d33673ed5689e4b87e1a29a852/results/convweights.png?inline=true\" alt=\"\" /></p>\n<p>These filters look like small parts of images of letters, which makes some sense because we are trying to identify whether a pixel sits on the stroke of a letter.</p>\n<p><img src=\"https://app.dominodatalab.com/colinpriest/cnn_leakage/raw/f9964bb023d2f7d33673ed5689e4b87e1a29a852/test_predicted/1.png?inline=true\" alt=\"\" /></p>\n<p>The output looks reasonable, although not as good as what I achieve using a combination of image processing techniques and deep learning.</p>\n<p><img src=\"https://app.dominodatalab.com/colinpriest/cnn_leakage/raw/f9964bb023d2f7d33673ed5689e4b87e1a29a852/test_predicted/106.png?inline=true\" alt=\"\" /></p>\n<p>In the output image above you can see some imperfections.</p>\n<p>I think that if I had changed the network architecture to include fully connected layers then I would have achieved a better result. Maybe one day when I have enough time I will experiment with that architecture.</p>\n<p>The full Python script is shown below:</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\">\n\nimport random\nimport numpy as np\nimport cv2\nimport os\nimport itertools\nimport math\nimport matplotlib.pyplot as plt\n\nfrom setup_GPU import setup_theano\nsetup_theano()\n\nfrom lasagne import layers\nfrom lasagne.updates import nesterov_momentum\nfrom lasagne.nonlinearities import softmax\nfrom lasagne.nonlinearities import sigmoid\nfrom nolearn.lasagne import BatchIterator\nfrom nolearn.lasagne import NeuralNet\nfrom nolearn.lasagne import TrainSplit\nfrom nolearn.lasagne import PrintLayerInfo\nfrom nolearn.lasagne.visualize import plot_loss\nfrom nolearn.lasagne.visualize import plot_conv_weights\nfrom nolearn.lasagne.visualize import plot_conv_activity\nfrom nolearn.lasagne.visualize import plot_occlusion\n\nimport theano.tensor as T\n\ndef load_image(path):\nreturn cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n\ndef write_image(img, path):\nreturn cv2.imwrite(path, img)\n\ndef image_matrix(img):\n\"\"\"\nThe output value has shape (<number of pixels>, <number of rows>, <number of columns>)\n\"\"\"\n# 420 x 540 or 258 x 540?\nif img.shape[0] == 258:\nreturn (img[0:258, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540))\nif img.shape[0] == 420:\nresult = []\nresult.append((img[0:258, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540)))\nresult.append((img[162:420, 0:540] / 255.0).astype('float32').reshape((1, 1, 258, 540)))\nresult = np.vstack(result).astype('float32').reshape((2, 1, 258, 540))\nreturn result\n\ndef load_train_set(file_list):\nxs = []\nys = []\nfor fname in file_list:\nx = image_matrix(load_image(os.path.join('./train_foreground/', fname)))\ny = image_matrix(load_image(os.path.join('./train_cleaned/', fname)))\nfor i in range(0, x.shape[0]):\nxs.append(x[i, :, :, :].reshape((1, 1, 258, 540)))\nys.append(y[i, :, :, :].reshape((1, 1, 258, 540)))\nreturn np.vstack(xs), np.vstack(ys)\n\ndef load_test_file(fname, folder):\nxs = []\nx = image_matrix(load_image(os.path.join(folder, fname)))\nfor i in range(0, x.shape[0]):\nxs.append(x[i, :, :, :].reshape((1, 1, 258, 540)))\nreturn np.vstack(xs)\n\ndef list_images(folder):\nincluded_extentions = ['jpg','bmp','png','gif' ]\nresults = [fn for fn in os.listdir(folder) if any([fn.endswith(ext) for ext in included_extentions])]\nreturn results\n\ndef do_test(inFolder, outFolder, nn):\ntest_images = list_images(inFolder)\nnTest = len(test_images)\nfor x in range(0, nTest):\nfname = test_images[x]\nx1 = load_test_file(fname, inFolder)\nx1 = x1 - 0.5\npred_y = nn.predict(x1)\ntempImg = []\nif pred_y.shape[0] == 1:\ntempImg = pred_y[0, 0, :, :].reshape(258, 540)\nif pred_y.shape[0] == 2:\ntempImg1 = pred_y[0, 0, :, :].reshape(258, 540)\ntempImg2 = pred_y[1, 0, :, :].reshape(258, 540)\ntempImg = np.empty((420, 540))\ntempImg[0:258, 0:540] = tempImg1\ntempImg[162:420, 0:540] = tempImg2\ntempImg[tempImg < 0] = 0\ntempImg[tempImg > 1] = 1\ntempImg = np.asarray(tempImg*255.0, dtype=np.uint8)\nwrite_image(tempImg, (os.path.join(outFolder, fname)))\n\nclass EarlyStopping(object):\ndef __init__(self, patience=100):\nself.patience = patience\nself.best_valid = np.inf\nself.best_valid_epoch = 0\nself.best_weights = None\n\ndef __call__(self, nn, train_history):\ncurrent_valid = train_history[-1]['valid_loss']\ncurrent_epoch = train_history[-1]['epoch']\nif current_valid < self.best_valid:\nself.best_valid = current_valid\nself.best_valid_epoch = current_epoch\nself.best_weights = nn.get_all_params_values()\nelif self.best_valid_epoch + self.patience < current_epoch:\nprint(\"Early stopping.\")\nprint(\"Best valid loss was {:.6f} at epoch {}.\".format(\nself.best_valid, self.best_valid_epoch))\nnn.load_params_from(self.best_weights)\nraise StopIteration()\n\ndef main():\nrandom.seed(1234)\n\ntraining_images = list_images(\"./train_foreground/\")\nrandom.shuffle(training_images)\nnTraining = len(training_images)\nTRAIN_IMAGES = training_images\n\ntrain_x, train_y = load_train_set(TRAIN_IMAGES)\ntest_x = train_x\ntest_y = train_y\n\n# centre on zero - has already been divided by 255\ntrain_x = train_x - 0.5\n\nnet2 = NeuralNet(\nlayers = [\n('input', layers.InputLayer),\n('conv1', layers.Conv2DLayer),\n('conv2', layers.Conv2DLayer),\n('conv3', layers.Conv2DLayer),\n('output', layers.FeaturePoolLayer),\n],\n#layer parameters:\ninput_shape = (None, 1, 258, 540),\nconv1_num_filters = 25, conv1_filter_size = (7, 7), conv1_pad = 'same',\nconv2_num_filters = 25, conv2_filter_size = (7, 7), conv2_pad = 'same',\nconv3_num_filters = 25, conv3_filter_size = (7, 7), conv3_pad = 'same',\noutput_pool_size = 25,\noutput_pool_function = T.sum,\ny_tensor_type=T.tensor4,\n\n#optimization parameters:\nupdate = nesterov_momentum,\nupdate_learning_rate = 0.005,\nupdate_momentum = 0.9,\nregression = True,\nmax_epochs = 200,\nverbose = 1,\nbatch_iterator_train=BatchIterator(batch_size=25),\non_epoch_finished=[EarlyStopping(patience=20),],\ntrain_split=TrainSplit(eval_size=0.25)\n)\n\nnet2.fit(train_x, train_y)\n\nplot_loss(net2)\nplt.savefig(\"./results/plotloss.png\")\nplot_conv_weights(net2.layers_[1], figsize=(4, 4))\nplt.savefig(\"./results/convweights.png\")\n\n#layer_info = PrintLayerInfo()\n#layer_info(net2)\n\nimport cPickle as pickle\nwith open('results/net2.pickle', 'wb') as f:\npickle.dump(net2, f, -1)\n\ny_pred2 = net2.predict(test_x)\nprint \"The accuracy of this network is: %0.2f\" % (abs(y_pred2 - test_y)).mean()\n\ndo_test(\"./train_foreground/\", './train_predicted/', net2)\ndo_test(\"./test_foreground/\", './test_predicted/', net2)\n\nif __name__ == '__main__':\nmain()\n\n</pre>\n",
  "wfw:commentRss": "https://colinpriest.com/2015/11/01/denoising-dirty-documents-part-10/feed/",
  "slash:comments": 27,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "robot eye test"
    },
    {
      "media:title": "colinpriest1966"
    },
    {
      "media:title": "_images/mylenet.png"
    },
    {
      "media:title": "robot eye test"
    },
    "",
    "",
    "",
    ""
  ]
}