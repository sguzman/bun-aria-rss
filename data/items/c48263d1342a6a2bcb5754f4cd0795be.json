{
  "title": "Evolving Stable Strategies",
  "link": "https://blog.otoro.net/2017/11/12/evolving-stable-strategies/",
  "guid": "https://blog.otoro.net/2017/11/12/evolving-stable-strategies/",
  "description": "<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/minitaur/duck_normal_small.gif\" width=\"100%\"/><br/>-->\n<video class=\"b-lazy\" autoplay=\"\" muted=\"\" playsinline=\"\" loop=\"\" style=\"display: block; margin: auto; width: 100%;\"><source src=\"/assets/20171109/minitaur/duck_normal.mp4\" type=\"video/mp4\" /></video><br />\n<!--<img src=\"/assets/20171109/biped/biped_cma.gif\" width=\"100%\"/><br/>-->\n<!--<i>Evolved Biped Walker.</i><br/>-->\n<p></p>\n<i>Going for a ride.</i><br />\n<code>\n<a href=\"https://github.com/hardmaru/estool/\">GitHub</a>\n</code>\n</center>\n<p></p>\n\n<p>In the <a href=\"/2017/10/29/visual-evolution-strategies/\">previous article</a>, I have described a few evolution strategies (ES) algorithms that can optimise the parameters of a function without the need to explicitly calculate gradients. These algorithms can be applied to reinforcement learning (RL) problems to help find a suitable set of model parameters for a neural network agent. In this article, I will explore applying ES to some of these RL problems, and also highlight methods we can use to find policies that are more stable and robust.</p>\n\n<h2 id=\"evolution-strategies-for-reinforcement-learning\">Evolution Strategies for Reinforcement Learning</h2>\n\n<p>While RL algorithms require a reward signal to be given to the agent at every timestep, ES algorithms only care about the final cumulative reward that an agent gets at the end of its rollout in an environment. In many problems, we only know the outcome at the end of the task, such as whether the agent wins or loses, whether the robot arm picks up the object or not, or whether the agent has survived, and these are problems where ES may have an advantage over traditional RL. Below is a pseudo-code that encapsulates a rollout of an agent in an <a href=\"https://gym.openai.com/docs/\">OpenAI Gym</a> environment, where we only care about the cumulative reward:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"k\">def</span> <span class=\"nf\">rollout</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"p\">):</span>\n  <span class=\"n\">obs</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n  <span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n  <span class=\"n\">total_reward</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n  <span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">agent</span><span class=\"o\">.</span><span class=\"n\">get_action</span><span class=\"p\">(</span><span class=\"n\">obs</span><span class=\"p\">)</span>\n    <span class=\"n\">obs</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>\n    <span class=\"n\">total_reward</span> <span class=\"o\">+=</span> <span class=\"n\">reward</span>\n  <span class=\"k\">return</span> <span class=\"n\">total_reward</span></code></pre></figure>\n\n<p>We can define <code class=\"highlighter-rouge\">rollout</code> to be the objective function that maps the model parameters of an agent into its fitness score, and use an ES solver to find a suitable set of model parameters as described in the previous <a href=\"/2017/10/29/visual-evolution-strategies/\">article</a>:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s\">'worlddomination-v0'</span><span class=\"p\">)</span>\n\n<span class=\"c\"># use our favourite ES</span>\n<span class=\"n\">solver</span> <span class=\"o\">=</span> <span class=\"n\">EvolutionStrategy</span><span class=\"p\">()</span>\n\n<span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n\n  <span class=\"c\"># ask the ES to give set of params</span>\n  <span class=\"n\">solutions</span> <span class=\"o\">=</span> <span class=\"n\">solver</span><span class=\"o\">.</span><span class=\"n\">ask</span><span class=\"p\">()</span>\n\n  <span class=\"c\"># create array to hold the results</span>\n  <span class=\"n\">fitlist</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">solver</span><span class=\"o\">.</span><span class=\"n\">popsize</span><span class=\"p\">)</span>\n\n  <span class=\"c\"># evaluate for each given solution</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">solver</span><span class=\"o\">.</span><span class=\"n\">popsize</span><span class=\"p\">):</span>\n\n    <span class=\"c\"># init the agent with a solution</span>\n    <span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span><span class=\"n\">solutions</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span>\n\n    <span class=\"c\"># rollout env with this agent</span>\n    <span class=\"n\">fitlist</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">rollout</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">env</span><span class=\"p\">)</span>\n\n  <span class=\"c\"># give scores results back to ES</span>\n  <span class=\"n\">solver</span><span class=\"o\">.</span><span class=\"n\">tell</span><span class=\"p\">(</span><span class=\"n\">fitness_list</span><span class=\"p\">)</span>\n\n  <span class=\"c\"># get best param &amp; fitness from ES</span>\n  <span class=\"n\">bestsol</span><span class=\"p\">,</span> <span class=\"n\">bestfit</span> <span class=\"o\">=</span> <span class=\"n\">solver</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n\n  <span class=\"c\"># see if our task is solved</span>\n  <span class=\"k\">if</span> <span class=\"n\">bestfit</span> <span class=\"o\">&gt;</span> <span class=\"n\">MY_REQUIREMENT</span><span class=\"p\">:</span>\n    <span class=\"k\">break</span></code></pre></figure>\n\n<h2 id=\"deterministic-and-stochastic-policies\">Deterministic and Stochastic Policies</h2>\n\n<p>Our agent takes the observation given to it by the environment as an input, and outputs an action at each timestep during a rollout inside the environment. We can model the agent however we want, and use methods from hard-coded rules, decision trees, linear functions to recurrent neural networks. In this article I use a simple feed-forward network with 2 hidden layers to map from an agent’s observation, a vector <script type=\"math/tex\">x</script>, directly to the actions, a vector <script type=\"math/tex\">y</script>:</p>\n\n<script type=\"math/tex; mode=display\">h_1 = f_h(W_1 \\; x + b_1)</script>\n\n<script type=\"math/tex; mode=display\">h_2 = f_h(W_2 \\; h_1 + b_2)</script>\n\n<script type=\"math/tex; mode=display\">y = f_{out}(W_{out} \\; h_2 + b_{out})</script>\n\n<p>The activation functions <script type=\"math/tex\">f_h</script>, <script type=\"math/tex\">f_{out}</script> can be <code class=\"highlighter-rouge\">tanh</code>, <code class=\"highlighter-rouge\">sigmoid</code>, <code class=\"highlighter-rouge\">relu</code>, or whatever we want to use. In all of my experiments I use <code class=\"highlighter-rouge\">tanh</code>. For the output layer, sometimes we may want <script type=\"math/tex\">f_{out}</script> to be a pass-through function without nonlinearities. If we concatenate all the weight and bias parameters into a single vector called <script type=\"math/tex\">W</script>, we see that the above neural network is a deterministic function <script type=\"math/tex\">y = F(x, W)</script>. We can then use ES to find a solution <script type=\"math/tex\">W</script> using the search loop described earlier.</p>\n\n<p>But what if we don’t want our agent’s policy to be deterministic? For certain tasks, even as simple as rock-paper-scissors, the optimal policy is a random action, so we want our agent to be able to learn a stochastic policy. One way to convert <script type=\"math/tex\">y=F(x, W)</script> into a stochastic policy is to make <script type=\"math/tex\">W</script> random. Each model parameter <script type=\"math/tex\">w_i \\in W</script> can be a random value drawn from a normal distribution <script type=\"math/tex\">N(\\mu_i, \\sigma_i)</script>.</p>\n\n<p>This type of stochastic network is called a <em>Bayesian Neural Network</em>. A <a href=\"http://edwardlib.org/tutorials/bayesian-neural-network\">Bayesian neural network</a> is a neural network with a prior distribution on its weights. In this case, the model parameters we want to solve for, are the set of <script type=\"math/tex\">\\mu</script> and <script type=\"math/tex\">\\sigma</script> vectors, rather than the weights <script type=\"math/tex\">W</script>. During each forward pass of the network, a new <script type=\"math/tex\">W</script> is drawn from <script type=\"math/tex\">N(\\mu, \\sigma I)</script>. There are lots of <a href=\"https://arxiv.org/abs/1703.02910\">interesting</a> <a href=\"https://github.com/andrewgordonwilson/bayesgan/blob/master/README.md\">works</a> in the literature applying Bayesian networks to many problems, and also <a href=\"http://bayesiandeeplearning.org/\">addressing</a> many challenges of <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.7138&amp;rep=rep1&amp;type=pdf\">training</a> these networks. ES can also be used to directly find solutions for a stochastic policy by setting the solutions space be <script type=\"math/tex\">\\mu</script> and <script type=\"math/tex\">\\sigma</script>, rather than <script type=\"math/tex\">W</script>.</p>\n\n<p>Stochastic policy networks are also popular in the RL literature. For example, in the <a href=\"https://arxiv.org/abs/1707.06347\">Proximal Policy Optimization (PPO)</a> algorithm, the final layer is a set of <script type=\"math/tex\">\\mu</script> and <script type=\"math/tex\">\\sigma</script> parameters and the action is sampled from <script type=\"math/tex\">N(\\mu, \\sigma I)</script>. Adding <a href=\"https://arxiv.org/abs/1707.06347\">noise</a> to parameters are also known to encourage the agent to explore the environment and escape from local optima. I find that for many tasks where we need an agent to explore, we do not need the entire <script type=\"math/tex\">W</script> to be random – just the bias is enough. For challenging locomotion tasks, such as the ones in the <a href=\"https://blog.openai.com/roboschool/\">roboschool</a> environment, I often need to use ES to find a stochastic policy where only the bias parameters are drawn from a normal distribution.</p>\n\n<h2 id=\"evolving-robust-policies-for-bipedal-walker\">Evolving Robust Policies for Bipedal Walker</h2>\n\n<!--Many RL environments are stochastic, meaning even if our agent follows a deterministic policy, the fitness score obtained by our agent will be different for each rollout. For instance, a robot arm grasping task will place objects in random locations to thest the agent's ability to generalize. In this [Bipedal Walker](https://gym.openai.com/envs/BipedalWalkerHardcore-v2/) environment, our bipedal robot agent has to travel through a randomly-generated terrain map of ladders, stumps and pitfalls.-->\n\n<p>One of the areas where I found ES useful is for searching for robust policies. I want to control the tradeoff between data efficiency, and how robust the policy is over several random trials. To demonstrate this, I tested ES on a nice environment called <a href=\"https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\">BipedalWalkerHardcore-v2</a> created by <a href=\"https://twitter.com/robo_skills\">Oleg Klimov</a> using the <a href=\"https://github.com/pybox2d/pybox2d/blob/master/README.md\">Box2D Physics Engine</a>, the same physics engine used in <a href=\"https://github.com/estevaofon/angry-birds-python/blob/master/README.md\">Angry Birds</a>.</p>\n\n<!--<center>\n<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>\n<i>Our agent solved <a href=\"https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\">BipedalWalkerHardcore-v2</a>.</i><br/>\n</center>\n<p></p>-->\n<center>\n<blockquote class=\"twitter-video\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Evolution Strategy Variant + OpenAI Gym <a href=\"https://t.co/t2R0QQ5qcH\">pic.twitter.com/t2R0QQ5qcH</a></p>&mdash; hardmaru (@hardmaru) <a href=\"https://twitter.com/hardmaru/status/889215446150291458?ref_src=twsrc%5Etfw\">July 23, 2017</a></blockquote> <script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n<i>Our agent solved <a href=\"https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\">BipedalWalkerHardcore-v2</a>.</i>\n</center>\n<p>\n</p>\n\n<p>In this environment our agent has to learn a policy to walk across randomly generated terrain within the time limit without falling over. There are 24 inputs, consisting of 10 lidar sensors, angles and contacts. The agent is not given the absolute coordinates of where it is on the map. The action space is 4 continuous values controlling the torques of its 4 motors. The total reward calculation is based on the total distance achieved by the agent. Generally, if the agent completes a map, it will get score of 300+ points, although a small amount of points will be subtracted based on how much motor torque was applied, so energy usage is also a constraint.</p>\n\n<p><a href=\"https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\">BipedalWalkerHardcore-v2</a> defines <em>solving</em> the task as getting an average score of 300+ over 100 consecutive random trials. While it is relatively easy to train an agent to successfully walk across the map using an RL algorithm, it is difficult to get the agent to do so consistently and efficiently, making this task an interesting challenge. To my knowledge, my agent is the only solution known to solve this task so far (as of October 2017).</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td><img id=\"learning_to_fall_img\" src=\"/assets/20171109/jpeg/learning_to_fall_img.jpeg\" width=\"100%\" /><br />\n  <center><i>Early stages. Learning to walk.</i></center>\n  </td>\n  <td><img id=\"learning_local_optima_img\" src=\"/assets/20171109/jpeg/learning_local_optima_img.jpeg\" width=\"100%\" />\n  <br />\n  <center><i>Learns to correct errors, but still slow ...</i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>Because the terrain map is randomly generated for each trial, sometimes we may end up with an easy terrain, or sometimes a very difficult terrain. We don’t want our natural selection process to allow agents with weak policies who had gotten lucky with an easy map to advance to the next generation. We also want to give agents with good policies a chance to redeem themselves. So what I ended up doing, is to define an agent’s episode, as the <em>average</em> of 16 random rollouts, and use the average of the cumulative rewards over 16 rollouts as its fitness score.</p>\n\n<p>Another way to look at this is to see that even though we are testing the agent over 100 trials, we usually train it on single trials, so the test-task is not the same as the training-task we are optimising for. By averaging each agent in the population multiple times in a stochastic environment, we narrow the gap between our training set and the test set. If we can overfit to the training set, we might as well overfit to the test set, since that’s an <a href=\"https://twitter.com/jacobandreas/status/924356906344267776\">okay</a> thing to do in RL :)</p>\n\n<p>Of course, the data efficiency of our algorithm is now 16x worse, but the final policy is a lot more robust. When I tested the final policy over 100 consecutive random trials, we got an average score of over 300 points required to solve this environment. Without this averaging method, the best agent can only obtain an average score of <script type=\"math/tex\">\\sim</script> 220 to 230 over 100 trials. To my knowledge, this is the first solution that solves this environment (as of October 2017).</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n<img id=\"biped_pepg_final_01_img\" src=\"/assets/20171109/jpeg/biped_pepg_final_01_img.jpeg\" width=\"100%\" />\n  </td>\n  <td>\n<img id=\"biped_pepg_final_02_img\" src=\"/assets/20171109/jpeg/biped_pepg_final_02_img.jpeg\" width=\"100%\" />\n  </td>\n</tr>\n</table>\n<i>Winning solutions evolved using <a href=\"/2017/10/29/visual-evolution-strategies/\">PEPG</a> using average-of-16 runs per episode.</i>\n<p></p>\n</center>\n\n<p>I also used <a href=\"https://arxiv.org/abs/1707.06347\">PPO</a>, a state-of-the-art policy gradient algorithm for RL, and tried to tune it to the best of my ability to perform well on this task. In the end, I was only able to get PPO to achieve average scores of <script type=\"math/tex\">\\sim</script> 240 to 250 over 100 random trials. But I’m sure someone else will be able to use PPO or another RL algorithm to solve this environment in the future. (Please let me know if you do so!)</p>\n\n<p><em>Update (Jan 2018): <a href=\"https://github.com/dgriff777\">dgriff777</a> was able to use a continuous version of A3C+LSTM with 4 stack frames as the input to train BipedalWalkerHardcore-v2 to obtain a score of 300 over 100 random trials. He provided this awesome implementation of his pytorch model on <a href=\"https://github.com/dgriff777/a3c_continuous/blob/master/README.md\">GitHub</a>.</em></p>\n\n<!--I used a deterministic neural network with 64 and 32 hidden units for each of the hidden layers. At first, when I was trying out this task using the various ES algorithms discussed (PEPG, OpenAI ES, CMA-ES), I was only able to obtain an average score of around 200-240 range after 2000 generations (or around 500K episodes as I used a population size of 256).\n\nI also used the PPO algorithm, and tried hard to tune it to the best of my abilities, but found that it obtained similar scores after 500k episodes. Letting both the ES and PPO run longer, even up to 20M episodes didn't seem to help improve the score. Looking at the leaderboard, the best average scores are also below 240.\n\nI also used the PPO algorithm, and tried hard to tune it to the best of my abilities, but found that it obtained similar scores after 500k episodes. Letting PPO run longer, even up to 20M episodes didn't seem to help improve the score. If you are able to use PPO to solve this task, please let me know!\n\nThe issue is because the maps are randomly generated, sometimes mediocre agents will get easier maps, and with some luck they are able to complete the task. After a while it becomes difficult to distinguish better agents in the population because there will always be many agents in the population who had achieved 300+ points in the rollout by being more lucky. A score of 300+ does not guarantee that the agent will be a consistent winner.\n\nSo while we are training our agents be able to complete a task only once, we are testing the agent on its ability to complete the task 100 times. Our ES is overfitting agents to sometimes get 300+ points on the training task, while the testing scores plateaus at 240. One solution is to make the train task closer to the test task, since it is okay to overfit to test set in RL :) I defined a \"rollout\" to be the average of 16 random rollouts for each agent, so during training, each agent is evaluated 16 random maps.\n\nBy averaging each agent in the population multiple times in a stochastic environment, we will be selecting agents who are consistent performers. This comes at a tradeoff, since we will need to evaluate 16x more episodes than before. Furthermore, the average rollout fitness scores progresses at a slower pace compared to a single rollout, requiring ES to run for more generations. After ~ 4000 generations, ES is able to obtain policies that can obtain 300-310 points over 100 average runs. While the training task uses the average of 16 runs, in principle, if we had enough compute we can even optimise directly for 100 runs.\n-->\n\n<p>The ability to control the tradeoff between data efficiency and policy robustness is quite powerful, and useful in the real world where we need safe policies. In theory, with enough compute, we could have even averaged over of the required 100 rollouts and optimised our Bipedal Walker directly to the requirements. Professional engineers are often required to have their designs satisfy specific Quality Assurance guarantees and meet certain safety factors. We need to be able to take into account such safety factors when we train agents to learn policies that may affect the real world.</p>\n\n<p>Here are a few other solutions that ES discovered:</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <img src=\"/assets/20171109/biped/biped_cma.gif\" width=\"100%\" />\n  <br />\n  <center><i><a href=\"/2017/10/29/visual-evolution-strategies/\">CMA-ES</a> solution</i></center>\n  </td>\n  <td>\n<img id=\"biped_oes_img\" src=\"/assets/20171109/jpeg/biped_oes_img.jpeg\" width=\"100%\" />\n  <br />\n  <center><i><a href=\"/2017/10/29/visual-evolution-strategies/\">OpenAI-ES</a> solution</i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>I also trained the agent with a stochastic policy network with high initial noise parameters, so the agent sees noise everywhere, and even its actions are noisy. It resulted in the agent learning the task despite not being confident of its input and outputs being accurate (this agent couldn’t get a score of 300+ though):</p>\n\n<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/bipedstoc/biped_noisy.gif\" width=\"100%\"/><br/>-->\n<img id=\"biped_noisy_img\" src=\"/assets/20171109/jpeg/biped_noisy_img.jpeg\" width=\"100%\" /><br />\n<!--<video id=\"biped_noisy_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/bipedstoc/biped_noisy.mp4\" type=\"video/mp4\"/></video><br/>-->\n<i>Bipedal walker using a stochastic policy.</i><br />\n</center>\n<p></p>\n\n<h3 id=\"kuka-robot-arm-grasping\">Kuka Robot Arm Grasping</h3>\n\n<p>I also tried to apply ES with this averaging technique on a simplified Kuka robot arm grasping task. This environment is available in the <a href=\"https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/bullet\">pybullet environment</a>. The Kuka model used in the simulation is designed to be similar to a real <a href=\"https://www.kuka.com/en-de/products\">Kuka</a> robot arm. In this simplified task, the agent is given the <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/kukaGymEnv.py#L106\">coordinates </a>of the object.</p>\n\n<p>More advanced RL environments may require the agent to infer an action directly from pixel inputs, but we could in principle combine this simplified model with a pre-trained convnet that gives us an estimate of the coordinates as well.</p>\n\n<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/kuka/kuka.gif\" width=\"100%\"/><br/>-->\n<img id=\"kuka_img\" src=\"/assets/20171109/jpeg/kuka_img.jpeg\" width=\"100%\" />\n<!--<video id=\"kuka_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/kuka/kuka.mp4\" type=\"video/mp4\"/></video>-->\n<br />\n<i>Robot arm grasping task using a stochastic policy.</i><br />\n</center>\n<p></p>\n\n<p>The agent obtains a score of 10000 if it successfully picks up the object, and 0 otherwise. Some points are deducted for energy usage. By averaging a sparse reward over 16 random trials, we can get the ES to optimise for robustness. However, in the end, I was able to get policies that can pick up the object only <script type=\"math/tex\">\\sim</script> 70 to 75% of the time with both deterministic and stochastic policies. There is still room for improvement.</p>\n\n<h2 id=\"getting-a-minitaur-to-learn-a-multiple-tasks\">Getting a Minitaur to Learn a Multiple Tasks</h2>\n\n<p>Learning to perform multiple difficult tasks at the same time make us better at performing individual tasks. For example, Shaolin monks who lift weights while standing on a pole will be able to balance better without the weights. Learning to not spill a cup of water while cruising a car at 80mph in the mountains will make the driver a better illegal street racer. We can also train agents to perform multiple tasks to make them learn more stable policies.</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <!--<img src=\"/assets/20171109/shaolin.gif\" width=\"100%\"/>-->\n<img id=\"shaolin_img\" src=\"/assets/20171109/jpeg/shaolin_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"shaolin_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/shaolin.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Shaolin Agents.</i></center>\n  </td>\n  <td>\n  <!--<img id=\"learning_to_drift_img\" src=\"/assets/20171109/learning_to_drift.gif\" width=\"100%\"/>-->\n<img id=\"learning_to_drift_img\" src=\"/assets/20171109/jpeg/learning_to_drift_img.jpeg\" width=\"100%\" />\n  <br />\n  <center><i>Learning to drift.</i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>This recent work on <a href=\"https://arxiv.org/abs/1710.03748\">self-playing</a> agents demonstrated that agents who learn difficult tasks such as Sumo wrestling (a sport that require many skills) are able to also perform easier tasks, like withstanding wind while walking, without the need for further training. <a href=\"https://twitter.com/erwincoumans/status/924352109511819264\">Erwin Coumans</a> recently tried to experiment with adding a <a href=\"https://twitter.com/erwincoumans/status/924352109511819264\">duck</a> on top of a Minitaur learning to walk ahead. If the duck fell, the Minitaur would also fail, so the hope is that these types of task augmentation will help transfer learned policies from simulation over to the real Minitaur. I took one of his <a href=\"https://gist.github.com/erwincoumans/c579e076cbaf7c76caa9a42829408e2e\">examples</a> and experimented with training the Minitaur and duck combination using ES.</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <!--<img src=\"/assets/20171109/minitaur/minitaur_faster.gif\" width=\"100%\"/>-->\n<img id=\"minitaur_faster_img\" src=\"/assets/20171109/jpeg/minitaur_faster_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"minitaur_faster_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/minitaur_faster.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>CMA-ES walking policy in <a href=\"https://pybullet.org\">pybullet</a>.</i></center>\n  </td>\n  <td>\n  <!--<img src=\"/assets/20171109/minitaur/real_minitaur.gif\" width=\"100%\"/>-->\n<img id=\"real_minitaur_img\" src=\"/assets/20171109/jpeg/real_minitaur_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"real_minitaur_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/real_minitaur.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Real Minitaur from <a href=\"https://www.ghostrobotics.io/\">Ghost Robotics.</a></i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>The Minitaur model in <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py\">pybullet</a> is designed to mimic the real physical Minitaur. However, a policy trained on a perfect simulation environment usually fails in the real world. It may not even generalise to small augmentations of the task inside the simulation. For example, in the figure above is a Minitaur trained to walk ahead (using CMA-ES), but we see that this policy is not always able to carry a duck across the room when we put a duck on top of it inside of the simulation.</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td><img id=\"duck_notrain_img\" src=\"/assets/20171109/jpeg/duck_notrain_img.jpeg\" width=\"100%\" />\n  <!--<img src=\"/assets/20171109/minitaur/duck_notrain.gif\" width=\"100%\"/>-->\n  <!--<video id=\"duck_notrain_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/duck_notrain.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Walking policy works with duck.</i></center>\n  </td>\n  <td>\n  <!--<img src=\"/assets/20171109/minitaur/duck_normal_small.gif\" width=\"100%\"/>-->\n  <video autoplay=\"\" muted=\"\" playsinline=\"\" loop=\"\" style=\"display: block; margin: auto; width: 100%;\"><source src=\"/assets/20171109/minitaur/duck_normal.mp4\" type=\"video/mp4\" /></video>\n  <br />\n  <center><i>Policy trained on duck.</i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>The policy learned from the pure walking task still works to some degree even when the duck is deployed, meaning that the addition of the duck wasn’t so difficult. The duck has a flat stable bottom so it wasn’t too difficult for the Minitaur to keep the duck from falling off its back. I tried to replace the duck with a ball to make the task much harder.</p>\n\n<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/minitaur/ball_cheating.gif\" width=\"100%\"/><br/>-->\n<img id=\"ball_cheating_img\" src=\"/assets/20171109/jpeg/ball_cheating_img.jpeg\" width=\"100%\" />\n<!--<video id=\"ball_cheating_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/ball_cheating.mp4\" type=\"video/mp4\"/></video>-->\n<br />\n<i>Learning to cheat.</i><br />\n</center>\n<p></p>\n\n<p>However, replacing the duck with a ball didn’t immediately result in a stable balancing policy. Instead, CMA-ES found a policy that still technically carried the ball across the floor by first having the ball slide into a hole made for its legs, and then carrying the ball inside this hole. The lesson learned here is that an objective-driven search algorithm will learn to take advantage of any design flaws in the environment and exploit them to reach its objective.</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <!--<img src=\"/assets/20171109/minitaur/ball_stoc.gif\" width=\"100%\"/>-->\n<img id=\"ball_stoc_img\" src=\"/assets/20171109/jpeg/ball_stoc_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"ball_stoc_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/ball_stoc.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Stochastic policy trained with ball.</i></center>\n  </td>\n  <td>\n  <!--<img src=\"/assets/20171109/minitaur/duck_stoc.gif\" width=\"100%\"/>-->\n<img id=\"duck_stoc_img\" src=\"/assets/20171109/jpeg/duck_stoc_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"duck_stoc_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/minitaur/duck_stoc.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Same policy with duck.</i></center>\n  </td>\n</tr>\n</table>\n</center>\n\n<p>After making the ball smaller, CMA-ES was able to find a stochastic policy that can walk and balance the ball at the same time. This policy also transferred back to the easier duck task. In the future, I hope these type of task augmentation techniques will be useful for transfer learning to real robots.</p>\n\n<h2 id=\"estool\">ESTool</h2>\n\n<p>One of the big selling points of ES is that it is easy to parallelise the computation using several workers running on different threads on different CPU cores, or even on <a href=\"https://blog.openai.com/evolution-strategies/\">different machines</a>. Python’s <a href=\"https://docs.python.org/2/library/multiprocessing.html\">multiprocessing</a> makes it simple to launch parallel processes. I prefer to use Message Passing Interface (MPI) with <a href=\"https://mpi4py.scipy.org/docs/\">mpi4py</a> to launch separate python processes for each job. This allows us to get around the <a href=\"https://en.wikipedia.org/wiki/Global_interpreter_lock\">global interpreter lock</a>, and also gives me confidence that each process has its own sandboxed numpy and gym instances which is important when it comes to seeding random number generators.</p>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <!--<img src=\"/assets/20171109/robo/roboschool.gif\" width=\"100%\"/>-->\n<img id=\"roboschool_img\" src=\"/assets/20171109/jpeg/roboschool_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"roboschool_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/robo/roboschool.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Roboschool Hopper, Walker, Ant.</i></center>\n  </td>\n  <td>\n<img id=\"reacher_img\" src=\"/assets/20171109/jpeg/reacher_img.jpeg\" width=\"100%\" />\n  <!--<video id=\"reacher_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/robo/reacher.mp4\" type=\"video/mp4\"/></video>-->\n  <br />\n  <center><i>Roboschool Reacher.</i></center>\n  </td>\n</tr>\n</table>\n<i>Agents evolved using <a href=\"https://github.com/hardmaru/estool/\"><code>estool</code></a> on various <a href=\"https://blog.openai.com/roboschool/\">roboschool</a> tasks.</i>\n<p></p>\n</center>\n\n<p>I have implemented a simple tool called <a href=\"https://github.com/hardmaru/estool/\"><code class=\"highlighter-rouge\">estool</code></a> that uses the <a href=\"https://github.com/hardmaru/estool/blob/master/es.py\"><code class=\"highlighter-rouge\">es.py</code></a> library described in the previous <a href=\"/2017/10/29/visual-evolution-strategies/\">article</a> to train simple feed-forward policy networks to perform continuous control RL tasks written with a gym interface. I have used <code class=\"highlighter-rouge\">estool</code> tool to easily train all of the experiments described earlier, as well as various other continuous control tasks inside gym and roboschool. <code class=\"highlighter-rouge\">estool</code> uses MPI for distributed processing so it shouldn’t require too much work to distribute workers over multiple machines.</p>\n\n<!--This tool will keep track of a population of agents, whos weights are sampled from an ES algorithm, to perform a given gym task. Unlike RL algorithms, only the comulative reward score of a rollout is used as a fitness score to evaluate each agent. `estool` uses MPI for distributed processing.-->\n\n<h2 id=\"estool-with-pybullet\">ESTool with pybullet</h2>\n\n<p><a href=\"https://github.com/hardmaru/estool/\">GitHub repo</a></p>\n\n<p>In addition to the environments that come with gym and roboschool, <code class=\"highlighter-rouge\">estool</code> works well with most <a href=\"https://pybullet.org\">pybullet</a> gym environments. It is also easy to build custom pybullet environments by modifying existing environments. For example, I was able to make the Minitaur with ball environment (in the <code class=\"highlighter-rouge\">custom_envs</code> directory of the repo) without much effort, and being able to tinker with the environment makes it easier to try out new ideas. If you want to incorporate 3D models from other software packages like <a href=\"http://gazebosim.org/tutorials/?tut=ros_urdf\">ROS</a> or <a href=\"https://www.blender-models.com/model-downloads/mechanicalelectronical/robotics/id/star-wars-pit-droid/\">Blender</a>, you can try building new and interesting pybullet environments and challenge others to try to solve them.</p>\n\n<p>Many models and environments in pybullet, such as the Kuka robot arm and the Minitaur, are modelled to be similar to the real robot as part of current exciting transfer learning research efforts. In fact, many of these recent <a href=\"https://stanfordvl.github.io/ntp/\">cutting</a> <a href=\"https://sites.google.com/view/multi-task-domain-adaptation\">edge</a> <a href=\"https://sermanet.github.io/imitate/\">research</a> <a href=\"https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html\">papers</a> are using pybullet to conduct transfer learning experiments.</p>\n\n<p>You don’t need an expensive Minitaur or Kuka robot arm to play with sim-to-real experiments though. There is a <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecar.py\">racecar</a> model inside pybullet that is modelled after the <a href=\"https://mit-racecar.github.io/\">MIT racecar</a> open source hardware kit. There’s even a pybullet environment that mounts a <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecarZEDGymEnv.py\">virtual camera</a> onto the virtual racecar to give the agent a virtual pixel screen as an input observation.</p>\n\n<p>Let’s try the easier version first, where the racecar simply needs to learn a policy to move towards a giant ball. In the <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecarGymEnv.py\">RacecarBulletEnv-v0</a> environment, the agent gets the relative coordinates of the ball as an input, and outputs continuous actions that control the motor speed and steering direction. The task is simple enough that it only takes 5 minutes (50 generations) on a 2014 Macbook Pro (with an 8-core CPU) to train. Using <code class=\"highlighter-rouge\">estool</code>, the command below will launch the training job on eight processes and assign each process 4 jobs, to get a total of 32 workers, using CMA-ES to evolve the policies:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code>python train.py bullet_racecar -o cma -n 8 -t 4\n</code></pre>\n</div>\n\n<p>The training progress, as well as the model parameters found will be stored in the <code class=\"highlighter-rouge\">log</code> subdirectory. We can run this command to visualise an agent inside the environment using the best policy found:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code>python model.py bullet_racecar log/bullet_racecar.cma.1.32.best.json\n</code></pre>\n</div>\n\n<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/robo/simple_racecar.gif\" width=\"100%\"/><br/>-->\n<img id=\"simple_racecar_img\" src=\"/assets/20171109/jpeg/simple_racecar_img.jpeg\" width=\"100%\" />\n<!--<video id=\"simple_racecar_video\" autoplay muted playsinline loop width=\"100%\"><source src=\"/assets/20171109/robo/simple_racecar.mp4\" type=\"video/mp4\"/></video>-->\n<br />\n<i>pybullet racecar environment, based on the <a href=\"https://mit-racecar.github.io/\">MIT Racecar</a>.</i><br />\n</center>\n<p></p>\n\n<p>In the simulation, we can use the mouse cursor to move the ball around, and even move the racecar around if we want to interact with it.</p>\n\n<p>The IPython notebook <code class=\"highlighter-rouge\">plot_training_progress.ipynb</code> can visualise the training history per generation of the racecar agents. At each generation, we can see the best score, the worse score, and the average score across the entire population.</p>\n\n<center>\n<img src=\"/assets/20171109/svg/bullet_racecar.wallclock.svg\" width=\"100%\" /><br />\n<img src=\"/assets/20171109/svg/bullet_racecar.generation.svg\" width=\"100%\" />\n</center>\n\n<p>Standard locomotion tasks similar to those in roboschool, such as Inverted Pendulum, Hopper, Walker, HalfCheetah, Ant, and Humanoid are also available in pybullet. I found a policy for pybullet’s ant that gets to a score of 3000 within hours on a multi-core machine with a population size of 256, using PEPG:</p>\n\n<div class=\"highlighter-rouge\"><pre class=\"highlight\"><code>python train.py bullet_ant -o pepg -n 64 -t 4\n</code></pre>\n</div>\n\n<center>\n<table style=\"width:100%\">\n<tr>\n  <td>\n  <center>\n  <!--<img src=\"/assets/20171109/robo/bullet_ant_demo.gif\" width=\"80%\"/>-->\n<img id=\"bullet_ant_demo_img\" src=\"/assets/20171109/jpeg/bullet_ant_demo_img.jpeg\" width=\"80%\" />\n  <!--<video id=\"bullet_ant_demo_video\" autoplay muted playsinline loop width=\"80%\"><source src=\"/assets/20171109/robo/bullet_ant_demo.mp4\" type=\"video/mp4\"/></video>-->\n  </center>\n  </td>\n  <td>\n  <!--<img src=\"/assets/20171109/robo/bullet_ant.gif\" width=\"120%\"/>-->\n<img id=\"bullet_ant_img\" src=\"/assets/20171109/jpeg/bullet_ant_img.jpeg\" width=\"120%\" />\n  <!--<video id=\"bullet_ant_video\" autoplay muted playsinline loop width=\"120%\"><source src=\"/assets/20171109/robo/bullet_ant.mp4\" type=\"video/mp4\"/></video>-->\n  </td>\n</tr>\n</table>\n<i>Example rollout of <a href=\"https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/gym_locomotion_envs.py\">AntBulletEnv</a>. We can still save rollouts as an .mp4 video using <code>gym.wrappers.Monitor</code></i>\n</center>\n\n<center>\n<img src=\"/assets/20171109/svg/bullet_ant.svg\" width=\"100%\" />\n</center>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>In this article, I discussed using ES to find policies for a feed-forward neural network agent to perform various continuous control RL tasks defined by a gym environment interface. I described the <a href=\"https://github.com/hardmaru/estool/\"><code class=\"highlighter-rouge\">estool</code></a> that allowed me to quickly try different ES algorithms with various settings in a distributed processing environment using the MPI framework.</p>\n\n<p>So far, I have only discussed methods for training an agent by having it learn a policy from trial-and-error in the environment. This form of training from scratch is referred to as <em>model-free</em> reinforcement learning. In the next article (<em>if I ever get to writing it</em>), I will discuss more about <em>model-based</em> learning, where our agent will learn to exploit a previously learned model to accomplish a given task. And yes, I will still be using evolution.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you find this work useful, please cite it as:</p>\n\n<p><code>\n@article{ha2017evolving,<br />\n&nbsp;&nbsp;title&nbsp;&nbsp;&nbsp;= \"Evolving Stable Strategies\",<br />\n&nbsp;&nbsp;author&nbsp;&nbsp;= \"Ha, David\",<br />\n&nbsp;&nbsp;journal&nbsp;= \"blog.otoro.net\",<br />\n&nbsp;&nbsp;year&nbsp;&nbsp;&nbsp;&nbsp;= \"2017\",<br />\n&nbsp;&nbsp;url&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= \"https://blog.otoro.net/2017/11/12/evolving-stable-strategies/\"<br />\n}\n</code></p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>I want to thank <a href=\"https://twitter.com/erwincoumans\">Erwin Coumans</a> for writing all these great environments, and also for helping me work on making \n<a href=\"https://github.com/hardmaru/estool\">ESTool</a> better. Great research cannot be done without great tools.</p>\n\n<center>\n<img src=\"/assets/20171109/biped/biped_cma.gif\" width=\"100%\" /><br />\n<i>In the end, it all comes to choices to turn stumbling blocks into stepping stones.</i><br />\n</center>\n<p></p>\n\n<h2 id=\"interesting-links\">Interesting Links</h2>\n\n<p><a href=\"https://www.youtube.com/watch?v=35VE9WykH1c\">“Fires of a Revolution” Incredible Fast Piano Music (EPIC)</a></p>\n\n<p><a href=\"/2017/10/29/visual-evolution-strategies/\">A Visual Guide to Evolution Strategies</a></p>\n\n<p><a href=\"https://github.com/hardmaru/estool\">ESTool</a></p>\n\n<p><a href=\"http://tuvalu.santafe.edu/~erica/stable.pdf\">Stable or Robust? What’s the Difference?</a></p>\n\n<p><a href=\"https://gym.openai.com/docs/\">OpenAI Gym Docs</a></p>\n\n<p><a href=\"https://blog.openai.com/evolution-strategies/\">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></p>\n\n<p><a href=\"http://edwardlib.org/\">Edward, A library for probabilistic modeling, inference, and criticism</a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=FD8l2vPU5FY\">History of Bayesian Neural Networks</a></p>\n\n<p><a href=\"https://gym.openai.com/envs/BipedalWalkerHardcore-v2/\">BipedalWalkerHardcore-v2</a></p>\n\n<p><a href=\"https://blog.openai.com/roboschool/\">roboschool</a></p>\n\n<p><a href=\"https://pybullet.org\">pybullet</a></p>\n\n<p><a href=\"https://arxiv.org/abs/1710.03748\">Emergent Complexity via Multi-Agent Competition</a></p>\n\n<p><a href=\"https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html\">GraspGAN</a></p>\n\n<script>\nvar replace_list=[\n[\n\"learning_to_fall_img\",\n\"/assets/20171109/biped/learning_to_fall.gif\"\n],[\n\"learning_local_optima_img\",\n\"/assets/20171109/biped/learning_local_optima.gif\"\n],[\n\"biped_pepg_final_01_img\",\n\"/assets/20171109/biped/biped_pepg_final_01.gif\"\n],[\n\"biped_pepg_final_02_img\",\n\"/assets/20171109/biped/biped_pepg_final_02.gif\"\n],[\n\"biped_oes_img\",\n\"/assets/20171109/biped/biped_oes.gif\"\n],[\n\"biped_noisy_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/bipedstoc/biped_noisy.gif\"\n],[\n\"kuka_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/kuka/kuka.gif\"\n],[\n\"shaolin_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/shaolin.gif\"\n],[\n\"learning_to_drift_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/learning_to_drift.gif\"\n],[\n\"minitaur_faster_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/minitaur_faster.gif\"\n],[\n\"real_minitaur_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/real_minitaur.gif\"\n],[\n\"duck_notrain_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/duck_notrain.gif\"\n],[\n\"ball_cheating_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/ball_cheating.gif\"\n],[\n\"ball_stoc_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/ball_stoc.gif\"\n],[\n\"duck_stoc_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/minitaur/duck_stoc.gif\"\n],[\n\"roboschool_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/robo/roboschool.gif\"\n],[\n\"reacher_img\",\n\"/assets/20171109/robo/reacher.gif\"\n],[\n\"simple_racecar_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/robo/simple_racecar.gif\"\n],[\n\"bullet_ant_demo_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/robo/bullet_ant_demo.gif\"\n],[\n\"bullet_ant_img\",\n\"https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/robo/bullet_ant.gif\"\n]];\n\nfunction replace_jpeg(tagname, newurl, time_delay) {\n  setTimeout(function(){\n    var img;\n    console.log('replacing '+tagname+' with a gif.');\n    img = document.getElementById(tagname);\n    img.src = newurl;\n  }, time_delay*1000);\n}\n\nfor(var i=0;i<replace_list.length;i++) {\n  replace_jpeg(replace_list[i][0], replace_list[i][1], 5+5*i);\n}\n\n</script>",
  "pubDate": "Sun, 12 Nov 2017 00:00:00 -0600"
}