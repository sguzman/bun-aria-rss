{
  "title": "Convergence in KL Divergence of the Inexact Langevin Algorithm with Application to Score-based Generative Models. (arXiv:2211.01512v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.01512",
  "description": "<p>We study the Inexact Langevin Algorithm (ILA) for sampling using estimated\nscore function when the target distribution satisfies log-Sobolev inequality\n(LSI), motivated by Score-based Generative Modeling (SGM). We prove a long-term\nconvergence in Kullback-Leibler (KL) divergence under a sufficient assumption\nthat the error of the score estimator has a bounded Moment Generating Function\n(MGF). Our assumption is weaker than $L^\\infty$ (which is too strong to hold in\npractice) and stronger than $L^2$ error assumption, which we show not\nsufficient to guarantee convergence in general. Under the $L^\\infty$ error\nassumption, we additionally prove convergence in R\\'enyi divergence, which is\nstronger than KL divergence. We then study how to get a provably accurate score\nestimator which satisfies bounded MGF assumption for LSI target distributions,\nby using an estimator based on kernel density estimation. Together with the\nconvergence results, we yield the first end-to-end convergence guarantee for\nILA in the population level. Last, we generalize our convergence analysis to\nSGM and derive a complexity guarantee in KL divergence for data satisfying LSI\nunder MGF-accurate score estimator.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Wibisono_A/0/1/0/all/0/1\">Andre Wibisono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaylee Yingxi Yang</a>"
}