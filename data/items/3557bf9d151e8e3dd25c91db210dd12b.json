{
  "title": "Use Github Samples with Amazon SageMaker Data Wrangler",
  "link": "https://aws.amazon.com/blogs/machine-learning/use-github-samples-with-amazon-sagemaker-data-wrangler/",
  "dc:creator": "Isha Dua",
  "pubDate": "Fri, 04 Nov 2022 15:41:02 +0000",
  "category": [
    "Amazon SageMaker",
    "Amazon SageMaker Data Wrangler",
    "Artificial Intelligence"
  ],
  "guid": "8b2d3d5ba7ec2e86b8ed42463c66bea696dab094",
  "description": "Amazon SageMaker Data Wrangler is a UI-based data preparation tool that helps perform data analysis, preprocessing, and visualization with features to clean, transform, and prepare data faster. Data Wrangler pre-built flow templates help make data preparation quicker for data scientists and machine learning (ML) practitioners by helping you accelerate and understand best practice patterns for […]",
  "content:encoded": "<p><a href=\"https://aws.amazon.com/sagemaker/data-wrangler/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&amp;sagemaker-data-wrangler-whats-new.sort-order=desc\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMake</a><a href=\"https://aws.amazon.com/sagemaker/data-wrangler/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&amp;sagemaker-data-wrangler-whats-new.sort-order=desc\" target=\"_blank\" rel=\"noopener noreferrer\">r Data Wrangler</a> is a UI-based data preparation tool that helps perform data analysis, preprocessing, and visualization with features to clean, transform, and prepare data faster. Data Wrangler pre-built flow templates help make data preparation quicker for data scientists and machine learning (ML) practitioners by helping you accelerate and understand best practice patterns for data flows using common datasets.</p> \n<p>You can use Data Wrangler flows to perform the following tasks:</p> \n<ul> \n <li><strong>Data visualization</strong> – Examining statistical properties for each column in the dataset, building histograms, studying outliers</li> \n <li><strong>Data cleaning </strong>– Removing duplicates, dropping or filling entries with missing values, removing outliers</li> \n <li><strong>Data enrichment and feature engineering</strong> – Processing columns to build more expressive features, selecting a subset of features for training</li> \n</ul> \n<p>This post will help you understand Data Wrangler using the following sample pre-built flows on <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>. The repository showcases tabular data transformation, time series data transformations, and joined dataset transforms. Each requires a different type of transformations because of their basic nature. Standard tabular or cross-sectional data is collected at a specific point in time. In contrast, time series data is captured repeatedly over time, with each successive data point dependent on its past values.</p> \n<p>Let’s look at an example of how we can use the sample data flow for tabular data.</p> \n<h2>Prerequisites</h2> \n<p>Data Wrangler is an <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker</a> feature available within <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Studio</a>, so we need to follow the Studio onboarding process to spin up the Studio environment and notebooks. Although you can choose from a few authentication methods, the simplest way to create a Studio domain is to follow the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html\" target=\"_blank\" rel=\"noopener noreferrer\">Quick start</a> instructions. The Quick start uses the same default settings as the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-iam.html\" target=\"_blank\" rel=\"noopener noreferrer\">standard Studio setup</a>. You can also choose to onboard using <a href=\"https://aws.amazon.com/single-sign-on/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS IAM Identity Center</a> (successor to AWS Single Sign-On) for authentication (see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-sso-users.html\" target=\"_blank\" rel=\"noopener noreferrer\">Onboard to Amazon SageMaker Domain Using IAM Identity Center</a>).</p> \n<h2>Import the dataset and flow files into Data Wrangler using Studio</h2> \n<p>The following steps outline how to import data into SageMaker to be consumed by Data Wrangler:</p> \n<p>Initialize Data Wrangler via the Studio UI by choosing <strong>New data flow</strong>.</p> \n<p><img loading=\"lazy\" class=\"wp-image-44487 size-full alignnone\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image001-7.jpg\" alt=\"ML-10599-sm-landing-screen\" width=\"1288\" height=\"763\"></p> \n<p>Clone the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a> to download the flow files into your Studio environment.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44488 size-full\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image002-3.jpg\" alt=\"\" width=\"1287\" height=\"404\"></p> \n<p>When the clone is complete, you should be able to see the repository content in the left pane.</p> \n<p><img loading=\"lazy\" class=\"wp-image-44489 size-full alignleft\" style=\"font-size: 16px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image003-2.jpg\" alt=\"\" width=\"1728\" height=\"1054\"></p> \n<p>Choose the file <strong>Hotel-Bookings-Classification.flow</strong> to import the flow file into Data Wrangler.</p> \n<p>If you use the time series or joined data flow, the flow will appear as a different name.After the flow has been imported, you should see the following screenshot. This shows us errors because we need to make sure that the flow file points to the correct data source in <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Simple Storage Service</a> (Amazon S3).</p> \n<p><img loading=\"lazy\" class=\"wp-image-44490 size-full alignleft\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image004-3.jpg\" alt=\"\" width=\"1288\" height=\"535\"></p> \n<p>Choose <strong>Edit dataset</strong> to bring up all your S3 buckets. Next, choose the dataset <code>hotel_bookings.csv</code> from your S3 bucket for running through the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler/tabular-dataflow\" target=\"_blank\" rel=\"noopener noreferrer\">tabular data flow</a>.</p> \n<p>Note that if you’re using the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler/joined-dataflow\" target=\"_blank\" rel=\"noopener noreferrer\">joined data flow</a>, you may have to import multiple datasets into Data Wrangler<img loading=\"lazy\" class=\"wp-image-44491 size-full alignnone\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image005-1.jpg\" alt=\"\" width=\"1288\" height=\"311\"></p> \n<p>In the right pane, make sure <strong>COMMA</strong> is chosen as the delimiter and <strong>Sampling</strong> is set to <strong>First K</strong>. Our dataset is small enough to run Data Wrangler transformations on the full dataset, but we wanted to highlight how you can import the dataset. If you have a large dataset, consider using sampling. Choose <strong>Import</strong> to import this dataset to Data Wrangler.</p> \n<p><img loading=\"lazy\" class=\"wp-image-44492 size-full alignnone\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image006-1.jpg\" alt=\"\" width=\"1288\" height=\"729\"></p> \n<p>After the dataset is imported, Data Wrangler automatically validates the dataset and detects the data types. You can see that the errors have gone away because we’re pointing to the correct dataset. The flow editor now shows two blocks showcasing that the data was imported from a source and data types recognized. You can also edit the data types if needed.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44493 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image007-1.jpg\" alt=\"\" width=\"1287\" height=\"735\"></p> \n<p>The following screenshot shows our data types.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44494 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image008-4.jpg\" alt=\"\" width=\"1288\" height=\"717\"></p> \n<p>Let’s look at some of the transforms done as a part of this tabular flow. If you’re using the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler/timeseries-dataflow\" target=\"_blank\" rel=\"noopener noreferrer\">time series</a> or <a href=\"https://github.com/ishneetdua/sagemaker-datawrangler/tree/main/joined-dataflow\" target=\"_blank\" rel=\"noopener noreferrer\">joined</a> data flows, check out some common transforms on the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler/joined-dataflow\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>. We performed some basic exploratory data analysis using data insights reports that studied the target leakage and feature collinearity in the dataset, table summary analyses, and quick modeling capability. Explore the steps on the <a href=\"https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-datawrangler/tabular-dataflow/data-transformations/Data-Transformations.md\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p> \n<p>Now we drop columns based on the recommendations provided by the Data Insights and Quality Report.</p> \n<ul> \n <li>For target leakage, drop <strong>reservation_status</strong>.</li> \n <li>For redundant columns, drop <strong>days_in_waiting_list, hotel, reserved_room_type, arrival_date_month, reservation_status_date, babies, </strong>and<strong> arrival_date_day_of_month</strong>.</li> \n <li>Based on linear correlation results, drop columns <strong>arrival_date_week_number </strong>and<strong> arrival_date_year</strong> because the correlation values for these feature (column) pairs are greater than the recommended threshold of 0.90.</li> \n <li>Based on non-linear correlation results, drop <strong>reservation_status</strong>. This column was already marked to be dropped based on the target leakage analysis.</li> \n <li>Process numeric values (min-max scaling) for <strong>lead_time, stays_in_weekend_nights, stays_in_weekday_nights, is_repeated_guest, prev_cancellations, prev_bookings_not_canceled, booking_changes, adr, total_of_specical_requests, </strong>and<strong> required_car_parking_spaces</strong>.</li> \n <li>One-hot encode categorical variables like <strong>meal, is_repeated_guest, market_segment, assigned_room_type, deposit_type, </strong>and<strong> customer_type</strong>.</li> \n <li>Balance the target variable Random oversample for class imbalance.Use the quick modeling capability to handle outliers and missing values.</li> \n</ul> \n<p><img loading=\"lazy\" class=\"wp-image-44495 size-full alignnone\" style=\"font-size: 16px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image009-1.jpg\" alt=\"\" width=\"1287\" height=\"713\"></p> \n<h2>Export to Amazon S3</h2> \n<p>Now we have gone through the different transforms and are ready to export the data to Amazon S3. This option creates a SageMaker processing job, which runs the Data Wrangler processing flow and saves the resulting dataset to a specified S3 bucket. Follow the next steps to set up the export to Amazon S3:</p> \n<p>Choose the plus sign next to a collection of transformation elements and choose <strong>Add destination</strong>, then <strong>Amazon S3</strong>.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44496 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image010-3.jpg\" alt=\"\" width=\"1288\" height=\"902\"></p> \n<ul> \n <li>For <strong>Dataset name</strong>, enter a name for the new dataset, for example <code>NYC_export</code>.</li> \n <li>For <strong>File type</strong>, choose <strong>CSV</strong>.</li> \n <li>For <strong>Delimiter</strong>, choose <strong>Comma</strong>.</li> \n <li>For <strong>Compression</strong>, choose <strong>None</strong>.</li> \n <li>For <strong>Amazon S3 location</strong>, use the same bucket name that we created earlier.</li> \n <li>Choose <strong>Add destination</strong>.</li> \n</ul> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44497 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image011-1.jpg\" alt=\"\" width=\"1287\" height=\"914\"></p> \n<p>Choose <strong>Create job</strong>.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44498 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image012-2.jpg\" alt=\"\" width=\"1288\" height=\"722\"></p> \n<p>For <strong>Job name</strong>, enter a name or keep the autogenerated option and choose <strong>destination</strong>. We have only one destination, <code>S3:testingtabulardata</code>, but you might have multiple destinations from different steps in your workflow. Leave the <strong>KMS key ARN</strong> field empty and choose <strong>Next</strong>.</p> \n<p>Now you have to configure the compute capacity for a job. You can keep all default values for this example.</p> \n<ul> \n <li>For <strong>Instance type</strong>, use ml.m5.4xlarge.</li> \n <li>For <strong>Instance count</strong>, use 2.</li> \n <li>You can explore <strong>Additional configuration</strong>, but keep the default settings.</li> \n <li>Choose <strong>Run</strong>.</li> \n</ul> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44499 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image013.jpg\" alt=\"\" width=\"1288\" height=\"729\"></p> \n<p>Now your job has started, and it takes some time to process 6 GB of data according to our Data Wrangler processing flow. The cost for this job will be around $2 USD, because ml.m5.4xlarge costs $0.922 USD per hour and we’re using two of them.</p> \n<p>If you choose the job name, you’re redirected to a new window with the job details.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44500 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image014-4.jpg\" alt=\"\" width=\"1287\" height=\"727\"></p> \n<p>On the job details page, you can see all the parameters from the previous steps.</p> \n<p>When the job status changes to Completed, you can also check the <strong>Processing time (seconds)</strong> value. This processing job takes around 5–10 minutes to complete.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44501 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image015-2.jpg\" alt=\"\" width=\"1289\" height=\"733\"></p> \n<p>When the job is complete, the train and test output files are available in the corresponding S3 output folders. You can find the output location from the processing job configurations.</p> \n<p><img loading=\"lazy\" class=\"alignnone wp-image-44502 size-full\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/image016-3.jpg\" alt=\"\" width=\"1289\" height=\"728\"></p> \n<p>After the Data Wrangler processing job is complete, we can check the results saved in our S3 bucket. Don’t forget to update the <code>job_name</code> variable with your job name.</p> \n<p>You can now use this exported data for running ML models.</p> \n<h2>Clean up</h2> \n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-bucket.html\" target=\"_blank\" rel=\"noopener noreferrer\">Delete your S3 buckets</a> and your <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-shut-down.html\" target=\"_blank\" rel=\"noopener noreferrer\">Data Wrangler flow</a> in order to delete the underlying resources and prevent unwanted costs after you finish the experiment.</p> \n<h2>Conclusion</h2> \n<p>In this post, we showed how you can import the tabular pre-built data flow into Data Wrangler, plug it against our dataset, and export the results to Amazon S3. If your use cases require you to manipulate time series data or join multiple datasets, you can go through the other pre-built sample flows in the <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p> \n<p>After you have imported a pre-built data prep workflow, you can integrate it with Amazon SageMaker Processing, <a href=\"https://aws.amazon.com/sagemaker/pipelines/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Pipelines</a>, and <a href=\"https://aws.amazon.com/sagemaker/feature-store/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Feature Store</a> to simplify the task of processing, sharing, and storing ML training data. You can also export this sample data flow to a Python script and create a custom ML data prep pipeline, thereby accelerating your release velocity.</p> \n<p>We encourage you to check out our <a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-datawrangler\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repository</a> to get hands-on practice and find new ways to improve model accuracy! To learn more about SageMaker, visit the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Developer Guide</a>.</p> \n<hr> \n<h3>About the Authors</h3> \n<p><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/isha-dua.jpg\"><img loading=\"lazy\" class=\"size-full wp-image-44537 alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/20/isha-dua.jpg\" alt=\"\" width=\"100\" height=\"126\"></a>Isha Dua</strong> is a Senior Solutions Architect based in the San Francisco Bay Area. She helps AWS Enterprise customers grow by understanding their goals and challenges, and guides them on how they can architect their applications in a cloud-native manner while making sure they are resilient and scalable. She’s passionate about machine learning technologies and environmental sustainability.</p>"
}