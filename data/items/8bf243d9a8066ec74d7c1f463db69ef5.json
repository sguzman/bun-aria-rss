{
  "title": "crazy large batch sizes",
  "link": "http://matpalm.com/blog/crazy_large_batch_sizes",
  "category": [
    "quick_hack",
    "tpu",
    "jax"
  ],
  "guid": "http://matpalm.com/blog/crazy_large_batch_sizes",
  "description": "crazy large batch sizes",
  "content:encoded": "<h1>\"use a bigger batch\"</h1>\n<p>a classic piece of advice i hear for people using tpus is that\n   they should \"try using a bigger batch size\".\n</p>\n<p>this got me thinking;\n   i wonder how big a batch size i could reasonably use?\n   how would the optimisation go?\n   how fast could i get things?\n</p>\n\n<h1>dataset</h1>\n<p>let's train a model on the\n   <a href=\"https://github.com/phelber/eurosat\">eurosat/rgb dataset</a>.\n   it's a 10 way classification problem on 64x64 images\n</p>\n<img src=\"/blog/imgs/2020/en/sample_images.png\"/>\n\n<p>with a training split of 80% we have 21,600 training examples.\n   we'll use another 10% for validation (2,700 images)\n   and just not use the final 10% test split #hack\n</p>\n\n<h1>model</h1>\n<p>for the\n   <a href=\"https://github.com/matpalm/large_batch/blob/master/model.py\">model</a>\n   we'll use a simple stack of convolutions with\n   channel sizes 32, 64, 128 and 256, a stride of 2 for spatial reduction\n   all with gelu activation. after the convolutions we'll do a simple global\n   spatial pooling, a single 128d dense layer with gelu and then a\n   10d logit output. a pretty vanilla architecture of ~400K params. nothing fancy.\n</p>\n\n<h1>splitting up the data</h1>\n<p>a v3-32 tpu pod slice is 4 hosts, each with 8 tpu devices.\n</p>\n<p>21,600 training examples total => 5,400 examples per host => 675 examples per device.\n</p>\n<p>this number of images easily fits on a device. great.\n</p>\n\n<h2>augmentation</h2>\n<p>now usually augmentation is something we do randomly per batch, but for this hack\n   we're interested in seeing how big a batch we can run. so why not\n   fill out the dataset a bit by just running a stack of augmentations before training?\n</p>\n<p>for each image we'll do 90, 180 and 270 deg rotations along with left/right flips\n   for a total of 8 augmented images for each original image. e.g.....\n</p>\n<img src=\"/blog/imgs/2021/lb/augmentations.png\"/>\n\n<p>this gives us now 172,800 images total => 43,200 per host => 5,400 per tpu device.\n   which stills fits no problem.\n</p>\n<p>side note: turns out doing this augmentation was one of the most fun parts of\n   this hack :)\n   see <a href=\"https://twitter.com/mat_kelcey/status/1360555768630501377\">this tweet thread</a>\n   for some more info on how i used nested pmaps and vmaps to do it!\n</p>\n\n<h2>optimisers?</h2>\n<p>one motivation i had for this hack was to compare adam to lamb. i'd\n   seen lamb referred to in the past, would it perform better for this model/dataset size?\n   turns out it does! a simple sweep comparing lamb, adam and sgd shows lamb consistently\n   doing the best. definitely one to add to the tuning mix from now on.\n</p>\n\n<h2>data / model / optimiser state placement</h2>\n<p>not only does the augmented data fit sharded across devices but we can replicate\n   both the model parameters and the optimiser state as well. this is important\n   for speed since the main training loop doesn't have to do any host/device\n   communication. taking a data parallel approach means the only cross device\n   comms is a gradient psum.\n</p>\n\n<h1>results</h1>\n<p>for training we run an inner loop just pumping the <code>param = update(params)</code> step.\n</p>\n<p>an outer loop runs the inner loop 100 times before doing a validation accuracy check.\n</p>\n<p>the inner loop runs at 1.5s for the 100 iterations and since each iteration is a\n   forward & backwards pass for all 172,800 images across all hosts that's 11M images\n   processed per second. ðŸ”¥ðŸ”¥ðŸ”¥\n</p>\n<p>at this speed the best result of 0.95 on validation takes 13 outer loops;\n   i.e. all done in under 20s. o_O !!\n</p>\n<p>when reviewing runs i did laugh to see sgd with momentum make a top 10 entry.\n</p>\n<p><em>new t-shirt slogan: \"sgd with momentum; always worth a try\"</em>\n</p>\n<img src=\"/blog/imgs/2021/lb/top_10.png\"/>\n\n\n<h1>code</h1>\n<p>all the code in hacktastic undocumented form\n   <a href=\"https://github.com/matpalm/large_batch\">on github</a>\n</p>"
}