{
  "title": "FIGS: Attaining XGBoost-level performance with the interpretability and speed of CART",
  "description": "<!-- twitter -->\n<meta name=\"twitter:title\" content=\"FIGS: Attaining XGBoost-level performance =with the interpretability and speed of CART\" />\n\n<meta name=\"twitter:card\" content=\"summary_large_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/figs/figs_intro.gif\" />\n\n<meta name=\"keywords\" content=\"figs,interpretability,trees,imodels\" />\n\n<meta name=\"description\" content=\"The BAIR Blog\" />\n\n<meta name=\"author\" content=\"Chandan Singh, Yan Shuo Tan, Bin Yu\" />\n\n<p style=\"text-align:center;\">\n    <a href=\"https://arxiv.org/abs/2201.11931\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_intro.gif\" width=\"90%\" /></a>\n<br />\n<b>FIGS (Fast Interpretable Greedy-tree Sums): </b><i>A method for building interpretable models by simultaneously growing an ensemble of decision trees in competition with one another.</i>\n</p>\n\n<p>Recent machine-learning advances have led to increasingly complex predictive models, often at the cost of interpretability. We often need interpretability, particularly in high-stakes applications such as in clinical decision-making; interpretable models help with all kinds of things, such as identifying errors, leveraging domain knowledge, and making speedy predictions.</p>\n\n<p>In this blog post we’ll cover <a href=\"https://arxiv.org/abs/2201.11931\">FIGS</a>, a new method for fitting an <em>interpretable model</em> that takes the form of a sum of trees. Real-world experiments and theoretical results show that FIGS can effectively adapt to a wide range of structure in data, achieving state-of-the-art performance in several settings, all without sacrificing interpretability.\n<!--more--></p>\n\n<h2 id=\"how-does-figs-work\">How does FIGS work?</h2>\n\n<p>Intuitively, FIGS works by extending CART, a typical greedy algorithm for growing a decision tree, to consider growing a <em>sum</em> of trees <em>simultaneously</em> (see Fig 1). At each iteration, FIGS may grow any existing tree it has already started or start a new tree; it greedily selects whichever rule reduces the total unexplained variance (or an alternative splitting criterion) the most. To keep the trees in sync with one another, each tree is made to predict the <em>residuals</em> remaining after summing the predictions of all other trees (see <a href=\"https://arxiv.org/abs/2201.11931\">the paper</a> for more details).</p>\n\n<p>FIGS is intuitively similar to ensemble approaches such as gradient boosting / random forest, but importantly since all trees are grown to compete with each other the model can adapt more to the underlying structure in the data. The number of trees and size/shape of each tree emerge automatically from the data rather than being manually specified.</p>\n\n<p style=\"text-align:center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_fitting.gif\" width=\"90%\" /></a>\n<br />\n<b>Fig 1. </b><i>High-level intuition for how FIGS fits a model.</i>\n</p>\n\n<h2 id=\"an-example-using-figs\">An example using <code class=\"language-plaintext highlighter-rouge\">FIGS</code></h2>\n\n<p>Using FIGS is extremely simple. It is easily installable through the <a href=\"https://github.com/csinva/imodels\">imodels package</a> (<code class=\"language-plaintext highlighter-rouge\">pip install imodels</code>) and then can be used in the same way as standard scikit-learn models: simply import a classifier or regressor and use the <code class=\"language-plaintext highlighter-rouge\">fit</code> and <code class=\"language-plaintext highlighter-rouge\">predict</code> methods. Here’s a full example of using it on a sample clinical dataset in which the target is risk of cervical spine injury (CSI).</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">imodels</span> <span class=\"kn\">import</span> <span class=\"n\">FIGSClassifier</span><span class=\"p\">,</span> <span class=\"n\">get_clean_dataset</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n\n<span class=\"c1\"># prepare data (in this a sample clinical dataset)\n</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">feat_names</span> <span class=\"o\">=</span> <span class=\"n\">get_clean_dataset</span><span class=\"p\">(</span><span class=\"s\">'csi_pecarn_pred'</span><span class=\"p\">)</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.33</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># fit the model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FIGSClassifier</span><span class=\"p\">(</span><span class=\"n\">max_rules</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>  <span class=\"c1\"># initialize a model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>   <span class=\"c1\"># fit model\n</span><span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"c1\"># discrete predictions: shape is (n_test, 1)\n</span><span class=\"n\">preds_proba</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"c1\"># predicted probabilities: shape is (n_test, n_classes)\n</span>\n<span class=\"c1\"># visualize the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">feature_names</span><span class=\"o\">=</span><span class=\"n\">feat_names</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"s\">'out.svg'</span><span class=\"p\">,</span> <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This results in a simple model – it contains only 4 splits (since we specified that the model should have no more than 4 splits (<code class=\"language-plaintext highlighter-rouge\">max_rules=4</code>). Predictions are made by dropping a sample down every tree, and <i>summing</i> the risk adjustment values obtained from the resulting leaves of each tree. This model is extremely interpretable, as a physician can now (i) easily make predictions using the 4 relevant features and (ii) vet the model to ensure it matches their domain expertise. Note that this model is just for illustration purposes, and achieves ~84\\% accuracy.</p>\n\n<p style=\"text-align:center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_csi_model_small.svg\" width=\"85%\" /></a>\n<br />\n<i><b>Fig 2.</b> Simple model learned by FIGS for predicting risk of cervical spinal injury. </i>\n</p>\n\n<p>If we want a more flexible model, we can also remove the constraint on the number of rules (changing the code to <code class=\"language-plaintext highlighter-rouge\">model = FIGSClassifier()</code>), resulting in a larger model (see Fig 3). Note that the number of trees and how balanced they are emerges from the structure of the data – only the total number of rules may be specified.</p>\n\n<p style=\"text-align:center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_csi_model_large.svg\" width=\"100%\" /></a>\n<br />\n<i><b>Fig 3.</b> Slightly larger model learned by FIGS for predicting risk of cervical spinal injury. </i>\n</p>\n\n<h2 id=\"how-well-does-figs-perform\">How well does FIGS perform?</h2>\n\n<p>In many cases when interpretability is desired, such as <a href=\"https://arxiv.org/abs/2205.15135\">clinical-decision-rule modeling</a>, FIGS is able to achieve state-of-the-art performance. For example, Fig 4 shows different datasets where FIGS achieves excellent performance, particularly when limited to using very few total splits.</p>\n\n<p style=\"text-align:center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_classification.png\" width=\"100%\" /></a>\n<br />\n<i><b>Fig 4.</b> FIGS predicts well with very few splits. </i>\n</p>\n\n<h2 id=\"why-does-figs-perform-well\">Why does FIGS perform well?</h2>\n\n<p>FIGS is motivated by the observation that single decision trees often have splits that are repeated in different branches, which may occur when there is <a href=\"https://proceedings.mlr.press/v151/shuo-tan22a/shuo-tan22a.pdf\">additive structure</a> in the data. Having multiple trees helps to avoid this by disentangling the additive components into separate trees.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Overall, interpretable modeling offers an alternative to common black-box modeling, and in many cases can offer massive improvements in terms of efficiency and transparency without suffering from a loss in performance.</p>\n\n<hr />\n\n<p><em>This post is based on two papers: <a href=\"https://arxiv.org/abs/2201.11931\">FIGS</a> and <a href=\"https://arxiv.org/abs/2205.15135\">G-FIGS</a> – all code is available through the <a href=\"https://github.com/csinva/imodels\">imodels package</a>. This is joint work with <a href=\"https://www.linkedin.com/in/nasseri/\">Keyan Nasseri</a>, <a href=\"https://www.linkedin.com/in/abhineet-agarwal-126171185/\">Abhineet Agarwal</a>, <a href=\"https://www.linkedin.com/in/james-pc-duncan/\">James Duncan</a>, <a href=\"https://www.linkedin.com/in/omer-ronen-48ba9412a/?originalSubdomain=il\">Omer Ronen</a>, and <a href=\"https://profiles.ucsf.edu/aaron.kornblith\">Aaron Kornblith</a>.</em></p>",
  "pubDate": "Thu, 30 Jun 2022 02:00:00 -0700",
  "link": "http://bair.berkeley.edu/blog/2022/06/30/figs/",
  "guid": "http://bair.berkeley.edu/blog/2022/06/30/figs/"
}