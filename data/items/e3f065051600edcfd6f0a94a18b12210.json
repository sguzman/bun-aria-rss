{
  "title": "Towards a new Theory of Learning: Statistical Mechanics of Deep Neural Networks",
  "link": "https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/",
  "comments": "https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/#comments",
  "dc:creator": "Charles H Martin, PhD",
  "pubDate": "Tue, 03 Dec 2019 16:14:04 +0000",
  "category": "Uncategorized",
  "guid": "http://calculatedcontent.com/?p=13259",
  "description": "Introduction For the past few years, we have talked a lot about how we can understand the properties of Deep &#8230; <a class=\"more-link\" href=\"https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/\">More</a>",
  "content:encoded": "\n<h2>Introduction</h2>\n\n\n\n<p>For the past few years, we have talked a lot about how we can understand the properties of Deep Neural Networks by examining the spectral properties of the layer weight matrices <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W} \" class=\"latex\" />. Specifically, we can form the correlation matrix </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X} =&#92;frac{1}{N}&#92;mathbf{W}^{T}&#92;mathbf{W}  \" class=\"latex\" />, </p>\n\n\n\n<p>and compute the eigenvalues <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda  \" class=\"latex\" /></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X}{&#92;mathbf{e}}=&#92;lambda{&#92;mathbf{e}}  \" class=\"latex\" />.</p>\n\n\n\n<p>By plotting the histogram of the eigenvalues (i.e the spectral density <img src=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho(&#92;lambda) \" class=\"latex\" />), we can monitor the training process and gain insight into the implicit regularization and convergence properties of DNN. Indeed, we have identified </p>\n\n\n\n<h3>5+1 Phases of Training</h3>\n\n\n\n<figure class=\"wp-block-image size-large\"><img data-attachment-id=\"13381\" data-permalink=\"https://calculatedcontent.com/screen-shot-2019-11-29-at-10-28-02-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png\" data-orig-size=\"1278,734\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2019-11-29-at-10.28.02-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=1024\" alt=\"\" class=\"wp-image-13381\" srcset=\"https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2019/11/screen-shot-2019-11-29-at-10.28.02-pm.png 1278w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p>Each of these phases roughly corresponds to a Universality class from Random matrix Theory (RMT).   And as we shall see below, we can use RMT to develop a new theory of learning.</p>\n\n\n\n<p>First, however, we note that for nearly every pretrained DNNs we have examined (and we have examined thousands of them), the phase appears to be in somewhere between Bulk-Decay and/or Heavy-Tailed .</p>\n\n\n\n<h3>Heavy Tailed Implicit Regularization</h3>\n\n\n\n<p> Moreover, for nearly all layers in all well trained, production quality DNNs, the layer spectral density <img src=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho(&#92;lambda) \" class=\"latex\" /> can be fit to a truncated power law,  with exponents frequently lying in the Fat Tailed range  [2-4], and the maximum eigenvalue no larger than say 100</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho(&#92;lambda)&#92;sim&#92;lambda^{-&#92;alpha}, \" class=\"latex\" /></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Ctext%7Bwhere%7D%5C%3B%5C%3B+%5Calpha%5Cin%5B2%2C4%5D+%2C%5C%3B%5C%3B%5Clambda_%7Bmax%7D%3C100+++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Ctext%7Bwhere%7D%5C%3B%5C%3B+%5Calpha%5Cin%5B2%2C4%5D+%2C%5C%3B%5C%3B%5Clambda_%7Bmax%7D%3C100+++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext%7Bwhere%7D%5C%3B%5C%3B+%5Calpha%5Cin%5B2%2C4%5D+%2C%5C%3B%5C%3B%5Clambda_%7Bmax%7D%3C100+++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;text{where}&#92;;&#92;; &#92;alpha&#92;in[2,4] ,&#92;;&#92;;&#92;lambda_{max}<100   \" class=\"latex\" />,</p>\n\n\n\n<p>Most importantly,  in 80-90% of the DNN architectures studied, on average, smaller exponents <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha \" class=\"latex\" /> correspond to smaller test errors.    </p>\n\n\n\n<h3> DNN Quality Metrics in Practice</h3>\n\n\n\n<p>Our empirical results suggest that the power law exponent can be used as (part of) a practical quality metric.  This led us to propose the <img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}\" class=\"latex\" /> metric for DNNs:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Csum%5Calpha_%7Bi%7D%5Clog%5Clambda_%7Bi%2Cmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Csum%5Calpha_%7Bi%7D%5Clog%5Clambda_%7Bi%2Cmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Chat%7B%5Calpha%7D%3A%3D%5Csum%5Calpha_%7Bi%7D%5Clog%5Clambda_%7Bi%2Cmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;hat{&#92;alpha}:=&#92;sum&#92;alpha_{i}&#92;log&#92;lambda_{i,max}\" class=\"latex\" /></p>\n\n\n\n<p>where we compute the exponent <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" /> and maximum eigenvalue <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{max}\" class=\"latex\" /> for each layer weight matrix (and Conv2D feature maps), and then form the total DNN quality as a simple weighted average of the exponents.  Amazingly, this metric correlates very well with the reported test accuracy of pretrained DNNs (such as the VGG models,  the ResNet models, etc)</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter is-resized\"><img src=\"https://github.com/CalculatedContent/PredictingTestAccuracies/raw/master/img/vgg-w_alphas.png\" alt=\"alt text\" width=\"460\" height=\"460\" /></figure></div>\n\n\n<h4>WeightWatcher</h4>\n\n\n\n<p>We have even built a open source, python command line tool&#8211;<a href=\"https://github.com/CalculatedContent/WeightWatcher\">weightwatcher</a>&#8211;so that other researchers can both reproduce and leverage our results </p>\n\n\n\n<p class=\"has-text-align-center\"><code>pip install weightwatcher</code></p>\n\n\n\n<p>And we have a Slack Channel for those who want to ask questions, dig deeper, and/or contribute to the work.  Email me, or <a href=\"https://www.linkedin.com/in/charlesmartin14/\">ping me on LinkedIn,</a> to join our vibrant group.</p>\n\n\n\n<p><br>All of this leads to a very basic question:  </p>\n\n\n\n<h2 class=\"has-text-align-center\"><strong><em>Why does this work ?</em></strong></h2>\n\n\n\n<p>To answer this, we will go back to the foundations of the theory of learning, from the physics perspective, and rebuild the theory using in both our experimental observations, some older results from Theoretical Physics,  and (fairly) recent results in Random Matrix Theory.</p>\n\n\n\n<h2>Statistical Mechanics of Learning</h2>\n\n\n\n<p>Here, I am going to sketch out the ideas we are currently researching to develop a new theory of generalization for Deep Neural Networks.  We have a lot of work to do, but I think we have made enough progress to present these ideas, informally, to flush out the basics.</p>\n\n\n\n<p><strong>What do we seek ? </strong> A practical theory that can be used to predict the generalization accuracy of a DNN solely by looking at the trained weight matrices, without looking at the test data.  </p>\n\n\n\n<p><strong>Why ? </strong>  Do you test a bridge by driving cars over it until it collapses ?  Of course not!  So why do we build DNNs and only rely on brute force testing ?  Surely we can do better.</p>\n\n\n\n<p><strong>What is the approach ?</strong> We start with the classic Perceptron Student-Teacher model from Statistical Mechanics of the 1990s.  The setup is similar, but the motivations are a bit different.  We have discussed this model earlier here  <a href=\"https://calculatedcontent.com/2018/04/01/rethinking-or-remembering-generalization-in-neural-networks/\">Remembering Generalization in DNNs.</a>  from our paper <a rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/1611.03530\" target=\"_blank\">Understanding Deep Learning Requires Rethinking Generalization</a><a href=\"https://arxiv.org/abs/1611.03530\">.</a>  </p>\n\n\n\n<p>Here, let us review the mathematical setup in some detail:</p>\n\n\n\n<h3>the Student-Teacher model</h3>\n\n\n\n<p>We start with the simple model presented in chapter 2, <a href=\"https://books.google.com/books/about/Statistical_Mechanics_of_Learning.html?id=qVo4IT9ByfQC\">Engel and Van der Brock</a>, interpreted in a modern context.   See also this classic 1992 paper, <em>Statistical Mechanics of Learning from Examples</em>.</p>\n\n\n\n<p>Here, we want to do something a little different, and use the formalism of Statistical Mechanics to both compute the average generalization error, and to interpret the global convergence properties of DNNs in light of this ,  giving us more insight into and to provide a new theory of <a href=\"https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/\">Why Deep Learning Works  (as proposed in 2015)</a>.</p>\n\n\n\n<p>Suppose we have some trained or pretrained DNN (i.e. like VGG19).  We want to compute the average or typical error that our Teacher DNN could make, just by examining the layer weight matrices.  <em>Without peeking at the data.</em></p>\n\n\n\n<p><strong>A Simple Layer Approach:</strong><em> We assume all layers are statistically independent, so that the average generalization quality </em><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q} \" class=\"latex\" />&nbsp;<em>(i.e. 1.0-error) is just the product of the contributions of from each layer weight matrix </em><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_l+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_l+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D_l+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{W}_l \" class=\"latex\" />&nbsp;.</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%3A%3D%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5Cmathcal%7BQ%7D%28%5Cmathbf%7BW%7D_l%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%3A%3D%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5Cmathcal%7BQ%7D%28%5Cmathbf%7BW%7D_l%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%3A%3D%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5Cmathcal%7BQ%7D%28%5Cmathbf%7BW%7D_l%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}:=&#92;underset{l}{&#92;prod}&#92;;&#92;mathcal{Q}(&#92;mathbf{W}_l) \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p><strong>Example:</strong>  The Product Norm is a Capacity (or Quality) measure for DNNs from<a href=\"https://arxiv.org/abs/1808.01174\"> traditional ML theory</a>.</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%5Csim%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5CVert%5Cmathbf%7BW%7D_l%5CVert%3D%5CVert%5Cmathbf%7BW%7D_%7B1%7D%5CVert%5CVert%5Cmathbf%7BW%7D_%7B2%7D%5CVert%5Ccdots%5CVert%5Cmathbf%7BW%7D_%7BL%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%5Csim%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5CVert%5Cmathbf%7BW%7D_l%5CVert%3D%5CVert%5Cmathbf%7BW%7D_%7B1%7D%5CVert%5CVert%5Cmathbf%7BW%7D_%7B2%7D%5CVert%5Ccdots%5CVert%5Cmathbf%7BW%7D_%7BL%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D%5Csim%5Cunderset%7Bl%7D%7B%5Cprod%7D%5C%3B%5CVert%5Cmathbf%7BW%7D_l%5CVert%3D%5CVert%5Cmathbf%7BW%7D_%7B1%7D%5CVert%5CVert%5Cmathbf%7BW%7D_%7B2%7D%5CVert%5Ccdots%5CVert%5Cmathbf%7BW%7D_%7BL%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}&#92;sim&#92;underset{l}{&#92;prod}&#92;;&#92;Vert&#92;mathbf{W}_l&#92;Vert=&#92;Vert&#92;mathbf{W}_{1}&#92;Vert&#92;Vert&#92;mathbf{W}_{2}&#92;Vert&#92;cdots&#92;Vert&#92;mathbf{W}_{L}&#92;Vert \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p>The Norm may be  Frobenius Norm, the Spectral Norm, or even their ratio, the Stable Rank.</p>\n\n\n\n<p>This independence assumption is probably not a great approximation  but it gets us closer to a realistic theory. Indeed, even traditional ML theory recognizes this, and may use Path Norm to correct for this. For now, this will suffice.</p>\n\n\n\n<p><strong>A Log Product Norm </strong>If we take the logarithm of each side, we can write the log Quality as the sum of the layer contributions.  More generally, we will express the log Quality as a weighted average of some (as yet unspecified) log norm of the weight matrix. </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Clog%5Cmathcal%7BQ%7D%5Csim%5Csum%5Climits_%7Bl%7Da_%7Bl%7D%5Clog%5CVert%5Cmathbf%7BW%7D_%7Bl%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clog%5Cmathcal%7BQ%7D%5Csim%5Csum%5Climits_%7Bl%7Da_%7Bl%7D%5Clog%5CVert%5Cmathbf%7BW%7D_%7Bl%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog%5Cmathcal%7BQ%7D%5Csim%5Csum%5Climits_%7Bl%7Da_%7Bl%7D%5Clog%5CVert%5Cmathbf%7BW%7D_%7Bl%7D%5CVert+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;log&#92;mathcal{Q}&#92;sim&#92;sum&#92;limits_{l}a_{l}&#92;log&#92;Vert&#92;mathbf{W}_{l}&#92;Vert \" class=\"latex\" />&nbsp;</p>\n\n\n\n<h4>Quality for a Single Layer: Perceptron model</h4>\n\n\n\n<p>We now set up the classic Student-Teacher model for a Perceptron&#8211;with a slight twist.  That is, from now on, we assume our models have 1 layer, like a Perceptron. </p>\n\n\n\n<p>Let&#8217;s call our trained or pretrained DNN the Teacher <strong>T</strong>.  The Teacher maps data to labels.  Of course, there could be many Teachers which map the same data to the same labels.  For <strong>our</strong> specific purposes here, <strong><em>we just fix the Teacher T</em></strong>.  We imagine that the learning process is for us to learn all possible Student Perceptrons <strong>J</strong> that also map the data to the labels, in the same way as the Teacher.</p>\n\n\n\n<p> But for a pretrained model,  we have no data, and we have no labels.   And that&#8217;s ok.  Following Engle and Van der Brock (and also Engle&#8217;s 2001 paper ), consider the following Figure, which depicts the vector space representations of <strong>T</strong> and <strong>J</strong>.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" data-attachment-id=\"13295\" data-permalink=\"https://calculatedcontent.com/student-teacher/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png\" data-orig-size=\"1126,664\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"student-teacher\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=1024\" alt=\"\" class=\"wp-image-13295\" width=\"591\" height=\"349\" srcset=\"https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=591 591w, https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2019/11/student-teacher.png 1126w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></figure></div>\n\n\n<p>To compute the average generalization error for a given model, we write first need the Student-Teacher (ST) error function <img src=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;epsilon(R)\" class=\"latex\" />.  This is simply the L2 loss between the 2 models, averaged over a random data set.</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29%3D%5Clangle%5Cfrac%7B1%7D%7B2%7D%5By_%7BT%7D-y_%7BS%7D%5D%5E%7B2%7D%5Crangle_%7Bdata%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29%3D%5Clangle%5Cfrac%7B1%7D%7B2%7D%5By_%7BT%7D-y_%7BS%7D%5D%5E%7B2%7D%5Crangle_%7Bdata%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon%28R%29%3D%5Clangle%5Cfrac%7B1%7D%7B2%7D%5By_%7BT%7D-y_%7BS%7D%5D%5E%7B2%7D%5Crangle_%7Bdata%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;epsilon(R)=&#92;langle&#92;frac{1}{2}[y_{T}-y_{S}]^{2}&#92;rangle_{data} \" class=\"latex\" /></p>\n\n\n\n<p>where   <img src=\"https://s0.wp.com/latex.php?latex=y_%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=y_%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y_%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"y_{S} \" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=y_%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=y_%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y_%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"y_{T} \" class=\"latex\" /> are the Student and Teacher labels, respectively.</p>\n\n\n\n<p>For a Linear Perceptron,  the labels are given by</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=y_%7BS%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=y_%7BS%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y_%7BS%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"y_{S}=&#92;mathbf{S}^{T}&#92;mathbf{x} \" class=\"latex\" /></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=y_%7BT%7D%3D%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=y_%7BT%7D%3D%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=y_%7BT%7D%3D%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7Bx%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"y_{T}=&#92;mathbf{T}^{T}&#92;mathbf{x} \" class=\"latex\" />  </p>\n\n\n\n<p>and the ST error function is simply 1 minus the ST overlap</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%3D1-R%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%3D1-R%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon%3D1-R%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;epsilon=1-R,&#92;;&#92;;R = &#92;dfrac{1}{N}&#92;mathbf{S}^{T}&#92;mathbf{T} \" class=\"latex\" /></p>\n\n\n\n<p>For the classic Boolean (Rosenblatt) Perceptron, the ST error  function is with the inverse (arc cosine) of the vector dot product between <strong>S </strong>and <strong>T</strong>:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%3D%5Cfrac%7B1%7D%7B%5Cpi%7D%5Carccos%5C%3BR%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cepsilon%3D%5Cfrac%7B1%7D%7B%5Cpi%7D%5Carccos%5C%3BR%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cepsilon%3D%5Cfrac%7B1%7D%7B%5Cpi%7D%5Carccos%5C%3BR%2C%5C%3B%5C%3BR+%3D+%5Cdfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;epsilon=&#92;frac{1}{&#92;pi}&#92;arccos&#92;;R,&#92;;&#92;;R = &#92;dfrac{1}{N}&#92;mathbf{S}^{T}&#92;mathbf{T} \" class=\"latex\" /></p>\n\n\n\n<p> If we just use the linear error model, then the quality of the Teacher is simply 1 minus the error, or, trivially , the ST overlap </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7BT%7D%28R%29%3DR+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7BT%7D%28R%29%3DR+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7BT%7D%28R%29%3DR+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}_{T}(R)=R \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p>To estimate the average total generalization quality of the Teacher, we write the total Quality as an integral over all possible Student matrices <strong>S</strong></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29R%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29R%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29R%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}_{&#92;mathbf{T}}&#92;simeq&#92;int d&#92;mu(&#92;mathbf{S})R, \" class=\"latex\" />Â </p>\n\n\n\n<p>(and this will turn out to be an approximation to the Annealed Free Energy of the matrix-generalized Linear Percepton&#8230;stay tuned!)</p>\n\n\n\n<p><strong>Fixing the Teacher:</strong> That&#8217;s it.  What&#8217;s so hard about this ?  Normally in Statistical Mechanics, one also has to average over all possible Teachers (T) that look the same &#8212; this complicates the story immensely.  What we have described this for a fixed Teacher, we will use this as a general formalism to derive the weightwatcher AlphaHat metric.</p>\n\n\n\n<p><strong>Our Proposal:</strong>&nbsp; We let <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BS%7D%2C+%5Cmathbf%7BT%7D%5C&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BS%7D%2C+%5Cmathbf%7BT%7D%5C&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BS%7D%2C+%5Cmathbf%7BT%7D%5C&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{S}, &#92;mathbf{T}&#92;\" class=\"latex\" /> be strongly correlated (NxM) real matrices, with truncated, Heavy Tailed ESDs.  Specifically, we assume that we know the Teacher <strong>T</strong> weight matrices exactly, and seek all Student matrices <strong>S</strong> that have the same shape as <strong>T</strong> and the same average spectral properties as <strong>T</strong>.  That is still a lot of Students.</p>\n\n\n\n<p>We can think of the class of Student matrices <strong>S</strong> as all matrices that are close to <strong>T</strong>.  What we really want is the best method for doing this, that has been tested experimentally.  Fortunately, Hinton and coworkers have recently revisited <a href=\"https://arxiv.org/abs/1905.00414\">Similarity of Neural Network Representations</a>, and found the best matrix similarity method is</p>\n\n\n\n<p class=\"has-text-align-left\"><strong>Canonical Correlation Analysis (CCA):&nbsp;</strong> <img src=\"https://s0.wp.com/latex.php?latex=R%3D%5CVert%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5CVert%5E%7B2%7D_%7BF%7D%3DTr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%29%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R%3D%5CVert%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5CVert%5E%7B2%7D_%7BF%7D%3DTr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%29%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%3D%5CVert%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5CVert%5E%7B2%7D_%7BF%7D%3DTr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%29%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R=&#92;Vert&#92;mathbf{S}^{T}&#92;mathbf{T}&#92;Vert^{2}_{F}=Tr[(&#92;mathbf{S}^{T}&#92;mathbf{T})^{2}]\" class=\"latex\" /> </p>\n\n\n\n<p>Using CCA, we can formulate our new, Semi-Empirical Theory with the following conjectures</p>\n\n\n\n<p><strong>Conjecture 1:</strong><em>  We can write the layer matrix contribution to the total average generalization error as an integral over all possible (random) matrices <strong>S</strong> that resemble the actual (pre-)trained weight matrices </em><strong>T</strong> (as given above),, such that</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}_{&#92;mathbf{T}}&#92;simeq&#92;int d&#92;mu(&#92;mathbf{S})&#92;left(Tr[&#92;mathbf{S}^{T}&#92;mathbf{T}]^{2}&#92;right), \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p><strong>Conjecture 2:</strong><em>  We </em>can express this integral in the Annealed Approximation (AA) which lets us re-express the log Quality in exponential form</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BS%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BT%7D%5D%5E%7B2%7D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}_{&#92;mathbf{T}}&#92;simeq&#92;log&#92;int d&#92;mu(&#92;mathbf{S})&#92;exp&#92;left(Tr[&#92;mathbf{S}^{T}&#92;mathbf{T}]^{2}&#92;right), \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p><strong>Conjecture 3:</strong><em>  We ca</em>n evaluate this integral over an Effective Correlation Space, spanned by the tail of the ESD, and such that we can replace the integral over all Student weight matrices <img src=\"https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BS%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BS%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BS%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"d&#92;mu(&#92;mathbf{S}) \" class=\"latex\" /> with an integral over all Student Correlation matrices <img src=\"https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BA%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BA%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=d%5Cmu%28%5Cmathbf%7BA%7D%29+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"d&#92;mu(&#92;mathbf{A}) \" class=\"latex\" />, where <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D%3D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{A}=&#92;mathbf{S}^{T}&#92;mathbf{S} \" class=\"latex\" /> giving</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BA%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BT%7D%5D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BA%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BT%7D%5D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D_%7B%5Cmathbf%7BT%7D%7D%5Csimeq%5Clog%5Cint+d%5Cmu%28%5Cmathbf%7BA%7D%29%5Cexp%5Cleft%28Tr%5B%5Cmathbf%7BT%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BT%7D%5D%5Cright%29%2C+&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathcal{Q}_{&#92;mathbf{T}}&#92;simeq&#92;log&#92;int d&#92;mu(&#92;mathbf{A})&#92;exp&#92;left(Tr[&#92;mathbf{T}^{T}&#92;mathbf{A}&#92;mathbf{T}]&#92;right), \" class=\"latex\" />&nbsp;</p>\n\n\n\n<p>We explain these in more detail below</p>\n\n\n\n<h3>RMT and HCIZ Integrals</h3>\n\n\n\n<p>These kinds of integrals traditionally appeared in Quantum Field Theory and String Theory, but also in the context of<a href=\"https://arxiv.org/abs/cond-mat/9801209\"> Random Matrix applied to Levy Spin Glasses</a>,  And it is this early work on Heavy-Tailed Random Matrices that has motivated our empirical work. Here, to complement and extend our studies, we lay out an (incomplete) overview of the Theory.</p>\n\n\n\n<p>These integrals are called Harish Chandra&#8211;Itzykson&#8211;Zuber (<em>HCIZ</em>)&nbsp;integrals.  A good introductory reference on both RMT and HCIZ integrals the recent book <a href=\"https://physics-complex-systems.fr/wp-content/uploads/2019/03/Notes_chap1-13.pdf\">&#8220;A First Course in Random Matrix Theory&#8221;</a>, although we will base our analysis here on the results of the <a href=\"https://iopscience.iop.org/article/10.1088/1742-6596/95/1/012002/pdf\">2008 paper by Tanaka,</a> </p>\n\n\n\n<p>First, we need to re-arrange a little of the algebra. We will call <strong>A</strong> the Student correlation matrix:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BS%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{A} =&#92;frac{1}{N}&#92;mathbf{S}^{T}&#92;mathbf{S}  \" class=\"latex\" /></p>\n\n\n\n<p>and let <strong>W</strong>, <strong>X</strong> be the original weight and correlation matrices for our pretrained DNN, as above:</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+%3D%5Cfrac%7B1%7D%7BN%7D%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BW%7D%2C%5C%3B%5C%3B%5Cmathbf%7BX%7D%7B%5Cmathbf%7Be%7D%7D%3D%5Clambda%7B%5Cmathbf%7Be%7D%7D++&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{X} =&#92;frac{1}{N}&#92;mathbf{W}^{T}&#92;mathbf{W},&#92;;&#92;;&#92;mathbf{X}{&#92;mathbf{e}}=&#92;lambda{&#92;mathbf{e}}  \" class=\"latex\" />,  </p>\n\n\n\n<p>and then expand the CCA Similarity metric as</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=Tr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BW%7D%29%5E%7B2%7D%5D%3DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Tr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BW%7D%29%5E%7B2%7D%5D%3DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Tr%5B%28%5Cmathbf%7BS%7D%5E%7BT%7D%5Cmathbf%7BW%7D%29%5E%7B2%7D%5D%3DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"Tr[(&#92;mathbf{S}^{T}&#92;mathbf{W})^{2}]=Tr[&#92;mathbf{W}^{T}&#92;mathbf{A}&#92;mathbf{W}]\" class=\"latex\" /> </p>\n\n\n\n<p>We can now express the log HCIZ integral, in using Tanaka&#8217;s result, as an expectation value of all random Student correlations matrices <strong>A</strong> that <em>resemble</em> <strong>X</strong>.  </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cunderset%7BN%5Crightarrow%5Cinfty%7D%7Blim%7D%5Cdfrac%7B1%7D%7BN%7D%5Clog%5C%3B%5Cmathbb%7BE%7D_%7BA%7D%5Cleft%5Bexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D%3D%5Cdfrac%7B%5Cbeta%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cunderset%7BN%5Crightarrow%5Cinfty%7D%7Blim%7D%5Cdfrac%7B1%7D%7BN%7D%5Clog%5C%3B%5Cmathbb%7BE%7D_%7BA%7D%5Cleft%5Bexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D%3D%5Cdfrac%7B%5Cbeta%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cunderset%7BN%5Crightarrow%5Cinfty%7D%7Blim%7D%5Cdfrac%7B1%7D%7BN%7D%5Clog%5C%3B%5Cmathbb%7BE%7D_%7BA%7D%5Cleft%5Bexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D%3D%5Cdfrac%7B%5Cbeta%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;underset{N&#92;rightarrow&#92;infty}{lim}&#92;dfrac{1}{N}&#92;log&#92;;&#92;mathbb{E}_{A}&#92;left[exp&#92;left(&#92;dfrac{&#92;beta}{2}Tr[&#92;mathbf{W}^{T}&#92;mathbf{A}&#92;mathbf{W}]&#92;right)&#92;right]=&#92;dfrac{&#92;beta}{2}&#92;sum&#92;limits_{i=1}^{M}&#92;;G_{A}(&#92;lambda_i)\" class=\"latex\" /> </p>\n\n\n\n<p>And this can be expressed as a sum over Generating functions <img src=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"G_{A}(&#92;lambda)\" class=\"latex\" />  that depends only the statistical properties of the random Student weight matrices <strong>A</strong>.  Specifically</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29%3A%3D%5Cint%5Climits_%7B0%7D%5E%7B%5Clambda%7DR_%7BA%7D%28z%29dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29%3A%3D%5Cint%5Climits_%7B0%7D%5E%7B%5Clambda%7DR_%7BA%7D%28z%29dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda%29%3A%3D%5Cint%5Climits_%7B0%7D%5E%7B%5Clambda%7DR_%7BA%7D%28z%29dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"G_{A}(&#92;lambda):=&#92;int&#92;limits_{0}^{&#92;lambda}R_{A}(z)dz\" class=\"latex\" /></p>\n\n\n\n<p>where <img src=\"https://s0.wp.com/latex.php?latex=R_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R_%7BA%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R_{A}(&#92;lambda)\" class=\"latex\" /> is the R-Transform from RMT.  </p>\n\n\n\n<p><strong>The R Transform</strong> is like an inverse Green&#8217;s function (i.e a Contour Integral), and is also a cumulant generating function.  As such, we can write <img src=\"https://s0.wp.com/latex.php?latex=R%28z%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R%28z%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%28z%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R(z)\" class=\"latex\" />  as a series expansion</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=R%28z%29%3D%5Csum%5Climits_%7Bk%3D1%7D%5E%7B%5Cinfty%7Dc_%7Bk%7Dz%5E%7Bk-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R%28z%29%3D%5Csum%5Climits_%7Bk%3D1%7D%5E%7B%5Cinfty%7Dc_%7Bk%7Dz%5E%7Bk-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%28z%29%3D%5Csum%5Climits_%7Bk%3D1%7D%5E%7B%5Cinfty%7Dc_%7Bk%7Dz%5E%7Bk-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R(z)=&#92;sum&#92;limits_{k=1}^{&#92;infty}c_{k}z^{k-1}\" class=\"latex\" /> </p>\n\n\n\n<p>where <img src=\"https://s0.wp.com/latex.php?latex=c_%7Bk%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=c_%7Bk%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=c_%7Bk%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"c_{k}\" class=\"latex\" />  are <strong><em>Generalized Cumulants </em></strong>from RMT.</p>\n\n\n\n<p>Now, since we expect the best  Students matrices <em>resemble</em> the  Teacher matrices, we expect the Student correlation matrix <strong>A</strong> to have similar spectral properties as our actual correlation matrices <strong>X</strong>.  And this where we can use our classification of the<em><strong> 5+1 Phases of Training</strong></em>.  Whatever phase <strong>X</strong> is in, we expect all the <strong>A</strong> to be in as well, and we therefore expect the R-Transform of <strong>A</strong> to have the same functional form as <strong>X</strong>.  </p>\n\n\n\n<p>That is, if our DNN weight matrix has a Heavy Tailed ESD</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho_{X}(&#92;lambda)&#92;sim&#92;lambda^{-&#92;alpha}\" class=\"latex\" /> </p>\n\n\n\n<p>then we expect all of the students to likewise have a Heavy Tailed ESD, and with the same exponent (at least for now).</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BA%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BA%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho_%7BA%7D%28%5Clambda%29%5Csim%5Clambda%5E%7B-%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho_{A}(&#92;lambda)&#92;sim&#92;lambda^{-&#92;alpha}\" class=\"latex\" /> </p>\n\n\n\n<p><strong>Quenched </strong> <strong>vs Annealed Averages</strong></p>\n\n\n\n<p>Formally, we just say we are averaging over all Students <strong>A</strong>.  More technically, what really want to do is fix some Student matrix (i.e. say <strong>A</strong> =  diagonal <strong>X</strong>), and then  integrate over all possible Orthogonal transformations <strong>O </strong>of <strong>A</strong> (see 6.2.3 of Potters and Bouchaud)</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Clog%5Cleft%5Clangle%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BO%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BOW%7D%5D%5Cright%29%5Cright%5Crangle_%7B%5Cmathbf%7BO%7D%7D%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D%5Csimeq%5Clog%5Cmathbb%7BE%7D%5Cleft%5B%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Clog%5Cleft%5Clangle%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BO%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BOW%7D%5D%5Cright%29%5Cright%5Crangle_%7B%5Cmathbf%7BO%7D%7D%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D%5Csimeq%5Clog%5Cmathbb%7BE%7D%5Cleft%5B%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Clog%5Cleft%5Clangle%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BO%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BOW%7D%5D%5Cright%29%5Cright%5Crangle_%7B%5Cmathbf%7BO%7D%7D%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D%5Csimeq%5Clog%5Cmathbb%7BE%7D%5Cleft%5B%5Cexp%5Cleft%28%5Cdfrac%7B%5Cbeta%7D%7B2%7DTr%5B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7BA%7D%5Cmathbf%7BW%7D%5D%5Cright%29%5Cright%5D_%7B%5Cmathbf%7BA%7D%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbb{E}&#92;left[&#92;log&#92;left&#92;langle&#92;exp&#92;left(&#92;dfrac{&#92;beta}{2}Tr[&#92;mathbf{W}^{T}&#92;mathbf{O}^{T}&#92;mathbf{A}&#92;mathbf{OW}]&#92;right)&#92;right&#92;rangle_{&#92;mathbf{O}}&#92;right]_{&#92;mathbf{A}}&#92;simeq&#92;log&#92;mathbb{E}&#92;left[&#92;exp&#92;left(&#92;dfrac{&#92;beta}{2}Tr[&#92;mathbf{W}^{T}&#92;mathbf{A}&#92;mathbf{W}]&#92;right)&#92;right]_{&#92;mathbf{A}}\" class=\"latex\" /> </p>\n\n\n\n<p>Then, we integrate over all possible<strong> A</strong>~diag(<strong>X</strong>) , which would account for fluctuations in the eigenvalues.  We conceptually assume this is the same as integrating over all possible Students <strong>A</strong>, and then taking the log.  </p>\n\n\n\n<p>The LHS is called the Quenched Average, and the RHS is the Annealed.  Technically, they are not the same, and in traditional Stat Mech theory, this makes a big difference.  In fact, in the original Student-Teacher model, we would also average over all Teachers, chosen uniformly (to satisfy the spherical constraints)</p>\n\n\n\n<p>Here, we are doing RMT a little differently, which may not be obvious until the end of the calculation. We do not assume a priori a model for the Student matrices.  That is, instead of fixing <strong>A</strong>=diag(<strong>X</strong>), we will fit the  ESD of <strong>X</strong> to a <em>continuous</em> (power law) distribution <img src=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho_{X}(&#92;lambda)\" class=\"latex\" /> , and then <em>effectively sample</em> over all <strong>A</strong> as if we had drawn the eigenvalues of <strong>A</strong> from <img src=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Crho_%7BX%7D%28%5Clambda%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;rho_{X}(&#92;lambda)\" class=\"latex\" />. (In fact, I suppose we could actually do this numerically instead of doing all this fancy math&#8211;but what fun is that?).</p>\n\n\n\n<p>The point is, we want to find an expression for the HCIZ integral (i.e the layer / matrix contribution to the Generalization Error) that only depends on observations of <strong>W</strong>, the weight matrix of the pretrained DNN (our Teacher network).  The result only depends on the eigenvalues of <strong>X</strong>, and the R-transform of <strong>A</strong> , which is parameterized by statistical information from<strong> X</strong>.  </p>\n\n\n\n<p>In principle, I supposed we could measure the generalized cumulants <img src=\"https://s0.wp.com/latex.php?latex=%28c_%7Bk%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%28c_%7Bk%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28c_%7Bk%7D%29&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"(c_{k})\" class=\"latex\" /> of <strong>X</strong>,. and assume we can plug these in for <img src=\"https://s0.wp.com/latex.php?latex=R_%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R_%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R_%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R_{A}\" class=\"latex\" />.  We will do something a little easier.</p>\n\n\n\n<p>Let us consider 2 classes of matrices as models for <strong>X.</strong>  </p>\n\n\n\n<p><strong>Gaussian (Wigner) Random Matrix:  </strong>Random-Like Phase</p>\n\n\n\n<p>The R-Transform for Gaussian Random matrix is well known:</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=R%28z%29%3Dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R%28z%29%3Dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%28z%29%3Dz&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R(z)=z\" class=\"latex\" /> </p>\n\n\n\n<p>Taking the integral and plugging this into the Generating function, we get</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B2%7Dz%5E%7B2%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B2%7D%5Clambda%5E%7B2%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B2%7Dz%5E%7B2%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B2%7D%5Clambda%5E%7B2%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B2%7Dz%5E%7B2%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B2%7D%5Clambda%5E%7B2%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"G_{A}(&#92;lambda_{i})=&#92;frac{1}{2}z^{2}&#92;bigg|_{z=0}^{z=&#92;lambda_i}=&#92;frac{1}{2}&#92;lambda^{2}_{i}\" class=\"latex\" /> </p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5Clambda%5E%7B2%7D_%7Bi%7D%3D%5Cfrac%7B1%7D%7B2%7DTr%5B%5Cmathbf%7BA%7D%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5Clambda%5E%7B2%7D_%7Bi%7D%3D%5Cfrac%7B1%7D%7B2%7DTr%5B%5Cmathbf%7BA%7D%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5Clambda%5E%7B2%7D_%7Bi%7D%3D%5Cfrac%7B1%7D%7B2%7DTr%5B%5Cmathbf%7BA%7D%5E%7B2%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;sum&#92;limits_{i=1}^{M}&#92;;G_{A}(&#92;lambda_i)=&#92;frac{1}{2}&#92;sum&#92;limits_{i=1}^{M}&#92;lambda^{2}_{i}=&#92;frac{1}{2}Tr[&#92;mathbf{A}^{2}]\" class=\"latex\" /> </p>\n\n\n\n<p>So when <strong>X</strong> is Random-Like , the layer / matrix contribution is like the Frobenius Norm (but squared), and thus average Generalization Error is given by a Frobenius Product Norm (squared).  Thing is, this is never observed in any production model, as every single model we have examined has a Heavy-Tailed ESD.  So we need something else.</p>\n\n\n\n<p><strong>Levy Random Matrix:  </strong> Very Heavy Tailed Phase&#8211;but with <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 3\" class=\"latex\" /></p>\n\n\n\n<p><a href=\"https://arxiv.org/abs/1901.08278\">Aswe have argued previously</a>, due to finite size effects, we expect that the Very Heavy Tailed matrices appearing in DNNs will more resemble Levy Random matrices that the Random-Like Phase.  So for now, we will close one eye and extend the results for <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+3&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 3\" class=\"latex\" /> to <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;in[2,6]\" class=\"latex\" />.  </p>\n\n\n\n<p>The R-Transform for a Levy Random Matrix has been given by <a href=\"https://arxiv.org/abs/0909.5228\">Burda</a></p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=R%28z%29%3Dz%5E%7B%5Calpha-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=R%28z%29%3Dz%5E%7B%5Calpha-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=R%28z%29%3Dz%5E%7B%5Calpha-1%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"R(z)=z^{&#92;alpha-1}\" class=\"latex\" /> </p>\n\n\n\n<p class=\"has-text-align-left\">Taking the integral and plugging this into the Generating function, we get</p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7Dz%5E%7B%5Calpha%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B%5Calpha%7D%5Clambda%5E%7B%5Calpha%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7Dz%5E%7B%5Calpha%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B%5Calpha%7D%5Clambda%5E%7B%5Calpha%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=G_%7BA%7D%28%5Clambda_%7Bi%7D%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7Dz%5E%7B%5Calpha%7D%5Cbigg%7C_%7Bz%3D0%7D%5E%7Bz%3D%5Clambda_i%7D%3D%5Cfrac%7B1%7D%7B%5Calpha%7D%5Clambda%5E%7B%5Calpha%7D_%7Bi%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"G_{A}(&#92;lambda_{i})=&#92;frac{1}{&#92;alpha}z^{&#92;alpha}&#92;bigg|_{z=0}^{z=&#92;lambda_i}=&#92;frac{1}{&#92;alpha}&#92;lambda^{&#92;alpha}_{i}\" class=\"latex\" /> </p>\n\n\n\n<p class=\"has-text-align-center\"> <img src=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7DTr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7DTr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%3D%5Cfrac%7B1%7D%7B%5Calpha%7DTr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;sum&#92;limits_{i=1}^{M}&#92;;G_{A}(&#92;lambda_i)=&#92;frac{1}{&#92;alpha}Tr[&#92;mathbf{A}^{&#92;alpha}]\" class=\"latex\" /></p>\n\n\n\n<p><strong>Towards our Heavy Tailed Quality Metric</strong></p>\n\n\n\n<p>1. Let us pull the power law exponent <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />  out of the Trace, effectively ignoring cross terms in the sum over <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda^{&#92;alpha}\" class=\"latex\" />  </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D%5Csimeq+Tr%5B%5Cmathbf%7BA%7D%5D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D%5Csimeq+Tr%5B%5Cmathbf%7BA%7D%5D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5E%7B%5Calpha%7D%5D%5Csimeq+Tr%5B%5Cmathbf%7BA%7D%5D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"Tr[&#92;mathbf{A}^{&#92;alpha}]&#92;simeq Tr[&#92;mathbf{A}]^{&#92;alpha}\" class=\"latex\" /></p>\n\n\n\n<p>2. We also assume we can replace the Trace of <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;mathbf{A}\" class=\"latex\" />   with its largest eigenvalue <img src=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;lambda_{max}\" class=\"latex\" /> , which is  actually a good approximation for very heavy tailed Levy matrices, when <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cll+2&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cll+2&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5Cll+2&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;ll 2\" class=\"latex\" />  </p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5D%3D%5Csum%5Clambda_%7Bi%7D%5Csimeq+%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5D%3D%5Csum%5Clambda_%7Bi%7D%5Csimeq+%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=Tr%5B%5Cmathbf%7BA%7D%5D%3D%5Csum%5Clambda_%7Bi%7D%5Csimeq+%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"Tr[&#92;mathbf{A}]=&#92;sum&#92;lambda_{i}&#92;simeq &#92;lambda_{max}\" class=\"latex\" /></p>\n\n\n\n<p>This gives an simple expression for the HCIZ integral expression for the layer contribution to the generalization error</p>\n\n\n\n<p class=\"has-text-align-center\"><img src=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5Clambda_%7Bmax%7D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5Clambda_%7Bmax%7D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5Clambda_%7Bmax%7D%5E%7B%5Calpha%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;sum&#92;limits_{i=1}^{M}&#92;;G_{A}(&#92;lambda_i)&#92;simeq&#92;lambda_{max}^{&#92;alpha}\" class=\"latex\" /></p>\n\n\n\n<p>Taking the logarithm  of both sides, gives our expression</p>\n\n\n\n<p class=\"has-text-align-center has-normal-font-size\"><img src=\"https://s0.wp.com/latex.php?latex=log%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5C%3B%5Calpha%5Clog%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=log%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5C%3B%5Calpha%5Clog%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=log%5Csum%5Climits_%7Bi%3D1%7D%5E%7BM%7D%5C%3BG_%7BA%7D%28%5Clambda_i%29%5Csimeq%5C%3B%5Calpha%5Clog%5Clambda_%7Bmax%7D&#038;bg=ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"log&#92;sum&#92;limits_{i=1}^{M}&#92;;G_{A}(&#92;lambda_i)&#92;simeq&#92;;&#92;alpha&#92;log&#92;lambda_{max}\" class=\"latex\" /></p>\n\n\n\n<p>We have now derived the our Heavy Tailed Quality metric using a matrix generalization of the classic Student Teacher model, with the help of some modern Random Matrix Theory.  </p>\n\n\n\n<p><strong>QED</strong></p>\n\n\n\n<p>I hope this has convince you that there is still a lot of very interesting theory to develop for AI / Deep Neural Networks.   And that you will stay tuned for the published form of this work.  And remember&#8230;</p>\n\n\n\n<p class=\"has-text-align-center\"><code>pip install weightwatcher</code></p>\n\n\n\n<p>A big thanks to <a href=\"https://www.stat.berkeley.edu/~mmahoney/\">Michael Mahoney at UC Berkeley</a> for collaborating with me on this work , and to <a href=\"https://www.linkedin.com/in/mircomilletari/\">Mirco Milletariâ</a> (Microsoft), who has been extremely helpful.  And to my good friend <a href=\"https://www.linkedin.com/in/matthew-w-lee/\">Matt Lee</a> (Triaxiom Capital, LLC) for long discussions about theoretical physics, RMT, quant finance, etc., , and for encouraging me to publish this.</p>\n\n\n\n<p></p>\n\n\n\n<p>And here&#8217;s the Empirical Evidence that this actually works:</p>\n\n\n\n<p> <a href=\"https://www.nature.com/articles/s41467-021-24025-8\" rel=\"nofollow\">https://www.nature.com/articles/s41467-021-24025-8</a></p>\n\n\n\n<p class=\"has-text-align-left\"><strong>If you have gotten this far,</strong></p>\n\n\n\n<p>and would be willing to read the draft of the real theory paper, that would be great.  Please just email me or ping me on Linkedin</p>\n\n\n\n<p></p>\n",
  "wfw:commentRss": "https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/feed/",
  "slash:comments": 1,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "screen-shot-2019-11-29-at-10.28.02-pm"
    },
    {
      "media:title": "charlesmartin14"
    },
    "",
    {
      "media:title": "alt text"
    },
    ""
  ]
}