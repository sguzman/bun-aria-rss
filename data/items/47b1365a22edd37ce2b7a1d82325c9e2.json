{
  "id": "tag:blogger.com,1999:blog-15418143.post-2190638093839413385",
  "published": "2016-01-13T04:20:00.001-05:00",
  "updated": "2016-06-14T03:46:36.791-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "The Future of Real-Time SLAM and Deep Learning vs SLAM",
  "content": "<div style=\"background-color: white; color: #222222;\">Last month's International Conference of Computer Vision (ICCV) was <a href=\"http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html\">full of Deep Learning</a>&nbsp;techniques, but before we declare an all-out ConvNet victory, let's see how the other \"non-learning\" geometric side of computer vision is doing. &nbsp;<b>S</b>imultaneous <b>L</b>ocalization <b>a</b>nd <b>M</b>apping, or <b>SLAM</b>, is arguably one of the most important algorithms in Robotics, with pioneering work done by both computer vision and robotics research communities. &nbsp;Today I'll be summarizing my key points from ICCV's&nbsp;<a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/programme/\">Future of Real-Time SLAM</a>&nbsp;Workshop, which was held on the last day of the conference (December 18th, 2015).<br /><br />Today's post contains a brief introduction to SLAM,&nbsp;a detailed description of what happened at the workshop (with summaries of all 7 talks),&nbsp;and some take-home messages from the <i>Deep Learning-focused panel discussion</i> at the end of the session.<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-3WNdePDKHQw/VpOAwv91xWI/AAAAAAAAOcY/Q6oXFwf14Jw/s1600/slammies2.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"290\" src=\"https://1.bp.blogspot.com/-3WNdePDKHQw/VpOAwv91xWI/AAAAAAAAOcY/Q6oXFwf14Jw/s400/slammies2.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>SLAM visualizations.&nbsp;</b>Can you identify any of these SLAM algorithms?</div><br /><h2>Part I: Why SLAM Matters</h2></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Visual SLAM algorithms are able to simultaneously build 3D maps of the world while tracking the location and orientation of the camera (hand-held or head-mounted for AR or mounted on a robot).&nbsp;</span>SLAM algorithms are complementary to ConvNets and Deep Learning: SLAM focuses on geometric problems and Deep Learning is the master of perception (recognition) problems. If you want a robot to go towards your refrigerator without hitting a wall, use SLAM. If you want the robot to identify the items inside your fridge, use ConvNets.<br /><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://openmvg.readthedocs.org/en/latest/_images/structureFromMotion.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em; text-align: center;\"><img border=\"0\" src=\"http://openmvg.readthedocs.org/en/latest/_images/structureFromMotion.png\" height=\"220\" width=\"400\" /></a></div><br /><div style=\"text-align: center;\"><b>Basics of SfM/SLAM</b>: From point observation and intrinsic camera parameters, the 3D structure of a scene is computed from the estimated motion of the camera. For details, see&nbsp;<a href=\"http://openmvg.readthedocs.org/en/latest/\">openMVG website</a>.</div><div style=\"text-align: center;\"><span style=\"font-family: inherit;\"><br /></span></div><span style=\"font-family: inherit;\">SLAM&nbsp;is a real-time version of&nbsp;<b>S</b>tructure&nbsp;<b>f</b>rom&nbsp;<b>M</b>otion (SfM). Visual SLAM or vision-based SLAM is a camera-only variant of SLAM which forgoes expensive laser sensors and&nbsp;<b>i</b>nertial <b>m</b>easurement <b>u</b>nits (IMUs). Monocular SLAM uses a single camera while non-monocular SLAM typically uses a pre-calibrated fixed-baseline stereo camera rig. SLAM is prime example of a what is called a \"Geometric Method\" in Computer Vision. In fact, CMU's Robotics Institute splits the graduate level computer vision curriculum into a <a href=\"http://graphics.cs.cmu.edu/courses/16-824-S15/index.html\">Learning-based Methods in Vision</a> course and a separate <a href=\"http://www.cs.cmu.edu/~hebert/geom.html\">Geometry-Based Methods in Vision</a> course.</span></div><div style=\"background-color: white; color: #222222;\"><br /></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><b></b><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Structure from Motion vs Visual SLAM</span></b></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">Structure from Motion (SfM) and SLAM are solving a very similar problem, but while SfM is traditionally performed in an offline fashion, SLAM has been slowly moving towards the low-power / real-time / single RGB camera mode of operation. Many of the today’s top experts in Structure from Motion work for some of the world’s biggest tech companies, helping make maps better. Successful mapping products like Google Maps could not have been built without intimate knowledge of multiple-view geometry, SfM, and SLAM. &nbsp;A typical SfM problem is the following: given a large collection of photos of a single outdoor structure (like the Colliseum), construct a 3D model of the structure and determine the camera's poses. The image collection is processed in an offline setting, and large reconstructions can take anywhere between hours and days.&nbsp;</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><a href=\"http://www.cs.cornell.edu/~snavely/bundler/images/Colosseum.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://www.cs.cornell.edu/~snavely/bundler/images/Colosseum.jpg\" height=\"138\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><b>SfM Software</b>:&nbsp;<a href=\"http://www.cs.cornell.edu/~snavely/bundler/\">Bundler</a>&nbsp;is&nbsp;one of the most successful SfM open source libraries</div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><br /></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">Here are some popular SfM-related software libraries:</span></div><ul><li><a href=\"http://www.cs.cornell.edu/~snavely/bundler/\" style=\"font-family: inherit;\">Bundler</a><span style=\"color: #222222; font-family: inherit;\">,&nbsp;an open-source Structure from Motion toolkit</span></li><li><a href=\"http://ceres-solver.org/\" style=\"font-family: inherit;\">Libceres</a><span style=\"color: #222222; font-family: inherit;\">, a non-linear least squares minimizer (useful for bundle adjustment problems)</span></li><li><span style=\"color: #222222; font-family: inherit;\">Andrew Zisserman's </span><a href=\"http://www.robots.ox.ac.uk/~vgg/hzbook/code/\" style=\"font-family: inherit;\">Multiple-View Geometry MATLAB Functions</a></li></ul></div><div style=\"background-color: white; color: #222222;\"><br /></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Visual SLAM vs Autonomous Driving</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">While self-driving cars are one of the most important applications of SLAM, according to Andrew Davison, one of the workshop organizers, SLAM for Autonomous Vehicles deserves its own research track. (And as we'll see, none of the workshop presenters talked about self-driving cars). For many years to come it will make sense to continue studying SLAM from a research perspective, independent of any single Holy-Grail application. While there are just too many system-level details and tricks involved with autonomous vehicles, research-grade SLAM systems require very little more than a webcam, knowledge of algorithms, and elbow grease. As a research topic, Visual SLAM is much friendlier to thousands of early-stage PhD students who’ll first need years of in-lab experience with SLAM before even starting to think about expensive robotic platforms such as self-driving cars.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://spectrum.ieee.org/image/1948541\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://spectrum.ieee.org/image/1948541\" height=\"215\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Google's Self-Driving Car's perception system</b>. From IEEE Spectrum's \"<a href=\"http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works\">How Google's Self-Driving Car Works</a>\"</div><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b>Related</b>: March 2015 blog post, <a href=\"http://www.computervisionblog.com/2015/03/mobileyes-quest-to-put-deep-learning.html\">Mobileye's quest to put Deep Learning inside every new car</a>.</div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><b>Related:</b> <a href=\"http://mappingignorance.org/2014/04/07/one-way-googles-cars-localize/\">One way Google's Cars Localize Themselves</a></span></div><div style=\"background-color: white; color: #222222;\"><h2><span style=\"font-family: inherit;\">Part II: The Future of Real-time SLAM</span></h2></div><div style=\"background-color: white;\"><span style=\"color: #222222; font-family: inherit;\">Now it's time to&nbsp;</span><span style=\"color: #222222;\">officially</span><span style=\"color: #222222; font-family: inherit;\">&nbsp;summarize and comment on the presentations from The Future of Real-time SLAM workshop.&nbsp;</span><span style=\"color: #222222; font-family: inherit;\"><a href=\"http://www.doc.ic.ac.uk/~ajd/index.html\">Andrew Davison</a> started the day with an excellent historical overview of SLAM called <a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/slides_ajd.pdf\">15 years of vision-based SLAM</a>, and his slides have good content for an introductory robotics course.</span></div><div style=\"background-color: white; color: #222222;\"><br /></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">For those of you who don’t know Andy, he is the one and only Professor Andrew Davison of Imperial College London. &nbsp;Most known for his 2003 MonoSLAM system, he was one of the first to show how to build SLAM systems from a single “<i>monocular”</i>&nbsp;camera at a time when just everybody thought you needed a stereo “<i>binocular</i>” camera rig. More recently, his work has influenced the trajectory of companies such as Dyson and the capabilities of their robotic systems (e.g., <a href=\"http://www.computervisionblog.com/2015/05/dyson-360-eye-and-baidu-deep-learning.html\">the brand new Dyson360</a>).</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">I remember Professor Davidson from the Visual SLAM tutorial he gave at the BMVC Conference back in <a href=\"http://www.cs.bris.ac.uk/Research/Vision/Realtime/bmvctutorial/\">2007</a>. Surprisingly very little has changed in SLAM compared to the rest of the machine-learning heavy work being done at the main vision conferences. In the past 8 years, object recognition has undergone 2-3 mini revolutions, while today's SLAM systems don't look much different than they did 8 years ago. The best way to see the progress of SLAM is to take a look at the most successful and memorable systems.&nbsp;</span><span style=\"font-family: inherit;\">In Davison’s workshop introduction talk, he discussed some of these exemplary systems which were produced by the research community over the last 10-15 years:&nbsp;</span></div><div style=\"background-color: white;\"><br /><ul><li><b style=\"color: #222222; font-family: inherit;\">MonoSLAM</b></li><li><b style=\"color: #222222; font-family: inherit;\">PTAM</b></li><li><b style=\"color: #222222; font-family: inherit;\">FAB-MAP</b></li><li><b style=\"color: #222222; font-family: inherit;\">DTAM</b></li><li><b style=\"color: #222222; font-family: inherit;\">KinectFusion</b></li></ul><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><br /></div></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><b><span style=\"font-family: inherit;\">Davison vs Horn: The next chapter in Robot Vision</span></b><br /><span style=\"font-family: inherit;\">Davison also mentioned that he is working on a new Robot Vision book, which should be an exciting treat for researchers in computer vision, robotics, and artificial intelligence. The last <a href=\"https://mitpress.mit.edu/books/robot-vision\">Robot Vision book</a> was written by B.K. Horn (1986), and it’s about time for an updated take on Robot Vision.&nbsp;</span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-Oh94IZlctLA/VpWlSwN_WEI/AAAAAAAAOdw/fKDBj8KQoGM/s1600/robotvision-01.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"117\" src=\"https://3.bp.blogspot.com/-Oh94IZlctLA/VpWlSwN_WEI/AAAAAAAAOdw/fKDBj8KQoGM/s400/robotvision-01.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>A new robot vision book?</b></div><span style=\"font-family: inherit;\"><br /></span><span style=\"font-family: inherit;\">While I’ll gladly read a tome that focuses on the philosophy of robot vision, personally I would like the book to focus on practical algorithms for robot vision, like the excellent <a href=\"http://www.robots.ox.ac.uk/~vgg/hzbook/\">Multiple View Geometry</a> book by Hartley and Zissermann or <a href=\"http://www.probabilistic-robotics.org/\">Probabilistic Robotics</a> by Thrun, Burgard, and Fox. A \"cookbook\" of visual SLAM problems would be a welcome addition to any serious vision researcher's collection.</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><b>Related</b>: Davison's <a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/slides_ajd.pdf\">15-years of vision-based SLAM</a>&nbsp;slides</span></div></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 1: Christian Kerl on Continuous Trajectories in SLAM</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">The first talk, by <a href=\"http://vision.in.tum.de/members/kerl\">Christian Kerl</a>, presented a dense tracking method to estimate a continuous-time trajectory. The key observation is that most SLAM systems estimate camera poses at a discrete number of time steps (either they key frames which are spaced several seconds apart, or the individual frames which are spaced approximately 1/25s apart).&nbsp;</span><br /><span style=\"font-family: inherit;\"><br /></span><a href=\"http://2.bp.blogspot.com/-vjaOqhDMTBg/VpODSBMvZ5I/AAAAAAAAOck/lbF_FQh5_EM/s1600/kerl.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em; text-align: center;\"><img border=\"0\" height=\"115\" src=\"https://2.bp.blogspot.com/-vjaOqhDMTBg/VpODSBMvZ5I/AAAAAAAAOck/lbF_FQh5_EM/s400/kerl.png\" width=\"400\" /></a><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Continuous Trajectories vs Discrete Time Points. </b>SLAM/SfM usually uses discrete time points, but why not go continuous?</div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><span style=\"font-family: inherit;\">Much of Kerl’s talk was focused on undoing the damage of rolling shutter cameras, and the system demo’ed by Kerl paid meticulous attention to modeling and removing these adverse rolling shutter effects.</span><br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-gXML1JkPLGs/VpOEM6IOsCI/AAAAAAAAOcs/yzLDs-WEMqM/s1600/shutter.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"245\" src=\"https://1.bp.blogspot.com/-gXML1JkPLGs/VpOEM6IOsCI/AAAAAAAAOcs/yzLDs-WEMqM/s400/shutter.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Undoing the damage of rolling shutter in Visual SLAM.</b></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><br /><span style=\"font-family: inherit;\"><b>Related:</b></span><span style=\"text-align: center;\">&nbsp;Kerl's&nbsp;</span><a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/kerl_etal_iccv2015_futureofslam_talk.pdf\" style=\"text-align: center;\">Dense continous-time tracking and mapping</a><span style=\"text-align: center;\">&nbsp;slides.</span><br /><span style=\"text-align: center;\"><b>Related:&nbsp;</b>Dense Continuous-Time Tracking and Mapping with Rolling Shutter RGB-D Cameras (C. Kerl, J. Stueckler, D. Cremers), In IEEE International Conference on Computer Vision (ICCV), 2015. [<a href=\"http://vision.in.tum.de/_media/spezial/bib/kerl15iccv.pdf\">pdf</a>]</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 2: Semi-Dense Direct SLAM by Jakob Engel</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">LSD-SLAM came out at ECCV 2014 and is one of my favorite SLAM systems today! <a href=\"http://vision.in.tum.de/members/engelj\">Jakob Engel</a> was there to present his system and show the crowd some of the coolest SLAM visualizations in town. LSD-SLAM is an acronym for Large-Scale Direct Monocular SLAM. LSD-SLAM is an important system for SLAM researchers because it does not use corners or any other local features. </span><span style=\"font-family: inherit;\"><b>Direct tracking is performed by image-to-image alignment</b> using a coarse-to-fine algorithm with a robust Huber loss. This is quite different than the feature-based systems out there. Depth estimation uses an inverse depth parametrization (like many other SLAM systems) and uses a large number or relatively small baseline image pairs. Rather than relying on image features, the algorithms is effectively performing “texture tracking”. Global mapping is performed by creating and solving a pose graph \"bundle adjustment\" optimization problem, and all of this works in real-time. The method is semi-dense because it only estimates depth at pixels solely near image boundaries. LSD-SLAM output is denser than traditional features, but not fully dense like Kinect-style RGBD SLAM.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td><a href=\"http://4.bp.blogspot.com/-VH3GehiSfKY/Vnl44gfSt3I/AAAAAAAAObw/MYun2V6_C4M/s1600/lsd-slam.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" height=\"225\" src=\"https://4.bp.blogspot.com/-VH3GehiSfKY/Vnl44gfSt3I/AAAAAAAAObw/MYun2V6_C4M/s400/lsd-slam.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\"><span style=\"font-size: small;\"><b>LSD-SLAM in Action:&nbsp;</b><a href=\"http://vision.in.tum.de/research/vslam/lsdslam\">LSD-SLAM</a>&nbsp;generates both a camera trajectory and a semi-dense 3D scene reconstruction. This approach works in real-time, does not use feature points as primitives, and performs direct image-to-image alignment.</span></td></tr></tbody></table></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span><span style=\"font-family: inherit;\">Engel gave us an overview of the original LSD-SLAM system as well as a handful of new results, extending their initial system to more creative applications and to more interesting deployments. (See paper citations below)</span><br /><span style=\"font-family: inherit;\"><br /></span><span style=\"font-family: inherit;\"><b>Related:</b>&nbsp;<a href=\"https://github.com/tum-vision/lsd_slam\">LSD-SLAM Open-Source Code on github</a>&nbsp;</span><a href=\"http://vision.in.tum.de/research/vslam/lsdslam\">LSD-SLAM project webpage</a><br /><span style=\"font-family: inherit;\"><b>Related:&nbsp;</b></span>LSD-SLAM: Large-Scale Direct Monocular SLAM&nbsp;(J. Engel, T. Schöps, D. Cremers), In European Conference on Computer Vision (ECCV), 2014. [<a href=\"http://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf\">pdf</a>] [youtube&nbsp;<a href=\"https://youtu.be/GnuQzP3gty4\">video</a>]<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">An extension to LSD-SLAM, <b>Omni LSD-SLAM</b> was created by the observation that the pinhole model does not allow for a large field of view. This work was presented at IROS 2015 (Caruso is first author) and allows a large field of view (ideally more than 180 degrees). From Engel’s presentation it was pretty clear that you can perform ballerina-like motions (extreme rotations) while walking around your office and holding the camera. This is one of those worst-case scenarios for narrow field of view SLAM, yet works quite well in Omni LSD-SLAM.</span><br /><div style=\"-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Times; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 1; word-spacing: 0px;\"><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\"><br class=\"Apple-interchange-newline\" /></div><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-WUowVgUNRuk/VpOE_93Z_gI/AAAAAAAAOc4/4kudUERtd80/s1600/omni.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"251\" src=\"https://4.bp.blogspot.com/-WUowVgUNRuk/VpOE_93Z_gI/AAAAAAAAOc4/4kudUERtd80/s400/omni.png\" style=\"cursor: move;\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\"><b>Omnidirectional LSD-SLAM Model.</b> See Engel's&nbsp;<a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/ICCV-SLAM-Workshop_JakobEngel.pdf\">Semi-Dense Direct SLAM</a>&nbsp;presentation slides.</div></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><b>Related:&nbsp;</b></span>Large-Scale Direct SLAM for Omnidirectional Cameras (D. Caruso, J. Engel, D. Cremers), In International Conference on Intelligent Robots and Systems (IROS), 2015. &nbsp;[<a href=\"http://vision.in.tum.de/_media/spezial/bib/caruso2015_omni_lsdslam.pdf\">pdf</a>] [youtube&nbsp;<a href=\"https://youtu.be/v0NqMm7Q6S8\">video</a>]<br /><span style=\"color: #222222;\"><br /></span><span style=\"font-family: inherit;\"><b>Stereo LSD-SLAM</b> is an extension of LSD-SLAM to a binocular camera rig. This helps in getting the&nbsp;</span><span style=\"font-family: inherit;\">absolute scale,&nbsp;</span><span style=\"font-family: inherit;\">initialization is instantaneous, and there are </span><span style=\"font-family: inherit;\">no issues with strong rotation. While monocular SLAM is very exciting from an academic point of view, if your robot is a 30,000$ car or 10,000$ drone prototype, you should have a good reason to not use a two+ camera rig. Stereo LSD-SLAM performs quite competitively on SLAM benchmarks.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-076ah06nqNo/VpWpi1ty0BI/AAAAAAAAOd8/nVhyKjMFoXU/s1600/stereo-lsd.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"149\" src=\"https://2.bp.blogspot.com/-076ah06nqNo/VpWpi1ty0BI/AAAAAAAAOd8/nVhyKjMFoXU/s400/stereo-lsd.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Stereo LSD-SLAM.</b>&nbsp;Excellent results on KITTI vehicle-SLAM dataset.</div><div><span style=\"font-family: inherit;\"><br /></span></div></div><div style=\"background-color: white; color: #222222;\">Stereo LSD-SLAM is quite practical, optimizes a&nbsp;pose graph in SE(3), and includes a correction for auto exposure. The goal of auto-exposure correcting is to make the error function invariant to affine lighting changes. The underlying parameters of the color-space affine transform are estimated during matching, but thrown away to estimate the image-to-image error. From Engel's talk, outliers (often caused by over-exposed image pixels) tend to be a problem, and much care needs to be taken to care of their effects.<br /><span style=\"font-family: inherit;\"><br /></span><b>Related: </b>Large-Scale Direct SLAM with Stereo Cameras&nbsp;(J. Engel, J. Stueckler, D. Cremers), In International Conference on Intelligent Robots and Systems (IROS), 2015. &nbsp;[<a href=\"http://vision.in.tum.de/_media/spezial/bib/engel2015_stereo_lsdslam.pdf\">pdf</a>] [youtube&nbsp;<a href=\"https://youtu.be/oJt3Ln8H03s\">video</a>]<br /><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Later in his presentation, Engel gave us a sneak peak on new research about i<i>ntegrating both stereo and inertial sensors</i>. For details, you’ll have to keep hitting refresh on Arxiv or talk to Usenko/Engel in person.&nbsp;</span>On the applications side, Engel's presentation included updated videos of an Autonomous Quadrotor driven by LSD-SLAM. The flight starts with an up-down motion to get the scale estimate and a free-space octomap is used to estimate the free-space so that the quadrotor can navigate space on its own. Stay tuned for an official publication...</div><div style=\"background-color: white; color: #222222;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-E7FYRfwgEZE/VpWrU97D4OI/AAAAAAAAOeI/Wf4toRYot88/s1600/quadrotor.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"256\" src=\"https://2.bp.blogspot.com/-E7FYRfwgEZE/VpWrU97D4OI/AAAAAAAAOeI/Wf4toRYot88/s400/quadrotor.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Quadrotor running Stereo LSD-SLAM.</b>&nbsp;</div><div class=\"separator\" style=\"clear: both; text-align: center;\">See<a href=\"https://youtu.be/eznMokFQmpc\"> Engel's quadrotor youtube video</a> from 2012.&nbsp;</div><br /></div><div style=\"background-color: white; color: #222222;\">The story of LSD-SLAM is also the story of <b>feature-based vs direct-methods</b> and Engel gave both sides of the debate a fair treatment.&nbsp;<span style=\"font-family: inherit;\">Feature-based methods are engineered to work on top of Harris-like corners, while direct methods use the entire image for alignment.&nbsp;</span><span style=\"font-family: inherit;\">Feature-based methods are faster (as of 2015), but direct methods are good for parallelism. Outliers can be retroactively removed from feature-based systems, while direct methods are less flexible w.r.t. outliners. Rolling shutter is a bigger problem for direct methods and it makes sense to use a global shutter or a rolling shutter model (see Kerl’s work). Feature-based methods require making decisions using incomplete information, but direct methods can use much more information. Feature-based methods have no need for good initialization and direct-based methods need some clever tricks for initialization. There is only about 4 years of research on direct methods and 20+ on sparse methods. Engel is optimistic that direct methods will one day rise to the top, and so am I.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-i2l2PgDiovE/VpOFbRPLrhI/AAAAAAAAOdA/mzE1KpKil4M/s1600/feature-vs-direct.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"300\" src=\"https://1.bp.blogspot.com/-i2l2PgDiovE/VpOFbRPLrhI/AAAAAAAAOdA/mzE1KpKil4M/s400/feature-vs-direct.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Feature-based vs direct methods of building SLAM systems.</b> Slide from Engel's talk.</div><br /></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">At the end of Engel's presentation, Davison asked about semantic segmentation and Engel wondered whether semantic segmentation can be performed directly on semi-dense \"near-image-boundary\" data.&nbsp; However, my personal opinion is that there are better ways to apply semantic segmentation to LSD-like SLAM systems. Semi-dense SLAM can focus on geometric information near boundaries, while object recognition can focus on reliable semantics away from the same boundaries, potentially creating a hybrid geometric/semantic interpretation of the image.</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><b>Related</b>: Engel's <a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/ICCV-SLAM-Workshop_JakobEngel.pdf\">Semi-Dense Direct SLAM presentation</a> slides</div><br /></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 3: Sattler on The challenges of Large-Scale Localization and Mapping</span></b></div><div style=\"background-color: white;\"><span style=\"color: #222222; font-family: inherit;\"><a href=\"https://www.graphics.rwth-aachen.de/person/21/\">Torsten Sattler</a> gave a talk on large-scale localization and mapping.&nbsp;</span><span style=\"color: #222222; font-family: inherit;\">The motivation for this work is to perform 6-dof localization inside an existing map, especially for mobile localization. One of the key points in the talk was that when you are using traditional feature-based methods, storing your descriptors soon becomes very costly. Techniques such as visual vocabularies (remember product quantization?) can significantly reduce memory overhead, and with clever optimization at some point storing descriptors no longer becomes the memory bottleneck.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Another important take-home message from Sattler’s talk is that the number of inliers is not actually a good confidence measure for camera pose estimation.&nbsp; When the feature point are all concentrated in a single part of the image, camera localization can be kilometers away! A better measure of confidence is the “effective inlier count” which looks at the area spanned by the inliers as a fraction of total image area.&nbsp; What you really want is feature matches from all over the image — if the information is spread out across the image you get a much better pose estimate.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Sattler’s take on the future of real-time slam is the following: we should focus on compact map representations, we should get better at understanding camera pose estimate confidences (like down-weighing features from trees), we should work on more challenging scenes (such as worlds with planar structures and nighttime localization against daytime maps).</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-zXNg4No_GME/VpOFsvOweII/AAAAAAAAOdI/l1pj_aH1UA4/s1600/mobileloc.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"218\" src=\"https://4.bp.blogspot.com/-zXNg4No_GME/VpOFsvOweII/AAAAAAAAOdI/l1pj_aH1UA4/s400/mobileloc.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Mobile Localisation: </b>Sattler's key problem is localizing yourself inside a large city with a single smartphone picture</div><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><span style=\"color: #222222;\"><b>Related: </b>Scalable 6-DOF Localization on Mobile Devices.&nbsp;Sven Middelberg, Torsten Sattler, Ole Untzelmann, Leif Kobbelt. In ECCV 2014. [<a href=\"https://www.graphics.rwth-aachen.de/publication/213/ECCV14_preprint.pdf\">pdf</a>]</span><br /><span style=\"color: #222222;\"><b>Related:&nbsp;</b>Torsten Sattler 's <a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/Sattler_challenges_large_scale_loc_and_mapping.pdf\">The challenges of large-scale localisation and mapping</a>&nbsp;slides</span><br /><div style=\"color: #222222;\"><br /></div></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 4: Mur-Artal on Feature-based vs Direct-Methods</span></b></div><div style=\"background-color: white;\"><span style=\"color: #222222;\">Raúl Mur-</span><span style=\"color: #222222; font-family: inherit;\">Artal, the creator of ORB-SLAM, dedicated his entire presentation to the Feature-based vs Direct-method debate in SLAM and he's definitely on the feature-based side. ORB-SLAM is available as an open-source SLAM package and it is hard to beat. During his evaluation of ORB-SLAM vs PTAM it seems that PTAM actually fails quite often (at least on the TUM RGB-D benchmark). LSD-SLAM errors are also much higher on the TUM RGB-D benchmark than expected.</span><br /><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-Vk0x1Y6ZFRU/VpX4gxX7TpI/AAAAAAAAOe8/8OLOkL7iDcw/s1600/types-of-slam.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"205\" src=\"https://4.bp.blogspot.com/-Vk0x1Y6ZFRU/VpX4gxX7TpI/AAAAAAAAOe8/8OLOkL7iDcw/s400/types-of-slam.jpg\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><b>Feature-Based SLAM vs Direct SLAM. </b>See Mur-Artal's&nbsp;<a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/ICCV15_SLAMWS_RaulMur.pdf\">Should we still do sparse feature based SLAM?</a> presentation slides</div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div></div><div style=\"background-color: white;\"><span style=\"color: #222222; font-family: inherit;\"><b>Related:</b>&nbsp;Mur-Artal's&nbsp;</span><span style=\"color: #222222;\"><a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/ICCV15_SLAMWS_RaulMur.pdf\">Should we still do sparse-feature based SLAM?</a>&nbsp;slides</span><br /><span style=\"color: #222222;\"><b>Related: </b>Monocular ORB-SLAM R. Mur-Artal, J. M. M. Montiel and J. D. Tardos. A versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics. 2015 [<a href=\"http://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf\">pdf</a>]</span><br /><span style=\"color: #222222;\"><b>Related:</b>&nbsp;<a href=\"http://github.com/raulmur/ORB_SLAM\">ORB-SLAM Open-source code on github</a>, <a href=\"http://webdiis.unizar.es/~raulmur/orbslam/\">Project Website</a></span><br /><span style=\"color: #222222;\"><br /></span></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 5: Project Tango and Visual loop-closure for image-2-image constraints</span></b></div><span style=\"color: #222222; font-family: inherit;\">Simply put, <a href=\"https://www.google.com/atap/project-tango/\">Google's Project Tango</a> is the world' first attempt at commercializing SLAM.&nbsp;</span><span style=\"color: #222222;\">Simon Lynen from Google Zurich (formerly ETH Zurich) came to the workshop with a Tango live demo (on a tablet) and a presentation on what's new in the world of Tango. In case you don't already know, Google wants to put SLAM capabilities into the next generation of Android Devices.&nbsp;</span><br /><span style=\"color: #222222; font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-cYVkHspdkXg/V1q0jg-pvlI/AAAAAAAAOrk/gUKjTTU-irsiQWwGaOq5ZfghTk6WlxsiQCLcB/s1600/Google-project-tango-3D-mapping-video.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"https://3.bp.blogspot.com/-cYVkHspdkXg/V1q0jg-pvlI/AAAAAAAAOrk/gUKjTTU-irsiQWwGaOq5ZfghTk6WlxsiQCLcB/s1600/Google-project-tango-3D-mapping-video.jpeg\" /></a></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; color: #222222; text-align: center;\">Google's Project Tango needs no introduction.</div><span style=\"color: #222222; font-family: inherit;\"><br /></span><span style=\"color: #222222; font-family: inherit;\">The Project Tango presentation discussed a new way of doing loop closure by finding certain patters in the image-to-image matching matrix. This comes from the “Placeless Place Recognition” work. They also do online bundle adjustment w/ vision-based loop closure.</span><br /><span style=\"color: #222222; font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-5Q_EOliJgwM/VpWuMVDPcGI/AAAAAAAAOeU/ONEAYjX8f58/s1600/placeless.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"101\" src=\"https://2.bp.blogspot.com/-5Q_EOliJgwM/VpWuMVDPcGI/AAAAAAAAOeU/ONEAYjX8f58/s400/placeless.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><b>Loop Closure inside a Project Tango?</b> Lynen et al's <a href=\"https://3234f89137bccf2ede29cc86e315c75116020d70.googledrive.com/host/0B64GJ60h3Ai1MVVwWTZwekhtcFU/publications/bib/lynen_3dv14.pdf\">Placeless Place Recognition</a>. The image-to-image matrix reveals a new way to look for loop-closure. See the algorithm in action in this <a href=\"https://www.youtube.com/watch?v=HfWvWQrCwwA\">youtube video</a>.</div><br /></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">The Project Tango folks are also working on combing multiple crowd-sourced maps at Google, where the goals to combine multiple mini-maps created by different people using Tango-equipped devices.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Simon showed a video of mountain bike trail tracking which is actually quite difficult in practice. The idea is to go down a mountain bike trail using a Tango device and create a map, then the follow-up goal is to have a separate person go down the trail. This currently “semi-works” when there are a few hours between the map building and the tracking step, but won’t work across weeks/months/etc.&nbsp;</span><br /><span style=\"font-family: inherit;\"><br /></span><span style=\"font-family: inherit;\">During the Tango-related discussion, Richard Newcombe pointed out that the “features” used by Project Tango are quite primitive w.r.t. getting a deeper understanding of the environment, and it appears that Project Tango-like methods won't work on outdoor scenes where the world is plagued by non-rigidity, massive illumination changes, etc. &nbsp;So are we to expect different systems being designed for outdoor systems or will Project Tango be an indoor mapping device?</span></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><b>Related:&nbsp;</b></span><span style=\"color: #222222; font-family: inherit;\"><a href=\"https://3234f89137bccf2ede29cc86e315c75116020d70.googledrive.com/host/0B64GJ60h3Ai1MVVwWTZwekhtcFU/publications/bib/lynen_3dv14.pdf\">Placeless Place Recognition.</a>&nbsp;</span><span style=\"color: #222222;\">Lynen, S. ; Bosse, M. ; Furgale, P. ; Siegwart, R. In 3DV 2014.</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><b>Related:&nbsp;</b><a href=\"https://www.youtube.com/watch?v=iP9m9a2KEN4\">Google I/O talk from May 29, 2015 about Tango</a></span></div><div style=\"color: #222222;\"><br /></div></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 6: ElasticFusion is DenseSLAM without a pose-graph</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">ElasticFusion is a dense SLAM technique which requires a RGBD sensor like the Kinect. 2-3 minutes to obtain a high-quality 3D scan of a single room is pretty cool. A pose-graph is used behind the scenes of many (if not most) SLAM systems, and this technique has a different (map-centric) approach. The approach focuses on building a map, but the trick is that the map is deformable, hence the name ElasticFusion. The “Fusion” part of the algorithm is in homage to KinectFusion which was one of the first high quality kinect-based reconstruction pipelines. Also surfels are used as the underlying primitives.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://2.bp.blogspot.com/-W6WF-6uN-Vs/VpOGviRVS9I/AAAAAAAAOdY/a6G-E0aRoSM/s1600/kintinuous.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"212\" src=\"https://2.bp.blogspot.com/-W6WF-6uN-Vs/VpOGviRVS9I/AAAAAAAAOdY/a6G-E0aRoSM/s400/kintinuous.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Image from Kintinuous, an early version of Whelan's Elastic Fusion.</div><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">Recovering light sources: we were given a sneak peak at new unpublished work from Imperial College London / dyson Robotics Lab. The idea is that detecting the light source direction and detecting specularities, you can improve 3D reconstruction results. Cool videos of recovering light source locations which work for up to 4 separate lights.</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><span style=\"color: #222222; font-family: inherit;\"><b>Related:&nbsp;</b></span><span style=\"color: #222222;\"><a href=\"http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/ElasticFusion.pdf\">Map-centric SLAM with ElasticFusion</a> presentation slides</span><br /><span style=\"color: #222222;\"><b>Related:</b>&nbsp;<a href=\"http://www.doc.ic.ac.uk/~bglocker/pdfs/whelan2015rss.pdf\">ElasticFusion: Dense SLAM Without A Pose Graph.&nbsp;</a>Whelan, Thomas and Leutenegger, Stefan and Salas-Moreno, Renato F and Glocker, Ben and Davison, Andrew J. In RSS 2015.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Talk 7: Richard Newcombe’s DynamicFusion</span></b><br /><span style=\"font-family: inherit;\">Richard Newcombe's (whose recently formed company was acquired by Oculus), was the last presenter. &nbsp;It's really cool to see the person behind <a href=\"http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_iccv2011.pdf\">DTAM</a>, <a href=\"http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_ismar2011.pdf\">KinectFusion</a>, and&nbsp;<a href=\"http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdf\">DynamicFusion</a> now working in the VR space.</span><br /><b><span style=\"font-family: inherit;\"><br /></span></b><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://4.bp.blogspot.com/-S3mMC77oNMM/VpWw25SfslI/AAAAAAAAOeg/16-YJs3a-sc/s1600/dynamicfusion.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"160\" src=\"https://4.bp.blogspot.com/-S3mMC77oNMM/VpWw25SfslI/AAAAAAAAOeg/16-YJs3a-sc/s400/dynamicfusion.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Newcombe's <a href=\"http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdf\">Dynamic Fusion</a> algorithm. The technique won the prestigious CVPR 2015 best paper award, and to see it in action just take a look at the authors' <a href=\"https://www.youtube.com/watch?v=i1eZekcc_lM\">DynamicFusion Youtube video</a>.</div><b><span style=\"font-family: inherit;\"><br /></span></b></div><div style=\"background-color: white;\"><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><b>Related</b>:&nbsp;</span><a href=\"http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdf\">DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time</a>, Richard A. Newcombe, Dieter Fox, Steven M. Seitz. In CVPR 2015. [<a href=\"http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdf\">pdf</a>] [Best-Paper winner]</div><span style=\"color: #222222;\"><b>Related:</b>&nbsp;<a href=\"http://homes.cs.washington.edu/~newcombe/papers/Salas-Moreno_etal_cvpr2013.pdf\">SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a> Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H. J. Kelly and Andrew J. Davison (CVPR 2013)</span><br /><span style=\"color: #222222;\"><b>Related:</b>&nbsp;</span><span style=\"color: #222222;\"><a href=\"http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_ismar2011.pdf\">KinectFusion: Real-Time Dense Surface Mapping and Tracking</a> Richard A. Newcombe Shahram Izadi,Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Andrew Fitzgibbon (ISMAR 2011, Best paper award!)</span><br /><span style=\"color: #222222;\"><br /></span><br /><div style=\"color: #222222;\"><b>Workshop Demos</b></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">During the demo sessions (held in the middle of the workshop), many of the presenter showed off their SLAM systems in action. Many of these systems are available as open-source (free for non-commercial use?) packages, so if you’re interested in real-time SLAM, downloading the code is worth a shot. However,<b> the one demo which stood out was Andrew Davison’s showcase of his MonoSLAM system from 2004</b>. Andy had to revive his 15-year old laptop (which was running Redhat Linux) to show off his original system, running on the original hardware. If the computer vision community is going to oneway decide on a “retro-vision” demo session, I’m just going to go ahead and nominate Andy for the best-paper prize, right now.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-AwQoCYKPPQY/VpYSrlbUf8I/AAAAAAAAOfk/MBKnnJh_Yss/s1600/IMG_0500.JPG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"400\" src=\"https://3.bp.blogspot.com/-AwQoCYKPPQY/VpYSrlbUf8I/AAAAAAAAOfk/MBKnnJh_Yss/s400/IMG_0500.JPG\" width=\"300\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Andry's Retro-Vision SLAM Setup (Pictured on December 18th, 2015)</div><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">It was interesting to watch the SLAM system experts wave their USB cameras around, showing their systems build 3D maps of the desk-sized area around their laptops.&nbsp; If you carefully look at the way these experts move the camera around (i.e., smooth circular motions), you can almost tell how long a person has been working with SLAM. When the non-experts hold the camera, probability of tracking failure is significantly higher.</span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"color: #222222;\"><span style=\"font-family: inherit;\">I had the pleasure of speaking with Andy during the demo session, and I was curious which line of work (in the past 15 years) surprised him the most. His reply was that PTAM, which showed how to perform real-time bundle adjustment, surprised him the most. The PTAM system was essentially a MonoSLAM++ system, but the significantly improved tracking results were due to taking a heavyweight algorithm (bundle adjustment) and making it real-time — something which Andy did not believe was possible in the early 2000s.</span></div><h2 style=\"color: #222222;\"><span style=\"font-family: inherit;\"><b>Part III: Deep Learning vs SLAM</b></span></h2></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">The SLAM panel discussion was a lot of fun. Before we jump to the important Deep Learning vs SLAM discussion, I should mention that each of the workshop presenters agreed that <b>semantics are necessary to build bigger and better SLAM systems</b>. There were lots of interesting mini-conversations about future directions. During the debates, <a href=\"https://www.inf.ethz.ch/personal/marc.pollefeys/\">Marc Pollefeys</a>&nbsp;(a well-known researcher in SfM and Multiple-View Geometry) reminded everybody that <b>Robotics is the killer application of SLAM</b>&nbsp;and suggested we keep an eye on the prize. This is quite surprising since SLAM was traditionally applied to Robotics problems, but the lack of Robotics success in the last few decades (Google Robotics?) has shifted the focus of SLAM away from Robots and towards large-scale map building (ala Google Maps) and Augmented Reality. Nobody at this workshop talked about Robots.</span><br /><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">Integrating semantic information into SLAM</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">There was a lot of interest in incorporating semantics into today’s top-performing SLAM systems. When it comes to semantics, the <b>SLAM community is unfortunately stuck in the world of bags-of-visual-words</b>, and doesn't have new ideas on how to integrate semantic information into their systems. On the other end, we’re now seeing real-time semantic segmentation demos (based on ConvNets) popping up at CVPR/ICCV/ECCV, and in my opinion SLAM needs Deep Learning as much as the other way around.</span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://1.bp.blogspot.com/-gOJoLF_DWKQ/VpOHU7r_O4I/AAAAAAAAOdg/vq85sOEnlBU/s1600/semantics.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"281\" src=\"https://1.bp.blogspot.com/-gOJoLF_DWKQ/VpOHU7r_O4I/AAAAAAAAOdg/vq85sOEnlBU/s400/semantics.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\">Integrating semantics into SLAM is often talk about, but it is easier said than done.</div><div class=\"separator\" style=\"clear: both; text-align: center;\">Figure 6.9 (page 142) from Moreno's PhD thesis:&nbsp;<a href=\"https://www.doc.ic.ac.uk/~rfs09/docs/Salas-Moreno-R-2014-PhD-Thesis.pdf\">Dense Semantic SLAM</a></div></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><br /></span></div><div style=\"background-color: white; color: #222222;\"><b><span style=\"font-family: inherit;\">\"Will end-to-end learning dominate SLAM?\"</span></b></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">Towards the end of the SLAM workshop panel, <a href=\"http://www.zeeshanzia.com/\">Dr. Zeeshan Zia</a> asked a question which <i>startled</i> the entire room and led to a memorable, energy-filled discussion. You should have seen the look on the panel’s faces.</span>&nbsp;It was a bunch of geometers being thrown a fireball of deep learning.<span style=\"font-family: inherit;\">&nbsp;Their facial expressions suggest both bewilderment, anger, and disgust. \"<i>How dare you question us?\" </i>they were thinking.&nbsp;And it is only during these fleeting moments that we can truly appreciate the conference experience. Zia's question was essentially: <b>Will end-to-end learning soon replace the mostly manual labor involved in building today’s SLAM systems?</b>.&nbsp;</span><br /><span style=\"font-family: inherit;\"><br /></span><span style=\"font-family: inherit;\">Zia's question is very important because end-to-end trainable systems have been slowly creeping up on many advanced computer science problems, and there's no reason to believe SLAM will be an exception. A handful of the presenters pointed out that current SLAM systems rely on too much geometry for a pure deep-learning based SLAM system to make sense -- we should use learning to make the point descriptors better, but leave the geometry alone. <i>Just because you can use deep learning to make a calculator, it doesn't mean you should.</i></span><br /><span style=\"font-family: inherit;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://3.bp.blogspot.com/-sOdinuQ3bBg/VpYGCkrwjcI/AAAAAAAAOfM/2JbO1Sny3A0/s1600/convnet_lecun_stereo.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"85\" src=\"https://3.bp.blogspot.com/-sOdinuQ3bBg/VpYGCkrwjcI/AAAAAAAAOfM/2JbO1Sny3A0/s400/convnet_lecun_stereo.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"http://arxiv.org/abs/1409.4326\">Learning Stereo Similarity Functions</a> via ConvNets, by Yan LeCun and collaborators.</div><span style=\"font-family: inherit;\"><br /></span><br /><span style=\"font-family: inherit;\">While many of the panel speakers responded with a somewhat affirmative \"no\", it was Newcombe which surprisingly championed what the marriage of Deep Learning and SLAM might look like.&nbsp;</span><br /><span style=\"font-family: inherit;\"><br /></span><b>Newcombe's Proposal:&nbsp;</b><span style=\"font-family: inherit;\"><b>Use SLAM to fuel Deep Learning</b></span><br /><span style=\"font-family: inherit;\">Although Newcombe didn’t provide much evidence or ideas on how Deep Learning might help SLAM, he provided <b>a clear path on how SLAM might help Deep Learning</b>.&nbsp; Think of all those maps that we've built using large-scale SLAM and all those correspondences that these systems provide — isn’t that a clear path for building terascale image-image \"association\" datasets which should be able to help deep learning? The basic idea is that today's SLAM systems are large-scale \"correspondence engines\" which can be used to generate large-scale datasets, precisely what needs to be fed into a deep ConvNet.</span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\"><b></b><br /></span><span style=\"font-family: inherit;\"><b>Concluding Remarks</b></span></div><div style=\"background-color: white; color: #222222;\"><span style=\"font-family: inherit;\">There is quite a large disconnect between the kind of work done at the mainstream ICCV conference (heavy on machine learning) and the kind of work presented at the real-time SLAM workshop (heavy on geometric methods like bundle adjustment). The mainstream Computer Vision community has witnessed several mini-revolutions within the past decade (e.g., Dalal-Triggs, DPM, ImageNet, ConvNets, R-CNN) while the SLAM systems of today don’t look very different than they did 8 years ago. The Kinect sensor has probably been the single largest game changer in SLAM, but the fundamental algorithms remain intact.</span></div><div style=\"background-color: white; color: #222222;\"><div style=\"-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Times; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 1; word-spacing: 0px;\"><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\"><a href=\"http://wordpress.viu.ca/ciel/files/2013/01/134992626.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"http://wordpress.viu.ca/ciel/files/2013/01/134992626.jpg\" height=\"296\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\"><b>Integrating semantic information: The next frontier in Visual SLAM.</b>&nbsp;</div><div class=\"separator\" style=\"clear: both; margin: 0px; text-align: center;\">Brain image from&nbsp;<a href=\"http://wordpress.viu.ca/ciel/2013/01/23/gaming-and-student-disengagement/\">Arwen Wallington</a>'s blog post.</div><div style=\"margin: 0px;\"><br /></div><div style=\"margin: 0px;\">Today’s SLAM systems help machines geometrically understand the immediate world (i.e., build associations in a local coordinate system) while today’s Deep Learning systems help machines reason categorically (i.e., build associations across distinct object instances). In conclusion, I share Newcombe and Davison excitement in Visual SLAM, as vision-based algorithms are going to turn Augmented and Virtual Reality into billion dollar markets. However, we should not forget to keep our eyes on the \"trillion-dollar\" market, the one that's going to redefine what it means to \"work\" -- namely <i>Robotics</i>. The day of Robot SLAM will come soon.</div></div></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 27
}