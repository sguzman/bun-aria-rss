{
  "title": "Experiment with Dask and TensorFlow",
  "link": "",
  "updated": "2017-02-11T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2017/02/11/dask-tensorflow",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>This post briefly describes potential interactions between Dask and TensorFlow\nand then goes through a concrete example using them together for distributed\ntraining with a moderately complex architecture.</p>\n\n<p>This post was written in haste and the attached experiment is of low quality,\nsee disclaimers below.  A similar and much better example with XGBoost is\nincluded in the comments at the end.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Dask and TensorFlow both provide distributed computing in Python.\nTensorFlow excels at deep learning applications while Dask is more generic.\nWe can combine both together in a few applications:</p>\n\n<ol>\n  <li><strong>Simple data parallelism:</strong> hyper-parameter searches during training\nand predicting already-trained models against large datasets are both\ntrivial to distribute with Dask as they would be trivial to distribute with\nany distributed computing system (Hadoop/Spark/Flink/etc..)  We won’t\ndiscuss this topic much.  It should be straightforward.</li>\n  <li><strong>Deployment:</strong> A common pain point with TensorFlow is that setup isn’t\nwell automated.  This plagues all distributed systems, especially those\nthat are run on a wide variety of cluster managers (see <a href=\"http://matthewrocklin.com/blog/work/2016/09/22/cluster-deployments\">cluster deployment\nblogpost</a>\nfor more information).  Fortunately, if you already have a Dask cluster\nrunning it’s trivial to stand up a distributed TensorFlow network on\ntop of it running within the same processes.</li>\n  <li><strong>Pre-processing:</strong> We pre-process data with dask.dataframe or dask.array,\nand then hand that data off to TensorFlow for training.  If Dask and\nTensorFlow are co-located on the same processes then this movement is\nefficient.  Working together we can build efficient and general use deep\nlearning pipelines.</li>\n</ol>\n\n<p>In this blogpost we look <em>very</em> briefly at the first case of simple\nparallelism.  Then go into more depth on an experiment that uses Dask and\nTensorFlow in a more complex situation.  We’ll find we can accomplish a fairly\nsophisticated workflow easily, both due to how sensible TensorFlow is to set up\nand how flexible Dask can be in advanced situations.</p>\n\n<h2 id=\"motivation-and-disclaimers\">Motivation and Disclaimers</h2>\n\n<p>Distributed deep learning is fundamentally changing the way humanity solves\nsome very hard computing problems like natural language translation,\nspeech-to-text transcription, image recognition, etc..  However, distributed\ndeep learning also suffers from public excitement, which may distort our image\nof its utility.  Distributed deep learning is not always the correct choice for\nmost problems.  This is for two reasons:</p>\n\n<ol>\n  <li>Focusing on <strong>single machine</strong> computation is often a better use of time.\nModel design, GPU hardware, etc. can have a more dramatic impact than\nscaling out.  For newcomers to deep learning, watching <a href=\"https://simons.berkeley.edu/talks/tutorial-deep-learning\">online video lecture\nseries</a> may be a\nbetter use of time than reading this blogpost.</li>\n  <li><strong>Traditional machine learning</strong> techniques like logistic regression, and\ngradient boosted trees can be more effective than deep learning if you have\nfinite data.  They can also sometimes provide valuable interpretability\nresults.</li>\n</ol>\n\n<p>Regardless, there are some concrete take-aways, even if distributed deep\nlearning is not relevant to your application:</p>\n\n<ol>\n  <li>TensorFlow is straightforward to set up from Python</li>\n  <li>Dask is sufficiently flexible out of the box to support complex settings\nand workflows</li>\n  <li>We’ll see an example of a typical distributed learning approach that\ngeneralizes beyond deep learning.</li>\n</ol>\n\n<p>Additionally the author does not claim expertise in deep learning and wrote\nthis blogpost in haste.</p>\n\n<h2 id=\"simple-parallelism\">Simple Parallelism</h2>\n\n<p>Most parallel computing is simple.  We easily apply one function to lots of\ndata, perhaps with slight variation.  In the case of deep learning this\ncan enable a couple of common workflows:</p>\n\n<ol>\n  <li>\n    <p>Build many different models, train each on the same data, choose the best\nperforming one.  Using dask’s concurrent.futures interface, this looks\nsomething like the following:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Hyperparameter search\n</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'dask-scheduler-address:8786'</span><span class=\"p\">)</span>\n<span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">train_and_evaluate</span><span class=\"p\">,</span> <span class=\"n\">hyper_param_list</span><span class=\"p\">,</span> <span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"n\">best</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"nb\">max</span><span class=\"p\">,</span> <span class=\"n\">scores</span><span class=\"p\">)</span>\n<span class=\"n\">best</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n</code></pre></div>    </div>\n  </li>\n  <li>\n    <p>Given an already-trained model, use it to predict outcomes on lots of data.\nHere we use a big data collection like dask.dataframe:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Distributed prediction\n</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">dd</span><span class=\"p\">.</span><span class=\"n\">read_parquet</span><span class=\"p\">(</span><span class=\"s\">'...'</span><span class=\"p\">)</span>\n<span class=\"p\">...</span> <span class=\"c1\"># do some preprocessing here\n</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s\">'outcome'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"n\">map_partitions</span><span class=\"p\">(</span><span class=\"n\">predict</span><span class=\"p\">)</span>\n</code></pre></div>    </div>\n  </li>\n</ol>\n\n<p>These techniques are relatively straightforward if you have modest exposure to\nDask and TensorFlow (or any other machine learning library like scikit-learn),\nso I’m going to ignore them for now and focus on more complex situations.</p>\n\n<p>Interested readers may find this blogpost on\n<a href=\"https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\">TensorFlow and Spark</a>\nof interest.  It is a nice writeup that goes over these two techniques in more\ndetail.</p>\n\n<h2 id=\"a-distributed-tensorflow-application\">A Distributed TensorFlow Application</h2>\n\n<p>We’re going to replicate <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py\">this TensorFlow example</a>\nwhich uses multiple machines to train a model that fits in memory using\nparameter servers for coordination.  Our TensorFlow network will have three\ndifferent kinds of servers:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/tensorflow-distributed-network.svg\" width=\"50%\" align=\"right\" alt=\"distributed TensorFlow training graph\" /></p>\n\n<ol>\n  <li><strong>Workers</strong>: which will get updated parameters, consume training data, and\nuse that data to generate updates to send back to the parameter servers</li>\n  <li><strong>Parameter Servers:</strong> which will hold onto model parameters, synchronizing\nwith the workers as necessary</li>\n  <li><strong>Scorer:</strong> which will periodically test the current parameters against\nvalidation/test data and emit a current cross_entropy score to see how well\nthe system is running.</li>\n</ol>\n\n<p>This is a fairly typical approach when the model can fit in one machine, but\nwhen we want to use multiple machines to accelerate training or because data\nvolumes are too large.</p>\n\n<p>We’ll use TensorFlow to do all of the actual training and scoring.  We’ll use\nDask to do everything else.  In particular, we’re about to do the following:</p>\n\n<ol>\n  <li>Prepare data with dask.array</li>\n  <li>Set up TensorFlow workers as long-running tasks</li>\n  <li>Feed data from Dask to TensorFlow while scores remain poor</li>\n  <li>Let TensorFlow handle training using its own network</li>\n</ol>\n\n<h2 id=\"prepare-data-with-daskarray\">Prepare Data with Dask.array</h2>\n\n<p>For this toy example we’re just going to use the mnist data that comes with\nTensorFlow.  However, we’ll artificially inflate this data by concatenating\nit to itself many times across a cluster:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">get_mnist</span><span class=\"p\">():</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">tensorflow.examples.tutorials.mnist</span> <span class=\"kn\">import</span> <span class=\"n\">input_data</span>\n    <span class=\"n\">mnist</span> <span class=\"o\">=</span> <span class=\"n\">input_data</span><span class=\"p\">.</span><span class=\"n\">read_data_sets</span><span class=\"p\">(</span><span class=\"s\">'/tmp/mnist-data'</span><span class=\"p\">,</span> <span class=\"n\">one_hot</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">labels</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dask</span> <span class=\"kn\">import</span> <span class=\"n\">delayed</span>\n\n<span class=\"n\">datasets</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">delayed</span><span class=\"p\">(</span><span class=\"n\">get_mnist</span><span class=\"p\">)()</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)]</span>  <span class=\"c1\"># 20 versions of same dataset\n</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">datasets</span><span class=\"p\">]</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"n\">datasets</span><span class=\"p\">]</span>\n\n<span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">from_delayed</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">55000</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'float32'</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">im</span> <span class=\"ow\">in</span> <span class=\"n\">images</span><span class=\"p\">]</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">from_delayed</span><span class=\"p\">(</span><span class=\"n\">la</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">55000</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'float32'</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">la</span> <span class=\"ow\">in</span> <span class=\"n\">labels</span><span class=\"p\">]</span>\n\n<span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">concatenate</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">images</span>\n<span class=\"n\">dask</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"o\">&lt;</span><span class=\"n\">concate</span><span class=\"p\">...,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1100000</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">chunksize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">55000</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">)</span><span class=\"o\">&gt;</span>\n\n<span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">persist</span><span class=\"p\">([</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">])</span>  <span class=\"c1\"># persist data in memory\n</span></code></pre></div></div>\n\n<p>This gives us a moderately large distributed array of around a million tiny\nimages.  If we wanted to we could inspect or clean up this data using normal\ndask.array constructs:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">im</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">].</span><span class=\"n\">compute</span><span class=\"p\">().</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s\">'gray'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/tf-images-one.png\" width=\"20%\" alt=\"mnist number 3\" /></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">im</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">).</span><span class=\"n\">compute</span><span class=\"p\">().</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s\">'gray'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/tf-images-mean.png\" width=\"20%\" alt=\"mnist mean\" /></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">im</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">.</span><span class=\"n\">var</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">).</span><span class=\"n\">compute</span><span class=\"p\">().</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s\">'gray'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/tf-images-var.png\" width=\"20%\" alt=\"mnist var\" /></p>\n\n<p>This shows off how one can use Dask collections to clean up and provide\npre-processing and feature generation on data in parallel before sending it to\nTensorFlow.  In our simple case we won’t actually do any of this, but it’s\nuseful in more real-world situations.</p>\n\n<p>Finally, after doing our preprocessing on the distributed array of all of our\ndata we’re going to collect images and labels together and batch them into\nsmaller chunks.  Again we use some dask.array constructs and\n<a href=\"http://dask.pydata.org/en/latest/delayed.html\">dask.delayed</a> when things get\nmessy.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">.</span><span class=\"n\">rechunk</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n\n<span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">.</span><span class=\"n\">to_delayed</span><span class=\"p\">().</span><span class=\"n\">flatten</span><span class=\"p\">().</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n<span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">.</span><span class=\"n\">to_delayed</span><span class=\"p\">().</span><span class=\"n\">flatten</span><span class=\"p\">().</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n<span class=\"n\">batches</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">delayed</span><span class=\"p\">([</span><span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">la</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">im</span><span class=\"p\">,</span> <span class=\"n\">la</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">)]</span>\n\n<span class=\"n\">batches</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">batches</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Now we have a few hundred pairs of NumPy arrays in distributed memory waiting\nto be sent to a TensorFlow worker.</p>\n\n<h2 id=\"setting-up-tensorflow-workers-alongside-dask-workers\">Setting up TensorFlow workers alongside Dask workers</h2>\n\n<p>Dask workers are just normal Python processes.  TensorFlow can launch itself\nfrom a normal Python process.  We’ve made a small function\n<a href=\"https://github.com/mrocklin/dask-tensorflow/blob/6fdadb6f52935788d593bdc01d441cfd9ad6a3be/dask_tensorflow/core.py\">here</a>\nthat launches TensorFlow servers alongside Dask workers using Dask’s ability to\nrun long-running tasks and maintain user-defined state.  All together, this is\nabout 80 lines of code (including comments and docstrings) and allows us to\ndefine our TensorFlow network on top of Dask as follows:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>$ pip install git+https://github.com/mrocklin/dask-tensorflow\n</code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distibuted</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span>  <span class=\"c1\"># we already had this above\n</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s\">'dask-scheduler-address:8786'</span><span class=\"p\">)</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">dask_tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">start_tensorflow</span>\n<span class=\"n\">tf_spec</span><span class=\"p\">,</span> <span class=\"n\">dask_spec</span> <span class=\"o\">=</span> <span class=\"n\">start_tensorflow</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">ps</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">worker</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">scorer</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">tf_spec</span><span class=\"p\">.</span><span class=\"n\">as_dict</span><span class=\"p\">()</span>\n<span class=\"p\">{</span><span class=\"s\">'ps'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'192.168.100.1:2227'</span><span class=\"p\">],</span>\n <span class=\"s\">'scorer'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'192.168.100.2:2222'</span><span class=\"p\">],</span>\n <span class=\"s\">'worker'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'192.168.100.3:2223'</span><span class=\"p\">,</span>\n            <span class=\"s\">'192.168.100.4:2224'</span><span class=\"p\">,</span>\n            <span class=\"s\">'192.168.100.5:2225'</span><span class=\"p\">,</span>\n            <span class=\"s\">'192.168.100.6:2226'</span><span class=\"p\">]}</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">dask_spec</span>\n<span class=\"p\">{</span><span class=\"s\">'ps'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'tcp://192.168.100.1:34471'</span><span class=\"p\">],</span>\n <span class=\"s\">'scorer'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'tcp://192.168.100.2:40623'</span><span class=\"p\">],</span>\n <span class=\"s\">'worker'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'tcp://192.168.100.3:33075'</span><span class=\"p\">,</span>\n            <span class=\"s\">'tcp://192.168.100.4:37123'</span><span class=\"p\">,</span>\n            <span class=\"s\">'tcp://192.168.100.5:32839'</span><span class=\"p\">,</span>\n            <span class=\"s\">'tcp://192.168.100.6:36822'</span><span class=\"p\">]}</span>\n</code></pre></div></div>\n\n<p>This starts three groups of TensorFlow servers in the Dask worker processes.\nTensorFlow will manage its own communication but co-exist right alongside Dask\nin the same machines and in the same shared memory spaces (note that in the\nspecs above the IP addresses match but the ports differ).</p>\n\n<p>This also sets up a normal Python queue along which Dask can safely send\ninformation to TensorFlow.  This is how we’ll send those batches of training\ndata between the two services.</p>\n\n<h2 id=\"define-tensorflow-model-and-distribute-roles\">Define TensorFlow Model and Distribute Roles</h2>\n\n<p>Now is the part of the blogpost where my expertise wanes.  I’m just going to\ncopy-paste-and-modify a canned example from the TensorFlow documentation.  This\nis a simplistic model for this problem and it’s entirely possible that I’m\nmaking transcription errors.  But still, it should get the point across.  You\ncan safely ignore most of this code.  Dask stuff gets interesting again\ntowards the bottom:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">math</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tempfile</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">from</span> <span class=\"nn\">queue</span> <span class=\"kn\">import</span> <span class=\"n\">Empty</span>\n\n<span class=\"n\">IMAGE_PIXELS</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n<span class=\"n\">hidden_units</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">sync_replicas</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n<span class=\"n\">replicas_to_aggregate</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">])</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">server</span><span class=\"p\">):</span>\n    <span class=\"n\">worker_device</span> <span class=\"o\">=</span> <span class=\"s\">\"/job:%s/task:%d\"</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">server</span><span class=\"p\">.</span><span class=\"n\">server_def</span><span class=\"p\">.</span><span class=\"n\">job_name</span><span class=\"p\">,</span>\n                                         <span class=\"n\">server</span><span class=\"p\">.</span><span class=\"n\">server_def</span><span class=\"p\">.</span><span class=\"n\">task_index</span><span class=\"p\">)</span>\n    <span class=\"n\">task_index</span> <span class=\"o\">=</span> <span class=\"n\">server</span><span class=\"p\">.</span><span class=\"n\">server_def</span><span class=\"p\">.</span><span class=\"n\">task_index</span>\n    <span class=\"n\">is_chief</span> <span class=\"o\">=</span> <span class=\"n\">task_index</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">replica_device_setter</span><span class=\"p\">(</span>\n                      <span class=\"n\">worker_device</span><span class=\"o\">=</span><span class=\"n\">worker_device</span><span class=\"p\">,</span>\n                      <span class=\"n\">ps_device</span><span class=\"o\">=</span><span class=\"s\">\"/job:ps/cpu:0\"</span><span class=\"p\">,</span>\n                      <span class=\"n\">cluster</span><span class=\"o\">=</span><span class=\"n\">tf_spec</span><span class=\"p\">)):</span>\n\n        <span class=\"n\">global_step</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"global_step\"</span><span class=\"p\">,</span> <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Variables of the hidden layer\n</span>        <span class=\"n\">hid_w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span>\n            <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">truncated_normal</span><span class=\"p\">(</span>\n                <span class=\"p\">[</span><span class=\"n\">IMAGE_PIXELS</span> <span class=\"o\">*</span> <span class=\"n\">IMAGE_PIXELS</span><span class=\"p\">,</span> <span class=\"n\">hidden_units</span><span class=\"p\">],</span>\n                <span class=\"n\">stddev</span><span class=\"o\">=</span><span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">IMAGE_PIXELS</span><span class=\"p\">),</span>\n            <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"hid_w\"</span><span class=\"p\">)</span>\n        <span class=\"n\">hid_b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">hidden_units</span><span class=\"p\">]),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"hid_b\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Variables of the softmax layer\n</span>        <span class=\"n\">sm_w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span>\n            <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">truncated_normal</span><span class=\"p\">(</span>\n                <span class=\"p\">[</span><span class=\"n\">hidden_units</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">],</span>\n                <span class=\"n\">stddev</span><span class=\"o\">=</span><span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">math</span><span class=\"p\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">hidden_units</span><span class=\"p\">)),</span>\n            <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sm_w\"</span><span class=\"p\">)</span>\n        <span class=\"n\">sm_b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"mi\">10</span><span class=\"p\">]),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"sm_b\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Ops: located on the worker specified with task_index\n</span>        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">IMAGE_PIXELS</span> <span class=\"o\">*</span> <span class=\"n\">IMAGE_PIXELS</span><span class=\"p\">])</span>\n        <span class=\"n\">y_</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">])</span>\n\n        <span class=\"n\">hid_lin</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">xw_plus_b</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">hid_w</span><span class=\"p\">,</span> <span class=\"n\">hid_b</span><span class=\"p\">)</span>\n        <span class=\"n\">hid</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">hid_lin</span><span class=\"p\">)</span>\n\n        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">xw_plus_b</span><span class=\"p\">(</span><span class=\"n\">hid</span><span class=\"p\">,</span> <span class=\"n\">sm_w</span><span class=\"p\">,</span> <span class=\"n\">sm_b</span><span class=\"p\">))</span>\n        <span class=\"n\">cross_entropy</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">reduce_sum</span><span class=\"p\">(</span><span class=\"n\">y_</span> <span class=\"o\">*</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">clip_by_value</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"mf\">1e-10</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">)))</span>\n\n        <span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">AdamOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">sync_replicas</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">replicas_to_aggregate</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n                <span class=\"n\">replicas_to_aggregate</span> <span class=\"o\">=</span> <span class=\"n\">num_workers</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">replicas_to_aggregate</span> <span class=\"o\">=</span> <span class=\"n\">replicas_to_aggregate</span>\n\n            <span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">SyncReplicasOptimizer</span><span class=\"p\">(</span>\n                      <span class=\"n\">opt</span><span class=\"p\">,</span>\n                      <span class=\"n\">replicas_to_aggregate</span><span class=\"o\">=</span><span class=\"n\">replicas_to_aggregate</span><span class=\"p\">,</span>\n                      <span class=\"n\">total_num_replicas</span><span class=\"o\">=</span><span class=\"n\">num_workers</span><span class=\"p\">,</span>\n                      <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">\"mnist_sync_replicas\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">train_step</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">cross_entropy</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"o\">=</span><span class=\"n\">global_step</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">sync_replicas</span><span class=\"p\">:</span>\n            <span class=\"n\">local_init_op</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">local_step_init_op</span>\n            <span class=\"k\">if</span> <span class=\"n\">is_chief</span><span class=\"p\">:</span>\n                <span class=\"n\">local_init_op</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">chief_init_op</span>\n\n            <span class=\"n\">ready_for_local_init_op</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">ready_for_local_init_op</span>\n\n            <span class=\"c1\"># Initial token and chief queue runners required by the sync_replicas mode\n</span>            <span class=\"n\">chief_queue_runner</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">get_chief_queue_runner</span><span class=\"p\">()</span>\n            <span class=\"n\">sync_init_op</span> <span class=\"o\">=</span> <span class=\"n\">opt</span><span class=\"p\">.</span><span class=\"n\">get_init_tokens_op</span><span class=\"p\">()</span>\n\n        <span class=\"n\">init_op</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n        <span class=\"n\">train_dir</span> <span class=\"o\">=</span> <span class=\"n\">tempfile</span><span class=\"p\">.</span><span class=\"n\">mkdtemp</span><span class=\"p\">()</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">sync_replicas</span><span class=\"p\">:</span>\n          <span class=\"n\">sv</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">Supervisor</span><span class=\"p\">(</span>\n              <span class=\"n\">is_chief</span><span class=\"o\">=</span><span class=\"n\">is_chief</span><span class=\"p\">,</span>\n              <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">train_dir</span><span class=\"p\">,</span>\n              <span class=\"n\">init_op</span><span class=\"o\">=</span><span class=\"n\">init_op</span><span class=\"p\">,</span>\n              <span class=\"n\">local_init_op</span><span class=\"o\">=</span><span class=\"n\">local_init_op</span><span class=\"p\">,</span>\n              <span class=\"n\">ready_for_local_init_op</span><span class=\"o\">=</span><span class=\"n\">ready_for_local_init_op</span><span class=\"p\">,</span>\n              <span class=\"n\">recovery_wait_secs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n              <span class=\"n\">global_step</span><span class=\"o\">=</span><span class=\"n\">global_step</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n          <span class=\"n\">sv</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">Supervisor</span><span class=\"p\">(</span>\n              <span class=\"n\">is_chief</span><span class=\"o\">=</span><span class=\"n\">is_chief</span><span class=\"p\">,</span>\n              <span class=\"n\">logdir</span><span class=\"o\">=</span><span class=\"n\">train_dir</span><span class=\"p\">,</span>\n              <span class=\"n\">init_op</span><span class=\"o\">=</span><span class=\"n\">init_op</span><span class=\"p\">,</span>\n              <span class=\"n\">recovery_wait_secs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n              <span class=\"n\">global_step</span><span class=\"o\">=</span><span class=\"n\">global_step</span><span class=\"p\">)</span>\n\n        <span class=\"n\">sess_config</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">ConfigProto</span><span class=\"p\">(</span>\n            <span class=\"n\">allow_soft_placement</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">log_device_placement</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n            <span class=\"n\">device_filters</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"/job:ps\"</span><span class=\"p\">,</span> <span class=\"s\">\"/job:worker/task:%d\"</span> <span class=\"o\">%</span> <span class=\"n\">task_index</span><span class=\"p\">])</span>\n\n        <span class=\"c1\"># The chief worker (task_index==0) session will prepare the session,\n</span>        <span class=\"c1\"># while the remaining workers will wait for the preparation to complete.\n</span>        <span class=\"k\">if</span> <span class=\"n\">is_chief</span><span class=\"p\">:</span>\n          <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Worker %d: Initializing session...\"</span> <span class=\"o\">%</span> <span class=\"n\">task_index</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n          <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Worker %d: Waiting for session to be initialized...\"</span> <span class=\"o\">%</span>\n                <span class=\"n\">task_index</span><span class=\"p\">)</span>\n\n        <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">sv</span><span class=\"p\">.</span><span class=\"n\">prepare_or_wait_for_session</span><span class=\"p\">(</span><span class=\"n\">server</span><span class=\"p\">.</span><span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">sess_config</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">sync_replicas</span> <span class=\"ow\">and</span> <span class=\"n\">is_chief</span><span class=\"p\">:</span>\n          <span class=\"c1\"># Chief worker will start the chief queue runner and call the init op.\n</span>          <span class=\"n\">sess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">sync_init_op</span><span class=\"p\">)</span>\n          <span class=\"n\">sv</span><span class=\"p\">.</span><span class=\"n\">start_queue_runners</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">chief_queue_runner</span><span class=\"p\">])</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y_</span><span class=\"p\">,</span> <span class=\"n\">train_step</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">cross_entropy</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">ps_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">scoring_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Scores Channel\n</span>        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">channel</span><span class=\"p\">(</span><span class=\"s\">'scores'</span><span class=\"p\">,</span> <span class=\"n\">maxlen</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Make Model\n</span>        <span class=\"n\">server</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span>\n        <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">cross_entropy</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Testing Data\n</span>        <span class=\"kn\">from</span> <span class=\"nn\">tensorflow.examples.tutorials.mnist</span> <span class=\"kn\">import</span> <span class=\"n\">input_data</span>\n        <span class=\"n\">mnist</span> <span class=\"o\">=</span> <span class=\"n\">input_data</span><span class=\"p\">.</span><span class=\"n\">read_data_sets</span><span class=\"p\">(</span><span class=\"s\">'/tmp/mnist-data'</span><span class=\"p\">,</span> <span class=\"n\">one_hot</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">test_data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">validation</span><span class=\"p\">.</span><span class=\"n\">images</span><span class=\"p\">,</span>\n                     <span class=\"n\">y_</span><span class=\"p\">:</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">validation</span><span class=\"p\">.</span><span class=\"n\">labels</span><span class=\"p\">}</span>\n\n        <span class=\"c1\"># Main Loop\n</span>        <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n            <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">cross_entropy</span><span class=\"p\">,</span> <span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"n\">test_data</span><span class=\"p\">)</span>\n            <span class=\"n\">scores</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">score</span><span class=\"p\">))</span>\n\n            <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">worker_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">channel</span><span class=\"p\">(</span><span class=\"s\">'scores'</span><span class=\"p\">)</span>\n        <span class=\"n\">num_workers</span> <span class=\"o\">=</span> <span class=\"n\">replicas_to_aggregate</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">])</span>\n\n        <span class=\"n\">server</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span>\n        <span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_queue</span>\n\n        <span class=\"c1\"># Make model\n</span>        <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y_</span><span class=\"p\">,</span> <span class=\"n\">train_step</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Main loop\n</span>        <span class=\"k\">while</span> <span class=\"ow\">not</span> <span class=\"n\">scores</span> <span class=\"ow\">or</span> <span class=\"n\">scores</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1000</span><span class=\"p\">:</span>\n            <span class=\"k\">try</span><span class=\"p\">:</span>\n                <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n            <span class=\"k\">except</span> <span class=\"n\">Empty</span><span class=\"p\">:</span>\n                <span class=\"k\">continue</span>\n\n            <span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n                          <span class=\"n\">y_</span><span class=\"p\">:</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]}</span>\n\n            <span class=\"n\">sess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">train_step</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"p\">],</span> <span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"n\">train_data</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The last three functions defined here, <code class=\"language-plaintext highlighter-rouge\">ps_task</code>, <code class=\"language-plaintext highlighter-rouge\">scorer_task</code> and\n<code class=\"language-plaintext highlighter-rouge\">worker_task</code> are functions that we want to run on each of our three groups of\nTensorFlow server types.  The parameter server task just starts a long-running\ntask and passively joins the TensorFlow network:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">ps_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>The scorer task opens up an <a href=\"http://distributed.readthedocs.io/en/latest/channels.html\">inter-worker\nchannel</a> of\ncommunication named “scores”, creates the TensorFlow model, then every second\nscores the current state of the model against validation data.  It reports the\nscore on the inter-worker channel:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">scoring_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">channel</span><span class=\"p\">(</span><span class=\"s\">'scores'</span><span class=\"p\">)</span>  <span class=\"c1\">#  inter-worker channel\n</span>\n        <span class=\"c1\"># Make Model\n</span>        <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">cross_entropy</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">)</span>\n\n        <span class=\"p\">...</span>\n\n        <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n            <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">cross_entropy</span><span class=\"p\">,</span> <span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"n\">test_data</span><span class=\"p\">)</span>\n            <span class=\"n\">scores</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">score</span><span class=\"p\">))</span>\n            <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>The worker task makes the model, listens on the Dask-TensorFlow Queue for new\ntraining data, and continues training until the last reported score is good\nenough.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">worker_task</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">local_client</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">c</span><span class=\"p\">:</span>\n        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">channel</span><span class=\"p\">(</span><span class=\"s\">'scores'</span><span class=\"p\">)</span>\n\n        <span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_queue</span>\n\n        <span class=\"c1\"># Make model\n</span>        <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y_</span><span class=\"p\">,</span> <span class=\"n\">train_step</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_server</span><span class=\"p\">)</span>\n\n        <span class=\"k\">while</span> <span class=\"n\">scores</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1000</span><span class=\"p\">:</span>\n            <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"n\">get</span><span class=\"p\">()</span>\n\n            <span class=\"n\">train_data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span>\n                          <span class=\"n\">y_</span><span class=\"p\">:</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]}</span>\n\n            <span class=\"n\">sess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">train_step</span><span class=\"p\">,</span> <span class=\"n\">global_step</span><span class=\"p\">],</span> <span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"n\">train_data</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We launch these tasks on the Dask workers that have the corresponding\nTensorFlow servers (see <code class=\"language-plaintext highlighter-rouge\">tf_spec</code> and <code class=\"language-plaintext highlighter-rouge\">dask_spec</code> above):</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">ps_tasks</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">ps_task</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"n\">worker</span><span class=\"p\">)</span>\n            <span class=\"k\">for</span> <span class=\"n\">worker</span> <span class=\"ow\">in</span> <span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'ps'</span><span class=\"p\">]]</span>\n\n<span class=\"n\">worker_tasks</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">worker_task</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"n\">addr</span><span class=\"p\">,</span> <span class=\"n\">pure</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n                <span class=\"k\">for</span> <span class=\"n\">addr</span> <span class=\"ow\">in</span> <span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">]]</span>\n\n<span class=\"n\">scorer_task</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">scoring_task</span><span class=\"p\">,</span> <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'scorer'</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>This starts long-running tasks that just sit there, waiting for external\nstimulation:</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/tf-long-running-task.png\" width=\"70%\" alt=\"long running TensorFlow tasks\" /></p>\n\n<p>Finally we construct a function to dump each of our batches of data\nfrom our Dask.array (from the very beginning of this post) into the\nDask-TensorFlow queues on our workers.  We make sure to only run these tasks\nwhere the Dask-worker has a corresponding TensorFlow training worker:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">distributed.worker_client</span> <span class=\"kn\">import</span> <span class=\"n\">get_worker</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">transfer_dask_to_tensorflow</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):</span>\n    <span class=\"n\">worker</span> <span class=\"o\">=</span> <span class=\"n\">get_worker</span><span class=\"p\">()</span>\n    <span class=\"n\">worker</span><span class=\"p\">.</span><span class=\"n\">tensorflow_queue</span><span class=\"p\">.</span><span class=\"n\">put</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span>\n\n<span class=\"n\">dump</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">transfer_dask_to_tensorflow</span><span class=\"p\">,</span> <span class=\"n\">batches</span><span class=\"p\">,</span>\n             <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">],</span> <span class=\"n\">pure</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If we want to we can track progress in our local session by subscribing to the\nsame inter-worker channel:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"n\">channel</span><span class=\"p\">(</span><span class=\"s\">'scores'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We can use this to repeatedly dump data into the workers over and over again\nuntil they converge.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">while</span> <span class=\"n\">scores</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1000</span><span class=\"p\">:</span>\n    <span class=\"n\">dump</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"p\">.</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">transfer_dask_to_tensorflow</span><span class=\"p\">,</span> <span class=\"n\">batches</span><span class=\"p\">,</span>\n                 <span class=\"n\">workers</span><span class=\"o\">=</span><span class=\"n\">dask_spec</span><span class=\"p\">[</span><span class=\"s\">'worker'</span><span class=\"p\">],</span> <span class=\"n\">pure</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n    <span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">dump</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>We discussed a non-trivial way to use TensorFlow to accomplish distributed\nmachine learning.  We used Dask to support TensorFlow in a few ways:</p>\n\n<ol>\n  <li>Trivially setup the TensorFlow network</li>\n  <li>Prepare and clean data</li>\n  <li>Coordinate progress and stopping criteria</li>\n</ol>\n\n<p>We found it convenient that Dask and TensorFlow could play nicely with each\nother.  Dask supported TensorFlow without getting in the way.  The fact that\nboth libraries play nicely within Python and the greater PyData stack\n(NumPy/Pandas) makes it trivial to move data between them without costly or\ncomplex tricks.</p>\n\n<p>Additionally, we didn’t have to work to integrate these two systems.  There is\nno need for a separate collaborative effort to integrate Dask and TensorFlow at\na core level.  Instead, they are designed in such a way so as to foster this\ntype of interaction without special attention or effort.</p>\n\n<p>This is also the first blogpost that I’ve written that, from a Dask\nperspective, uses some more complex features like <a href=\"http://distributed.readthedocs.io/en/latest/task-launch.html#submit-tasks-from-worker\">long running\ntasks</a>\nor publishing state between workers with\n<a href=\"http://distributed.readthedocs.io/en/latest/channels.html\">channels</a>.  These\nmore advanced features are invaluable when creating more complex/bespoke\nparallel computing systems, such as are often found within companies.</p>\n\n<h2 id=\"what-we-could-have-done-better\">What we could have done better</h2>\n\n<p>From a deep learning perspective this example is both elementary and\nincomplete.  It would have been nice to train on a dataset that was larger and\nmore complex than MNIST.  Also it would be nice to see the effects of training\nover time and the performance of using different numbers of workers.  In\ndefense of this blogpost I can only claim that Dask shouldn’t affect any of\nthese scaling results, because TensorFlow is entirely in control at these\nstages and TensorFlow already has plenty of published scaling information.</p>\n\n<p>Generally speaking though, this experiment was done in a weekend afternoon and\nthe blogpost was written in a few hours shortly afterwards.  If anyone is\ninterested in performing and publishing about a more serious distributed deep\nlearning experiment with TensorFlow and Dask I would be happy to support them\non the Dask side.  I think that there is plenty to learn here about best\npractices.</p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>The following individuals contributed to the construction of this blogpost:</p>\n\n<ul>\n  <li><a href=\"http://stephanhoyer.com/\">Stephan Hoyer</a> contributed with conversations\nabout how TensorFlow is used in practice and with concrete experience on\ndeployment.</li>\n  <li><a href=\"https://github.com/electronwill\">Will Warner</a> and\n<a href=\"https://github.com/eriknw\">Erik Welch</a> both provided valuable editing and\nlanguage recommendations</li>\n</ul>"
}