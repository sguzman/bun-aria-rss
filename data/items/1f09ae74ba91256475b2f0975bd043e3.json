{
  "id": "tag:blogger.com,1999:blog-6141980.post-6618722984616933274",
  "published": "2021-04-06T10:23:00.002-05:00",
  "updated": "2021-04-06T11:08:27.949-05:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "The $1,000 GPT-3",
  "content": "**&nbsp;<a href=\"https://nuit-blanche.blogspot.com/\">Nuit Blanche</a> is now on Twitter: <a href=\"https://twitter.com/NuitBlog\">@NuitBlog</a>&nbsp;**<div><br /></div><div><p class=\"graf graf--p\" name=\"6dac\" style=\"text-align: justify;\">Progress usually comes from a steady technology bootstrap…until it doesn’t.</p><p class=\"graf graf--p\" name=\"df6d\" style=\"text-align: justify;\">Take for instance the race for the $1,000 genome that started in the early 2000s. Initially, <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost\" href=\"https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost\" rel=\"noopener\" target=\"_blank\">sequencing the human genome</a> meant a race between the well-funded public and private sectors but more importantly, the resources for the first breakthrough ended up costing upwards of $450M. Yet despite all the economic promise of genome sequencing, had Moore’s law been applied, sequencing one full genome would still cost $100,000 today. However, once the goal became clearer to everyone, a diversity of technologies and challengers emerged. This intense competition eventually yielded a growth faster than Moore’s Law. The main takeaway is that one cannot rely on the steady progress of one specific technology alone to commoditize tools.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-MislHcqaKKA/YGx4HrYtoxI/AAAAAAAAX5o/zkVtJQ3_51ceDI2SykQYjueRbsARgdbywCLcBGAsYHQ/s1000/cost%2Bof%2Bhuman%2Bgenome.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"562\" data-original-width=\"1000\" height=\"225\" src=\"https://1.bp.blogspot.com/-MislHcqaKKA/YGx4HrYtoxI/AAAAAAAAX5o/zkVtJQ3_51ceDI2SykQYjueRbsARgdbywCLcBGAsYHQ/w400-h225/cost%2Bof%2Bhuman%2Bgenome.png\" width=\"400\" /></a></div><br /><figure class=\"graf graf--figure\" name=\"04a6\"><br /><figcaption class=\"imageCaption\">Figure from NIH <a class=\"markup--anchor markup--figure-anchor\" data-href=\"https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost\" href=\"https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost\" rel=\"noopener\" target=\"_blank\">“Facts sheets about genomics: The cost of Sequencing a Human Genome”</a>, Dec 7th,&nbsp;2020.</figcaption></figure><p class=\"graf graf--p\" name=\"443c\" style=\"text-align: justify;\">What does this have to do with the current state of silicon computing and the new demand for Large Language Models (LLMs)? Everything if you ask us and here is how.</p><p class=\"graf graf--p\" name=\"6cf0\" style=\"text-align: justify;\">Less than a year into existence, Large Language Models like GPT-3 have already <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://openai.com/blog/gpt-3-apps/\" href=\"https://openai.com/blog/gpt-3-apps/\" rel=\"noopener\" target=\"_blank\">spawned a new generation of startups</a> built on the ability of the model to respond to requests for which it was not trained. More importantly for us, hardware manufacturers are positing that <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/\" href=\"https://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/\" rel=\"noopener\" target=\"_blank\">one or several customers will be willing to put a billion dollars</a> on the table to train an even larger model in the coming years.</p><p class=\"graf graf--p\" name=\"e331\" style=\"text-align: justify;\">Interestingly, much like the mass industrialization in the 1930s, the good folks at OpenAI are sketching new <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://arxiv.org/abs/2001.08361\" href=\"https://arxiv.org/abs/2001.08361\" rel=\"noopener\" target=\"_blank\">scaling laws</a> for the industrialization of these larger models.</p><p class=\"graf graf--p\" name=\"51fc\" style=\"text-align: justify;\">The sad truth is that extrapolating their findings to the training of a 10 Trillion parameters model involves a supercomputer <em class=\"markup--em markup--p-em\">running</em> <em class=\"markup--em markup--p-em\">continuously for</em> <em class=\"markup--em markup--p-em\">two decades</em>. The minimum capital expenditure of this adventure is estimated in the realm of several hundreds of million dollars.</p><p class=\"graf graf--p\" name=\"6dab\" style=\"text-align: justify;\">Much like what happened in sequencing, while silicon improvement and architecture may achieve speedups in the following years, it is fair to say that, even with Moore’s law, no foreseeable technology can reasonably train a fully scaled-up GPT-4 and grab the economic value associated with it<strong class=\"markup--strong markup--p-strong\">.</strong></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-5VGxt7bSgJY/YGx4Ar4eWNI/AAAAAAAAX5k/Yhg-RAJwYs0KCpJ_W1xRGGFxvSUDQGuPACLcBGAsYHQ/s1365/lighton%2Bmore%2Bcompute%2Bless%2Bhardware.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"761\" data-original-width=\"1365\" height=\"223\" src=\"https://1.bp.blogspot.com/-5VGxt7bSgJY/YGx4Ar4eWNI/AAAAAAAAX5k/Yhg-RAJwYs0KCpJ_W1xRGGFxvSUDQGuPACLcBGAsYHQ/w400-h223/lighton%2Bmore%2Bcompute%2Bless%2Bhardware.png\" width=\"400\" /></a></div><br /><figure class=\"graf graf--figure\" name=\"b0b0\"><br /></figure><p class=\"graf graf--p\" name=\"202c\" style=\"text-align: justify;\"><strong class=\"markup--strong markup--p-strong\">Rebooting silicon with a different physics, light, and NvNs</strong></p><p class=\"graf graf--p\" name=\"5cb0\" style=\"text-align: justify;\">For a real breakthrough to occur, much like what happened in the sequencing story, different technologies need to be jointly optimized. In our case, this means performing co-design with new hardware and physics but also going rogue on full programmability.</p><p class=\"graf graf--p\" name=\"4ad3\" style=\"text-align: justify;\"><a class=\"markup--anchor markup--p-anchor\" data-href=\"https://lighton.ai/lighton-appliance/\" href=\"https://lighton.ai/lighton-appliance/\" rel=\"noopener\" target=\"_blank\">LightOn’s photonic hardware</a> can produce massively parallel matrix-vector multiplications with an equivalent of 2 trillion parameters “for free”: this is about one-fifth of the number of parameters needed for GPT-4. Next comes revisiting the programmability. Current LightOn’s technology keeps these weights fixed <em class=\"markup--em markup--p-em\">by design</em>. Co-design means finding the algorithms for which CPUs and GPUs can perform some of the most intelligent computations and how LightOn’s massive Non-von Neumann (NvN) hardware can do the heavy lifting. We <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://papers.nips.cc/paper/2020/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf\" href=\"https://papers.nips.cc/paper/2020/file/69d1fc78dbda242c43ad6590368912d4-Paper.pdf\" rel=\"noopener\" target=\"_blank\">already published</a> how we are replacing backpropagation, the workhorse of Deep Learning, with an <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://venturebeat.com/2020/06/03/lighton-researchers-explain-how-they-trained-an-ai-model-on-an-optical-co-processor/\" href=\"https://venturebeat.com/2020/06/03/lighton-researchers-explain-how-they-trained-an-ai-model-on-an-optical-co-processor/\" rel=\"noopener\" target=\"_blank\">algorithm that unleashes</a> the full potential of our hardware in distributed training. We are also working similarly on an inference step that will take full advantage of the massive number of parameters at our disposal. This involved effort relies in a heavy part thanks to our access to ½ million GPU hours on some of <a class=\"markup--anchor markup--p-anchor\" data-href=\"http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html\" href=\"http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html\" rel=\"noopener\" target=\"_blank\">France</a>’s and Europe’s largest supercomputers.</p><p class=\"graf graf--p\" name=\"7537\" style=\"text-align: justify;\">And this is just the beginning. There is a vast untapped potential for repurposing large swaths of optical technologies directed primarily for entertainment and telecommunication into computing.</p><p class=\"graf graf--p\" name=\"b829\" style=\"text-align: justify;\"><strong class=\"markup--strong markup--p-strong\">The road towards a $1,000 GPT-3</strong></p><p class=\"graf graf--p\" name=\"71d0\" style=\"text-align: justify;\">Based on the GPT-3 <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://lambdalabs.com/blog/demystifying-gpt-3/\" href=\"https://lambdalabs.com/blog/demystifying-gpt-3/\" rel=\"noopener\" target=\"_blank\">training cost estimates</a>, achieving a $1,000 GPT-3 requires four orders of magnitude improvements. Much like what occurred in 2007 with the genome sequencing revolution, Moore’s law may take care of the first two orders of magnitude in the coming decade but the next two rely on an outburst of new efficient technologies — hardware <em class=\"markup--em markup--p-em\">and </em>algorithms. It just so happens that GPT-3 has close to 100 layers, so achieving two orders of magnitude savings may arise faster than you can imagine. Stay tuned!</p><p class=\"graf graf--p\" name=\"d1a8\" style=\"text-align: justify;\">Igor Carron is the CEO and co-founder at <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://lighton.ai\" href=\"https://lighton.ai\" rel=\"noopener\" target=\"_blank\">LightOn</a></p></div><div style=\"text-align: justify;\"><br /></div><div>&nbsp; <br />Follow <a href=\"https://twitter.com/NuitBlog\">@NuitBlog</a>&nbsp;or join the <a href=\"http://www.reddit.com/r/CompressiveSensing/\">CompressiveSensing Reddit</a>,&nbsp;the <a href=\"https://www.facebook.com/pages/Nuit-Blanche/166441866740790\">Facebook page</a>, the Compressive Sensing group on&nbsp;<a href=\"http://www.linkedin.com/groups?gid=683737&amp;trk=myg_ugrp_ovr\">LinkedIn</a><a href=\"http://www.linkedin.com/groups?gid=683737&amp;trk=myg_ugrp_ovr\">&nbsp;</a>&nbsp;or&nbsp;the Advanced Matrix Factorization group on&nbsp;<a href=\"http://www.linkedin.com/groups?gid=4084620&amp;trk=myg_ugrp_ovr\">LinkedIn</a><br /><br /><a href=\"http://feeds.feedburner.com/blogspot/wCeDd\" rel=\"alternate\" title=\"Subscribe to my feed\" type=\"application/rss+xml\"></a><a href=\"http://feeds.feedburner.com/blogspot/wCeDd\" rel=\"alternate\" title=\"Subscribe to my feed\" type=\"application/rss+xml\"><img alt=\"\" src=\"http://www.feedburner.com/fb/images/pub/feed-icon32x32.png\" style=\"border: 0px;\" /></a><a href=\"http://feeds.feedburner.com/blogspot/wCeDd\" rel=\"alternate\" title=\"Subscribe to my feed\" type=\"application/rss+xml\">Liked this entry ? subscribe to Nuit Blanche's feed, there's more where that came from</a>.&nbsp;You can also <a href=\"http://feedburner.google.com/fb/a/mailverify?uri=blogspot/wCeDd&amp;loc=en_US\">subscribe to Nuit Blanche by Email</a>.<br /><br />Other links:<br /><b><u><i>Paris Machine Learning</i></u></b>:&nbsp;<a href=\"http://www.meetup.com/Paris-Machine-learning-applications-group/\">Meetup.com</a>||<a href=\"http://nuit-blanche.blogspot.dk/p/paris-based-meetups-on-machine-learning.html\">@Archives</a>||<a href=\"https://www.linkedin.com/groups/6400776/\">LinkedIn</a>||<a href=\"https://www.facebook.com/ParisMachineLearning\">Facebook</a>|| <a href=\"https://twitter.com/ParisMLgroup\">@ParisMLGroup</a> <b><u><i>About&nbsp;<a href=\"http://www.lighton.io/\">LightOn</a></i></u></b>:&nbsp;<a href=\"http://us14.campaign-archive1.com/home/?u=701605c9443ad5e332f87331f&amp;id=85e0ce1094\">Newsletter</a> ||<a href=\"https://twitter.com/LightOnIO\">@LightOnIO</a>|| on <a href=\"https://www.linkedin.com/company/lighton/\">LinkedIn </a>|| on <a href=\"https://www.crunchbase.com/organization/lighton\">CrunchBase</a> || our <a href=\"https://medium.com/@LightOnIO/\">Blog</a><br /><u><i><b>About myself</b></i></u>:&nbsp;<a href=\"http://www.lighton.io/\">LightOn</a> || <a href=\"https://scholar.google.fr/citations?user=Cjrs0lAAAAAJ&amp;hl=fr&amp;oi=sra\">Google Scholar</a> || <a href=\"http://www.linkedin.com/in/igorcarron\">LinkedIn</a> ||<a href=\"http://www.twitter.com/igorcarron\">@IgorCarron</a> ||<a href=\"https://sites.google.com/site/igorcarron2/home\">Homepage</a>||<a href=\"https://arxiv.org/search/?query=igor+carron&amp;searchtype=all\">ArXiv</a></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Igor",
    "uri": "http://www.blogger.com/profile/17474880327699002140",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}