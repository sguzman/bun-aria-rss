{
  "title": "Java Autonomous driving &#8211; Car detection",
  "link": "http://ramok.tech/2018/01/18/java-autonomous-driving-car-detection/",
  "dc:creator": "Klevis Ramo",
  "pubDate": "Thu, 18 Jan 2018 22:54:09 +0000",
  "category": [
    "Convolutional Neural Network",
    "Machine Learning",
    "Neural Networks",
    "autonomous driving",
    "car detection",
    "deep learning",
    "deeplearning4j",
    "java autonomous driving",
    "java deeplearning",
    "java machine learning",
    "neural networks",
    "object detection",
    "real time video object detection",
    "Tiny YOLO",
    "YOLO"
  ],
  "guid": "http://ramok.tech/?p=1669",
  "description": "Are you Java Developer and eager to learn more about Deep Learning and his applications, but you are not feeling like learning another language at the moment ? Are you facing lack of the support or confusion with Machine Learning and Java? Well you are not alone , as a Java Developer with more than &#8230; <a href=\"http://ramok.tech/2018/01/18/java-autonomous-driving-car-detection/\" class=\"more-link\">Continue reading<span class=\"screen-reader-text\"> \"Java Autonomous driving &#8211; Car detection\"</span></a>",
  "content:encoded": "<p style=\"text-align: center;\"><span style=\"font-weight: 400; font-family: helvetica, arial, sans-serif; font-size: 10pt;\">Are you<strong> Java Developer</strong> and eager to learn more about <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/java-machine-learning-computer-vision-video\">Deep Learning and his applications</a>, but you are not feeling like learning another language at the moment ? Are you facing lack of the support or confusion with Machine Learning and Java?</span></p>\n<p style=\"text-align: center;\"><span style=\"font-weight: 400; font-family: helvetica, arial, sans-serif; font-size: 10pt;\">Well you are not alone , as a Java Developer with more than 10 years of experience and several java certification I understand the obstacles and how you feel.</span></p>\n<p style=\"text-align: center;\"><span style=\"font-family: helvetica, arial, sans-serif; font-size: 10pt;\"><span style=\"font-weight: 400;\">From my experience I know what obstacles a Java software engineering faces with the Deep Learning so I can</span><a href=\"https://www.packtpub.com/big-data-and-business-intelligence/java-machine-learning-computer-vision-video\"><span style=\"font-weight: 400;\"> be of a great</span><span style=\"font-weight: 400;\"> help </span></a><span style=\"font-weight: 400;\">to you in making the </span><span style=\"font-weight: 400;\">journey with <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/java-machine-learning-computer-vision-video\">deep learning an exciting experience</a>.</span></span></p>\n<p>In this post we are going to build a <a href=\"https://www.dropbox.com/s/79w21dfrndadkfi/autonomousDriving.zip?dl=0\">Java Real Time Video Object Detection Application</a> for Car Detection, the key component in autonomous driving systems. In <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/\">previous post</a> we were able to build an image classifier(cats&dogs) while now we are going to also detect the object(cars,pedestrians) and additionally mark them with bounding boxes(rectangles).  Feel free to download <a href=\"https://github.com/klevis/AutonomousDriving\">code</a> or run the <a href=\"https://www.dropbox.com/s/79w21dfrndadkfi/autonomousDriving.zip?dl=0\">application</a> with your own videos(<a href=\"http://ramok.tech/wp-content/uploads/2018/01/2018-01-22_15h29_24.mp4\">short live example</a>).</p>\n<h2>Object Detection Nature</h2>\n<h3><strong>Object Classification</strong></h3>\n<p>First we have the problem of <strong>Object Classification</strong> in which we have an image and want to know if this image contains any of particular categories like : <em>image contains a car VS image doesn&#8217;t contain any car</em></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1674\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/car.png?resize=444%2C199\" alt=\"\" width=\"444\" height=\"199\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/car.png?w=444 444w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/car.png?resize=300%2C134 300w\" sizes=\"(max-width: 444px) 85vw, 444px\" data-recalc-dims=\"1\" /></p>\n<p>We saw how to build an image classifier in <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/\">previous post</a>  using existing architecture like <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#VGG_8211_16\">VGG-16</a> and <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#Transfer_Learning\">transfer learning</a>.</p>\n<h3><strong>Object Localization</strong></h3>\n<p>Now that we can say with high confidence if an image has a particular object or not, rises the challenge to <strong>localize the object </strong>position in the image. Usually this is done by marking the object with a rectangle or bounding box.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1677 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/localize.png?resize=197%2C199\" alt=\"\" width=\"197\" height=\"199\" data-recalc-dims=\"1\" /></p>\n<p>Beside classification of the image we need to additionally specify the position of the object in the image.This is done by defining a <strong>bounding box</strong>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>A bounding box is usually represented by the <strong>center( b<sup>x </sup>, b<sup>y</sup> )</strong> ,<strong> rectangle height</strong> ( <strong>b<sup>h</sup></strong> ), <strong>rectangle width</strong>( <strong>b<sup>w</sup></strong> ).</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\" wp-image-1749 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h21_57.jpg?resize=231%2C187\" alt=\"\" width=\"231\" height=\"187\" data-recalc-dims=\"1\" /></p>\n<p>Now we need to define this four variables in our training data per each of the objects in the images. Than the network will output not only the probability for the image class number(20% cat[1], 70%  dog[2], 10% tiger[3]) but also the four variables defining the bounding box of the objects.</p>\n<p>Notice that just by providing the bounding boxes points(<em>center,width,height</em>) now our model is able to predict more information by giving us a more detailed view of the content.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\" wp-image-1686 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/face.png?resize=212%2C239\" alt=\"\" width=\"212\" height=\"239\" data-recalc-dims=\"1\" />It will not be hard to imagine that adding more points in the image can provide us even greater insight into the image. Like e.g adding more points position in the human face (mouth, eyes)can tell us if the person is smiling, crying , angry or happy.</p>\n<p>&nbsp;</p>\n<h3><strong>Object Detection</strong></h3>\n<p>We can go even a step further by localizing not only one object but rather multiple objects in the image and this will lead us to the <strong>Object Detection Problem. </strong></p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1678\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/multicars.jpg?resize=480%2C360\" alt=\"\" width=\"480\" height=\"360\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/multicars.jpg?w=480 480w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/multicars.jpg?resize=300%2C225 300w\" sizes=\"(max-width: 480px) 85vw, 480px\" data-recalc-dims=\"1\" /></p>\n<p>Although the structure does not change much the problem here becomes a harder because we need more data preparation(multiple bounding boxes per image).</p>\n<p>In principle this problem is solved by dividing the image into smaller rectangles and per each rectangle we have the same additional five variables we already saw <b>P</b><sup style=\"font-weight: bold;\">c , </sup><strong>( b<sup>x </sup>, b<sup>y</sup> )</strong> ,   <strong>b<sup>h</sup></strong> , <strong>b<sup>w</sup></strong> and of course the normal prediction probabilities(20% cat[1], 70%  dog[2]). We will see more details on this later but for now lets say the structure is the same as Object Localization with the difference we have multiple of that structure.</p>\n<h2>Sliding Window Solution</h2>\n<p>This is a quite intuitive solution that one can come up by his/her self. The idea is that instead of using general car images we crop as much as possible so the image contains only the car.</p>\n<p>Than we train a Convolution Neural Network similar to <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#VGG_8211_16\">VGG-16</a> or any <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#Other_Great_Architectures\">other deep network</a> with the cropped images.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1692\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/cropimage.png?resize=840%2C420\" alt=\"\" width=\"840\" height=\"420\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/cropimage.png?w=1200 1200w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/cropimage.png?resize=300%2C150 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/cropimage.png?resize=768%2C384 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/cropimage.png?resize=1024%2C512 1024w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px\" data-recalc-dims=\"1\" /> <img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1691 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/croppedcar.png?resize=155%2C100\" alt=\"\" width=\"155\" height=\"100\" data-recalc-dims=\"1\" /></p>\n<p>This works quite well with the exception than now the model was trained to detect a images that have only cars so it will have troubles with real images since they contain more than just a car(trees , people, sings&#8230;).</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1696 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?resize=322%2C177\" alt=\"\" width=\"322\" height=\"177\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?w=1532 1532w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?resize=300%2C165 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?resize=768%2C422 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?resize=1024%2C563 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/16116_image.jpg?resize=1200%2C660 1200w\" sizes=\"(max-width: 322px) 85vw, 322px\" data-recalc-dims=\"1\" /></p>\n<p>The size of real images is way bigger than the cropped images and can contain many cars as well.</p>\n<p>To overcome those problems we can analyse only a small part of the image and try to figure out if that part has a car or not. More precisely we can scan the image with a sliding rectangle window and each time let our model to predict if there is a car in it or not. Lets see an example :</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1698\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_14h50_00.gif?resize=840%2C507\" alt=\"\" width=\"840\" height=\"507\" data-recalc-dims=\"1\" /></p>\n<p>To summarize we use a normal Convolution Neural Network( <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#VGG_8211_16\">VGG-16</a> ) to train the model with crop images of different sizes and than use this rectangle sizes to scan the images for the objects(cars). As we can see by using different sizes of rectangles we can figure out quite different shape of cars and their positions.</p>\n<p>This algorithm is not that complex and it works. But anyway it has <strong>two downsides</strong>:</p>\n<ol>\n<li>First is the low performance. We need to ask the model for a prediction a lot of times. Each time a rectangle moves we need to execute the model in order to get a prediction.<br />\nAnd this is not all, additionally we need to do it all over again for different rectangles size as well. Maybe one way to address the performance is to increase stride of rectangles(bigger steps) but than we may fail to detect some of the objects.<br />\nIn past the models used to be mostly linear and have features design/mixed by hand so the prediction was not that expensive therefore this algorithm used to do just fine. With nowadays network sizes(138 million parameters for VGG-16) this algorithm is slow and it almost not useful for real time video object detection like autonomous driving.<br />\n<img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1703 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_15h43_51.jpg?resize=305%2C291\" alt=\"\" width=\"305\" height=\"291\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_15h43_51.jpg?w=305 305w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_15h43_51.jpg?resize=300%2C286 300w\" sizes=\"(max-width: 305px) 85vw, 305px\" data-recalc-dims=\"1\" />Another way to see why the algorithm is not very efficient is to notice how when moving the rectangle(right and down) a lot of shared pixel are not being reused(pixels when colors get mixed) but they get just re-calculated all over again. <em><span style=\"text-decoration: underline;\">Next section we will see a state of the art technique overcoming this problem by using convolution</span></em>.</li>\n</ol>\n<p><em>2.<span style=\"text-decoration: underline;\"><img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1745 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h09_06.jpg?resize=254%2C289\" alt=\"\" width=\"254\" height=\"289\" data-recalc-dims=\"1\" /></span></em>Even with different bounding box sizes we may fail to precisely mark the object with a bounding box. The model may not output very accurate bounding boxes like the box may include only a part of the object.<em><span style=\"text-decoration: underline;\">Next sections will explore YOLO(You Look Only Once) algorithm which solves this problem for us.</span></em></p>\n<h2>Convolutional Sliding Window Solution</h2>\n<p>We saw how Sliding Window had performance problems due to the fact that it didn&#8217;t reuse many of already computed values. Each time the window was moved we had to execute all the model(million of parameters) for all pixels in order to get a prediction. In reality most of the computation there could be reused by introducing Convolution.</p>\n<h3>Turn Fully Connected Layers Into Convolution</h3>\n<p>We saw in previous post that <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#Well_Known_Architectures\">image classification architectures</a>  regardless with their size and configuration in the end they used to feed a fully connected  <a href=\"http://ramok.tech/2017/11/29/digit-recognizer-with-neural-networks/#Multiple_NeuronsNetwork\">Neural Network</a> with different number of layers and output several prediction depending on classes.</p>\n<p>For simplicity we will take a smaller network model as example but anyway the same logic is valid of any convolutional network(<a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#VGG_8211_16\">VGG-16</a>, <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#AlexNet\">AlexNet</a>).</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1709\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?resize=840%2C229\" alt=\"\" width=\"840\" height=\"229\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?w=1331 1331w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?resize=300%2C82 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?resize=768%2C209 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?resize=1024%2C279 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-13_23h48_54.jpg?resize=1200%2C327 1200w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px\" data-recalc-dims=\"1\" /></p>\n<p>For a more detailed explanation of convolution and his intuition please have a look at one of <a href=\"http://ramok.tech/2017/12/13/java-digit-recognizer-with-convolutional-neural-networks/#Edge_Detection\">my previous posts</a>. This simple network takes as input a colored image(RGB) of size 32 X 32 X 3.  It uses a Same Convolution(<em>this convolution leaves first two dimensions <strong>width</strong> X <strong>height</strong> unchanged</em>) 3 X 3 X <span style=\"text-decoration: underline;\"><strong>64</strong></span> to get an output 32 X 32 X <span style=\"text-decoration: underline;\"><strong>64</strong></span>(<em>notice how the third dimension is same as convolution matrix <strong>64</strong>, usually increased from input</em>). It uses a Max Pooling layer to reduce width and height and leaving the third dimension unchanged <span style=\"text-decoration: underline;\"><strong>16 X 16 </strong></span>X 64. After that we feed a Fully Connected Neural Network with two hidden layers 256 and 128 neurons each. In the end we output probabilities(usually using soft-max) for 5 classes.</p>\n<p>Now lets see how we can replace Fully Connected Layers to Convolution Layers while leaving the mathematical effect the same(linear function of the input 16 X 16 X 64).</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1733\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?resize=840%2C223\" alt=\"\" width=\"840\" height=\"223\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?w=1309 1309w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?resize=300%2C80 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?resize=768%2C204 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?resize=1024%2C271 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_14h54_40.jpg?resize=1200%2C318 1200w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px\" data-recalc-dims=\"1\" /></p>\n<p>So what we did is just replace Fully Connected layers with Convolution Filters. In reality 16 X 16 X 256 convolution filter is <strong>16 X 16 X <span style=\"text-decoration: underline;\">64</span> X 256 matrix </strong>(<a href=\"http://ramok.tech/2017/12/13/java-digit-recognizer-with-convolutional-neural-networks/#Convolution_on_RGB_ImagesExamples\"> why?</a> <a href=\"http://ramok.tech/2017/12/13/java-digit-recognizer-with-convolutional-neural-networks/#Multiple_FIlters\">multiple filters</a>) .Third dimension <strong>64</strong> is always same as the input third dimension <strong>16 X 16 X <span style=\"text-decoration: underline;\">64</span></strong> sofor the sake of simplicity it is referred as <b>16 x 16 x 256 </b>by omitting the <strong>64</strong><b>. </b>This means that actually this is equivalent to Fully Connected Layer because :<br />\n<em><strong>out:1 X 1 X <span style=\"color: #99cc00;\">256</span> = in:[<span style=\"color: #ff0000;\">16 X 16 X 64<span style=\"color: #000000;\">]</span></span> * conv:[<span style=\"color: #ff0000;\">16 X 16 X 64</span> X <span style=\"color: #99cc00;\">256<span style=\"color: #000000;\">]</span></span></strong></em><br />\nso every element of the output <span style=\"text-decoration: underline;\">1 X 1 X 256</span> is a linear function of every element of the input <span style=\"text-decoration: underline;\">16 X 16 X 64</span>.</p>\n<p>The reason why we bothered to convert Fully Connected(FC) Layers to Convolution Layers is because this will give us more flexibility in the way output is reproduced. With FC you will always will have the same output size which is number of classes.</p>\n<h3>Convolution Sliding Window</h3>\n<p>To see the great the idea behind replacing FC with convolution filter we will need to take in input image that is bigger than original one of 32 X 32 X 3. So lets take an input image of 36 X 36 X 3:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\" wp-image-1722 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_01h36_41.jpg?resize=237%2C231\" alt=\"\" width=\"237\" height=\"231\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_01h36_41.jpg?w=332 332w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_01h36_41.jpg?resize=300%2C293 300w\" sizes=\"(max-width: 237px) 85vw, 237px\" data-recalc-dims=\"1\" /></p>\n<p style=\"text-align: left;\">So this image has 4 columns and 4 rows(with green 36 X 36) more than original image(blue 32 X 32).If we would use sliding window with Stride=2 and FC than we need to move the original image size      <span style=\"text-decoration: underline;\"><strong>9 times</strong></span> to cover all (e.x 3 moves shown with black rectangle) therefore<strong> execute model <span style=\"text-decoration: underline;\">9 times</span></strong> as well.</p>\n<p>&nbsp;</p>\n<p>Lets try now to apply this new bigger matrix as input to our new model with Convolution Layers only.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-1738 zoooom\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?resize=604%2C172\" alt=\"\" width=\"604\" height=\"172\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?w=1302 1302w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?resize=300%2C85 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?resize=768%2C218 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?resize=1024%2C291 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_15h48_49.jpg?resize=1200%2C341 1200w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px\" data-recalc-dims=\"1\" /></p>\n<p>Now as we can see the output changed from <strong>1 X 1 X 5</strong> to <strong>3 X 3 X 5</strong> in comparison to FC where the output will always be <strong>1 X 1 X 5</strong>. Recalling in the case of Sliding Window we had to move sliding window  <strong>9 times to cover all image</strong> <b>, wait isn&#8217;t that equal to 3X3 output of convolution? </b>Yes indeed those <strong>3X3</strong> cells each represent the Sliding Windows <strong>1 X 1 X 5 classes </strong>probability prediction results! So instead of having only one 1 X 1 X 5 as output now with one shot we have 3 X 3 X 5 so all 9 combinations without needed to move and execute 9 times the sliding window.</p>\n<p>This is really sate of the art technique as we were able just in one pass to get all 9 results immediately without needed to execute model with millions of parameters several times.</p>\n<h2><a href=\"https://pjreddie.com/darknet/yolo/\">YOLO(You Look Only Once)</a></h2>\n<p><em><span style=\"text-decoration: underline;\"><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1745 size-thumbnail alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h09_06.jpg?resize=150%2C150\" alt=\"\" width=\"150\" height=\"150\" data-recalc-dims=\"1\" /></span></em>Although we already addressed the performance problem by introducing convolution sliding window our model still may not output very accurate bounding boxes even with several bounding boxes sizes. Lets see how YOLO solves that problem as well.</p>\n<p>First we normally go on each image and mark the objects we want to detect. Each object is marked by a bounding box with four variables <strong>center of the object( b<sup>x </sup>, b<sup>y</sup> )</strong> ,<strong> rectangle height</strong> ( <strong>b<sup>h</sup></strong> ), <strong>rectangle width</strong>( <strong>b<sup>w</sup></strong> ). After that each image is split into smaller number of rectangles(boxes) , usually 13 X 13 rectangles but here for simplicity is 8 X 9.<img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-1753 zoooom\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h38_45-1.jpg?resize=551%2C392\" alt=\"\" width=\"551\" height=\"392\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h38_45-1.jpg?w=551 551w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_20h38_45-1.jpg?resize=300%2C213 300w\" sizes=\"(max-width: 551px) 85vw, 551px\" data-recalc-dims=\"1\" /></p>\n<p>The bounding box(red) and the object can be part of several boxes(blue) so we assign the object and the bounding box only to the box owning the center of the object(yellow boxes). So we train our model with four additional(beside telling the object is a car)variables (<strong>b<sup>x </sup>, b<sup>y</sup> ,b<sup>h</sup> ,b<sup>w</sup> </strong>) and assign those to the box owning the center <strong>b<sup>x </sup>, b<sup>y</sup> .</strong> Since the neural network is trained with this labeled data it also predicts this four variables(beside what object is) values or bounding boxes.</p>\n<p>Instead of scanning with predefined bounding box sizes and trying to fit the object we let the model learn how to mark objects with bounding boxes therefore the bounding boxes now are flexible(are learned) . This way the accuracy of the bounding boxes is much higher and flexible.</p>\n<p>Lets see how we can represent the output now that we have additional 4 variables(<strong>b<sup>x </sup>, b<sup>y</sup> ,b<sup>h</sup> ,b<sup>w</sup> </strong>) beside classes like 1-car,2-pedestrian&#8230; In reality there is added also another variable <strong>P<sup>c </sup></strong>which simply tells if  the image has any of the objects we want to detect at all. <em><img decoding=\"async\" loading=\"lazy\" class=\"wp-image-1682 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-12_01h30_27.jpg?resize=421%2C374\" alt=\"\" width=\"421\" height=\"374\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-12_01h30_27.jpg?w=723 723w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-12_01h30_27.jpg?resize=300%2C267 300w\" sizes=\"(max-width: 421px) 85vw, 421px\" data-recalc-dims=\"1\" /></em></p>\n<ul>\n<li><b>P<sup style=\"font-weight: bold;\">c </sup>=1(red) </b><em>means there is at least one of the objects so is worth to look at probabilities and bounding box. </em></li>\n<li><b>P</b><sup style=\"font-weight: bold;\">c </sup><b>=0(red) </b><em>image has none of the objects we want so we do not care about probabilities or bounding box </em></li>\n</ul>\n<p>&nbsp;</p>\n<h3>Bounding Box Specification</h3>\n<p>We need to label our training data in some specific way so the <a href=\"https://pjreddie.com/darknet/yolo/\">YOLO</a> algorithm will work correctly. YOLO V2 format requires bounding box dimensions <strong>b<sup>x</sup> ,b<sup>y</sup><sup> </sup></strong>and  <strong>b<sup>h</sup> ,b<sup>w </sup></strong>to be relative to original image width and height. Lets suppose we have an image <strong>300 X 400</strong> and the bounding box dimension are B<sup>width</sup> =30 , B<sup>height</sup>=15 , B<sup>x</sup>=150 ,B<sup>y</sup>=80. This has to be transformed to:<br />\nB<sup>w</sup> =30/300 , B<sup>h</sup>=15/400 , B<sup>x</sup>=150/300 ,B<sup>y</sup>=80/400</p>\n<p>This <a href=\"https://timebutt.github.io/static/how-to-train-yolov2-to-detect-custom-objects/\">post</a> shows how to label data using  <a href=\"https://github.com/puzzledqs/BBox-Label-Tool\">BBox Label Tool</a> with less pain. The tool labels bounding boxes a bit different(gives up left point and lower right point) from YOLO V2 format but converting is fairly straight forward task.</p>\n<p>Regardless how YOLO requires the labeling of training data , internally the prediction is done  a bit differently.</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1765 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h57_52.jpg?resize=384%2C303\" alt=\"\" width=\"384\" height=\"303\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h57_52.jpg?w=384 384w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h57_52.jpg?resize=300%2C237 300w\" sizes=\"(max-width: 384px) 85vw, 384px\" data-recalc-dims=\"1\" />A predicted bounding box by YOLO is defined relatively to the box that owns the center of the object(yellow). The upper left corner of the box start from (0,0) and the bottom right (1,1). So the center (<strong>b<sup>x </sup>, b<sup>y)</sup> </strong>is for sure on range 0-1(sigmoid function makes sure) since the point is inside box. While <strong>b<sup>h</sup> ,b<sup>w</sup> </strong>are calculated in proportion to <span style=\"color: #000000;\"><strong>w</strong> and <strong>h </strong></span>values(yellow) of the box so values can be greater than 1(exponential used for positive values)<strong>. </strong>In the picture we can see that the width <strong>b<sup>w</sup></strong> of the bounding box is almost <strong>1.8 the size</strong> of the box width <strong>w.</strong> Similarly<strong> b<sup>h</sup></strong> is approx <strong>1.6 the size</strong> of box height <strong>h</strong>.</p>\n<p>After prediction we see how much the predicted box intersects with the real bounding box labeled at the beginning. Basically we try to maximize the intersection between them so ideally the predicted bounding box is fully intersecting to labeled bounding box.</p>\n<p>In principle that&#8217;s it! You provide more specialized labeled data with bounding boxes(<strong>b<sup>x </sup>, b<sup>y</sup> ,b<sup>h</sup> ,b<sup>w</sup> </strong>) , split the image and assign to the box containing the center(the only responsible for detecting the object) , train with &#8216;Convolution Sliding Window Network&#8217; and predict the object and his position.</p>\n<h3>Two More Problems</h3>\n<p>Although we are not going to give a very detailed explenation in this post in reality there are yet two more small problems to solve:</p>\n<ol>\n<li><img decoding=\"async\" loading=\"lazy\" class=\" wp-image-1760 alignleft zoooom\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h29_41.jpg?resize=365%2C258\" alt=\"\" width=\"365\" height=\"258\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h29_41.jpg?w=619 619w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h29_41.jpg?resize=300%2C212 300w\" sizes=\"(max-width: 365px) 85vw, 365px\" data-recalc-dims=\"1\" />Even if in the training time the object is assign to one box(the one containing the object center) at test time (when predicting) it may happen several boxes(yellow) think they have the center of the object(with red) therefore defining their own bounding boxes for same object. This is solve by <strong>Non-max Suppression algorithm</strong>. Currently <a href=\"http://deeplearning4j.org\">deeplearning4j</a> does not provide an implementation so please find at <a href=\"https://github.com/klevis/AutonomousDriving/blob/master/src/main/java/ramo/klevis/TinyYoloPrediction.java\">github</a> a simple implementation(<em><strong>removeObjectsIntersectingWithMax</strong></em>).   What it does is first choose as prediction the box with maximum P<sup>c </sup>probability(yes it has not only 1 or 0 values but rather in the 0-1 range). Than every box that intersects above certain threshold with that box is removed. It starts again the same logic until there no more bounding boxes left.</li>\n<li><img decoding=\"async\" loading=\"lazy\" class=\"size-full wp-image-1762 alignleft\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h37_45.jpg?resize=302%2C197\" alt=\"\" width=\"302\" height=\"197\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h37_45.jpg?w=302 302w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-14_23h37_45.jpg?resize=300%2C196 300w\" sizes=\"(max-width: 302px) 85vw, 302px\" data-recalc-dims=\"1\" />Since we are predicting multiple objects (car , pedestrians, traffic lights ) it may happen the center of two or more objects is one box. This cases are solved by introducing <strong>Anchor Boxes. </strong>With anchor boxes we choose several shapes of bounding boxes we find more used for the object we want to detect. YOLO V2 paper is doing this with <a href=\"http://ramok.tech/2017/09/18/kmeans-image-color-reduction-with-java-and-spark/\">K-Means algorithm</a> but it can be done also manually. After that we modify the output to contain the same structure we saw previously(<strong>P<sup>c</sup></strong>,<strong>b<sup>x </sup>, b<sup>y</sup> ,b<sup>h</sup> ,b<sup>w, </sup>C1,C2&#8230;</strong>) but for each of chosen Anchor Boxes shapes . So we may have something like this now:<img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-1785 zoooom\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?resize=840%2C499\" alt=\"\" width=\"840\" height=\"499\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?w=1342 1342w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?resize=300%2C178 300w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?resize=768%2C456 768w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?resize=1024%2C608 1024w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-16_23h46_56.jpg?resize=1200%2C713 1200w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px\" data-recalc-dims=\"1\" /></li>\n</ol>\n<h2>Application</h2>\n<p>Training deep networks takes a lot of effort and requires significat data and processing power. So as we did in <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/\">previous post</a>  we will use <a href=\"http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/#Transfer_Learning\">transfer learning</a>. This time we are not going to modify the architecture and train with different data but rather use the network directly.</p>\n<p>We are going to use<a href=\"https://pjreddie.com/darknet/yolo/\"> Tiny YOLO</a> ,citing from site:</p>\n<blockquote><p>Tiny YOLO is based off of the <a href=\"https://pjreddie.com/darknet/imagenet/#reference\">Darknet reference network</a> and is much faster but less accurate than the normal YOLO model. To use the version trained on VOC:</p>\n<pre><code>wget https://pjreddie.com/media/files/tiny-yolo-voc.weights\n./darknet detector test cfg/voc.data cfg/tiny-yolo-voc.cfg tiny-yolo-voc.weights data/dog.jpg\n</code></pre>\n<p>Which, ok, it&#8217;s not perfect, but boy it sure is fast. On GPU it runs at >200 FPS.</p></blockquote>\n<p>Current release version of <a href=\"http://deeplearning4j.org\">deeplearning4j</a> 0.9.1 does not offer TinyYOLO but the 0.9.2-SNAPSHOT it does. So first we need to tell maven to load SNAPSHOT:</p>\n<pre><repositories>\n    <repository>\n        <id>a</id>\n        <url>http://repo1.maven.org/maven2/</url>\n    </repository>\n    <repository>\n        <id>snapshots-repo</id>\n        <url>https://oss.sonatype.org/content/repositories/snapshots</url>\n        <releases>\n            <enabled>false</enabled>\n        </releases>\n        <snapshots>\n            <enabled>true</enabled>\n            <updatePolicy>daily</updatePolicy>\n        </snapshots>\n    </repository>\n</repositories>\n\n<dependencies>\n\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>deeplearning4j-core</artifactId>\n        <version>${deeplearning4j}</version>\n    </dependency>\n\n    <dependency>\n        <groupId>org.nd4j</groupId>\n        <artifactId>nd4j-native-platform</artifactId>\n        <version>${deeplearning4j}</version>\n    </dependency></pre>\n<p>Than we are ready to load the model with fairly short code:</p>\n<pre>private TinyYoloPrediction() {\n    try {\n        preTrained = (ComputationGraph) new TinyYOLO().initPretrained();\n        prepareLabels();\n    } catch (IOException e) {\n        throw new RuntimeException(e);\n    }\n}</pre>\n<p><strong>prepareLabels</strong> is just using labels from the dataset <a href=\"https://github.com/allanzelener/YAD2K/blob/master/model_data/pascal_classes.txt\">PASCAL VOC</a> that used to train the model. Feel free to run <strong><em>preTrained .summary()</em></strong> to see model architecture details.</p>\n<p>The video frames are captures using <a href=\"https://github.com/bytedeco/javacv\">JavaCV</a> with <a href=\"https://github.com/klevis/AutonomousDriving/blob/master/src/main/java/ramo/klevis/CarVideoDetection.java\">CarVideoDetection</a>:</p>\n<pre>FFmpegFrameGrabber grabber;\ngrabber = new FFmpegFrameGrabber(f);\ngrabber.start();\nwhile (!stop) {\n    videoFrame[0] = grabber.grab();\n    if (videoFrame[0] == null) {\n        stop();\n        break;\n    }\n    v[0] = new OpenCVFrameConverter.ToMat().convert(videoFrame[0]);\n    if (v[0] == null) {\n        continue;\n    }\n    if (winname == null) {\n        winname = AUTONOMOUS_DRIVING_RAMOK_TECH + ThreadLocalRandom.current().nextInt();\n    }\n\n    if (thread == null) {\n        thread = new Thread(() -> {\n            while (videoFrame[0] != null && !stop) {\n                try {\n                    TinyYoloPrediction.getINSTANCE().markWithBoundingBox(v[0], videoFrame[0].imageWidth, videoFrame[0].imageHeight, true, winname);\n                } catch (java.lang.Exception e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        });\n        thread.start();\n    }\n\n    TinyYoloPrediction.getINSTANCE().markWithBoundingBox(v[0], videoFrame[0].imageWidth, videoFrame[0].imageHeight, false, winname);\n\n    imshow(winname, v[0]);</pre>\n<p>So what the code is doing is getting frames from the video and passing to the TinyYOLO pre trained model. From there image frame is first scaled to 416X416X3(RGB) and than is given to TinyYOLO for predicting and marking the bounding boxes:</p>\n<pre>public void markWithBoundingBox(Mat file, int imageWidth, int imageHeight, boolean newBoundingBOx,String winName) throws Exception {\n    int width = 416;\n    int height = 416;\n    int gridWidth = 13;\n    int gridHeight = 13;\n    double detectionThreshold = 0.5;\n\n    Yolo2OutputLayer outputLayer = (Yolo2OutputLayer) preTrained.getOutputLayer(0);\n   \n        INDArray indArray = prepareImage(file, width, height);\n        INDArray results = preTrained.outputSingle(indArray);\n        predictedObjects = outputLayer.getPredictedObjects(results, detectionThreshold);\n        System.out.println(\"results = \" + predictedObjects);\n        markWithBoundingBox(file, gridWidth, gridHeight, imageWidth, imageHeight);\n    imshow(winName, file);\n}</pre>\n<p>After the prediction we should have ready the predicted values of bounding box dimensions.We have implemented also Non-Max Suppression (<a href=\"https://github.com/klevis/AutonomousDriving/blob/master/src/main/java/ramo/klevis/TinyYoloPrediction.java\"><strong>removeObjectsIntersectingWithMax</strong></a>) because as we mention YOLO at testing time predicts more than one bounding boxes per object. Rather than using <strong>b<sup>x </sup>, b<sup>y</sup> ,b<sup>h</sup> ,b<sup>w </sup></strong> will use <strong>topLeft</strong> and <b>bottomRight </b>points. <strong>gridWidth and gridHeight</strong> are the number of the small boxes we split our image into, which in our case is 13X13. <strong>w,h </strong>are original image frame dimensions.</p>\n<pre>private void markWithBoundingBox(Mat file, int gridWidth, int gridHeight, int w, int h, DetectedObject obj) {\n\n    double[] xy1 = obj.getTopLeftXY();\n    double[] xy2 = obj.getBottomRightXY();\n    int predictedClass = obj.getPredictedClass();\n    int x1 = (int) Math.round(w * xy1[0] / gridWidth);\n    int y1 = (int) Math.round(h * xy1[1] / gridHeight);\n    int x2 = (int) Math.round(w * xy2[0] / gridWidth);\n    int y2 = (int) Math.round(h * xy2[1] / gridHeight);\n    rectangle(file, new Point(x1, y1), new Point(x2, y2), Scalar.RED);\n    putText(file, map.get(predictedClass), new Point(x1 + 2, y2 - 2), FONT_HERSHEY_DUPLEX, 1, Scalar.GREEN);\n}</pre>\n<p>After that using another thread(beside the one playing the video) we update the video to have the rectangle and the label for the object it was detected.</p>\n<p>The prediction is really fast(real-time) considering that is run on CPU, on GPUs we will have even better real time detection.</p>\n<h3>Running Application</h3>\n<p><a href=\"https://www.dropbox.com/s/79w21dfrndadkfi/autonomousDriving.zip?dl=0\">Application </a>can be <a href=\"https://www.dropbox.com/s/79w21dfrndadkfi/autonomousDriving.zip?dl=0\">downloaded </a>and executed without any knowledge of java beside JAVA has to be installed on your computer. Feel to try with your own videos.</p>\n<p>It is possible to run the from <a href=\"https://github.com/klevis/AutonomousDriving\">source </a>by simply executing the <strong>RUN</strong> class or if you do not fill to open it with IDE just run <em><strong>mvn clean install exec:java.</strong></em></p>\n<p>After running the <a href=\"https://www.dropbox.com/s/79w21dfrndadkfi/autonomousDriving.zip?dl=0\">application</a> you should be able to see below view:</p>\n<p><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter size-full wp-image-1794\" src=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-18_23h37_56.jpg?resize=646%2C502\" alt=\"\" width=\"646\" height=\"502\" srcset=\"https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-18_23h37_56.jpg?w=646 646w, https://i0.wp.com/ramok.tech/wp-content/uploads/2018/01/2018-01-18_23h37_56.jpg?resize=300%2C233 300w\" sizes=\"(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px\" data-recalc-dims=\"1\" /></p>\n<p>Enjoy :)!</p>\n",
  "enclosure": "",
  "post-id": 1669
}