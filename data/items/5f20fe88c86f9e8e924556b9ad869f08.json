{
  "title": "What I Wish Someone Had Told Me About Tensor Computation Libraries",
  "link": "",
  "id": "https://www.georgeho.org/tensor-computation-libraries/",
  "updated": "2020-12-15T00:00:00Z",
  "published": "2020-12-15T00:00:00Z",
  "content": "<p>I get confused with tensor computation libraries (or computational graph libraries, or symbolic\nalgebra libraries, or whatever they&rsquo;re marketing themselves as these days).</p>\n<p>I was first introduced to PyTorch and TensorFlow and, having no other reference, thought they were\nprototypical examples of tensor computation libraries. Then I learnt about Theano &mdash; an older and\nless popular project, but different from PyTorch and TensorFlow and better in some meaningful ways.\nThis was followed by JAX, which seemed to be basically NumPy with more bells and whistles (although\nI couldn&rsquo;t articulate what exactly they were). Then came <a href=\"https://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b\">the announcement by the PyMC developers\nthat Theano would have a new JAX\nbackend</a>.</p>\n<p>Anyways, this confusion prompted a lot of research and eventually, this blog post.</p>\n<p>Similar to <a href=\"https://www.georgeho.org/prob-prog-frameworks/\">my previous post on the anatomy of probabilistic programming\nframeworks</a>, I’ll first discuss tensor computation\nlibraries in general &mdash; what they are and how they can differ from one another. Then I&rsquo;ll discuss\nsome libraries in detail, and finally offer an observation on the future of Theano in the context of\ncontemporary tensor computation libraries.</p>\n<div>\n<h2>Contents</h2>\n<nav id=\"TableOfContents\">\n<ul>\n<li><a href=\"#dissecting-tensor-computation-libraries\">Dissecting Tensor Computation Libraries</a>\n<ul>\n<li><a href=\"#tensor-computation-library-----maybe-not-the-best-name\">&ldquo;Tensor Computation Library&rdquo; &mdash; Maybe Not The Best Name</a></li>\n<li><a href=\"#some-differences-between-tensor-computation-libraries\">(Some) Differences Between Tensor Computation Libraries</a></li>\n</ul>\n</li>\n<li><a href=\"#a-zoo-of-tensor-computation-libraries\">A Zoo of Tensor Computation Libraries</a>\n<ul>\n<li><a href=\"#pytorchhttpspytorchorg\"><a href=\"https://pytorch.org/\">PyTorch</a></a></li>\n<li><a href=\"#jaxhttpsjaxreadthedocsioenlatest\"><a href=\"https://jax.readthedocs.io/en/latest/\">JAX</a></a></li>\n<li><a href=\"#theanohttpstheano-pymcreadthedocsioenlatest\"><a href=\"https://theano-pymc.readthedocs.io/en/latest/\">Theano</a></a></li>\n</ul>\n</li>\n<li><a href=\"#an-observation-on-static-graphs-and-theano\">An Observation on Static Graphs and Theano</a></li>\n<li><a href=\"#some-follow-ups-a-week-later\">Some Follow-Ups, A Week Later</a></li>\n</ul>\n</nav>\n</div>\n<h2 id=\"dissecting-tensor-computation-libraries\">Dissecting Tensor Computation Libraries</h2>\n<p>First, a characterization: what do tensor computation libraries even do?</p>\n<ol>\n<li>They provide ways of specifying and building computational graphs,</li>\n<li>They run the computation itself (duh), but also run &ldquo;related&rdquo; computations that either (a) <em>use\nthe computational graph</em>, or (b) operate <em>directly on the computational graph itself</em>,\n<ul>\n<li>The most salient example of the former is computing gradients via\n<a href=\"https://arxiv.org/abs/1502.05767\">autodifferentiation</a>,</li>\n<li>A good example of the latter is optimizing the computation itself: think symbolic\nsimplifications (e.g. <code>xy/x = y</code>) or modifications for numerical stability (e.g. <a href=\"https://cs.stackexchange.com/q/68411\"><code>log(1 + x)</code>\nfor small values of <code>x</code></a>).</li>\n</ul>\n</li>\n<li>And they provide &ldquo;best execution&rdquo; for the computation: whether it&rsquo;s changing the execution by JIT\n(just-in-time) compiling it, by utilizing special hardware (GPUs/TPUs), by vectorizing the\ncomputation, or in any other way.</li>\n</ol>\n<h3 id=\"tensor-computation-library-----maybe-not-the-best-name\">&ldquo;Tensor Computation Library&rdquo; &mdash; Maybe Not The Best Name</h3>\n<p>As an aside: I realize that the name &ldquo;tensor computation library&rdquo; is too broad, and that the\ncharacterization above precludes some libraries that might also justifiably be called &ldquo;tensor\ncomputation libraries&rdquo;. Better names might be &ldquo;graph computation library&rdquo; (although that might get\nmixed up with libraries like <a href=\"https://networkx.org/\"><code>networkx</code></a>) or &ldquo;computational graph management\nlibrary&rdquo; or even &ldquo;symbolic tensor algebra libraries&rdquo;.</p>\n<p>So for the avoidance of doubt, here is a list of libraries that this blog post is <em>not</em> about:</p>\n<ul>\n<li>NumPy and SciPy\n<ul>\n<li>These libraries don&rsquo;t have a concept of a computational graph &mdash; they’re more like a toolbox of\nfunctions, called from Python and executed in C or Fortran.</li>\n<li>However, this might be a controversial distinction &mdash; as we’ll see later, JAX also doesn&rsquo;t build\nan explicit computational graph either, and I definitely want to include JAX as a &ldquo;tensor\ncomputation library&rdquo;&hellip; ¯\\_(ツ)_/¯</li>\n</ul>\n</li>\n<li>Numba and Cython\n<ul>\n<li>These libraries provide best execution for code (and in fact some tensor computation libraries,\nsuch as Theano, make good use them), but like NumPy and SciPy, they do not actually manage the\ncomputational graph itself.</li>\n</ul>\n</li>\n<li>Keras, Trax, Flax and PyTorch-Lightning\n<ul>\n<li>These libraries are high-level wrappers around tensor computation libraries &mdash; they basically\nprovide abstractions and a user-facing API to utilize tensor computation libraries in a\nfriendlier way.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"some-differences-between-tensor-computation-libraries\">(Some) Differences Between Tensor Computation Libraries</h3>\n<p>Anyways, back to tensor computation libraries.</p>\n<p>All three aforementioned goals are ambitious undertakings with sophisticated solutions, so it\nshouldn&rsquo;t be surprising to learn that decisions in pursuit on goal can have implications for (or\neven incur a trade-off with!) other goals. Here&rsquo;s a list of common differences along all three axes:</p>\n<ol>\n<li>\n<p>Tensor computation libraries can differ in how they represent the computational graph, and how it\nis built.</p>\n<ul>\n<li>Static or dynamic graphs: do we first define the graph completely and then inject data to run\n(a.k.a. define-and-run), or is the graph defined on-the-fly via the actual forward computation\n(a.k.a. define-by-run)?\n<ul>\n<li>TensorFlow 1.x was (in)famous for its static graphs, which made users feel like they were\n&ldquo;working with their computational graph through a keyhole&rdquo;, especially when <a href=\"https://news.ycombinator.com/item?id=13429355\">compared to\nPyTorch&rsquo;s dynamic graphs</a>.</li>\n</ul>\n</li>\n<li>Lazy or eager execution: do we evaluate variables as soon as they are defined, or only when a\ndependent variable is evaluated? Usually, tensor computation libraries either choose to support\ndynamic graphs with eager execution, or static graphs with lazy execution &mdash; for example,\n<a href=\"https://www.tensorflow.org/guide/eager\">TensorFlow 2.0 supports both modes</a>.</li>\n<li>Interestingly, some tensor computation libraries (e.g. <a href=\"https://thinc.ai/\">Thinc</a>) don&rsquo;t even\nconstruct an explicit computational graph: they represent it as <a href=\"https://thinc.ai/docs/concept\">chained higher-order\nfunctions</a>.</li>\n</ul>\n</li>\n<li>\n<p>Tensor computation libraries can also differ in what they want to use the computational graph\n<em>for</em> &mdash; for example, are we aiming to do things that basically amount to running the\ncomputational graph in a &ldquo;different mode&rdquo;, or are we aiming to modify the computational graph\nitself?</p>\n<ul>\n<li>Almost all tensor computation libraries support autodifferentiation in some capacity (either\nforward-mode, backward-mode, or both).</li>\n<li>Obviously, how you represent the computational graph and what you want to use it for are very\nrelated questions! For example, if you want to be able to represent aribtrary computation as a\ngraph, you&rsquo;ll have to handle control flow like if-else statements or for-loops &mdash; this leads\nto common gotchas with <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Control-Flow\">using Python for-loops in\nJAX</a>\nor needing to use <a href=\"https://discuss.pytorch.org/t/can-you-have-for-loops-in-the-forward-prop/68295\"><code>torch.nn.ModuleList</code> in for-loops with\nPyTorch</a>.</li>\n<li>Some tensor computation libraries (e.g. <a href=\"https://github.com/Theano/Theano\">Theano</a> and its\nfork, <a href=\"https://theano-pymc.readthedocs.io/en/latest/index.html\">Theano-PyMC</a>) aim to <a href=\"https://theano-pymc.readthedocs.io/en/latest/extending/optimization.html\">optimize\nthe computational graph\nitself</a>, for which an\n<a href=\"#an-observation-on-static-graphs-and-theano\">explicit graph is necessary</a>.</li>\n</ul>\n</li>\n<li>\n<p>Finally, tensor computation libraries can also differ in how they execute code.</p>\n<ul>\n<li>All tensor computation libraries run on CPU, but the strength of GPU and TPU support is a major\ndifferentiator among tensor computation libraries.</li>\n<li>Another differentiator is how tensor computation libraries compile code to be executed on\nhardware. For example, do they use JIT compilation or not? Do they use &ldquo;vanilla&rdquo; C or CUDA\ncompilers, or <a href=\"https://tensorflow.google.cn/xla\">the XLA compiler for machine-learning specific\ncode</a>?</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"a-zoo-of-tensor-computation-libraries\">A Zoo of Tensor Computation Libraries</h2>\n<p>Having outlined the basic similarities and differences of tensor computation libraries, I think\nit&rsquo;ll be helpful to go through several of the popular libraries as examples. I&rsquo;ve tried to link to\nthe relevant documentation where possible.<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup></p>\n<h3 id=\"pytorchhttpspytorchorg\"><a href=\"https://pytorch.org/\">PyTorch</a></h3>\n<ol>\n<li>How is the computational graph represented and built?\n<ul>\n<li>PyTorch dynamically builds (and eagerly evaluates) an explicit computational graph. For more\ndetail on how this is done, check out <a href=\"https://pytorch.org/docs/stable/notes/autograd.html\">the PyTorch docs on autograd\nmechanics</a>.</li>\n<li>For more on how PyTorch computational graphs, see <a href=\"https://jdhao.github.io/2017/11/12/pytorch-computation-graph/\"><code>jdhao</code>&rsquo;s introductory blog post on\ncomputational graphs in\nPyTorch</a>.</li>\n</ul>\n</li>\n<li>What is the computational graph used for?\n<ul>\n<li>To quote the <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch docs</a>, &ldquo;PyTorch is an\noptimized tensor library for deep learning using GPUs and CPUs&rdquo; &mdash; as such, the main focus is\non <a href=\"https://pytorch.org/docs/stable/notes/autograd.html\">autodifferentiation</a>.</li>\n</ul>\n</li>\n<li>How does the library ensure &ldquo;best execution&rdquo; for computation?\n<ul>\n<li>PyTorch has <a href=\"https://pytorch.org/docs/stable/notes/cuda.html\">native GPU support</a> via CUDA.</li>\n<li>PyTorch also has support for TPU through projects like\n<a href=\"https://github.com/pytorch/xla\">PyTorch/XLA</a> and\n<a href=\"https://www.pytorchlightning.ai/\">PyTorch-Lightning</a>.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"jaxhttpsjaxreadthedocsioenlatest\"><a href=\"https://jax.readthedocs.io/en/latest/\">JAX</a></h3>\n<ol>\n<li>\n<p>How is the computational graph represented and built?</p>\n<ul>\n<li>\n<p>Instead of building an explicit computational graph to compute gradients, JAX simply supplies a\n<code>grad()</code> that returns the gradient function of any supplied function. As such, there is\ntechnically no concept of a computational graph &mdash; only pure (i.e. stateless and\nside-effect-free) functions and their gradients.</p>\n</li>\n<li>\n<p><a href=\"https://sjmielke.com/jax-purify.htm\">Sabrina Mielke summarizes the situation very well</a>:</p>\n<blockquote>\n<p>PyTorch builds up a graph as you compute the forward pass, and one call to <code>backward()</code> on\nsome &ldquo;result&rdquo; node then augments each intermediate node in the graph with the gradient of the\nresult node with respect to that intermediate node. JAX on the other hand makes you express\nyour computation as a Python function, and by transforming it with <code>grad()</code> gives you a\ngradient function that you can evaluate like your computation function — but instead of the\noutput it gives you the gradient of the output with respect to (by default) the first\nparameter that your function took as input.</p>\n</blockquote>\n</li>\n</ul>\n</li>\n<li>\n<p>What is the computational graph used for?</p>\n<ul>\n<li>According to the <a href=\"https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\">JAX quickstart</a>,\nJAX bills itself as &ldquo;NumPy on the CPU, GPU, and TPU, with great automatic differentiation for\nhigh-performance machine learning research&rdquo;. Hence, its focus is heavily on\nautodifferentiation.</li>\n</ul>\n</li>\n<li>\n<p>How does the library ensure &ldquo;best execution&rdquo; for computation?</p>\n<ul>\n<li>\n<p>This is best explained by quoting the <a href=\"https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\">JAX quickstart</a>:</p>\n<blockquote>\n<p>JAX uses XLA to compile and run your NumPy code on [&hellip;] GPUs and TPUs. Compilation happens\nunder the hood by default, with library calls getting just-in-time compiled and executed. But\nJAX even lets you just-in-time compile your own Python functions into XLA-optimized kernels\n[&hellip;] Compilation and automatic differentiation can be composed arbitrarily [&hellip;]</p>\n</blockquote>\n</li>\n<li>\n<p>For more detail on JAX’s four-function API (<code>grad</code>, <code>jit</code>, <code>vmap</code> and <code>pmap</code>), see\n<a href=\"http://alexminnaar.com/2020/08/15/jax-overview.html\">Alex Minaar&rsquo;s overview of how JAX works</a>.</p>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"theanohttpstheano-pymcreadthedocsioenlatest\"><a href=\"https://theano-pymc.readthedocs.io/en/latest/\">Theano</a></h3>\n<blockquote>\n<p><strong>Note:</strong> the <a href=\"https://github.com/Theano/Theano\">original Theano</a> (maintained by\n<a href=\"https://mila.quebec/en/\">MILA</a>) has been discontinued, and the PyMC developers have forked the\nproject: <a href=\"https://github.com/pymc-devs/Theano-PyMC\">Theano-PyMC</a> (soon to be renamed Aesara). I&rsquo;ll\ndiscuss both the original and forked projects below.</p>\n</blockquote>\n<ol>\n<li>How is the computational graph represented and built?\n<ul>\n<li>Theano statically builds (and lazily evaluates) an explicit computational graph.</li>\n</ul>\n</li>\n<li>What is the computational graph used for?\n<ul>\n<li>Theano is unique among tensor computation libraries in that it places more emphasis on\nreasoning about the computational graph itself. In other words, while Theano has <a href=\"https://theano-pymc.readthedocs.io/en/latest/library/gradient.html\">strong\nsupport for\nautodifferentiation</a>,\nrunning the computation and computing gradients isn&rsquo;t the be-all and end-all: Theano has an\nentire module for <a href=\"https://theano-pymc.readthedocs.io/en/latest/optimizations.html\">optimizing the computational graph\nitself</a>, and makes it fairly\nstraightforward to compile the Theano graph to different computational backends (by default,\nTheano compiles to C or CUDA, but it’s straightforward to compile to JAX).</li>\n<li>Theano is often remembered as a library for deep learning research, but it’s so much more than\nthat!</li>\n</ul>\n</li>\n<li>How does the library ensure &ldquo;best execution&rdquo; for computation?\n<ul>\n<li>The original Theano used the GCC C compiler for CPU computation, and the NVCC CUDA compiler for\nGPU computation.</li>\n<li>The Theano-PyMC fork project <a href=\"https://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b\">will use JAX as a\nbackend</a>,\nwhich can utilize CPUs, GPUs and TPUs as available.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"an-observation-on-static-graphs-and-theano\">An Observation on Static Graphs and Theano</h2>\n<p>Finally, a quick observation on static graphs and the niche that Theano fills that other tensor\ncomputation libraries do not. I had huge help from <a href=\"https://twiecki.io/\">Thomas Wiecki</a> and\n<a href=\"https://brandonwillard.github.io/\">Brandon Willard</a> with this section.</p>\n<p>There&rsquo;s been a consistent movement in most tensor computation libraries away from static graphs (or\nmore precisely, statically <em>built</em> graphs): PyTorch and TensorFlow 2 both support dynamically\ngenerated graphs by default, and JAX forgoes an explicit computational graph entirely.</p>\n<p>This movement is understandable &mdash; building the computational graph dynamically matches people&rsquo;s\nprogramming intuition much better. When I write <code>z = x + y</code>, I don&rsquo;t mean <em>&ldquo;I want to register a sum\noperation with two inputs, which is waiting for data to be injected&rdquo;</em> &mdash; I mean <em>&ldquo;I want to compute\nthe sum of <code>x</code> and <code>y</code>&rdquo;.</em> The extra layer of indirection is not helpful to most users, who just want\nto run their tensor computation at some reasonable speed.</p>\n<p>So let me speak in defence of statically built graphs.</p>\n<p>Having an explicit representation of the computational graph is immensely useful for certain things,\neven if it makes the graph harder to work with. You can modify the graph (e.g. graph optimizations,\nsimplifications and rewriting), and you can reason about and analyze the graph. Having the\ncomputation as an actual <em>object</em> helps immeasurably for tasks where you need to think about the\ncomputation itself, instead of just blindly running it.</p>\n<p>On the other hand, with dynamically generated graphs, the computational graph is never actually\ndefined anywhere: the computation is traced out on the fly and behind the scene. You can no longer\ndo anything interesting with the computational graph: for example, if the computation is slow, you\ncan&rsquo;t reason about <em>what</em> parts of the graph are slow. The end result is that you basically have to\nhope that the framework internals are doing the right things, which they might not!</p>\n<p>This is the niche that Theano (or rather, Theano-PyMC/Aesara) fills that other contemporary tensor\ncomputation libraries do not: the promise is that if you take the time to specify your computation\nup front and all at once, Theano can optimize the living daylight out of your computation &mdash; whether\nby graph manipulation, efficient compilation or something else entirely &mdash; and that this is something\nyou would only need to do once.</p>\n<hr>\n<h2 id=\"some-follow-ups-a-week-later\">Some Follow-Ups, A Week Later</h2>\n<p><em>2020-12-22</em></p>\n<p>The blog post trended <a href=\"https://news.ycombinator.com/item?id=25435028\">on Hacker\nNews</a> and got some discussion.\nIt&rsquo;s stupefying how the most upvoted comments are either unrelated or\nself-promotional, but I suppose that&rsquo;s to be expected with the Internet.</p>\n<p>However, one nugget of gold in the junk pit is <a href=\"https://news.ycombinator.com/item?id=25436656\">this comment by Albert\nZeyer</a> and the <a href=\"https://news.ycombinator.com/item?id=25439483\">response by the\nPyMC developer spearheading the Aesara project, Brandon\nWillard</a>. I had two takeaways\nfrom this exchange:</p>\n<ol>\n<li>Theano is messy, either in a code hygiene sense, or in an API design sense.\n<ul>\n<li>For example, the graph optimization/rewriting process can require entire\ngraphs to be copied at multiple points along the way. This obliterates\nperformance and was almost entirely due to some design oddities.</li>\n</ul>\n</li>\n<li>The JAX backend arose as a proof-of-concept of how extensible Theano is,\nboth in terms of &ldquo;hackability&rdquo; and how much mileage we can get out of the\ndesign choices behind Theano (e.g. static graphs). The JAX backend isn&rsquo;t the\nfocus of the fork, but it&rsquo;s easily the difference that will stand out most\nat the user level. The focus of the Aesara is <em>resolving the design\nshortcomings of Theano</em>.</li>\n</ol>\n<p>On the one hand, I&rsquo;m glad that I finally understand the <em>real</em> focus of the\nAesara fork &mdash; I feel like I have a <em>much</em> greater appreciation of what Aesara\nreally is, and it&rsquo;s place in the ecosystem of tensor computation libraries.</p>\n<p>On the other hand, I&rsquo;m discomfited by the implication that meaningful\ncontributions to Aesara must involve deep expertise on computational graphs and\ngraph optimizations - neither of which I have experience in (and I suspect are\nrare even among the open source community). Moreover, meaningful contributions\nto Aesara will probably require deep familiarity with Theano&rsquo;s design and its\nshortcomings. This isn&rsquo;t to discourage me (or anyone else!) from contributing\nto Aesara, but it&rsquo;s good to acknowledge the bottomless pit of technical\nexpertise that goes on behind the user-facing Bayesian modelling.</p>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>Some readers will notice the conspicuous lack of TensorFlow from this list - its exclusion isn&rsquo;t out of malice, merely a lack of time and effort to do the necessary research to do it justice. Sorry.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}