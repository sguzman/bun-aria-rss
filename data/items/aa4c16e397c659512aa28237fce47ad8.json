{
  "title": "Fast Message Serialization",
  "link": "",
  "updated": "2016-04-14T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/04/14/dask-distributed-optimizing-protocol",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><em>Very high performance isn’t about doing one thing well, it’s about doing\nnothing poorly.</em></p>\n\n<p>This week I optimized the inter-node communication protocol used by\n<code class=\"language-plaintext highlighter-rouge\">dask.distributed</code>.  It was a fun exercise in optimization that involved\nseveral different and unexpected components.  I separately had to deal with\nPickle, NumPy, Tornado, MsgPack, and compression libraries.</p>\n\n<p>This blogpost is not advertising any particular functionality, rather it’s a\nstory of the problems I ran into when designing and optimizing a protocol to\nquickly send both very small and very large numeric data between machines on\nthe Python stack.</p>\n\n<p>We care very strongly about both the many small messages case (thousands of\n100 byte messages per second) <em>and</em> the very large messages case (100-1000 MB).\nThis spans an interesting range of performance space.  We end up with a\nprotocol that costs around 5 microseconds in the small case and operates at\n1-1.5 GB/s in the large case.</p>\n\n<h2 id=\"identify-a-problem\">Identify a Problem</h2>\n\n<p>This came about as I was preparing a demo using <code class=\"language-plaintext highlighter-rouge\">dask.array</code> on a distributed\ncluster for a Continuum webinar.  I noticed that my computations were taking\nmuch longer than expected.  The\n<a href=\"http://distributed.readthedocs.org/en/latest/web.html\">Web UI</a> quickly pointed\nme to the fact that my machines were spending 10-20 seconds moving 30 MB chunks\nof numpy array data between them.  This is very strange because I was on\n100MB/s network, and so I expected these transfers to happen in more like 0.3s\nthan 15s.</p>\n\n<p>The Web UI made this glaringly apparent, so my first lesson was how valuable\nvisual profiling tools can be when they make performance issues glaringly\nobvious.  Thanks here goes to the Bokeh developers who helped the development\nof the Dask real-time Web UI.</p>\n\n<h2 id=\"problem-1-tornados-sentinels\">Problem 1: Tornado’s sentinels</h2>\n\n<p>Dask’s networking is built off of Tornado’s TCP IOStreams.</p>\n\n<p>There are two common ways to delineate messages on a socket, sentinel values\nthat signal the end of a message, and prefixing a length before every message.\nEarly on we tried both in Dask but found that prefixing a length before every\nmessage was slow.  It turns out that this was because TCP sockets try to batch\nsmall messages to increase bandwidth.  Turning this optimization off ended up\nbeing an effective and easy solution, see the <a href=\"http://www.unixguide.net/network/socketfaq/2.16.shtml\"><code class=\"language-plaintext highlighter-rouge\">TCP_NODELAY</code> parameter</a>.</p>\n\n<p>However, before we figured that out we used sentinels for a long time.\nUnfortunately Tornado does not handle sentinels well for large messages.  At\nthe receipt of every new message it reads through all buffered data to see if\nit can find the sentinel.  This makes lots and lots of copies and reads through\nlots and lots of bytes.  This isn’t a problem if your messages are a few\nkilobytes, as is common in web development, but it’s terrible if your messages\nare millions or billions of bytes long.</p>\n\n<p>Switching back to prefixing messages with lengths and turning off the no-delay\noptimization moved our bandwidth up from 3MB/s to 20MB/s per node.  Thanks goes\nto Ben Darnell (main Tornado developer) for helping us to track this down.</p>\n\n<h2 id=\"problem-2-memory-copies\">Problem 2: Memory Copies</h2>\n\n<p>A nice machine can copy memory at 5 GB/s.  If your network is only 100 MB/s\nthen you can easily suffer several memory copies in your system without caring.\nThis leads to code that looks like the following:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>socket.send(header + payload)\n</code></pre></div></div>\n\n<p>This code concatenates two bytestrings, <code class=\"language-plaintext highlighter-rouge\">header</code> and <code class=\"language-plaintext highlighter-rouge\">payload</code> before\nsending the result down a socket.  If we cared deeply about avoiding memory\ncopies then we might instead send these two separately:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>socket.send(header)\nsocket.send(payload)\n</code></pre></div></div>\n\n<p>But who cares, right?  At 5 GB/s copying memory is cheap!</p>\n\n<p>Unfortunately this breaks down under either of the following conditions</p>\n\n<ol>\n  <li>You are sloppy enough to do this multiple times</li>\n  <li>You find yourself on a machine with surprisingly low memory bandwidth,\nlike 10 times slower, as is the case on <a href=\"http://stackoverflow.com/questions/36523142/why-is-copying-memory-on-ec2-machines-slow\">some EC2 machines.</a></li>\n</ol>\n\n<p>Both of these were true for me but fortunately it’s usually straightforward to\nreduce the number of copies down to a small number (we got down to three),\nwith moderate effort.</p>\n\n<h2 id=\"problem-3-unwanted-compression\">Problem 3: Unwanted Compression</h2>\n\n<p>Dask compresses all large messages with LZ4 or Snappy if they’re available.\nUnfortunately, if your data isn’t very compressible then this is mostly lost\ntime.  Doubly unforutnate is that you also have to decompress the data on the\nrecipient side.  Decompressing not-very-compressible data was surprisingly\nslow.</p>\n\n<p>Now we compress with the following policy:</p>\n\n<ol>\n  <li>If the message is less than 10kB, don’t bother</li>\n  <li>Pick out five 10kB samples of the data and compress those.  If the result\nisn’t well compressed then don’t bother compressing the full payload.</li>\n  <li>Compress the full payload, if it doesn’t compress well then just send along\nthe original to spare the receiver’s side from compressing.</li>\n</ol>\n\n<p>In this case we use cheap checks to guard against unwanted compression.  We\nalso avoid any cost at all for small messages, which we care about deeply.</p>\n\n<h2 id=\"problem-4-cloudpickle-is-not-as-fast-as-pickle\">Problem 4: Cloudpickle is not as fast as Pickle</h2>\n\n<p>This was surprising, because cloudpickle mostly defers to Pickle for the easy\nstuff, like NumPy arrays.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]:</span> <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s\">'u1'</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">10000000</span><span class=\"p\">)</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]:</span> <span class=\"kn\">import</span> <span class=\"nn\">pickle</span><span class=\"p\">,</span> <span class=\"n\">cloudpickle</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">pickle</span><span class=\"p\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">protocol</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">8.65</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">8.42</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">17.1</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">16.9</span> <span class=\"n\">ms</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">]:</span> <span class=\"mi\">10000161</span>\n\n<span class=\"n\">In</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span> <span class=\"o\">%</span><span class=\"n\">time</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">cloudpickle</span><span class=\"p\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">protocol</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">CPU</span> <span class=\"n\">times</span><span class=\"p\">:</span> <span class=\"n\">user</span> <span class=\"mf\">20.6</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"p\">:</span> <span class=\"mf\">24.5</span> <span class=\"n\">ms</span><span class=\"p\">,</span> <span class=\"n\">total</span><span class=\"p\">:</span> <span class=\"mf\">45.1</span> <span class=\"n\">ms</span>\n<span class=\"n\">Wall</span> <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"mf\">44.4</span> <span class=\"n\">ms</span>\n<span class=\"n\">Out</span><span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]:</span> <span class=\"mi\">10000161</span>\n</code></pre></div></div>\n\n<p>But it turns out that cloudpickle is using the Python implementation, while\npickle itself (or <code class=\"language-plaintext highlighter-rouge\">cPickle</code> in Python 2) is using the compiled C implemenation.\nFortunately this is easy to correct, and a quick typecheck on common large\ndataformats in Python (NumPy and Pandas) gets us this speed boost.</p>\n\n<h2 id=\"problem-5-pickle-is-still-slower-than-youd-expect\">Problem 5: Pickle is still slower than you’d expect</h2>\n\n<p>Pickle runs at about half the speed of memcopy, which is what you’d expect from\na protocol that is mostly just “serialize the dtype, strides, then tack on the\ndata bytes”.  There must be an extraneous memory copy in there.</p>\n\n<p>See <a href=\"https://github.com/numpy/numpy/issues/7544\">issue 7544</a></p>\n\n<h2 id=\"problem-6-msgpack-is-bad-at-large-bytestrings\">Problem 6: MsgPack is bad at large bytestrings</h2>\n\n<p>Dask serializes most messages with MsgPack, which is ordinarily very fast.\nUnfortunately the MsgPack spec doesn’t support bytestrings greater than 4GB\n(which do come up for us) and the Python implementations don’t pass through\nlarge bytestrings very efficiently.  So we had to handle large bytestrings\nseparately.  Any message that contains bytestrings over 1MB in size will have\nthem stripped out and sent along in a separate frame.  This both avoids the\nMsgPack overhead and avoids a memory copy (we can send the bytes directly to\nthe socket).</p>\n\n<h2 id=\"problem-7-tornado-makes-a-copy\">Problem 7: Tornado makes a copy</h2>\n\n<p>Sockets on Windows don’t accept payloads greater than 128kB in size.  As a\nresult Tornado chops up large messages into many small ones.  On linux this\nmemory copy is extraneous.  It can be removed with a bit of logic within\nTornado.  I might do this in the moderate future.</p>\n\n<h2 id=\"results\">Results</h2>\n\n<p>We serialize small messages in about 5 microseconds (thanks msgpack!) and move\nlarge bytes around in the cost of three memory copies (about 1-1.5 GB/s) which\nis generally faster than most networks in use.</p>\n\n<p>Here is a profile of sending and receiving a gigabyte-sized NumPy array of\nrandom values through to the same process over localhost (500 MB/s on my\nmachine.)</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>         381360 function calls (381323 primitive calls) in 1.451 seconds\n\n   Ordered by: internal time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.366    0.366    0.366    0.366 {built-in method dumps}\n        8    0.289    0.036    0.291    0.036 iostream.py:360(write)\n    15353    0.228    0.000    0.228    0.000 {method 'join' of 'bytes' objects}\n    15355    0.166    0.000    0.166    0.000 {method 'recv' of '_socket.socket' objects}\n    15362    0.156    0.000    0.398    0.000 iostream.py:1510(_merge_prefix)\n     7759    0.101    0.000    0.101    0.000 {method 'send' of '_socket.socket' objects}\n    17/14    0.026    0.002    0.686    0.049 gen.py:990(run)\n    15355    0.021    0.000    0.198    0.000 iostream.py:721(_read_to_buffer)\n        8    0.018    0.002    0.203    0.025 iostream.py:876(_consume)\n       91    0.017    0.000    0.335    0.004 iostream.py:827(_handle_write)\n       89    0.015    0.000    0.217    0.002 iostream.py:585(_read_to_buffer_loop)\n   122567    0.009    0.000    0.009    0.000 {built-in method len}\n    15355    0.008    0.000    0.173    0.000 iostream.py:1010(read_from_fd)\n    38369    0.004    0.000    0.004    0.000 {method 'append' of 'list' objects}\n     7759    0.004    0.000    0.104    0.000 iostream.py:1023(write_to_fd)\n        1    0.003    0.003    1.451    1.451 ioloop.py:746(start)\n</code></pre></div></div>\n\n<p>Dominant unwanted costs include the following:</p>\n\n<ol>\n  <li>400ms: Pickling the NumPy array</li>\n  <li>400ms: Bytestring handling within Tornado</li>\n</ol>\n\n<p>After this we’re just bound by pushing bytes down a wire.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Writing fast code isn’t about writing any one thing particularly well, it’s\nabout mitigating everything that can get in your way.  As you approch peak\nperformance, previously minor flaws suddenly become your dominant bottleneck.\nSuccess here depends on frequent profiling and keeping your mind open to\nunexpected and surprising costs.</p>\n\n<h2 id=\"links\">Links</h2>\n\n<ul>\n  <li><a href=\"http://stackoverflow.com/questions/36523142/why-is-copying-memory-on-ec2-machines-slow\">EC2 slow memory copy StackOverflow question.</a></li>\n  <li><a href=\"https://github.com/tornadoweb/tornado/issues/1685\">Tornado issue for sending large messages</a></li>\n  <li><a href=\"https://en.wikipedia.org/wiki/Nagle%27s_algorithm\">Wikipedia page on Nagle’s algorithm for TCP protocol for small packets</a></li>\n  <li><a href=\"https://github.com/numpy/numpy/issues/7544\">NumPy issue for double memory copy</a></li>\n  <li><a href=\"https://github.com/cloudpipe/cloudpickle/issues/59\">Cloudpickle issue for memoryview support</a></li>\n  <li><a href=\"https://distributed.readthedocs.org/en/latest/\">dask.distributed</a></li>\n</ul>"
}