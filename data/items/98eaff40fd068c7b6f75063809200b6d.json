{
  "title": "DBS: Dynamic Batch Size For Distributed Deep Neural Network Training. (arXiv:2007.11831v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2007.11831",
  "description": "<p>Synchronous strategies with data parallelism, such as the Synchronous\nStochasticGradient Descent (S-SGD) and the model averaging methods, are widely\nutilizedin distributed training of Deep Neural Networks (DNNs), largely owing\nto itseasy implementation yet promising performance. Particularly, each worker\nofthe cluster hosts a copy of the DNN and an evenly divided share of the\ndatasetwith the fixed mini-batch size, to keep the training of DNNs\nconvergence. In thestrategies, the workers with different computational\ncapability, need to wait foreach other because of the synchronization and\ndelays in network transmission,which will inevitably result in the\nhigh-performance workers wasting computation.Consequently, the utilization of\nthe cluster is relatively low. To alleviate thisissue, we propose the Dynamic\nBatch Size (DBS) strategy for the distributedtraining of DNNs. Specifically,\nthe performance of each worker is evaluatedfirst based on the fact in the\nprevious epoch, and then the batch size and datasetpartition are dynamically\nadjusted in consideration of the current performanceof the worker, thereby\nimproving the utilization of the cluster. To verify theeffectiveness of the\nproposed strategy, extensive experiments have been conducted,and the\nexperimental results indicate that the proposed strategy can fully utilizethe\nperformance of the cluster, reduce the training time, and have good\nrobustnesswith disturbance by irrelevant tasks. Furthermore, rigorous\ntheoretical analysis hasalso been provided to prove the convergence of the\nproposed strategy.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Mingjia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>"
}