{
  "title": "Derivation: Maximum Likelihood for Boltzmann Machines",
  "link": "https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/",
  "comments": "https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/#comments",
  "dc:creator": "dustinstansbury",
  "pubDate": "Wed, 24 Sep 2014 01:39:55 +0000",
  "category": [
    "Density Estimation",
    "Derivations",
    "Feature Learning",
    "Gradient Descent",
    "Machine Learning",
    "Maximum Likelihood",
    "MCMC",
    "Neural Networks",
    "Optimization",
    "Statistics",
    "Boltzmann machines",
    "feature learning",
    "Gibbs sampling",
    "maximum likelihood",
    "restricted boltzmann machine"
  ],
  "guid": "http://theclevermachine.wordpress.com/?p=4704",
  "description": "In this post I will review the gradient descent algorithm that is commonly used to train the general class of models known as Boltzmann machines. Though the primary goal of the post is to supplement another post on restricted Boltzmann machines, I hope that those readers who are curious about how Boltzmann machines are trained, but have found it difficult to track down a [&#8230;]",
  "content:encoded": "<p style=\"text-align:left;\">In this post I will review the gradient descent algorithm that is commonly used to train the general class of models known as Boltzmann machines. Though the primary goal of the post is to supplement another post on restricted Boltzmann machines, I hope that those readers who are curious about how Boltzmann machines are trained, but have found it difficult to track down a complete or straight-forward derivation of the maximum likelihood learning algorithm for these models (as I have), will also find the post informative.</p>\n<p style=\"text-align:left;\">First, a little background: Boltzmann machines are stochastic neural networks that can be thought of as the probabilistic extension of the <a title=\"Hopfield network\" href=\"http://en.wikipedia.org/wiki/Hopfield_network\" target=\"_blank\">Hopfield network</a>. The goal of the Boltzmann machine is to model a set of observed data in terms of a set of visible random variables <img src=\"https://s0.wp.com/latex.php?latex=v&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"v\" class=\"latex\" />  and a set of latent/unobserved random variables <img src=\"https://s0.wp.com/latex.php?latex=h&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"h\" class=\"latex\" />. Due to the relationship between Boltzmann machines and neural networks, the random variables are often are often referred to as &#8220;units.&#8221; The role of the visible units is to approximate the true distribution of the data, while the role of the latent variables it to extend the expressiveness of the model by capturing underlying features in the observed data. The latent variables are often referred to as hidden units, as they do not result directly from the observed data and are generally marginalized over to obtain the likelihood of the observed data,  i.e.</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B%5Cbegin%7Barray%7D%7Brcl%7D+p%28v%3B%5Ctheta%29+%26%3D%26+%5Csum_h+p%28v%2Ch%3B+%5Ctheta%29+%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{&#92;begin{array}{rcl} p(v;&#92;theta) &=& &#92;sum_h p(v,h; &#92;theta) &#92;end{array}}\" class=\"latex\" />,</p>\n<p>&nbsp;</p>\n<p>where <img src=\"https://s0.wp.com/latex.php?latex=p%28v%2Ch%3B+%5Ctheta%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(v,h; &#92;theta)\" class=\"latex\" /> is the joint probability distribution over the visible and hidden units based on the current model parameters <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />. The general Boltzmann machine defines <img src=\"https://s0.wp.com/latex.php?latex=p%28v%2Ch%3B+%5Ctheta%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(v,h; &#92;theta)\" class=\"latex\" /> through a set of weighted,  symmetric connections between all visible and hidden units (but no connections from any unit to itself). The graphical model for the general Boltzmann machine is shown in Figure 1.</p>\n<div data-shortcode=\"caption\" id=\"attachment_4756\" style=\"width: 410px\" class=\"wp-caption aligncenter\"><a href=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png\"><img loading=\"lazy\" aria-describedby=\"caption-attachment-4756\" data-attachment-id=\"4756\" data-permalink=\"https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/bm/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png\" data-orig-size=\"1135,910\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"bm\" data-image-description=\"\" data-image-caption=\"<p>Graphical Model of the Boltzmann machine model (biases not depicted).</p>\n\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=300\" data-large-file=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=914\" class=\"wp-image-4756\" src=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=400&#038;h=321\" alt=\"Graphical Model of the Boltzmann machine model (biases not depicted).\" width=\"400\" height=\"321\" srcset=\"https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=400&h=321 400w, https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=800&h=642 800w, https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=150&h=120 150w, https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=300&h=241 300w, https://theclevermachine.files.wordpress.com/2014/09/bm.png?w=768&h=616 768w\" sizes=\"(max-width: 400px) 100vw, 400px\" /></a><p id=\"caption-attachment-4756\" class=\"wp-caption-text\">Figure 1: Graphical Model of the Boltzmann machine (biases not depicted).</p></div>\n<p>Given the current state of the visible and hidden units, the overall configuration of the model network is described by a connectivity function <img src=\"https://s0.wp.com/latex.php?latex=E%28v%2Ch%3B%5Ctheta%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E(v,h;&#92;theta)\" class=\"latex\" />, parameterized by <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+%7BW%2C+A%2C+B%2C+a%2C+b%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;theta = {W, A, B, a, b}\" class=\"latex\" />:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B%5Cbegin%7Barray%7D%7Brcl%7D+E%28v%2Ch%3B+%5Ctheta%29+%26%3D%26+v%5ET+W+h+%2B+h%5ET+A+h+%2B+v%5ET+B+v+%2B+h%5ET+a+%2B+v%5ET+b+%5Cend%7Barray%7D%7D.&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{&#92;begin{array}{rcl} E(v,h; &#92;theta) &=& v^T W h + h^T A h + v^T B v + h^T a + v^T b &#92;end{array}}.\" class=\"latex\" /></p>\n<p>The parameter matrix <img src=\"https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W\" class=\"latex\" /> defines the connection strength between the visible and hidden units. The parameters <img src=\"https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"A\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=B&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"B\" class=\"latex\" /> define the connection strength amongst hidden units and visible units, respectively. The model also includes a set of  biases <img src=\"https://s0.wp.com/latex.php?latex=a&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"a\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=b&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"b\" class=\"latex\" /> that capture offsets for each of the hidden and visible units.</p>\n<p>The Boltzmann machine has been used for years in field of statistical mechanics to model physical systems based on the principle of energy minimization. In the statistical mechanics, the connectivity function is often referred to the &#8220;energy function,&#8221; a term that is has also been standardized in the statistical learning literature. Note that the energy function returns a single scalar value for any configuration of the network parameters and random variable states.</p>\n<p>Given the energy function, the Boltzmann machine models the joint probability of the visible and hidden unit states as a <a title=\"Boltzmann Distribution\" href=\"http://en.wikipedia.org/wiki/Boltzmann_distribution\" target=\"_blank\">Boltzmann distribution</a>:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B%5Cbegin%7Barray%7D%7Brcl%7D+p%28v%2Ch%3B+%5Ctheta%29+%26%3D%26+%5Cfrac%7B%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%3B+%5Ctheta%29%7D%7D%7BZ%28%5Ctheta%29%7D+%5Ctext%7B+%2C+where%7D+%5C%5C+%5C%5C++Z%28%5Ctheta%29+%26%3D%26+%5Csum_%7Bv%27%7D+%5Csum_%7Bh%27%7D+%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%3B+%5Ctheta%29%7D%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{&#92;begin{array}{rcl} p(v,h; &#92;theta) &=& &#92;frac{&#92;mathrm{e}^{-E(v,h; &#92;theta)}}{Z(&#92;theta)} &#92;text{ , where} &#92;&#92; &#92;&#92;  Z(&#92;theta) &=& &#92;sum_{v&#039;} &#92;sum_{h&#039;} &#92;mathrm{e}^{-E(v&#039;,h&#039;; &#92;theta)}&#92;end{array}}\" class=\"latex\" /></p>\n<p>The partition function <img src=\"https://s0.wp.com/latex.php?latex=Z%28%5Ctheta%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Z(&#92;theta)\" class=\"latex\" /> is a normalizing constant that is calculated by summing over all possible states of the network <img src=\"https://s0.wp.com/latex.php?latex=%28v%27%2C+h%27%29+%5Cin+%28V%27%2CH%27%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"(v&#039;, h&#039;) &#92;in (V&#039;,H&#039;)\" class=\"latex\" />. Here we assume that all random variables take on discrete values, but the analogous derivation holds for continuous or mixed variable types by replacing the sums with integrals accordingly.</p>\n<p>The common way to train the Boltzmann machine is to determine the parameters that maximize the likelihood of the observed data. To determine the parameters, we perform gradient descent on the log of the likelihood function (In order to simplify the notation in the remainder of the derivation, we do not include the explicit dependency on the parameters <img src=\"https://s0.wp.com/latex.php?latex=%5Ctheta&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;theta\" class=\"latex\" />. To further simplify things, let&#8217;s also assume that we calculate the gradient of the likelihood based on a single observation.):</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B+%5Cbegin%7Barray%7D%7Brcl%7D+l%28v%3B+%5Ctheta%29+%26%3D%26+%5Clog+p%28v%29+%5C%5C++%26%3D%26+%5Clog+%5Csum_h+p%28v%2Ch%29+%5C%5C++%26%3D%26+%5Clog+%5Cfrac%7B%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D%7D%7BZ%7D+%5C%5C++%26%3D%26+%5Clog+%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D+-+%5Clog+Z+%5C%5C++%26%3D%26+%5Clog+%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D+-+%5Csum_%7Bv%27%7D+%5Csum_%7Bh%27%7D+%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D++%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{ &#92;begin{array}{rcl} l(v; &#92;theta) &=& &#92;log p(v) &#92;&#92;  &=& &#92;log &#92;sum_h p(v,h) &#92;&#92;  &=& &#92;log &#92;frac{&#92;sum_h &#92;mathrm{e}^{-E(v,h)}}{Z} &#92;&#92;  &=& &#92;log &#92;sum_h &#92;mathrm{e}^{-E(v,h)} - &#92;log Z &#92;&#92;  &=& &#92;log &#92;sum_h &#92;mathrm{e}^{-E(v,h)} - &#92;sum_{v&#039;} &#92;sum_{h&#039;} &#92;mathrm{e}^{-E(v&#039;,h&#039;)}  &#92;end{array}}\" class=\"latex\" /></p>\n<p>The gradient calculation is as follows:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B+%5Cbegin%7Barray%7D%7Brcl%7D+%5Cfrac%7B%5Cpartial+l%28v%3B%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D+%26%3D%26+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Clog+%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D+-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Clog+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D+%5C%5C++%26%3D%26+%5Cfrac%7B1%7D%7B%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D+-+%5Cfrac%7B1%7D%7B%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D+%5C%5C++%26%3D%26+-+%5Cfrac%7B1%7D%7B%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D%7D+%5Csum_h+%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Cfrac%7B1%7D%7B%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D%7D+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D++%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{ &#92;begin{array}{rcl} &#92;frac{&#92;partial l(v;&#92;theta)}{&#92;partial &#92;theta} &=& &#92;frac{&#92;partial}{&#92;partial &#92;theta}&#92;log &#92;sum_h &#92;mathrm{e}^{-E(v,h)} - &#92;frac{&#92;partial}{&#92;partial &#92;theta} &#92;log &#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)} &#92;&#92;  &=& &#92;frac{1}{&#92;sum_h &#92;mathrm{e}^{-E(v,h)}} &#92;frac{&#92;partial}{&#92;partial &#92;theta} &#92;sum_h &#92;mathrm{e}^{-E(v,h)} - &#92;frac{1}{&#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)}} &#92;frac{&#92;partial}{&#92;partial &#92;theta} &#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)} &#92;&#92;  &=& - &#92;frac{1}{&#92;sum_h &#92;mathrm{e}^{-E(v,h)}} &#92;sum_h &#92;mathrm{e}^{-E(v,h)}&#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;frac{1}{&#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)}} &#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)}&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta}  &#92;end{array}}\" class=\"latex\" /></p>\n<p>Here we can simplify the expression somewhat by noting that <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathrm%7Be%7D%5E%7B-E%28v%2Ch%29%7D+%3D+Z+p%28v%2Ch%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;mathrm{e}^{-E(v,h)} = Z p(v,h)\" class=\"latex\" />, that <img src=\"https://s0.wp.com/latex.php?latex=Z+%3D+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7D%5Cmathrm%7Be%7D%5E%7B-E%28v%27%2Ch%27%29%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Z = &#92;sum_{v&#039;}&#92;sum_{h&#039;}&#92;mathrm{e}^{-E(v&#039;,h&#039;)}\" class=\"latex\" />, and also that <img src=\"https://s0.wp.com/latex.php?latex=Z&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Z\" class=\"latex\" /> is a constant:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B+%5Cbegin%7Barray%7D%7Brcl%7D+%5Cfrac%7B%5Cpartial+l%28v%3B%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D+%26%3D%26+-+%5Cfrac%7B1%7D%7BZ%5Csum_h+p%28v%2Ch%29%7D+Z+%5Csum_h+p%28v%2Ch%29+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Cfrac%7B1%7D%7BZ%7D+Z+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7Dp%28v%27%2Ch%27%29%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D+%5C%5C++%26%3D%26+-+%5Cfrac%7B1%7D%7B%5Csum_h+p%28v%2Ch%29%7D+%5Csum_h+p%28v%2Ch%29+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7Dp%28v%27%2Ch%27%29%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D+%5C%5C++%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{ &#92;begin{array}{rcl} &#92;frac{&#92;partial l(v;&#92;theta)}{&#92;partial &#92;theta} &=& - &#92;frac{1}{Z&#92;sum_h p(v,h)} Z &#92;sum_h p(v,h) &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;frac{1}{Z} Z &#92;sum_{v&#039;}&#92;sum_{h&#039;}p(v&#039;,h&#039;)&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta} &#92;&#92;  &=& - &#92;frac{1}{&#92;sum_h p(v,h)} &#92;sum_h p(v,h) &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;sum_{v&#039;}&#92;sum_{h&#039;}p(v&#039;,h&#039;)&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta} &#92;&#92;  &#92;end{array}}\" class=\"latex\" /></p>\n<p>If we also note that <img src=\"https://s0.wp.com/latex.php?latex=%5Csum_h+p%28v%2Ch%29%3D+p%28v%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;sum_h p(v,h)= p(v)\" class=\"latex\" />, and use the definition of conditional probability <img src=\"https://s0.wp.com/latex.php?latex=p%28h%7Cv%29+%3D+%5Cfrac%7Bp%28v%2Ch%29%7D%7Bp%28v%29%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(h|v) = &#92;frac{p(v,h)}{p(v)}\" class=\"latex\" />, we can further simplify the expression for the gradient:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B+%5Cbegin%7Barray%7D%7Brcl%7D+%5Cfrac%7B%5Cpartial+l%28v%3B%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D+%26%3D%26+-+%5Cfrac%7B1%7D%7Bp%28v%29%7D+%5Csum_h+p%28v%2Ch%29+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7Dp%28v%27%2Ch%27%29%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D+%5C%5C++%26%3D%26+-%5Csum_h+%5Cfrac%7Bp%28v%2Ch%29%7D%7Bp%28v%29%7D+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7Dp%28v%27%2Ch%27%29%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D+%5C%5C++%26%3D%26+-%5Csum_h+p%28h+%7C+v%29+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Csum_%7Bv%27%7D%5Csum_%7Bh%27%7Dp%28v%27%2Ch%27%29%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D+%5C%5C++%26%3D%26+-%5Cmathbb%7BE%7D_%7Bp%28h+%7C+v%29%7D+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%2B+%5Cmathbb%7BE%7D_%7Bp%28v%27%2Ch%27%29%7D%5Cfrac%7B%5Cpartial+E%28v%27%2Ch%27%29%7D%7B%5Cpartial+%5Ctheta%7D.+%5C%5C++%5Cend%7Barray%7D%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{ &#92;begin{array}{rcl} &#92;frac{&#92;partial l(v;&#92;theta)}{&#92;partial &#92;theta} &=& - &#92;frac{1}{p(v)} &#92;sum_h p(v,h) &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;sum_{v&#039;}&#92;sum_{h&#039;}p(v&#039;,h&#039;)&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta} &#92;&#92;  &=& -&#92;sum_h &#92;frac{p(v,h)}{p(v)} &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;sum_{v&#039;}&#92;sum_{h&#039;}p(v&#039;,h&#039;)&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta} &#92;&#92;  &=& -&#92;sum_h p(h | v) &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;sum_{v&#039;}&#92;sum_{h&#039;}p(v&#039;,h&#039;)&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta} &#92;&#92;  &=& -&#92;mathbb{E}_{p(h | v)} &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} + &#92;mathbb{E}_{p(v&#039;,h&#039;)}&#92;frac{&#92;partial E(v&#039;,h&#039;)}{&#92;partial &#92;theta}. &#92;&#92;  &#92;end{array}}\" class=\"latex\" /></p>\n<p>Here <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bp%28%2A%29%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb{E}_{p(*)}\" class=\"latex\" /> is the expected value under the distribution <img src=\"https://s0.wp.com/latex.php?latex=p%28%2A%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(*)\" class=\"latex\" />. Thus the gradient of the likelihood function is composed of two parts. The first part is expected gradient of the energy function with respect to the conditional distribution <img src=\"https://s0.wp.com/latex.php?latex=p%28h%7Cv%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(h|v)\" class=\"latex\" />. The second part is expected gradient of the energy function with respect to the joint distribution over all variable states. However, calculating these expectations is generally infeasible for any realistically-sized model, as it involves summing over a huge number of possible states/configurations. The general approach for solving this problem is to use Markov Chain Monte Carlo (MCMC) to approximate these sums:</p>\n<p style=\"text-align:center;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CLarge%7B%5Cbegin%7Barray%7D%7Brcl%7D+%5Cfrac%7B%5Cpartial+l%28v%3B%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D+%26%5Capprox%26+-%5Cleft+%5Clangle+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%5Cright+%5Crangle_%7Bp%28h_%7B%5Ctext%7Bdata%7D%7D%7Cv_%7B%5Ctext%7Bdata%7D%7D%29%7D+%2B+%5Cleft+%5Clangle+%5Cfrac%7B%5Cpartial+E%28v%2Ch%29%7D%7B%5Cpartial+%5Ctheta%7D+%5Cright+%5Crangle_%7Bp%28h_%7B%5Ctext%7Bmodel%7D%7D%7Cv_%7B%5Ctext%7Bmodel%7D%7D%29%7D+%5C%5C+%5Cend%7Barray%7D%7D.&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Large{&#92;begin{array}{rcl} &#92;frac{&#92;partial l(v;&#92;theta)}{&#92;partial &#92;theta} &&#92;approx& -&#92;left &#92;langle &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} &#92;right &#92;rangle_{p(h_{&#92;text{data}}|v_{&#92;text{data}})} + &#92;left &#92;langle &#92;frac{&#92;partial E(v,h)}{&#92;partial &#92;theta} &#92;right &#92;rangle_{p(h_{&#92;text{model}}|v_{&#92;text{model}})} &#92;&#92; &#92;end{array}}.\" class=\"latex\" /></p>\n<p>Here <img src=\"https://s0.wp.com/latex.php?latex=%5Clangle+%5Crangle_%7Bp%28%2A%29%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;langle &#92;rangle_{p(*)}\" class=\"latex\" /> is the sample average of samples drawn according to the process <img src=\"https://s0.wp.com/latex.php?latex=p%28%2A%29&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"p(*)\" class=\"latex\" />. The first term is calculated by taking the average value of the energy function gradient when the visible and hidden units are being driven by observed data samples. In practice, this first term is generally straightforward to calculate. Calculating the second term is generally more complicated and involves running a set of Markov chains until they reach the current model&#8217;s equilibrium distribution (i.e. via Gibbs sampling, Metropolis-Hastings, or the like), then taking the average energy function gradient based on those samples. See <a title=\"MCMC\" href=\"https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/\" target=\"_blank\">this post on MCMC methods</a> for details. It turns out that there is a subclass of Boltzmann machines that, due to a restricted connectivity/energy function (specifically, the parameters <img src=\"https://s0.wp.com/latex.php?latex=%28A%2C+B%29%3D0&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"(A, B)=0\" class=\"latex\" />), allow for efficient MCMC by way of blocked Gibbs sampling. These models, known as <em>restricted Boltzman machines</em> have become an important component for unsupervised pretraining in the field of deep learning and will be the focus of a related post.</p>\n",
  "wfw:commentRss": "https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/feed/",
  "slash:comments": 1,
  "media:content": [
    {
      "media:title": "dustinstansbury"
    },
    {
      "media:title": "Graphical Model of the Boltzmann machine model (biases not depicted)."
    }
  ]
}