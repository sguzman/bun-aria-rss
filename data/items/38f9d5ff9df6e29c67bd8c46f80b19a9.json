{
  "title": "Weekly Review: 12/23/2017",
  "link": "https://codesachin.wordpress.com/2017/12/24/weekly-review-12-23-2017/",
  "comments": "https://codesachin.wordpress.com/2017/12/24/weekly-review-12-23-2017/#comments",
  "dc:creator": "srjoglekar246",
  "pubDate": "Sat, 23 Dec 2017 22:46:02 +0000",
  "category": [
    "Machine Learning/AI",
    "Programming",
    "captcha",
    "google",
    "machine learning",
    "quantum computing",
    "xgboost"
  ],
  "guid": "http://codesachin.wordpress.com/?p=920",
  "description": "Happy Holidays people! If you live in the Bay Area then the next week is probably your time off, so I hope you have fun and enjoy the holiday season! As for Robotics, I just finished Week 2 of Perception, and will probably kick off Week 3 in 2018. I am excited for the last &#8230; <a href=\"https://codesachin.wordpress.com/2017/12/24/weekly-review-12-23-2017/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">Weekly Review: 12/23/2017</span> <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p><em><strong>Happy Holidays people!</strong></em> If you live in the Bay Area then the next week is probably your time off, so I hope you have fun and enjoy the holiday season! As for Robotics, I just finished Week 2 of <a href=\"https://www.coursera.org/learn/robotics-perception/home/welcome\">Perception</a>, and will probably kick off Week 3 in 2018. I am excited for the last &#8216;real&#8217; course (Estimation & Learning), and then building my own robot as part of the &#8216;Capstone&#8217; project after that :-D.</p>\n<p>This week&#8217;s articles:</p>\n<p><strong><a href=\"https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\">XGBoost</a></strong></p>\n<p>I recently came across XGBoost (<em>eXtreme Gradient Boosting</em>), an improvement over standard Gradient Boosting &#8211; thats actually a shame, considering how popular this method is in Data Science. If you are rusty on ensemble learning, take a look at this <a href=\"https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/\">article on bagging/random Forests</a>, and my own <a href=\"https://codesachin.wordpress.com/2016/03/06/a-small-introduction-to-boosting/\">intro to Boosting.</a></p>\n<p>XGBoost is one of the most efficient versions of Gradient Boosting, and apparently works really well on structured/tabular data. It also provides features such as sparse-awareness (being able to handle missing values), and the ability to update models with &#8216;continued training&#8217;. Its effectiveness for tabular data has made it very popular with Kaggle winners, with one of them quoting: <em>&#8220;When in doubt, use xgboost&#8221;</em>!</p>\n<p>Take a look at the <a href=\"https://arxiv.org/pdf/1603.02754.pdf\">original paper</a> to dig deeper.</p>\n<p><strong><a href=\"https://www.technologyreview.com/s/609804/a-startup-uses-quantum-computing-to-boost-machine-learning/\">Quantum Computing + Machine Learning</a></strong></p>\n<p>A lot of companies, such as Google, Microsoft, etc have recently shown interest in the domain of Quantum Computing. <a href=\"https://www.rigetti.com/\">Rigetti</a> happens to be a startup that aims to rival these juggernauts with its great solution to cloud-Quantum Computing (called <a href=\"https://www.rigetti.com/forest\">Forest</a>). They even have their own Python integration!</p>\n<p>The article in question details their efforts to prototype simple clustering with quantum computing. It is still pretty crude, and is by no means a replacement to traditional systems &#8211; for now. One of the major critical points is &#8220;Applying Quantum Computing to Machine Learning will only make a black-box system more difficult to understand&#8221;. This is infact true, but the author suggests that ML could actually/maybe help us understand the behavior of Quantum Computers by modelling them!</p>\n<p><strong><a href=\"https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\">Breaking a CAPTCHA with ML</a></strong></p>\n<p>A simple, easy-to-read, fun article on how you could break the simplest CAPTCHA algorithms with CV+Deep Learning.</p>\n<p><strong><a href=\"https://arxiv.org/abs/1712.01208\">Learning Indexing Structures with ML</a></strong></p>\n<p><em>Indexing structures</em> are essentially data structures meant for efficient data access. For example, a <a href=\"https://en.wikipedia.org/wiki/B-tree\">B-Tree Index</a> is used for efficient range-queries, a Hash-table is used for fast key-based access, etc. However, all of these data structures are pretty rigid in their behavior &#8211; they do not fine-tune/change their parameters based on the structure of the data.</p>\n<p>This paper (that includes the Google legend Jeff Dean as an author) explores the possibility of using Neural Networks (infact, a hierarchy of them) as indexing structures. Basically, you would use a Neural Network to compute the function &#8211; <em>f: data -> hash/position</em>.</p>\n<p>Some key takeaways from the paper:</p>\n<ol>\n<li><span style=\"font-weight:400;\">Range Index models essentially ‘learn’ a cumulative distribution function.</span></li>\n<li><span style=\"font-weight:400;\">The overall ‘learned index’ by this paper is a hierarchy of models (but not a tree, since two models at a certain layer can point to the same model in the next layer)</span>\n<ol>\n<li><span style=\"font-weight:400;\">As you go down the layers, the models deal with smaller and smaller subsets of the data.</span></li>\n</ol>\n</li>\n<li>Unlike a B-Tree, n<span style=\"font-weight:400;\">o ‘search’ involved, since each model predicts the next model for hash generation.</span></li>\n</ol>\n<p><strong><a href=\"https://research.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html\">Tacotron 2</a></strong></p>\n<p>This post on the Google Research blog details the development of a <a href=\"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\">WaveNet</a>-like framework to generate Human Speech from text.</p>\n<p>&nbsp;</p>\n",
  "wfw:commentRss": "https://codesachin.wordpress.com/2017/12/24/weekly-review-12-23-2017/feed/",
  "slash:comments": 1,
  "media:content": {
    "media:title": "srjoglekar246"
  }
}