{
  "title": "A novel approach to Document Embedding using Partition Averaging on Bag of Words",
  "link": "https://datasciencevademecum.com/2021/01/09/a-novel-approach-to-document-embedding-using-partition-averaging-on-bag-of-words/",
  "dc:creator": "Gianmario",
  "pubDate": "Sat, 09 Jan 2021 16:35:02 +0000",
  "category": "Uncategorized",
  "guid": "https://datasciencevademecum.com/?p=2715",
  "description": "<p>How to take a collection of vector embeddings and average them preserving the multi-sense topicality of their manifold structures. This is the third article of the &#8220;Embed, Cluster, and Average&#8221; series. Before diving deep into this tutorial, I recommend reading first the previous two articles: Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL &#8230; <a href=\"https://datasciencevademecum.com/2021/01/09/a-novel-approach-to-document-embedding-using-partition-averaging-on-bag-of-words/\" class=\"more-link\">Continue reading <span class=\"screen-reader-text\">A novel approach to Document Embedding using Partition Averaging on Bag of Words</span></a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2021/01/09/a-novel-approach-to-document-embedding-using-partition-averaging-on-bag-of-words/\">A novel approach to Document Embedding using Partition Averaging on Bag of Words</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "content:encoded": "\n<p>How to take a collection of vector embeddings and average them preserving the multi-sense topicality of their manifold structures.</p>\n\n\n\n<p>This is the third article of the &#8220;Embed, Cluster, and Average&#8221; series. Before diving deep into this tutorial, I recommend reading first the previous two articles: <a href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL</a> and <a href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\">Manifold clustering in the embedding space using UMAP and GMM</a>.</p>\n\n\n\n<p>In this tutorial, we will take the embedding extracted from COCO pictures using the ResNext-WSL model, the sparse topic representation provided by the UMAP transformation, the GMM clustering model, and we will produce an embedding representation for collections of pictures (bag of words documents). We will show why traditional averaging techniques don&#8217;t work very well for documents made of several multi-topics objects, and we will extend the <a href=\"https://arxiv.org/abs/2005.09069\">Partitioned Smoothed Inverse Frequency (P-SIF)</a> methodology to work for a general-purpose domain beyond words and texts. </p>\n\n\n\n<h2>The bag-of-words model</h2>\n\n\n\n<p>The bag-of-words model represent documents as a weighted collection of objects (the words). Each document is represented as a term-frequency matrix where the rows are the documents, the columns are the words in the set of possible values (the vocabulary), and the values are number of occurrences of the word in the document. </p>\n\n\n\n<p>For this tutorial we will assume that the words are the pictures and the document can be any collection of them. For convenience, we will use the COCO categories to define the documents and the term-frequency to correspond to the number of annotations of that category in the pictures.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" width=\"660\" height=\"371\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bag-of-words-with-pictures.png?resize=660%2C371&#038;ssl=1\" alt=\"\" class=\"wp-image-2720\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bag-of-words-with-pictures.png?w=960&ssl=1 960w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bag-of-words-with-pictures.png?resize=300%2C169&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/bag-of-words-with-pictures.png?resize=768%2C432&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Term-frequency matrix of the bag-of-word model of the COCO categories</figcaption></figure>\n\n\n\n<p>If we plot the sum the term frequencies in each row we have the following distribution:</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"540\" height=\"901\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-23.png?resize=540%2C901&#038;ssl=1\" alt=\"\" class=\"wp-image-2721\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-23.png?w=540&ssl=1 540w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-23.png?resize=180%2C300&ssl=1 180w\" sizes=\"(max-width: 540px) 100vw, 540px\" data-recalc-dims=\"1\" /><figcaption>Sum of term frequencies for each document</figcaption></figure></div>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"352\" height=\"368\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-24.png?resize=352%2C368&#038;ssl=1\" alt=\"\" class=\"wp-image-2722\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-24.png?w=352&ssl=1 352w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-24.png?resize=287%2C300&ssl=1 287w\" sizes=\"(max-width: 352px) 100vw, 352px\" data-recalc-dims=\"1\" /><figcaption>Distribution of documents sizes</figcaption></figure></div>\n\n\n\n<p>We can observe that person and chair are very popular annotations in the dataset but on average each document (the category) is made of a hundred words (the pictures) and we know that each word may represent multiple contexts.</p>\n\n\n\n<h2>Words weighted averaging</h2>\n\n\n\n<p>The naive way to represent each document would be to take the embedding vectors of each picture and perform a weighted average based on the term frequency. By doing so we would have represent the document in the same embedding space of the pictures, which is a convenient property. Nevertheless as we will see in the comparison section at the bottom, averaging over many multi-topics words would end up in almost identical vectors for all of the documents. It would be very hard to preserve the original information in the words as they would compensate each other resulting in flat less informative vectors.</p>\n\n\n\n<h2>Term frequency-inverse document frequency (tf-idf) averaging</h2>\n\n\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf-idf</a> is a a statistics that measure how important is a word to a document in a collection of them (the corpus). The more frequent a word is across the document and the less important will be.  Likely, rare words will have a higher weight. </p>\n\n\n\n<p>This technique works quite well for text data in order to filter out common words such as the, a, an, is, at, on, and to give importance to keywords. When the word dictionary (the set of all possible pictures) does not follow a language distribution then this technique does not give much improvement.<br><br>The major issue with our kind of averaging is not in the weighting of each word but rather the algebraic operation (the sum/mean) that strips off all of the informative components of the vectors.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"406\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-26.png?resize=660%2C406&#038;ssl=1\" alt=\"\" class=\"wp-image-2724\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-26.png?w=907&ssl=1 907w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-26.png?resize=300%2C185&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-26.png?resize=768%2C472&ssl=1 768w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>PCA 3D projection of the TF-IDF document matrix colored by COCO supercategories</figcaption></figure>\n\n\n\n<p>We can see that documents belonging to the same supercategory tend to cluster near each other but the separation is not very clear.</p>\n\n\n\n<h2>Partitioned SIF weighted averaging</h2>\n\n\n\n<p>The novel approach we introduce is inspired by the <a href=\"https://vgupta123.github.io/docs/AAAI-GuptaV.3656.pdf\">P-SIF averaging methodology</a>. </p>\n\n\n\n<p>The key intuitions are:</p>\n\n\n\n<ol><li>Cluster all of the &#8220;words&#8221; in &#8220;topics&#8221; using sparse coding (the paper author refers to as dictionary learning but for us, they are manifold clusters).</li><li>For each topic, do the weighted average of the pictures by <a href=\"https://openreview.net/pdf?id=SyK00v5xx\">Smooth Inverse Frequency</a>.</li><li>Concat the averages vectors in each topic.</li><li>Remove the first principal component.</li></ol>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"186\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?resize=660%2C186&#038;ssl=1\" alt=\"\" class=\"wp-image-2725\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?resize=1024%2C288&ssl=1 1024w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?resize=300%2C84&ssl=1 300w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?resize=768%2C216&ssl=1 768w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?resize=1536%2C432&ssl=1 1536w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?w=1920&ssl=1 1920w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/p-sif-overview.png?w=1320&ssl=1 1320w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Overview of the P-SIF document averaging algorithm. Image by Author.</figcaption></figure></div>\n\n\n\n<p>Let&#8217;s go through each step one by one.</p>\n\n\n\n<h3>Topic clustering</h3>\n\n\n\n<p>The original paper used a sparse encoding via dictionary learning algorithms such as <a href=\"https://en.wikipedia.org/wiki/K-SVD\">K-SVD</a>, a generalization of k-means for sparse coding. K-SVD would learn a sparse representation of the words via singular value decomposition (SVD), similarly to PCA, and would represent each word as a linear combination of the atoms. Each atom is like a cluster centroid. Thus, the algorithm works by alternating by learning a sparse coding of the data and updating the atoms to better fit the data. It is like optimizing SVD and k-means in a single algorithm. </p>\n\n\n\n<p>In the previous article, <a href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\">Manifold clustering in the embedding space using UMAP and GMM</a>, we have already discussed the limitations of those linear projections and k-means in the case of data arranged in manifold structures. Moreover, as of today, we could not find any mature implementation of the K-SVD algorithm in addition to what the paper author provided in the <a href=\"https://github.com/vgupta123/P-SIF\">related GitHub repository.</a> Therefore, we have replaced K-SVD with the combination of UMAP and GMM.</p>\n\n\n\n<h3>SIF Averaging</h3>\n\n\n\n<p>In order to perform the Smooth Inverse Frequency (SIF), we first need to calculate the probability of each word to appear in a document p.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"353\" height=\"368\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-27.png?resize=353%2C368&#038;ssl=1\" alt=\"\" class=\"wp-image-2726\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-27.png?w=353&ssl=1 353w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-27.png?resize=288%2C300&ssl=1 288w\" sizes=\"(max-width: 353px) 100vw, 353px\" data-recalc-dims=\"1\" /><figcaption>Distribution of p(word), the probability of the word to appear in a document</figcaption></figure></div>\n\n\n\n<p>We can then calculate the SIF weights as a function of the smoothing parameter alpha (we have used a value of 0.001).</p>\n\n\n\n<p>In order to perform the SIF document averaging we have to multiply each word in the document by the SIF weights and the word topic probabilities. We can now average the words in each topic with the combined weights.</p>\n\n\n\n<h3>Concatenate the topic vectors</h3>\n\n\n\n<p>In the previous step, we generated one vector for each document and topic. The concatenation would simply stack all of those topic vectors together. <br>We can think of the concatenated vector as a way to preserve most of the original information in the bag of words. Only the words belonging to a particular topic will contribute to the averaging, or the document coordinates, in the embedding topic sub-space.</p>\n\n\n\n<p>In our tutorial, we had embedding vectors of dimensionality 2048 and 40 clusters. Thus, each document is now represented in 2048*40 = 81920 dimensions. Pretty high, huh?</p>\n\n\n\n<p>We can reduce this dimensionality either by reducing the embedding space of the words before averaging or by reducing the number of topics. Those two aspects were already sub-optimized during the previous clustering step. We could, but not mandatorily, replace the original word embedding space with the reduced UMAP space (50 dimensions). Hence, we could obtain document vectors of size 50*40 = 2000 dimensions. </p>\n\n\n\n<p>Generally speaking, we can apply any dimensionality reduction transformation and not necessarily the same one used during the clustering. The convenient choice of re-using the same UMAP space used for clustering should not limit the numerous options available. <br>The aim of the clustering step is to discover topics and assign words to topics probabilities.</p>\n\n\n\n<h2>Remove the first principal component</h2>\n\n\n\n<p>In order to the SIF averaging technique to be effective, and consistent with the theoretical math behind, we need to remove the common discourse vector. The idea is that all of the words are generated according to a process that depends on the micro-topic (the local discourse). Nevertheless, all of the documents will share a component of this discourse and that component can be removed using SVD. As a proxy, we can therefore calculate the first principal component using PCA and then subtract it from the documents embedding matrix obtained after averaging and concatenation.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"378\" height=\"278\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-28.png?resize=378%2C278&#038;ssl=1\" alt=\"\" class=\"wp-image-2727\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-28.png?w=378&ssl=1 378w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-28.png?resize=300%2C221&ssl=1 300w\" sizes=\"(max-width: 378px) 100vw, 378px\" data-recalc-dims=\"1\" /><figcaption>Reconstruction error removing the principal components to the documents vectors, the dot represents the first component</figcaption></figure></div>\n\n\n\n<p>We can also observe that by retaining the remaining 50 components we could reduce the dimensionality of document vectors further without loss of information.</p>\n\n\n\n<div class=\"wp-block-jetpack-tiled-gallery aligncenter is-style-columns\"><div class=\"tiled-gallery__gallery\"><div class=\"tiled-gallery__row\"><div class=\"tiled-gallery__col\" style=\"flex-basis:50.398408352963656%\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/pca-orig.png?strip=info&#038;w=378&#038;ssl=1 378w\" alt=\"\" data-height=\"278\" data-id=\"2729\" data-link=\"https://datasciencevademecum.com/?attachment_id=2729\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/pca-orig.png\" data-width=\"378\" src=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/pca-orig.png?ssl=1\" layout=\"responsive\"/></figure></div><div class=\"tiled-gallery__col\" style=\"flex-basis:49.60159164703635%\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/pca-umap.png?strip=info&#038;w=372&#038;ssl=1 372w\" alt=\"\" data-height=\"278\" data-id=\"2730\" data-link=\"https://datasciencevademecum.com/?attachment_id=2730\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/pca-umap.png\" data-width=\"372\" src=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/pca-umap.png?ssl=1\" layout=\"responsive\"/></figure></div></div></div></div>\n\n\n\n<p>The left plot is the reconstruction curve for the document vectors concatenated in the original space while the plot on the right is the reconstructions in the UMAP space. <br>The dot represents the first principal component. We can also observe that by retaining the remaining 50 components we could have likely reduced the dimensionality of document vectors further without significant loss of information.</p>\n\n\n\n<h2>Results comparison</h2>\n\n\n\n<p>In order to test our results, we will calculate the cosine similarity between each pair of documents (in our example the COCO picture categories). We should expect similar documents, documents belonging to the same COCO supercategory, to have a higher similarity compared to the remaining ones.</p>\n\n\n\n<p>We compare 6 configurations:</p>\n\n\n\n<ol><li>bag of words </li><li>bag of words in the UMAP space</li><li>TF-IDF</li><li>TF-IDF in the UMAP space</li><li>p-SIF</li><li>p-SIF in the UMAP space</li></ol>\n\n\n\n<p>We can plot a heatmap of the similarity matrix of each configuration:</p>\n\n\n\n<div class=\"wp-block-jetpack-tiled-gallery aligncenter is-style-square\"><div class=\"tiled-gallery__gallery\"><div class=\"tiled-gallery__row columns-3\"><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2736\" data-link=\"https://datasciencevademecum.com/?attachment_id=2736\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png\" data-width=\"2508\" src=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-bow.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2735\" data-link=\"https://datasciencevademecum.com/?attachment_id=2735\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png\" data-width=\"2508\" src=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/cov-mat-umap.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2734\" data-link=\"https://datasciencevademecum.com/?attachment_id=2734\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png\" data-width=\"2508\" src=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div></div><div class=\"tiled-gallery__row columns-3\"><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2733\" data-link=\"https://datasciencevademecum.com/?attachment_id=2733\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png\" data-width=\"2508\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-tfidf-umap.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2732\" data-link=\"https://datasciencevademecum.com/?attachment_id=2732\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png\" data-width=\"2508\" src=\"https://i1.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div><div class=\"tiled-gallery__col\"><figure class=\"tiled-gallery__item\"><img decoding=\"async\" srcset=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=600%2C600&#038;strip=info&#038;ssl=1 600w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=900%2C900&#038;strip=info&#038;ssl=1 900w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=1200%2C1200&#038;strip=info&#038;ssl=1 1200w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=1500%2C1500&#038;strip=info&#038;ssl=1 1500w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=1800%2C1800&#038;strip=info&#038;ssl=1 1800w,https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?resize=2000%2C2000&#038;strip=info&#038;ssl=1 2000w\" alt=\"\" data-height=\"2284\" data-id=\"2731\" data-link=\"https://datasciencevademecum.com/?attachment_id=2731\" data-url=\"https://datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png\" data-width=\"2508\" src=\"https://i2.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/covmat-psif-umap.png?ssl=1&resize=2000%2C2000\" layout=\"responsive\"/></figure></div></div></div></div>\n\n\n\n<p> We can observe that the UMAP transformation with the traditional bag of words and TF-IDF averaging leads to completely misleading results where all of the documents end up with a similar representation (and a similar cosine distance) and likely in areas of the UMAP space without any real meaning. The TF-IDF transformation does not make any relevant difference. We already expected the technique to not work well for non-language distributions. We can finally see that our revised p-SIF, whether in the original embedding space and the UMAP space, does provide sparser and more consistent results.</p>\n\n\n\n<p>In order to better visualize how they perform, let&#8217;s calculate the ratio between the average cosine similarity within and without pairs of documents belonging to the same supercategory. A higher ratio corresponds to a better representation.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"404\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-29.png?resize=660%2C404&#038;ssl=1\" alt=\"\" class=\"wp-image-2743\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-29.png?w=697&ssl=1 697w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-29.png?resize=300%2C184&ssl=1 300w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Average ratio of within and without cosine similarity of all of the pairs in each supercategory.</figcaption></figure>\n\n\n\n<p>We can observe that p-SIF always dominates for all of the supercategories and for a few of them (e.g. animal, appliance, electronic) it is more than 7x better. By micro-averaging the results of all of the supercategories we can summarize in a single metric: the total ratio of within/without mean cosine similarity.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"660\" height=\"416\" src=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-30.png?resize=660%2C416&#038;ssl=1\" alt=\"\" class=\"wp-image-2744\" srcset=\"https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-30.png?w=706&ssl=1 706w, https://i0.wp.com/datasciencevademecum.com/wp-content/uploads/2020/12/image-30.png?resize=300%2C189&ssl=1 300w\" sizes=\"(max-width: 660px) 100vw, 660px\" data-recalc-dims=\"1\" /><figcaption>Average ratio of within and without cosine similarity of all of the pairs in all of the supercategories.</figcaption></figure>\n\n\n\n<p>Our p-SIF is confirmed to be a better technique for document embedding averaging in the case of multiple contexts. The difference between the p-SIF in the original space and the p-SIF in the UMAP space is minimal but allows us to reduce by a factor of ~40 the size of our vectors.</p>\n\n\n\n<h2>Conclusions</h2>\n\n\n\n<p>In this article we have proposed a re-adapted version of the <a href=\"https://arxiv.org/abs/2005.09069\">partitioned-Smooth Inverse Frequency (p-SIF) </a>algorithm. We have replaced the K-SVD dictionary learning step with the combination of UMAP + GMM. We believe the latter is a more suitable technique for bag-of-words models of embedding vectors generated from deep neural networks and arranged in manifolds in high-dimensional spaces. We have demonstrated the effectiveness of preserving the multiple topics and context of the original pictures that are averaged into the COCO supercategory documents. This novel proposed technique can be used for measuring the similarity between documents but can also be used for generating averaged embedding vectors in a new feature space that is tailored for representing the multi-sense topics that generic documents might represent.</p>\n\n\n\n<p>Read the previous two articles of the &#8220;Embed, Cluster, and Average&#8221; series:</p>\n\n\n\n<p><a href=\"https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/\">Extracting rich embedding features from COCO pictures using PyTorch and ResNeXt-WSL</a>.</p>\n\n\n\n<p><a href=\"https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/\">Manifold clustering in the embedding space using UMAP and GMM</a>.</p>\n\n\n\n<p>You can find the code and the notebooks at <a href=\"https://github.com/gm-spacagna/docem\" rel=\"noreferrer noopener\" target=\"_blank\">https://github.com/gm-spacagna/docem</a>.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://datasciencevademecum.com/2021/01/09/a-novel-approach-to-document-embedding-using-partition-averaging-on-bag-of-words/\">A novel approach to Document Embedding using Partition Averaging on Bag of Words</a> appeared first on <a rel=\"nofollow\" href=\"https://datasciencevademecum.com\">Vademecum of Practical Data Science</a>.</p>\n",
  "post-id": 2715
}