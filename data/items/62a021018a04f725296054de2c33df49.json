{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-4616939676419842373",
  "published": "2022-10-13T09:53:00.014-07:00",
  "updated": "2022-10-21T10:44:33.146-07:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "Crossmodal-3600 — Multilingual Reference Captions for Geographically Diverse Images",
  "content": "<span class=\"byline-author\">Posted by Ashish Thapliyal, Software Engineer, and Jordi Pont-Tuset, Research Scientist, Google Research</span><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhA3qznWJUsCY3Jh7Z5CPA5U3dJIBWD28I3OOEXklBBY4TseAh5iap2pmDdYi-s4PbBZZu7lgMut_pW9WpbEr9FLyRK0WBnzejaerkRjJfNAc4tKhOvbnxhTY70akSiIrEir7NUxXhgbFyA1DopoQdqY0feRtoiTRkuyh6Goiqs2m73Yz9bFeBvkhekSg/s600/xm3600_animation_long.gif\" style=\"display: none;\" /><p><a href=\"https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html\">Image captioning</a> is the machine learning task of automatically generating a fluent natural language description for a given image. This task is important for <a href=\"https://blog.google/outreach-initiatives/accessibility/more-accessible-web-images-arrive-10-new-languages/\">improving accessibility</a> for visually impaired users and is a core task in multimodal research encompassing both vision and language modeling.  </p><a name='more'></a> <p>However, datasets for image captioning are primarily available in English. Beyond that, there are only a few datasets covering a limited number of languages that represent just a small fraction of the world’s population. Further, these datasets feature images that severely under-represent the richness and diversity of cultures from across the globe. These aspects have hindered research on image captioning for a wide variety of languages, and directly hamper the deployment of accessibility solutions for a large potential audience around the world.  </p><p>Today we present and make publicly available the <a href=\"https://google.github.io/crossmodal-3600/\">Crossmodal 3600</a> (XM3600) image captioning evaluation dataset as a robust benchmark for multilingual image captioning that enables researchers to reliably compare research contributions in this emerging field. XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images. We show that the captions are of high quality and the style is consistent across languages. </p><div class=\"separator\" style=\"clear: both; text-align: center;\">  <video align=\"center\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" syle=\"margin-left:20%\" width=\"80%\"><source src=\"https://google.github.io/crossmodal-3600/web-data/xm3600_animation.mp4\" type=\"video/mp4\"></source></video></div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td class=\"tr-caption\" style=\"text-align: center;\">The Crossmodal 3600 dataset includes reference captions in 36 languages for each of a geographically diverse set of 3600 images. All images used with permission under the <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC-BY 2.0 license</a>.</td></tr></tbody></table> <h2>Overview of the Crossmodal 3600 Dataset</h2><p>Creating large training and evaluation datasets in multiple languages is a resource-intensive endeavor. <a href=\"https://aclanthology.org/2020.acl-main.16/\">Recent work</a> has shown that it is feasible to build multilingual image captioning models trained on machine-translated data with English captions as the starting point. However, some of the most reliable automatic metrics for image captioning are much less effective when applied to evaluation sets with translated image captions, resulting in poorer agreement with human evaluations compared to the English case. As such, trustworthy model evaluation at present can only be based on extensive human evaluation. Unfortunately, such evaluations usually cannot be replicated across different research efforts, and therefore do not offer a fast and reliable mechanism to automatically evaluate multiple model parameters and configurations (e.g., model <a href=\"https://en.wikipedia.org/wiki/Hill_climbing\">hill climbing</a>) or to compare multiple lines of research. </p><p>XM3600 provides 261,375 human-generated reference captions in 36 languages for a geographically diverse set of 3600 images from the <a href=\"https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html\">Open Images</a> dataset. We measure the quality of generated captions by comparing them to the manually provided captions using the <a href=\"https://arxiv.org/abs/1411.5726\">CIDEr</a> metric, which ranges from 0 (unrelated to the reference captions) to 10 (perfectly matching the reference captions). When comparing pairs of models, we observed strong correlations between the differences in the CIDEr scores of the model outputs, and side-by-side human evaluations comparing the model outputs. , making XM3600 is a reliable tool for high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. </p><div style=\"line-height:40%;\">    <br></div><h2>Language Selection</h2><p>We chose 30 languages beyond English, roughly based on their percentage of web content. In addition, we chose an additional five languages that include under-resourced languages that have many native speakers or major native languages from continents that would not be covered otherwise. Finally, we also included English as a baseline, thus resulting in a total of 36 languages, as listed in the table below. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: 9%; margin-right: 9%;\">    <colgroup>     <col style=\"width: 12%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 12%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 12%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 12%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 12%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 12%;\"></col>  </colgroup>  <tbody>  <tr><td style=\"text-align: left;\">Arabic     </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Bengali*    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Chinese    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Croatian    </td><td>&nbsp;&nbsp;&nbsp;</td>    <td style=\"text-align: left;\"><span style=\"font-size: small;\">Cusco<br />Quechua*</span>   </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Czech    </td>  </tr>  <tr>   <td style=\"text-align: left;\">Danish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Dutch    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">English    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Filipino    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Finnish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">French    </td>  </tr>  <tr>   <td style=\"text-align: left;\">German    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Greek    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Hebrew    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Hindi    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Hungarian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Indonesian    </td>  </tr>  <tr>   <td style=\"text-align: left;\">Italian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Japanese    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Korean    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Maori*    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Norwegian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Persian    </td>  </tr>  <tr>   <td style=\"text-align: left;\">Polish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Portuguese    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Romanian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Russian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Spanish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Swahili*    </td>  </tr>  <tr>   <td style=\"text-align: left;\">Swedish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Telugu*    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Thai    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Turkish    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Ukrainian    </td><td>&nbsp;&nbsp;&nbsp;</td>   <td style=\"text-align: left;\">Vietnamese </td></tr></tbody></table><br><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">List of languages used in XM3600. &nbsp; *Low-resource languages with many native speakers, or major native languages from continents that would not be covered otherwise.</td></tr>  </tbody></table> <h2>Image Selection</h2><p>The images were selected from among those in the <a href=\"https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html\">Open Images</a> dataset that have location metadata. Since there are many regions where more than one language is spoken, and some areas are not well covered by these images, we designed an algorithm to maximize the correspondence between selected images and the regions where the targeted languages are spoken. The algorithm starts with the selection of images with geo-data corresponding to the languages for which we have the smallest pool (e.g., Persian) and processes them in increasing order of their candidate image pool size. If there aren't enough images in an area where a language is spoken, then we gradually expand the geographic selection radius to: (i) a country where the language is spoken; (ii) a continent where the language is spoken; and, as last resort, (iii) from anywhere in the world. This strategy succeeded in providing our target number of 100 images from an appropriate region for most of the 36 languages, except for Persian (where 14 continent-level images are used) and Hindi (where all 100 images are at the global level, because the in-region images were assigned to Bengali and Telugu). </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td style=\"text-align: center;\">English<br><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s640/1-1-car.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiUXdopufo1pMrTexTsSfYyhLtAZxopwhRzfHUr3VSxJLw8xQJEEo7Sm_xOwLxM8nCCl5s_C_k1qIsvo2WRQh8Hmg1qtaqdPbBGh2UUxMU82HtQzO1ZJWxRLtlvdRn3w-_auygp_QCL9IBBtq8wf_mBARP9Ytoj6yaHQzKYVLFAoABLNBiE-TKeG-uGyQ/s320/1-1-car.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/lodekka/5072748008\">Photo</a> by <a href=\"https://www.flickr.com/people/lodekka/\">Chris Sampson</a></em></span></td>    <td>&nbsp;&nbsp;</td>    <td style=\"text-align: center;\">Swahili<br><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s640/1-2-giraffe.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJpLGjrS_EZAPP9qOKLccHt3k6mAmHPyLxQ-Ekrf3JDmqqVRlTqM_dmYOtxRWmBhyrCe28RmNCYkKcf2NYUDCLpY7GPLO5Ci5qjhonWrgCrOrgGrXWgwX3QmDcmfLeV_I6IpYduolhCGqJH28LwtSryoaZbdyib3YEqxqTg5ZEitR8sfcHJJyT1lkdsg/s320/1-2-giraffe.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/henrikpalm/7555980588\">Photo</a> by <a href=\"https://www.flickr.com/people/henrikpalm/\">Henrik Palm</a></em></span></td>    <td>&nbsp;&nbsp;</td>    <td style=\"text-align: center;\">Telugu<br><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s640/1-3-statue.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhddQGm3mPIfOcY890mh4C9tQ-Xo0cEumcT0O7aqimhcZbepCNvNrV5oDMB2eIIRVf-8ivNsBJt2kFZuTqE_udgyKfKbp_fAtuYQPa-72OxCvWOfFdG5tIV1pdV-4ednxTfeRUPwYZSPEMPwCo0UR049ELcc1beaCkDOmN1ySxlnk2fyK7euV7uJmKwAw/s320/1-3-statue.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/14675798@N06/6901350577\">Photo</a> by <a href=\"https://www.flickr.com/people/14675798@N06/\">rojypala</a></em></span></td></tr>  <tr> <td><br /></td> <td><br /></td> <td><br /></td> <td><br /></td> <td><br /></td> </tr>    <tr><td style=\"text-align: center;\">Cusco Quechua<br /><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s640/2-1-tree.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTIJpYoJsVkjbASNxVmKnHNmliR7nNgzwHDrc7fRfdpfufCaaaThJyrmyH1JO7rblvKzZaTEhube_88MONPKMyDJKtR76JytM3NA1WKUzDrtGsehZFC0rhoXpab4eYCUTuWnmCIkZq7D6G-PE60qPjIp6ATUGTK5chOo0mSB7854JbU18nUqF1WeZ1aw/s320/2-1-tree.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/mckaysavage/8296819497\">Photo</a> by <a href=\"https://www.flickr.com/people/mckaysavage/\">McKay Savage</a></em></span></td>    <td>&nbsp;&nbsp;</td>    <td style=\"text-align: center;\">Filipino<br /><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s640/2-2-boats.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjbgawSApxqJ3sQsKu44_hq1pvwD-EwvI1BopfQuYzByO2fACkuu5VcBJZMKyAyqbZ88JQX8SBpU0HR1yGCyxKAmOkpHs7WSHgZqNuC9W5aRb_sl-4ZKfH5dC93z7jK_Ah7BoKMieVaT4p26ejONReBy8F5ftIUOUzRcpuBFgx-u5T1CWdVxj7aD2x2qw/s320/2-2-boats.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/schoeters/2683433042\">Photo</a> by <a href=\"https://www.flickr.com/people/schoeters/\">Simon Schoeters</a></em></span></td>    <td>&nbsp;&nbsp;</td>    <td style=\"text-align: center;\">Chinese<br /><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s640/2-3-carving.jpg\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"640\" height=\"225\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiU2PoU1t2XWou_P-DbBgczROZd8rM3sh0eAjqFxb-wvKrQbfqOiiBqMXNhrsh0UcIUK8Pcp1U8sgbQ6eeqNGVnVP-f3BVGlR88ODnCXzXI1ZZ2FdW9A0i2EeRqBRs6phGw_WomOqyHVZXqgP5YpO75qL0Nb8pCce_a0t88CkDp-iZMUS_I1MuU0wolLA/s320/2-3-carving.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/rapidtravelchai/8639346056\">Photo</a> by <a href=\"https://www.flickr.com/people/rapidtravelchai/\">Stefan Krasowski</a></em></span></td></tr></tbody></table>    <br><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Sample images showcasing the geographical diversity of the annotated images. Images used under <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0 license</a>.</td></tr></tbody></table>    <h2>Caption Generation</h2><p>In total, all 3600 images (100 images per language) are annotated in all 36 languages, each with an average of two annotations per language, yielding a total of 261,375 captions. </p><p>Annotators work in batches of 15 images. The first screen shows all 15 images with their captions in English as generated by a captioning model trained to output a consistent style of the form \"&lt;main salient objects&gt; doing &lt;activities&gt; in the &lt;environment&gt;\", often with object attributes, such as a \"smiling\" person, \"red\" car, etc. The annotators are asked to rate the caption quality given guidelines for a 4-point scale from \"excellent\" to \"bad\", plus an option for \"not_enough_information\". This step forces the annotators to carefully assess caption quality and it primes them to internalize the style of the captions. The following screens show the images again but individually and without the English captions, and the annotators are asked to produce descriptive captions in the target language for each image.  </p><p>The image batch size of 15 was chosen so that the annotators would internalize the style without remembering the exact captions. Thus, we expect the raters to generate captions based on the image content only and lacking translation artifacts. For example in the example shown below, the Spanish caption mentions “number 42” and the Thai caption mentions “convertibles”, none of which are mentioned in the English captions. The annotators were also provided with a protocol to use when creating the captions, thus achieving style consistency across languages. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">  <colgroup>     <col style=\"width: 30%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 10%;\"></col>     <col style=\"width: 2%;\"></col>     <col style=\"width: 58%;\"></col>  </colgroup>  <tbody>  <tr><td rowspan=\"8\" style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s641/image3.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"479\" data-original-width=\"641\" height=\"239\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgBYq6ilHcp-miykaD9OAZ4ICJi0qJFJX273i6gN2iuLkdu04HLsS_hDwvi8TUF06PC5Lo6ZzmLNCg9v-KUZGj7OOe_qxwFYNGaZgoTwUtXe0BywmrtNiArB_vV8b7YLL0AcP4PylDdD_5KcJSFDpHNm5an3Q3Hu03nEKTVSMLIioWJTRbXlGpvUvywtw/s320/image3.jpg\" width=\"320\" /></a><br /><span style=\"font-size: small;\"><em><a href=\"https://www.flickr.com/photos/briansolis/5129089526\">Photo</a> by <a href=\"https://www.flickr.com/people/briansolis/\">Brian Solis</a></em></span></td>    <td rowspan=\"8\">&nbsp;&nbsp;&nbsp;</td>    <td rowspan=\"2\" style=\"text-align: left;\"><span style=\"font-size: small;\">English</span></td>    <td rowspan=\"2\">&nbsp;&nbsp;&nbsp;</td>    <td style=\"text-align: left;\">• <span style=\"font-size: small;\">A vintage sports car in a showroom with many other vintage sports cars</span></td>  </tr>  <tr><td style=\"text-align: left;\">• <span style=\"font-size: small;\">The branded classic cars in a row at display</span></td>  </tr>    <tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>  <tr><td rowspan=\"2\" style=\"text-align: left;\"><span style=\"font-size: small;\">Spanish</span></td>    <td rowspan=\"2\">&nbsp;&nbsp;&nbsp;</td>    <td style=\"text-align: left;\">• <span style=\"font-size: small;\">Automóvil clásico deportivo en exhibición de automóviles de galería — <em>(Classic sports car in gallery car show)</em></span></td>  </tr>  <tr><td style=\"text-align: left;\">• <span style=\"font-size: small;\">Coche pequeño de carreras color plateado con el número 42 en una exhibición de coches — <em>(Small silver racing car with the number 42 at a car show)</em></span></td>  </tr>    <tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>  <tr><td rowspan=\"2\" style=\"text-align: left;\"><span style=\"font-size: small;\">Thai</span></td>    <td rowspan=\"2\">&nbsp;&nbsp;&nbsp;</td>    <td style=\"text-align: left;\">• <span style=\"font-size: small;\">รถเปิดประทุนหลายสีจอดเรียงกันในที่จัดแสดง — <em>(Multicolored convertibles line up in the exhibit)</em></span></td>  </tr>  <tr>    <td style=\"text-align: left;\">• <span style=\"font-size: small;\">รถแข่งวินเทจจอดเรียงกันหลายคันในงานจัดแสดง — <em>(Several vintage racing cars line up at the show.)</em></span></td>  </tr></tbody></table>  <br><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td class=\"tr-caption\" style=\"text-align: center;\">Sample captions in three different languages (out of 36 — see full list of captions in Appendix A of the <a href=\"https://arxiv.org/abs/2205.12522\">Crossmodal-3600 paper</a>), showcasing the creation of annotations that are consistent in style across languages, while being free of direct-translation artifacts (e.g., the Spanish “number 42” or the Thai “convertibles” would not be possible when directly translating from the English versions). Image used under <a href=\"https://creativecommons.org/licenses/by/2.0/\">CC BY 2.0 license</a>.</td>  </tr>  </tbody></table>  <h2>Caption Quality and Statistics</h2><p>We ran two to five pilot studies per language to troubleshoot the caption generation process and to ensure high quality captions. We then manually evaluated a random subset of captions. First we randomly selected a sample of 600 images. Then, to measure the quality of captions in a particular language, for each image, we selected for evaluation one of the manually generated captions. We found that: </p><ul><li>For 25 out of 36 languages, the percentage of captions rated as “Good” or “Excellent” is above 90%, and the rest are all above 70%.</li>  <li>For 26 out of 36 languages, the percentage of captions rated as “Bad” is below 2%, and the rest are all below 5%.</li></ul><p>For languages that use spaces to separate words, the number of words per caption can be as low as 5 or 6 for some <a href=\"https://en.wikipedia.org/wiki/Agglutinative_language\">agglutinative languages</a> like Cusco Quechua and Czech, and as high as 18 for an <a href=\"https://en.wikipedia.org/wiki/Analytic_language\">analytic language</a> like Vietnamese. The number of characters per caption also varies drastically — from mid-20s for Korean to mid-90s for Indonesian — depending on the alphabet and the script of the language. </p><div style=\"line-height:40%;\">    <br></div><h2>Empirical Evaluation and Results</h2><p>We empirically measured the ability of the XM3600 annotations to rank image captioning model variations by training four variations of a multilingual image captioning model and comparing the CIDEr differences of the models’ outputs over the XM3600 dataset for 30+ languages, to side-by-side human evaluations. We observed strong correlations between the CIDEr differences and the human evaluations. These results support the use of the XM3600 references as a means to achieve high-quality automatic comparisons between image captioning models on a wide variety of languages beyond English. </p><div style=\"line-height:40%;\">    <br></div><h2>Recent Uses</h2><p>Recently <a href=\"https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html\">PaLI</a> used XM3600 to evaluate model performance beyond English for image captioning, image-to-text retrieval and text-to-image retrieval. The key takeaways they found when evaluating on XM3600 were that multilingual captioning greatly benefits from scaling the PaLI models, especially for low-resource languages. </p><div style=\"line-height:40%;\">    <br></div><h2>Acknowledgements</h2><p><em>We would like to acknowledge the coauthors of this work: Xi Chen and Radu Soricut.</em>   </p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}