{
  "title": "Speed Up the Cold-Start Learning in Two-Sided Bandits with Many Arms. (arXiv:2210.00340v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2210.00340",
  "description": "<p>Multi-armed bandit (MAB) algorithms are efficient approaches to reduce the\nopportunity cost of online experimentation and are used by companies to find\nthe best product from periodically refreshed product catalogs. However, these\nalgorithms face the so-called cold-start at the onset of the experiment due to\na lack of knowledge of customer preferences for new products, requiring an\ninitial data collection phase known as the burn-in period. During this period,\nMAB algorithms operate like randomized experiments, incurring large burn-in\ncosts which scale with the large number of products. We attempt to reduce the\nburn-in by identifying that many products can be cast into two-sided products,\nand then naturally model the rewards of the products with a matrix, whose rows\nand columns represent the two sides respectively. Next, we design two-phase\nbandit algorithms that first use subsampling and low-rank matrix estimation to\nobtain a substantially smaller targeted set of products and then apply a UCB\nprocedure on the target products to find the best one. We theoretically show\nthat the proposed algorithms lower costs and expedite the experiment in cases\nwhen there is limited experimentation time along with a large product set. Our\nanalysis also reveals three regimes of long, short, and ultra-short horizon\nexperiments, depending on dimensions of the matrix. Empirical evidence from\nboth synthetic data and a real-world dataset on music streaming services\nvalidates this superior performance.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Bayati_M/0/1/0/all/0/1\">Mohsen Bayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Junyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wanning Chen</a>"
}