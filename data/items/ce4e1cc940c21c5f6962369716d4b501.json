{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-553665404692333245",
  "published": "2022-10-20T13:39:00.015-07:00",
  "updated": "2022-10-21T09:27:14.310-07:00",
  "category": [
    "",
    ""
  ],
  "title": "PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations",
  "content": "<span class=\"byline-author\">Posted by Wenhao Yu, Research Scientist, Robotics at Google, and Kuang-Huei Lee, Research Engineer, Google Research, Brain team</span> <p><a href=\"https://en.wikipedia.org/wiki/Evolution_strategy\">Evolution strategy</a> (ES) is a family of optimization techniques inspired by the ideas of <a href=\"https://en.wikipedia.org/wiki/Natural_selection\">natural selection</a>: a population of candidate solutions are usually evolved over generations to better adapt to an optimization objective. ES has been applied to a variety of challenging decision making problems, such as <a href=\"https://openreview.net/forum?id=NDYbXf-DvwZ\">legged locomotion</a>, <a href=\"https://ieeexplore.ieee.org/document/9307102\">quadcopter control</a>, and even <a href=\"https://ieeexplore.ieee.org/abstract/document/9477182\">power system control</a>.  </p><a name='more'></a><p>Compared to gradient-based <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a> (RL) methods like <a href=\"https://arxiv.org/abs/1707.06347\">proximal policy optimization</a> (PPO) and <a href=\"https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html\">soft actor-critic</a> (SAC), ES has several advantages. First, ES directly explores in the space of controller parameters, while gradient-based methods often explore within a limited action space, which indirectly influences the controller parameters. More direct exploration has been shown to <a href=\"https://openai.com/blog/better-exploration-with-parameter-noise/\">boost learning performance</a> and enable large scale data collection with parallel computation. Second, a major challenge in RL is long-horizon credit assignment, e.g., when a robot accomplishes a task in the end, determining which actions it performed in the past were the most critical and should be assigned a greater reward. Since ES directly considers the total reward, it relieves researchers from needing to explicitly handle credit assignment. In addition, because ES does not rely on gradient information, it can naturally handle highly non-smooth objectives or controller architectures where gradient computation is non-trivial, such as <a href=\"https://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html\">meta–reinforcement learning</a>. However, a major weakness of ES-based algorithms is their difficulty in scaling to problems that require high-dimensional sensory inputs to encode the environment dynamics, such as training robots with complex vision inputs. </p><p>In this work, we propose “<a href=\"https://arxiv.org/abs/2207.13224\">PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations</a>”, a learning algorithm that combines <a href=\"https://en.wikipedia.org/wiki/Feature_learning\">representation learning</a> and ES to effectively solve high dimensional problems in a scalable way. The core idea is to leverage <a href=\"https://arxiv.org/abs/cond-mat/9902341\">predictive information</a>, a representation learning objective, to obtain a compact representation of the high-dimensional environment dynamics, and then apply <a href=\"https://arxiv.org/abs/1803.07055\">Augmented Random Search</a> (ARS), a popular ES algorithm, to transform the learned compact representation into robot actions. We tested PI-ARS on the challenging problem of visual-locomotion for legged robots. PI-ARS enables fast training of performant vision-based locomotion controllers that can traverse a variety of difficult environments. Furthermore, the controllers trained in simulated environments successfully transfer to a real quadruped robot. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s480/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"270\" data-original-width=\"480\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9-FBUlQSRlE2rFmeloCUXYYfBngkIWz6RIDURErqOxjFSd785fpbkjetz4lkfbUhxY8rDB88yOy26ml669f2WcP16SkXH8uZfy60jCgMq28Ggm__ombQNfdhvYe-1qsOF1imso96y26PjhT3pSukVmy__RHkVdKOJRbMTTS_k0hEJfWexdcbY4QJ2QA/s16000/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">PI-ARS trains reliable visual-locomotion policies that are transferable to the real world.</td></tr></tbody></table>  <h2>Predictive Information</h2><p>A good representation for policy learning should be both <em>compressive</em>, so that ES can focus on solving a much lower dimensional problem than learning from raw observations would entail, and <em>task-critical</em>, so the learned controller has all the necessary information needed to learn the optimal behavior. For robotic control problems with high-dimensional input space, it is critical for the policy to understand the environment, including the dynamic information of both the robot itself and its surrounding objects.  </p><p>As such, we propose an observation encoder that preserves information from the raw input observations that allows the policy to predict the future states of the environment, thus the name <em>predictive information</em> (PI). More specifically, we optimize the encoder such that the encoded version of what the robot has seen and planned in the past can accurately predict what the robot might see and be rewarded in the future. One mathematical tool to describe such a property is that of <a href=\"https://en.wikipedia.org/wiki/Mutual_information\">mutual information</a>, which measures the amount of information we obtain about one random variable <em>X</em> by observing another random variable <em>Y</em>. In our case, <em>X</em> and <em>Y</em> would be what the robot saw and planned in the past, and what the robot sees and is rewarded in the future. Directly optimizing the mutual information objective is <a href=\"https://arxiv.org/pdf/1905.06922.pdf\">a challenging problem</a> because we usually only have access to samples of the random variables, but not their underlying distributions. In this work we follow <a href=\"https://arxiv.org/abs/2007.12401\">a previous approach</a> that uses <a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE</a>, a contrastive variational bound on mutual information to optimize the objective. </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s1988/image5.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"946\" data-original-width=\"1988\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP4JXHr2GcWAuGdptvFApp4KqavxmxmDKZ-_OUPBfAOE85mkXcVMNreXx4gZBUp9Hw58xzy0tjKex-m7Ca8xIWpYKXkxg4JFbnljFTpWRMknSi5_Ye8DlcSZiutk7YMVQAMRO7dJb7EmxVbK8Cd1TcSrw_z6oO5kgcEp3x_zLqmpmoBU0E8kaqWlr18w/s16000/image5.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left</b>: We use representation learning to encode PI of the environment. <b>Right</b>: We train the representation by replaying trajectories from the replay buffer and maximize the predictability between the observation and motion plan in the past and the observation and reward in the future of the trajectory.</td></tr></tbody></table>  <h2>Predictive Information with Augmented Random Search</h2><p>Next, we combine PI with <a href=\"https://arxiv.org/abs/1803.07055\">Augmented Random Search</a> (ARS), an algorithm that has shown excellent optimization performance for challenging decision-making tasks. At each iteration of ARS, it samples a population of perturbed controller parameters, evaluates their performance in the testing environment, and then computes a gradient that moves the controller towards the ones that performed better.  </p><p>We use the learned compact representation from PI to connect PI and ARS, which we call PI-ARS. More specifically, ARS optimizes a controller that takes as input the learned compact representation PI and predicts appropriate robot commands to achieve the task. By optimizing a controller with smaller input space, it allows ARS to find the optimal solution more efficiently. Meanwhile, we use the data collected during ARS optimization to further improve the learned representation, which is then fed into the ARS controller in the next iteration. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s1100/image1.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"750\" data-original-width=\"1100\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZRt3o2sJcpLqpk0o8cDA4qExoJlNdNeW6iZKBPXLCuyTEjrntvzV_HbXJcuqOGyPdDnjP1_AzxDi8HSEaw0vmF1G1OoTjIQmOhWOZabcgyviV9BKxMAYzmnm7elT3ymzFFXgdWytUFAI6y0uDNXJX3qxw8j8XGdRrbCNuwHvJrsmfSmJ2Spp9vgbJ5g/s16000/image1.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An overview of the PI-ARS data flow. Our algorithm interleaves between two steps: 1) optimizing the PI objective that updates the policy, which is the weights for the neural network that extracts the learned representation; and 2) sampling new trajectories and updating the controller parameters using ARS.</td></tr></tbody></table>  <h2>Visual-Locomotion for Legged Robots</h2><p>We evaluate PI-ARS on the problem of visual-locomotion for legged robots. We chose this problem for two reasons: visual-locomotion is a key bottleneck for legged robots to be applied in real-world applications, and the high-dimensional vision-input to the policy and the complex dynamics in legged robots make it an ideal test-case to demonstrate the effectiveness of the PI-ARS algorithm. A demonstration of our task setup in simulation can be seen below. Policies are first trained in simulated environments, and then transferred to hardware. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s480/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"270\" data-original-width=\"480\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuPw15yfYlg6428R8dkbMyEtYZSBAK27PN5HF4ym4bTFJAu4gfsOdmnaCsQZq3E08H85_v1Jo5zJIqM14KzdyEZ2kWWk4BKmvsYOZOzAqcf8mMMrGELgGnyjMGdvQ9y4rIeiUxLW8RfTPoKTDn0ZhQ2446F7ZwcARIBlApK9r0AMKPJ8DnXGCz6Nu0nQ/s16000/image2.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An illustration of the visual-locomotion task setup. The robot is equipped with two cameras to observe the environment (illustrated by the transparent pyramids). The observations and robot state are sent to the policy to generate a high-level motion plan, such as feet landing location and desired moving speed. The high-level motion plan is then achieved by a low-level Motion Predictive Control (MPC) controller.</td></tr></tbody></table>  <h2>Experiment Results</h2><p>We first evaluate the PI-ARS algorithm on four challenging simulated tasks: </p><ul> <li><em>Uneven stepping stones</em>: The robot needs to walk over uneven terrain while avoiding gaps.  </li><li><em>Quincuncial piles</em>: The robot needs to avoid gaps both in front and sideways.  </li><li><em>Moving platforms</em>: The robot needs to walk over stepping stones that are randomly moving horizontally or vertically. This task illustrates the flexibility of learning a vision-based policy in comparison to explicitly reconstructing the environment.  </li><li><em>Indoor navigation</em>: The robot needs to navigate to a random location while avoiding obstacles in an indoor environment. </li></ul><p>As shown below, PI-ARS is able to significantly outperform ARS in all four tasks in terms of the total task reward it can obtain (by 30-50%). </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s1417/image11.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1417\" data-original-width=\"900\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj66o8DzwYZV_nCYvp-On0X4nTtmUCQJTdnizSuKvwZdF7XYROIS2VN8E9QunXB4WE7VgPth9RykGhpPIhXYrtdryUyUZHRTqZbdkJIsS9SYAGMfXW4Xb_2pZjG-fqaFVf2P6Jz089mv9qLbAx59DzFNz1XIERlnZZ2u81m3h2obr0skf3IEmkKJLjg_w/s16000/image11.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left: </b>Visualization of PI-ARS policy performance in simulation. <b>Right: </b>Total task reward (i.e., episode return) for PI-ARS (<b>green line</b>) and ARS (<b>red line</b>). The PI-ARS algorithm significantly outperforms ARS on four challenging visual-locomotion tasks.</td></tr></tbody></table>   <p>We further deploy the trained policies to a real <a href=\"http://www.unitree.cc/e/action/ShowInfo.php?classid=6&amp;id=1\">Laikago</a> robot on two tasks: <a href=\"https://youtu.be/59vMC3fTsuQ?t=20\">random stepping stone</a> and <a href=\"https://youtu.be/59vMC3fTsuQ?t=43\">indoor navigation</a>. We demonstrate that our trained policies can successfully handle real-world tasks. Notably, the success rate of the random stepping stone task improved from 40% in <a href=\"https://openreview.net/forum?id=NDYbXf-DvwZ\">the prior work</a> to 100%. </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s480/image1.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"270\" data-original-width=\"480\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiw71HD7nnemeCE7GWYPQlGx1invJqNhOpy16wwoYjKP0PQ89nxPlx9uIEDtVVajh5_gy9_Rq2k0gQtVyMHN7ivFBjhGHWlqwtFercs_Il9jnwVHQq_EwxyQTmOJVgjEqbij1dyvGxj7Jp18J1RhzpN-yyGcNwvj16KMeUJj4UtKNPO9YiFmRNzwctlFg/s16000/image1.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">PI-ARS trained policy enables a real Laikago robot to navigate around obstacles.</td></tr></tbody></table>  <h2>Conclusion</h2><p>In this work, we present a new learning algorithm, PI-ARS, that combines gradient-based representation learning with gradient-free evolutionary strategy algorithms to leverage the advantages of both. PI-ARS enjoys the effectiveness, simplicity, and parallelizability of gradient-free algorithms, while relieving a key bottleneck of ES algorithms on handling high-dimensional problems by optimizing a low-dimensional representation. We apply PI-ARS to a set of challenging visual-locomotion tasks, among which PI-ARS significantly outperforms the state of the art. Furthermore, we validate the policy learned by PI-ARS on a real quadruped robot. It enables the robot to walk over randomly-placed stepping stones and navigate in an indoor space with obstacles. Our method opens the possibility of incorporating modern large neural network models and large-scale data into the field of evolutionary strategy for robotics control. </p><div style=\"line-height:40%;\">    <br>  </div><h2>Acknowledgements</h2><p><em>We would like to thank our paper co-authors: Ofir Nachum, Tingnan Zhang, Sergio Guadarrama, and Jie Tan. We would also like to thank Ian Fischer and John Canny for valuable feedback.</em></p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}