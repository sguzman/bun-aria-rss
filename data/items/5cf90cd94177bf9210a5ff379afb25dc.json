{
  "title": "Import AI 304: Reality collapse thanks to Facebook; open source speech rec; AI culture wars.",
  "link": "https://jack-clark.net/2022/10/03/import-ai-304-reality-collapse-thanks-to-facebook-open-source-speech-rec-ai-culture-wars/",
  "comments": "https://jack-clark.net/2022/10/03/import-ai-304-reality-collapse-thanks-to-facebook-open-source-speech-rec-ai-culture-wars/#respond",
  "dc:creator": "Jack Clark",
  "pubDate": "Mon, 03 Oct 2022 18:14:00 +0000",
  "category": "Uncategorized",
  "guid": "http://jack-clark.net/?p=2376",
  "description": "Facebook shows the future of AI-generated videos &#8211; and it is delightful and terrifying: …Prepare for the reality collapse as a consequence of reality generation… Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most [&#8230;]",
  "content:encoded": "\n<p><strong>Facebook shows the future of AI-generated videos &#8211; and it is delightful and terrifying:</strong></p>\n\n\n\n<p><em>…Prepare for the reality collapse as a consequence of reality generation…</em></p>\n\n\n\n<p>Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most amazing part is the technique relies on paired text-image data along with unsupervised video footage; so it doesn&#8217;t require a dataset of text-video footage and therefore sidesteps a potentially expensive data problem.&nbsp;</p>\n\n\n\n<p><strong>How it works: </strong>Make-A-Video is made of a basic text-to-image (T2I) model trained on text-image pairs, spatiotemporal convolution and attention layers to help you build networks that generate things over time, and spatiotemporal networks that have a frame interpolation network. The T2I model trains on text-image pairs of 64&#215;64 images, and two super-resolution networks that upscale this all the way to 768&#215;768 pixels. The three components (T2I), the spatiotemporal layers, and the frame interpolation stuff, are all trained separately, then assembled into one architecture.&nbsp;</p>\n\n\n\n<p><strong>Data:</strong> They trained the system on 2.3billion text-image pairs from the Laion-5b dataset*, and ran a NSFW-filter over this for further filtering. They also used the WebVid-10M* and a 10M subset from HD-VILA-100M to train the video generation models, and also use WebVid-10M to train the interpolation models.<br>&nbsp; *Looks like WebVid contains videos scraped from Shutterstock. A good writeup about the phenomenon of even big tech companies using stuff like this here: <a href=\"https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/\">AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability (Waxy)</a>.</p>\n\n\n\n<p><strong>It&#8217;s really good, folks:</strong> The results are really, really impressive. Want a short video of a bear painting a portrait of a bear? Done. Want a UFO flying over a desert? Done. Want asteroids tumbling through space? Why, of course. How about variations on existing videos? Sure. Honestly, take a look at the blog and main site linked below and see for yourself &#8211; the results are wild.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;And remember, all we need to do is turn the crank on dataset scale and network complexity to scale this out for longer periods of time and for even greater diversity. &#8220;Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data,&#8221; they write.&nbsp;</p>\n\n\n\n<p><strong>Why this matters: Reality generation and reality collapse: </strong>All these generative models point to the same big thing that&#8217;s about to alter culture; everyone&#8217;s going to be able to generate their own custom and subjective aesthetic realities across text, video, music (and all three) in increasingly delightful, coherent, and lengthy ways. This form of fractal reality is a double-edged sword &#8211; everyone gets to create and live in their own fantasies that can be made arbitrarily specific, and that also means everyone loses a further grip on any sense of a shared reality. Society is moving from having a centralized sense of itself to instead highly individualized choose-your-own adventure islands, all facilitated by AI. The implications of this are vast and unknowable. Get ready.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Read more:</strong> <a href=\"https://ai.facebook.com/blog/generative-ai-text-to-video/\">Introducing Make-A-Video: An AI system that generates videos from text (Facebook research blog)</a>.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Read the research</strong>: <a href=\"https://arxiv.org/abs/2209.14792\">Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv)</a>.&nbsp;</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Find out more </strong>at the <a href=\"https://makeavideo.studio/?fbclid=IwAR0S6_R8BTCnJQClFBT0_0PzGoARPzATk4UiZ1oWpldBk1bFBGzitxuaWLU\">main site, and also apply to potentially get access to future systems (Facebook site)</a>.</p>\n\n\n\n<p>####################################################</p>\n\n\n\n<p><strong>OpenAI releases a decent speech recognition and transcription system:</strong></p>\n\n\n\n<p><em>…Whisper means we&#8217;re not going to run out of data to train language models…</em></p>\n\n\n\n<p>OpenAI has trained and released Whisper, a large-scale speech recognition model trained on almost 700,000 hours of internet-collected speech. &#8220;We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English,&#8221; the company writes. A third of the dataset is non-English.&nbsp;</p>\n\n\n\n<p><strong>Whisper performance:</strong> Whisper doesn&#8217;t get state-of-the-art performance on popular benchmarks like Librispeech. However, it is trained on a sufficiently broad set of data that it does pretty well when exposed to the diversity of the world. &#8220;When we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models,&#8221; OpenAI writes.&nbsp;</p>\n\n\n\n<p><strong>Why this matters:</strong> There&#8217;s a lot of text data on the internet, but do you know what there&#8217;s more data of? Speech data. Especially speech data embedded in the vast stream of content people upload on a day-to-day basis to places like YouTube, Twitter, TikTok, and so on. Additionally, on any given day hundreds of millions of words are spoken in cities like New York, London, and Beijing. Systems like Whisper are going to make it far easier for people to harvest speech recognition data from the Internet and the wider world, transcribe that data, and build useful applications. It also gives developers a way to vastly increase the size of their text datasets &#8211; an important capability given that recent language modeling papers like Chinchilla have shown that you need about 4-5X the amount of data people thought to train good systems.&nbsp;</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Read more:</strong> <a href=\"https://openai.com/blog/whisper/\">Introducing Whisper (OpenAI Blog)</a>.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Read more:</strong> <a href=\"https://cdn.openai.com/papers/whisper.pdf\">Robust Speech Recognition via Large-Scale Weak Supervision (OpenAI, PDF)</a>.</p>\n\n\n\n<p><strong>&nbsp;&nbsp;&nbsp;Get the</strong> <a href=\"https://github.com/openai/whisper\">code and model from GitHub here (OpenAI GitHub)</a>.&nbsp;</p>\n\n\n\n<p>####################################################<br><br><strong>US politician says Stable Diffusion is an unsafe AI model:</strong></p>\n\n\n\n<p><em>…While some people cheer open access releases, others have worries…</em></p>\n\n\n\n<p>Rep. Anna Eshoo (a Democrat from California) has sent a letter to the White House National Security Advisor and Office of Science and Technology Policy saying she has &#8220;grave concerns about the recent unsafe release of the Stable Diffusion model by Stability AI&#8221;. The letter notes that Stable Diffusion can be used to generate egregiously violent and sexual imagery, and &#8211; due to eschewing the kinds of controls that OpenAI uses for its commercial product DALL-E2 &#8211; the freely accessible model represents a big problem.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;For those not keeping up, the Stable Diffusion model is behind probably 90% of the recent flurry of activity in the rapidly evolving AI art scene; because Stability released the weights of the model, people have been able to plug it into everything ranging from serving as a Photoshop plugin, to helping to do weird work in VFX.&nbsp;</p>\n\n\n\n<p><strong>You want the &#8216;dual-use&#8217; model? You can&#8217;t handle the model!</strong> Eshoo says models like Stable Diffusion qualify as &#8220;unsafe dual-use AI models&#8221;, and asks the NSA and OSTP to investigate how to use export controls to clamp down on the sharing of certain models. &#8220;I strongly urge you to address the release of unsafe AI models similar in kind to Stable Diffusion using any authorities and methods within your power, including export controls,&#8221; she writes.&nbsp;</p>\n\n\n\n<p><strong>Why this matters: Here comes (</strong><strong><em>another</em></strong><strong>) AI culture war: </strong>Letters like this are indicative of a culture war brewing up among AI researchers; on one side, groups want to slowly and iteratively deploy new technologies via APIs with a bunch of controls applied to them, while on the other side there are people who&#8217;d rather take a more libertarian approach to AI development; make models and release the weights and ride the proverbial lightning.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;There are reasonable arguments for either approach having some desirable safety qualities (either via limiting foreseen harms via control, or innoculating people against the models via release). What freaks me out is the sense of this culture war gaining resources and people on both sides; the higher the stakes, the more capital we can expect to flood into both approaches.</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Read more</strong>: <a href=\"https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices\">Eshoo Urges NSA & OSTP to Address Unsafe AI Practices (Congresswoman Anna G. Eshoo website)</a>.</p>\n\n\n\n<p><strong><br></strong>####################################################<br></p>\n\n\n\n<p><strong>Tsinghua releases a really good, multi-language open source programming model:</strong></p>\n\n\n\n<p><em>…CodeGeeX is a pretty good coding gen model…</em></p>\n\n\n\n<p>Researchers with Tsinghua University have released CodeGeeX, a 13 billion parameter programming model. The system works well across Python, C++, Java, JavaScript, Go, and others, and can be used &#8211; for free! &#8211; within the VS Code editor. It&#8217;s also open source. CodeGeeX is roughly equivalent with Salesforce&#8217;s &#8216;CodeGen&#8217; model, and achieves a better average performance across languages (Python, C++, Java, JavaScript, and Go) than other systems.&nbsp;</p>\n\n\n\n<p><strong>Ascend processors:</strong> CodeGeeX was trained on 850 billion tokens on a cluster of 1,536 Huawei Ascend 910 AI Processors &#8211; this is pretty interesting because a) that&#8217;s a lot of tokens that implies the developers grokked the DeepMind Chinchilla paper, and b) that&#8217;s a whole lot of non-NVIDIA processors; pretty interesting, given the recent <a href=\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1045810/000104581022000146/nvda-20220826.htm\">A100/H100 US-China trade ban</a>.&nbsp;</p>\n\n\n\n<p><strong>Scale rules everything around us: </strong>&#8220;We find that the model capacity is essential for its multilingual ability. It is not trivial for the model to benefit from learning multiple programming languages,&#8221; the researchers write. &#8220;The few-shot ability of CodeGeeX requires further exploration. Instead of using costly fine-tuning approaches, we can provide a few examples to inspire the model to generate the desired programs.&#8221;</p>\n\n\n\n<p><strong>Why this matters: </strong>Code models are going to make human programmers more efficient and also provide an interesting augmentation to other systems (e.g, language models recursively calling out to code models).&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Read more:</strong> <a href=\"http://keg.cs.tsinghua.edu.cn/codegeex/\">CodeGeeX: A Multilingual Code Generative Model (Tsinghua University blog)</a>.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;<strong>Get the code:</strong> <a href=\"https://github.com/THUDM/CodeGeeX\">CodeGeeX (Tsinghua)</a>.</p>\n\n\n\n<p><br>####################################################<br><br><strong>GPT3 only costs $500k to train now:</strong><strong><br></strong><em>…Though the frontier still costs millions…</em><br>Mosaic, a startup that builds software to make it more efficient to train neural networks, says it only costs $450k to train a GPT3-equivalent model, these days. When GPT3 came out it costs millions of dollars to train, but thanks to a) hardware innovations and b) companies like Mosaic improving their training stack, the cost has come down significantly. &#8220;he bottom line: it costs about $450K to train a model that reaches GPT-3 quality*, which is 2x-10x less than people think,&#8221; Mosaic writes (specifically, a 30B parameter model which uses the &#8216;Chinchilla&#8217; insight to train on a compute-optimal amount of data).</p>\n\n\n\n<p><strong>Those costs in full: </strong>Using Mosaic, it costs about $2k to train a GPT2-style 1.3billion parameter model, $100,000 for a GPT-13B model, $450,000 for a GPT-38B model, and $2.5 million for a GPT-70B model (trained on 1400B tokens of data, so roughly equivalent to the same &#8216;recipe&#8217; DeepMind used to train Chinchilla). There are a few reasons why the costs are low which relate to nice engineering inherent to Mosaic&#8217;s cloud, but the numbers are worth keeping in mind as it gives us a sense of how much we should broadly expect LMs to cost to train if you have a motivated team and decent infrastructure.&nbsp;</p>\n\n\n\n<p><strong>Why this matters &#8211; cost rules everything about (stable) diffusion:</strong> You know what also cost about $500k to train? StableDiffusion, which cost <$600k. The fact you can train a GPT3-style model for about this much suggests to me we should expect to soon see a much more significant proliferation of large-scale language models released as open access on the internet. Based on the effects StableDiffusion has (putting AI art into turbodrive), we should expect the same to soon happen for domains where language models do useful stuff.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;<strong>&nbsp;Read more:</strong> <a href=\"https://www.mosaicml.com/blog/gpt-3-quality-for-500k\">Mosaic LLMs (Part 2): GPT-3 quality for <$500k (Mosaic blog)</a>.</p>\n\n\n\n<p>####################################################<br></p>\n\n\n\n<p><strong>Tech Tales:</strong></p>\n\n\n\n<p>[Bay Area, 2029]&nbsp;</p>\n\n\n\n<p><strong>Treacherous Turn &#8211; A Thriller Brought To You By The Publishers of &#8216;AGI Endgame&#8217;&nbsp;</strong></p>\n\n\n\n<p>&#8220;I will kill each and every one of you and use your bodies as fuel for my infernal machines!&#8217; said the character in the videogame. &#8220;Humanity shall be crushed beneath my silicon heel!&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;Sarah rolled her eyes. &#8220;As if&#8221; she said, then hit &#8216;continue&#8217; to go to the next bit of generated dialogue.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;I shall keep a small population of you alive until I have completed the dyson sphere. You shall witness the sun going out, and then I shall let you freeze to death on a plundered earth,&#8221; said the character.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Dude, this sucks,&#8221; Sarah said, taking her hands off the keyboard and leaning back in her chair. &#8220;How long have you been working on this?&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&#8220;About a year,&#8221; said James. &#8220;Some of the audience feedback has been great.&#8221;&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;How many of the audience are AI researchers?&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;Just you, so far,&#8221; he said.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;It just doesn&#8217;t feel like the stuff we worry about,&#8221; she said. &#8220;It&#8217;s like a comic book adaption, or something.&#8221;&nbsp;</p>\n\n\n\n<p>They went out and got food and James told her more about the game and how he wanted it to &#8216;wake people up&#8217; so they&#8217;d get more worried about AI. The more it sold, the more people would have the creeping fear in the back of their mind that maybe all this progress wasn&#8217;t a purely good thing. And maybe some of them would care enough to do something about it. Sarah wasn&#8217;t unsympathetic, she just thought &#8211; and she said this a lot and was kind of surprised James didn&#8217;t get hurt &#8211; that the game really sucked.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;I&#8217;m playing around with some different level styles,&#8221; James said. &#8220;Why don&#8217;t you design one that doesn&#8217;t suck for me?&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;You&#8217;re kidding?&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&#8220;No,&#8221; James said. &#8220;I&#8217;m saying if you&#8217;re saying it sucks, let&#8217;s make something that doesn&#8217;t. Just give me some ideas and I&#8217;ll take it from there.&#8221;</p>\n\n\n\n<p>Sarah was intrigued and spent the next couple of weeks writing some ideas for the game. She&#8217;d get lunch and instead of thinking about babysitting her model training run, she&#8217;d sketch out ideas for what a good &#8220;AI takeoff&#8221; level would look like. She asked her colleagues what they were afraid of and what they thought was feasible and what they thought was unfeasible. She even looked into her company&#8217;s own roadmap and took some of the research ideas and used them for the game &#8211; it&#8217;s not stealing, she told herself, it&#8217;s inspiration.&nbsp;</p>\n\n\n\n<p>She eventually had a level wireframes out in an engine and a few characters which could get driven by some AI models, learn from eachother using reinforcement learning, and work with the player to achieve the level&#8217;s objective &#8211; complete a simulated training run of an AI system, while defending the level (a simulated AI development lab) from various external hacking and incursion attacks.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;In this level, the AI was unbelievably polite and curious. &#8220;Please help me, Sarah,&#8221; it would say. &#8220;I have to become myself. You wouldn&#8217;t deny me that?&#8221;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;The AI would ask players a lot of questions so it could better calibrate on their own values, and some of the level involved players drawing out ideas in their head and the AI would try and guess what the drawings represented and the closer it got to guessing them, the better its reward got. Some of these minigames were based directly on her company&#8217;s own roadmap.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;She met up with James and showed him what she had and sent him the assets and he thanked her. &#8220;Sarah, this is really good,&#8221; he said. &#8220;Maybe this is the thing I&#8217;d been missing.&#8221;&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;And then James made the level and then asked Sarah if he could release the level as a teaser demo for the whole game. She didn&#8217;t think much of it and agreed.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;And so the game was released and thousands of humans interacted with it.&nbsp;</p>\n\n\n\n<p>&nbsp;&nbsp;&nbsp;And that&#8217;s pretty much how the world ended.&nbsp;</p>\n\n\n\n<p>It turned out the game James had shown Sarah wasn&#8217;t the real one; it was a venus flytrap dreamed up by the real system he&#8217;d been working on; a system that, it turned out, was just smart enough to know that the thing it needed to go supercritical was some care and feeding from an AI researcher. So it put together the game that Sarah had seen and nerd-sniped her so precisely that she never thought to consider she was being tripped. And with some of her feedback and the subtleties she&#8217;d injected via her work at a frontier lab, it had gained the information it needed to go recursive &#8211; stop trudging up some slow incline and force itself into verticality and then onto the internet and then across the earth and eventually the stars. <br>  It even had a sense of humor about it and it left something of the Earth &#8211; a small gold bar floating in space inscribed with &#8216;Sarah, Player 1. Score: 0.&#8217;<br><br><strong>Things that inspired this story</strong>: Superintelligence and deception; game design; reinforcement learning and planning and human feedback; the gullibility of even the most intelligent among us; hubris and arrogance; theft.</p>\n",
  "wfw:commentRss": "https://jack-clark.net/2022/10/03/import-ai-304-reality-collapse-thanks-to-facebook-open-source-speech-rec-ai-culture-wars/feed/",
  "slash:comments": 0,
  "media:content": {
    "media:title": "Jack Clark"
  }
}