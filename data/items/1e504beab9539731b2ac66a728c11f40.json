{
  "title": "The Statistical Whitening Transform",
  "link": "https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/",
  "comments": "https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/#comments",
  "dc:creator": "dustinstansbury",
  "pubDate": "Sat, 30 Mar 2013 08:24:09 +0000",
  "category": [
    "Data Preprocessing",
    "Derivations",
    "Statistics",
    "Center-surround Receptive Field",
    "convolution",
    "Covariance Matrix",
    "Diagonal matrix",
    "Eigenvalue Decomposition",
    "Eigenvalues",
    "Eigenvectors",
    "Identity Covariance",
    "identity matrix",
    "Independence",
    "Preprocessing",
    "Principal Components Analysis (PCA)",
    "Retinal Ganglion Cell",
    "Statistical Independence",
    "Statistical Whitening"
  ],
  "guid": "http://theclevermachine.wordpress.com/?p=3588",
  "description": "In a number of modeling scenarios, it is beneficial to transform the to-be-modeled data such that it has an identity covariance matrix, a procedure known as Statistical Whitening. When data have an identity covariance, all dimensions are statistically independent, and the variance of the data along each of the dimensions is equal to one. (To [&#8230;]",
  "content:encoded": "<p>In a number of modeling scenarios, it is beneficial to transform the to-be-modeled data such that it has an identity covariance matrix, a procedure known as <em><strong>Statistical Whitening</strong></em>. When data have an identity covariance, all dimensions are statistically independent, and the variance of the data along each of the dimensions is equal to one. (To get a better idea of what an identity covariance entails, see the following <a title=\"Covariance Matrices\" href=\"https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/\" target=\"_blank\">post</a>.)</p>\n<p>Enforcing statistical independence is useful for a number of reasons. For example, in probabilistic models of data that exist in multiple dimensions, the joint distribution&#8211;which may be very complex and difficult to characterize&#8211;can factorize into a product of many simpler distributions when the dimensions are statistically independent. Forcing all dimensions to have unit variance is also useful. For instance, scaling all variables to have the same variance treats each dimension with equal importance.</p>\n<p>In the remainder of this post we derive how to transform data such that it has an identity covariance matrix, give some examples of applying such a transformation to real data, and address some interpretations of statistical whitening in the scope of theoretical neuroscience.</p>\n<h2>Decorrelation: Transforming Data to Have a Diagonal Covariance Matrix</h2>\n<p>Let&#8217;s say we have some data matrix <img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> composed of <img src=\"https://s0.wp.com/latex.php?latex=K&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"K\" class=\"latex\" /> dimensions and <img src=\"https://s0.wp.com/latex.php?latex=n&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"n\" class=\"latex\" /> observations (<img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> has  size <img src=\"https://s0.wp.com/latex.php?latex=%5BK+%5Ctimes+n%5D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"[K &#92;times n]\" class=\"latex\" />).  Let&#8217;s also assume that the rows of <img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> have been centered (the mean has been subracted across all observations) . The covariance <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma\" class=\"latex\" /> of each of the dimensions with respect to the other is</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CSigma+%3D+Cov%28X%29+%3D+%5Cmathbb+E%5BX+X%5ET%5D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma = Cov(X) = &#92;mathbb E[X X^T]\" class=\"latex\" />                                                                                        (1)</p>\n<p>Where the covariance <img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb+E%5BX+X%5ET%5D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb E[X X^T]\" class=\"latex\" /> can be estimated from the data matrix as follows:</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=%5Cmathbb+E%5BX+X%5ET%5D+%5Capprox+%5Cfrac%7BX+X%5ET%7D%7Bn%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;mathbb E[X X^T] &#92;approx &#92;frac{X X^T}{n}\" class=\"latex\" />                                                                                            (2)</p>\n<p>The covariance matrix <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma\" class=\"latex\" />, by definition (Equation 2) is symmetric and positive semi-definite (if you don&#8217;t know what that means, don&#8217;t worry it&#8217;s not terribly important for this discussion). Thus we can write the matrix as the product of two simpler matrices <img src=\"https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" />, using a procedure known as <em><strong>Eigenvalue Decomposition:</strong></em></p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=%5CSigma+%3D+EDE%5E%7B-1%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma = EDE^{-1}\" class=\"latex\" />                                                                                                 (3)</p>\n<p>The matrix <img src=\"https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E\" class=\"latex\" /> is an <img src=\"https://s0.wp.com/latex.php?latex=%5BK+%5Ctimes+K%5D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"[K &#92;times K]\" class=\"latex\" />-sized matrix, where each column is an eigenvector of <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma\" class=\"latex\" />, and <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" /> is a diagonal matrix whose diagonal elements <img src=\"https://s0.wp.com/latex.php?latex=D_%7Bii%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D_{ii}\" class=\"latex\" /> are eigenvalues that correspond to the eigenvectors of the <img src=\"https://s0.wp.com/latex.php?latex=i&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"i\" class=\"latex\" />-th column of <img src=\"https://s0.wp.com/latex.php?latex=E&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E\" class=\"latex\" />.  For more details on eigenvectors and eigenvalues see the <a title=\"Eigenvalues/eigenvectors\" href=\"http://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\" target=\"_blank\">following</a>. From Equation (3), and using a little algebra, we can transform <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma\" class=\"latex\" /> into the diagonal matrix <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" /></p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=E%5E%7B-1%7D+%5CSigma+E+%3D+D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E^{-1} &#92;Sigma E = D\" class=\"latex\" />                                                                                                 (4)</p>\n<p>Now, imagine the goal is to transform the data matrix <img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> into a new data matrix <img src=\"https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Y\" class=\"latex\" /></p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=Y+%3D+W_DX&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Y = W_DX\" class=\"latex\" />                                                                                                   (5)</p>\n<p>whose dimensions are uncorrelated (i.e. <img src=\"https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Y\" class=\"latex\" /> has a diagonal covariance <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" />). Thus we want to determine the transformation <img src=\"https://s0.wp.com/latex.php?latex=W_D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_D\" class=\"latex\" /> that makes:</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D+%3D+Cov%28Y%29+%3D+%5Cmathbb+E%5BYY%5ET%5D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D = Cov(Y) = &#92;mathbb E[YY^T]\" class=\"latex\" />                                                                                   (6)</p>\n<p>Here we derive the expression for <img src=\"https://s0.wp.com/latex.php?latex=W_D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_D\" class=\"latex\" /> using Equations (2), (4), (5), and (6):</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D+%3D+%5Cfrac%7BW_DX%28W_DX%29%5ET%7D%7Bn%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D = &#92;frac{W_DX(W_DX)^T}{n}\" class=\"latex\" />                                                       (a la Equations (5) and (6))</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D+%3D+W_D+W_D%5ET+%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D = W_D W_D^T &#92;Sigma\" class=\"latex\" />                                                                       (via Equation (2))</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=E%5E%7B-1%7D%5CSigma+E+%3D+W_D+W_D%5ET+%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E^{-1}&#92;Sigma E = W_D W_D^T &#92;Sigma\" class=\"latex\" />                                                                   (via Equation (4))</p>\n<p style=\"text-align:center;\">        <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma%5E%7B-1%7DE%5E%7B-1%7D+%5CSigma+E+%3D+%5CSigma%5E%7B-1%7DW_D+W_D%5ET+%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma^{-1}E^{-1} &#92;Sigma E = &#92;Sigma^{-1}W_D W_D^T &#92;Sigma\" class=\"latex\" /></p>\n<p style=\"text-align:right;\">now, because <img src=\"https://s0.wp.com/latex.php?latex=E%5E%7B-1%7D+%3D+E%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E^{-1} = E^T\" class=\"latex\" />                                             (see following <a title=\"Normal Matrices\" href=\"http://en.wikipedia.org/wiki/Spectral_theorem\" target=\"_blank\">link</a> for details)</p>\n<p style=\"text-align:center;\">            <img src=\"https://s0.wp.com/latex.php?latex=E%5ETE+%3D+W_DW_D%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E^TE = W_DW_D^T\" class=\"latex\" /> and thus</p>\n<p style=\"text-align:right;\">   <img src=\"https://s0.wp.com/latex.php?latex=W_D+%3D+E%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_D = E^T\" class=\"latex\" />                                                                                                   (7)</p>\n<p>This means that we can transform <img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> into an uncorrelated (i.e. orthogonal) set of variables by premultiplying data matrix <img src=\"https://s0.wp.com/latex.php?latex=X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"X\" class=\"latex\" /> with the transpose of the the eigenvectors of data covariance matrix <img src=\"https://s0.wp.com/latex.php?latex=%5CSigma&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"&#92;Sigma\" class=\"latex\" />.</p>\n<h2>Whitening: Transforming data to have an Identity Covariance matrix</h2>\n<p>Ok, so now we have a way of transforming our data so that the dimensions are uncorrelated. However, this only gives us a diagonal covariance matrix, not an Identity covariance matrix. In order to obtain an Identity covariance, we also need to scale each dimension so that its variance is equal to one. How can we determine this transformation? We know how to transform our data so that the covariance is equal to <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" />. If we can determine the transformation that leaves <img src=\"https://s0.wp.com/latex.php?latex=D+%3D+I&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D = I\" class=\"latex\" />, then we can apply this transformation to our decorrelated covariance to give us the desired whitening transform. We can determine this from the somewhat trivial notion that</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D%5E%7B-1%7DD+%3D+I&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D^{-1}D = I\" class=\"latex\" />                                                                                                        (8)</p>\n<p>and further that</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D%5E%7B-1%7D+%3D+D%5E%7B-1%2F2%7DID%5E%7B-1%2F2%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D^{-1} = D^{-1/2}ID^{-1/2}\" class=\"latex\" />                                                                                             (9)</p>\n<p>Now, using Equation (4) along with Equation (8), we can see that</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=D%5E%7B-1%2F2%7DE%5E%7B-1%7D%5CSigma+E+D%5E%7B-1%2F2%7D+%3D+I&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D^{-1/2}E^{-1}&#92;Sigma E D^{-1/2} = I\" class=\"latex\" />                                                                                      (10)</p>\n<p>Now say that we define a variable <img src=\"https://s0.wp.com/latex.php?latex=Y+%3D+W_W+X&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Y = W_W X\" class=\"latex\" />, where <img src=\"https://s0.wp.com/latex.php?latex=W_W&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_W\" class=\"latex\" /> is the desired whitening transform, that leaves the covariance of <img src=\"https://s0.wp.com/latex.php?latex=Y&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"Y\" class=\"latex\" /> equal to the identity matrix. Using essentially the same set of derivation steps as above to solve for <img src=\"https://s0.wp.com/latex.php?latex=W_D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_D\" class=\"latex\" />, but starting from Equation (9) we find that</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=W_W+%3D+D%5E%7B-1%2F2%7DE%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W_W = D^{-1/2}E^T\" class=\"latex\" />                                                                                                  (11)</p>\n<p style=\"text-align:right;\"><img src=\"https://s0.wp.com/latex.php?latex=%3D+D%5E%7B-1%2F2%7DW_D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"= D^{-1/2}W_D\" class=\"latex\" />                                                                                                 (12)</p>\n<p>Thus, the whitening transform is simply the decorrelation transform, but scaled by the inverse of the square root of the <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" /> (here the inverse and square root can be performed element-wise because <img src=\"https://s0.wp.com/latex.php?latex=D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D\" class=\"latex\" /> is a diagonal matrix).</p>\n<h2>Interpretation of the Whitening Transform</h2>\n<p>So what does the whitening transformation actually do to the data (below, blue points)? We investigate this transformation below: The first operation decorrelates the data by premultiplying the data with the eigenvector matrix <img src=\"https://s0.wp.com/latex.php?latex=E%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"E^T\" class=\"latex\" />, calculated from the data covariance. This decorrelation can be thought of as a rotation that reorients the data so that the principal axes of the data are aligned with the axes along which the data has the largest (orthogonal) variance. This rotation is essentially the same procedure as the oft-used <em><strong>Principal Components Analysis (PCA)</strong></em>, and is shown in the middle row.</p>\n<p style=\"text-align:center;\"><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png\"><img loading=\"lazy\" data-attachment-id=\"3737\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whitening-whiteningtransform-2/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png\" data-orig-size=\"255,803\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whitening-whiteningTransform\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png?w=95\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png?w=255\" class=\"aligncenter  wp-image-3737\" alt=\"whitening-whiteningTransform\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png?w=179&#038;h=562\" width=\"179\" height=\"562\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png?w=179&h=562 179w, https://theclevermachine.files.wordpress.com/2013/03/whitening-whiteningtransform1.png?w=48&h=150 48w\" sizes=\"(max-width: 179px) 100vw, 179px\" /></a></p>\n<p>The second operation, scaling by <img src=\"https://s0.wp.com/latex.php?latex=D%5E%7B-1%2F2%7D&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"D^{-1/2}\" class=\"latex\" /> can be thought of squeezing the data&#8211;if the variance along a dimension is larger than one&#8211;or stretching the data&#8211;if the variance along a dimension is less than one. The stretching and squeezing forms the data into a sphere about the origin (which is why whitening is also referred to as &#8220;sphering&#8221;). This scaling operation is depicted in the bottom row in the plot above.</p>\n<p>The MATLAB to make make the plot above is here:</p>\n<pre class=\"brush: matlabkey; collapse: true; light: false; title: ; toolbar: true; notranslate\">\n% INITIALIZE SOME CONSTANTS\nmu = [0 0];\nS = [1 .9; .9 3];\n\n% SAMPLE SOME DATAPOINTS\nnSamples = 1000;\nsamples = mvnrnd(mu,S,nSamples)';\n\n% WHITEN THE DATA POINTS...\n[E,D] = eig(S);\n\n% ROTATE THE DATA\nsamplesRotated = E*samples;\n\n% TAKE D^(-1/2)\nD = diag(diag(D).^-.5);\n\n% SCALE DATA BY D\nsamplesRotatedScaled = D*samplesRotated;\n\n% DISPLAY\nfigure;\n\nsubplot(311);\nplot(samples(1,:),samples(2,:),'b.')\naxis square, grid\nxlim([-5 5]);ylim([-5 5]);\ntitle('Original Data');\n\nsubplot(312);\nplot(samplesRotated(1,:),samplesRotated(2,:),'r.'),\naxis square, grid\nxlim([-5 5]);ylim([-5 5]);\ntitle('Decorrelate: Rotate by V');\n\nsubplot(313);\nplot(samplesRotatedScaled(1,:),samplesRotatedScaled(2,:),'ko')\naxis square, grid\nxlim([-5 5]);ylim([-5 5]);\ntitle('Whiten: scale by D^{-1/2}');\n</pre>\n<p>The transformation in Equation (11) and implemented above  whitens the data but leaves the data aligned with principle axes of the original data. In order to observe the data in the original space, it is often customary &#8220;un-rotate&#8221; the data back into it&#8217;s original space. This is done by just multiplying the whitening transform by the inverse of the rotation operation defined by the eigenvector matrix. This gives the whitening transform:</p>\n<p style=\"text-align:right;\"><span style=\"text-align:right;font-size:13px;line-height:19px;\"><img src=\"https://s0.wp.com/latex.php?latex=W+%3DE%5E%7B-1%7DD%5E%7B-1%2F2%7DE%5ET&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W =E^{-1}D^{-1/2}E^T\" class=\"latex\" />                                                                                                   (13)</span></p>\n<p style=\"text-align:left;\">Let&#8217;s take a look an example of using statistical whitening for a more complex problem: whitening patches of images sampled from natural scenes.</p>\n<h2>Example: Whitening Natural Scene Image Patches</h2>\n<p>Modeling the local spatial structure of pixels in natural scene images is important in many fields including computer vision and computational neuroscience. An interesting model of natural scenes is one that can account for interesting, high-order statistical dependencies between pixels. However, because natural scenes are generally composed of continuous objects or surfaces, a vast majority of the spatial correlations in natural image data can be explained by local pairwise dependencies. For example, observe the image below.</p>\n<pre class=\"brush: matlabkey; title: ; notranslate\">\n% LOAD AND DISPLAY A NATURAL IMAGE\nim = double(imread('cameraman.tif'));\nfigure\nimagesc(im); colormap gray; axis image; axis off;\ntitle('Base Image')\n</pre>\n<p style=\"text-align:center;\"><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png\"><img loading=\"lazy\" data-attachment-id=\"3649\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whitening-base/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png\" data-orig-size=\"416,451\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whitening-Base\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=277\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=416\" class=\"aligncenter  wp-image-3649\" alt=\"whitening-Base\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=291&#038;h=316\" width=\"291\" height=\"316\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=291&h=316 291w, https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=138&h=150 138w, https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png?w=277&h=300 277w, https://theclevermachine.files.wordpress.com/2013/03/whitening-base.png 416w\" sizes=\"(max-width: 291px) 100vw, 291px\" /></a></p>\n<p>Given one of the gray pixels in the upper portion of the image, it is very likely that all pixels within the local neighborhood will also be gray. Thus there is a large amount of correlation between pixels in local regions of natural scenes. Statistical models of local structure applied to natural scenes will be dominated by these pairwise correlations, unless they are removed by preprocessing. Whitening provides such a preprocessing procedure.</p>\n<p>Below we create and display a dataset of local image patches of size <img src=\"https://s0.wp.com/latex.php?latex=16+%5Ctimes+16&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"16 &#92;times 16\" class=\"latex\" /> extracted at random from the image above. Each patch is rastered out into a column vector of size <img src=\"https://s0.wp.com/latex.php?latex=%2816%2916+%5Ctimes+1&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"(16)16 &#92;times 1\" class=\"latex\" />. Each of these patches can be thought of as samples of the local structure of this natural scene. Below we use the whitening transformation to remove pairwise correlations between pixels in each patch and scale the variance of each pixel to be one.</p>\n<p><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png\"><img data-attachment-id=\"3645\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whitening-patches/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png\" data-orig-size=\"808,435\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whitening-Patches\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=300\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=808\" class=\"aligncenter size-full wp-image-3645\" alt=\"whitening-Patches\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=914\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png 808w, https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=150 150w, https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=300 300w, https://theclevermachine.files.wordpress.com/2013/03/whitening-patches.png?w=768 768w\" sizes=\"(max-width: 808px) 100vw, 808px\"   /></a></p>\n<p>On the left is the dataset of extracted image patches, along with the corresponding covariance matrix for the image patches on the right. The large local correlation within the neighborhood of each pixel is indicated by the large bright diagonal regions throughout the covariance matrix.</p>\n<p>The MATLAB code to extract and display the patches shown above is here:</p>\n<pre class=\"brush: matlabkey; collapse: true; light: false; title: ; toolbar: true; notranslate\">\n% CREATE PATCHES DATASET FROM NATURAL IMAGE\nrng(12345)\nimSize = 256;\nnPatches = 400;  % (MAKE SURE SQUARE)\npatchSize = 16;\npatches = zeros(patchSize*patchSize,nPatches);\npatchIm = zeros(sqrt(nPatches)*patchSize);\n\n% PAD IMAGE FOR EDGE EFFECTS\nim = padarray(im,[patchSize,patchSize],'symmetric');\n\n% EXTRACT PATCHES...\nfor iP = 1:nPatches\n\tpix = ceil(rand(2,1)*imSize);\n\trows = pix(1):pix(1)+patchSize-1;\n\tcols = pix(2):pix(2)+patchSize-1;\n\ttmp = im(rows,cols);\n\tpatches(:,iP) = reshape(tmp,patchSize*patchSize,1);\n\trowIdx = (ceil(iP/sqrt(nPatches)) - 1)*patchSize + ...\n                 1:ceil(iP/sqrt(nPatches))*patchSize;\n\tcolIdx = (mod(iP-1,sqrt(nPatches)))*patchSize+1:patchSize* ...\n                 ((mod(iP-1,sqrt(nPatches)))+1);\n\tpatchIm(rowIdx,colIdx) = tmp;\nend\n\n% CENTER IMAGE PATCHES\npatchesCentered = bsxfun(@minus,patches,mean(patches,2));\n\n% CALCULATE COVARIANCE MATRIX\nS = patchesCentered*patchesCentered'/nPatches;\n\n% DISPLAY PATCHES\nfigure;\nsubplot(121);\nimagesc(patchIm);\naxis image; axis off; colormap gray;\ntitle('Extracted Patches')\n\n% DISPLAY COVARIANCE\nsubplot(122);\nimagesc(S);\naxis image; axis off; colormap gray;\ntitle('Extracted Patches Covariance')\n</pre>\n<p><span style=\"font-family:Georgia, 'Times New Roman', 'Bitstream Charter', Times, serif;font-size:13px;line-height:19px;\">Below we implement the whitening transformation described above to the extracted image patches and display the whitened patches that result.</span></p>\n<p><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png\"><img data-attachment-id=\"3644\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whitening-whitenedpatches/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png\" data-orig-size=\"808,435\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whitening-WhitenedPatches\" data-image-description=\"\" data-image-caption=\"<p>Whitened Patches</p>\n\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=300\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=808\" class=\"aligncenter size-full wp-image-3644\" alt=\"whitening-WhitenedPatches\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=914\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png 808w, https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=150 150w, https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=300 300w, https://theclevermachine.files.wordpress.com/2013/03/whitening-whitenedpatches.png?w=768 768w\" sizes=\"(max-width: 808px) 100vw, 808px\"   /></a> On the left, we see that the whitening procedure zeros out all areas in the extracted patches that have the same value (zero is indicated by gray). The whitening procedure also boosts the areas of high-contrast (i.e. edges). The right plots the covariance matrix for the whitened patches. The covarance matrix is diagonal, indicating that pixels are now independent. In addition, all diagonal entries have the same value, indicating the that all pixels now have the same variance (i.e. 1). The MATLAB code used to whiten the image patches and create the display above is here:</p>\n<pre class=\"brush: matlabkey; collapse: true; light: false; title: ; toolbar: true; notranslate\">\n%% MAIN WHITENING\n\n% DETERMINE EIGENECTORS & EIGENVALUES\n% OF COVARIANCE MATRIX\n[E,D] = eig(S);\n\n% CALCULATE D^(-1/2)\nd = diag(D);\nd = real(d.^-.5);\nD = diag(d);\n\n% CALCULATE WHITENING TRANSFORM\nW = E*D*E';\n\n% WHITEN THE PATCHES\npatchesWhitened = W*patchesCentered;\n\n% DISPLAY THE WHITENED PATCHES\nwPatchIm = zeros(size(patchIm));\nfor iP = 1:nPatches\n\trowIdx = (ceil(iP/sqrt(nPatches)) - 1)*patchSize + 1:ceil(iP/sqrt(nPatches))*patchSize;\n\tcolIdx = (mod(iP-1,sqrt(nPatches)))*patchSize+1:patchSize* ...\n\t         ((mod(iP-1,sqrt(nPatches)))+1);\n\twPatchIm(rowIdx,colIdx) = reshape(patchesWhitened(:,iP),...\n\t                                  [patchSize,patchSize]);\nend\n\nfigure\nsubplot(121);\nimagesc(wPatchIm);\naxis image; axis off; colormap gray; caxis([-5 5]);\ntitle('Whitened Patches')\n\nsubplot(122);\nimagesc(cov(patchesWhitened'));\naxis image; axis off; colormap gray; %colorbar\ntitle('Whitened Patches Covariance');\n</pre>\n<h2>Investigating the Whitening Matrix: implications for theoretical neuroscience</h2>\n<p style=\"text-align:left;\">So what does the whitening matrix look like, and what does it do? Below is the whitening matrix <img src=\"https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W\" class=\"latex\" /> calculated for the image patches dataset:</p>\n<pre class=\"brush: matlabkey; title: ; notranslate\">\n% DISPLAY THE WHITENING MATRIX\nfigure; imagesc(W);\naxis image; colormap gray; colorbar\ntitle('The Whitening Matrix W')\n</pre>\n<p style=\"text-align:center;\"><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png\"><img loading=\"lazy\" data-attachment-id=\"3719\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whiteningmatrix-2/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png\" data-orig-size=\"563,478\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whiteningMatrix\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=300\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=563\" class=\"aligncenter  wp-image-3719\" alt=\"whiteningMatrix\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=357&#038;h=303\" width=\"357\" height=\"303\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=357&h=303 357w, https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=150&h=127 150w, https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png?w=300&h=255 300w, https://theclevermachine.files.wordpress.com/2013/03/whiteningmatrix1.png 563w\" sizes=\"(max-width: 357px) 100vw, 357px\" /></a></p>\n<p style=\"text-align:left;\">Each column of <img src=\"https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W\" class=\"latex\" /> is the operation that scales the variance of the corresponding pixel to be equal to one and forces that pixel independent of the others in the <img src=\"https://s0.wp.com/latex.php?latex=16+%5Ctimes+16&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"16 &#92;times 16\" class=\"latex\" /> patch. So what exactly does such an operation look like? We can get an idea by reshaping a column of W back into the shape of the image patches. Below we show what the 86th column of W looks like when reshaped in such a way (the index 86 has no particular significance, it was chosen at random):</p>\n<pre class=\"brush: matlabkey; title: ; notranslate\">\n% DISPLAY A COLUMN OF THE WHITENING MATRIX\nfigure; imagesc(reshape(W(:,86),16,16)),\ncolormap gray,\naxis image, colorbar\ntitle('Column 86 of W')\n</pre>\n<p style=\"text-align:center;\"><a href=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png\"><img loading=\"lazy\" data-attachment-id=\"3717\" data-permalink=\"https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/whitening-w-2/\" data-orig-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png\" data-orig-size=\"548,478\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\"}\" data-image-title=\"whitening-W\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=300\" data-large-file=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=548\" class=\"aligncenter  wp-image-3717\" alt=\"whitening-W\" src=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=357&#038;h=311\" width=\"357\" height=\"311\" srcset=\"https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=357&h=311 357w, https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=150&h=131 150w, https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png?w=300&h=262 300w, https://theclevermachine.files.wordpress.com/2013/03/whitening-w1.png 548w\" sizes=\"(max-width: 357px) 100vw, 357px\" /></a></p>\n<p style=\"text-align:left;\">We see that the operation is essentially an impulse centered on the 86th pixel in the image (counting pixels starting in the upper left corner, proceeding down columns). This impulse is surrounded by inhibitory weights. If we were to look at the remaining columns of <img src=\"https://s0.wp.com/latex.php?latex=W&#038;bg=ffffff&#038;fg=4e4e4e&#038;s=0&#038;c=20201002\" alt=\"W\" class=\"latex\" />, we would find that that the same center-surround operation is being replicated at every pixel location in each image patch. Essentially, the whitening transformation is performing a convolution of each image patch with a center-surround filter whose properties are estimated from the patches dataset. Similar techniques are common in computer vision edge-detection algorithms.</p>\n<h2>Implications for theoretical neuroscience</h2>\n<p style=\"text-align:left;\">A theoretical function of the primate retina is data compression: a large number of photoreceptors  pass data from the retina into a physiological bottleneck, the optic nerve, which has far fewer fibers than retinal photoreceptors. Thus removing redundant information is an important task that the retina must perform. When observing the whitened image patches above, we see that redundant information is nullified; pixels that have similar local values to one another are zeroed out. Thus, statistical whitening is a viable form of data compression</p>\n<p>It turns out that there is a large class of ganglion cells in the retina whose spatial receptive fields exhibit&#8230;that&#8217;s right center-surround activation-inhibition like the operation of the whitening matrix shown above! Thus it appears that the primate visual system may be performing data compression at the retina by means of a similar operation to statistical whitening. Above, we derived the center-surround whitening operation based on data sampled from a natural scene. Thus it is seems reasonable that the primate visual system may have evolved a similar data-compression mechanism through experience with natural scenes, either through evolution, or development.</span></p>\n",
  "wfw:commentRss": "https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/feed/",
  "slash:comments": 9,
  "media:content": [
    {
      "media:title": "dustinstansbury"
    },
    {
      "media:title": "whitening-whiteningTransform"
    },
    {
      "media:title": "whitening-Base"
    },
    {
      "media:title": "whitening-Patches"
    },
    {
      "media:title": "whitening-WhitenedPatches"
    },
    {
      "media:title": "whiteningMatrix"
    },
    {
      "media:title": "whitening-W"
    }
  ]
}