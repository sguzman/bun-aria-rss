{
  "title": "Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models. (arXiv:2211.00915v2 [cs.CL] UPDATED)",
  "link": "http://arxiv.org/abs/2211.00915",
  "description": "<p>Retriever-reader models achieve competitive performance across many different\nNLP tasks such as open question answering and dialogue conversations. In this\nwork, we notice these models easily overfit the top-rank retrieval passages and\nstandard training fails to reason over the entire retrieval passages. We\nintroduce a learnable passage mask mechanism which desensitizes the impact from\nthe top-rank retrieval passages and prevents the model from overfitting.\nControlling the gradient variance with fewer mask candidates and selecting the\nmask candidates with one-shot bi-level optimization, our learnable\nregularization strategy enforces the answer generation to focus on the entire\nretrieval passages. Experiments on different tasks across open question\nanswering, dialogue conversation, and fact verification show that our method\nconsistently outperforms its baselines. Extensive experiments and ablation\nstudies demonstrate that our method can be general, effective, and beneficial\nfor many NLP tasks.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingchao Liu</a>"
}