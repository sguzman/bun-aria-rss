{
  "title": "Rolling and Unrolling RNNs",
  "link": "https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/",
  "comments": "https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/#comments",
  "dc:creator": "Jesse Johnson",
  "pubDate": "Thu, 28 Apr 2016 02:34:09 +0000",
  "category": "Uncategorized",
  "guid": "http://shapeofdata.wordpress.com/?p=1286",
  "description": "A while back, I discussed Recurrent Neural Networks (RNNs), a type of artificial neural network in which some of the connections between neurons point &#8220;backwards&#8221;. When a sequence of inputs is fed into such a network, the backward arrows feed &#8230; <a href=\"https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>A while back, I discussed <a href=\"https://shapeofdata.wordpress.com/2015/10/20/recurrent-neural-networks/\">Recurrent Neural Networks</a> (RNNs), a type of <a href=\"https://shapeofdata.wordpress.com/2013/06/11/neural-networks-1-the-neuron/\">artificial neural network</a> in which some of the connections between neurons point &#8220;backwards&#8221;. When a sequence of inputs is fed into such a network, the backward arrows feed information about earlier input values back into the system at later steps. One thing that I didn&#8217;t describe in that post was how to train such a network. So in this post, I want to present one way of thinking about training an RNN, called <em>unrolling</em>.</p>\n<p><span id=\"more-1286\"></span>Recall that a neural network is defined by a directed graph, i.e. a graph in which each edge has an arrow pointing from one endpoint to the other. In my earlier post on RNNs, I described this graph in terms of the classical neural network picture in which each vertex is a neuron that emits a single value. But for this post, it&#8217;ll be easier to describe things in the tensor setting, where each vertex represents a vector defined by a row/layer of neurons. That way, we can think of our network as starting with a single vertex representing the input vector and ending at a single output vertex representing the output vector. We can get to every vertex in the graph by starting from this input vertex and following edges in the direction that their arrows point. Similarly, we can get from any vertex to the output vertex by following some path of edges.</p>\n<p>A standard (non-recurrent) feed-forward network is a directed acyclic graph (DAG) which means that in addition to being directed, it has the property that if you start at any vertex and follow edges in the directions that the arrows point, you&#8217;ll never get back to where you started (acyclic). As a result there&#8217;s a natural flow through the network that allows us to calculate the vectors represented by each vertex one at a time so that by the time we calculate each vector, we&#8217;ve already calculated its inputs, i.e. vectors on the other ends of the edges that point to it.</p>\n<p>In an RNN, the graph has cycles, so no matter how we arrange the vertices, there will always be edges pointing backwards, from vertices whose vectors we haven&#8217;t yet calculated. But we can deal with this by using the output from the previous step.</p>\n<p><img data-attachment-id=\"1456\" data-permalink=\"https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/rnn_example/\" data-orig-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_example2.png\" data-orig-size=\"115,375\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"RNN_example\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_example2.png?w=92\" data-large-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_example2.png?w=115\" class=\" size-full wp-image-1456 alignright\" src=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_example2.png?w=640\" alt=\"RNN_example\"   />For example, the vector <img src=\"https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A\" class=\"latex\" /> in the Figure to the right is calculated from the input vector and the intermediate vector <img src=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C\" class=\"latex\" /> (multiplied by a weight matrix <img src=\"https://s0.wp.com/latex.php?latex=M_A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_A\" class=\"latex\" />). The circle with the &#8216;c&#8217; in it represents concatenating the vectors <img src=\"https://s0.wp.com/latex.php?latex=in&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=in&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=in&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"in\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C\" class=\"latex\" />, which means creating a new (higher dimensional) vector where the first half of the entries come from the input vector and the second half come from <img src=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C\" class=\"latex\" />.</p>\n<p>When the first input value <img src=\"https://s0.wp.com/latex.php?latex=in_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=in_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=in_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"in_0\" class=\"latex\" /> gets to vertex <img src=\"https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A\" class=\"latex\" />, we don&#8217;t yet have a value to use for <img src=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C\" class=\"latex\" />, so we&#8217;ll just use the zero vector of the appropriate dimension, and we&#8217;ll let <img src=\"https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A_0\" class=\"latex\" /> be the value that we calculate. Similarly, we can calculate <img src=\"https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B_0\" class=\"latex\" /> by multiplying <img src=\"https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A_0\" class=\"latex\" /> by the matrix <img src=\"https://s0.wp.com/latex.php?latex=M_B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_B&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_B\" class=\"latex\" /> and the first output value <img src=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_0\" class=\"latex\" /> from <img src=\"https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B_0\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=M_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_C&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_C\" class=\"latex\" />.</p>\n<p>Then comes the second value in the input sequence, which we&#8217;ll call <img src=\"https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"in_1\" class=\"latex\" />. When we go to calculate <img src=\"https://s0.wp.com/latex.php?latex=S_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=S_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=S_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"S_1\" class=\"latex\" />, we haven&#8217;t yet calculated <img src=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_1\" class=\"latex\" />, but we do have <img src=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_0\" class=\"latex\" /> lying around from the last step. So we&#8217;ll calculate <img src=\"https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A_1\" class=\"latex\" /> using <img src=\"https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=in_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"in_1\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_0\" class=\"latex\" />. Then we can calculate <img src=\"https://s0.wp.com/latex.php?latex=B_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=B_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=B_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"B_1\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_1\" class=\"latex\" />. This <img src=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_1\" class=\"latex\" /> will be used to calculate <img src=\"https://s0.wp.com/latex.php?latex=A_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A_2&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A_2\" class=\"latex\" /> in the next step, and so on.</p>\n<p>In order to better understand what&#8217;s going on here, lets draw a new graph that represents all the values that we&#8217;ll calculate for the vertices in the original graph. So, in particular <img src=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_0\" class=\"latex\" /> and <img src=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_1\" class=\"latex\" /> will be two separate vertices in this new graph, and an edge goes from <img src=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=C_0&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"C_0\" class=\"latex\" /> to the concatenation operator that leads into vertex <img src=\"https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=A_1&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"A_1\" class=\"latex\" />. This is shown in the Figure below for the first four steps in an input sequence.</p>\n<p>&nbsp;</p>\n<p><img data-attachment-id=\"1460\" data-permalink=\"https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/rnn_unrolled/\" data-orig-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png\" data-orig-size=\"480,381\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"RNN_unrolled\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png?w=300\" data-large-file=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png?w=480\" class=\" size-full wp-image-1460 aligncenter\" src=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png?w=640\" alt=\"RNN_unrolled\" srcset=\"https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png 480w, https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png?w=150 150w, https://shapeofdata.files.wordpress.com/2016/04/rnn_unrolled1.png?w=300 300w\" sizes=\"(max-width: 480px) 100vw, 480px\"   /></p>\n<p>The first thing you should notice about this graph is that it has multiple inputs &#8211; one for each vector in the input sequence &#8211; and multiple outputs. The second thing you might have noticed is that this graph is acyclic. In particular, the cycle that characterized the original graph has been &#8220;unrolled&#8221; into a longer path that you can follow to the right in the new graph.</p>\n<p>(For any readers who have studied topology, this is nicely reminiscent of the construction of a universal cover. In fact, if you unroll infinitely in both the positive and negative directions, the unrolled graph will be a covering space of original graph. And as noted above, it will be acyclic (in terms of directed cycles, though not necessarily undirected cycles), which is analogous to being simply connected. So maybe there&#8217;s some category theoretic sense in which it really is a universal cover&#8230; but I digress.)</p>\n<p>It turns out you can always form a DAG from a cyclic directed graph by this procedure, which is called <em>unrolling</em>. Note that in the unrolled graph, we have lots of copies of the weight matrices <img src=\"https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_A, M_B, M_C, M_o\" class=\"latex\" />. These are the same at every step in the sequence so they don&#8217;t get unrolled.</p>\n<p>However, we do need to update the weight matrices in order to train the neural network, and this is where the idea of unrolling really comes in handy. Because the unrolled network is a DAG, we can train it using back-propagation just like a standard neural network. But the input to this unrolled network isn&#8217;t a single vector from the sequence &#8211; it&#8217;s the entire sequence, all at once! And the target output we use to calculate the gradients is the entire sequence of output values we would like the network to produce for each step in the input sequence. In practice, it&#8217;s common to truncate the network and only use a portion of the sequence for each training step.</p>\n<p>In the back-propagation step, we calculate gradients and use them to update the weight matrices <img src=\"https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=M_A%2C+M_B%2C+M_C%2C+M_o&#038;bg=ffffff&#038;fg=333333&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"M_A, M_B, M_C, M_o\" class=\"latex\" />. Since we have multiple copies of each weight matrix, we&#8217;re probably going to get different gradients for each copy. But we want all the copies of each matrix to stay the same, so we&#8217;ll combine all the gradients, usually by taking an average, and use this to update the base matrix that all the copies are taken from.</p>\n<p>In practice, you don&#8217;t necessarily need to explicitly construct the unrolled network in order to train an RNN with back-propagation. As long as you&#8217;re willing to deal with some complex book keeping, you can calculate the gradients for the weight matrices directly from the original graph. But nonetheless, unrolling is a nice way to think about the training process, independent of how it&#8217;s actually done.</p>\n",
  "wfw:commentRss": "https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/feed/",
  "slash:comments": 4,
  "media:content": [
    {
      "media:title": "jessejohnson"
    },
    {
      "media:title": "RNN_example"
    },
    {
      "media:title": "RNN_unrolled"
    }
  ]
}