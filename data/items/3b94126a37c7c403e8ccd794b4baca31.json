{
  "id": "tag:blogger.com,1999:blog-19803222.post-9064414353327996441",
  "published": "2016-08-05T10:27:00.004-06:00",
  "updated": "2016-08-05T12:29:01.081-06:00",
  "title": "Fast & easy baseline text categorization with vw",
  "content": "About a month ago, the paper <a href=\"https://arxiv.org/pdf/1607.01759v2.pdf\">Bag of Tricks for Efficient Text Categorization</a> was posted to arxiv. I found it thanks to <a href=\"https://twitter.com/yoavgo/status/751178795323908096\">Yoav Goldberg's rather incisive tweet</a>:<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://3.bp.blogspot.com/-Zi9S01ZSI7c/V6S169qQnhI/AAAAAAAAAY0/OTv42npepZI0K_D5kyWDaQ3g5K5pMDiHgCLcB/s1600/yaov.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"110\" src=\"https://3.bp.blogspot.com/-Zi9S01ZSI7c/V6S169qQnhI/AAAAAAAAAY0/OTv42npepZI0K_D5kyWDaQ3g5K5pMDiHgCLcB/s400/yaov.png\" width=\"400\" /></a></div>Yoav is basically referring to the fact that the paper is all about (a) hashing features and (b) bigrams and (c) a projection that doesn't totally make sense to me, which (a) vw does by default (b) requires \"--ngrams 2\" and (c) I don't totally understand I don't think is necessary. (See <a href=\"https://github.com/hal3/vwnlp\">this tutorial</a> for more on how to do NLP in VW.)<br /><br />At the time, I said if they gave me the data, I'd run <a href=\"https://github.com/JohnLangford/vowpal_wabbit\">vw</a> on it and report results. They were nice enough to share the data but I never got around to running it. The code for their technique (\"fastText\") was just released, which goaded me into finally doing something.<br /><br />So my goal here was to try to tell, without tuning any parameters, how competitive a baseline vw is to the results from fastText with minimal effort.<br /><br />Here are the results:<br /><br /><table border=\"1\" cellpadding=\"2\" cellspacing=\"0\"><tbody><tr><td></td><td></td><td colspan=\"2\" style=\"text-align: center;\"><b>fastText</b></td><td colspan=\"2\" style=\"text-align: center;\"><b>vw</b></td></tr><tr><td style=\"text-align: center;\"><b>Dataset</b></td><td style=\"text-align: center;\"><b>ng</b></td><td style=\"text-align: center;\"><b>time</b></td><td style=\"text-align: center;\"><b>acc</b></td><td style=\"text-align: center;\"><b>time</b></td><td style=\"text-align: center;\"><b>acc</b></td></tr><tr><td>ag news</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\">91.5</td><td>2s</td><td style=\"text-align: right;\"><b>91.9</b></td></tr><tr><td>ag news</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">3s</td><td style=\"text-align: right;\"><b>92.5</b></td><td>5s</td><td style=\"text-align: right;\">92.3</td></tr><tr><td>amazon full</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\"><b>55.8</b></td><td>47s</td><td style=\"text-align: right;\">53.6</td></tr><tr><td>amazon full</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">33s</td><td style=\"text-align: right;\"><b>60.2</b></td><td>69s</td><td style=\"text-align: right;\">56.6</td></tr><tr><td>amazon polarity</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\">91.2</td><td>46s</td><td style=\"text-align: right;\"><b>91.3</b></td></tr><tr><td>amazon polarity</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">52s</td><td style=\"text-align: right;\"><b>94.6</b></td><td>68s</td><td style=\"text-align: right;\">94.2</td></tr><tr><td>dbpedia</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\">98.1</td><td>8s</td><td style=\"text-align: right;\"><b>98.4</b></td></tr><tr><td>dbpedia</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">8s</td><td style=\"text-align: right;\">98.6</td><td>17s</td><td style=\"text-align: right;\"><b>98.7</b></td></tr><tr><td>sogou news</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\"><b>93.9</b></td><td>25s</td><td style=\"text-align: right;\">93.6</td></tr><tr><td>sogou news</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">36s</td><td style=\"text-align: right;\">96.8</td><td>30s</td><td style=\"text-align: right;\"><b>96.9</b></td></tr><tr><td>yahoo answers</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\"><b>72.0</b></td><td>30s</td><td style=\"text-align: right;\">70.6</td></tr><tr><td>yahoo answers</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">27s</td><td style=\"text-align: right;\"><b>72.3</b></td><td>48s</td><td style=\"text-align: right;\">71.0</td></tr><tr><td>yelp full</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\"><b>60.4</b></td><td>16s</td><td style=\"text-align: right;\">56.9</td></tr><tr><td>yelp full</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">18s</td><td style=\"text-align: right;\"><b>63.9</b></td><td>37s</td><td style=\"text-align: right;\">60.0</td></tr><tr><td>yelp polarity</td><td style=\"text-align: center;\">1</td><td style=\"text-align: right;\"><br /></td><td style=\"text-align: right;\"><b>93.8</b></td><td>10s</td><td style=\"text-align: right;\">93.6</td></tr><tr><td>yelp polarity</td><td style=\"text-align: center;\">2</td><td style=\"text-align: right;\">15s</td><td style=\"text-align: right;\"><b>95.7</b></td><td>20s</td><td style=\"text-align: right;\">95.5</td></tr></tbody></table><br />(Average accuracy for fastText is 83.2; for vw is 82.2.)<br /><br />In terms of accuracy, the two are roughly on par. vw occasionally wins; when it does, it's usually by 0.1% to 0.5%. fastText wins a bit more often, and on one dataset it wins significantly (yelp full: winning by 3%-4%) and on one a bit less (yahoo answers, up by about 1.3%). But the numbers are pretty much in line, and could almost certainly be brought up for vw with a wee bit of hyperparameter tuning (namely the learning rate, which is tuned in fastText).<br /><br />In terms of training time, fastText is maybe 30% faster on average, though these are such small datasets (eg 500k examples) that a difference of 52s versus 68s is not too significant. I also noticed that for most of the datasets, simply writing the model to disk for vw took a nontrivial amount of time. But wait, there's more. That 30% faster for fastText was <b>run on 20 cores in parallel</b> whereas the vw run did not use parallelized learning (vw runs two threads, one for I/O and one for learning).<br />That said, a <b>major caveat</b> on comparing the training times. They're run on different machines. I don't know what type of machine the fastText results were achieved on, but it was a parallel 20-core run. The vw experiments were run on a single core, one pass over the data, on a 3.1Ghz Core i5-2400. Yes, I could have hogwild-ed vw and gotten it faster but it really didn't seem worth it for datasets this small. And yes, I could've rerun fastText on my machine, but... what can I say? I'm lazy.<br /><br />What did I do to get these vw numbers? Here's the entire training script:<br /><blockquote class=\"tr_bq\"><pre>% cat run.sh <br />#!/bin/bash<br />d=$1<br />for ngram in 1 2 ; do<br />  cat $d/train.csv | ./csv2vw.pl | \\<br />    time vowpal_wabbit/vowpalwabbit/vw --oaa `cat $d/classes.txt | wc -l` \\<br />                                  -b25 --ngram $ngram -f $d/model.$ngram<br />  cat $d/test.csv  | ./csv2vw.pl | \\<br />    time vowpal_wabbit/vowpalwabbit/vw -t -i $d/model.$ngram<br />done<br /></pre></blockquote>Basically the only flags to vw are (1) telling it to do multiclass classification with one-against-all, (2) telling it to use 25 bits (not tuned), and telling it to either use unigrams or bigrams. [Comparison note: this means vw is using 33m hash bins; fastText used 10m for unigram models and 100m for bigram models.]<br /><br />The only(*) data munging that occurs is in csv2vw.pl, which is a lightweight script for converting the data, lowercasing, and doing very minor tokenization:<br /><blockquote class=\"tr_bq\"><pre>% cat csv2vw.pl<br />#!/usr/bin/perl -w<br />use strict;<br />while (&lt;&gt;) {<br />    chomp;<br />    if (/^\"*([0-9]+)\"*,\"(.+)\"*$/) {<br />        print $1 . ' | ';<br />        $_ = lc($2);<br />        s/\",\"/ /g;<br />        s/\"\"/\"/g;<br />        s/([^a-z0-9 -\\\\]+)/ $1 /g;<br />        s/:/C/g;<br />        s/\\|/P/g;<br />        print $_ . \"\\n\";<br />    } else { <br />        die \"malformed line '$_'\";<br />    }<br />}<br /></pre></blockquote>There are two exceptions where I did slightly more data munging. The datasets released for dbpedia and Soguo were not properly shuffled, which makes online learning hard. I preprocessed the training data by randomly shuffling it. This took 2.4s for dbpedia and 12s for Soguo.<br /><br /><b>[[[EDIT 2:20p 5 Aug 2016: </b>Out of curiosity, I upped the number of bits that vw uses for the experiments to 27 (so that it's on par with the 100m used by fastText). This makes it take about 5 seconds longer to run (writing the model to disk is slower). Performance stays the same on: ag news, amazon polarity, dbpedia, sogou, and yelp polarity; and it goes up from from 53.6/56.6 to 55.0/58.8 on amazon full, from 70.6/71.0 to 71.1/71.6 on yahoo answers, from 56.9/60.0 to 58.5/61.6 on yelp full. This puts the vw average with more bits at 82.6, which is 0.6% behind the fastText average.<b>]]]</b><br /><br />Long story short... am I switching from vw to fastText? Probably not any time soon.",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "hal",
    "uri": "http://www.blogger.com/profile/02162908373916390369",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 6
}