{
  "id": "tag:blogger.com,1999:blog-5547907074344788039.post-5002251547468095189",
  "published": "2013-02-19T09:38:00.000-08:00",
  "updated": "2013-02-19T09:38:13.572-08:00",
  "title": "When tf*idf and cosine similarity fail",
  "content": "<br /><div class=\"p1\">In this post I cover 2 edge cases of cosine similarity with tf*idf weights that fail, i.e. that don't provide the cosine similarity values that intuition and common sense says that they should return.&nbsp;</div><div class=\"p1\"><br /></div><div class=\"p1\">In information retrieval, <a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">tf*idf</a> forms the basis of scoring documents for relevance when querying a corpus, as in a search engine.&nbsp;It is the product of two terms: <i>term frequency</i> and <i>inverse document frequency</i>.</div><div class=\"p2\"><br /></div><div class=\"p1\"><i><span style=\"color: #6fa8dc;\">Term frequency</span></i> is the frequency of some term in the document, typically an absolute count or relative frequency.&nbsp;Documents with more mentions of a term are more likely to be relevant with respect to that term. For example, when querying for \"dog,\" a document about caring for your dog which mentions \"dog\" 46 times is more likely to be relevant than a document with a single mention of \"the dog days of summer.\"</div><div class=\"p2\"><br /></div><div class=\"p1\"><i><span style=\"color: #6fa8dc;\">Inverse document frequency</span></i> (IDF) measures the dispersion of that term across the corpus. If every document contains \"the,\" then \"the\" is not a particularly discriminating word. IDF is the ratio of the corpus size to the number of documents containing that term. The smaller the proportion of documents containing that term, the higher the magnitude of this metric. (In reality, we take the log of the ratio. That is, idf = log(N/n_i)).</div><div class=\"p2\"><br /></div><div class=\"p1\">These two measures quantify the frequency within a document and the relative rarity across the corpus.&nbsp;Taking the product we arrive at a simple, satisfyingly intuitive but surprisingly powerful metric to score documents. For each term <b><span style=\"color: #6fa8dc;\">t</span></b> in each document <b><span style=\"color: #6fa8dc;\">d</span></b> in some corpus <b><span style=\"color: #6fa8dc;\">D</span></b> we can compute the tf*idf score. Let's call this tfidf (t,d).</div><div class=\"p2\"><br /></div><div class=\"p1\">We rarely query a corpus for a single term. Instead, we have a query&nbsp;<b><span style=\"color: #6fa8dc;\">q</span></b>&nbsp;consisting of multiple terms. Now we want to compute the similarity between the query <b><span style=\"color: #6fa8dc;\">q</span></b> and each document <b><span style=\"color: #6fa8dc;\">d</span></b> in the corpus. For this, we tend to use something called <a href=\"http://en.wikipedia.org/wiki/Cosine_similarity\">cosine similarity</a>.&nbsp;</div><div class=\"p3\"><br /></div><div class=\"p1\">This is a measure of the angle between two unit vectors:</div><div class=\"p4\"><br /></div><div class=\"p5\"><span style=\"color: #6fa8dc;\">similarity&nbsp;</span></div><div class=\"p5\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; = cos(a,b)&nbsp;</span></div><div class=\"p5\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; = <a href=\"http://en.wikipedia.org/wiki/Dot_product\">dotproduct</a>(a,b) / (&nbsp;<a href=\"http://en.wikipedia.org/wiki/Norm_(mathematics)\">norm</a>(a) * norm(b) )&nbsp;</span></div><div class=\"p5\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; =&nbsp;a.b&nbsp;/ ||a|| * ||b||</span></div><div class=\"p6\"><br /></div><div class=\"p7\"><i>[Definition: if a = (a1,a2,...,an) and b&nbsp;= (b1,b2,...,bn)&nbsp;then&nbsp;a.b = Sum(a1*b1 + a2*b2 + ... + an*bn)&nbsp;</i></div><div class=\"p1\"><i>and ||a|| = sqrt(a1^2 + a2^2 + ... + an^2) and ||b|| = sqrt(b1^2 + b2^2 + ... + bn^2). ]</i></div><div class=\"p4\"><br /></div><div class=\"p1\">The smaller the angle, the more similar are the two vectors.</div><div class=\"p2\"><br /></div><div class=\"p1\">In this case, the variables of <b><span style=\"color: #6fa8dc;\">a</span></b> and <b><span style=\"color: #6fa8dc;\">b</span></b> are the set of unique terms in <b><span style=\"color: #6fa8dc;\">q</span></b> and <b><span style=\"color: #6fa8dc;\">d</span></b>. For example, when&nbsp;<b><span style=\"color: #6fa8dc;\">q</span>&nbsp;</b>= \"big red balloon\" and <b><span style=\"color: #6fa8dc;\">d</span>&nbsp;</b>=\"small green balloon\" then the variables are&nbsp;(big,red,balloon,small,green) and&nbsp;<b><span style=\"color: #6fa8dc;\">a</span>&nbsp;</b>= (1,1,1,0,0) and <b><span style=\"color: #6fa8dc;\">b</span>&nbsp;</b>= (0,0,1,1,1).</div><div class=\"p2\"><br /></div><div class=\"p1\">Not all words are created equally. Some are more important than others when computing similarity. Rather than use the count or the presence/absence of each term, we can use a weight. For example, we can give a lower weight to common words.&nbsp;What would make a suitable weighting? tf*idf of course. Putting this altogether,</div><div class=\"p2\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">similarity(q,d) =&nbsp;<span class=\"s5\">a.b&nbsp;/ ||a|| * ||b||&nbsp;</span></span></div><div class=\"p1\"><br /></div><div class=\"p1\">where</div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">a = (</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"big\",q),&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"red\",q),&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"balloon\",q),&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"small\",q),</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"green\",q)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;)</span></div><div class=\"p1\"><br /></div><div class=\"p1\">and</div><div class=\"p1\"><br /></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">b = (</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"big\",d),</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"red\",d),&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"balloon\",d),</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"small\",d),&nbsp;</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp; tfidf(\"green\",d)</span></div><div class=\"p1\"><span style=\"color: #6fa8dc;\">&nbsp; &nbsp;)</span>.</div><div class=\"p2\"><br /></div><div class=\"p1\">While cosine similarity with tf*idf works well, <i>really well</i>, there are a couple of edge cases where it fails, corner cases that don't seem to be covered in most introductory explanations and tutorials.</div><div class=\"p2\"><br /></div><div class=\"p1\"><b><span style=\"color: #6fa8dc;\">FAIL 1</span></b>: imagine you have a corpus <b><span style=\"color: #6fa8dc;\">D</span></b> consisting of one document <b><span style=\"color: #6fa8dc;\">d</span></b>. You come along with a query <b><span style=\"color: #6fa8dc;\">q</span></b> where <span style=\"color: #6fa8dc;\"><b>q</b> == <b>d</b></span>. That is, the corpus has <i>exactly</i> what you are looking for. Intuition should say that we expect that cosine similarity would be 1 because <span style=\"color: #6fa8dc;\"><b>q</b> == <b>d</b></span>. So, what do we get? While the dot product of <b><span style=\"color: #6fa8dc;\">q</span></b> and <b><span style=\"color: #6fa8dc;\">d</span></b> should be 1 giving cosine similarity 1, it is not when you use tf*idf weights. The tf*idf of each term of <b><span style=\"color: #6fa8dc;\">d</span></b> will be zero--each term of <b><span style=\"color: #6fa8dc;\">d</span></b> is in all documents (<span style=\"color: #6fa8dc;\"><b>D</b>==<b>d</b></span>). Therefore, the dot product is zero but the norms of the two vectors is also zero and will generate a division by zero error. In summary, similarity is 0/0 and so undefined.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\"><b><span style=\"color: #6fa8dc;\">FAIL 2</span></b>: imagine you have a corpus with two documents, <b><span style=\"color: #6fa8dc;\">d_1</span>&nbsp;</b>= \"blue bag\" and <b><span style=\"color: #6fa8dc;\">d_2</span>&nbsp;</b>= \"green bag\". What is their similarity? Intuition says there are some similarities between them, they both contain \"bag,\" but there are some differences: \"blue\" vs \"green\". Thus, this should mean that we get a cosine similarity somewhere between 0 and 1. Wrong! Tf*idf for \"bag,\" the common term, is zero because IDF is zero. \"blue\" is not a shared term and so that term of the dot product is zero as is for term \"green.\" In other words, where they differ it pumps zero terms into the dot product and where they are similar, those terms effectively convey no information whatsoever and so also generate zero values.</div><div class=\"p2\"><br /></div><div class=\"p1\">While these two scenarios may seem contrived, I encountered them while writing unit tests where I wanted to use minimal corpora possible to test my code. It seems that one needs three distinct documents to avoid the problems above, or your code must handle a NaN.</div><div class=\"p2\"><br /></div><div class=\"p1\">I use tf*idf and cosine similarity frequently. It can get you far with little cost (if your documents are not enormous). It does have a big limitation though, it is a \"bag of words\" model meaning it does not consider word order. In many cases, specific word order matters a lot---a red couch with gold legs is very different from a gold couch with red legs. What one can do is to use the fast and cheap cosine similarity with tf*idf weights to narrow down some larger corpus to a smaller subset of documents for which you run a more computationally expensive, more domain specific model or algorithm that does consider word order.</div><br /><div><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Carl Anderson",
    "uri": "http://www.blogger.com/profile/11930448254473684406",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 3
}