{
  "title": "Non-Zero Initial States for Recurrent Neural Networks",
  "link": "",
  "published": "2016-11-20T00:00:00-05:00",
  "updated": "2016-11-20T00:00:00-05:00",
  "author": {
    "name": "Silviu Pitis"
  },
  "id": "tag:r2rt.com,2016-11-20:/non-zero-initial-states-for-recurrent-neural-networks.html",
  "summary": "The default approach to initializing the state of an RNN is to use a zero state. This often works well, particularly for sequence-to-sequence tasks like language modeling where the proportion of outputs that are significantly impacted by the initial state is small. In some cases, however, it makes sense to (1) train the initial state as a model parameter, (2) use a noisy initial state, or (3) both. This post examines the rationale behind trained and noisy intial states briefly, and presents drop-in Tensorflow implementations.",
  "content": "<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title></title>\n  <style type=\"text/css\">code{white-space: pre;}</style>\n  <style type=\"text/css\">\ndiv.sourceCode { overflow-x: auto; }\ntable.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {\n  margin: 0; padding: 0; vertical-align: baseline; border: none; }\ntable.sourceCode { width: 100%; line-height: 100%; }\ntd.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }\ntd.sourceCode { padding-left: 5px; }\ncode > span.kw { color: #007020; font-weight: bold; } /* Keyword */\ncode > span.dt { color: #902000; } /* DataType */\ncode > span.dv { color: #40a070; } /* DecVal */\ncode > span.bn { color: #40a070; } /* BaseN */\ncode > span.fl { color: #40a070; } /* Float */\ncode > span.ch { color: #4070a0; } /* Char */\ncode > span.st { color: #4070a0; } /* String */\ncode > span.co { color: #60a0b0; font-style: italic; } /* Comment */\ncode > span.ot { color: #007020; } /* Other */\ncode > span.al { color: #ff0000; font-weight: bold; } /* Alert */\ncode > span.fu { color: #06287e; } /* Function */\ncode > span.er { color: #ff0000; font-weight: bold; } /* Error */\ncode > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */\ncode > span.cn { color: #880000; } /* Constant */\ncode > span.sc { color: #4070a0; } /* SpecialChar */\ncode > span.vs { color: #4070a0; } /* VerbatimString */\ncode > span.ss { color: #bb6688; } /* SpecialString */\ncode > span.im { } /* Import */\ncode > span.va { color: #19177c; } /* Variable */\ncode > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */\ncode > span.op { color: #666666; } /* Operator */\ncode > span.bu { } /* BuiltIn */\ncode > span.ex { } /* Extension */\ncode > span.pp { color: #bc7a00; } /* Preprocessor */\ncode > span.at { color: #7d9029; } /* Attribute */\ncode > span.do { color: #ba2121; font-style: italic; } /* Documentation */\ncode > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */\ncode > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */\ncode > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */\n  </style>\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<p>The default approach to initializing the state of an RNN is to use a zero state. This often works well, particularly for sequence-to-sequence tasks like language modeling where the proportion of outputs that are significantly impacted by the initial state is small. In some cases, however, it makes sense to (1) train the initial state as a model parameter, (2) use a noisy initial state, or (3) both. This post examines the rationale behind trained and noisy intial states briefly, and presents drop-in Tensorflow implementations.</p>\n<h3 id=\"training-the-initial-state\">Training the initial state</h3>\n<p>If there are enough sequences or state resets in the training data (e.g., this will often be the case if we are doing sequence classification), it may make sense to train the initial state as a variable. This way, the model can learn a good default state. If we have only a few state resets, however, training the initial state as a variable may result in overfitting on the start of each sequence. To see this, consider that with n-step truncated backpropagation, only the first n-steps of each sequence will contribute to the gradient of the initial state, so that even if our single training sequence has one million steps, only thirty of them will be used to train the initial state.</p>\n<p>I haven’t seen anyone evaluate this technique (edit 11/22/16: though it appears to be common knowledge), and so I don’t have a good citation for empirical results. Instead, please see the experimental results in this post.</p>\n<h3 id=\"using-a-noisy-initial-state\">Using a noisy initial state</h3>\n<p>Using a zero-valued initial state can also result in overfitting, though in a different way. Ordinarily, losses at the early steps of a sequence-to-sequence model (i.e., those immediately after a state reset) will be larger than those at later steps, because there is less history. Thus, their contribution to the gradient during learning will be relatively higher. But if all state resets are associated with a zero-state, the model can (and will) learn how to compensate for precisely this. As the ratio of state resets to total observations increases, the model parameters will become increasingly tuned to this zero state, which may affect performance on later time steps.</p>\n<p>One simple solution is to make the initial state noisy. This is the approach suggested by <a href=\"http://www.scs-europe.net/conf/ecms2015/invited/Contribution_Zimmermann_Grothmann_Tietz.pdf\">Zimmerman et al. (2012)</a>, who take it even a step further by making the magnitude of the initial state noise change according to the backpropagated error. This post will only take the first step of making the initial state noisy.</p>\n<h2 id=\"tensorflow-implementations\">Tensorflow implementations</h2>\n<p>In some cases, e.g., as in my post on <a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html\">variable length sequences</a>, creating a variable or noisy initial state to match the cell state is straightforward. However, we often want to switch out the RNN cell or build complicated cells with nested states. My motivation for writing this post was to provide a method like the <code>zero_state</code> method of Tensorflow’s base RNNCell class that automatically constructs a variable or noisy intitial state.</p>\n<h5 id=\"implementation-model\">Implementation model</h5>\n<p>We’ll model the implementation after the <code>zero_state</code> method of Tensorflow’s base RNNCell class, shown below with minor modifications to make it a top-level function. You can view the original <code>zero_state</code> method <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\">here</a>.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> numpy <span class=\"im\">as</span> np, tensorflow <span class=\"im\">as</span> tf\n<span class=\"im\">from</span> tensorflow.python.util <span class=\"im\">import</span> nest\n_state_size_with_prefix <span class=\"op\">=</span> tf.nn.rnn_cell._state_size_with_prefix\n\n<span class=\"kw\">def</span> zero_state(cell, batch_size, dtype):\n    <span class=\"co\">&quot;&quot;&quot;Return zero-filled state tensor(s).</span>\n<span class=\"co\">    Args:</span>\n<span class=\"co\">      cell: RNNCell.</span>\n<span class=\"co\">      batch_size: int, float, or unit Tensor representing the batch size.</span>\n<span class=\"co\">      dtype: the data type to use for the state.</span>\n<span class=\"co\">    Returns:</span>\n<span class=\"co\">      If `state_size` is an int or TensorShape, then the return value is a</span>\n<span class=\"co\">      `N-D` tensor of shape `[batch_size x state_size]` filled with zeros.</span>\n<span class=\"co\">      If `state_size` is a nested list or tuple, then the return value is</span>\n<span class=\"co\">      a nested list or tuple (of the same structure) of `2-D` tensors with</span>\n<span class=\"co\">    the shapes `[batch_size x s]` for each s in `state_size`.</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    state_size <span class=\"op\">=</span> cell.state_size\n    <span class=\"cf\">if</span> nest.is_sequence(state_size):\n        state_size_flat <span class=\"op\">=</span> nest.flatten(state_size)\n        zeros_flat <span class=\"op\">=</span> [\n            tf.zeros(\n              tf.pack(_state_size_with_prefix(s, prefix<span class=\"op\">=</span>[batch_size])),\n              dtype<span class=\"op\">=</span>dtype)\n            <span class=\"cf\">for</span> s <span class=\"kw\">in</span> state_size_flat]\n        <span class=\"cf\">for</span> s, z <span class=\"kw\">in</span> <span class=\"bu\">zip</span>(state_size_flat, zeros_flat):\n            z.set_shape(_state_size_with_prefix(s, prefix<span class=\"op\">=</span>[<span class=\"va\">None</span>]))\n        zeros <span class=\"op\">=</span> nest.pack_sequence_as(structure<span class=\"op\">=</span>state_size,\n                                    flat_sequence<span class=\"op\">=</span>zeros_flat)\n    <span class=\"cf\">else</span>:\n        zeros_size <span class=\"op\">=</span> _state_size_with_prefix(state_size, prefix<span class=\"op\">=</span>[batch_size])\n        zeros <span class=\"op\">=</span> tf.zeros(tf.pack(zeros_size), dtype<span class=\"op\">=</span>dtype)\n        zeros.set_shape(_state_size_with_prefix(state_size, prefix<span class=\"op\">=</span>[<span class=\"va\">None</span>]))\n\n    <span class=\"cf\">return</span> zeros</code></pre></div>\n<h5 id=\"implementation\">Implementation</h5>\n<p>Rather than rewriting the <code>zero_state</code> method to initialize the state with a variable (or with noise) directly, we will abstract out the <code>tf.zeros</code> function, to make the method more flexible. Our abstracted function, <code>get_initial_cell_state</code>, takes an additional <code>initializer</code> argument, which takes the place of <code>tf.zeros</code> and determines how the state is initialized. This would be a simple modification, but for the fact that we need to be careful with how variable states are created (e.g., we don’t want a different variable for each sample in the batch), which pushes some of the complexity into the <code>initializer</code> function.</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> get_initial_cell_state(cell, initializer, batch_size, dtype):\n    <span class=\"co\">&quot;&quot;&quot;Return state tensor(s), initialized with initializer.</span>\n<span class=\"co\">    Args:</span>\n<span class=\"co\">      cell: RNNCell.</span>\n<span class=\"co\">      batch_size: int, float, or unit Tensor representing the batch size.</span>\n<span class=\"co\">      initializer: function with two arguments, shape and dtype, that</span>\n<span class=\"co\">          determines how the state is initialized.</span>\n<span class=\"co\">      dtype: the data type to use for the state.</span>\n<span class=\"co\">    Returns:</span>\n<span class=\"co\">      If `state_size` is an int or TensorShape, then the return value is a</span>\n<span class=\"co\">      `N-D` tensor of shape `[batch_size x state_size]` initialized</span>\n<span class=\"co\">      according to the initializer.</span>\n<span class=\"co\">      If `state_size` is a nested list or tuple, then the return value is</span>\n<span class=\"co\">      a nested list or tuple (of the same structure) of `2-D` tensors with</span>\n<span class=\"co\">    the shapes `[batch_size x s]` for each s in `state_size`.</span>\n<span class=\"co\">    &quot;&quot;&quot;</span>\n    state_size <span class=\"op\">=</span> cell.state_size\n    <span class=\"cf\">if</span> nest.is_sequence(state_size):\n        state_size_flat <span class=\"op\">=</span> nest.flatten(state_size)\n        init_state_flat <span class=\"op\">=</span> [\n            initializer(_state_size_with_prefix(s), batch_size, dtype, i)\n                <span class=\"cf\">for</span> i, s <span class=\"kw\">in</span> <span class=\"bu\">enumerate</span>(state_size_flat)]\n        init_state <span class=\"op\">=</span> nest.pack_sequence_as(structure<span class=\"op\">=</span>state_size,\n                                    flat_sequence<span class=\"op\">=</span>init_state_flat)\n    <span class=\"cf\">else</span>:\n        init_state_size <span class=\"op\">=</span> _state_size_with_prefix(state_size)\n        init_state <span class=\"op\">=</span> initializer(init_state_size, batch_size, dtype, <span class=\"va\">None</span>)\n\n    <span class=\"cf\">return</span> init_state</code></pre></div>\n<p><code>initializer</code> must be a function with four arguments: <code>shape</code> and <code>dtype</code>, a la <code>tf.zeros</code>, and additionally <code>batch_size</code> and <code>index</code>, which are introduced to play nice with variables. We can achieve the same behavior as the original <code>zero_state</code> method with the following <code>initializer</code> function:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> zero_state_initializer(shape, batch_size, dtype, index):\n    z <span class=\"op\">=</span> tf.zeros(tf.pack(_state_size_with_prefix(shape, [batch_size])), dtype)\n    z.set_shape(_state_size_with_prefix(shape, prefix<span class=\"op\">=</span>[<span class=\"va\">None</span>]))\n    <span class=\"cf\">return</span> z</code></pre></div>\n<p>Then calling <code>get_initial_cell_state(cell, zero_state_initializer, batch_size, tf.float32)</code> does the same thing as calling <code>zero_state(cell, batch_size, tf.float32)</code>.</p>\n<p>Given this abstraction, we add support for a variable initializer like so:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> make_variable_state_initializer(<span class=\"op\">**</span>kwargs):\n    <span class=\"kw\">def</span> variable_state_initializer(shape, batch_size, dtype, index):\n        args <span class=\"op\">=</span> kwargs.copy()\n\n        <span class=\"cf\">if</span> args.get(<span class=\"st\">&#39;name&#39;</span>):\n            args[<span class=\"st\">&#39;name&#39;</span>] <span class=\"op\">=</span> args[<span class=\"st\">&#39;name&#39;</span>] <span class=\"op\">+</span> <span class=\"st\">&#39;_&#39;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(index)\n        <span class=\"cf\">else</span>:\n            args[<span class=\"st\">&#39;name&#39;</span>] <span class=\"op\">=</span> <span class=\"st\">&#39;init_state_&#39;</span> <span class=\"op\">+</span> <span class=\"bu\">str</span>(index)\n\n        args[<span class=\"st\">&#39;shape&#39;</span>] <span class=\"op\">=</span> shape\n        args[<span class=\"st\">&#39;dtype&#39;</span>] <span class=\"op\">=</span> dtype\n\n        var <span class=\"op\">=</span> tf.get_variable(<span class=\"op\">**</span>args)\n        var <span class=\"op\">=</span> tf.expand_dims(var, <span class=\"dv\">0</span>)\n        var <span class=\"op\">=</span> tf.tile(var, tf.pack([batch_size] <span class=\"op\">+</span> [<span class=\"dv\">1</span>] <span class=\"op\">*</span> <span class=\"bu\">len</span>(shape)))\n        var.set_shape(_state_size_with_prefix(shape, prefix<span class=\"op\">=</span>[<span class=\"va\">None</span>]))\n        <span class=\"cf\">return</span> var\n\n    <span class=\"cf\">return</span> variable_state_initializer</code></pre></div>\n<p>We can now get a variable initial state by calling <code>get_initial_cell_state(cell, make_variable_initializer(), batch_size, tf.float32)</code>.</p>\n<p>Finally, we can add a noisy wrapper for our zero or variable state intializers like so:</p>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> make_gaussian_state_initializer(initializer, deterministic_tensor<span class=\"op\">=</span><span class=\"va\">None</span>, stddev<span class=\"op\">=</span><span class=\"fl\">0.3</span>):\n    <span class=\"kw\">def</span> gaussian_state_initializer(shape, batch_size, dtype, index):\n        init_state <span class=\"op\">=</span> initializer(shape, batch_size, dtype, index)\n        <span class=\"cf\">if</span> deterministic_tensor <span class=\"kw\">is</span> <span class=\"kw\">not</span> <span class=\"va\">None</span>:\n            <span class=\"cf\">return</span> tf.cond(deterministic_tensor,\n                <span class=\"kw\">lambda</span>: init_state,\n                <span class=\"kw\">lambda</span>: init_state <span class=\"op\">+</span> tf.random_normal(tf.shape(init_state), stddev<span class=\"op\">=</span>stddev))\n        <span class=\"cf\">else</span>:\n            <span class=\"cf\">return</span> init_state <span class=\"op\">+</span> tf.random_normal(tf.shape(init_state), stddev<span class=\"op\">=</span>stddev)\n    <span class=\"cf\">return</span> gaussian_state_initializer</code></pre></div>\n<p>This wrapper adds gaussian noise to the underlying initial_state. E.g., to create an initializer function that initializes the state with a mean of zero and standard deviation of 0.1, we call <code>make_gaussian_state_initializer(zero_state_initializer, stddev=0.01)</code>. The deterministic_tensor is an optional boolean tensor that can be used to disable added noise at test time (recommended).</p>\n<h2 id=\"an-experiment-on-the-truncated-ptb-dataset\">An experiment on the truncated PTB dataset</h2>\n<p>Now let us test our initializers on a “truncated” PTB language modeling task. This will be the same as the regular PTB dataset, except that we will modify the usual training routine so as to <em>not</em> propagate the final state forward (i.e., it will truncate the state propagation). By reseting the state between each training step, we make the PTB dataset behave like a dataset with many state resets.</p>\n<h5 id=\"helper-functions\">Helper functions</h5>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">from</span> tensorflow.models.rnn.ptb <span class=\"im\">import</span> reader\n<span class=\"im\">from</span> enum <span class=\"im\">import</span> Enum\n\n<span class=\"co\">#data from http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</span>\nraw_data <span class=\"op\">=</span> reader.ptb_raw_data(<span class=\"st\">&#39;ptb_data&#39;</span>)\ntrain_data, val_data, test_data, num_classes <span class=\"op\">=</span> raw_data\nbatch_size, num_steps <span class=\"op\">=</span> <span class=\"dv\">30</span>, <span class=\"dv\">50</span>\n\n<span class=\"kw\">def</span> gen_epochs(n, num_steps, batch_size, dataset<span class=\"op\">=</span>train_data):\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(n):\n        <span class=\"cf\">yield</span> reader.ptb_iterator(dataset, batch_size, num_steps)\n\n<span class=\"kw\">def</span> reset_graph():\n    <span class=\"cf\">if</span> <span class=\"st\">&#39;sess&#39;</span> <span class=\"kw\">in</span> <span class=\"bu\">globals</span>() <span class=\"kw\">and</span> sess:\n        sess.close()\n    tf.reset_default_graph()\n\n<span class=\"kw\">def</span> eval_network(sess, g, num_steps <span class=\"op\">=</span> num_steps, batch_size <span class=\"op\">=</span> batch_size):\n    losses <span class=\"op\">=</span> []\n    <span class=\"cf\">for</span> X, Y <span class=\"kw\">in</span> <span class=\"bu\">next</span>(gen_epochs(<span class=\"dv\">1</span>, num_steps, batch_size, dataset<span class=\"op\">=</span>val_data<span class=\"op\">+</span>test_data)):\n        feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: X, g[<span class=\"st\">&#39;y&#39;</span>]: Y, g[<span class=\"st\">&#39;deterministic&#39;</span>]: <span class=\"va\">True</span>}\n        loss_ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;loss&#39;</span>]], feed_dict)[<span class=\"dv\">0</span>]\n        losses.append(loss_)\n    <span class=\"cf\">return</span> np.mean(losses, axis<span class=\"op\">=</span><span class=\"dv\">0</span>)\n\n<span class=\"kw\">def</span> train_network(sess, g, num_epochs, num_steps <span class=\"op\">=</span> num_steps, batch_size <span class=\"op\">=</span> batch_size):\n    sess.run(tf.initialize_all_variables())\n    losses <span class=\"op\">=</span> []\n    val_losses <span class=\"op\">=</span> []\n    <span class=\"cf\">for</span> idx, epoch <span class=\"kw\">in</span> <span class=\"bu\">enumerate</span>(gen_epochs(num_epochs, num_steps, batch_size)):\n        loss <span class=\"op\">=</span> []\n        <span class=\"cf\">for</span> X, Y <span class=\"kw\">in</span> epoch:\n            feed_dict<span class=\"op\">=</span>{g[<span class=\"st\">&#39;x&#39;</span>]: X, g[<span class=\"st\">&#39;y&#39;</span>]: Y}\n            loss_, _ <span class=\"op\">=</span> sess.run([g[<span class=\"st\">&#39;loss&#39;</span>], g[<span class=\"st\">&#39;train_step&#39;</span>]], feed_dict)\n            loss.append(loss_)\n\n        val_loss <span class=\"op\">=</span> eval_network(sess, g)\n        <span class=\"bu\">print</span>(<span class=\"st\">&quot;Average perplexity for Epoch&quot;</span>, idx,\n              <span class=\"st\">&quot;: Training -&quot;</span>, np.exp(np.mean(loss)),\n              <span class=\"st\">&quot;Validation -&quot;</span>, np.exp(np.mean(val_loss)))\n        losses.append(np.mean(loss, axis<span class=\"op\">=</span><span class=\"dv\">0</span>))\n        val_losses.append(val_loss)\n    <span class=\"cf\">return</span> np.array(losses), np.array(val_losses)\n\n<span class=\"kw\">class</span> StateInitializer(Enum):\n    ZERO_STATE <span class=\"op\">=</span> <span class=\"dv\">1</span>\n    VARIABLE_STATE <span class=\"op\">=</span> <span class=\"dv\">2</span>\n    NOISY_ZERO_STATE <span class=\"op\">=</span> <span class=\"dv\">3</span>\n    NOISY_VARIABLE_STATE <span class=\"op\">=</span> <span class=\"dv\">4</span></code></pre></div>\n<h5 id=\"graph\">Graph</h5>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"kw\">def</span> build_graph(\n    state_initializer,\n    state_size <span class=\"op\">=</span> <span class=\"dv\">200</span>,\n    num_classes <span class=\"op\">=</span> num_classes,\n    batch_size <span class=\"op\">=</span> batch_size,\n    num_steps <span class=\"op\">=</span> num_steps,\n    num_layers <span class=\"op\">=</span> <span class=\"dv\">2</span>):\n\n    reset_graph()\n\n    x <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, num_steps], name<span class=\"op\">=</span><span class=\"st\">&#39;input_placeholder&#39;</span>)\n    y <span class=\"op\">=</span> tf.placeholder(tf.int32, [batch_size, num_steps], name<span class=\"op\">=</span><span class=\"st\">&#39;labels_placeholder&#39;</span>)\n    lr <span class=\"op\">=</span> tf.constant(<span class=\"fl\">1.</span>)\n    deterministic <span class=\"op\">=</span> tf.constant(<span class=\"va\">False</span>)\n\n    embeddings <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;embedding_matrix&#39;</span>, [num_classes, state_size])\n\n    rnn_inputs <span class=\"op\">=</span> tf.nn.embedding_lookup(embeddings, x)\n\n    cell <span class=\"op\">=</span> tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple<span class=\"op\">=</span><span class=\"va\">True</span>)\n    cell <span class=\"op\">=</span> tf.nn.rnn_cell.MultiRNNCell([cell] <span class=\"op\">*</span> num_layers, state_is_tuple<span class=\"op\">=</span><span class=\"va\">True</span>)\n\n    <span class=\"cf\">if</span> state_initializer <span class=\"op\">==</span> StateInitializer.ZERO_STATE:\n        initializer <span class=\"op\">=</span> zero_state_initializer\n    <span class=\"cf\">elif</span> state_initializer <span class=\"op\">==</span> StateInitializer.VARIABLE_STATE:\n        initializer <span class=\"op\">=</span> make_variable_state_initializer()\n    <span class=\"cf\">elif</span> state_initializer <span class=\"op\">==</span> StateInitializer.NOISY_ZERO_STATE:\n        initializer <span class=\"op\">=</span> make_gaussian_state_initializer(zero_state_initializer,\n                                                     deterministic)\n    <span class=\"cf\">elif</span> state_initializer <span class=\"op\">==</span> StateInitializer.NOISY_VARIABLE_STATE:\n        initializer <span class=\"op\">=</span> make_gaussian_state_initializer(make_variable_state_initializer(),\n                                                      deterministic)\n\n    init_state <span class=\"op\">=</span> get_initial_cell_state(cell, initializer, batch_size, tf.float32)\n    rnn_outputs, final_state <span class=\"op\">=</span> tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state<span class=\"op\">=</span>init_state)\n\n    <span class=\"cf\">with</span> tf.variable_scope(<span class=\"st\">&#39;softmax&#39;</span>):\n        W <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;W&#39;</span>, [state_size, num_classes])\n        b <span class=\"op\">=</span> tf.get_variable(<span class=\"st\">&#39;b&#39;</span>, [num_classes], initializer<span class=\"op\">=</span>tf.constant_initializer(<span class=\"fl\">0.0</span>))\n\n    <span class=\"co\">#reshape rnn_outputs and y so we can get the logits in a single matmul</span>\n    rnn_outputs <span class=\"op\">=</span> tf.reshape(rnn_outputs, [<span class=\"op\">-</span><span class=\"dv\">1</span>, state_size])\n    y_reshaped <span class=\"op\">=</span> tf.reshape(y, [<span class=\"op\">-</span><span class=\"dv\">1</span>])\n\n    logits <span class=\"op\">=</span> tf.matmul(rnn_outputs, W) <span class=\"op\">+</span> b\n\n    losses <span class=\"op\">=</span> tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped),\n                        [batch_size, num_steps])\n\n    loss_by_timestep <span class=\"op\">=</span> tf.reduce_mean(losses, reduction_indices<span class=\"op\">=</span><span class=\"dv\">0</span>)\n    train_step <span class=\"op\">=</span> tf.train.AdamOptimizer().minimize(loss_by_timestep)\n\n    <span class=\"cf\">return</span> <span class=\"bu\">dict</span>(\n        x <span class=\"op\">=</span> x,\n        y <span class=\"op\">=</span> y,\n        lr <span class=\"op\">=</span> lr,\n        deterministic <span class=\"op\">=</span> deterministic,\n        init_state <span class=\"op\">=</span> init_state,\n        final_state <span class=\"op\">=</span> final_state,\n        loss <span class=\"op\">=</span> loss_by_timestep,\n        train_step <span class=\"op\">=</span> train_step\n    )</code></pre></div>\n<h5 id=\"experiment\">Experiment</h5>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">tr_losses, val_losses <span class=\"op\">=</span> [<span class=\"va\">None</span>] <span class=\"op\">*</span> <span class=\"dv\">4</span>, [<span class=\"va\">None</span>] <span class=\"op\">*</span> <span class=\"dv\">4</span>\ng <span class=\"op\">=</span> build_graph(state_initializer<span class=\"op\">=</span>StateInitializer.ZERO_STATE)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\ntr_losses[<span class=\"dv\">0</span>], val_losses[<span class=\"dv\">0</span>] <span class=\"op\">=</span> train_network(sess, g, num_epochs<span class=\"op\">=</span><span class=\"dv\">20</span>)</code></pre></div>\n<pre><code>Average perplexity for Epoch 0 : Training - 674.599 Validation - 483.888\nAverage perplexity for Epoch 1 : Training - 421.366 Validation - 348.751\nAverage perplexity for Epoch 2 : Training - 305.943 Validation - 272.674\nAverage perplexity for Epoch 3 : Training - 241.748 Validation - 235.801\nAverage perplexity for Epoch 4 : Training - 205.29 Validation - 212.853\nAverage perplexity for Epoch 5 : Training - 180.5 Validation - 198.029\nAverage perplexity for Epoch 6 : Training - 160.867 Validation - 186.862\nAverage perplexity for Epoch 7 : Training - 145.657 Validation - 179.394\nAverage perplexity for Epoch 8 : Training - 133.973 Validation - 173.399\nAverage perplexity for Epoch 9 : Training - 124.281 Validation - 169.236\nAverage perplexity for Epoch 10 : Training - 115.586 Validation - 166.216\nAverage perplexity for Epoch 11 : Training - 108.34 Validation - 163.99\nAverage perplexity for Epoch 12 : Training - 101.959 Validation - 162.627\nAverage perplexity for Epoch 13 : Training - 96.3985 Validation - 162.423\nAverage perplexity for Epoch 14 : Training - 91.6309 Validation - 163.904\nAverage perplexity for Epoch 15 : Training - 87.29 Validation - 163.679\nAverage perplexity for Epoch 16 : Training - 83.2224 Validation - 164.169\nAverage perplexity for Epoch 17 : Training - 79.5156 Validation - 165.162\nAverage perplexity for Epoch 18 : Training - 76.1198 Validation - 166.714\nAverage perplexity for Epoch 19 : Training - 73.1628 Validation - 168.515</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_graph(state_initializer<span class=\"op\">=</span>StateInitializer.VARIABLE_STATE)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\ntr_losses[<span class=\"dv\">1</span>], val_losses[<span class=\"dv\">1</span>] <span class=\"op\">=</span> train_network(sess, g, num_epochs<span class=\"op\">=</span><span class=\"dv\">20</span>)</code></pre></div>\n<pre><code>Average perplexity for Epoch 0 : Training - 525.724 Validation - 325.364\nAverage perplexity for Epoch 1 : Training - 275.811 Validation - 239.312\nAverage perplexity for Epoch 2 : Training - 210.521 Validation - 204.103\nAverage perplexity for Epoch 3 : Training - 176.135 Validation - 184.352\nAverage perplexity for Epoch 4 : Training - 153.307 Validation - 171.528\nAverage perplexity for Epoch 5 : Training - 136.591 Validation - 162.493\nAverage perplexity for Epoch 6 : Training - 123.592 Validation - 156.533\nAverage perplexity for Epoch 7 : Training - 113.033 Validation - 152.028\nAverage perplexity for Epoch 8 : Training - 104.201 Validation - 149.743\nAverage perplexity for Epoch 9 : Training - 96.7272 Validation - 148.263\nAverage perplexity for Epoch 10 : Training - 90.313 Validation - 147.438\nAverage perplexity for Epoch 11 : Training - 84.7536 Validation - 147.409\nAverage perplexity for Epoch 12 : Training - 79.8758 Validation - 147.533\nAverage perplexity for Epoch 13 : Training - 75.5331 Validation - 148.11\nAverage perplexity for Epoch 14 : Training - 71.5848 Validation - 149.513\nAverage perplexity for Epoch 15 : Training - 67.9394 Validation - 151.243\nAverage perplexity for Epoch 16 : Training - 64.6299 Validation - 153.503\nAverage perplexity for Epoch 17 : Training - 61.6355 Validation - 156.37\nAverage perplexity for Epoch 18 : Training - 58.9116 Validation - 160.145\nAverage perplexity for Epoch 19 : Training - 56.4397 Validation - 164.863</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_graph(state_initializer<span class=\"op\">=</span>StateInitializer.NOISY_ZERO_STATE)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\ntr_losses[<span class=\"dv\">2</span>], val_losses[<span class=\"dv\">2</span>] <span class=\"op\">=</span> train_network(sess, g, num_epochs<span class=\"op\">=</span><span class=\"dv\">20</span>)</code></pre></div>\n<pre><code>Average perplexity for Epoch 0 : Training - 625.676 Validation - 407.948\nAverage perplexity for Epoch 1 : Training - 337.045 Validation - 277.074\nAverage perplexity for Epoch 2 : Training - 245.198 Validation - 230.573\nAverage perplexity for Epoch 3 : Training - 202.941 Validation - 205.394\nAverage perplexity for Epoch 4 : Training - 175.752 Validation - 189.294\nAverage perplexity for Epoch 5 : Training - 156.077 Validation - 178.006\nAverage perplexity for Epoch 6 : Training - 141.035 Validation - 170.011\nAverage perplexity for Epoch 7 : Training - 128.985 Validation - 164.033\nAverage perplexity for Epoch 8 : Training - 118.946 Validation - 160.09\nAverage perplexity for Epoch 9 : Training - 110.475 Validation - 157.405\nAverage perplexity for Epoch 10 : Training - 103.191 Validation - 155.624\nAverage perplexity for Epoch 11 : Training - 96.9187 Validation - 154.584\nAverage perplexity for Epoch 12 : Training - 91.4146 Validation - 154.25\nAverage perplexity for Epoch 13 : Training - 86.494 Validation - 154.48\nAverage perplexity for Epoch 14 : Training - 82.1429 Validation - 155.172\nAverage perplexity for Epoch 15 : Training - 78.1957 Validation - 156.681\nAverage perplexity for Epoch 16 : Training - 74.6005 Validation - 158.523\nAverage perplexity for Epoch 17 : Training - 71.3612 Validation - 160.869\nAverage perplexity for Epoch 18 : Training - 68.3056 Validation - 163.278\nAverage perplexity for Epoch 19 : Training - 65.4805 Validation - 165.645</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">g <span class=\"op\">=</span> build_graph(state_initializer<span class=\"op\">=</span>StateInitializer.NOISY_VARIABLE_STATE)\nsess <span class=\"op\">=</span> tf.InteractiveSession()\ntr_losses[<span class=\"dv\">3</span>], val_losses[<span class=\"dv\">3</span>] <span class=\"op\">=</span> train_network(sess, g, num_epochs<span class=\"op\">=</span><span class=\"dv\">20</span>)</code></pre></div>\n<pre><code>Average perplexity for Epoch 0 : Training - 517.27 Validation - 331.341\nAverage perplexity for Epoch 1 : Training - 278.846 Validation - 239.6\nAverage perplexity for Epoch 2 : Training - 210.333 Validation - 203.027\nAverage perplexity for Epoch 3 : Training - 174.959 Validation - 182.456\nAverage perplexity for Epoch 4 : Training - 151.81 Validation - 169.388\nAverage perplexity for Epoch 5 : Training - 135.121 Validation - 160.613\nAverage perplexity for Epoch 6 : Training - 122.301 Validation - 154.474\nAverage perplexity for Epoch 7 : Training - 111.991 Validation - 150.337\nAverage perplexity for Epoch 8 : Training - 103.425 Validation - 147.664\nAverage perplexity for Epoch 9 : Training - 96.1806 Validation - 145.957\nAverage perplexity for Epoch 10 : Training - 89.8921 Validation - 145.308\nAverage perplexity for Epoch 11 : Training - 84.3145 Validation - 145.255\nAverage perplexity for Epoch 12 : Training - 79.3745 Validation - 146.052\nAverage perplexity for Epoch 13 : Training - 74.96 Validation - 147.01\nAverage perplexity for Epoch 14 : Training - 71.0005 Validation - 148.22\nAverage perplexity for Epoch 15 : Training - 67.3658 Validation - 150.713\nAverage perplexity for Epoch 16 : Training - 64.0655 Validation - 153.78\nAverage perplexity for Epoch 17 : Training - 61.0874 Validation - 157.101\nAverage perplexity for Epoch 18 : Training - 58.3892 Validation - 160.376\nAverage perplexity for Epoch 19 : Training - 55.9478 Validation - 164.157</code></pre>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\"><span class=\"im\">import</span> matplotlib.pyplot <span class=\"im\">as</span> plt\n<span class=\"op\">%</span>matplotlib inline\n<span class=\"im\">import</span> seaborn <span class=\"im\">as</span> sns\nsns.<span class=\"bu\">set</span>(color_codes<span class=\"op\">=</span><span class=\"va\">True</span>)\n\n<span class=\"kw\">def</span> best_epoch(val_losses):\n    <span class=\"cf\">return</span> np.argmin(np.mean(val_losses, axis<span class=\"op\">=</span><span class=\"dv\">1</span>))\n\nlabels <span class=\"op\">=</span> [<span class=\"st\">&#39;Zero&#39;</span>, <span class=\"st\">&#39;Variable&#39;</span>, <span class=\"st\">&#39;Noisy&#39;</span>, <span class=\"st\">&#39;Noisy Variable&#39;</span>]\n\n<span class=\"kw\">def</span> plot_losses(losses, title, y_range):\n    <span class=\"kw\">global</span> val_losses\n    fig, ax <span class=\"op\">=</span> plt.subplots()\n    <span class=\"cf\">for</span> i <span class=\"kw\">in</span> <span class=\"bu\">range</span>(<span class=\"bu\">len</span>(losses)):\n        data <span class=\"op\">=</span> np.exp(losses[i][best_epoch(val_losses[i])])\n        ax.plot(<span class=\"bu\">range</span>(<span class=\"dv\">0</span>,num_steps),data,label<span class=\"op\">=</span>labels[i])\n    ax.set_xlabel(<span class=\"st\">&#39;Step number&#39;</span>)\n    ax.set_ylabel(<span class=\"st\">&#39;Average loss&#39;</span>)\n    ax.set_ylim(y_range)\n    ax.set_title(title)\n    ax.legend(loc<span class=\"op\">=</span><span class=\"dv\">1</span>)\n    plt.show()</code></pre></div>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plot_losses(tr_losses, <span class=\"st\">&#39;Best epoch training perplexities&#39;</span>, [<span class=\"dv\">70</span>, <span class=\"dv\">110</span>])</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/NonzeroStateInit_output_25_0.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<div class=\"sourceCode\"><pre class=\"sourceCode python\"><code class=\"sourceCode python\">plot_losses(val_losses, <span class=\"st\">&#39;Best epoch validation perplexities&#39;</span>, [<span class=\"dv\">120</span>, <span class=\"dv\">200</span>])</code></pre></div>\n<figure>\n<img src=\"https://r2rt.com/static/images/NonzeroStateInit_output_26_0.png\" alt=\"png\" /><figcaption>png</figcaption>\n</figure>\n<h3 id=\"empirical-results\">Empirical results</h3>\n<p>From the above experiment we make the following observations:</p>\n<ul>\n<li>All non-zero state intializations sped up training and improved generalization.</li>\n<li>Training the initial state as a variable was more effective than using a noisy zero-mean initial state.</li>\n<li>Adding noise to a variable initial state provided only marginal benefit.</li>\n</ul>\n<p>Finally, I would note that “truncating” the PTB dataset produced worse results than would be obtained if the dataset were not truncated, even if we use noisy or variable state initializations. We can see this by comparing the above results to the “non-regularized LSTM” from <a href=\"https://arxiv.org/pdf/1409.2329v5.pdf\">Zaremba et al. (2015)</a>, which had a very similar architecture, but did not truncate the sequences in the dataset. I would expect truncation will have this effect in general, so that these non-zero state initializations will only really be useful for datasets that have many naturally-occuring state resets.</p>\n</body>\n</html>"
}