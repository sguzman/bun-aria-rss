{
  "title": "GPU Dask Arrays, first steps",
  "link": "",
  "updated": "2019-01-03T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2019/01/03/dask-array-gpus-first-steps",
  "content": "<p>The following code creates and manipulates 2 TB of randomly generated data.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n\n<span class=\"n\">rs</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">RandomState</span><span class=\"p\">()</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">rs</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">500000</span><span class=\"p\">,</span> <span class=\"mi\">500000</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">))</span>\n<span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[::</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">].</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'threads'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>On a single CPU, this computation takes two hours.</p>\n\n<p>On an eight-GPU single-node system this computation takes nineteen seconds.</p>\n\n<h2 id=\"combine-dask-array-with-cupy\">Combine Dask Array with CuPy</h2>\n\n<p>Actually this computation isn’t that impressive.\nIt’s a simple workload,\nfor which most of the time is spent creating and destroying random data.\nThe computation and communication patterns are simple,\nreflecting the simplicity commonly found in data processing workloads.</p>\n\n<p>What <em>is</em> impressive is that we were able to create a distributed parallel GPU\narray quickly by composing these three existing libraries:</p>\n\n<ol>\n  <li>\n    <p><a href=\"https://cupy.chainer.org/\">CuPy</a> provides a partial implementation of\nNumpy on the GPU.</p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.dask.org/en/latest/array.html\">Dask Array</a> provides chunked\nalgorithms on top of Numpy-like libraries like Numpy and CuPy.</p>\n\n    <p>This enables us to operate on more data than we could fit in memory\nby operating on that data in chunks.</p>\n  </li>\n  <li>\n    <p>The <a href=\"https://distributed.dask.org\">Dask distributed</a> task scheduler runs\nthose algorithms in parallel, easily coordinating work across many CPU\ncores or GPUs.</p>\n  </li>\n</ol>\n\n<p>These tools already exist. We had to connect them together with a small amount\nof glue code and minor modifications.  By mashing these tools together we can\nquickly build and switch between different architectures to explore what is\nbest for our application.</p>\n\n<p>For this example we relied on the following changes upstream:</p>\n\n<ul>\n  <li><a href=\"https://github.com/cupy/cupy/pull/1689\">cupy/cupy #1689: Support Numpy arrays as seeds in RandomState</a></li>\n  <li><a href=\"https://github.com/dask/dask/pull/4041\">dask/dask #4041 Make da.RandomState accessible to other modules</a></li>\n  <li><a href=\"https://github.com/dask/distributed/pull/2432\">dask/distributed #2432: Add LocalCUDACluster</a></li>\n</ul>\n\n<h2 id=\"comparison-among-singlemulti-cpugpu\">Comparison among single/multi CPU/GPU</h2>\n\n<p>We can now easily run some experiments on different architectures.  This is\neasy because …</p>\n\n<ul>\n  <li>We can switch between CPU and GPU by switching between Numpy and CuPy.</li>\n  <li>We can switch between single/multi-CPU-core and single/multi-GPU\nby switching between Dask’s different task schedulers.</li>\n</ul>\n\n<p>These libraries allow us to quickly judge the costs of this computation for\nthe following hardware choices:</p>\n\n<ol>\n  <li>Single-threaded CPU</li>\n  <li>Multi-threaded CPU with 40 cores (80 H/T)</li>\n  <li>Single-GPU</li>\n  <li>Multi-GPU on a single machine with 8 GPUs</li>\n</ol>\n\n<p>We present code for these four choices below,\nbut first,\nwe present a table of results.</p>\n\n<h3 id=\"results\">Results</h3>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n  <tr>\n    <th>Architecture</th>\n    <th>Time</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th> Single CPU Core </th>\n      <td> 2hr 39min </td>\n    </tr>\n    <tr>\n      <th> Forty CPU Cores </th>\n      <td> 11min 30s </td>\n    </tr>\n    <tr>\n      <th> One GPU </th>\n      <td> 1 min 37s </td>\n    </tr>\n    <tr>\n      <th> Eight GPUs </th>\n      <td> 19s </td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"setup\">Setup</h3>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">cupy</span>\n<span class=\"kn\">import</span> <span class=\"nn\">dask.array</span> <span class=\"k\">as</span> <span class=\"n\">da</span>\n\n<span class=\"c1\"># generate chunked dask arrays of mamy numpy random arrays\n</span><span class=\"n\">rs</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">RandomState</span><span class=\"p\">()</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">rs</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">500000</span><span class=\"p\">,</span> <span class=\"mi\">500000</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">))</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">nbytes</span> <span class=\"o\">/</span> <span class=\"mf\">1e9</span><span class=\"p\">)</span>  <span class=\"c1\"># 2 TB\n# 2000.0\n</span></code></pre></div></div>\n\n<h3 id=\"cpu-timing\">CPU timing</h3>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[::</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">].</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'single-threaded'</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[::</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">].</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'threads'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h3 id=\"single-gpu-timing\">Single GPU timing</h3>\n\n<p>We switch from CPU to GPU by changing our data source to generate CuPy arrays\nrather than NumPy arrays.  Everything else should more or less work the same\nwithout special handling for CuPy.</p>\n\n<p><em>(This actually isn’t true yet, many things in dask.array will break for\nnon-NumPy arrays, but we’re working on it actively both within Dask, within\nNumPy, and within the GPU array libraries.  Regardless, everything in this\nexample works fine.)</em></p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># generate chunked dask arrays of mamy cupy random arrays\n</span><span class=\"n\">rs</span> <span class=\"o\">=</span> <span class=\"n\">da</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">RandomState</span><span class=\"p\">(</span><span class=\"n\">RandomState</span><span class=\"o\">=</span><span class=\"n\">cupy</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">RandomState</span><span class=\"p\">)</span>  <span class=\"c1\"># &lt;-- we specify cupy here\n</span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">rs</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">500000</span><span class=\"p\">,</span> <span class=\"mi\">500000</span><span class=\"p\">),</span> <span class=\"n\">chunks</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">))</span>\n</code></pre></div></div>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[::</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">].</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">(</span><span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"s\">'single-threaded'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<h3 id=\"multi-gpu-timing\">Multi GPU timing</h3>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">dask.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">Client</span><span class=\"p\">,</span> <span class=\"n\">LocalCUDACluster</span>  <span class=\"c1\"># this is experimental\n</span>\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">LocalCUDACluster</span><span class=\"p\">()</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span>\n\n<span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)[::</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">].</span><span class=\"nb\">sum</span><span class=\"p\">().</span><span class=\"n\">compute</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>And again, here are the results:</p>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n  <tr>\n    <th>Architecture</th>\n    <th>Time</th>\n  </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th> Single CPU Core </th>\n      <td> 2hr 39min </td>\n    </tr>\n    <tr>\n      <th> Forty CPU Cores </th>\n      <td> 11min 30s </td>\n    </tr>\n    <tr>\n      <th> One GPU </th>\n      <td> 1 min 37s </td>\n    </tr>\n    <tr>\n      <th> Eight GPUs </th>\n      <td> 19s </td>\n    </tr>\n  </tbody>\n</table>\n\n<p>First, this is my first time playing with an 40-core system.  I was surprised\nto see that many cores.  I was also pleased to see that Dask’s normal threaded\nscheduler happily saturates many cores.</p>\n\n<p><img src=\"https://mrocklin.github.io/blog/images/python-gil-8000-percent.png\" width=\"100%\" /></p>\n\n<p>Although later on it did dive down to around 5000-6000%, and if you do the math\nyou’ll see that we’re not getting a 40x speedup.  My <em>guess</em> is that\nperformance would improve if we were to play with some mixture of threads and\nprocesses, like having ten processes with eight threads each.</p>\n\n<p>The jump from the biggest multi-core CPU to a single GPU is still an order of\nmagnitude though.  The jump to multi-GPU is another order of magnitude, and\nbrings the computation down to 19s, which is short enough that I’m willing to\nwait for it to finish before walking away from my computer.</p>\n\n<p>Actually, it’s quite fun to watch on the dashboard (especially after you’ve\nbeen waiting for three hours for the sequential solution to run):</p>\n\n<blockquote class=\"imgur-embed-pub\" lang=\"en\" data-id=\"a/6hkPPwA\">\n  <p><a href=\"//imgur.com/6hkPPwA\"></a></p>\n</blockquote>\n<script async=\"\" src=\"//s.imgur.com/min/embed.js\" charset=\"utf-8\"></script>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>This computation was simple, but the range in architecture just explored was\nextensive.  We swapped out the underlying architecture from CPU to GPU (which\nhad an entirely different codebase) and tried both multi-core CPU parallelism\nas well as multi-GPU many-core parallelism.</p>\n\n<p>We did this in less than twenty lines of code, making this experiment something\nthat an undergraduate student or other novice could perform at home.\nWe’re approaching a point where experimenting with multi-GPU systems is\napproachable to non-experts (at least for array computing).</p>\n\n<p><a href=\"https://gist.github.com/mrocklin/57be0ca4143974e6015732d0baacc1cb\">Here is a notebook for the experiment above</a></p>\n\n<h2 id=\"room-for-improvement\">Room for improvement</h2>\n\n<p>We can work to expand the computation above in a variety of directions.\nThere is a ton of work we still have to do to make this reliable.</p>\n\n<ol>\n  <li>\n    <p><strong>Use more complex array computing workloads</strong></p>\n\n    <p>The Dask Array algorithms were designed first around Numpy.  We’ve only\nrecently started making them more generic to other kinds of arrays (like\nGPU arrays, sparse arrays, and so on).  As a result there are still many\nbugs when exploring these non-Numpy workloads.</p>\n\n    <p>For example if you were to switch <code class=\"language-plaintext highlighter-rouge\">sum</code> for <code class=\"language-plaintext highlighter-rouge\">mean</code> in the computation above\nyou would get an error because our <code class=\"language-plaintext highlighter-rouge\">mean</code> computation contains an easy to\nfix error that assumes Numpy arrays exactly.</p>\n  </li>\n  <li>\n    <p><strong>Use Pandas and cuDF instead of Numpy and CuPy</strong></p>\n\n    <p>The cuDF library aims to reimplement the Pandas API on the GPU,\nmuch like how CuPy reimplements the NumPy API.\nUsing Dask DataFrame with cuDF will require some work on both sides,\nbut is quite doable.</p>\n\n    <p>I believe that there is plenty of low-hanging fruit here.</p>\n  </li>\n  <li>\n    <p><strong>Improve and move LocalCUDACluster</strong></p>\n\n    <p>The <code class=\"language-plaintext highlighter-rouge\">LocalCUDAClutster</code> class used above is an experimental <code class=\"language-plaintext highlighter-rouge\">Cluster</code> type\nthat creates as many workers locally as you have GPUs, and assigns each\nworker to prefer a different GPU.  This makes it easy for people to load\nbalance across GPUs on a single-node system without thinking too much about\nit.  This appears to be a common pain-point in the ecosystem today.</p>\n\n    <p>However, the LocalCUDACluster probably shouldn’t live in the\n<code class=\"language-plaintext highlighter-rouge\">dask/distributed</code> repository (it seems too CUDA specific) so will probably\nmove to some dask-cuda repository.  Additionally there are still many\nquestions about how to handle concurrency on top of GPUs, balancing between\nCPU cores and GPU cores, and so on.</p>\n  </li>\n  <li>\n    <p><strong>Multi-node computation</strong></p>\n\n    <p>There’s no reason that we couldn’t accelerate computations like these\nfurther by using multiple multi-GPU nodes.  This is doable today with\nmanual setup, but we should also improve the existing deployment solutions\n<a href=\"https://kubernetes.dask.org\">dask-kubernetes</a>,\n<a href=\"https://yarn.dask.org\">dask-yarn</a>, and\n<a href=\"https://jobqueue.dask.org\">dask-jobqueue</a>, to make this easier for\nnon-experts who want to use a cluster of multi-GPU resources.</p>\n  </li>\n  <li>\n    <p><strong>Expense</strong></p>\n\n    <p>The machine I ran this on is expensive.  Well, it’s nowhere close to as\nexpensive to own and operate as a traditional cluster that you would need\nfor these kinds of results, but it’s still well beyond the price point of a\nhobbyist or student.</p>\n\n    <p>It would be useful to run this on a more budget system to get a sense of\nthe tradeoffs on more reasonably priced systems.  I should probably also\nlearn more about provisioning GPUs on the cloud.</p>\n  </li>\n</ol>\n\n<h3 id=\"come-help\">Come help!</h3>\n\n<p>If the work above sounds interesting to you then come help!\nThere is a lot of low-hanging and high impact work to do.</p>\n\n<p>If you’re interested in being paid to focus more on these topics, then consider\napplying for a job.  The NVIDIA corporation is hiring around the use of Dask\nwith GPUs.</p>\n\n<ul>\n  <li><a href=\"https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/US-TX-Austin/Senior-Library-Software-Engineer---RAPIDS_JR1919608-1\">Senior Library Software Engineer - RAPIDS</a></li>\n</ul>\n\n<p>That’s a fairly generic posting.  If you’re interested the posting doesn’t seem\nto fit then please apply anyway and we’ll tweak things.</p>"
}