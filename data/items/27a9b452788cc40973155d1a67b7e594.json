{
  "title": "Querying S3 Object Stores with Presto or Trino",
  "link": "",
  "published": "2021-03-03T00:00:00-06:00",
  "updated": "2021-03-03T00:00:00-06:00",
  "id": "http://janakiev.com/blog/presto-trino-s3",
  "content": "<p>Querying big data on Hadoop can be challenging to get running, but alternatively, many solutions are using S3 object stores which you can access and query with Presto or Trino. In this guide you will see how to install, configure, and run Presto or Trino on Debian or Ubuntu with the S3 object store of your choice and the Hive standalone metastore.</p>\n\n<p>This guide was tested on <a href=\"https://min.io/\">MinIO</a>, <a href=\"https://www.linode.com/products/object-storage/\">Linode object storage</a>, and <a href=\"https://aws.amazon.com/s3/\">AWS S3</a>, with a personal preference for <a href=\"https://www.linode.com/?r=e54e6c8185f5399de2527f9f3bc7cde39bbcc624\">Linode</a>. For installing Trino or Presto I recommend <a href=\"https://hetzner.cloud/?ref=FkpdQcqbGXhP\">Hetzner</a> VPS instances. To run the queries yourself, you can check out this <a href=\"https://github.com/njanakiev/trino-minio-docker\">repository</a> to help you replicate this guide locally with MinIO and docker. If you want to run Presto or Trino on Hadoop HDFS, have a look at this <a href=\"https://janakiev.com/blog/presto-cluster/\">tutorial</a>.</p>\n\n<h1 id=\"s3-object-stores\">S3 Object Stores</h1>\n\n<p>Amazon started <a href=\"https://aws.amazon.com/s3/\">AWS Simple Storage Service (S3)</a> in 2006 and it is the most popular <a href=\"https://en.wikipedia.org/wiki/Object_storage\">object storage</a> to date. In S3 your data is grouped in buckets with a globally unique name and the data can be stored unstructured without a schema. It offers high scalability, high availability, and low latency storage and it is a common alternative to using HDFS and the <a href=\"https://hadoop.apache.org/\">Hadoop</a> ecosystem. Although S3 is no official standard, many vendors and projects support the interface and are compatible with S3. Depending on your needs and applications, there is a variety of possible S3 compatible object stores to choose from:</p>\n\n<ul>\n  <li><a href=\"https://min.io/\">MinIO</a></li>\n  <li><a href=\"https://ceph.io/\">Ceph</a></li>\n  <li><a href=\"https://aws.amazon.com/s3/\">AWS S3</a></li>\n  <li><a href=\"https://contabo.com/object-storage/\">Contabo Object Storage</a></li>\n  <li><a href=\"https://www.linode.com/products/object-storage/\">Linode Object Storage</a></li>\n  <li><a href=\"https://azure.microsoft.com/en-us/services/storage/blobs/\">Azure Blob Storage</a></li>\n  <li><a href=\"https://cloud.google.com/storage\">Google Cloud Storage</a></li>\n  <li><a href=\"https://www.digitalocean.com/products/spaces\">DigitalOcean Object Storage</a></li>\n</ul>\n\n<h2 id=\"interacting-with-s3-buckets-using-s3cmd\">Interacting with S3 Buckets using s3cmd</h2>\n\n<p>In order to interact with your S3 bucket, you need some tool. Many vendors have their own CLI tooling like the <a href=\"https://docs.aws.amazon.com/cli/latest/reference/s3/\">AWS CLI</a> or <a href=\"https://www.linode.com/docs/guides/linode-cli/\">Linode CLI</a>. A platform independent tool is <a href=\"https://s3tools.org/s3cmd\">s3cmd</a> which we will be using here. You can install s3cmd on Ubuntu/Debian with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>apt update\n<span class=\"nb\">sudo </span>apt <span class=\"nb\">install </span>s3cmd\n</code></pre></div></div>\n\n<p>Now, you need to configure the s3cmd environment with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>s3cmd <span class=\"nt\">--configure</span>\n</code></pre></div></div>\n\n<p>After following all prompts, you can find the configuration in <code class=\"language-plaintext highlighter-rouge\">~/.s3cfg</code>. If you want to save it as a custom profile you can add the <code class=\"language-plaintext highlighter-rouge\">-c</code>/<code class=\"language-plaintext highlighter-rouge\">--config</code> argument with the location of the configuration:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>s3cmd <span class=\"nt\">--config</span> aws.s3cfg <span class=\"nt\">--configure</span>\n</code></pre></div></div>\n\n<p><strong>Warning: the access keys are saved in plain text</strong>. Here is a list of useful commands when working with <code class=\"language-plaintext highlighter-rouge\">s3cmd</code>:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd mb s3://bucket</code> Make bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd rb s3://bucket</code> Remove bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd ls</code> List available buckets</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd ls s3://bucket</code> List folders within bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd get s3://bucket/file.txt</code> Download file from bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd get -r s3://bucket/folder</code> Download recursively files from bucket/directory</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd put file.txt s3://bucket</code> Upload files to bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd put -r folder s3://bucket</code> Upload folder to bucket</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">s3cmd del s3://bucket/file.txt</code> Delete file or folder from bucket</li>\n</ul>\n\n<p>For more commands and documentation, have a look at the <a href=\"https://s3tools.org/usage\">s3cmd usage</a>.</p>\n\n<h1 id=\"hive-standalone-metastore\">Hive Standalone Metastore</h1>\n\n<p>In any database, you need a place to manage the various tables, schemas, relationships, and views. This is commonly done in a metastore. When using S3 it is common to have the tables stored as CSV, <a href=\"https://parquet.apache.org/\">Apache Parquet</a>, and <a href=\"https://orc.apache.org/\">Apache ORC</a> files among others. To store the schemas of those tables Trino/Presto needs <a href=\"https://hive.apache.org/\">Apache Hive</a> for the query engine to access the metadata of those tables. Hive is also commonly used as a metastore in the Hadoop ecosystem in projects like Apache Impala, Apache Spark, and Apache Drill.</p>\n\n<h2 id=\"installation\">Installation</h2>\n\n<p>In my <a href=\"https://janakiev.com/blog/presto-cluster/\">previous tutorial</a>, the installation relied on Hadoop and HDFS, but in this case, it will use a standalone version of the Hive metastore which runs without the rest of Hive. Hive metastore requires a database to store the schemas. For this, you can use DerbyDB, MySQL, MariaDB, and PostgreSQL. In this tutorial, you will see how to use it with MariaDB. Further, the metastore and Trino/Presto require Java 11. To install MariaDB and the Java 11 JRE, type:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>apt update\n<span class=\"nb\">sudo </span>apt <span class=\"nb\">install</span> <span class=\"nt\">-y</span> <span class=\"se\">\\</span>\n    mariadb openjdk-11-jre-headless\n</code></pre></div></div>\n\n<p>Now make sure itâ€™s enabled and running by typing:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>systemctl <span class=\"nb\">enable </span>mariadb.service\n<span class=\"nb\">sudo </span>systemctl start mariadb.service\n</code></pre></div></div>\n\n<p>To check its status, you can type <code class=\"language-plaintext highlighter-rouge\">systemctl status mariadb.service</code>. Next, you will need to create a user and a database for the Hive metastore, which you can do with the following command:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">sudo </span>mysql <span class=\"nt\">-u</span> root <span class=\"nt\">-e</span> <span class=\"s2\">\"\n    DROP DATABASE IF EXISTS metastore;\n    CREATE DATABASE metastore;\n\n    CREATE USER 'hive'@localhost IDENTIFIED BY 'hive';\n    GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost';\n    FLUSH PRIVILEGES;\"</span>\n</code></pre></div></div>\n\n<p>Great, now that the database is set up, we can continue with downloading and extracting the metastore to <code class=\"language-plaintext highlighter-rouge\">/usr/local/metastore</code> with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://repo1.maven.org/maven2/org/apache/hive/hive-standalone-metastore/3.1.2/hive-standalone-metastore-3.1.2-bin.tar.gz\"</span>\n<span class=\"nb\">tar</span> <span class=\"nt\">-zxvf</span> hive-standalone-metastore-3.1.2-bin.tar.gz\n<span class=\"nb\">sudo mv </span>apache-hive-metastore-3.1.2-bin /usr/local/metastore\n<span class=\"nb\">sudo chown </span>user:user /usr/local/metastore\n</code></pre></div></div>\n\n<p>If you want to use another version instead of version <code class=\"language-plaintext highlighter-rouge\">3.1.2</code> have a look at the following <a href=\"https://repo1.maven.org/maven2/org/apache/hive/hive-standalone-metastore/\">list</a>. Hive metastore requires some dependencies from Hadoop, therefore you need to download Hadoop as well with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://downloads.apache.org/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz\"</span>\n<span class=\"nb\">tar </span>xvf hadoop-3.2.1.tar.gz\n<span class=\"nb\">sudo mv </span>hadoop-3.2.1 /usr/local/hadoop\n<span class=\"nb\">sudo chown </span>user:user /usr/local/hadoop\n</code></pre></div></div>\n\n<p>There are a few dependencies that you need to copy and change to make it compatible with S3 and Hadoop. Here are the commands for that:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">rm</span> /usr/local/metastore/lib/guava-19.0.jar\n<span class=\"nb\">cp</span> /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar <span class=\"se\">\\</span>\n  /usr/local/metastore/lib/\n<span class=\"nb\">cp</span> /usr/local/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.1.jar <span class=\"se\">\\</span>\n  /usr/local/metastore/lib/\n<span class=\"nb\">cp</span> /usr/local/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar <span class=\"se\">\\</span>\n  /usr/local/metastore/lib/\n</code></pre></div></div>\n\n<h2 id=\"configuration\">Configuration</h2>\n\n<p>Now, Hive needs connection details to your S3 bucket. This can be done in the <code class=\"language-plaintext highlighter-rouge\">/usr/local/metastore/conf/metastore-site.xml</code> file. Open the existing <code class=\"language-plaintext highlighter-rouge\">metastore-site.xml</code> and add the following properties within the <code class=\"language-plaintext highlighter-rouge\">&lt;configuration&gt;</code> section:</p>\n\n<div class=\"language-xml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>javax.jdo.option.ConnectionURL<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>javax.jdo.option.ConnectionDriverName<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>com.mysql.jdbc.Driver<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>javax.jdo.option.ConnectionUserName<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>hive<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>javax.jdo.option.ConnectionPassword<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>hive<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>hive.metastore.event.db.notification.api.auth<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>false<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n</code></pre></div></div>\n\n<p>Further, add those properties to specify the S3 connection:</p>\n\n<div class=\"language-xml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>fs.s3a.access.key<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>S3_ACCESS_KEY<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>fs.s3a.secret.key<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>S3_SECRET_KEY<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>fs.s3a.connection.ssl.enabled<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>false<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>fs.s3a.path.style.access<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>true<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n<span class=\"nt\">&lt;property&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>fs.s3a.endpoint<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;value&gt;</span>S3_ENDPOINT<span class=\"nt\">&lt;/value&gt;</span>\n<span class=\"nt\">&lt;/property&gt;</span>\n</code></pre></div></div>\n\n<p>Additionally, you will need to define the <code class=\"language-plaintext highlighter-rouge\">JAVA_HOME</code> and <code class=\"language-plaintext highlighter-rouge\">HADOOP_HOME</code> environment variables, which you can set with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">JAVA_HOME</span><span class=\"o\">=</span>/usr/lib/jvm/java-11-openjdk-amd64\n<span class=\"nb\">export </span><span class=\"nv\">HADOOP_HOME</span><span class=\"o\">=</span>/usr/local/hadoop\n</code></pre></div></div>\n\n<p>To have those ready every time you open the shell, you need to append those two lines in the <code class=\"language-plaintext highlighter-rouge\">~/.bashrc</code> file. Once you have everything configured, you can initialize the metastore with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>/usr/local/metastore/bin/schematool <span class=\"nt\">-initSchema</span> <span class=\"nt\">-dbType</span> mysql\n</code></pre></div></div>\n\n<p>After the initialization is finished, you can start the metastore service with:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>/usr/local/metastore/bin/start-metastore &amp;\n</code></pre></div></div>\n\n<p>For more information about the metastore configuration, have a look at the <a href=\"https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+3.0+Administration\">documentation</a> and  more specifically on <a href=\"https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+3.0+Administration#AdminManualMetastore3.0Administration-RunningtheMetastoreWithoutHive\">Running the Metastore Without Hive</a>.</p>\n\n<h1 id=\"trino-and-presto\">Trino and Presto</h1>\n\n<p><a href=\"https://trino.io/\">Trino</a> and <a href=\"https://prestodb.io/\">Presto</a> are both open-source distributed query engines for big data across a large variety of data sources including HDFS, S3, PostgreSQL, MySQL, Cassandra, MongoDB, and Elasticsearch among others. To see the difference between both projects, have a look at this <a href=\"https://trino.io/blog/2020/12/27/announcing-trino.html\">article</a>. In this installation, you will see how to install Trino 352 in particular, but all the steps and queries were also tested on Presto 0.247.</p>\n\n<h2 id=\"installation-1\">Installation</h2>\n\n<p>After you have S3 and the Hive standalone metastore ready, you can proceed with installing and configuring Trino on your server. To install it, download it from <a href=\"https://trino.io/download.html\">here</a> and extract it in <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino</code> as outlined here:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://repo1.maven.org/maven2/io/trino/trino-server/352/trino-server-352.tar.gz\"</span>\n<span class=\"nb\">tar</span> <span class=\"nt\">-xzvf</span> trino-server-352.tar.gz\n<span class=\"nb\">sudo mv </span>trino-server-352 /usr/local/trino\n<span class=\"nb\">sudo chown</span> <span class=\"nv\">$$</span>USER:<span class=\"nv\">$$</span>USER /usr/local/trino\n</code></pre></div></div>\n\n<p>Additionally, you will need the CLI in order to access the query engine, which you can download to the <code class=\"language-plaintext highlighter-rouge\">bin</code> folder of the same directory and make it executable:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>wget <span class=\"s2\">\"https://repo1.maven.org/maven2/io/trino/trino-cli/352/trino-cli-352-executable.jar\"</span>\n<span class=\"nb\">mv </span>trino-cli-352-executable.jar /usr/local/trino/bin/trino\n<span class=\"nb\">sudo chmod</span> +x /usr/local/trino/bin/trino\n</code></pre></div></div>\n\n<h2 id=\"configuration-1\">Configuration</h2>\n\n<p>Lastly, you need to configure Trino. For this, you have a few configuration files that are required. First you need the configuration for the JVM in <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino/etc/jvm.config</code>. You can fill it with the following contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>-server\n-Xmx6G\n-XX:+UseG1GC\n-XX:G1HeapRegionSize=32M\n-XX:+UseGCOverheadLimit\n-XX:+ExplicitGCInvokesConcurrent\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:+ExitOnOutOfMemoryError\n-Djdk.attach.allowAttachSelf=true\n</code></pre></div></div>\n\n<p>Here, make sure that you utilize your RAM memory properly by specifying <code class=\"language-plaintext highlighter-rouge\">-Xmx</code> with around 80% of your available memory. This way you will have enough memory for the system as long as you donâ€™t have anything else running on this machine. In this example, it is set to 6 GB. Next up, is the <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino/etc/node.properties</code> which contains configuration for each node:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>node.environment=production\nnode.id=ffffffff-ffff-ffff-ffff-ffffffffffff\nnode.data-dir=/usr/local/trino/data\n</code></pre></div></div>\n\n<p>Here you need to specify the name of the environment with <code class=\"language-plaintext highlighter-rouge\">node.environment</code>, the unique identifier of the node with <code class=\"language-plaintext highlighter-rouge\">node.id</code>, and finally the directory of the data directory with <code class=\"language-plaintext highlighter-rouge\">node.data-dir</code>. Nest, you need to add the configuration for the Trino server in <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino/etc/config.properties</code>. Here is a possible configuration:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\nquery.max-memory=50GB\nquery.max-memory-per-node=1GB\nquery.max-total-memory-per-node=2GB\ndiscovery-server.enabled=true\ndiscovery.uri=http://localhost:8080\n</code></pre></div></div>\n\n<p>The configuration of how much memory requires some trial and error and depends on the expected workload and the number of queries that will run simultaneously. A good tutorial on the topic that you can follow is <a href=\"https://techjogging.com/memory-setup-prestodb-cluster.html\">Memory Configuration in Presto Cluster</a>. Finally, you need to configure the connection to S3. For this create the file <code class=\"language-plaintext highlighter-rouge\">/usr/local/trino/etc/catalog/hive.properties</code> with the following contents:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>connector.name=hive-hadoop2\nhive.metastore.uri=thrift://localhost:9083\nhive.s3.path-style-access=true\nhive.s3.endpoint=S3_ENDPOINT\nhive.s3.aws-access-key=S3_ACCESS_KEY\nhive.s3.aws-secret-key=S3_SECRET_KEY\nhive.s3.ssl.enabled=false\n</code></pre></div></div>\n\n<p>This should do the trick. For more information on the deployment and the Hive connector have a look at <a href=\"https://trino.io/docs/current/installation/deployment.html\">Deploying Trino</a>, the <a href=\"https://trino.io/docs/current/connector/hive.html\">Hive connector</a> documentation, and the <a href=\"https://trino.io/docs/current/connector/hive-s3.html\">Hive connector with Amazon S3</a> documentation. For multi-node configuration, follow this <a href=\"https://janakiev.com/blog/presto-cluster/\">tutorial</a>.</p>\n\n<p>Now, you should be able to start Trino by running:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>/usr/local/trino/bin/launcher start\n</code></pre></div></div>\n\n<p>Once it is running, you should open the Trino web UI at <code class=\"language-plaintext highlighter-rouge\">localhost:8080</code> with the port previously defined in <code class=\"language-plaintext highlighter-rouge\">config.properties</code>. Once you run queries, they should be listed there.</p>\n\n<h2 id=\"query-data-stored-on-s3\">Query Data stored on S3</h2>\n\n<p>We will work with a previously uploaded parquet file which you can find in this <a href=\"https://github.com/njanakiev/trino-minio-docker\">repository</a>, which was converted from the famous <a href=\"https://archive.ics.uci.edu/ml/datasets/iris\">Iris data set</a>. In this example the file is stored on the bucket at <code class=\"language-plaintext highlighter-rouge\">s3a://iris/iris_parquet/iris.parq</code>. First, you need to create a schema to access the file which you can do by running the following SQL statement:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">SCHEMA</span> <span class=\"n\">IF</span> <span class=\"k\">NOT</span> <span class=\"k\">EXISTS</span> <span class=\"n\">hive</span><span class=\"p\">.</span><span class=\"n\">iris</span>\n<span class=\"k\">WITH</span> <span class=\"p\">(</span><span class=\"k\">location</span> <span class=\"o\">=</span> <span class=\"s1\">'s3a://iris/'</span><span class=\"p\">);</span>\n</code></pre></div></div>\n\n<p>Next, you need to create a table to the existing data set on S3 with:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">NOT</span> <span class=\"k\">EXISTS</span> <span class=\"n\">hive</span><span class=\"p\">.</span><span class=\"n\">iris</span><span class=\"p\">.</span><span class=\"n\">iris_parquet</span> <span class=\"p\">(</span>\n  <span class=\"n\">sepal_length</span> <span class=\"nb\">DOUBLE</span><span class=\"p\">,</span>\n  <span class=\"n\">sepal_width</span>  <span class=\"nb\">DOUBLE</span><span class=\"p\">,</span>\n  <span class=\"n\">petal_length</span> <span class=\"nb\">DOUBLE</span><span class=\"p\">,</span>\n  <span class=\"n\">petal_width</span>  <span class=\"nb\">DOUBLE</span><span class=\"p\">,</span>\n  <span class=\"k\">class</span>        <span class=\"nb\">VARCHAR</span>\n<span class=\"p\">)</span>\n<span class=\"k\">WITH</span> <span class=\"p\">(</span>\n  <span class=\"n\">external_location</span> <span class=\"o\">=</span> <span class=\"s1\">'s3a://iris/iris_parquet'</span><span class=\"p\">,</span>\n  <span class=\"n\">format</span> <span class=\"o\">=</span> <span class=\"s1\">'PARQUET'</span>\n<span class=\"p\">);</span>\n</code></pre></div></div>\n\n<p>Now, you should be able to query the data with:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">SELECT</span> \n  <span class=\"n\">sepal_length</span><span class=\"p\">,</span>\n  <span class=\"k\">class</span>\n<span class=\"k\">FROM</span> <span class=\"n\">hive</span><span class=\"p\">.</span><span class=\"n\">iris</span><span class=\"p\">.</span><span class=\"n\">iris_parquet</span> \n<span class=\"k\">LIMIT</span> <span class=\"mi\">10</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>To show all tables in a particular schema, you can type <code class=\"language-plaintext highlighter-rouge\">SHOW TABLES IN hive.iris;</code>.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>There you have it. You have seen how to setup Trino or Presto to query data stored on S3 storage like AWS S3, Linode object storage, or MinIO among others. If you are already familiar with S3, this makes it incredibly easy to query large data sets instead of dealing with preparing HDFS on local infrastructure. Trino and Presto provide separation of data and compute which makes it a one-stop-shop to query across multiple data sources with their federated queries in reasonable time and low technical overhead. For more useful information, have a look at the following resources:</p>\n\n<h2 id=\"resources\">Resources</h2>\n\n<ul>\n  <li>Github - <a href=\"https://github.com/s3tools/s3cmd\">s3tools/s3cmd</a> Command line tool for managing Amazon S3 and CloudFront services</li>\n  <li>Github - <a href=\"https://github.com/njanakiev/trino-minio-docker\">njanakiev/trino-minio-docker</a> Minimal example to run Trino, Minio, and Hive standalone metastore on docker</li>\n  <li>Github - <a href=\"https://github.com/starburstdata/presto-minio\">starburstdata/presto-minio</a> Presto and Minio on Docker Infrastructure</li>\n  <li>2020 - <a href=\"https://trino.io/blog/2020/10/20/intro-to-hive-connector.html\">A gentle introduction to the Hive connector</a></li>\n  <li>2020 - <a href=\"https://techjogging.com/memory-setup-prestodb-cluster.html\">Memory Configuration in Presto Cluster</a></li>\n  <li>2019 - <a href=\"https://blog.min.io/running-presto-on-minio-benchmarking-vs-aws-s3/\">Running Presto on MinIO: Benchmarking vs. AWS S3</a></li>\n  <li>2018 - <a href=\"https://blog.min.io/interactive-sql-query-with-presto-on-minio-cloud-storage/\">Interactive SQL query with Presto on MinIO Cloud Storage</a></li>\n  <li><a href=\"https://www.linode.com/docs/guides/how-to-use-object-storage/\">How to Use Linode Object Storage</a></li>\n</ul>\n\n<p>There were also some related podcasts in this topic by the <a href=\"https://www.dataengineeringpodcast.com\">Data Engineering Podcast</a>, which were worth a listen:</p>\n\n<ul>\n  <li>2020 - <a href=\"https://www.dataengineeringpodcast.com/linode-object-storage-service-episode-125/\">Behind The Scenes Of The Linode Object Storage Service</a></li>\n  <li>2020 - <a href=\"https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/\">Simplify Your Data Architecture With The Presto Distributed SQL Engine</a></li>\n</ul>\n\n<h2 id=\"appendix\">Appendix</h2>\n\n<p>Running the Hive metastore can be tiresome and for quick tests it can be sometimes useful to use the build-in FileHiveMetastore. Be warned, <strong>It is not advised to use it in production</strong> and there are barely any documentation about it except for these articles and discussions:</p>\n\n<ul>\n  <li>2020 - <a href=\"https://techjogging.com/access-minio-s3-storage-prestodb-cluster.html\">Access MinIO S3 Storage in Presto with File Metastore</a></li>\n  <li>2019 - <a href=\"https://blog.min.io/building-an-on-premise-ml-ecosystem-with-minio-powered-by-presto-r-and-s3select-feature/\">Building an on-premise ML ecosystem with MinIO Powered by Presto, R and S3 Select Feature</a></li>\n  <li>2018 - <a href=\"https://stackoverflow.com/questions/48932907/setup-standalone-hive-metastore-service-for-presto-and-aws-s3\">Setup Standalone Hive Metastore Service For Presto and AWS S3</a></li>\n  <li>Github - <a href=\"https://github.com/prestodb/presto/issues/11943\">FileHiveMetastore example/documentation? #11943</a></li>\n</ul>",
  "author": {
    "name": "Nikolai Janakiev"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "Querying big data on Hadoop can be challenging to get running, but alternatively, many solutions are using S3 object stores which you can access and query with Presto or Trino. In this guide you will see how to install, configure, and run Presto or Trino on Debian or Ubuntu with the S3 object store of your choice and the Hive standalone metastore.",
  "media:thumbnail": "",
  "media:content": ""
}