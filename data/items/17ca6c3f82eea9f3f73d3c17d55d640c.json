{
  "title": "Ordering Requests to Accelerate Disk I/O",
  "link": "",
  "published": "2021-04-04T00:00:00+00:00",
  "updated": "2021-04-04T00:00:00+00:00",
  "id": "https://pkolaczk.github.io/disk-access-ordering",
  "content": "<p>In <a href=\"/disk-parallelism/\">the earlier post</a> I showed how accessing data on\nan SSD in parallel can greatly improve read performance. However, that technique\nis not very effective for data stored on spinning drives. In some cases parallel access\ncan even deteriorate performance significantly. Fortunately, there exists a class of optimizations\nthat can strongly help with HDDs: request ordering. By requesting data in proper order,\nthe disk seek latency can be reduced by an order of magnitude. Since I introduced that \noptimization in <a href=\"https://github.com/pkolaczk/fclones\">fclones 0.9</a>, <code class=\"language-plaintext highlighter-rouge\">fclones</code> became the \nfastest duplicate file finder I know of.</p>\n\n<!--more-->\n\n<h1 id=\"hdd-performance-characteristics\">HDD Performance Characteristics</h1>\n\n<p>Spinning drives, contrary to SSDs, have significant access latency, limiting the effective\nnumber of IO operations per second they can serve. The disk access latency is mostly comprised of:</p>\n<ul>\n  <li>seek latency – the time needed to position the head on the desired track,</li>\n  <li>rotational latency – the time needed to wait for the disk to rotate so that the right sector is under the head,</li>\n</ul>\n\n<p>The seek latency is higher the more distance the head has to move to get to the right track. \nThe typical average seek latency advertised by manufacturers are about 5-12 ms. \nThe average rotational latency is equal to the time needed for the disk plates to do half of the turn.\nIn case of a 7200 RPM drive, this equals to 60/7200/2 = 4.2 ms. Overall the total average \nlatency can be about 10 ms, and worst-case latency more than 20 ms.</p>\n\n<p>Now if we want to process a bunch of tiny files placed randomly on the disk, and we access them in random\norder, we should not expect to process vastly more than about 100 per second \n(you might get lucky though, that some of your files would be located close to each other which may improve it a bit).</p>\n\n<p>This back-of-the envelope calculation holds pretty well in the real world. \nHere is an example <code class=\"language-plaintext highlighter-rouge\">iostat</code> output while searching for duplicates\non a 7200 RPM HDD with an old version of <code class=\"language-plaintext highlighter-rouge\">fclones</code> (0.8):</p>\n\n<pre>\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda             127,40       655,20         0,00       3276          0\nsdb               0,60         0,00        67,20          0        336\n\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda             135,00       659,20         0,00       3296          0\nsdb              26,00         0,00       174,40          0        872\n\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda             132,60       669,60         0,00       3348          0\nsdb               0,40         0,00         8,00          0         40\n\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda             127,40       683,20         0,00       3416          0\nsdb               0,40         2,40        28,00         12        140\n</pre>\n\n<p>We can see the number of transactions per second (<code class=\"language-plaintext highlighter-rouge\">tps</code>) is slightly more than 100. That’s a \nridiculously low number considering SSDs can handle tens or even hundreds thousands random accesses \nper second. The data rate is also essentially “killed” by latency. This drive can read at more than\n100 MB/s continuously, but here we get a rate in the range of hundreds of kilobytes.</p>\n\n<p>Unfortunately, even in 2021 you may still have plenty of gigabytes in small files lying \naround on older HDDs. Does it mean you’re doomed to hours of waiting if you ever want to \nprocess all of them (e.g. search, backup or deduplicate)?</p>\n\n<h1 id=\"basic-request-ordering\">Basic Request Ordering</h1>\n\n<p>In <a href=\"Lunde2009\">[1]</a> the authors presented some nice techniques of improving performance by sorting I/O requests before\nsending them to the operating system for execution. I implemented them in <code class=\"language-plaintext highlighter-rouge\">fclones</code> and the results are\ntruly amazing!</p>\n\n<p>Up to version 0.8, <code class=\"language-plaintext highlighter-rouge\">fclones</code> processed files in the order dictated by their <em>size</em>, because that was the order\nnaturally obtained from the first phase of grouping. As you may expect, it turns out, \nfile size isn’t correlated with the physical location of a file at all. Hence, the performance on HDD was actually\nworse than as if the files were processed in the order obtained from scanning the directory tree. \nAt least, when processing files in the order returned by the directory listing, there are high \nchances they were saved at the similar time (e.g. as a result of a directory copy operation) and are actually\nplaced very close to each other. And indeed,\nsome alternative programs like <code class=\"language-plaintext highlighter-rouge\">fdupes</code> or <code class=\"language-plaintext highlighter-rouge\">rdfind</code> outperformed <code class=\"language-plaintext highlighter-rouge\">fclones</code> on HDD, despite not really doing anything special\nto speed up disk access.</p>\n\n<p>One of the first ideas I tried from the paper was to reorder the files by their inode identifiers. \nThis was quite easy, because the inode identifiers were available already in the file metadata structures in order to properly detect\nhard-links. Honestly, I wasn’t expecting much improvement from this technique, as theoretically the inode number of a file\nhas nothing to do with the physical data location. \nIn practice though, there seems to be a lot of correlation. This technique alone worked like a charm, despite some minor added cost\nof sorting the entries!</p>\n\n<script type=\"text/javascript\" src=\"/assets/graphs/graphs.js\"></script>\n\n<div class=\"figure\">\n    <div style=\"height:7em\">\n        <canvas id=\"inodeOrdering\"></canvas>\n    </div>\n    <script>\n    makeBarChartDeferred(\"inodeOrdering\", \"time [s]\", \"ordering\",\n        [\"by size\", \"by inode\"],\n        {\"time\": [217, 28.43]});\n    </script>\n    <span class=\"caption\"> Fig.1: Time to find duplicates among 50k+ files stored on a 7200 RPM drive</span>\n</div>\n\n<h1 id=\"ordering-by-physical-data-location\">Ordering by Physical Data Location</h1>\n\n<p>We can do better. Some file systems, like Linux EXT4, offer an API for fetching information about file extents: <code class=\"language-plaintext highlighter-rouge\">FIEMAP ioctl</code>.\nWe can use this API to get a data structure that contains information on the physical placement of the file data. \nThen, the physical placement of the beginning of the data can be used to sort the files so that we can process\nall files in a single sweep. A great news is that this API is also available for non-root users.</p>\n\n<p>Using <code class=\"language-plaintext highlighter-rouge\">FIEMAP</code> in Rust is easy, because there is already a Rust crate for that: <a href=\"https://crates.io/crates/fiemap\"><code class=\"language-plaintext highlighter-rouge\">fiemap</code></a>. \nThe relevant fragment of <code class=\"language-plaintext highlighter-rouge\">fclones</code> code looks like this:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">#[cfg(target_os</span> <span class=\"nd\">=</span> <span class=\"s\">\"linux\"</span><span class=\"nd\">)]</span>\n<span class=\"k\">pub</span> <span class=\"k\">fn</span> <span class=\"nf\">get_physical_file_location</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"o\">&amp;</span><span class=\"n\">Path</span><span class=\"p\">)</span> <span class=\"k\">-&gt;</span> <span class=\"nn\">io</span><span class=\"p\">::</span><span class=\"n\">Result</span><span class=\"o\">&lt;</span><span class=\"nb\">Option</span><span class=\"o\">&lt;</span><span class=\"nb\">u64</span><span class=\"o\">&gt;&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">extents</span> <span class=\"o\">=</span> <span class=\"nn\">fiemap</span><span class=\"p\">::</span><span class=\"nf\">fiemap</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">path</span><span class=\"nf\">.to_path_buf</span><span class=\"p\">())</span><span class=\"o\">?</span><span class=\"p\">;</span>\n    <span class=\"k\">match</span> <span class=\"n\">extents</span><span class=\"nf\">.next</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"nf\">Some</span><span class=\"p\">(</span><span class=\"n\">fe</span><span class=\"p\">)</span> <span class=\"k\">=&gt;</span> <span class=\"nf\">Ok</span><span class=\"p\">(</span><span class=\"nf\">Some</span><span class=\"p\">(</span><span class=\"n\">fe</span><span class=\"o\">?</span><span class=\"py\">.fe_physical</span><span class=\"p\">)),</span>\n        <span class=\"nb\">None</span> <span class=\"k\">=&gt;</span> <span class=\"nf\">Ok</span><span class=\"p\">(</span><span class=\"nb\">None</span><span class=\"p\">),</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>I was initially worried that an additional system call for every file would add some initial cost, canceling the gains\nfrom the access ordering. Fortunately it turned out the cost was really low – 50k files could be queried for extents in less than\na second! I guess that the fact the metadata for the files were already queried in an earlier stage, so all the\nrequired information was already in the cache. Fig. 2 shows that despite the higher number of system calls, \nthe total time of the task decreased even more, down to about 19 seconds! This is over 10x faster than the earlier release.</p>\n\n<div class=\"figure\">\n    <div style=\"height:9em\">\n        <canvas id=\"fiemapOrdering\"></canvas>\n    </div>\n    <script>\n    makeBarChartDeferred(\"fiemapOrdering\", \"time [s]\", \"ordering\",\n        [\"by size\", \"by inode\", \"by physical location\"],\n        {\"time\": [217, 28.43, 19.45]});\n    </script>\n    <span class=\"caption\"> Fig.2: Impact of physical data ordering on time to find duplicates among 50k+ files stored on a 7200 RPM drive</span>\n</div>\n\n<p>The number of transactions per second and throughput reported by <code class=\"language-plaintext highlighter-rouge\">iostat</code> also went up considerably.\nMany files are read now in a single disk plate turn.</p>\n\n<pre>\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda            2424,40     11605,60         0,00      58028          0\nsdb               1,00         4,80        11,20         24         56\n\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda            2388,20     10436,80         0,00      52184          0\nsdb               6,60       356,80        38,40       1784        192\n\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\nsda            2397,00     11188,00         0,00      55940          0\nsdb               3,20        80,80        56,80        404        284\n</pre>\n\n<h1 id=\"impact-of-parallelism\">Impact of Parallelism</h1>\n\n<p>Before introducing the reordering, I’ve found that issuing requests in parallel for reading small (4-64 kB) chunks of data improved speed.\nThe operating system definitely made a good use of knowing some files in advance and reordered accesses by itself. \nIs it still the case after we order the reads? Maybe giving the operating system a bit more requests in advance could still save some time? \nI thought the system could technically work on fetching the next file while the app is still processing the earlier one.</p>\n\n<p>Unfortunately, at least on my system, this seems to not work as I thought. Fetching files in parallel degraded performance a bit (Fig. 3). The effect\nwasn’t as huge as for sequential access of big files, but big enough that I changed the defaults in <code class=\"language-plaintext highlighter-rouge\">fclones 0.9.1</code> to now use always a \nsingle-thread per HDD device.</p>\n\n<div class=\"figure\">\n    <div style=\"height:9em\">\n        <canvas id=\"parallelAccess\"></canvas>\n    </div>\n    <script>\n    makeBarChartDeferred(\"parallelAccess\", \"time [s]\", \"# threads\",\n        [1, 2, 8],\n        {\"time\": [19.45, 25.22, 29.11]});\n    </script>\n    <span class=\"caption\"> Fig.3: Impact of parallelism on performance of ordered disk access</span>\n</div>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>The order of file I/O requests has a tremendous impact on I/O performance on spinning drives.\nIf your application needs to process a batch of small files, make sure you request them in the \nsame order as their physical placement on disk. If you can’t do it because your file system\nor your operating system does not provide physical block placement information, at least\nsort the files by their identifiers. If you’re lucky, the identifiers would be highly correlated\nwith the physical placement of data, and such ordering would still do some magic.</p>\n\n<p>Please let me know in the comments if you tried this and how big improvements you’ve got.</p>\n\n<h1 id=\"references\">References</h1>\n<ol>\n  <li>C. Lunde, H. Espeland, H. Stensland, and P. Halvorsen, “Improving File Tree Traversal Performance by Scheduling I/O Operations in User space,” Dec. 2009, pp. 145–152, doi: 10.1109/PCCC.2009.5403829.</li>\n</ol>",
  "author": {
    "name": "Piotr Kołaczkowski"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "In the earlier post I showed how accessing data on an SSD in parallel can greatly improve read performance. However, that technique is not very effective for data stored on spinning drives. In some cases parallel access can even deteriorate performance significantly. Fortunately, there exists a class of optimizations that can strongly help with HDDs: request ordering. By requesting data in proper order, the disk seek latency can be reduced by an order of magnitude. Since I introduced that optimization in fclones 0.9, fclones became the fastest duplicate file finder I know of."
}