{
  "title": "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v2 [cs.CV] UPDATED)",
  "link": "http://arxiv.org/abs/2211.01335",
  "description": "<p>The tremendous success of CLIP (Radford et al., 2021) has promoted the\nresearch and application of contrastive learning for vision-language\npretraining. In this work, we construct a large-scale dataset of image-text\npairs in Chinese, where most data are retrieved from publicly available\ndatasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5\nChinese CLIP models of multiple sizes, spanning from 77 to 958 million\nparameters. Furthermore, we propose a two-stage pretraining method, where the\nmodel is first trained with the image encoder frozen and then trained with all\nparameters being optimized, to achieve enhanced model performance. Our\ncomprehensive experiments demonstrate that Chinese CLIP can achieve the\nstate-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups\nof zero-shot learning and finetuning, and it is able to achieve competitive\nperformance in zero-shot image classification based on the evaluation on the\nELEVATER benchmark (Li et al., 2022). We have released our codes, models, and\ndemos in https://github.com/OFA-Sys/Chinese-CLIP\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junshu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"
}