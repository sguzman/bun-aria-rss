{
  "title": "Introducing the Amazon SageMaker Serverless Inference Benchmarking Toolkit",
  "link": "https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-sagemaker-serverless-inference-benchmarking-toolkit/",
  "dc:creator": "Simon Zamarin",
  "pubDate": "Wed, 26 Oct 2022 16:42:53 +0000",
  "category": [
    "Advanced (300)",
    "Amazon SageMaker",
    "Artificial Intelligence",
    "Amazon SageMaker Serverless Inference Benchmarking Toolkit"
  ],
  "guid": "cea92f90c1e2be923ca3675069ab7f398e84cff6",
  "description": "Amazon SageMaker Serverless Inference is a purpose-built inference option that makes it easy for you to deploy and scale machine learning (ML) models. It provides a pay-per-use model, which is ideal for services where endpoint invocations are infrequent and unpredictable. Unlike a real-time hosting endpoint, which is backed by a long-running instance, compute resources for […]",
  "content:encoded": "<p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Serverless Inference</a> is a purpose-built inference option that makes it easy for you to deploy and scale machine learning (ML) models. It provides a pay-per-use model, which is ideal for services where endpoint invocations are infrequent and unpredictable. Unlike a real-time hosting endpoint, which is backed by a long-running instance, compute resources for serverless endpoints are provisioned on demand, thereby eliminating the need to choose instance types or manage scaling policies.</p> \n<p>The following high-level architecture illustrates how a serverless endpoint works. A client invokes an endpoint, which is backed by AWS managed infrastructure.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-44578\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image001-9-1024x468.png\" alt=\"\" width=\"1024\" height=\"468\"></p> \n<p>However, serverless endpoints are prone to cold starts in the order of seconds, and is therefore more suitable for intermittent or unpredictable workloads.</p> \n<p>To help determine whether a serverless endpoint is the right deployment option from a cost and performance perspective, we have developed the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker Serverless Inference Benchmarking Toolkit</a>, which tests different endpoint configurations and compares the most optimal one against a comparable real-time hosting instance.</p> \n<p>In this post, we introduce the toolkit and provide an overview of its configuration and outputs.</p> \n<h2>Solution overview</h2> \n<p>You can download the toolkit and install it from the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>. Getting started is easy: simply install the library, create a <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker model</a>, and provide the name of your model along with a JSON lines formatted file containing a sample set of invocation parameters, including the payload body and content type. A convenience function is provided to convert a list of sample invocation arguments to a JSON lines file or a pickle file for binary payloads such as images, video, or audio.</p> \n<h2>Install the toolkit</h2> \n<p>First install the benchmarking library into your Python environment using pip:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-bash\">pip install sm-serverless-benchmarking</code></pre> \n</div> \n<p>You can run the following code from an <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Studio</a> instance, <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker notebook instance</a>, or any instance with <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-creds-create\" target=\"_blank\" rel=\"noopener noreferrer\">programmatic access</a> to AWS and the appropriate <a href=\"http://aws.amazon.com/iam\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Identity and Access Management</a> (IAM) permissions. The requisite IAM permissions are documented in the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>. For additional guidance and example policies for IAM, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security_iam_service-with-iam.html\" target=\"_blank\" rel=\"noopener noreferrer\">How Amazon SageMaker Works with IAM</a>. This code runs a benchmark with a default set of parameters on a model that expects a CSV input with two example records. It’s a good practice to provide a representative set of examples to analyze how the endpoint performs with different input payloads.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from sm_serverless_benchmarking import benchmark\nfrom sm_serverless_benchmarking.utils import convert_invoke_args_to_jsonl\nmodel_name = \"&lt;SageMaker Model Name&gt;\"\nexample_invoke_args = [\n        {'Body': '1,2,3,4,5', \"ContentType\": \"text/csv\"},\n        {'Body': '6,7,8,9,10', \"ContentType\": \"text/csv\"}\n        ]\nexample_args_file = convert_invoke_args_to_jsonl(example_invoke_args,\noutput_path=\".\")\nr = benchmark.run_serverless_benchmarks(model_name, example_args_file)</code></pre> \n</div> \n<p>Additionally, you can run the benchmark as a SageMaker Processing job, which may be a more reliable option for longer-running benchmarks with a large number of invocations. See the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from sm_serverless_benchmarking.sagemaker_runner import run_as_sagemaker_job\nrun_as_sagemaker_job(\n                    role=\"&lt;execution_role_arn&gt;\",\n                    model_name=\"&lt;model_name&gt;\",\n                    invoke_args_examples_file=\"&lt;invoke_args_examples_file&gt;\",\n                    )</code></pre> \n</div> \n<p>Note that this will incur additional cost of running an ml.m5.large SageMaker Processing instance for the duration of the benchmark.</p> \n<p>Both methods accept a number of parameters to configure, such as a list of memory configurations to benchmark and the number of times each configuration will be invoked. In most cases, the default options should suffice as a starting point, but refer to the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a> for a complete list and descriptions of each parameter.</p> \n<h2>Benchmarking configuration</h2> \n<p>Before delving into what the benchmark does and what outputs it produces, it’s important to understand a few key concepts when it comes to configuring serverless endpoints.</p> \n<p>There are <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create.html\" target=\"_blank\" rel=\"noopener noreferrer\">two key configuration options</a>: <code>MemorySizeInMB</code> and <code>MaxConcurrency</code>. <code>MemorySizeInMB</code> configures the amount of memory that is allocated to the instance, and can be 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB. The number of vCPUs also scales proportionally to the amount of memory allocated. The <code>MaxConcurrency</code> parameter adjusts how many concurrent requests an endpoint is able to service. With a <code>MaxConcurrency</code> of 1, a serverless endpoint can only process a single request at a time.</p> \n<p>To summarize, the <code>MemorySizeInMB</code> parameter provides a mechanism for vertical scalability, allowing you to adjust memory and compute resources to serve larger models, whereas <code>MaxConcurrency</code> provides a mechanism for horizontal scalability, allowing your endpoint to process more concurrent requests.</p> \n<p>The cost of operating an endpoint is largely determined by the memory size, and there is no cost associated with increasing the max concurrency. However, there is a per-Region account limit for max concurrency across all endpoints. Refer to <a href=\"https://docs.aws.amazon.com/general/latest/gr/sagemaker.html\" target=\"_blank\" rel=\"noopener noreferrer\">SageMaker endpoints and quotas</a> for the latest limits.</p> \n<h2>Benchmarking outputs</h2> \n<p>Given this, the goal of benchmarking a serverless endpoint is to determine the most cost-effective and reliable memory size setting, and the minimum max concurrency that can handle your expected traffic patterns.</p> \n<p>By default, the tool runs two benchmarks. The first is a stability benchmark, which deploys an endpoint for each of the specified memory configurations and invokes each endpoint with the provided sample payloads. The goal of this benchmark is to determine the most effective and stable MemorySizeInMB setting. The benchmark captures the invocation latencies and computes the expected per-invocation cost for each endpoint. It then compares the cost against a similar real-time hosting instance.</p> \n<p>When the benchmarking is complete, the tool generates several outputs in the specified <code>result_save_path</code> directory with the following directory structure:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">├── benchmarking_report\n├── concurrency_benchmark_raw_results\n├── concurrency_benchmark_summary_results\n├── cost_analysis_summary_results\n├── stability_benchmark_raw_results\n├── stability_benchmark_summary_results</code></pre> \n</div> \n<p>The <code>benchmarking_report</code> directory contains a consolidated report with all the summary outputs that we outline in this post. Additional directories contain raw and intermediate outputs that you can use for additional analyses. Refer to the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a> for a more detailed description of each output artifact.</p> \n<p>Let’s examine a few actual benchmarking outputs for an endpoint serving a computer vision MobileNetV2 TensorFlow model. If you’d like to reproduce this example, refer to the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking/tree/main/sample_notebooks/tensorflow_mobilenet_ic\" target=\"_blank\" rel=\"noopener noreferrer\">example notebooks</a> directory in the GitHub repo.</p> \n<p>The first output within the consolidated report is a summary table that provides the minimum, mean, medium, and maximum latency metrics for each <code>MemorySizeInMB</code> successful memory size configuration. As shown in the following table, the average invocation latency (<code>invocation_latency_mean</code>) continued to improve as memory configuration was increased to 3072 MB, but stopped improving thereafter.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44579\" style=\"margin: 10px 0px 10px 0px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image003-3.jpg\" alt=\"\" width=\"1142\" height=\"201\"></p> \n<p>In addition to the high-level descriptive statistics, a chart is provided showing the distribution of latency as observed from the client for each of the memory configurations. Again, we can observe that the 1024 MB configuration isn’t as performant as the other options, but there isn’t a substantial difference in performance in configurations of 2048 and above.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44580\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image005-2.jpg\" alt=\"\" width=\"892\" height=\"552\"></p> \n<p><a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon CloudWatch</a> metrics associated with each endpoint configuration are also provided. One key metric here is <code>ModelSetupTime</code>, which measures how long it took to load the model when the endpoint was invoked in a cold state. The metric may not always appear in the report as an endpoint is launched in a warm state. A <code>cold_start_delay</code> parameter is available for specifying the number of seconds to sleep before starting the benchmark on a deployed endpoint. Setting this parameter to a higher number such as 600 seconds should increase the likelihood of a cold state invocation and improve the chances of capturing this metric. Additionally, this metric is far more likely to be captured with the concurrent invocation benchmark, which we discuss later in this section.</p> \n<p>The following table shows the metrics captured by CloudWatch for each memory configuration.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44581\" style=\"margin: 10px 0px 10px 0px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image007-8.png\" alt=\"\" width=\"786\" height=\"208\"></p> \n<p>The next chart shows the performance and cost trade-offs of different memory configurations. One line shows the estimated cost of invoking the endpoint 1 million times, and the other shows the average response latency. These metrics can inform your decision of which endpoint configuration is most cost-effective. In this example, we see that the average latency flattens out after 2048 MB, whereas the cost continues to increase, indicating that for this model a memory size configuration of 2048 would be most optimal.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44582\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image009-3.jpg\" alt=\"\" width=\"932\" height=\"608\"></p> \n<p>The final output of the cost and stability benchmark is a recommended memory configuration, along with a table comparing the cost of operating a serverless endpoint against a comparable SageMaker hosting instance. Based on the data collected, the tool determined that the 2048 MB configuration is the most optimal one for this model. Although the 3072 configuration provides roughly 10 milliseconds better latency, that comes with a 30% increase in cost, from $4.55 to $5.95 per 1 million requests. Additionally, the output shows that a serverless endpoint would provide savings of up to 88.72% against a comparable real-time hosting instance when there are fewer than 1 million monthly invocation requests, and breaks even with a real-time endpoint after 8.5 million requests.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44583\" style=\"margin: 10px 0px 10px 0px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image011-2.jpg\" alt=\"\" width=\"711\" height=\"515\"></p> \n<p>The second type of benchmark is optional and tests various <code>MaxConcurency</code> settings under different traffic patterns. This benchmark is usually run using the optimal <code>MemorySizeInMB</code> configuration from the stability benchmark. The two key parameters for this benchmark is a list of <code>MaxConcurency</code> settings to test along with a list of client multipliers, which determine the number of simulated concurrent clients that the endpoint is tested with.</p> \n<p>For example, by setting the <code>concurrency_benchmark_max_conc parameter</code> to [4, 8] and <code>concurrency_num_clients_multiplier</code> to [1, 1.5, 2], two endpoints are launched: one with <code>MaxConcurency</code> of 4 and the other 8. Each endpoint is then benchmarked with a (<code>MaxConcurency</code> x multiplier) number of simulated concurrent clients, which for the endpoint with a concurrency of 4 translates to load test benchmarks with 4, 6, and 8 concurrent clients.</p> \n<p>The first output of this benchmark is a table that shows the latency metrics, throttling exceptions, and transactions per second metrics (TPS) associated with each <code>MaxConcurrency</code> configuration with different numbers of concurrent clients. These metrics help determine the appropriate <code>MaxConcurrency</code> setting to handle the expected traffic load. In the following table, we can see that an endpoint configured with a max concurrency of 8 was able to handle up to 16 concurrent clients with only two throttling exceptions out of 2,500 invocations made at an average of 24 transactions per second.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-large wp-image-44591\" style=\"margin: 10px 0px 10px 0px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image_14-1024x237.png\" alt=\"\" width=\"1024\" height=\"237\"></p> \n<p>The next set of outputs provides a chart for each <code>MaxConcurrency</code> setting showing the distribution of latency under different loads. In this example, we can see that an endpoint with a <code>MaxConcurrency</code> setting of 4 was able to successfully process all requests with up to 8 concurrent clients with a minimal increase in invocation latency.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44816\" style=\"margin: 10px 0px 10px 0px;border: 1px solid #CCCCCC\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/image015-3.jpg\" alt=\"\" width=\"884\" height=\"482\"></p> \n<p>The final output provides a table with CloudWatch metrics for each <code>MaxConcurrency</code> configuration. Unlike the previous table showing the distribution of latency for each memory configuration, which may not always display the cold start <code>ModelSetupTime</code> metric, this metric is far more likely to appear in this table due to the larger number of invocation requests and a greater <code>MaxConcurrency</code>.</p> \n<p><img loading=\"lazy\" class=\"alignnone size-full wp-image-44817\" style=\"margin: 10px 0px 10px 0px\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/image017-2.jpg\" alt=\"\" width=\"725\" height=\"129\"></p> \n<h2>Conclusion</h2> \n<p>In this post, we introduced the SageMaker Serverless Inference Benchmarking Toolkit and provided an overview of its configuration and outputs. The tool can help you make a more informed decision with regards to serverless inference by load testing different configurations with realistic traffic patterns. Try the benchmarking toolkit with your own models to see for yourself the performance and cost saving you can expect by deploying a serverless endpoint. Please refer to the <a href=\"https://github.com/aws-samples/sagemaker-serverless-inference-benchmarking\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a> for additional documentation and example notebooks.</p> \n<h2>Additional resources</h2> \n<ul> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-create-invoke-update-delete.html\" target=\"_blank\" rel=\"noopener noreferrer\">Create, Invoke, Update, and Delete an Endpoint</a></li> \n <li><a href=\"https://github.com/aws/amazon-sagemaker-examples/tree/main/serverless-inference\" target=\"_blank\" rel=\"noopener noreferrer\">Serverless Inference Examples repo</a></li> \n</ul> \n<hr> \n<h3>About the authors</h3> \n<p style=\"clear: both\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/simon-zamarin.jpg\"><img loading=\"lazy\" class=\"wp-image-44796 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/24/simon-zamarin.jpg\" alt=\"\" width=\"100\" height=\"133\"></a><strong>Simon Zamarin</strong> is an AI/ML Solutions Architect whose main focus is helping customers extract value from their data assets. In his spare time, Simon enjoys spending time with family, reading sci-fi, and working on various DIY house projects.</p> \n<p style=\"clear: both\"><img loading=\"lazy\" class=\"wp-image-44588 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image021.png\" alt=\"\" width=\"100\" height=\"97\"><strong>Dhawal Patel</strong> is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and artificial intelligence. He focuses on deep learning, including NLP and computer vision domains. He helps customers achieve high-performance model inference on SageMaker.</p> \n<p style=\"clear: both\"><img loading=\"lazy\" class=\"wp-image-44589 size-full alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image022-3.png\" alt=\"\" width=\"100\" height=\"133\"><strong>Rishabh Ray Chaudhury</strong> is a Senior Product Manager with Amazon SageMaker, focusing on machine learning inference. He is passionate about innovating and building new experiences for machine learning customers on AWS to help scale their workloads. In his spare time, he enjoys traveling and cooking. You can find him on <a href=\"https://www.linkedin.com/in/rishabh-ray-chaudhury-58528030/\" target=\"_blank\" rel=\"noopener noreferrer\">LinkedIn</a>.</p>"
}