{
  "title": "Deriving Expectation-Maximization",
  "link": "",
  "published": "2018-11-11T16:00:00-05:00",
  "updated": "2018-11-11T16:00:00-05:00",
  "author": {
    "name": "Will Wolf"
  },
  "id": "tag:willwolf.io,2018-11-11:/2018/11/11/em-for-lda/",
  "summary": "<p>Deriving the expectation-maximization algorithm, and the beginnings of its application to LDA. Once finished, its intimate connection to variational inference is apparent.</p>",
  "content": "<p>Consider a model with parameters <span class=\"math\">\\(\\theta\\)</span> and latent variables <span class=\"math\">\\(\\mathbf{Z}\\)</span>; the expectation-maximization algorithm (EM) is a mechanism for computing the values of <span class=\"math\">\\(\\theta\\)</span> that, under this model, maximize the likelihood of some observed data <span class=\"math\">\\(\\mathbf{X}\\)</span>.</p>\n<p>The joint probability of our model can be written as follows:</p>\n<div class=\"math\">$$\np(\\mathbf{X}, \\mathbf{Z}\\vert \\theta) = p(\\mathbf{X}\\vert \\mathbf{Z}, \\theta)p(\\mathbf{Z}\\vert \\theta)\n$$</div>\n<p>where, once more, our stated goal is to maximize the marginal likelihood of our data:</p>\n<div class=\"math\">$$\n\\log{p(\\mathbf{X}\\vert\\theta)} = \\log{\\sum_{\\mathbf{Z}}p(\\mathbf{X, Z}\\vert\\theta)}\n$$</div>\n<p>An example of a latent variable model is the Latent Dirichlet Allocation<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup> (LDA) model for uncovering latent topics in documents of text. Once finished deriving the general EM equations, we'll (begin to) apply them to this model.</p>\n<h2>Why not maximum likelihood estimation?</h2>\n<p>As the adage goes, computing the MLE with respect to this marginal is \"hard.\" For one, it requires summing over an (implicitly) humongous number of configurations of latent variables <span class=\"math\">\\(z\\)</span>. Further, as Bishop<sup id=\"fnref:2\"><a class=\"footnote-ref\" href=\"#fn:2\">2</a></sup> states:</p>\n<blockquote>\n<p>A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution <span class=\"math\">\\(p(\\mathbf{X, Z}\\vert\\theta)\\)</span> belongs to the exponential family, the marginal distribution <span class=\"math\">\\(p(\\mathbf{X}\\vert\\theta)\\)</span> typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.</p>\n</blockquote>\n<p><strong>We'll want something else to maximize instead.</strong></p>\n<h2>A lower bound</h2>\n<p>Instead of maximizing the log-marginal <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> (with respect to model parameters <span class=\"math\">\\(\\theta\\)</span>), let's maximize a lower-bound with a less-problematic form.</p>\n<p>Perhaps, we'd work with <span class=\"math\">\\(\\log{p(\\mathbf{X}, \\mathbf{Z}\\vert \\theta)}\\)</span> which, almost tautologically, removes the summation over latent variables <span class=\"math\">\\(\\mathbf{Z}\\)</span>.</p>\n<p>As such, let's derive a lower-bound which features this term. As <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> is often called the log-\"evidence,\" we'll call our expression the \"evidence lower-bound,\" or ELBO.</p>\n<h2>Jensen's inequality</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen's inequality</a><sup id=\"fnref:3\"><a class=\"footnote-ref\" href=\"#fn:3\">3</a></sup> generalizes the statement that the line secant to a <strong>concave function</strong> lies below this function. An example is illustrative:</p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://alliance.seas.upenn.edu/~cis520/dynamic/2017/wiki/uploads/Lectures/jensen.png\"/></p>\n<p>First, we note that the red line is below the blue for all points for which it is defined.</p>\n<p>Second, working through the example, and assuming:</p>\n<ul>\n<li><span class=\"math\">\\(f(x) = \\exp(-(x - 2)^2)\\)</span></li>\n<li><span class=\"math\">\\(v_1 = 1; v_2 = 2.5; \\alpha = .3\\)</span></li>\n</ul>\n<div class=\"math\">$$\n\\begin{align*}\nf(v_1) &amp;\\approx .3679\\\\\nf(v_2) &amp;\\approx .7788\\\\\n\\alpha f(v_1) + (1 - \\alpha)f(v_2) &amp;\\approx \\bf{.6555}\\\\\n\\end{align*}\n$$</div>\n<div class=\"math\">$$\n\\begin{align*}\n\\alpha v_1 + (1 - \\alpha)v_2 &amp;= 2.05\\\\\nf(\\alpha v_1 + (1 - \\alpha)v_2) &amp;\\approx \\bf{.9975}\\\\\n\\end{align*}\n$$</div>\n<p>we see that <strong><span class=\"math\">\\(\\alpha f(v_1) + (1 - \\alpha)f(v_2) \\leq f(\\alpha v_1 + (1 - \\alpha)v_2)\\)</span></strong>.</p>\n<p>Finally, we arrive at a general form:</p>\n<div class=\"math\">$$\n\\mathop{\\mathbb{E}_{v}}[f(v)] \\leq f(\\mathop{\\mathbb{E}_{v}}[v])\n$$</div>\n<p>where <span class=\"math\">\\(p(v) = \\alpha\\)</span>.</p>\n<h2>Deriving the ELBO</h2>\n<p>In trying to align <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\n= \\log{\\sum\\limits_{\\mathbf{Z}}p(\\mathbf{X, Z}\\vert\\theta)}\\)</span> with <span class=\"math\">\\(f(\\mathop{\\mathbb{E}_{v}}[v])\\)</span>, we see a function <span class=\"math\">\\(f = \\log\\)</span> yet no expectation inside. However, given the summation over <span class=\"math\">\\(\\mathbf{Z}\\)</span>, introducing some distribution <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> would give us the expectation we desire.</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\log{p(\\mathbf{X}\\vert\\theta)}\n&amp;= \\log{\\sum_{\\mathbf{Z}}p(\\mathbf{X, Z}\\vert\\theta)}\\\\\n&amp;= \\log{\\sum_{\\mathbf{Z}}q(\\mathbf{Z})\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\\\\n&amp;= \\log{\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg]\\\n\\end{align*}\n$$</div>\n<p>where <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> is some distribution over <span class=\"math\">\\(\\mathbf{Z}\\)</span> with parameters <span class=\"math\">\\(\\lambda\\)</span> (omitted for cleanliness) and known form (e.g. a Gaussian). It is often referred to as a <strong>variational distribution</strong>.</p>\n<p>From here, via Jensen's inequality, we can derive the lower-bound:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\log{p(\\mathbf{X}\\vert\\theta)} = \\log{\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}\\bigg]}\n&amp;\\geq \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg]\\\\\n&amp;= \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg] + R\n\\end{align*}\n$$</div>\n<p><em>Et voilà</em>, we see that this term contains <span class=\"math\">\\(\\log{p(\\mathbf{X, Z}\\vert\\theta)}\\)</span>; the ELBO should now be easier to optimize with respect to our parameters <span class=\"math\">\\(\\theta\\)</span>.</p>\n<h1>So, what's <span class=\"math\">\\(R\\)</span>?</h1>\n<div class=\"math\">$$\n\\begin{align*}\nR\n&amp;= \\log{p(\\mathbf{X}\\vert\\theta)} -  \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg]\\\\\n&amp;= \\log{p(\\mathbf{X}\\vert\\theta)} -  \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{X, Z}\\vert\\theta) - \\log{q(\\mathbf{Z})}}\\bigg]\\\\\n&amp;= \\log{p(\\mathbf{X}\\vert\\theta)} -  \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)} + \\log{p(\\mathbf{X}\\vert\\theta)} - \\log{q(\\mathbf{Z})}\\bigg]\\\\\n&amp;= \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{X}\\vert\\theta)}\\bigg] -  \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)} + \\log{p(\\mathbf{X}\\vert\\theta)} - \\log{q(\\mathbf{Z})}\\bigg]\\\\\n&amp;= \\sum_{\\mathbf{Z}}q(\\mathbf{Z})\\bigg(\\log{p(\\mathbf{X}\\vert\\theta)} - \\log{p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)} - \\log{p(\\mathbf{X}\\vert\\theta)} + \\log{q(\\mathbf{Z})}\\bigg)\\\\\n&amp;= \\sum_{\\mathbf{Z}}q(\\mathbf{Z})\\bigg( - \\log{p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)} + \\log{q(\\mathbf{Z})}\\bigg)\\\\\n&amp;=\n\\sum_{\\mathbf{Z}}q(\\mathbf{Z})\\log{\\frac{q(\\mathbf{Z})}{p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)}}\\\\\n&amp;= \\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\\\\n\\end{align*}\n$$</div>\n<p><strong>Putting it back together:</strong></p>\n<div class=\"math\">$$\n\\log{p(\\mathbf{X}\\vert\\theta)} = \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg] + \\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\n$$</div>\n<h2>The EM algorithm</h2>\n<p>The algorithm can be described by a few simple observations.</p>\n<ol>\n<li>\n<p><span class=\"math\">\\(\\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\)</span> is a divergence metric which is strictly non-negative.</p>\n</li>\n<li>\n<p>As <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> does not depend on <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>—if we decrease <span class=\"math\">\\(\\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\)</span> by changing <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>, the ELBO must increase to compensate.</p>\n</li>\n<li>\n<p>If we increase the ELBO by changing <span class=\"math\">\\(\\theta\\)</span>, <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> will increase as well. In addition, as <span class=\"math\">\\(p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\)</span> now (likely) diverges from <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> in non-zero amount, <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> will increase even more.</p>\n</li>\n</ol>\n<p><strong>The EM algorithm is a repeated alternation between Step 2 (E-step) and Step 3 (M-step).</strong> After each M-Step, <span class=\"math\">\\(\\log{p(\\mathbf{X}\\vert\\theta)}\\)</span> is guaranteed to increase (unless it is already at a maximum)<sup id=\"fnref2:2\"><a class=\"footnote-ref\" href=\"#fn:2\">2</a></sup>.</p>\n<p>A graphic<sup id=\"fnref3:2\"><a class=\"footnote-ref\" href=\"#fn:2\">2</a></sup> is further illustrative.</p>\n<h3>Initial decomposition</h3>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/em-for-lda/initial_decomp.png\"/></p>\n<p>Here, the ELBO is written as <span class=\"math\">\\(\\mathcal{L}(q, \\theta)\\)</span>.</p>\n<h3>E-step</h3>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/em-for-lda/e_step.png\"/></p>\n<p>In other words, holding the parameters <span class=\"math\">\\(\\theta\\)</span> constant, minimize <span class=\"math\">\\(\\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\)</span> with respect to <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>. Remember, as <span class=\"math\">\\(q\\)</span> is a distribution with a fixed functional form, this amounts to updating its parameters <span class=\"math\">\\(\\lambda\\)</span>.</p>\n<p>The caption implies that we can always compute <span class=\"math\">\\(q(\\mathbf{Z}) = p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\)</span>. We will show below that this is not the case for LDA, nor for many interesting models.</p>\n<h3>M-step</h3>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/em-for-lda/m_step.png\"/></p>\n<p>In other words, in the M-step, maximize the ELBO with respect to the model parameters <span class=\"math\">\\(\\theta\\)</span>.</p>\n<p>Expanding the ELBO:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg]\n&amp;= \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{X, Z}\\vert\\theta)}\\bigg] - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{q(\\mathbf{Z})}\\bigg]\\\\\n&amp;= \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{p(\\mathbf{X, Z}\\vert\\theta)}\\bigg] + \\mathbf{H}[q(\\mathbf{Z})]\n\\end{align*}\n$$</div>\n<p>we see that it decomposes into an expectation of the joint distribution over data and latent variables given parameters <span class=\"math\">\\(\\theta\\)</span> with respect to the variational distribution <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>, plus the entropy of <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>.</p>\n<p>As our task is to maximize this expression with respect to <span class=\"math\">\\(\\theta\\)</span>, we can treat the latter term as a constant.</p>\n<h2>EM for LDA</h2>\n<p>To give an example of the above, we'll examine the classic Latent Dirichlet Allocation<sup id=\"fnref2:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup> paper.</p>\n<h3>Model</h3>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/em-for-lda/lda_formulation.png\"/></p>\n<p>\"Given the parameters <span class=\"math\">\\(\\alpha\\)</span> and <span class=\"math\">\\(\\beta\\)</span>, the joint distribution of a topic mixture <span class=\"math\">\\(\\theta\\)</span>, a set of <span class=\"math\">\\(N\\)</span> topics <span class=\"math\">\\(\\mathbf{z}\\)</span>, and a set of <span class=\"math\">\\(N\\)</span> words <span class=\"math\">\\(\\mathbf{w}\\)</span> is given by:\"<sup id=\"fnref3:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup></p>\n<div class=\"math\">$$\np(\\theta, \\mathbf{z}, \\mathbf{w}\\vert \\alpha, \\beta) = p(\\theta\\vert \\alpha)\\prod\\limits_{n=1}^{N}p(z_n\\vert \\theta)p(w_n\\vert z_n, \\beta)\n$$</div>\n<h3>Log-evidence</h3>\n<p>The (problematic) log-evidence of a single document:</p>\n<div class=\"math\">$$\n\\log{p(\\mathbf{w}\\vert \\alpha, \\beta)} = \\log{\\int p(\\theta\\vert \\alpha)\\prod\\limits_{n=1}^{N}\\sum\\limits_{z_n} p(z_n\\vert \\theta)p(w_n\\vert z_n, \\beta)d\\theta}\n$$</div>\n<p>NB: The parameters of our model are <span class=\"math\">\\(\\{\\alpha,  \\beta\\}\\)</span> and <span class=\"math\">\\(\\{\\theta, \\mathbf{z}\\}\\)</span> are our latent variables.</p>\n<h3>ELBO</h3>\n<div class=\"math\">$$\n\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\bigg(\\frac{p(\\theta\\vert \\alpha)}{q(\\mathbf{Z})}}\\prod\\limits_{n=1}^{N}p(z_n\\vert \\theta)p(w_n\\vert z_n, \\beta)\\bigg)\\bigg]\n$$</div>\n<p>where <span class=\"math\">\\(\\mathbf{Z} = \\{\\theta, \\mathbf{z}\\}\\)</span>.</p>\n<h3>KL term</h3>\n<div class=\"math\">$$\n\\text{KL}\\big(q(\\mathbf{Z})\\Vert \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w}\\vert \\alpha, \\beta)}{p(\\mathbf{w}\\vert \\alpha, \\beta)}\\big)\n$$</div>\n<p>Peering at the denominator, we see that it includes an integration over all values <span class=\"math\">\\(\\theta\\)</span>, which we assume is intractable to compute. As such, the \"ideal\" E-step solution <span class=\"math\">\\(q(\\mathbf{Z}) = p(\\theta, \\mathbf{z}\\vert \\mathbf{w}, \\alpha, \\beta)\\)</span> will elude us as well.</p>\n<p>In the next post, we'll cover how to minimize this KL term with respect to <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> in detail. This effort will begin with the derivation of the mean-field algorithm.</p>\n<h2>Summary</h2>\n<p>In this post, we motivated the expectation-maximization algorithm then derived its general form. We then applied this framework to the LDA model.</p>\n<p>In the next post, we'll expand this logic into mean-field variational Bayes, and eventually, variational inference more broadly.</p>\n<p>Thanks for reading.</p>\n<h2>References</h2>\n<div class=\"footnote\">\n<hr/>\n<ol>\n<li id=\"fn:1\">\n<p>D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003. <a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">↩</a><a class=\"footnote-backref\" href=\"#fnref2:1\" title=\"Jump back to footnote 1 in the text\">↩</a><a class=\"footnote-backref\" href=\"#fnref3:1\" title=\"Jump back to footnote 1 in the text\">↩</a></p>\n</li>\n<li id=\"fn:2\">\n<p>C. M. Bishop. Pattern recognition and machine learning,\npage 229. Springer-Verlag New York, 2006. <a class=\"footnote-backref\" href=\"#fnref:2\" title=\"Jump back to footnote 2 in the text\">↩</a><a class=\"footnote-backref\" href=\"#fnref2:2\" title=\"Jump back to footnote 2 in the text\">↩</a><a class=\"footnote-backref\" href=\"#fnref3:2\" title=\"Jump back to footnote 2 in the text\">↩</a></p>\n</li>\n<li id=\"fn:3\">\n<p>Wikipedia contributors. \"Jensen's inequality.\" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 29 Oct. 2018. Web. 11 Nov. 2018. <a class=\"footnote-backref\" href=\"#fnref:3\" title=\"Jump back to footnote 3 in the text\">↩</a></p>\n</li>\n</ol>\n</div>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}