{
  "title": "A simpler derivation of f-GANs",
  "link": "http://artem.sobolev.name/posts/2019-12-01-a-simpler-derivation-of-f-gans.html",
  "description": "<p>I have been looking at <span class=\"math inline\">\\(f\\)</span>-GANs derivation doing some of my research, and found an easier way to derive its lower bound, without invoking convex conjugate functions.</p>\n<!--more-->\n<p><a href=\"https://arxiv.org/abs/1606.00709\"><span class=\"math inline\">\\(f\\)</span>-GANs</a> are a generalization of standard GANs to arbitrary <span class=\"math inline\">\\(f\\)</span>-divergence. Given a convex function <span class=\"math inline\">\\(f\\)</span>, <a href=\"https://en.wikipedia.org/wiki/F-divergence\"><span class=\"math inline\">\\(f\\)</span>-divergence</a>, in turn, can be used to measure “difference” between the data distribution <span class=\"math inline\">\\(p_\\text{data}(x)\\)</span> and our model <span class=\"math inline\">\\(q(x)\\)</span>:</p>\n<p><span class=\"math display\">\\[\nD_f(p_\\text{data}(x) \\mid\\mid q(x)) = \\E_{q(x)} f \\left( \\frac{p_\\text{data}(x)}{q(x)} \\right)\n\\]</span></p>\n<p>Of course, we don’t know the data-generating distribution <span class=\"math inline\">\\(p_\\text{data}(x)\\)</span>. Moreover, in a typical GAN setting <span class=\"math inline\">\\(q(x)\\)</span> is an implicit model, and thus its density is not known either <a href=\"#fn1\" class=\"footnoteRef\" id=\"fnref1\"><sup>1</sup></a>. Thus, to make things tractable GANs employ tractable sample-based lower bounds <a href=\"#fn2\" class=\"footnoteRef\" id=\"fnref2\"><sup>2</sup></a>.</p>\n<h2 id=\"simple-derivation\">Simple Derivation</h2>\n<p>Our derivation is based on the following simple inequality, a very well-known fact for convex functions<a href=\"#fn3\" class=\"footnoteRef\" id=\"fnref3\"><sup>3</sup></a>, namely that a convex function is always greater than its tangent or is equal to at the point of tangency (denoted <span class=\"math inline\">\\(r(x)\\)</span>):</p>\n<p><span class=\"math display\">\\[\nf\\left( \\frac{p_\\text{data}(x)}{q(x)} \\right)\n\\ge\nf\\left( r(x) \\right)\n+\nf'\\left( r(x) \\right) \\left( \\frac{p_\\text{data}(x)}{q(x)} - r(x) \\right)\n\\]</span></p>\n<p>For any non-negative function <span class=\"math inline\">\\(r(x)\\)</span>. Now we take the expected value</p>\n<p><span class=\"math display\">\\[\n\\begin{align*}\nD_f(p_\\text{data}(x) \\mid\\mid q(x))\n&\\ge\n\\E_{q(x)}\n\\left[\nf\\left( r(x) \\right)\n+\nf'\\left( r(x) \\right) \\left( \\frac{p_\\text{data}(x)}{q(x)} - r(x) \\right)\n\\right] \\\\\n& = \n\\E_{q(x)}\nf\\left( r(x) \\right)\n+\n\\E_{p_\\text{data}(x)} f'\\left( r(x) \\right) - \\E_{q(x)} f'\\left( r(x) \\right) r(x)\n\\tag{1}\n\\end{align*}\n\\]</span></p>\n<p>This bound has several nice properties:</p>\n<ol style=\"list-style-type: decimal\">\n<li>It does not require knowing densities, only having samples.</li>\n<li>By construction, it’s a lower bound for all <span class=\"math inline\">\\(r(x)\\)</span>.</li>\n<li>Plugging <span class=\"math inline\">\\(r^*(x) = \\frac{p_\\text{data}(x)}{q(x)}\\)</span> recovers the <span class=\"math inline\">\\(f\\)</span>-divergence.</li>\n</ol>\n<p>However, this formula looks different from the one in the <span class=\"math inline\">\\(f\\)</span>-GANs paper. Are they related? We’ll now show they’re exactly the same.</p>\n<h2 id=\"f-gans-derivation\"><span class=\"math inline\">\\(f\\)</span>-GANs Derivation</h2>\n<p>The original derivation, which probably should be attributed to <a href=\"http://dept.stat.lsa.umich.edu/~xuanlong/Papers/Nguyen-Wainwright-Jordan-10.pdf\">“Estimating divergence functionals and the likelihood ratio by convex risk minimization” by XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan (2010)</a> is based on the <a href=\"https://en.wikipedia.org/wiki/Convex_conjugate\">convex conjugate</a> concept. The convex conjugate <span class=\"math inline\">\\(f^*\\)</span> for a function <span class=\"math inline\">\\(f\\)</span> is <span class=\"math display\">\\[\nf^*(t) = \\sup_{u \\in \\text{dom}(f)} \\left[ u t - f(u) \\right]\n\\]</span></p>\n<p>Nguen et al. have shown the following variational characterization of the <span class=\"math inline\">\\(f\\)</span>-divergence <a href=\"#fn4\" class=\"footnoteRef\" id=\"fnref4\"><sup>4</sup></a>: <span class=\"math display\">\\[\nD_f(p(x) \\mid\\mid q(x)) = \\sup_{T(x)} \\left[ \\E_{p(x)} T(x) - \\E_{q(x)} f^*(T(x)) \\right]\n\\]</span> Where <span class=\"math inline\">\\(f^*(t)\\)</span> is the aforementioned convex conjugate for <span class=\"math inline\">\\(f(t)\\)</span>, and the supremum is taken over all functions. However, we’re safe to restrict the range of <span class=\"math inline\">\\(T(x)\\)</span> to those values where <span class=\"math inline\">\\(f^*\\)</span> is finite, that is, the set <span class=\"math inline\">\\(\\mathcal{V} = \\{t \\in \\mathbb{R} \\mid f^*(t) < \\infty \\}\\)</span>. Now this form is already amendable to practical applications, just make <span class=\"math inline\">\\(T(x)\\)</span> a neural network whose activation respects <span class=\"math inline\">\\(\\mathcal{V}\\)</span> and maximize the lower bound w.r.t. its parameters. The question then is how to construct this activation.</p>\n<p>Skipping the more detailed analysis, we note that the optimal <span class=\"math inline\">\\(T(x)\\)</span> is known to be <span class=\"math display\">\\[T^*(x) = f'\\left( \\frac{p(x)}{q(x)} \\right)\\]</span> Since we’re only interested in approximating the optimal value, we might as well consider the following parametrization for <span class=\"math inline\">\\(T(x)\\)</span> (using a non-negative function <span class=\"math inline\">\\(r(x)\\)</span>): <span class=\"math display\">\\[\nT(x) = f'(r(x))\n\\]</span> Which gives us the following objective</p>\n<p><span class=\"math display\">\\[\nD_f(p(x) \\mid\\mid q(x)) = \\sup_{r(x)} \\left[ \\E_{p(x)} f'(r(x)) - \\E_{q(x)} f^*(f'(r(x))) \\right]\n\\tag{2}\n\\]</span></p>\n<p>Finally, we use <a href=\"https://math.stackexchange.com/a/1428011/463191\">an important property of convex conjugate functions</a>: <span class=\"math display\">\\[\n\\begin{align*}\nf^*(f'(r(x)))\n&= \\sup_u \\left[ u f'(r(x)) - f(u) \\right] \\\\\n&= \\sup_u \\left[ u f'(r(x)) - r(x) f'(r(x)) - f(u) \\right] + r(x) f'(r(x)) \\\\\n&= \\sup_u \\left[ \\underbrace{f(r(x)) + f'(r(x)) (u  - r(x)) - f(u)}_\\text{$\\le 0$ due to convexity of $f$}  \\right] + r(x) f'(r(x)) - f(r(x)) \\\\\n&= r(x) f'(r(x)) - f(r(x)) \\\\\n\\end{align*}\n\\]</span> Where in the last line we’ve used the fact that for a convex <span class=\"math inline\">\\(f(t)\\)</span> its tangent at any point is always a lower bound, and the surpremum of 0 is achieved for <span class=\"math inline\">\\(u = r(x)\\)</span>.</p>\n<p>Now we plug this equivalent formula into the objective and obtain <span class=\"math display\">\\[\n\\begin{align*}\nD_f(p(x) \\mid\\mid q(x))\n& = \\sup_{r(x)} \\left[ \\mathbb{E}_{p(x)} f'(r(x)) - \\mathbb{E}_{q(x)} \\left( r(x) f'(r(x)) - f(r(x)) \\right) \\right] \\\\\n& = \\sup_{r(x)} \\left[ \\mathbb{E}_{q(x)} f(r(x)) + \\mathbb{E}_{p(x)} f'(r(x)) - \\mathbb{E}_{q(x)} r(x) f'(r(x)) \\right]\n\\end{align*}\n\\]</span></p>\n<p>Which <strong>exactly</strong> recovers the formula (1). Moreover, the conjugate identity holds for all realizations of random variables involved, so not only the bounds (1) and (2) are the same, but their stochastic estimates are too<a href=\"#fn5\" class=\"footnoteRef\" id=\"fnref5\"><sup>5</sup></a>.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The presented derivation and objective form is interesting for several reasons. First, by design the optimal “discriminator” <span class=\"math inline\">\\(r^*(x) = \\frac{p_\\text{data}(x)}{q(x)}\\)</span> is independent of the particular <span class=\"math inline\">\\(f\\)</span>-divergence used. Second, thinking of <span class=\"math inline\">\\(r(x)\\)</span> as of importance weights approximation gives an intuitive understanding of different terms in the objective (1): the first term is <span class=\"math inline\">\\(f\\)</span>-divergence approximation that uses learned density ratio <span class=\"math inline\">\\(r(x)\\)</span> instead of the actual density ratio. The rest two terms balance the first one to ensure the lower bound guarantee. In particular, the last term uses <span class=\"math inline\">\\(r(x)\\)</span> as an importance weight to “approximate” the second one so that they cancel out when the <span class=\"math inline\">\\(r(x)\\)</span> is optimal. The last, but not least, the presented derivation is <em>simpler</em>.</p>\n<div class=\"footnotes\">\n<hr />\n<ol>\n<li id=\"fn1\"><p>Actually, most of the time it does not exist at all. But that’s a story for another time.<a href=\"#fnref1\">↩</a></p></li>\n<li id=\"fn2\"><p>Although, a lower bound on the loss is not something you’d like to minimize, this is how things are done in the GAN realm.<a href=\"#fnref2\">↩</a></p></li>\n<li id=\"fn3\"><p>We assume <span class=\"math inline\">\\(f\\)</span> is differentiable here, but if it’s not, the statement still holds with <span class=\"math inline\">\\(f'\\)</span> being replaced with a subgradient.<a href=\"#fnref3\">↩</a></p></li>\n<li id=\"fn4\"><p>Nguen et al. use a bit different convention for <span class=\"math inline\">\\(f\\)</span>-divergences, namely <span class=\"math display\">\\[D_f(p(x) \\mid\\mid q(x)) = \\E_{p(x)} f\\left(\\frac{q(x)}{p(x)}\\right)\\]</span><a href=\"#fnref4\">↩</a></p></li>\n<li id=\"fn5\"><p>As long as you use the same samples to estimate different expectations over the distribution <span class=\"math inline\">\\(q(x)\\)</span>.<a href=\"#fnref5\">↩</a></p></li>\n</ol>\n</div>",
  "pubDate": "Sun, 01 Dec 2019 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2019-12-01-a-simpler-derivation-of-f-gans.html",
  "dc:creator": "Artem"
}