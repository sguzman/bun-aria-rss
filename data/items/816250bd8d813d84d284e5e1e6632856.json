{
  "title": "Hadoop Streaming with Amazon Elastic MapReduce, Python and mrjob",
  "description": "<p>In a previous rant about <a title=\"Data Science &amp; Innovation\" href=\"http://randyzwitch.com/data-science-innovation/\" target=\"_blank\">data science &amp; innovation</a>, I made reference to a problem I‚Äôm having at work where I wanted to classify roughly a quarter-billion URLs by predicted website content (without having to actually visit the website). A few colleagues have asked how you go about even starting to solve a problem like that, and the answer is <em>massively parallel processing</em>.</p>",
  "pubDate": "Wed, 31 Jul 2013 12:34:58 +0000",
  "link": "http://randyzwitch.com/amazon-elastic-map-reduce-mrjob-python/",
  "guid": "http://randyzwitch.com/amazon-elastic-map-reduce-mrjob-python/",
  "content": "<p>In a previous rant about <a title=\"Data Science &amp; Innovation\" href=\"http://randyzwitch.com/data-science-innovation/\" target=\"_blank\">data science &amp; innovation</a>, I made reference to a problem I‚Äôm having at work where I wanted to classify roughly a quarter-billion URLs by predicted website content (without having to actually visit the website). A few colleagues have asked how you go about even starting to solve a problem like that, and the answer is <em>massively parallel processing</em>.</p>\n\n<h2 id=\"attacking-the-problem-using-a-local-machine\">Attacking the problem using a local machine</h2>\n\n<p>In order to classify the URLs, the first thing that‚Äôs needed is a customized dictionary of words relative to our company‚Äôs subject matter. When you have a corpus of words that are already defined (such as a digitized book), finding the population of words is relatively simple: split the text based on spaces &amp; punctuation and you‚Äôre more or less done. However, with a URL, you have one continuous string with no word boundaries. One way to try and find the boundaries would be the following in Python:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n</pre></td><td class=\"code\"><pre><span class=\"kn\">import</span> <span class=\"nn\">collections</span>\n<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n\n<span class=\"c1\">#Dictionary from Unix\n</span><span class=\"n\">internal_dict</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"/usr/share/dict/words\"</span><span class=\"p\">)</span>\n<span class=\"c1\">#Stopwords corpus from NLTK\n</span><span class=\"n\">stopwords</span> <span class=\"o\">=</span> <span class=\"n\">nltk</span><span class=\"p\">.</span><span class=\"n\">corpus</span><span class=\"p\">.</span><span class=\"n\">stopwords</span><span class=\"p\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s\">'english'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#Build english_dictionary of prospect words\n</span><span class=\"n\">english_dictionary</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">internal_dict</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"n\">line</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">stopwords</span> <span class=\"ow\">and</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">4</span><span class=\"p\">:</span>  <span class=\"c1\">#make sure only \"big\", useful words included\n</span>        <span class=\"n\">english_dictionary</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">.</span><span class=\"n\">rstrip</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"se\">\\n</span><span class=\"s\">'</span><span class=\"p\">))</span>\n\n<span class=\"c1\">#How many words are in the complete dictionary?        \n</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">english_dictionary</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#Import urls\n</span><span class=\"n\">urls</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">line</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"/path/to/urls/file.csv\"</span><span class=\"p\">)]</span>\n\n<span class=\"c1\">#Build counter dictionary\n</span><span class=\"n\">wordcount</span> <span class=\"o\">=</span> <span class=\"n\">collections</span><span class=\"p\">.</span><span class=\"n\">Counter</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">english_dictionary</span><span class=\"p\">:</span>    <span class=\"c1\">#Loop over all possible English words\n</span>  <span class=\"k\">for</span> <span class=\"n\">url</span> <span class=\"ow\">in</span> <span class=\"n\">urls</span><span class=\"p\">:</span>     <span class=\"c1\">#Loop over all urls in list\n</span>    <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">url</span><span class=\"p\">:</span>\n      <span class=\"n\">wordcount</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span> <span class=\"c1\">#Once word found, add to dictionary counter</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<p>The problem with approaching the word searching problem in this manner is you are limited to the power of your local machine. In my case with a relatively new MacBook Pro, I can process 1,000 lines in 19 seconds as a single-threaded process. At 250,000,000 URLs, that‚Äôs 4.75 million seconds‚Ä¶197,916 minutes‚Ä¶3,298 hours‚Ä¶137 days‚Ä¶<strong>4.58 months!¬†</strong> Of course, 4.58 months is for the data I have <span style=\"text-decoration: underline;\">now</span>, which is accumulating every second of every day. Clearly, to find just the custom dictionary of words, I‚Äôll need to employ MANY more computers/tasks.</p>\n\n<h2 id=\"amazon-elasticmapreduce--lots-of-horsepower\">Amazon ElasticMapreduce = Lots of Horsepower</h2>\n\n<p>One thing you might notice about the Python code above is that the two loops have no real reason to be run serially; each comparison of URL and dictionary word can be run independently of each other (often referred to as ‚Äú<a title=\"Embarassingly parallel\" href=\"http://english.stackexchange.com/questions/83677/what-is-embarrassing-about-an-embarrassingly-parallel-problem\" target=\"_blank\">embarrassingly parallel</a>‚Äù). This type of programming pattern is one that is well suited to running on a Hadoop cluster. With Amazon ElasticMapReduce (EMR), we can provision tens, hundreds, even thousands of computer instances to process this URL-dictionary word comparison, and thus getting our answer much faster. The one downside of using Amazon EMR to access Hadoop is that EMR expects to get a Java ``.jar` file containing your MapReduce code. Luckily, there is a Python package called <a title=\"MRjob Python package\" href=\"http://pythonhosted.org/mrjob/\" target=\"_blank\">MRJob</a> that does the Python-to-Java translation automatically, so that users don‚Äôt have to switch languages to get massively parallel processing.</p>\n\n<h2 id=\"writing-mapreduce-code\">Writing MapReduce code</h2>\n\n<p>The Python code above, keeping a tally of words &amp; number of occurrences IS a version of the MapReduce coding paradigm. Going through the looping process to do the comparison is the ‚ÄúMap‚Äù portion of the code and the sum of the word values is the ‚ÄúReduce‚Äù step. However, in order to use EMR, we need to modify the above code to remove the outer URL loop:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n</pre></td><td class=\"code\"><pre><span class=\"kn\">from</span> <span class=\"nn\">mrjob.job</span> <span class=\"kn\">import</span> <span class=\"n\">MRJob</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">MRWordCounter</span><span class=\"p\">(</span><span class=\"n\">MRJob</span><span class=\"p\">):</span>    \n\n  <span class=\"k\">def</span> <span class=\"nf\">mapper</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">english_dict</span><span class=\"p\">,</span> <span class=\"n\">line</span><span class=\"p\">):</span>\n  <span class=\"n\">english_dict</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'aal'</span><span class=\"p\">,</span> <span class=\"s\">'aalii'</span><span class=\"p\">,</span> <span class=\"s\">'aam'</span><span class=\"p\">,</span> <span class=\"s\">'aani'</span><span class=\"p\">...</span><span class=\"s\">'zythum'</span><span class=\"p\">,</span> <span class=\"s\">'zyzomys'</span><span class=\"p\">,</span> <span class=\"s\">'zyzzogeton'</span><span class=\"p\">]</span>\n\n  <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">english_dict</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">line</span><span class=\"p\">:</span>\n                <span class=\"k\">yield</span> <span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"mi\">1</span>\n\n  <span class=\"k\">def</span> <span class=\"nf\">reducer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"n\">occurrences</span><span class=\"p\">):</span>\n        <span class=\"k\">yield</span> <span class=\"n\">word</span><span class=\"p\">,</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">occurrences</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"s\">'__main__'</span><span class=\"p\">:</span>\n    <span class=\"n\">MRWordCounter</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<p>The reason why we remove the outer loop that loops over the lines of the URL file is because that is implicit to the EMR/Hadoop style of processing. We will specify a file that we want to process in our Python script, then EMR will distribute the URLs file across all the Hadoop nodes. Essentially, our 250,000,000 million lines of URLs will become 1,000 tasks of length 250,000 urls (assuming 125 nodes of 8 tasks each).</p>\n\n<h3 id=\"calling-emr-from-the-python-command-line\">Calling EMR from the Python command line</h3>\n\n<p>Once we have our Python MRJob code written, we can submit our code to EMR from the command line. Here‚Äôs what an example code looks like:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n</pre></td><td class=\"code\"><pre>python ~/Desktop/mapreduce.py <span class=\"nt\">-r</span> emr s3://&lt;s3bucket&gt;/url_unload/0000_part_01 <span class=\"nt\">--output-dir</span><span class=\"o\">=</span>s3://&lt;s3bucket&gt;/url_output <span class=\"nt\">--num-ec2-instances</span><span class=\"o\">=</span>81\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<p>There are many more options that are possible for the MRJob package, so I highly suggest that you read the <a title=\"MRJobs EMR options\" href=\"http://pythonhosted.org/mrjob/guides/emr-quickstart.html\" target=\"_blank\">documentation for EMR options</a>. One thing to also note is that MRJob uses a configuration file to host various options for EMR called ‚Äúrunners‚Äù.¬† Yelp (the maker of the MRJob package) has posted an <a title=\"MRJob .conf file\" href=\"https://github.com/Yelp/mrjob/blob/master/mrjob.conf.example\" target=\"_blank\">example of the mrjob.conf file</a> with the most common options to use. In this file, you can specify your Amazon API keys, the type of instances you want to use (I use c1.xlarge spot instances for the most part), where your SSH keys are located and so on.</p>\n\n<h2 id=\"results\">Results</h2>\n\n<p>In terms of performance, I have 8 files of 5GB‚Äôs each of URLs (~17.5 million lines per file) that I‚Äôm running through the MRJob code above. The first file was run with 19 c1.xlarge instances, creating on average 133 mappers and 65 reducers and taking 917 minutes (<em>3.14 seconds/1000 lines</em>).¬† The second file was run with 80 c1.xlarge instances, creating 560 mappers and 160 reducers and taking 218 minutes (<em>0.75 seconds/1000 lines</em>). So using four times as many instances leads to one-fourth of the run-time.</p>\n\n<p>For the most part, you can expect linear performance in terms of adding nodes to your EMR cluster. I know at some point, Hadoop will decide that it no longer needs to add any more mappers/reducers, but I haven‚Äôt had the desire to find out exactly how many I‚Äôd need to add to get to that point! üôÇ</p>"
}