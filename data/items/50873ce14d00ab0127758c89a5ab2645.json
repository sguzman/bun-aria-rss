{
  "title": "Joining the Transformer Encoder and Decoder Plus Masking",
  "link": "https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/",
  "comments": "https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/#comments",
  "dc:creator": "Stefania Cristina",
  "pubDate": "Sun, 09 Oct 2022 16:00:50 +0000",
  "category": [
    "Attention",
    "attention",
    "decoder",
    "encoder",
    "masking",
    "natural language processing",
    "transformer"
  ],
  "guid": "https://35.82.237.216/?p=13441",
  "description": "<p>Last Updated on November 2, 2022 We have arrived at a point where we have implemented and tested the Transformer encoder and decoder separately, and we may now join the two together into a complete model. We will also see how to create padding and look-ahead masks by which we will suppress the input values [&#8230;]</p>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">Joining the Transformer Encoder and Decoder Plus Masking</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "content:encoded": "<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13441 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Joining+the+Transformer+Encoder+and+Decoder+Plus+Masking&url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Joining+the+Transformer+Encoder+and+Decoder+Plus+Masking&url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p id=\"last-modified-info\">Last Updated on November 2, 2022</p>\n<p>We have arrived at a point where we have implemented and tested the Transformer <a href=\"https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras\">encoder</a> and <a href=\"https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras\">decoder</a> separately, and we may now join the two together into a complete model. We will also see how to create padding and look-ahead masks by which we will suppress the input values that will not be considered in the encoder or decoder computations. Our end goal remains to apply the complete model to Natural Language Processing (NLP).</p>\n<p>In this tutorial, you will discover how to implement the complete Transformer model and create padding and look-ahead masks.<span class=\"Apple-converted-space\"> </span></p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>How to create a padding mask for the encoder and decoder</li>\n<li>How to create a look-ahead mask for the decoder</li>\n<li>How to join the Transformer encoder and decoder into a single model</li>\n<li>How to print out a summary of the encoder and decoder layers</li>\n</ul>\n<p>Let’s get started.<span class=\"Apple-converted-space\"> </span></p>\n<div id=\"attachment_13467\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><a href=\"http://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-scaled.jpg\"><img aria-describedby=\"caption-attachment-13467\" loading=\"lazy\" class=\"wp-image-13467 size-large\" src=\"http://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-1024x767.jpg\" alt=\"\" width=\"1024\" height=\"767\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-1024x767.jpg 1024w, https://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-300x225.jpg 300w, https://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-768x576.jpg 768w, https://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-1536x1151.jpg 1536w, https://machinelearningmastery.com/wp-content/uploads/2022/04/model_cover-2048x1535.jpg 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><p id=\"caption-attachment-13467\" class=\"wp-caption-text\">Joining the Transformer encoder and decoder and Masking<br />Photo by <a href=\"https://unsplash.com/photos/ykeLTANUQyE\">John O&#8217;Nolan</a>, some rights reserved.</p></div>\n<h2><b>Tutorial Overview</b></h2>\n<p>This tutorial is divided into four parts; they are:</p>\n<ul>\n<li>Recap of the Transformer Architecture</li>\n<li>Masking\n<ul>\n<li>Creating a Padding Mask</li>\n<li>Creating a Look-Ahead Mask</li>\n</ul>\n</li>\n<li>Joining the Transformer Encoder and Decoder</li>\n<li>Creating an Instance of the Transformer Model\n<ul>\n<li>Printing Out a Summary of the Encoder and Decoder Layers</li>\n</ul>\n</li>\n</ul>\n<h2><b>Prerequisites</b></h2>\n<p>For this tutorial, we assume that you are already familiar with:</p>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/the-transformer-model/\">The Transformer model</a></li>\n<li><a href=\"https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras\">The Transformer encoder</a></li>\n<li><a href=\"https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras\">The Transformer decoder</a></li>\n</ul>\n<h2><b>Recap of the Transformer Architecture</b></h2>\n<p><a href=\"https://machinelearningmastery.com/the-transformer-model/\">Recall</a> having seen that the Transformer architecture follows an encoder-decoder structure. The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.</p>\n<div id=\"attachment_12821\" style=\"width: 379px\" class=\"wp-caption aligncenter\"><a href=\"http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"><img aria-describedby=\"caption-attachment-12821\" loading=\"lazy\" class=\"wp-image-12821\" src=\"http://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" alt=\"\" width=\"369\" height=\"520\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png 727w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-213x300.png 213w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png 768w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-1090x1536.png 1090w, https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png 1320w\" sizes=\"(max-width: 369px) 100vw, 369px\" /></a><p id=\"caption-attachment-12821\" class=\"wp-caption-text\">The encoder-decoder structure of the Transformer architecture <br />Taken from &#8220;<a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>&#8220;</p></div>\n<p>In generating an output sequence, the Transformer does not rely on recurrence and convolutions.</p>\n<p>You have seen how to implement the Transformer encoder and decoder separately. In this tutorial, you will join the two into a complete Transformer model and apply padding and look-ahead masking to the input values. <span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s start first by discovering how to apply masking.<span class=\"Apple-converted-space\"> </span></p>\n<p><strong>Kick-start your project</strong> with my book <a href=\"https://machinelearningmastery.com/transformer-models-with-attention/\">Building Transformer Models with Attention</a>. It provides <strong>self-study tutorials</strong> with <strong>working code</strong> to guide you into building a fully-working transformer models that can<br><em>translate sentences from one language to another</em>...</p>\n<h2><b>Masking</b></h2>\n<h3><b>Creating a Padding Mask</b></h3>\n<p>You should already be familiar with the importance of masking the input values before feeding them into the encoder and decoder.<span class=\"Apple-converted-space\"> </span></p>\n<p>As you will see when you proceed to <a href=\"https://machinelearningmastery.com/training-the-transformer-model\">train the Transformer model</a>, the input sequences fed into the encoder and decoder will first be zero-padded up to a specific sequence length. The importance of having a padding mask is to make sure that these zero values are not processed along with the actual input values by both the encoder and decoder.<span class=\"Apple-converted-space\"> </span></p>\n<p>Let’s create the following function to generate a padding mask for both the encoder and decoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from tensorflow import math, cast, float32\n\ndef padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n    mask = math.equal(input, 0)\n    mask = cast(mask, float32)\n\n    return mask</pre><p>Upon receiving an input, this function will generate a tensor that marks by a value of <em>one</em> wherever the input contains a value of <em>zero</em>. <span class=\"Apple-converted-space\"> </span></p>\n<p>Hence, if you input the following array:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from numpy import array\n\ninput = array([1, 2, 3, 4, 0, 0, 0])\nprint(padding_mask(input))</pre><p>Then the output of the <code>padding_mask</code> function would be the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">tf.Tensor([0. 0. 0. 0. 1. 1. 1.], shape=(7,), dtype=float32)</pre><p></p>\n<h3><b>Creating a Look-Ahead Mask</b></h3>\n<p>A look-ahead mask is required to prevent the decoder from attending to succeeding words, such that the prediction for a particular word can only depend on known outputs for the words that come before it.</p>\n<p>For this purpose, let’s create the following function to generate a look-ahead mask for the decoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from tensorflow import linalg, ones\n\ndef lookahead_mask(shape):\n    # Mask out future entries by marking them with a 1.0\n    mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n\n    return mask</pre><p>You will pass to it the length of the decoder input. Let&#8217;s make this length equal to 5, as an example:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">print(lookahead_mask(5))</pre><p>Then the output that the <code>lookahead_mask</code> function returns is the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">tf.Tensor(\n[[0. 1. 1. 1. 1.]\n [0. 0. 1. 1. 1.]\n [0. 0. 0. 1. 1.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)</pre><p>Again, the <em>one</em> values mask out the entries that should not be used. In this manner, the prediction of every word only depends on those that come before it.<span class=\"Apple-converted-space\"> </span></p>\n<h2><b>Joining the Transformer Encoder and Decoder</b></h2>\n<p>Let’s start by creating the class, <code>TransformerModel</code>, which inherits from the <code>Model</code> base class in Keras:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">class TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n        ...</pre><p>Our first step in creating the <code>TransformerModel</code> class is to initialize instances of the <code>Encoder</code> and <code>Decoder</code> classes implemented earlier and assign their outputs to the variables, <code>encoder</code> and <code>decoder</code>, respectively. If you saved these classes in separate Python scripts, do not forget to import them. I saved my code in the Python scripts <i>encoder.py</i> and <i>decoder.py</i>, so I need to import them accordingly.<span class=\"Apple-converted-space\"> </span></p>\n<p>You will also include one final dense layer that produces the final output, as in the Transformer architecture of <a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al. (2017)</a>.<span class=\"Apple-converted-space\"> </span></p>\n<p>Next, you shall create the class method, <code>call()</code>, to feed the relevant inputs into the encoder and decoder.</p>\n<p>A padding mask is first generated to mask the encoder input, as well as the encoder output, when this is fed into the second self-attention block of the decoder:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\ndef call(self, encoder_input, decoder_input, training):\n\n    # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n    enc_padding_mask = self.padding_mask(encoder_input)\n...</pre><p>A padding mask and a look-ahead mask are then generated to mask the decoder input. These are combined together through an element-wise <code>maximum</code> operation:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# Create and combine padding and look-ahead masks to be fed into the decoder\ndec_in_padding_mask = self.padding_mask(decoder_input)\ndec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\ndec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n...</pre><p>Next, the relevant inputs are fed into the encoder and decoder, and the Transformer model output is generated by feeding the decoder output into one final dense layer:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# Feed the input into the encoder\nencoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n# Feed the encoder output into the decoder\ndecoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n# Pass the decoder output through a final dense layer\nmodel_output = self.model_last_layer(decoder_output)\n\nreturn model_output</pre><p>Combining all the steps gives us the following complete code listing:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from encoder import Encoder\nfrom decoder import Decoder\nfrom tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense\n\n\nclass TransformerModel(Model):\n    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n        super(TransformerModel, self).__init__(**kwargs)\n\n        # Set up the encoder\n        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Set up the decoder\n        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n\n        # Define the final dense layer\n        self.model_last_layer = Dense(dec_vocab_size)\n\n    def padding_mask(self, input):\n        # Create mask which marks the zero padding values in the input by a 1.0\n        mask = math.equal(input, 0)\n        mask = cast(mask, float32)\n\n        # The shape of the mask should be broadcastable to the shape\n        # of the attention weights that it will be masking later on\n        return mask[:, newaxis, newaxis, :]\n\n    def lookahead_mask(self, shape):\n        # Mask out future entries by marking them with a 1.0\n        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n\n        return mask\n\n    def call(self, encoder_input, decoder_input, training):\n\n        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n        enc_padding_mask = self.padding_mask(encoder_input)\n\n        # Create and combine padding and look-ahead masks to be fed into the decoder\n        dec_in_padding_mask = self.padding_mask(decoder_input)\n        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n\n        # Feed the input into the encoder\n        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n\n        # Feed the encoder output into the decoder\n        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n\n        # Pass the decoder output through a final dense layer\n        model_output = self.model_last_layer(decoder_output)\n\n        return model_output</pre><p>Note that you have performed a small change to the output that is returned by the <code>padding_mask</code> function. Its shape is made broadcastable to the shape of the attention weight tensor that it will mask when you train the Transformer model.<span class=\"Apple-converted-space\"> </span></p>\n<h2><b>Creating an Instance of the Transformer Model</b></h2>\n<p>You will work with the parameter values specified in the paper, <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, by Vaswani et al. (2017):</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">h = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n...</pre><p>As for the input-related parameters, you will work with dummy values for now until you arrive at the stage of <a href=\"https://machinelearningmastery.com/training-the-transformer-model\">training the complete Transformer model</a>. At that point, you will use actual sentences:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\nenc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n\nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence\n...</pre><p>You can now create an instance of the <code>TransformerModel</code> class as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from model import TransformerModel\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)</pre><p>The complete code listing is as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">enc_vocab_size = 20 # Vocabulary size for the encoder\ndec_vocab_size = 20 # Vocabulary size for the decoder\n\nenc_seq_length = 5  # Maximum length of the input sequence\ndec_seq_length = 5  # Maximum length of the target sequence\n\nh = 8  # Number of self-attention heads\nd_k = 64  # Dimensionality of the linearly projected queries and keys\nd_v = 64  # Dimensionality of the linearly projected values\nd_ff = 2048  # Dimensionality of the inner fully connected layer\nd_model = 512  # Dimensionality of the model sub-layers' outputs\nn = 6  # Number of layers in the encoder stack\n\ndropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n\n# Create model\ntraining_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)</pre><p></p>\n<h3><b>Printing Out a Summary of the Encoder and Decoder Layers</b></h3>\n<p>You may also print out a summary of the encoder and decoder blocks of the Transformer model. The choice to print them out separately will allow you to be able to see the details of their individual sub-layers. In order to do so, add the following line of code to the <code>__init__()</code> method of both the <code>EncoderLayer</code> and <code>DecoderLayer</code> classes:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">self.build(input_shape=[None, sequence_length, d_model])</pre><p>Then you need to add the following method to the <code>EncoderLayer</code> class:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def build_graph(self):\n    input_layer = Input(shape=(self.sequence_length, self.d_model))\n    return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))</pre><p>And the following method to the <code>DecoderLayer</code> class:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">def build_graph(self):\n    input_layer = Input(shape=(self.sequence_length, self.d_model))\n    return Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))</pre><p>This results in the <code>EncoderLayer</code> class being modified as follows (the three dots under the <code>call()</code> method mean that this remains the same as the one that was implemented <a href=\"https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras\">here</a>):</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\n\nclass EncoderLayer(Layer):\n    def __init__(self, sequence_length, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n        super(EncoderLayer, self).__init__(**kwargs)\n        self.build(input_shape=[None, sequence_length, d_model])\n        self.d_model = d_model\n        self.sequence_length = sequence_length\n        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n        self.dropout1 = Dropout(rate)\n        self.add_norm1 = AddNormalization()\n        self.feed_forward = FeedForward(d_ff, d_model)\n        self.dropout2 = Dropout(rate)\n        self.add_norm2 = AddNormalization()\n\n    def build_graph(self):\n        input_layer = Input(shape=(self.sequence_length, self.d_model))\n        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n\n    def call(self, x, padding_mask, training):\n        ...</pre><p>Similar changes can be made to the <code>DecoderLayer</code> class too.</p>\n<p>Once you have the necessary changes in place, you can proceed to create instances of the <code>EncoderLayer</code> and <code>DecoderLayer</code> classes and print out their summaries as follows:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">from encoder import EncoderLayer\nfrom decoder import DecoderLayer\n\nencoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\nencoder.build_graph().summary()\n\ndecoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\ndecoder.build_graph().summary()</pre><p>The resulting summary for the encoder is the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 5, 512)]     0           []                               \n                                                                                                  \n multi_head_attention_18 (Multi  (None, 5, 512)      131776      ['input_1[0][0]',                \n HeadAttention)                                                   'input_1[0][0]',                \n                                                                  'input_1[0][0]']                \n                                                                                                  \n dropout_32 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_18[0][0]']\n                                                                                                  \n add_normalization_30 (AddNorma  (None, 5, 512)      1024        ['input_1[0][0]',                \n lization)                                                        'dropout_32[0][0]']             \n                                                                                                  \n feed_forward_12 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_30[0][0]']   \n                                                                                                  \n dropout_33 (Dropout)           (None, 5, 512)       0           ['feed_forward_12[0][0]']        \n                                                                                                  \n add_normalization_31 (AddNorma  (None, 5, 512)      1024        ['add_normalization_30[0][0]',   \n lization)                                                        'dropout_33[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 2,233,536\nTrainable params: 2,233,536\nNon-trainable params: 0\n__________________________________________________________________________________________________</pre><p>While the resulting summary for the decoder is the following:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 5, 512)]     0           []                               \n                                                                                                  \n multi_head_attention_19 (Multi  (None, 5, 512)      131776      ['input_2[0][0]',                \n HeadAttention)                                                   'input_2[0][0]',                \n                                                                  'input_2[0][0]']                \n                                                                                                  \n dropout_34 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_19[0][0]']\n                                                                                                  \n add_normalization_32 (AddNorma  (None, 5, 512)      1024        ['input_2[0][0]',                \n lization)                                                        'dropout_34[0][0]',             \n                                                                  'add_normalization_32[0][0]',   \n                                                                  'dropout_35[0][0]']             \n                                                                                                  \n multi_head_attention_20 (Multi  (None, 5, 512)      131776      ['add_normalization_32[0][0]',   \n HeadAttention)                                                   'input_2[0][0]',                \n                                                                  'input_2[0][0]']                \n                                                                                                  \n dropout_35 (Dropout)           (None, 5, 512)       0           ['multi_head_attention_20[0][0]']\n                                                                                                  \n feed_forward_13 (FeedForward)  (None, 5, 512)       2099712     ['add_normalization_32[1][0]']   \n                                                                                                  \n dropout_36 (Dropout)           (None, 5, 512)       0           ['feed_forward_13[0][0]']        \n                                                                                                  \n add_normalization_34 (AddNorma  (None, 5, 512)      1024        ['add_normalization_32[1][0]',   \n lization)                                                        'dropout_36[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 2,365,312\nTrainable params: 2,365,312\nNon-trainable params: 0\n__________________________________________________________________________________________________</pre><p></p>\n<h2><b>Further Reading</b></h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3><b>Books</b></h3>\n<ul>\n<li><a href=\"https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X\">Advanced Deep Learning with Python</a>, 2019</li>\n<li><a href=\"https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798\">Transformers for Natural Language Processing</a>, 2021</li>\n</ul>\n<h3><b>Papers</b></h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a>, 2017</li>\n</ul>\n<h2><b>Summary</b></h2>\n<p>In this tutorial, you discovered how to implement the complete Transformer model and create padding and look-ahead masks.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How to create a padding mask for the encoder and decoder</li>\n<li>How to create a look-ahead mask for the decoder</li>\n<li>How to join the Transformer encoder and decoder into a single model</li>\n<li>How to print out a summary of the encoder and decoder layers</li>\n</ul>\n<p>Do you have any questions?<br />\nAsk your questions in the comments below and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-13441 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Joining+the+Transformer+Encoder+and+Decoder+Plus+Masking&url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Joining+the+Transformer+Encoder+and+Decoder+Plus+Masking&url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\">Joining the Transformer Encoder and Decoder Plus Masking</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
  "wfw:commentRss": "https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/feed/",
  "slash:comments": 4
}