{
  "title": "Reliable Off-policy Evaluation for Reinforcement Learning. (arXiv:2011.04102v3 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2011.04102",
  "description": "<p>In a sequential decision-making problem, off-policy evaluation estimates the\nexpected cumulative reward of a target policy using logged trajectory data\ngenerated from a different behavior policy, without execution of the target\npolicy. Reinforcement learning in high-stake environments, such as healthcare\nand education, is often limited to off-policy settings due to safety or ethical\nconcerns, or inability of exploration. Hence it is imperative to quantify the\nuncertainty of the off-policy estimate before deployment of the target policy.\nIn this paper, we propose a novel framework that provides robust and optimistic\ncumulative reward estimates using one or multiple logged trajectories data.\nLeveraging methodologies from distributionally robust optimization, we show\nthat with proper selection of the size of the distributional uncertainty set,\nthese estimates serve as confidence bounds with non-asymptotic and asymptotic\nguarantees under stochastic or adversarial environments. Our results are also\ngeneralized to batch reinforcement learning and are supported by empirical\nanalysis.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>"
}