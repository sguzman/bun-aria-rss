{
  "title": "Improved Bonferroni correction factors for multiple pairwise comparisons",
  "link": "",
  "published": "2016-04-10T07:58:00-07:00",
  "updated": "2016-04-10T07:58:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2016-04-10:/bonferroni-correction-for-multiple-pairwise-comparison-tests",
  "summary": "<p>A common task in applied statistics is the pairwise comparison of the responses of <span class=\"math\">\\(N\\)</span> treatment groups in some statistical test &#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the …</p>",
  "content": "<p>A common task in applied statistics is the pairwise comparison of the responses of <span class=\"math\">\\(N\\)</span> treatment groups in some statistical test &#8212; the goal being to decide which pairs exhibit differences that are statistically significant. Now, because there is one comparison being made for each pairing, a naive application of the Bonferroni correction analysis suggests that one should set the individual pairwise test sizes to <span class=\"math\">\\(\\alpha_i \\to \\alpha_f/{N \\choose 2}\\)</span> in order to obtain a desired family-wise type 1 error rate of <span class=\"math\">\\(\\alpha_f\\)</span>. Indeed, this solution is suggested by many texts. However, implicit in the Bonferroni analysis is the assumption that the comparisons being made are each mutually independent. This is not the case here, and we show that as a consequence the naive approach often returns type 1 error rates far from those desired. We provide adjusted formulas that allow for error-free Bonferroni-like corrections to be&nbsp;made.</p>\n<p>(edit (7/4/2016): After posting this article, I&#8217;ve since found that the method we suggest here is related to / is a generalization of Tukey&#8217;s range test &#8212; see <a href=\"https://en.wikipedia.org/wiki/Tukey%27s_range_test\">here</a>.)</p>\n<p>(edit (6/11/2018): I&#8217;ve added the notebook used below to our Github, <a href=\"https://github.com/EFavDB/improved_bonferroni\">here</a>)</p>\n<h3>Introduction</h3>\n<p>In this post, we consider a particular kind of statistical test where one examines <span class=\"math\">\\(N\\)</span> different treatment groups, measures some particular response within each, and then decides which of the <span class=\"math\">\\({N \\choose 2}\\)</span> pairs appear to exhibit responses that differ significantly. This is called the pairwise comparison problem (or sometimes &#8220;posthoc analysis&#8221;). It comes up in many contexts, and in general it will be of interest whenever one is carrying out a multiple-treatment&nbsp;test.</p>\n<p>Our specific interest here is in identifying the appropriate individual measurement error bars needed to guarantee a given family-wise type 1 error rate, <span class=\"math\">\\(\\alpha_f\\)</span>. Briefly, <span class=\"math\">\\(\\alpha_f\\)</span> is the probability that we incorrectly make any assertion that two measurements differ significantly when the true effect sizes we&#8217;re trying to measure are actually all the same. This can happen due to the nature of statistical fluctuations. For example, when measuring the heights of <span class=\"math\">\\(N\\)</span> identical objects, measurement error can cause us to incorrectly think that some pair have slightly different heights, even though that&#8217;s not the case. A classical approach to addressing this problem is given by the Bonferroni approximation: If we consider <span class=\"math\">\\(\\mathcal{N}\\)</span> independent comparisons, and each has an individual type 1 error rate of <span class=\"math\">\\(\\alpha_i,\\)</span> then the family-wise probability of not making any type 1 errors is simply the product of the probabilities that we don&#8217;t make any individual type 1&nbsp;errors,\n</p>\n<div class=\"math\">$$ \\tag{1} \\label{bon1}\np_f = (1 - \\alpha_f) = p_i^{\\mathcal{N}} \\equiv \\left ( 1 - \\alpha_i \\right)^{\\mathcal{N}} \\approx 1 - \\mathcal{N} \\alpha_i.\n$$</div>\n<p>\nThe last equality here is an expansion that holds when <span class=\"math\">\\(p_f\\)</span> is close to <span class=\"math\">\\(1\\)</span>, the limit we usually work in. Rearranging (\\ref{bon1}) gives a simple&nbsp;expression,\n</p>\n<div class=\"math\">$$ \\tag{2} \\label{bon2}\n\\alpha_i = \\frac{\\alpha_f}{\\mathcal{N}}.\n$$</div>\n<p>\nThis is the (naive) Bonferroni approximation &#8212; it states that one should use individual tests of size <span class=\"math\">\\(\\alpha_f / \\mathcal{N}\\)</span> in order to obtain a family-wise error rate of <span class=\"math\">\\(\\alpha_f\\)</span>.</p>\n<p>The reason why we refer to (\\ref{bon2}) as the naive Bonferroni approximation is that it doesn&#8217;t actually apply to the problem we consider here. The reason why is that <span class=\"math\">\\(p_f \\not = p_i^{\\mathcal{N}}\\)</span> in (\\ref{bon1}) if the <span class=\"math\">\\(\\mathcal{N}\\)</span> comparisons considered are not independent: This is generally the case for our system of <span class=\"math\">\\(\\mathcal{N} = {N \\choose 2}\\)</span> comparisons, since they are based on an underlying set of measurements having only <span class=\"math\">\\(N\\)</span> degrees of freedom (the object heights, in our example). Despite this obvious issue, the naive approximation is often applied in this context. Here, we explore the nature of the error incurred in such applications, and we find that it is sometimes very significant. We also show that it&#8217;s actually quite simple to apply the principle behind the Bonferroni approximation without error: One need only find a way to evaluate the true <span class=\"math\">\\(p_f\\)</span> for any particular choice of error bars. Inverting this then allows one to identify the error bars needed to obtain the desired <span class=\"math\">\\(p_f\\)</span>.</p>\n<h3>General&nbsp;treatment</h3>\n<p>In this section, we derive a formal expression for the type 1 error rate in the pairwise comparison problem. For simplicity, we will assume 1) that the uncertainty in each of our <span class=\"math\">\\(N\\)</span> individual measurements is the same (e.g., the variance in the case of Normal variables), and 2) that our pairwise tests assert that two measurements differ statistically if and only if they are more than <span class=\"math\">\\(k\\)</span> units&nbsp;apart.</p>\n<p>To proceed, we consider the probability that a type 1 error does not occur, <span class=\"math\">\\(p_f\\)</span>. This requires that all <span class=\"math\">\\(N\\)</span> measurements sit within <span class=\"math\">\\(k\\)</span> units of each other. For any set of values satisfying this condition, let the smallest of the set be <span class=\"math\">\\(x\\)</span>. We have <span class=\"math\">\\(N\\)</span> choices for which of the treatments sit as this position. The remaining <span class=\"math\">\\((N-1)\\)</span> values must all be within the region <span class=\"math\">\\((x, x+k)\\)</span>. Because we&#8217;re considering the type 1 error rate, we can assume that each of the independent measurements takes on the same distribution <span class=\"math\">\\(P(x)\\)</span>. These considerations&nbsp;imply\n</p>\n<div class=\"math\">$$ \\tag{3} \\label{gen}\np_{f} \\equiv 1 - \\alpha_{f} = N \\int_{-\\infty}^{\\infty} P(x) \\left \\{\\int_x^{x+k} P(y) dy \\right \\}^{N-1} dx.\n$$</div>\n<p>\nEquation (\\ref{gen}) is our main result. It is nice for a couple of reasons. First, its form implies that when <span class=\"math\">\\(N\\)</span> is large it will scale like <span class=\"math\">\\(a \\times p_{1,eff}^N\\)</span>, for some <span class=\"math\">\\(k\\)</span>-dependent numbers <span class=\"math\">\\(a\\)</span> and <span class=\"math\">\\(p_{1,eff}\\)</span>. This is reminiscent of the expression (\\ref{bon1}), where <span class=\"math\">\\(p_f\\)</span> took the form <span class=\"math\">\\(p_i^{\\mathcal{N}}\\)</span>. Here, we see that the correct value actually scales like some number to the <span class=\"math\">\\(N\\)</span>-th power, not the <span class=\"math\">\\(\\mathcal{N}\\)</span>-th. This reflects the fact that we actually only have <span class=\"math\">\\(N\\)</span> independent degrees of freedom here, not <span class=\"math\">\\({N \\choose 2}\\)</span>. Second, when the inner integral above can be carried out formally, (\\ref{gen}) can be expressed as a single one-dimensional integral. In such cases, the integral can be evaluated numerically for any <span class=\"math\">\\(k\\)</span>, allowing one to conveniently identify the <span class=\"math\">\\(k\\)</span> that returns any specific, desired <span class=\"math\">\\(p_f\\)</span>. We illustrate both points in the next two sections, where we consider Normal and Cauchy variables,&nbsp;respectively.</p>\n<h3>Normally-distributed&nbsp;responses</h3>\n<p>We now consider the case where the individual statistics are each Normally-distributed about zero, and we reject any pair if they are more than <span class=\"math\">\\(k \\times \\sqrt{2} \\sigma\\)</span> apart, with <span class=\"math\">\\(\\sigma^2\\)</span> the variance of the individual statistics. In this case, the inner integral of (\\ref{gen}) goes&nbsp;to\n</p>\n<div class=\"math\">$$\\tag{4} \\label{inner_g}\n\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_x^{x+k \\sqrt{2} \\sigma} \\exp\\left [ -\\frac{y^2}{2 \\sigma^2} \\right] dy = \\frac{1}{2} \\left [\\text{erf}(k + \\frac{x}{\\sqrt{2} \\sigma}) - \\text{erf}(\\frac{x}{\\sqrt{2} \\sigma})\\right].\n$$</div>\n<p>\nPlugging this into (\\ref{gen}), we&nbsp;obtain\n</p>\n<div class=\"math\">$$\\tag{5} \\label{exact_g}\np_f = \\int \\frac{N e^{-x^2 / 2 \\sigma^2}}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left ((N-1) \\log \\frac{1}{2} \\left [\\text{erf}(k + \\frac{x}{\\sqrt{2} \\sigma}) - \\text{erf}(\\frac{x}{\\sqrt{2} \\sigma})\\right]\\right)dx.\n$$</div>\n<p>\nThis exact expression (\\ref{exact_g}) can be used to obtain the <span class=\"math\">\\(k\\)</span> value needed to achieve any desired family-wise type 1error rate. Example solutions obtained in this way are compared to the <span class=\"math\">\\(k\\)</span>-values returned by the naive Bonferroni approach in the table below. The last column <span class=\"math\">\\(p_{f,Bon}\\)</span> shown is the family-wise success rate that you get when you plug in <span class=\"math\">\\(k_{Bon},\\)</span> the naive Bonferroni <span class=\"math\">\\(k\\)</span> value targeting <span class=\"math\">\\(p_{f,exact}\\)</span>.</p>\n<table>\n<thead>\n<tr>\n<th><span class=\"math\">\\(N\\)</span></th>\n<th><span class=\"math\">\\(p_{f,exact}\\)</span></th>\n<th><span class=\"math\">\\(k_{exact}\\)</span></th>\n<th><span class=\"math\">\\(k_{Bon}\\)</span></th>\n<th><span class=\"math\">\\(p_{f, Bon}\\)</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4</td>\n<td>0.9</td>\n<td>2.29</td>\n<td>2.39</td>\n<td>0.921</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.9</td>\n<td>2.78</td>\n<td>2.91</td>\n<td>0.929</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.95</td>\n<td>2.57</td>\n<td>2.64</td>\n<td>0.959</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.95</td>\n<td>3.03</td>\n<td>3.1</td>\n<td>0.959</td>\n</tr>\n</tbody>\n</table>\n<p>Examining the table shown, you can see that the naive approach is consistently overestimating the <span class=\"math\">\\(k\\)</span> values (error bars) needed to obtain the desired family-wise rates &#8212; but not dramatically so. The reason for the near-accuracy is that two solutions basically scale the same way with <span class=\"math\">\\(N\\)</span>. To see this, one can carry out an asymptotic analysis of (\\ref{exact_g}). We skip the details and note only that at large <span class=\"math\">\\(N\\)</span> we&nbsp;have\n</p>\n<div class=\"math\">$$\\tag{6} \\label{asy_g}\np_f \\sim \\text{erf} \\left ( \\frac{k}{2}\\right)^N\n\\sim \\left (1 - \\frac{e^{-k^2 / 4}}{k \\sqrt{\\pi}/2} \\right)^N.\n$$</div>\n<p>\nThis is interesting because the individual pairwise tests have p-values given&nbsp;by\n</p>\n<div class=\"math\">$$ \\tag{7} \\label{asy_i}\np_i = \\int_{-k\\sqrt{2}\\sigma}^{k\\sqrt{2}\\sigma} \\frac{e^{-x^2 / (4 \\sigma^2)}}{\\sqrt{4 \\pi \\sigma^2 }} = \\text{erf}(k /\\sqrt{2}) \\sim 1 - \\frac{e^{-k^2/2}}{k \\sqrt{\\pi/2}}.\n$$</div>\n<p>\nAt large <span class=\"math\">\\(k\\)</span>, this is dominated by the exponential. Comparing with (\\ref{asy_g}), this&nbsp;implies\n</p>\n<div class=\"math\">$$ \\tag{8} \\label{fin_g}\np_f \\sim \\left (1 - \\alpha_i^{1/2} \\right)^N \\sim 1 - N \\alpha_i^{1/2} \\equiv 1 - \\alpha_f.\n$$</div>\n<p>\nFixing <span class=\"math\">\\(\\alpha_f\\)</span>, this requires that <span class=\"math\">\\(\\alpha_i\\)</span> scale like <span class=\"math\">\\(N^{-2}\\)</span>, the same scaling with <span class=\"math\">\\(N\\)</span> as the naive Bonferroni solution. Thus, in the case of Normal variables, the Bonferroni approximation provides an inexact, but reasonable approximation (nevertheless, we suggest going with the exact approach using (\\ref{exact_g}), since it&#8217;s just as easy!). We show in the next section that this is not the case for Cauchy&nbsp;variables.</p>\n<h3>Cauchy-distributed&nbsp;variables</h3>\n<p>We&#8217;ll now consider the case of <span class=\"math\">\\(N\\)</span> independent, identically-distributed Cauchy variables having half widths <span class=\"math\">\\(a\\)</span>,\n</p>\n<div class=\"math\">$$ \\tag{9} \\label{c_dist}\nP(x) = \\frac{a}{\\pi} \\frac{1}{a^2 + x^2}.\n$$</div>\n<p>\nWhen we compare any two, we will reject the null if they are more than <span class=\"math\">\\(ka\\)</span> apart. With this choice, the inner integral of (\\ref{gen}) is&nbsp;now\n</p>\n<div class=\"math\">$$\n\\tag{10} \\label{inner_c}\n\\frac{a}{\\pi} \\int_x^{x+ k a} \\frac{1}{a^2 + y^2} dy =\\\\ \\frac{1}{\\pi} \\left [\\tan^{-1}(k + x/a) - \\tan^{-1}(x/a) \\right].\n$$</div>\n<p>\nPlugging into into (\\ref{gen}) now&nbsp;gives</p>\n<div class=\"math\">$$\\tag{11} \\label{exact_c}\np_f = \\int \\frac{N a/\\pi}{a^2 + x^2} e^{(N-1) \\log\n\\frac{1}{\\pi} \\left [\\tan^{-1}(k + x/a) - \\tan^{-1}(x/a) \\right]\n}.\n$$</div>\n<p>\nThis is the analog of (\\ref{exact_g}) for Cauchy variables &#8212; it can be used to find the exact <span class=\"math\">\\(k\\)</span> value needed to obtain a given family-wise type 1 error rate. The table below compares the exact values to those returned by the naive Bonferroni analysis [obtained using the fact that the difference between two independent Cauchy variables of width <span class=\"math\">\\(a\\)</span> is itself a Cauchy distributed variable, but with width <span class=\"math\">\\(2a\\)</span>].</p>\n<table>\n<thead>\n<tr>\n<th><span class=\"math\">\\(N\\)</span></th>\n<th><span class=\"math\">\\(p_{f,exact}\\)</span></th>\n<th><span class=\"math\">\\(k_{exact}\\)</span></th>\n<th><span class=\"math\">\\(k_{Bon}\\)</span></th>\n<th><span class=\"math\">\\(p_{f, Bon}\\)</span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>4</td>\n<td>0.9</td>\n<td>27</td>\n<td>76</td>\n<td>0.965</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.9</td>\n<td>55</td>\n<td>350</td>\n<td>0.985</td>\n</tr>\n<tr>\n<td>4</td>\n<td>0.95</td>\n<td>53</td>\n<td>153</td>\n<td>0.983</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.95</td>\n<td>107</td>\n<td>700</td>\n<td>0.993</td>\n</tr>\n</tbody>\n</table>\n<p>In this case, you can see that the naive Bonferroni approximation performs badly. For example, in the last line, it suggests using error bars that are seven times too large for each point estimate. The error gets even worse as <span class=\"math\">\\(N\\)</span> grows: Again, skipping the details, we note that in this limit, (\\ref{exact_c}) scales&nbsp;like\n</p>\n<div class=\"math\">$$\\tag{12} \\label{asym_c}\np_f \\sim \\left [\\frac{2}{\\pi} \\tan^{-1}(k/2) \\right]^N.\n$$</div>\n<p>\nThis can be related to the individual <span class=\"math\">\\(p_i\\)</span> values, which are given&nbsp;by\n</p>\n<div class=\"math\">$$ \\tag{13} \\label{asym2_c}\np_i = \\int_{-ka}^{ka} \\frac{2 a / \\pi}{4 a^2 + x^2}dx = \\frac{2}{\\pi}\\tan^{-1}(k/2).\n$$</div>\n<p>\nComparing the last two lines, we&nbsp;obtain\n</p>\n<div class=\"math\">$$ \\tag{14} \\label{asym3_c}\np_f \\equiv 1 - \\alpha_f \\sim p_i^N \\sim 1 - N \\alpha_i.\n$$</div>\n<p>\nAlthough we&#8217;ve been a bit sloppy with coefficients here, (\\ref{asym3_c}) gives the correct leading <span class=\"math\">\\(N\\)</span>-dependence: <span class=\"math\">\\(k_{exact} \\sim 1/\\alpha_i \\propto N\\)</span>. We can see this linear scaling in the table above. This explains why <span class=\"math\">\\(k_{exact}\\)</span> and <span class=\"math\">\\(k_{Bon}\\)</span> &#8212; which scales like <span class=\"math\">\\({N \\choose 2} \\sim N^2\\)</span> &#8212; differ more and more as <span class=\"math\">\\(N\\)</span> grows. In this case, you should definitely never use the naive approximation, but instead stick to the exact analysis based on&nbsp;(\\ref{exact_c}).</p>\n<h3>Conclusion</h3>\n<p>Some people criticize the Bonferroni correction factor as being too conservative. However, our analysis here suggests that this feeling may be due in part to its occasional improper application. The naive approximation simply does not apply in the case of pairwise comparisons because the <span class=\"math\">\\({N \\choose 2}\\)</span> pairs considered are not independent &#8212; there are only <span class=\"math\">\\(N\\)</span> independent degrees of freedom in this problem. Although the naive correction does not apply to the problem of pairwise comparisons, we&#8217;ve shown here that it remains a simple matter to correctly apply the principle behind it: One can easily select any desired family-wise type 1 error rate through an appropriate selection of the individual test sizes &#8212; just use&nbsp;(\\ref{gen})!</p>\n<p>We hope you enjoyed this post &#8212; we anticipate writing a bit more on hypothesis testing in the near&nbsp;future.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}