{
  "title": "How AI Will Change Chip Design",
  "link": "https://spectrum.ieee.org/ai-chip-design-matlab",
  "description": "\n<img src=\"https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0\"/><br/><br/><p>The end of <a href=\"https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing\" target=\"_self\">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href=\"https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip\" target=\"_self\">miniaturize transistors</a> and <a href=\"https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors\" target=\"_self\">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href=\"https://spectrum.ieee.org/processing-in-dram-accelerates-ai\" target=\"_self\">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href=\"https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests\" target=\"_self\">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href=\"https://www.linkedin.com/in/heather-gorr-phd\" rel=\"noopener noreferrer\" target=\"_blank\">Heather Gorr</a>, senior product manager for <a href=\"https://www.mathworks.com/\" rel=\"noopener noreferrer\" target=\"_blank\">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" data-rm-resized-container=\"25%\" style=\"float: left;\">\n<img alt=\"Portrait of a woman with blonde-red hair smiling at the camera\" class=\"rm-shortcode rm-resized-image\" data-rm-shortcode-id=\"1f18a02ccaf51f5c766af2ebc4af18e1\" data-rm-shortcode-name=\"rebelmouse-image\" id=\"2dc00\" loading=\"lazy\" src=\"https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980\" style=\"max-width: 100%\"/>\n<small class=\"image-media media-caption\" placeholder=\"Add Photo Caption...\" style=\"max-width: 100%;\">Heather Gorr</small><small class=\"image-media media-photo-credit\" placeholder=\"Add Photo Credit...\" style=\"max-width: 100%;\">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href=\"https://en.wikipedia.org/wiki/Model_order_reduction\" rel=\"noopener noreferrer\" target=\"_blank\">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href=\"https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html\" rel=\"noopener noreferrer\" target=\"_blank\">parameter sweeps</a>, your optimizations, your <a href=\"https://www.ibm.com/cloud/learn/monte-carlo-simulation\" rel=\"noopener noreferrer\" target=\"_blank\">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it's not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It's a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href=\"https://github.com/\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub</a> or <a href=\"https://www.mathworks.com/matlabcentral/\" rel=\"noopener noreferrer\" target=\"_blank\">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It's very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>",
  "pubDate": "Tue, 08 Feb 2022 14:00:01 +0000",
  "guid": "https://spectrum.ieee.org/ai-chip-design-matlab",
  "category": [
    "Chip fabrication",
    "Moore’s law",
    "Chip design",
    "Ai",
    "Matlab",
    "Digital twins"
  ],
  "dc:creator": "Rina Diane Caballar",
  "media:content": ""
}