{
  "title": "Book Review: Computer Age Statistical Inference",
  "link": "",
  "published": "2016-11-23T21:00:00+00:00",
  "updated": "2016-11-23T21:00:00+00:00",
  "author": {
    "name": "Sebastian Nowozin"
  },
  "id": "tag:www.nowozin.net,2016-11-23:/sebastian/blog/book-review-computer-age-statistical-inference.html",
  "summary": "<p><img alt=\"Book cover: Computer Age Statistical\nInference\" src=\"http://www.nowozin.net/sebastian/blog/images/computer-age-statistical-inference.jpg\"></p>\n<p>A new book, <a href=\"http://www.cambridge.org/us/academic/subjects/statistics-probability/statistical-theory-and-methods/computer-age-statistical-inference-algorithms-evidence-and-data-science\">Computer Age Statistical Inference: Algorithms, Evidence, and Data\nScience</a> by\n<a href=\"http://statweb.stanford.edu/~ckirby/brad/\">Bradley Efron</a> and\n<a href=\"http://web.stanford.edu/~hastie/\">Trevor Hastie</a>, was released in July this\nyear.  I finished reading it a few weeks ago and this is a short review from\nthe â€¦</p>",
  "content": "<p><img alt=\"Book cover: Computer Age Statistical\nInference\" src=\"http://www.nowozin.net/sebastian/blog/images/computer-age-statistical-inference.jpg\"></p>\n<p>A new book, <a href=\"http://www.cambridge.org/us/academic/subjects/statistics-probability/statistical-theory-and-methods/computer-age-statistical-inference-algorithms-evidence-and-data-science\">Computer Age Statistical Inference: Algorithms, Evidence, and Data\nScience</a> by\n<a href=\"http://statweb.stanford.edu/~ckirby/brad/\">Bradley Efron</a> and\n<a href=\"http://web.stanford.edu/~hastie/\">Trevor Hastie</a>, was released in July this\nyear.  I finished reading it a few weeks ago and this is a short review from\nthe point of view of a machine learning researcher.</p>\n<p>Living in Cambridge I indulge myself every once in a while by taking a break\nat the <a href=\"http://www.cambridge.org/about-us/visit-bookshop/history-bookshop\">Cambridge University Press\nbookstore</a>\nat the market square, located just opposite of King's College it is the oldest\nbook shop in England.\nBesides having an excellent collection of mathematics and computer science\nbooks, at the entrance of the shop they showcase new releases from Cambridge\nUniversity Press.\nMost of these new books fall outside my interest, but what a pleasure it was\nto discover a new bold book on the broad theme of statistics in the modern\nage, written by two experts in the field!\nI took a look at the table of contents and a minute later purchased the book.</p>\n<h1>Review</h1>\n<p>The book examines statistics broadly through three lenses.</p>\n<p><em>First</em>, it tells the history of the field of statistics, often with\ninteresting remarks about the prevalent views at the time a method was\ninvented.\n<em>Second</em>, correlated with the chronological order, the authors classify\nmethods by their use of computation.  Classic methods use few to none\ncomputation but often leverage asymptotic arguments.  Newer methods\nare increasingly realistic in their assumptions but rely on heavy use of\nmachine computation.\n<em>Third</em>, the flavour of the presented methods is interpreted as <em>Fisherian</em>,\n<em>frequentist</em>, or <em>Bayesian</em>.</p>\n<p>The terminology in the book is easily accessible to a person with basic\nstatistics training, perhaps with the exception of the word \"<em>Inference</em>\" in\nthe title.\nIn the book the authors use \"inference\" to describe the means by which\nlegitimacy of statistical results can be established.\nThis sense is different from the common use of the word in the machine\nlearning community, where it would usually refer in a broad sense to \"perform\ncomputation of consequences given a model and observations\".</p>\n<p>From a machine learner's perspective the most interesting parts of the book\nare the wide applicability of the empirical Bayes methodology, which is\ndemonstrated in a number of generally relevant applications including\n<em>large-scale testing</em> and <em>deconvolution</em>.</p>\n<p>Another benefit for someone with a machine learning background is the modern\nview on classic methods such as resampling methods (bootstrap and jackknife),\na readable motivation for topics and applications which are popular in\nstatistics but not popular in machine learning (survival analysis, large-scale\ntesting, confidence intervals, etc.), and the historical remarks and\nsubjective commentary on developments in the field.</p>\n<p>The subjective commentary in the <em>Epilogue</em> makes predictions about the field\nof statistics and data science as a whole, with the main trends being a\nbranching out into applications and an increased reliance on computation.</p>\n<h2>Criticism</h2>\n<p>The book is a wonderful book and many readers will enjoy reading it, as I did.\nThere are only two minor points where I feel the book could be improved.</p>\n<p><em>First</em>, while the authors readily acknowledge that many topics could have\nbeen added to the book, I feel that certain topics should have been included\ndue to their broad applicability and heavy use of computation in many\nsuccessful models:\nvariational Bayesian inference, approximate Bayesian computation (ABC),\nkernel methods more generally, and Bayesian nonparametrics.\nPerhaps variational inference and kernel methods have not reached the core\nstatistics community yet, but ABC and Bayesian nonparametrics originate with\nthem and are only possible because of the massive computation available today.</p>\n<p><em>Second</em>, in the description of solutions to statistical problems throughout\nthe book there is a strong emphasis on empirical Bayes and the bootstrap.</p>\n<h1>Summary</h1>\n<p>If you enjoy statistics, computation, or machine learning, get the book!\nThe breadth of topics and the independence between the chapters will make it\neasy for you to find something interesting.</p>\n<p><em>Acknowledgements.</em>  Thanks to Diana Gillooly for corrections.</p>",
  "category": ""
}