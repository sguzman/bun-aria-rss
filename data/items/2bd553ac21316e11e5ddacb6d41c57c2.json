{
  "title": "Keras plays catch, a single file Reinforcement Learning example",
  "link": "http://edersantana.github.io/articles/keras_rl/",
  "pubDate": "Wed, 16 Mar 2016 22:25:00 -0400",
  "guid": "http://edersantana.github.io/articles/keras_rl/",
  "author": "",
  "description": "<p>Get started with reinforcement learning in less that 200 lines of code with\nKeras (Theano or Tensorflow, it’s your choice).</p>\n<p><span class=\"more\"></span></p>\n<p><img src=\"/articles/keras_rl/catch.gif\" alt=\"Keras play catch\"></p>\n<p>So you are a (Supervised) Machine Learning practitioner that was also sold the\nhype of making your labels weaker and to the\npossibility of getting neural networks to play your favorite games. You want to\ndo Reinforcement Learning (RL), but you find it hard to read all those\nfull featured libraries just to get a feeling of what is actually going on.</p>\n<p>Here we’ve got your back: we took the game engine complexities out of the way and\nshow a minimal Reinforcement Learning example with less than 200 lines of code.\nAnd yes, the example does use Keras, your favorite deep learning library!</p>\n<p>Before I give you a link to the code make sure you read Nervana’s blog post \n<a href=\"http://www.nervanasys.com/demystifying-deep-reinforcement-learning/\">Demystifying Deep Reinforcement Learning</a>.\nThere you will learn about Q-learning, which is one of the many ways of doing\nRL. Also, at this point you already know that neural nets love mini-batches and there\nyou will see what Experience Replay is and how to use it to get you them\nbatches - even in problems where an agent only sees one sample of the environment state\nat a time.</p>\n<p>So here is the <a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093\">link for our code</a>.\nIn that code Keras plays the catch game, where it should\ncatch a single pixel “fruit” using a three pixel “basket”. The fruit falls one\npixel per step and the Keras network gets a reward of +1 if it catches the\nfruit and -1 otherwise. The networks see the entire <a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L34-L40\">10x10 pixels grid</a>\nas input and outputs <a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L122\">three values</a>,\neach value corresponds to an action (move left, stay,\nmove right). Since these values represent the expected accumulated future\nreward, we just go greedy and pick the\n<a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L147-L148\">action corresponding to the largest value</a>.</p>\n<p>One thing to note though, is that this network is not quite like you in exotic\nrestaurants, it doesn’t take the very same action exploiting what it already knows\nat every time, once in a while we force system to take a <a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L144-L145\">random action</a>.\nThis would be the equivalent of you learning that life is more than just Penang Curry with\nfried Tempeh by trial and error.</p>\n<p>In the link you will also find scripts that \nplays the game with no random actions and generates the pictures\nfor the animation above.</p>\n<p>Enjoy!</p>\n<h2 id=\"faq\">FAQ</h2>\n<p><strong>1) How does this Q-learning thing even work?</strong></p>\n<p>C’mon read the blog post I just mentioned above… Anyway, think like this: the\nfruit is almost hitting the ground and your model is just one pixel away from a\n“catching” position. The model will face similar cases many many times. If it decides to\nstay or move left, it will be punished (imagine it smelling a bunch of rotten fruits\nin the ground because it was lazy). Thus, it learns to assign a small Q-value\n(sounds much better than just “output of neural net”, han?) to those two actions\nwhenever it sees that picture as input. But, since catching the fruit also\ngives a juicy +1 reward, the model will learn to assign a larger Q-value to the\n“move right” action in that case. This is what minimizing the\n<a href=\"https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L98-L106\">reward - Q-value error</a> does.</p>\n<p>One step before that, there will be no reward in the next step.</p>\n<p>I liked how the previous phrase sounded, so I decided to give it its own\nparagraph. But, although in that case there is no juicy reward right after,\nthe model can be trained\nusing the maximum Q-value of the future state in the next step.\nThink about it. If you’re in the\nkitchen you know that you can just open the fridge to get food. But now you’re\nin your bedroom writing bad jokes and feel hungry. But you have this vague\nmemory that going to the kitchen could help with that. You just go to the kitchen \nand there you figure how to help yourself. You have to learn all that by living\nthe game. I know, being Markovian is hard!\nBut then the rest is just propagating these reward expectations further and\nfurther into the past, assigning high values for good choices and low values\nfor bad choices (don’t forget that sometimes you hit those random choices in\ncollege so you learn the parts of life they don’t talk about in school).\nFor everything else, if you believe in Stochastic Gradient Descent then it is\neasy to see this actually making sense… I hope…</p>\n<p><strong>2) How different is that from AlphaGo?</strong></p>\n<p>Not much… But instead of learning Q-values, AlphaGo thought it was\nsmarter to use REINFORCE and learn to output\nactions probabilities directly. After that, she played several\ngames against herself, so many that it could later learn the probability of\nwinning from each position. Using all that information, during play time she\nuses a search technique to look for possible actions that would take her to\npositions with higher probability of winning. But she told me to mention here\nthat she doesn’t search as many possibilities in the future as her older cousin\nDeepBlue did. She also said that she can play pretty well using just one GPU, the other\n99 were running high resolution Netflix series so she can catch up with human\nculture.</p>\n<p>That being said, you should be able to modify this script in 2 or 3 days to get\na reimplementation or AlphaGo and Skynet should be 4 weeks away?</p>\n<p>JK</p>\n<p><strong>3) Your code sucks why don’t you write something better?</strong></p>\n<p><a href=\"https://github.com/EderSantana/X/blob/master/examples/catcher.py\">I’m trying…</a></p>\n<p><strong>4) Did you learn that by yourself?</strong></p>\n<p>The bad parts, yes. The good things were taught to me by my friends\nEvan Kriminger and Matthew Emigh.</p>"
}