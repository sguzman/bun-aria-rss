{
  "title": "Neural Variational Inference: Importance Weighted Autoencoders",
  "link": "http://artem.sobolev.name/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html",
  "description": "<p>Previously we covered <a href=\"/posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html\">Variational Autoencoders</a> (VAE) — popular inference tool based on neural networks. In this post we’ll consider, a followup work from Torronto by Y. Burda, R. Grosse and R. Salakhutdinov, <a href=\"https://arxiv.org/abs/1509.00519\">Importance Weighted Autoencoders</a> (IWAE). The crucial contribution of this work is introduction of a new lower-bound on the marginal log-likelihood <span class=\"math inline\">\\(\\log p(x)\\)</span> which generalizes ELBO, but also allows one to use less accurate approximate posteriors <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span>.</p>\n<p>On a dessert we’ll discuss another paper, <a href=\"https://arxiv.org/abs/1602.06725\">Variational inference for Monte Carlo objectives</a> by A. Mnih and D. Rezende which aims to broaden the applicability of this approach to models where reparametrization trick can not be used (e.g. for discrete variables).</p>\n<!--more-->\n<h3>\nImportance Weighted Autoencoders\n</h3>\n<p>Let’s first answer the question of how one can come up with a lower bound for the marginal log-likelihood? In the very beginning of the series, <a href=\"/posts/2016-07-01-neural-variational-inference-classical-theory.html\">Classical Theory</a> post, we used some trickery to come up with the ELBO. That massaging of the marginal log-likelihood wasn’t particulary enlightening on how one could invent that lower bound. Now we’re going to consider a principled approach to invention of new lower bounds based on <a href=\"https://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen’s inequality</a>.</p>\n<p>Suppose we have some unbiased estimator <span class=\"math inline\">\\(f(x, z)\\)</span> of <span class=\"math inline\">\\(p(x)\\)</span>, that is, <span class=\"math inline\">\\(\\mathbb{E}_z f(x, z) = p(x)\\)</span>. Then</p>\n<p><span class=\"math display\">\\[\n\\log p(x) = \\log \\mathbb{E}_z f(x, z) \\stackrel{\\text{Jensen}}{\\ge} \\mathbb{E}_z \\log f(x, z)\n\\]</span></p>\n<p>In particular, if <span class=\"math inline\">\\(z \\sim q(z \\mid x)\\)</span> and <span class=\"math inline\">\\(f(x, z) = \\tfrac{p(x, z)}{q(z \\mid x)}\\)</span>, we obtain the standard ELBO. The IWAE paper proposes another estimate (actually, a family of estimators parametrized by an integer <span class=\"math inline\">\\(K\\)</span>) of marginal <span class=\"math inline\">\\(p(x)\\)</span>:</p>\n<p><span class=\"math display\">\\[\nf(x, z_1, \\dots, z_K) = \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x, z_k)}{q(z_k \\mid x)}\n\\]</span></p>\n<p>Where each <span class=\"math inline\">\\(z_k\\)</span> comes from the same distribution <span class=\"math inline\">\\(q(z_k \\mid x) = q(z \\mid x)\\)</span>. Obviously, <span class=\"math inline\">\\(f(x, z_1, \\dots, z_K)\\)</span> is still an unbiased estimator of the <span class=\"math inline\">\\(p(x)\\)</span>, and therefore <span class=\"math inline\">\\(\\mathbb{E}_z \\log f(x, z_1, \\dots, z_K)\\)</span> is a valid lower-bound on the marginal log-likelihood.</p>\n<p>Let’s analyze this new lower-bound now. First, let’s dissect the ELBO:</p>\n<p><span class=\"math display\">\\[\n\\mathcal{L}(\\Theta, \\Lambda)\n= \\mathbb{E}_q \\log \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)}\n= \\mathbb{E}_q \\left[\\log \\frac{p(z \\mid x, \\Theta)}{q(z \\mid x, \\Lambda)} \\right] + \\log p(x \\mid \\Theta)\n\\]</span></p>\n<p>If <span class=\"math inline\">\\(q\\)</span> approximates the true posterior accurately, the first term (which is a KL-divergence, BTW) is close to zero. However, when estmating it using Monte Carlo samples, the ELBO heavily penalizes inaccurate approximations: if <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span> gives us samples from high probability regions of the true posterior <span class=\"math inline\">\\(p(z \\mid x, \\Theta)\\)</span> only occasionally (like 20% of times), the gap between the ELBO and the marginal log-likelihood would be huge (<span class=\"math inline\">\\(p(z\\mid x, \\Theta)\\)</span> is small, <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span> is big), which does not help learning. As you might have guessed, IWAE allows us to use several samples. Let’s see it in detail:</p>\n<p><span class=\"math display\">\\[\n\\mathcal{L}_K(\\Theta, \\Lambda)\n:= \\mathbb{E}_q \\left[\\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x, z_k \\mid \\Theta)}{q(z_k \\mid x, \\Lambda)} \\right]\n:= \\mathbb{E}_q \\left[\\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p(z_k \\mid x, \\Theta)}{q(z_k \\mid x, \\Lambda)} \\right] + \\log p(x \\mid \\Theta)\n\\]</span></p>\n<p>This averaging of posterior ratios saves us from bad samples screwing the lower bound, as it’ll be pushed up by good samples (provided the approximation has a reasonable probability of generating a good sample in <span class=\"math inline\">\\(K\\)</span> attempts). This allows one to perform model inference even with poor approximations <span class=\"math inline\">\\(q(z \\mid x, \\Lambda)\\)</span>. The more samples <span class=\"math inline\">\\(K\\)</span> we use — the less accurate approximation we can tolerate. In fact, authors prove the following theorem:</p>\n<blockquote>\n<p><strong>Theorem 1</strong>. For all <span class=\"math inline\">\\(K\\)</span>, the lower bounds satisfy <span class=\"math display\">\\[\n\\log p(x \\mid \\Theta) \\ge \\mathcal{L}_{K+1}(\\Theta, \\Lambda) \\ge \\mathcal{L}_{K}(\\Theta, \\Lambda)\n\\]</span></p>\nMoreover, if <span class=\"math inline\">\\(p(z, x \\mid \\Theta) / q(z \\mid x, \\Lambda)\\)</span> is bounded, then <span class=\"math inline\">\\(\\mathcal{L}_{K}(\\Theta, \\Lambda)\\)</span> approaches <span class=\"math inline\">\\(\\log p(x \\mid \\Theta)\\)</span> as <span class=\"math inline\">\\(K\\)</span> goes to infinity.\n</blockquote>\n<p>The convergence result follows from the <a href=\"https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law\">strong law of large numbers</a>.</p>\n<p>As with VAE, we use the reparametrization trick to avoid backpropagation through stochastic units:</p>\n<p><span class=\"math display\">\\[\n\\mathcal{L}_K(\\Theta, \\Lambda) = \\mathbb{E}_{\\varepsilon_1, \\dots, \\varepsilon_K} \\log \\frac{1}{K} \\sum_{k=1}^K \\overbrace{\\frac{p(x, g(\\varepsilon_k; \\Lambda) \\mid \\Theta)}{q(g(\\varepsilon_k; \\Lambda) \\mid x, \\Lambda)}}^{w(x, \\varepsilon_k, \\Theta, \\Lambda)}\n\\]</span></p>\n<p>The gradients then are</p>\n<p><span class=\"math display\">\\[\n\\nabla_\\Theta \\mathcal{L}_K(\\Theta, \\Lambda) = \\mathbb{E}_{\\varepsilon_1, \\dots, \\varepsilon_K} \\sum_{k=1}^K \\hat w_k(x, \\varepsilon_{1 \\dots K}, \\Theta, \\Lambda) \\nabla_\\Theta \\log w(x, \\varepsilon_k, \\Theta, \\Lambda) \\\\\n\\nabla_\\Lambda \\mathcal{L}_K(\\Theta, \\Lambda) = \\mathbb{E}_{\\varepsilon_1, \\dots, \\varepsilon_K} \\sum_{k=1}^K \\hat w_k(x, \\varepsilon_{1 \\dots K}, \\Theta, \\Lambda) \\nabla_\\Lambda \\log w(x, \\varepsilon_k, \\Theta, \\Lambda) \\\\\n\\text{where } \\hat w_k(x, \\varepsilon_{1 \\dots K}, \\Theta, \\Lambda) := \\frac{w(x, \\varepsilon_k, \\Theta, \\Lambda)}{\\sum_{k=1}^K w(x, \\varepsilon_k, \\Theta, \\Lambda)}\n\\]</span></p>\n<p>(We used the identity <span class=\"math inline\">\\(\\nabla_x f(x) = f(x) \\nabla_x \\log f(x)\\)</span> here).</p>\n<p>Just as one would expect, setting <span class=\"math inline\">\\(K=1\\)</span> reduces these gradients to ones we’ve seen in VAEs as the only importance weight <span class=\"math inline\">\\(\\hat w_1\\)</span> is equal to 1. Unfortunatelly, this approach does not allow one to decompose the lower-bound into the reconstruction error and KL-divergence to analytically compute the later. However, authors report indistinguishable performance of 2 approaches (with KL computed analytically or estimated using Monte Carlo) in case of <span class=\"math inline\">\\(K=1\\)</span>.</p>\n<p>BTW, <a href=\"http://info.usherbrooke.ca/hlarochelle/index_en.html\">Hugo Larochelle</a> writes <a href=\"https://twitter.com/hugo_larochelle/timelines/639067398511968256\">notes</a> on different papers, and he has written and made publicly available <a href=\"https://www.evernote.com/shard/s189/sh/e2c8a133-1814-474c-b267-366600a1921b/06a756f1618cd47ababc7aae0e514dbf\">Notes on Importance Weighted Autoencoders</a>.</p>\n<h3>\nVariational inference for Monte Carlo objectives\n</h3>\n<p>As I said in the introduction, IWAE has been “generalized” to discrete variables — a case when one can not employ the reparametrization trick, and instead has to somehow reduce high variance of a score function-based estimator. Previously, during our discussion of the <a href=\"/posts/2016-07-05-neural-variational-inference-blackbox.html\">Blackbox VI and variance reduction techniques</a> we covered NVIL (Neural Variational Inference and Learning) estimator, which uses another neural network to estimate marginal likelihood and reduce the variance. This work is built upon a similar idea.</p>\n<p>First, let’s derive score-function-based gradients for variational parameters <span class=\"math inline\">\\(\\Lambda\\)</span> (where <span class=\"math inline\">\\(w\\)</span> now is defined as <span class=\"math inline\">\\(w(x, z, \\Theta, \\Lambda) = \\frac{p(x, z \\mid \\Theta)}{q(z \\mid x, \\Lambda)}\\)</span>, and <span class=\"math inline\">\\(\\hat w\\)</span> is a normalized across all samples <span class=\"math inline\">\\(z_{1\\dots K}\\)</span> version):</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\nabla_\\Lambda \\mathcal{L}_K(\\Theta, \\Lambda)\n&= \\nabla_\\Lambda \\mathbb{E}_{q(z_1, \\dots, z_K \\mid x, \\Lambda)} \\log \\frac{1}{K} \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda) \\\\\n&= \\nabla_\\Lambda \\int q(z_1, \\dots, z_K \\mid x, \\Lambda) \\log \\frac{1}{K} \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda) \\; dz_1 \\dots dz_K \\\\\n&= \\mathbb{E}_{q(z_1, \\dots, z_K \\mid x, \\Lambda)} \\left[ \\sum_{k=1}^K \\nabla_\\Lambda \\log q(z_k \\mid x, \\Lambda) \\log \\frac{1}{K} \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda) \\right] \\\\\n& \\quad+ \\mathbb{E}_{q(z_1, \\dots, z_K \\mid x, \\Lambda)} \\nabla_\\Lambda \\log \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda) \\\\\n&= \\mathbb{E}_{q(z_1, \\dots, z_K \\mid x, \\Lambda)} \\left[ \\sum_{k=1}^K \\nabla_\\Lambda \\log q(z_k \\mid x, \\Lambda) \\log \\frac{1}{K} \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda) \\right] \\\\\n& \\quad+ \\mathbb{E}_{q(z_1, \\dots, z_K \\mid x, \\Lambda)} \\left[ \\sum_{k=1}^K \\hat w_k(x, z_{1 \\dots K}, \\Theta, \\Lambda) \\nabla_\\Lambda \\log w(x, z_k, \\Theta, \\Lambda) \\right]\n\\end{align}\n\\]</span></p>\n<p>The second term is exactly the gradient of the reparametrized case, and it does not cause us any troubles. The first term, however has some issues.</p>\n<p>First, it does not distinguish individual samples’ contributions: indeed, gradients for all samples have the same weight of <span class=\"math inline\">\\(\\log \\tfrac{1}{K} \\sum_{k=1}^K w(x, z_k, \\Theta, \\Lambda)\\)</span> (called the <em>learning signal</em>) regardless of how probable they’re in terms of the true posterior (that is, how well they describe an observation <span class=\"math inline\">\\(x\\)</span>). Compare it with the second term, where gradient for each sample <span class=\"math inline\">\\(z_k\\)</span> is weighted in proportion to its importance weight <span class=\"math inline\">\\(\\hat w_k\\)</span>.</p>\n<p>Second problem is that the learning signal is unbounded, and can be quite high. Again, the second term does not suffer this as importance weights <span class=\"math inline\">\\(\\hat w_k\\)</span> are normalized to sum to 1.</p>\n<p>One can use the NVIL estimator we’ve discussed previously to reduce the variance due to large magnitude of a learning signal. However, it does not address the problem of all gradients having the same weight. For this the authors propose to introduce per-sample baselines that minimize dependencies between samples.</p>\n<p>This paper has also caught Dr. Larochelle’s attention: <a href=\"https://www.evernote.com/shard/s189/sh/54a9fb88-1a71-4e8a-b0e3-f13480a68b8d/0663de49b93d397f519c7d7f73b6a441\">Notes on Variational inference for Monte Carlo objectives</a>.</p>",
  "pubDate": "Thu, 14 Jul 2016 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html",
  "dc:creator": "Artem"
}