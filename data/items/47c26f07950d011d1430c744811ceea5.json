{
  "title": "Linear Discriminant Analysis for Starters",
  "link": "",
  "id": "https://www.georgeho.org/lda/",
  "updated": "2017-12-30T00:00:00Z",
  "published": "2017-12-30T00:00:00Z",
  "content": "<p><em>Linear discriminant analysis</em> (commonly abbreviated to LDA, and not to be\nconfused with <a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">the other\nLDA</a>) is a very\ncommon dimensionality reduction technique for classification problems. However,\nthat&rsquo;s something of an understatement: it does so much more than “just”\ndimensionality reduction.</p>\n<p>In plain English, if you have high-dimensional data (i.e. a large number of\nfeatures) from which you wish to classify observations, LDA will help you\ntransform your data so as to make the classes as distinct as possible. More\nrigorously, LDA will find the linear projection of your data into a\nlower-dimensional subspace that optimizes some measure of class separation. The\ndimension of this subspace is necessarily strictly less than the number of\nclasses.</p>\n<p>This separation-maximizing property of LDA makes it so good at its job that it&rsquo;s\nsometimes considered a classification algorithm in and of itself, which leads to\nsome confusion. <em>Linear discriminant analysis</em> is a form of dimensionality\nreduction, but with a few extra assumptions, it can be turned into a classifier.\n(Avoiding these assumptions gives its relative, <em>quadratic discriminant\nanalysis</em>, but more on that later). Somewhat confusingly, some authors call the\ndimensionality reduction technique “discriminant analysis”, and only prepend the\n“linear” once we begin classifying. I actually like this naming convention more\n(it tracks the mathematical assumptions a bit better, I think), but most people\nnowadays call the entire technique “LDA”, so that&rsquo;s what I&rsquo;ll call it.</p>\n<p>The goal of this post is to give a comprehensive introduction to, and\nexplanation of, LDA. I&rsquo;ll look at LDA in three ways:</p>\n<ol>\n<li>LDA as an algorithm: what does it do, and how does it do it?</li>\n<li>LDA as a theorem: a mathematical derivation of LDA</li>\n<li>LDA as a machine learning technique: practical considerations when using LDA</li>\n</ol>\n<p>This is a lot for one post, but my hope is that there&rsquo;s something in here for\neveryone.</p>\n<div>\n<h2>Contents</h2>\n<nav id=\"TableOfContents\">\n<ul>\n<li><a href=\"#lda-as-an-algorithm\">LDA as an Algorithm</a>\n<ul>\n<li><a href=\"#problem-statement\">Problem statement</a></li>\n<li><a href=\"#solution\">Solution</a></li>\n</ul>\n</li>\n<li><a href=\"#lda-as-a-theorem\">LDA as a Theorem</a></li>\n<li><a href=\"#lda-as-a-machine-learning-technique\">LDA as a Machine Learning Technique</a>\n<ul>\n<li><a href=\"#regularization-aka-shrinkage\">Regularization (a.k.a. shrinkage)</a></li>\n<li><a href=\"#lda-as-a-classifier\">LDA as a classifier</a></li>\n<li><a href=\"#close-relatives-pca-qda-anova\">Close relatives: PCA, QDA, ANOVA</a></li>\n</ul>\n</li>\n</ul>\n</nav>\n</div>\n<h2 id=\"lda-as-an-algorithm\">LDA as an Algorithm</h2>\n<h3 id=\"problem-statement\">Problem statement</h3>\n<p>Before we dive into LDA, it&rsquo;s good to get an intuitive grasp of what LDA\ntries to accomplish.</p>\n<p>Suppose that:</p>\n<ol>\n<li>You have very high-dimensional data, and that</li>\n<li>You are dealing with a classification problem</li>\n</ol>\n<p>This could mean that the number of features is greater than the number of\nobservations, or it could mean that you suspect there are noisy features that\ncontain little information, or anything in between.</p>\n<p>Given that this is the problem at hand, you wish to accomplish two things:</p>\n<ol>\n<li>Reduce the number of features (i.e. reduce the dimensionality of your feature\nspace), and</li>\n<li>Preserve (or even increase!) the “distinguishability” of your classes or the\n“separatedness” of the classes in your feature space.</li>\n</ol>\n<p>This is the problem that LDA attempts to solve. It should be fairly obvious why\nthis problem might be worth solving.</p>\n<p>To judiciously appropriate a term from signal processing, we are interested in\nincreasing the signal-to-noise ratio of our data, by both extracting or\nsynthesizing features that are useful in classifying our data (amplifying our\nsignal), and throwing out the features that are not as useful (attenuating our\nnoise).</p>\n<p>Below is simple illustration I made, inspired by <a href=\"https://www.quora.com/Can-you-explain-the-comparison-between-principal-component-analysis-and-linear-discriminant-analysis-in-dimensionality-reduction-with-MATLAB-code-Which-one-is-more-efficient\">Sebastian\nRaschka</a>\nthat may help our intuition about the problem:</p>\n<p><img src=\"https://www.georgeho.org/assets/images/lda-pic.png\" alt=\"Projections of two-dimensional data (in two clusters) onto the x and y axes\"></p>\n<p>A couple of points to make:</p>\n<ul>\n<li>LD1 and LD2 are among the projections that LDA would consider. In reality, LDA\nwould consider <em>all possible</em> projections, not just those along the x and y\naxes.</li>\n<li>LD1 is the one that LDA would actually come up with: this projection gives the\nbest “separation” of the two classes.</li>\n<li>LD2 is a horrible projection by this metric: both classes get horribly\noverlapped… (this actually relates to PCA, but more on that later)</li>\n</ul>\n<p><strong>UPDATE:</strong> For another illustration, Rahul Sangole made a simple but great\ninteractive visualization of LDA\n<a href=\"https://rsangole.shinyapps.io/LDA_Visual/\">here</a> using\n<a href=\"https://shiny.rstudio.com/\">Shiny</a>.</p>\n<h3 id=\"solution\">Solution</h3>\n<p>First, some definitions:</p>\n<p>Let:</p>\n<ul>\n<li>$n$ be the number of classes</li>\n<li>$\\mu$ be the mean of all observations</li>\n<li>$N_i$ be the number of observations in the $i$th class</li>\n<li>$\\mu_i$ be the mean of the $i$th class</li>\n<li>$\\Sigma_i$ be the <a href=\"https://en.wikipedia.org/wiki/Scatter_matrix\">scatter\nmatrix</a> of the $i$th class</li>\n</ul>\n<p>Now, define $S_W$ to be the <em>within-class scatter matrix</em>, given by</p>\n<p>$$\n\\begin{align*}\nS_W = \\sum_{i=1}^{n}{\\Sigma_i}\n\\end{align*}\n$$</p>\n<p>and define $S_B$ to be the <em>between-class scatter matrix</em>, given by</p>\n<p>$$\n\\begin{align*}\nS_B = \\sum_{i=1}^{n}{N_i (\\mu_i - \\mu) (\\mu_i - \\mu)^T}\n\\end{align*}\n$$</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Diagonalizable_matrix\">Diagonalize</a> $S_W^{-1}\nS_B$ to get its eigenvalues and eigenvectors.</p>\n<p>Pick the $k$ largest eigenvalues, and their associated eigenvectors. We will\nproject our observations onto the subspace spanned by these vectors.</p>\n<p>Concretely, what this means is that we form the matrix $A$, whose columns are the\n$k$ eigenvectors chosen above. $W$ will allow us to transform our\nobservations into the new subspace via the equation $y = A^T x$, where $y$ is\nour transformed observation, and $x$ is our original observation.</p>\n<p>And that&rsquo;s it!</p>\n<p>For a more detailed and intuitive explanation of the LDA “recipe”, see\n<a href=\"http://sebastianraschka.com/Articles/2014_python_lda.html\">Sebastian Raschka&rsquo;s blog post on\nLDA</a>.</p>\n<h2 id=\"lda-as-a-theorem\">LDA as a Theorem</h2>\n<p><strong>Sketch of Derivation:</strong></p>\n<p>In order to maximize class separability, we need some way of measuring it as a\nnumber. This number should be bigger when the between-class scatter is bigger,\nand smaller when the within-class scatter is larger. There are many such\nformulas/numbers that have this property: <a href=\"https://www.elsevier.com/books/introduction-to-statistical-pattern-recognition/fukunaga/978-0-08-047865-4\">Fukunaga&rsquo;s <em>Introduction to\nStatistical Pattern\nRecognition</em></a>\nconsiders no less than four! Here, we&rsquo;ll concern ourselves with just one:</p>\n<p>$$ J_1 = tr(S_{WY}^{-1} S_{BY}) $$</p>\n<p>where I denote the within and between-class scatter matrices of the projection\nvector $Y$ by $S_{WY}$ and $S_{BY}$, to avoid confusion with the\ncorresponding matrices for the projected vector $X$.</p>\n<p>Now, a standard result from probability is that for any random variable $X$\nand matrix $A$, we have $cov(A^T X) = A^T cov(X) A$. We&rsquo;ll apply this\nresult to our projection $y = A^T x$. It follows that</p>\n<p>$$ S_{WY} = A^T S_{WX} A $$</p>\n<p>and</p>\n<p>$$ S_{BY} = A^T S_{BX} A $$</p>\n<p>where $S_{BX}$ and $S_{BY}$ are the between-class scatter matrices, and\n$S_{WX}$ and $S_{WY}$ are the within-class scatter matrices, for $X$\nand its projection $Y$, respectively.</p>\n<p>It&rsquo;s now a simple matter to write $J_1$ in terms of $A$, and maximize\n$J_1$. Without going into the details, we set $\\frac{\\partial J_1}{\\partial\nA} = 0$ (whatever that means), and use the fact that <a href=\"https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues\">the trace of a matrix is\nthe sum of its\neigenvalues</a>.</p>\n<p>I don&rsquo;t want to go into the weeds with this here, but if you really want to see\nthe algebra, Fukunaga is a great resource. The end result, however, is the same\ncondition on the eigenvalues and eigenvectors as stated above: in other words,\nthe optimization gives us LDA as presented.</p>\n<p>There&rsquo;s one more quirk of LDA that&rsquo;s very much worth knowing. Suppose you have\n10 classes, and you run LDA. It turns out that the <em>maximum</em> number of features\nLDA can give you is one less than the number of class, so in this case, 9!</p>\n<p><strong>Proposition:</strong> $S_W^{-1} S_B$ has at most $n-1$ non-zero eigenvalues, which\nimplies that LDA is must reduce the dimension to <em>at least</em> $n-1$.</p>\n<p>To prove this, we first need a lemma.</p>\n<p><strong>Lemma:</strong> Suppose ${v_i}<em>{i=1}^{n}$ is a set of linearly dependent vectors, and\nlet $\\alpha_i$ be $n$ coefficients. Then, $M = \\sum</em>{i=1}^{n}{\\alpha_i v_i\nv_i^{T}}$, a linear combination of outer products of the vectors with\nthemselves, is rank deficient.</p>\n<p><strong>Proof:</strong> The row space of $M$ is generated by the set of vectors ${v_1, v_2,\n&hellip;, v_n}$. However, because this set of vectors is linearly dependent, it must\nspan a vector space of dimension strictly less than $n$, or in other words\nless than or equal to $n-1$. But the dimension of the row space is precisely\nthe rank of the matrix $M$. Thus, $rank(M) \\leq n-1$, as desired.</p>\n<p>With the lemma, we&rsquo;re now ready to prove our proposition.</p>\n<p><strong>Proof:</strong> We have that</p>\n<p>$$\n\\begin{align*}\n\\frac{1}{n} \\sum_{i=1}^{n}{\\mu_i} = \\mu \\implies \\sum_{i=1}^{n}{\\mu_i-\\mu} = 0\n\\end{align*}\n$$</p>\n<p>So ${\\mu_i-\\mu}_{i=1}^{n}$ is a linearly dependent set. Applying our lemma, we\nsee that</p>\n<p>$$ S_B = \\sum_{i=1}^{n}{N_i (\\mu_i-\\mu)(\\mu_i-\\mu)^{T}} $$</p>\n<p>must be rank deficient. Thus, $rank(S_W) \\leq n-1$. Now, $rank(AB) \\leq\nrank(A)rank(B)$, so</p>\n<p>$$\n\\begin{align*}\nrank(S_W^{-1}S_B) \\leq \\min{(rank(S_W^{-1}), rank(S_B))} = n-1\n\\end{align*}\n$$</p>\n<p>as desired.</p>\n<h2 id=\"lda-as-a-machine-learning-technique\">LDA as a Machine Learning Technique</h2>\n<p>OK so we&rsquo;re done with the math, but how is LDA actually used in practice? One of\nthe easiest ways is to look at how LDA is actually implemented in the real\nworld. <code>scikit-learn</code> has <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis\">a very well-documented implementation of\nLDA</a>:\nI find that reading the docs is a great way to learn stuff.</p>\n<p>Below are a few miscellaneous comments on practical considerations when using\nLDA.</p>\n<h3 id=\"regularization-aka-shrinkage\">Regularization (a.k.a. shrinkage)</h3>\n<p><code>scikit-learn</code>&rsquo;s implementation of LDA has an interesting optional parameter:\n<code>shrinkage</code>. What&rsquo;s that about?</p>\n<p><a href=\"https://stats.stackexchange.com/questions/106121/does-it-make-sense-to-combine-pca-and-lda/109810#109810\">Here&rsquo;s a wonderful Cross Validated\npost</a>\non how LDA can introduce overfitting. In essence, matrix inversion is an\nextremely sensitive operation (in that small changes in the matrix may lead to\nlarge changes in its inverse, so that even a tiny bit of noise will be amplified\nupon inverting the matrix), and so unless the estimate of the within-class\nscatter matrix $S_W$ is very good, its inversion is likely to introduce\noverfitting.</p>\n<p>One way to combat that is through regularizing LDA. It basically replaces\n$S_W$ with $(1-t)S_W + tI$, where $I$ is the identity matrix, and $t$ is\nthe <em>regularization parameter</em>, or the <em>shrinkage constant</em>. That&rsquo;s what\n<code>scikit</code>&rsquo;s <code>shrinkage</code> parameter is: it&rsquo;s $t$.</p>\n<p>If you&rsquo;re interested in <em>why</em> this linear combination of the within-class\nscatter and the identity give such a well-conditioned estimate of $S_W$, check\nout <a href=\"https://www.semanticscholar.org/paper/A-well-conditioned-estimator-for-large-dimensional-Ledoit-Wolf/23d8219db1aff006b41007effc696fca6fbcabcf\">the original paper by Ledoit and\nWolf</a>.\nTheir original motivation was in financial portfolio optimization, so they&rsquo;ve\nalso authored several other papers\n(<a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=433840&amp;rec=1&amp;srcabs=290916&amp;alg=7&amp;pos=6\">here</a>\nand\n<a href=\"https://www.semanticscholar.org/paper/A-well-conditioned-estimator-for-large-dimensional-Ledoit-Wolf/23d8219db1aff006b41007effc696fca6fbcabcf\">here</a>)\nthat go into the more financial details. That needn&rsquo;t concern us though:\ncovariance matrices are literally everywhere.</p>\n<p>For an illustration of this, <code>amoeba</code>&rsquo;s post on Cross Validated gives a good\nexample of LDA overfitting, and how regularization can help combat that.</p>\n<h3 id=\"lda-as-a-classifier\">LDA as a classifier</h3>\n<p>We&rsquo;ve talked a lot about how LDA is a dimensionality reduction technique. But in\naddition to it, you can make two extra assumptions, and LDA becomes a very\nrobust classifier as well! Here they are:</p>\n<ol>\n<li>Assume that the class conditional distributions are Gaussian, and</li>\n<li>Assume that these Gaussians have the same covariance matrix (a.k.a.\nassume <a href=\"https://en.wikipedia.org/wiki/Homoscedasticity\">homoskedasticity</a>)</li>\n</ol>\n<p>Now, <em>how</em> LDA acts as a classifier is a bit complicated: the problem is solved\nfairly easily if there are only two classes. In this case, the optimal Bayesian\nsolution is to classify the observation depending on whether the log of the\nlikelihood ratio is less than or greater than some threshold. This turns out to\nbe a simple dot product: $\\vec{w} \\cdot \\vec{x} &gt; c$, where $\\vec{w} =\n\\Sigma^{-1} (\\vec{\\mu_1} - \\vec{\\mu_2})$. <a href=\"https://en.wikipedia.org/wiki/Linear_discriminant_analysis#LDA_for_two_classes\">Wikipedia has a good derivation of\nthis</a>.</p>\n<p>There isn&rsquo;t really a nice dot-product solution for the multiclass case. So,\nwhat&rsquo;s commonly done is to take a “one-against-the-rest” approach, in which\nthere are $k$ binary classifiers, one for each of the $k$ classes. Another\ncommon technique is to take a pairwise approach, in which there are $k(k-1)/2$\nclassifiers, one for each pair of classes. In either case, the outputs of all\nthe classifiers are combined in some way to give the final classification.</p>\n<h3 id=\"close-relatives-pca-qda-anova\">Close relatives: PCA, QDA, ANOVA</h3>\n<p>LDA is similar to a lot of other techniques, and the fact that they all go by\nacronyms doesn&rsquo;t do anyone a favor. My goal here isn&rsquo;t to introduce or explain\nthese various techniques, but rather point out their differences.</p>\n<p><em>1) Principal components analysis (PCA):</em></p>\n<p>LDA is very similar to <a href=\"http://setosa.io/ev/principal-component-analysis\">PCA</a>:\nin fact, the question posted in the Cross Validated post above was actually\nabout whether or not it would make sense to perform PCA followed by LDA.</p>\n<p>There is a crucial difference between the two techniques, though. PCA tries to\nfind the axes with <em>maximum variance</em> for the whole data set, whereas LDA tries\nto find the axes for best <em>class separability</em>.</p>\n<p><img src=\"https://www.georgeho.org/assets/images/lda-pic.png\" alt=\"Projections of two-dimensional data (in two clusters) onto the x and y axes\"></p>\n<p>Going back to the illustration from before (reproduced above), it&rsquo;s not hard to\nsee that PCA would give us LD2, whereas LDA would give us LD1. This makes the\nmain difference between PCA and LDA painfully obvious: just because a feature\nhas a high variance, doesn&rsquo;t mean that it&rsquo;s predictive of the classes!</p>\n<p><em>2) Quadratic discriminant analysis (QDA):</em></p>\n<p>QDA is a generalization of LDA as a classifer. As mentioned above, LDA must\nassume that the class contidtional distributions are Gaussian with the same\ncovariance matrix, if we want it to do any classification for us.</p>\n<p>QDA doesn&rsquo;t make this homoskedasticity assumption (assumption number 2 above),\nand attempts to estimate the covariance of all classes. While this might seem\nlike a more robust algorithm (fewer assumptions! Occam&rsquo;s razor!), this means\nthere is a much larger number of parameters to estimate. In fact, the number of\nparameters grows quadratically with the number of classes! So unless you can\nguarantee that your covariance estimates are reliable, you might not want to use\nQDA.</p>\n<p>After all of this, there might be some confusion about the relationship between\nLDA, QDA, what&rsquo;s for dimensionality reduction, what&rsquo;s for classification, etc.\n<a href=\"https://stats.stackexchange.com/questions/71489/three-versions-of-discriminant-analysis-differences-and-how-to-use-them/71571#71571\">This CrossValidated\npost</a>\nand everything that it links to, might help clear things up.</p>\n<p><em>3) Analysis of variance (ANOVA):</em></p>\n<p>LDA and <a href=\"https://en.wikipedia.org/wiki/Analysis_of_variance\">ANOVA</a> seem to have\nsimilar aims: both try to “decompose” an observed variable into several\nexplanatory/discriminatory variables. However, there is an important difference\nthat <a href=\"https://en.wikipedia.org/wiki/Linear_discriminant_analysis\">the Wikipedia article on\nLDA</a> puts very\nsuccinctly (my emphases):</p>\n<blockquote>\n<p>LDA is closely related to analysis of variance (ANOVA) and regression\nanalysis, which also attempt to express one dependent variable as a linear\ncombination of other features or measurements. However, ANOVA uses\n<strong>categorical</strong> independent variables and a <strong>continuous</strong> dependent variable,\nwhereas discriminant analysis has <strong>continuous</strong> independent variables and a\n<strong>categorical</strong> dependent variable (i.e. the class label).</p>\n</blockquote>"
}