{
  "title": "Deep and Hierarchical Implicit Models",
  "link": "http://dustintran.com/blog/deep-and-hierarchical-implicit-models",
  "guid": "http://dustintran.com/blog/deep-and-hierarchical-implicit-models",
  "description": "<p>I’m excited to announce a paper that Rajesh Ranganath, Dave Blei, and\nI released today on arXiv, titled\n<a href=\"https://arxiv.org/abs/1702.08896\">Deep and Hierarchical Implicit Models</a>.</p>\n\n<p>Implicit probabilistic models are all about sampling as a primitive:\nthey define a process to simulate data and do not require tractable\ndensities\n(<a href=\"#diggle1984monte\">Diggle &amp; Gratton (1984)</a>,\n<a href=\"#hartig2011statistical\">Hartig, Calabrese, Reineking, Wiegand, &amp; Huth (2011)</a>)\n. We leverage this fundamental idea to develop new classes of\nmodels: they encompass simulators in the scientific communities,\ngenerative adversarial networks\n<a href=\"#goodfellow2014generative\">(Goodfellow et al., 2014)</a>,\nand deep generative models such as sigmoid\nbelief nets\n<a href=\"#neal1990learning\">(Neal, 1990)</a>\nand deep latent Gaussian models\n(<a href=\"#rezende2014stochastic\">Rezende, Mohamed, &amp; Wierstra (2014)</a>,\n<a href=\"#kingma2014autoencoding\">Kingma &amp; Welling (2014)</a>).\nThese modeling developments could not really be done without\ninference, and we develop a variational inference algorithm that\nunderpins them all.</p>\n\n<p>Biased as I am, I think this is quite a dense paper—chock full of\nsimple ideas that are rife with deep implications. There are many\nnuggets of wisdom that I could ramble on about, and I just might in\nseparate blog posts.</p>\n\n<p>As a practical example, we show how you can take any standard neural\nnetwork and turn it into a deep implicit model: simply inject noise\ninto the hidden layers. The hidden units in these layers are now\ninterpreted as latent variables. Further, the induced latent variables\nare astonishingly flexible, going beyond Gaussians (or exponential\nfamilies\n<a href=\"#ranganath2015deep\">(Ranganath, Tang, Charlin, &amp; Blei, 2015)</a>)\nto arbitrary probability distributions. Deep generative modeling could\nnot be any simpler!</p>\n\n<p>Here’s a 2-layer deep implicit model in <a href=\"http://edwardlib.org\">Edward</a>.\nIt defines the generative process,</p>\n\n<script type=\"math/tex; mode=display\">\\begin{aligned}\n\\mathbf{z}_{n,2} = g_2(\\mathbf{\\epsilon}_{n,2}),\\qquad\n\\mathbf{\\epsilon}_{n, 2} \\sim \\text{Normal}(0, 1), \\\\\n\\mathbf{z}_{n,1} = g_1(\\mathbf{\\epsilon}_{n,1}\\mid\\mathbf{z}_{n,2}),\\qquad\n\\mathbf{\\epsilon}_{n, 1} \\sim \\text{Normal}(0, 1), \\\\\n\\mathbf{x}_{n} = g_0(\\mathbf{\\epsilon}_{n,0}\\mid\\mathbf{z}_{n,1}),\\qquad\n\\mathbf{\\epsilon}_{n, 0} \\sim \\text{Normal}(0, 1).\n\\end{aligned}</script>\n\n<p>This generates layers of latent variables <script type=\"math/tex\">\\mathbf{z}_{n,1}</script>, <script type=\"math/tex\">\\mathbf{z}_{n,2}</script> and data <script type=\"math/tex\">\\mathbf{x}_{n}</script> via functions of noise <script type=\"math/tex\">\\mathbf{\\epsilon}</script>.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">edward.models</span> <span class=\"kn\">import</span> <span class=\"n\">Normal</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span>\n\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">55000</span>  <span class=\"c\"># number of data points</span>\n<span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>  <span class=\"c\"># noise dimensionality</span>\n\n<span class=\"c\"># random noise is Normal(0, 1)</span>\n<span class=\"n\">eps2</span> <span class=\"o\">=</span> <span class=\"n\">Normal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]))</span>\n<span class=\"n\">eps1</span> <span class=\"o\">=</span> <span class=\"n\">Normal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]))</span>\n<span class=\"n\">eps0</span> <span class=\"o\">=</span> <span class=\"n\">Normal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">([</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">d</span><span class=\"p\">]))</span>\n\n<span class=\"c\"># alternate latent layers z with hidden layers h</span>\n<span class=\"n\">z2</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">)(</span><span class=\"n\">eps2</span><span class=\"p\">)</span>\n<span class=\"n\">h2</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">)(</span><span class=\"n\">z2</span><span class=\"p\">)</span>\n<span class=\"n\">z1</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">)(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">([</span><span class=\"n\">eps1</span><span class=\"p\">,</span> <span class=\"n\">h2</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">h1</span> <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">)(</span><span class=\"n\">z1</span><span class=\"p\">)</span>\n<span class=\"n\">x</span>  <span class=\"o\">=</span> <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">)(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">([</span><span class=\"n\">eps0</span><span class=\"p\">,</span> <span class=\"n\">h1</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<p>The model uses Keras, where <code class=\"highlighter-rouge\">Dense(256)(x)</code> denotes a fully connected\nlayer with <script type=\"math/tex\">256</script> hidden units applied to input <code class=\"highlighter-rouge\">x</code>. To define a\nstochastic layer, we concatenate noise with the previous layer. The\nmodel alternates between stochastic and deterministic layers to\ngenerate data points <script type=\"math/tex\">\\mathbf{x}_n\\in\\mathbb{R}^{10}</script>.</p>\n\n<p>Check out the paper for how you can work with, or even interpret, such a model.</p>\n\n<p>EDIT (2017/03/02): The algorithm is now <a href=\"https://github.com/blei-lab/edward/pull/491\">merged into Edward</a>.</p>\n\n<h2 id=\"references\">References</h2>\n\n<ol class=\"bibliography\"><li><span id=\"diggle1984monte\">Diggle, P. J., &amp; Gratton, R. J. (1984). Monte Carlo methods of inference for implicit statistical models. <i>Journal of the Royal Statistical Society Series B</i>.</span></li>\n<li><span id=\"goodfellow2014generative\">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In <i>Neural Information Processing Systems</i>.</span></li>\n<li><span id=\"hartig2011statistical\">Hartig, F., Calabrese, J. M., Reineking, B., Wiegand, T., &amp; Huth, A. (2011). Statistical inference for stochastic simulation models - theory and application. <i>Ecology Letters</i>, <i>14</i>(8), 816–827.</span></li>\n<li><span id=\"kingma2014autoencoding\">Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. In <i>International Conference on Learning Representations</i>.</span></li>\n<li><span id=\"neal1990learning\">Neal, R. M. (1990). <i>Learning Stochastic Feedforward Networks</i>.</span></li>\n<li><span id=\"ranganath2015deep\">Ranganath, R., Tang, L., Charlin, L., &amp; Blei, D. M. (2015). Deep Exponential Families. In <i>Artificial Intelligence and Statistics</i>.</span></li>\n<li><span id=\"rezende2014stochastic\">Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In <i>International Conference on Machine Learning</i>.</span></li></ol>",
  "pubDate": "Tue, 28 Feb 2017 00:00:00 -0800"
}