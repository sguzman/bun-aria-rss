{
  "id": "tag:blogger.com,1999:blog-15418143.post-2934467168970752428",
  "published": "2019-11-19T05:18:00.001-05:00",
  "updated": "2019-11-19T22:13:06.249-05:00",
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "title": "Computer Vision and Visual SLAM vs. AI Agents",
  "content": "<div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">With all the recent advancements in end-to-end deep learning, it is now possible to train AI agents to perform many different tasks (some in simulation and some in the real-world). End-to-end learning allows one to replace a multi-component, hand-engineered system with a single learning network that can process raw sensor data and output actions for the AI to take in the physical world. I will discuss the implications of these ideas while highlighting some new research trends regarding Deep Learning for Visual SLAM and conclude with some predictions regarding the kinds of spatial reasoning algorithms that we will need in the future.&nbsp;</span><br /><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-cM2rvEvC2hc/XdO8N33q7JI/AAAAAAAAQys/LBF93qVtv3wHe_wt7yY65h55KHUmpnBiACLcBGAsYHQ/s1600/computer-vision-vs-ai-agents-cover.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"800\" height=\"225\" src=\"https://1.bp.blogspot.com/-cM2rvEvC2hc/XdO8N33q7JI/AAAAAAAAQys/LBF93qVtv3wHe_wt7yY65h55KHUmpnBiACLcBGAsYHQ/s400/computer-vision-vs-ai-agents-cover.png\" width=\"400\" /></a></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">In today's article, we will go over three ideas:</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;I.) Does Computer Vision Matter for Action?</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;II.) Visual SLAM for AI agents</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;III.) Quō vādis Visual SLAM? Trends and research forecast</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-size: large;\">I. Does Computer Vision Matter for Action?</span></strong></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">At last month's International Conference of Computer Vision (ICCV 2019), I heard the following thought-provoking question,</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><blockquote class=\"tr_bq\"><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">\"What do Artificial Intelligence Agents need (if anything) from the field of Computer Vision?\"</em></blockquote></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">The question was posed by <a href=\"http://vladlen.info/\">Vladlen Koltun</a>&nbsp;(from Intel Research) during his talk at the Deep Learning for Visual SLAM Workshop at ICCV 2019 in Seoul. He spoke about building AI agents with and without the aid of computer vision to guide representation learning. While Koltun has worked on classical Visual SLAM (see his <a href=\"http://vladlen.info/publications/direct-sparse-odometry/\">Direct Sparse Odometry (DSO) system</a> [2]), at this workshop, he decided to not speak about his older work on geometry, alignment, or 3D point cloud processing. His talk included numerous ideas spanning several of his team's research papers, some humor (see video), and plenty of Koltun's philosophical views towards general artificial intelligence.&nbsp;</span><br /><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Recent techniques show that it is possible to learn actions (the output quantities that we really want) from pixels (raw inputs) directly without any intermediate computer vision processing like object recognition, depth estimation, and segmentation. But just because it is possible to solve some AI tasks without intermediate representations (i.e., the computer vision stuff), does that mean that we should abandon computer vision research and let end-to-end learning take care of everything?&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Probably not.</em></strong></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">From a very practical standpoint, let's ask the following question:&nbsp;</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><blockquote class=\"tr_bq\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">\"Is an agent who is aware of computer vision stuff more robust than an agent trained without intermediate representations?\"&nbsp;</span></blockquote></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Recent research from Koltun's lab [3] indicates that the answer is&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">yes</em></strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">: training with intermediate representations, as done by supervision from per-frame computer vision tasks, gives rise to more robust agents that learn faster and are more robust in a variety of performance tasks! The next natural question is: which computer vision tasks matter most for agent robustness? Koltun's research suggests that depth estimation is one particular task that works well as an auxiliary task when training agents that have to move through space (i.e., most video games). A depth estimation network should help an AI agent navigate an unknown environment as depth estimation is one key component in many of today's RGBD Visual SLAM systems. The best way to learn about Koltun's paper, titled <a href=\"http://vladlen.info/publications/computer-vision-matter-action/\">Does Computer Vision Matter for Action?</a>, is to see the video on YouTube.</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><div style=\"text-align: center;\"><br /></div></div><div style=\"text-align: center;\"></div><div style=\"text-align: center;\"><iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/4MfWa2yZ0Jc\" width=\"560\"></iframe></div><div style=\"text-align: center;\"><b>Video describing Koltun's Does Computer Vision Matter for Action? [3]</b></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Let's imagine that you want to deploy a robot into the world sometimes from now until 2025 based on your large-scale AI agent training, and you're debating whether you should avoid intermediate representations or not.&nbsp;</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Intermediate representations facilitate explainability, debuggability, and testing. Explainability is a key to success when systems require spatial reasoning capabilities in the real-world. If your agents are misbehaving, take a look at their intermediate representations. If you want to improve your AI, you can analyze the computer vision systems to prioritize better your data collection effort. Visualization should be a first-order citizen in your deep learning toolbox.</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">But today's computer vision ecosystem offers more than algorithms that process individual images. Visual SLAM systems rapidly process images while updating the camera's trajectory and updating the 3D map of the world. Visual SLAM, or VSLAM, algorithms are the real-time variants of Structure-from-Motion (SfM), which has been around for a while. SfM uses bundle adjustment -- a minimization of reprojection error, usually solved with Levenberg Marquardt. If there any kind of robot you see moving around today (2019), it is likely that it is running some variant of SLAM (localization and mapping) and not an end-to-end trained network -- at least not today. So what does Visual SLAM mean for AI agents?&nbsp;</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-size: large;\">II. Visual SLAM for AI Agents</span></strong></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">While no single per-frame computer vision algorithm is close to sufficient to enable robust action in an environment, there is a class of real-time computer vision systems like Visual SLAM that can be used to guide agents through space. The <a href=\"http://visualslam.ai/\">Workshop on Deep Learning for Visual SLAM at ICCV 2019</a>&nbsp;showcased a variety of different Visual SLAM approaches and included a discussion panel. The workshop featured talks on Visual SLAM on mobile platforms (<a href=\"http://www.robots.ox.ac.uk/~victor/\">Victor Prisacariu</a>&nbsp;from <a href=\"http://6d.ai/\">6d.ai</a>), autonomous cars (<a href=\"https://vision.in.tum.de/members/cremers\">Daniel Cremers</a> from TUM and <a href=\"http://artisense.ai/\">ArtiSense.ai</a>), high-detail indoor modeling (<a href=\"https://angeladai.github.io/\">Angela Dai </a>from TUM), AI Agents (<a href=\"http://vladlen.info/\">Vladlen Koltun</a> from Intel Research) and mixed-reality (<a href=\"http://tom.ai/\">Tomasz Malisiewicz</a> from Magic Leap).&nbsp;</span><br /><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-DTGBOTHDobE/XdOdgi_kp4I/AAAAAAAAQx0/i48ux7V_9cwSqFomb7rqTu270ElHxkV8QCLcBGAsYHQ/s1600/2nd_workshop_on_visual_slam_iccv_2019.png\" style=\"margin-left: auto; margin-right: auto;\"><img alt=\"2nd Workshop on Deep Learning for Visual SLAM\" border=\"0\" data-original-height=\"523\" data-original-width=\"1141\" height=\"182\" src=\"https://1.bp.blogspot.com/-DTGBOTHDobE/XdOdgi_kp4I/AAAAAAAAQx0/i48ux7V_9cwSqFomb7rqTu270ElHxkV8QCLcBGAsYHQ/s400/2nd_workshop_on_visual_slam_iccv_2019.png\" title=\"\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Teaser Image for the 2nd Workshop on Deep Learning for Visual SLAM from <a href=\"http://www.ronnieclark.co.uk/\">Ronnie Clark</a>.<br />See info at <a href=\"http://visualslam.ai/\">http://visualslam.ai</a></b></td></tr></tbody></table></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">When it comes to spatial perception capabilities, Koltun's talk made it clear that we, as computer vision researchers, could think bolder. There is a spectrum of spatial perception capabilities that AI agents need that only somewhat overlaps with traditional Visual SLAM (whether deep learning-based or not).</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Koltun's work is in favor of using intermediate representations based on computer vision to produce more robust AI agents. However, Koltun is not convinced that 6dof Visual SLAM, as is currently defined, needs to be solved for AI agents. Let's consider ordinary human tasks like walking, washing your hands, and flossing your teeth -- each one requires a different amount of spatial reasoning abilities. It is reasonable to assume that AI agents would need varying degrees of spatial localization and mapping capabilities to perform such tasks.</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Visual SLAM techniques, like the ones used inside Augmented Reality systems, build metric 3D maps of the environment for the task of high-precision placement of digital content -- but such high-precision systems might never be used directly inside AI agents. When the camera is hand-held (augmented reality) or head-mounted (mixed reality), a human decides where to move.  AI agents have to make their own movement decisions, and this requires more than feature correspondences and bundle adjustment -- more than what is inside the scope of computer vision.</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Inside a head-mounted display, you might look at digital content 30 feet away from you, and for everything to look correct geometrically, you must have a decent 3D map of the world (spanning at least 30 feet) and a reasonable estimate of your pose. But for many tasks that AI agents need to perform, metric-level representations of far-away geometry are un-necessary. It is as if proper action requires local, high-quality metric maps and something coarser like topological maps for large-range maps. Visual SLAM systems (stereo-based and depth-sensor based) are likely to find numerous applications in industry such as mixed reality and some branches of robotics, where millimeter precision matters.&nbsp;</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">More general end-to-end learning for AI agents will show us new kinds of spatial intelligence, automatically learned from data. There is a lot of exciting research to be done to answer questions like the following: What kind of tasks can we train Visual AI Agents for such that map-building and localization capabilities arise? Or What type of core spatial reasoning capabilities can we pre-build to enable further self-supervised learning from the 3D world?</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-size: large;\"><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">III.</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Quō vādis Visual SLAM? Trends and research forecast</strong></span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">At the Deep Learning Workshop for Visual SLAM, an interesting question that came up in the panel focused on the convergence of methods in Visual SLAM. Or alternatively,</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><blockquote class=\"tr_bq\"><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">\"Will a single Visual SLAM framework rule them all?\"</em></blockquote></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">The world of applied research is moving towards more deep learning -- by 2019, many of the critical tasks inside computer vision exist as some form of a (convolutional/graph) neural network. I don't believe that we will see a single SLAM framework/paradigm dominate all others -- I think we will see a plurality of Visual SLAM systems based on inter-changeable deep learning components. This new generation of deep learning-based components will allow more creative applications of end-to-end learning and be typically useful as modules within other real-world systems. We should create tools that will enable others to make better tools.&nbsp;</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">PyTorch is making it easy to build multiple-view geometry tools like <a href=\"https://kornia.github.io/\">Kornia</a>&nbsp;-- such that the right parts of computer vision are brought directly into today's deep learning ecosystem as first-order citizens. And PyTorch is winning over the world of research. A dramatic increase in usage happened from 2017 to 2019, with PyTorch now the recommended framework amongst most of my fellow researchers.</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">To take a look at what the end goal in terms of end-to-end deep learning for visual SLAM might look like, take a look at <a href=\"http://montrealrobotics.ca/gradSLAM/\">gradSLAM</a> from <a href=\"https://krrish94.github.io/\">Krishna&nbsp;</a></span><a href=\"https://krrish94.github.io/\">Murthy</a>, a Ph.D. student in MILA, and collaborators at CMU. Their paper offers a new way of thinking of SLAM as made up of differentiable blocks. From the article, \"This amalgamation of dense SLAM with computational graphs enables us to backprop from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.\"<br /><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XCfJDu1p44g/XdOmBzwclgI/AAAAAAAAQyU/fyrzqoXNlA0-IC7H6WUz3-BDqMmVwGoIgCLcBGAsYHQ/s1600/gradslam.png\" style=\"margin-left: auto; margin-right: auto;\"><img alt=\"Key Figure from the gradSLAM paper on end-to-end learning for SLAM.\" border=\"0\" data-original-height=\"494\" data-original-width=\"1600\" height=\"122\" src=\"https://1.bp.blogspot.com/-XCfJDu1p44g/XdOmBzwclgI/AAAAAAAAQyU/fyrzqoXNlA0-IC7H6WUz3-BDqMmVwGoIgCLcBGAsYHQ/s400/gradslam.png\" title=\"\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Key Figure from the <a href=\"http://montrealrobotics.ca/gradSLAM/\">gradSLAM</a> paper on end-to-end learning for SLAM. [5]</b></td></tr></tbody></table><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span>Another key trend that seems to be on the rise inside the context of Deep Visual SLAM is self-supervised learning. We are seeing more and more practical successes of self-supervised learning for multi-view problems where geometry enables us to get away from strong supervision. Even the ConvNet-based point detector <a href=\"https://arxiv.org/abs/1712.07629\">SuperPoint</a> [7], which my team and I developed at Magic Leap, uses self-supervision to train more robust interest point detectors. In our case, it was impossible to get ground truth interest points on images, and self-labeling was the only way out. One of my favorite researchers working on self-supervised techniques is <a href=\"https://twitter.com/adnothing\">Adrien Gaidon</a> from TRI, who studies how such methods can be used to make smarter cars. Adrien gave some great talks at other ICCV 2019 Workshops related to autonomous vehicles, and his work is closely related to Visual SLAM and useful for anybody working on similar problems.<br /><div style=\"text-align: center;\"><br /></div></div><div style=\"text-align: center;\"></div><div style=\"text-align: center;\"><iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/SLEK2vAgjOI\" width=\"560\"></iframe></div><div style=\"text-align: center;\"><b>Adrien Gaidon's talk from October 11th, 2019 on Self-Supervised Learning in the context of Autonomous Cars</b></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><div style=\"text-align: center;\"><br /></div><div style=\"text-align: left;\">Another excellent presentation about this topic from <a href=\"https://people.eecs.berkeley.edu/~efros/\">Alyosha Efros</a>. He does a great job convincing you why you should love self-supervision.</div><div style=\"text-align: center;\"><br /></div><div style=\"text-align: center;\"></div><div style=\"text-align: center;\"><iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/_V-WpE8cmpc\" width=\"560\"></iframe></div><div style=\"text-align: center;\"><b>A presentation about self-supervision from Alyosha Efros on May 25th, 2018</b></div><br /><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-size: large;\">Conclusion</span></strong><br /><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-size: large;\"><br /></span></strong></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">As more and more spatial reasoning skills get baked into deep networks, we must face two opposing forces. On the one hand, specifying internal representations makes it difficult to scale to new tasks -- it is easier to trick the deep nets into doing all the hard work for you. On the other hand, we want interpretability and some amount of safety when we deploy AI agents into the real world, so some intermediate tasks like object recognition are likely to be involved in today's spatial perception recipe. Lots of exciting work is happening with <a href=\"https://openai.com/blog/emergent-tool-use/\">multi-agents from OpenAI</a> [6], but full end-to-end learning will not give real-world robots such as autonomous cars anytime soon.</span><br /><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span><br /><div style=\"text-align: center;\"></div><div style=\"text-align: center;\"><br /></div><div style=\"text-align: center;\"><iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/kopoLzvh5jY\" width=\"560\"></iframe></div><div style=\"text-align: center;\"><b>Video from OpenAI showing Multi-Agent Hide and Seek. </b>[6]&nbsp; &nbsp;</div><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">More practical Visual SLAM research will focus on differentiable high-level blocks. As more deep learning happens in Visual SLAM, it will create a renaissance in Visual SLAM as sharing entire SLAM systems will be as easy as sharing CNNs today. I cannot wait until the following is possible:</span><br /><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><br /></span><br /><blockquote class=\"tr_bq\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\"><b>pip install DeepSLAM</b></span></blockquote></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">I hope you enjoyed learning about the different approaches to Visual SLAM, and that you have found my blog post insightful and educational. Until next time!</span></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">References:</strong></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[1].</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Vladlen Koltun.</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Chief Scientist for Intelligent Systems at Intel.&nbsp;</span><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"http://vladlen.info/\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">http://vladlen.info/</span></a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[2].&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Direct Sparse Odometry.</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Jakob Engel, Vladlen Koltun, and Daniel Cremers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(3), 2018.&nbsp;</span><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"http://vladlen.info/publications/direct-sparse-odometry/\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">http://vladlen.info/publications/direct-sparse-odometry/</span></a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[3].&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Does Computer Vision Matter for Action?</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Brady Zhou, Philipp Krähenbühl, and Vladlen Koltun. Science Robotics, 4(30), 2019.&nbsp;</span><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"http://vladlen.info/publications/computer-vision-matter-action/\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">http://vladlen.info/publications/computer-vision-matter-action/</span></a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[4].&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Kornia: an Open Source Differentiable Computer Vision Library for PyTorch</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">. Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski. Winter Conference on Applications of Computer Vision, 2019.&nbsp;</span><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"https://kornia.github.io/\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">https://kornia.github.io/</span></a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[5].&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">gradSLAM: Dense SLAM meets Automatic Differentiation.</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Krishna Murthy J., Ganesh&nbsp;</span>Iyer,<span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;and&nbsp;</span>Liam&nbsp;<span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Paull. In&nbsp;</span><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">arXiv</em><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">, 2019.&nbsp;</span><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"http://montrealrobotics.ca/gradSLAM/\" style=\"color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">http://montrealrobotics.ca/gradSLAM/</span></a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">[6]&nbsp;</span><strong style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">Emergent tool use from multi-agent autocurricula.</strong><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">&nbsp;Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. In&nbsp;</span><em style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">arXiv&nbsp;</em><span data-preserver-spaces=\"true\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; margin-bottom: 0pt; margin-top: 0pt;\">2019.&nbsp;</span><span data-preserver-spaces=\"true\" style=\"color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\"><a class=\"_e75a791d-denali-editor-page-rtfLink\" href=\"https://openai.com/blog/emergent-tool-use/\" style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #4a6ee0; margin-bottom: 0pt; margin-top: 0pt;\" target=\"_blank\">https://openai.com/blog/emergent-tool-use/</a></span><br />[7] <b>SuperPoint: Self-supervised interest point detection and description.</b> Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018.&nbsp;<a href=\"https://arxiv.org/abs/1712.07629\">https://arxiv.org/abs/1712.07629</a></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div><br /><div style=\"background-attachment: initial; background-clip: initial; background-image: initial; background-origin: initial; background-position: initial; background-repeat: initial; background-size: initial; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"></div><br /><div style=\"background: transparent; color: #1c1e29; margin-bottom: 0pt; margin-top: 0pt;\"><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Tomasz Malisiewicz",
    "uri": "http://www.blogger.com/profile/17507234774392358321",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 2
}