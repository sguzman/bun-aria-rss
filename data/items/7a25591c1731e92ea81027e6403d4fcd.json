{
  "title": "MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v1 [eess.AS])",
  "link": "http://arxiv.org/abs/2211.01515",
  "description": "<p>We present Multiscale Audio Spectrogram Transformer (MAST) for audio\nclassification, which brings the concept of multiscale feature hierarchies to\nthe Audio Spectrogram Transformer (AST). Given an input audio spectrogram we\nfirst patchify and project it into an initial temporal resolution and embedding\ndimension, post which the multiple stages in MAST progressively expand the\nembedding dimension while reducing the temporal resolution of the input. We use\na pyramid structure that allows early layers of MAST operating at a high\ntemporal resolution but low embedding space to model simple low-level acoustic\ninformation and deeper temporally coarse layers to model high-level acoustic\ninformation with high-dimensional embeddings. We also extend our approach to\npresent a new Self-Supervised Learning (SSL) method called SS-MAST, which\ncalculates a symmetric contrastive loss between latent representations from a\nstudent and a teacher encoder. In practice, MAST significantly outperforms AST\nby an average accuracy of 3.4% across 8 speech and non-speech tasks from the\nLAPE Benchmark. Moreover, SS-MAST achieves an absolute average improvement of\n2.6% over SSAST for both AST and MAST encoders. We make all our codes available\non GitHub at the time of publication.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"
}