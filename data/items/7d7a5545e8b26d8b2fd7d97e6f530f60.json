{
  "title": "Scalable Benchmarking with Rust Streams",
  "link": "",
  "published": "2020-11-30T00:00:00+00:00",
  "updated": "2020-11-30T00:00:00+00:00",
  "id": "https://pkolaczk.github.io/benchmarking-cassandra-with-rust-streams",
  "content": "<p>In <a href=\"/benchmarking-cassandra/\">the previous post</a> I showed how to use asynchronous \nRust to measure throughput and response times of a Cassandra cluster. \nThat approach works pretty well on a developer’s laptop, but it turned out it doesn’t scale to bigger machines. \nI’ve hit a hard limit around 150k requests per\nsecond, and it wouldn’t go faster regardless of the performance of the server. \nIn this post I share a different approach that doesn’t have these scalability problems. \nI was able to saturate a 24-core single node Cassandra server\nat 800k read queries per second with a single client machine.</p>\n\n<!--more-->\n\n<p>The original idea was based on a single-threaded loop that spawns asynchronous tasks.\nEach task sends an async query, records its duration when the results are back, and sends the recorded</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>    <span class=\"k\">let</span> <span class=\"n\">parallelism_limit</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span><span class=\"p\">;</span>\n    <span class=\"k\">let</span> <span class=\"n\">semaphore</span> <span class=\"o\">=</span> <span class=\"nn\">Arc</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"nn\">Semaphore</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">parallelism_limit</span><span class=\"p\">));</span>\n    <span class=\"k\">let</span> <span class=\"p\">(</span><span class=\"n\">tx</span><span class=\"p\">,</span> <span class=\"k\">mut</span> <span class=\"n\">rx</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"nn\">tokio</span><span class=\"p\">::</span><span class=\"nn\">sync</span><span class=\"p\">::</span><span class=\"nn\">mpsc</span><span class=\"p\">::</span><span class=\"nf\">unbounded_channel</span><span class=\"p\">();</span>\n    <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"nn\">Arc</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">);</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"n\">in</span> <span class=\"mi\">0</span><span class=\"o\">..</span><span class=\"n\">count</span> <span class=\"p\">{</span>\n        <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">statement</span><span class=\"nf\">.bind</span><span class=\"p\">();</span>\n        <span class=\"n\">statement</span><span class=\"nf\">.bind</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"k\">as</span> <span class=\"nb\">i64</span><span class=\"p\">)</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n\n        <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.clone</span><span class=\"p\">();</span>\n        <span class=\"k\">let</span> <span class=\"n\">tx</span> <span class=\"o\">=</span> <span class=\"n\">tx</span><span class=\"nf\">.clone</span><span class=\"p\">();</span>\n        <span class=\"k\">let</span> <span class=\"n\">permit</span> <span class=\"o\">=</span> <span class=\"n\">semaphore</span><span class=\"nf\">.clone</span><span class=\"p\">()</span><span class=\"nf\">.acquire_owned</span><span class=\"p\">()</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n        <span class=\"nn\">tokio</span><span class=\"p\">::</span><span class=\"nf\">spawn</span><span class=\"p\">(</span><span class=\"k\">async</span> <span class=\"k\">move</span> <span class=\"p\">{</span>\n            <span class=\"k\">let</span> <span class=\"n\">query_start</span> <span class=\"o\">=</span> <span class=\"nn\">Instant</span><span class=\"p\">::</span><span class=\"nf\">now</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.execute</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">statement</span><span class=\"p\">);</span>\n            <span class=\"n\">result</span><span class=\"k\">.await</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">query_end</span> <span class=\"o\">=</span> <span class=\"nn\">Instant</span><span class=\"p\">::</span><span class=\"nf\">now</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">duration_micros</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">query_end</span> <span class=\"o\">-</span> <span class=\"n\">query_start</span><span class=\"p\">)</span><span class=\"nf\">.as_micros</span><span class=\"p\">();</span>\n            <span class=\"n\">tx</span><span class=\"nf\">.send</span><span class=\"p\">(</span><span class=\"n\">duration_micros</span><span class=\"p\">)</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n            <span class=\"k\">drop</span><span class=\"p\">(</span><span class=\"n\">permit</span><span class=\"p\">);</span>\n        <span class=\"p\">});</span>\n    <span class=\"p\">}</span>   \n    <span class=\"k\">drop</span><span class=\"p\">(</span><span class=\"n\">tx</span><span class=\"p\">);</span>\n\n    <span class=\"c\">// ... receive the durations from rx and compute statistics</span>\n</code></pre></div></div>\n\n<p>I assumed invoking async queries should be so fast that the server would be the only bottleneck.\nI was wrong.</p>\n\n<p>When running this code on a nice 24-core machine, I observed a surprising effect:\nthe benchmarking client managed to send about 120k read requests per second, but both the client and the server\nmachines had plenty of idle CPU available.</p>\n\n<h1 id=\"tuning\">Tuning</h1>\n\n<p>The first idea to fix this was to play with the number of I/O threads used internally by the C++ Driver.\nSusprisingly that didn’t help a lot. While going from 1 to 4 I/O threads improved performance slightly to about 150k requests per second,\nincreasing this further didn’t have much effect and going extreme to &gt;32 threads actually even worsened the performance. \nI also didn’t get much luckier by tuning the number of client connections per each I/O thread. 4-8 threads with 1 connection each\nseemed to be a sweet spot, but very far from saturating the hardware I had.</p>\n\n<p>The next thing that came to my mind was looking closer at Tokio setup.\nTokio allows to choose either a single-threaded scheduler or a multi-threaded one.\nA single-threaded scheduler uses a single OS thread to run all async tasks.\nBecause I assumed the majority of hard work is supposed to be done by the Cassandra C++ driver code\nand because the C++ driver comes with its own <code class=\"language-plaintext highlighter-rouge\">libuv</code> based thread-pool, I initially set up Tokio with\na single-threaded scheduler. How costly could it be to count queries or compute the histogram of durations, anyways?\nShould’t it be easily in the range of millions of items per second, even on a single thread?</p>\n\n<p>Indeed, counting queries seemed to be fast, but <code class=\"language-plaintext highlighter-rouge\">perf</code> suggested the majority of time is being spent in two places:</p>\n<ul>\n  <li>C++ Driver code</li>\n  <li>Tokio runtime</li>\n</ul>\n\n<p>So maybe that wasn’t a good idea to use a single thread to run all the Tokio stuff? \nHere is the code for setting up Tokio with a multi-threaded scheduler:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">fn</span> <span class=\"nf\">async_main</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n  <span class=\"c\">// Run async benchmark code</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">fn</span> <span class=\"nf\">main</span><span class=\"p\">()</span> <span class=\"p\">{</span>        \n    <span class=\"nn\">tokio</span><span class=\"p\">::</span><span class=\"nn\">runtime</span><span class=\"p\">::</span><span class=\"nn\">Builder</span><span class=\"p\">::</span><span class=\"nf\">new_multi_thread</span><span class=\"p\">()</span>\n        <span class=\"nf\">.max_threads</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">)</span>    \n        <span class=\"nf\">.enable_time</span><span class=\"p\">()</span>\n        <span class=\"nf\">.build</span><span class=\"p\">()</span>\n        <span class=\"nf\">.unwrap</span><span class=\"p\">()</span>\n        <span class=\"nf\">.block_on</span><span class=\"p\">(</span><span class=\"nf\">async_main</span><span class=\"p\">());</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>This change alone without any modification to the main loop of the benchmark allowed to increase the performance\nto about 220k requests per second. Obviously, this didn’t satisfy me, because I knew these machines could go much faster.\nJust running 3 instances of my Rust benchmarking program at the same time allowed to reach throughput of around 450k req/s.\nAnd running 12 Java-based <code class=\"language-plaintext highlighter-rouge\">cassandra-stress</code> clients, each from a separate node, made ~760k req/s.</p>\n\n<p>Additionally the change of the scheduler had a negative side effect: the CPU usage on the client\nincreased by about 50% and now in the other tests when running the benchmarking program on the same machine \nas the benchmarked server the performance was slightly worse than before. So, overall the benchmarking tool \ngot slightly faster, but <em>less efficient</em>.</p>\n\n<h1 id=\"rethinking-the-main-loop\">Rethinking the Main Loop</h1>\n<p>There are several things that limit the speed at which new requests can be spawned:</p>\n<ul>\n  <li>Spawning an async task in Tokio is quite costly - it requires adding the task to a shared queue\nand possibly some (lightweight) synchronization.</li>\n  <li>Each task sends the result to an mpsc channel. There is some contention there as well.</li>\n  <li>The Tokio async semaphore also seems to add some overhead.</li>\n  <li>Cloning the referenced-counted pointer to a shared session is another point of contention between threads.</li>\n  <li>Finally, binding query parameters and sending the query also requires some CPU work.</li>\n</ul>\n\n<p>As an experiment I removed all the calls related to sending Cassandra queries from the main loop,\nand I got only ~800k loops per second, when benchmarking “nothing”. This led me to thinking this code needs to be improved.</p>\n\n<p>In the <a href=\"https://www.reddit.com/r/rust/comments/j5n04h/benchmarking_apache_cassandra_with_rust/g7vi6bi?utm_source=share&amp;utm_medium=web2x&amp;context=3\">comment</a> \nunder the original blog post, <a href=\"https://www.reddit.com/user/kostaw/\">kostaw</a> suggested to use Streams instead\nof manual looping. Below I present a version of code after minor modifications to make it compile:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"c\">/// Invokes count statements and returns a stream of their durations.</span>\n<span class=\"c\">/// Note: this does *not* spawn a new thread. </span>\n<span class=\"c\">/// It runs all async code on the caller's thread.</span>\n<span class=\"k\">fn</span> <span class=\"n\">make_stream</span><span class=\"o\">&lt;</span><span class=\"nv\">'a</span><span class=\"o\">&gt;</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"o\">&amp;</span><span class=\"nv\">'a</span> <span class=\"n\">Session</span><span class=\"p\">,</span> <span class=\"n\">statement</span><span class=\"p\">:</span> <span class=\"o\">&amp;</span><span class=\"nv\">'a</span> <span class=\"n\">PreparedStatement</span><span class=\"p\">,</span> <span class=\"n\">count</span><span class=\"p\">:</span> <span class=\"nb\">usize</span><span class=\"p\">)</span>\n    <span class=\"k\">-&gt;</span> <span class=\"k\">impl</span> <span class=\"n\">Stream</span><span class=\"o\">&lt;</span><span class=\"n\">Item</span><span class=\"o\">=</span><span class=\"n\">Duration</span><span class=\"o\">&gt;</span> <span class=\"o\">+</span> <span class=\"nv\">'a</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">let</span> <span class=\"n\">parallelism_limit</span> <span class=\"o\">=</span> <span class=\"mi\">128</span><span class=\"p\">;</span>\n    <span class=\"nn\">futures</span><span class=\"p\">::</span><span class=\"nn\">stream</span><span class=\"p\">::</span><span class=\"nf\">iter</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"o\">..</span><span class=\"n\">count</span><span class=\"p\">)</span>\n        <span class=\"nf\">.map</span><span class=\"p\">(</span><span class=\"k\">move</span> <span class=\"p\">|</span><span class=\"n\">i</span><span class=\"p\">|</span> <span class=\"k\">async</span> <span class=\"k\">move</span> <span class=\"p\">{</span>\n            <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">statement</span><span class=\"nf\">.bind</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">statement</span><span class=\"nf\">.bind</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"k\">as</span> <span class=\"nb\">i64</span><span class=\"p\">)</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">query_start</span> <span class=\"o\">=</span> <span class=\"nn\">Instant</span><span class=\"p\">::</span><span class=\"nf\">now</span><span class=\"p\">();</span>\n            <span class=\"k\">let</span> <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.execute</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">statement</span><span class=\"p\">);</span>\n            <span class=\"n\">result</span><span class=\"k\">.await</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n            <span class=\"n\">query_start</span><span class=\"nf\">.elapsed</span><span class=\"p\">()</span>\n        <span class=\"p\">})</span>\n        <span class=\"c\">// This will run up to `parallelism_limit` futures at a time:</span>\n        <span class=\"nf\">.buffer_unordered</span><span class=\"p\">(</span><span class=\"n\">parallelism_limit</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">async</span> <span class=\"k\">fn</span> <span class=\"nf\">benchmark</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"k\">let</span> <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"mi\">1000000</span><span class=\"p\">;</span>\n\n    <span class=\"c\">// Connect to the database and prepare the statement:</span>\n    <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"c\">// ...</span>\n    <span class=\"k\">let</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.prepare</span><span class=\"p\">(</span><span class=\"cm\">/** statement */</span><span class=\"p\">)</span><span class=\"nf\">.unwrap</span><span class=\"p\">()</span><span class=\"k\">.await</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n    <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">stream</span> <span class=\"o\">=</span> <span class=\"nf\">make_stream</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"o\">&amp;</span><span class=\"n\">statement</span><span class=\"p\">,</span> <span class=\"n\">count</span><span class=\"p\">)</span>\n\n    <span class=\"c\">// Process the received durations: </span>\n    <span class=\"k\">let</span> <span class=\"n\">benchmark_start</span> <span class=\"o\">=</span> <span class=\"nn\">Instant</span><span class=\"p\">::</span><span class=\"nf\">now</span><span class=\"p\">();</span>\n    <span class=\"k\">while</span> <span class=\"k\">let</span> <span class=\"nf\">Some</span><span class=\"p\">(</span><span class=\"n\">duration</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">stream</span><span class=\"nf\">.next</span><span class=\"p\">()</span><span class=\"k\">.await</span> <span class=\"p\">{</span>\n        <span class=\"c\">// ... optionally compute durations statistics</span>\n    <span class=\"p\">}</span>\n    <span class=\"nd\">println!</span><span class=\"p\">(</span>\n        <span class=\"s\">\"Throughput: {:.1} request/s\"</span><span class=\"p\">,</span>\n        <span class=\"mf\">1000000.0</span> <span class=\"o\">*</span> <span class=\"n\">count</span> <span class=\"k\">as</span> <span class=\"nb\">f64</span> <span class=\"o\">/</span> <span class=\"n\">benchmark_start</span><span class=\"nf\">.elapsed</span><span class=\"p\">()</span><span class=\"nf\">.as_micros</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"nb\">f64</span>\n    <span class=\"p\">);</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>There are several advantages to this approach:</p>\n<ul>\n  <li>The code is simpler and much more elegant: no channels, no semaphore to limit parallelism</li>\n  <li>We don’t need <code class=\"language-plaintext highlighter-rouge\">Arc</code> anymore to deal with lifetimes! Standard lifetime annotations are enough\nto tell Rust that <code class=\"language-plaintext highlighter-rouge\">Session</code> lives at least as long as the <code class=\"language-plaintext highlighter-rouge\">Stream</code> we return.</li>\n  <li>There is no task spawning.</li>\n</ul>\n\n<p>This code indeed has a much lower overhead. After removing the <code class=\"language-plaintext highlighter-rouge\">statement.bind</code> and <code class=\"language-plaintext highlighter-rouge\">session.execute</code> calls,\nthe stream was able to generate over 10 million items per second on my laptop. That’s a nice 12x improvement.</p>\n\n<p>Unfortunately, this way we only reduced some overhead, but the main scalability problem is still there:</p>\n<ul>\n  <li>The code runs statement parameter binding, time measurement and processing of the results on a single thread.</li>\n  <li>With a fast enough server, that one thread will be saturated and we’ll see a hard throughput limit again.</li>\n</ul>\n\n<h1 id=\"going-insanely-multithreaded\">Going Insanely Multithreaded</h1>\n<p>We can run multiple streams, each on its own thread. \nTo do this, we need <code class=\"language-plaintext highlighter-rouge\">tokio::spawn</code> again, but this time we’ll do it a different level, only once per each thread.</p>\n\n<p>Let’s first define a function that can consume a stream in a Tokio task and returns how long it took.\nIf we use a multitheaded scheduler, it would be likely executed by another thread:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">fn</span> <span class=\"nf\">run_to_completion</span><span class=\"p\">(</span><span class=\"k\">mut</span> <span class=\"n\">stream</span><span class=\"p\">:</span> <span class=\"k\">impl</span> <span class=\"n\">Stream</span><span class=\"o\">&lt;</span><span class=\"n\">Item</span><span class=\"o\">=</span><span class=\"n\">Duration</span><span class=\"o\">&gt;</span> <span class=\"o\">+</span> <span class=\"n\">Unpin</span> <span class=\"o\">+</span> <span class=\"nb\">Send</span> <span class=\"o\">+</span> <span class=\"nv\">'static</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">let</span> <span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"nn\">tokio</span><span class=\"p\">::</span><span class=\"nf\">spawn</span><span class=\"p\">(</span><span class=\"k\">async</span> <span class=\"k\">move</span> <span class=\"p\">{</span>\n        <span class=\"k\">while</span> <span class=\"k\">let</span> <span class=\"nf\">Some</span><span class=\"p\">(</span><span class=\"n\">duration</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">stream</span><span class=\"nf\">.next</span><span class=\"p\">()</span><span class=\"k\">.await</span> <span class=\"p\">{}</span>\n    <span class=\"p\">});</span>\n    <span class=\"n\">task</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Because we’re passing the stream to the lambda given to <code class=\"language-plaintext highlighter-rouge\">tokio::spawn</code>, the stream needs to have <code class=\"language-plaintext highlighter-rouge\">'static</code> \nlifetime. Unfortunately, this will make it problematic to use with the <code class=\"language-plaintext highlighter-rouge\">make_stream</code> function we defined earlier:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">stream</span> <span class=\"o\">=</span> <span class=\"nf\">make_stream</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"o\">&amp;</span><span class=\"n\">statement</span><span class=\"p\">,</span> <span class=\"n\">count</span><span class=\"p\">);</span>\n<span class=\"k\">let</span> <span class=\"n\">elapsed</span> <span class=\"o\">=</span> <span class=\"nf\">run_to_completion</span><span class=\"p\">(</span><span class=\"n\">stream</span><span class=\"p\">)</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<pre>\nerror[E0597]: `session` does not live long enough\n   --&gt; src/main.rs:104:34\n    |\n104 |     let mut stream = make_stream(&amp;session, &amp;statement, count);\n    |                      ------------^^^^^^^^--------------------\n    |                      |           |\n    |                      |           borrowed value does not live long enough\n    |                      argument requires that `session` is borrowed for `'static`\n...\n112 | }\n    | - `session` dropped here while still borrowed\n</pre>\n\n<p>It looks quite familiar. We’ve run into this problem already before, when spawning a task for each query.\nWe have solved that with <code class=\"language-plaintext highlighter-rouge\">Arc</code>, and now we’ll do the same. Notice that this time cloning shouldn’t affect\nperformance, because we do it once per the whole stream:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">fn</span> <span class=\"nf\">run_stream</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"nb\">Arc</span><span class=\"o\">&lt;</span><span class=\"n\">Session</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"n\">statement</span><span class=\"p\">:</span> <span class=\"nb\">Arc</span><span class=\"o\">&lt;</span><span class=\"n\">PreparedStatement</span><span class=\"o\">&gt;</span><span class=\"p\">,</span> <span class=\"n\">count</span><span class=\"p\">:</span> <span class=\"nb\">usize</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">let</span> <span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"nn\">tokio</span><span class=\"p\">::</span><span class=\"nf\">spawn</span><span class=\"p\">(</span><span class=\"k\">async</span> <span class=\"k\">move</span> <span class=\"p\">{</span>\n        <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.as_ref</span><span class=\"p\">();</span>\n        <span class=\"k\">let</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">statement</span><span class=\"nf\">.as_ref</span><span class=\"p\">();</span>\n        <span class=\"k\">let</span> <span class=\"k\">mut</span> <span class=\"n\">stream</span> <span class=\"o\">=</span> <span class=\"nf\">make_stream</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"n\">statement</span><span class=\"p\">,</span> <span class=\"n\">count</span><span class=\"p\">);</span>\n        <span class=\"k\">while</span> <span class=\"k\">let</span> <span class=\"nf\">Some</span><span class=\"p\">(</span><span class=\"n\">duration</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">stream</span><span class=\"nf\">.next</span><span class=\"p\">()</span><span class=\"k\">.await</span> <span class=\"p\">{}</span>\n    <span class=\"p\">});</span>\n    <span class=\"n\">task</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Note that we had to move the creation of the <code class=\"language-plaintext highlighter-rouge\">session</code> and <code class=\"language-plaintext highlighter-rouge\">statement</code> raw references \nand the creation of the stream to inside of the <code class=\"language-plaintext highlighter-rouge\">spawn</code> lambda, so they live as long as \nthe async task.</p>\n\n<p>Now we can actually call <code class=\"language-plaintext highlighter-rouge\">run_stream</code> multiple times and create multiple parallel\nstatement streams:</p>\n\n<div class=\"language-rust highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">async</span> <span class=\"k\">fn</span> <span class=\"nf\">benchmark</span><span class=\"p\">()</span> <span class=\"p\">{</span>    \n    <span class=\"k\">let</span> <span class=\"n\">count</span> <span class=\"o\">=</span> <span class=\"mi\">1000000</span><span class=\"p\">;</span>\n\n    <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"c\">// ... connect</span>\n    <span class=\"k\">let</span> <span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"nn\">Arc</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">);</span>\n    <span class=\"k\">let</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"nf\">.prepare</span><span class=\"p\">(</span><span class=\"s\">\"SELECT * FROM keyspace1.test WHERE pk = ?\"</span><span class=\"p\">)</span><span class=\"nf\">.unwrap</span><span class=\"p\">()</span><span class=\"k\">.await</span><span class=\"nf\">.unwrap</span><span class=\"p\">();</span>\n    <span class=\"k\">let</span> <span class=\"n\">statement</span> <span class=\"o\">=</span> <span class=\"nn\">Arc</span><span class=\"p\">::</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">statement</span><span class=\"p\">);</span>\n\n    <span class=\"k\">let</span> <span class=\"n\">benchmark_start</span> <span class=\"o\">=</span> <span class=\"nn\">Instant</span><span class=\"p\">::</span><span class=\"nf\">now</span><span class=\"p\">();</span>\n    <span class=\"k\">let</span> <span class=\"n\">thread_1</span> <span class=\"o\">=</span> <span class=\"nf\">run_stream</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"nf\">.clone</span><span class=\"p\">(),</span> <span class=\"n\">statement</span><span class=\"nf\">.clone</span><span class=\"p\">(),</span> <span class=\"n\">count</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">);</span>\n    <span class=\"k\">let</span> <span class=\"n\">thread_2</span> <span class=\"o\">=</span> <span class=\"nf\">run_stream</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"nf\">.clone</span><span class=\"p\">(),</span> <span class=\"n\">statement</span><span class=\"nf\">.clone</span><span class=\"p\">(),</span> <span class=\"n\">count</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">);</span>\n    <span class=\"n\">thread_1</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n    <span class=\"n\">thread_2</span><span class=\"k\">.await</span><span class=\"p\">;</span>\n\n    <span class=\"nd\">println!</span><span class=\"p\">(</span>\n        <span class=\"s\">\"Throughput: {:.1} request/s\"</span><span class=\"p\">,</span>\n        <span class=\"mf\">1000000.0</span> <span class=\"o\">*</span> <span class=\"n\">count</span> <span class=\"k\">as</span> <span class=\"nb\">f64</span> <span class=\"o\">/</span> <span class=\"n\">benchmark_start</span><span class=\"nf\">.elapsed</span><span class=\"p\">()</span><span class=\"nf\">.as_micros</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"nb\">f64</span>\n    <span class=\"p\">);</span>\n</code></pre></div></div>\n\n<h1 id=\"results\">Results</h1>\n<p>Switching my Apache Cassandra Benchmarking Tool <a href=\"https://github.com/pkolaczk/latte\">Latte</a> to use\nthis new approach caused the throughput on bigger machines to skyrocket:</p>\n\n<pre>\nCONFIG =================================================================================================\n            Date        : Mon, 09 Nov 2020                                                         \n            Time        : 14:17:36 +0000                                                           \n             Tag        :                                                                          \n        Workload        : read                                                                     \n      Compaction        : STCS                                                                     \n      Partitions        :      1000                                                                 \n         Columns        :         1                                                                 \n     Column size     [B]:        16                                                                 \n         Threads        :        24                                                                 \n     Connections        :         4                                                                 \n Max parallelism   [req]:       256                                                                 \n        Max rate [req/s]:                                                                          \n          Warmup   [req]:         1                                                                 \n      Iterations   [req]:  10000000                                                                 \n        Sampling     [s]:       1.0                                                                 \n\nLOG ====================================================================================================\n    Time  Throughput        ----------------------- Response times [ms]---------------------------------\n     [s]     [req/s]           Min        25        50        75        90        95        99       Max\n   0.000      791822          0.29      6.57      7.01      7.62      9.68     10.90     16.03     67.14\n   1.000      830663          1.06      6.68      7.11      7.72      9.25     10.59     12.05     21.57\n   2.000      798252          1.49      6.99      7.42      7.93      9.47     11.11     12.35     44.83\n   3.000      765633          0.88      6.91      7.34      7.91      9.57     11.24     14.86     72.70\n   4.000      797175          1.27      7.00      7.43      7.97      9.57     11.18     12.37     23.04\n   5.000      767988          1.35      6.88      7.30      7.85      9.41     11.06     14.46     72.70\n   6.000      800712          0.69      6.98      7.40      7.90      9.38     11.06     12.43     22.59\n   7.000      800809          1.55      6.98      7.40      7.91      9.25     11.06     12.45     22.88\n   8.000      765714          1.54      6.87      7.31      7.90      9.59     11.28     14.51     71.93\n   9.000      798496          1.25      6.97      7.42      7.95      9.50     11.13     12.50     25.23\n  10.000      763279          1.02      6.88      7.37      7.92      9.60     11.29     15.04     73.28\n  11.000      798546          1.10      6.98      7.43      7.95      9.39     11.13     12.43     26.19\n  12.000      797906          1.39      6.98      7.43      7.98      9.49     11.19     12.56     37.22\n\nSUMMARY STATS ==========================================================================================\n         Elapsed     [s]:    12.656                                                                 \n        CPU time     [s]:   294.045          ( 48.4%)                                               \n       Completed   [req]:  10000000          (100.0%)                                               \n          Errors   [req]:         0          (  0.0%)                                               \n      Partitions        :  10000000                                                                 \n            Rows        :  10000000                                                                 \n         Samples        :        13                                                                 \nMean sample size   [req]:    769231                                                                 \n      Throughput [req/s]:    790538 ± 17826                                                         \n Mean resp. time    [ms]:      7.76 ± 0.18                                                          \n</pre>\n\n<p>Unfortunately, the server machine was completely saturated at this level.\nThat’s a pity, because the client reported only 48.4% of CPU utilisation and it could probably\ngo faster with a faster server.</p>\n\n<h1 id=\"takeaways\">Takeaways</h1>\n<ul>\n  <li>Don’t assume that if a piece of code is <em>simple</em> and <em>looks fast</em>, it won’t become a bottleneck eventually.\nIt might not be a bottleneck on the laptop, but may be a problem on a bigger iron or with a different workload.</li>\n  <li>I’ve read somewhere you should spawn plenty of small tasks so the Tokio scheduler can do its\njob of balancing work well. This is a good advice, but don’t go extreme with that. \nHundreds thousands of tasks per second is probably a bad idea and would cause CPU time to be spent on scheduling them \ninstead of doing real work.</li>\n  <li>Rust async streams offer very nice properties related to object lifetimes and code readability. Learn them! Now! :)</li>\n</ul>",
  "author": {
    "name": "Piotr Kołaczkowski"
  },
  "category": [
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ],
  "summary": "In the previous post I showed how to use asynchronous Rust to measure throughput and response times of a Cassandra cluster. That approach works pretty well on a developer’s laptop, but it turned out it doesn’t scale to bigger machines. I’ve hit a hard limit around 150k requests per second, and it wouldn’t go faster regardless of the performance of the server. In this post I share a different approach that doesn’t have these scalability problems. I was able to saturate a 24-core single node Cassandra server at 800k read queries per second with a single client machine."
}