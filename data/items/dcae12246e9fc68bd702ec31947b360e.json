{
  "title": "Assessing Resource-Performance Trade-off of Natural Language Models using Data Envelopment Analysis. (arXiv:2211.01486v1 [cs.CL])",
  "link": "http://arxiv.org/abs/2211.01486",
  "description": "<p>Natural language models are often summarized through a high-dimensional set\nof descriptive metrics including training corpus size, training time, the\nnumber of trainable parameters, inference times, and evaluation statistics that\nassess performance across tasks. The high dimensional nature of these metrics\nyields challenges with regard to objectively comparing models; in particular it\nis challenging to assess the trade-off models make between performance and\nresources (compute time, memory, etc.).\n</p>\n<p>We apply Data Envelopment Analysis (DEA) to this problem of assessing the\nresource-performance trade-off. DEA is a nonparametric method that measures\nproductive efficiency of abstract units that consume one or more inputs and\nyield at least one output. We recast natural language models as units suitable\nfor DEA, and we show that DEA can be used to create an effective framework for\nquantifying model performance and efficiency. A central feature of DEA is that\nit identifies a subset of models that live on an efficient frontier of\nperformance. DEA is also scalable, having been applied to problems with\nthousands of units. We report empirical results of DEA applied to 14 different\nlanguage models that have a variety of architectures, and we show that DEA can\nbe used to identify a subset of models that effectively balance resource\ndemands against performance.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zachary Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachariah_A/0/1/0/all/0/1\">Alisha Zachariah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conathan_D/0/1/0/all/0/1\">Devin Conathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_J/0/1/0/all/0/1\">Jeffery Kline</a>"
}