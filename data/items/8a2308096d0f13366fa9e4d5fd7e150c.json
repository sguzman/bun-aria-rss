{
  "id": "tag:blogger.com,1999:blog-5825758052688213474.post-6560793314987327113",
  "published": "2021-04-19T18:46:00.003-07:00",
  "updated": "2021-04-20T10:23:33.331-07:00",
  "title": "Why model calibration matters and how to achieve it",
  "content": "by LEE RICHARDSON &amp; TAYLOR POSPISIL<br /><br /><i>Calibrated models make probabilistic predictions that match real world probabilities.  This post explains why calibration matters, and how to achieve it. It discusses practical issues that calibrated predictions solve and presents a flexible framework to calibrate any classifier. Calibration applies in many applications, and hence the practicing data scientist must understand this useful tool.<br /></i><div><br /></div><div><br /></div><br /><h2 style=\"text-align: left;\">What is calibration?</h2>At Google we make predictions for a large number of binary events such as “will a user click this ad” or “is this email spam”.  In addition to the raw classification of $Y = 0$/'NotSpam' or $Y = 1$/'Spam' we are also interested in predicting the probability of the binary event $\\Pr(Y = 1 | X)$ for some covariates $X$.  One useful property of these predictions is calibration.  To explain, let’s borrow a quote from Nate Silver’s <u>The Signal and the Noise</u>:<div><br /></div><blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px;\"><div style=\"text-align: left;\">One of the most important tests of a forecast — I would argue that it is the single most important one — is called calibration. Out of all the times you said there was a 40 percent chance of rain, how often did rain actually occur? If, over the long run, it really did rain about 40 percent of the time, that means your forecasts were well calibrated. If it wound up raining just 20 percent of the time instead, or 60 percent of the time, they weren’t.</div></blockquote><div><p>Mathematically we can define this as $\\hat{p} = \\Pr(Y | \\hat{p})$, where $\\hat{p}$ is the predicted probability given by our classifier.<br /><br />While calibration seems like a straightforward and perhaps trivial property, miscalibrated models are actually quite common. For example, some complex models are miscalibrated out-of-the-box, such as <a href=\"https://scikit-learn.org/stable/modules/calibration.html\">Random Forests, SVMs, Naive Bayes</a>, and (modern) <a href=\"https://arxiv.org/abs/1706.04599\">neural networks</a>.  Simpler models, such as logistic regression, will be miscalibrated if the conditional probability doesn’t follow the specified functional form (e.g. a sigmoid). And even if your simpler model is correctly specified, you still might suffer from the curse of dimensionality (see figure 3 in <a href=\"https://www.pnas.org/content/116/29/14516\">Candes and Sur</a>). Things get even more complicated when you bring in regularization, complex reweighting schemes, or <a href=\"https://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf\">boosting</a>. Given all of the potential causes of miscalibration, you shouldn’t assume your model will be calibrated.</p><br /><h2 style=\"text-align: left;\">Why calibration matters</h2>What are the consequences of miscalibrated models?  Intuitively, you want to have calibration so that you can interpret your estimated probabilities as long-run frequencies. In that sense, the question could be “why wouldn’t you want to be calibrated?”.  But let’s go further and point out some practical reasons why we want our model to be calibrated:</div><div><h3 style=\"text-align: left;\">Practical Reason #1: Estimated probabilities allow flexibility</h3>If you are predicting whether a user will click on an ad with a classifier, it’s useful to be able to rank ads by their probability of being clicked.  This does not require calibration.  But if you want to calculate the expected number of clicks then you need calibrated probabilities.  This expected value can be helpful for simulating the impact of an experiment (does this increase expected clicks enough to merit running an actual experiment?) or may be used directly by not serving ads which don’t have expected revenue greater than their cost.</div><div><h3 style=\"text-align: left;\">Practical Reason #2: Model Modularity</h3>In complex machine learning systems, models depend on each other.  Single classifiers are often inputs into larger systems that make the final decisions.<br /><br />For these ML systems, calibration simplifies interaction. Calibration allows each model to focus on estimating its particular probabilities as well as possible. And since the interpretation is stable, other system components don’t need to shift whenever models change.<br /><br />For example, let’s say you quantify the importance of an email using a $\\Pr(\\mbox{Important})$ model.  This is then an input to a $\\Pr(\\mbox{Spam})$ model, and the $\\Pr(\\mbox{Spam})$ model decides which emails get flagged as spam. Now the $\\Pr(\\mbox{Important})$ model becomes miscalibrated and starts assigning too high of  probabilities for emails being important.  In this case, you can just change the threshold for $\\Pr(\\mbox{Important})$ and the system seems to be back to normal.  However, downstream your $\\Pr(\\mbox{Spam})$ model just sees the shift and starts under-predicting spam because the upstream importance signal is telling it that they’re likely to be important. The numerical value of the signal became decoupled from the event it was measuring even as the ordinal value remained unchanged. And users may start receiving a lot more spam!<br /><br /><br /><h2 style=\"text-align: left;\">Calibration and other considerations</h2>Calibration is a desirable property, but it is not the only important metric.  Indeed, merely having calibration may not even be helpful for the task at hand.  Consider predicting whether an email is spam.  Assuming 10% of messages are spam, we predict $\\Pr(\\mathrm{Spam}) = 0.1$ for each individual email.  This is well-calibrated but it doesn’t do anything to help your inbox.<br /><br />The problem with the previous example is when we predicted $\\Pr(\\mathrm{Spam}) = 0.1$&nbsp;&nbsp;we didn’t condition on any covariates.  A model that considers whether the email came from a known-address and predicts $\\Pr(\\mathrm{Spam}|\\mathrm{Known\\ Sender}) = 0.01$ and $\\Pr(\\mathrm{Spam}|\\mathrm{Unknown\\ Sender}) = 0.4$ could be perfectly calibrated and also more useful.</div><div><br /></div><div>To examine the difference between these two models, let’s consider the expected quadratic loss function, which we can decompose as$$<br /><br /><br />E[(\\hat{p}-Y)^2] = E[(\\hat{p} - \\pi(\\hat{p}))^2] - E[(\\pi(\\hat{p}) - \\bar{\\pi})^2] +<br /> \\bar{\\pi} (1 -  \\bar{\\pi})<br /><br />$$where $\\pi(\\hat{p})$ is $\\Pr(Y | \\hat{p})$ and $\\bar{\\pi}$ is $\\Pr(Y)$.<br /><br />Let’s examine each of these terms in the decomposition:<br /><ul style=\"text-align: left;\"><li>$E[(\\hat{p} - \\pi(\\hat{p}))^2]$: This term is <b>calibration</b>.  If you have a perfectly calibrated classifier this will be zero and deviations from calibration will hurt the loss.</li><li>$- E[(\\pi(\\hat{p}) - \\bar{\\pi})^2]$: This term is <b>sharpness</b>.  The further your predictions are from the global average the more you improve the loss.</li><li>$\\bar{\\pi} (1 - \\bar{\\pi})$: This is the irreducible loss due to uncertainty.</li></ul>This shows why $\\Pr(\\mathrm{Spam}) = 0.1$&nbsp;isn’t good enough: it optimizes the calibration term, but pays the price in sharpness. And if our model has useful features (known senders are less likely to send spam), the global average model ($\\Pr(\\mathrm{Spam}) = 0.1$) should have a worse quadratic loss than our model.<br /><br />The relationship between calibration and sharpness is complicated.  For instance, we can always coarsen prediction to improve calibration: indeed we can coarsen all the way to the global average and achieve perfect calibration. But is there some intrinsic tradeoff between the two — will calibration always decrease sharpness?</div><div><br />This depends on the nature of the miscalibration, i.e., whether the model is over or under confident.  Overconfidence occurs when the model is too close to the extremes — it predicts something with 99% when it actually happens at 80%.  This is symmetric — it is also over confident when it predicts something at 1% when it actually happens at 20%.  The ultimate overconfident model would just predict 0s or 1s as probabilities.  The opposite problem occurs when the model is underconfident: the ultimate underconfident model might just predict 0.5 (or the global average) for each observation.</div><div><br />If the model is overconfident and too far into the tails, we lose sharpness to improve calibration. If models are under confident and not far enough into the tails, we can improve both calibration and sharpness.  In principle, this means you can end up with either a lower or higher quadratic loss (or other loss functions) for finite samples after implementing the calibration methods we discuss below. In practice, we haven’t observed worse performance, in either the quadratic loss or log loss.<br /><br />Other important losses we consider are accuracy (the proportion of correct classifications)  and discrimination based metrics like AUC.  These are less affected by calibration because they are only functions of the <a href=\"https://blog.revolutionanalytics.com/2016/08/roc-curves-in-two-lines-of-code.html\">ordered probabilities and their labels</a> (assuming you change your threshold for accuracy appropriately).  We discuss below that we can choose calibration functions which keep accuracy and AUC unchanged.<br /><br />This implies that if we care about AUC, but calibration also matters for our application, we can take the shortcut of just picking the best model according to AUC and applying a calibration fix on top of it. In fact, this is exactly our situation in the notifications case-study described in a later section.</div><div><br />How should practitioners integrate calibration into their workflow? First, you should decide if calibration matters for your application. If calibration matters, our recommendation is to follow the paradigm proposed by <a href=\"https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jrssb.pdf\">Gneiting (2007)</a>: pick the best performing model amongst models that are approximately calibrated, where \"approximately calibrated\" is discussed in the next section.</div><br /><h2 style=\"text-align: left;\">How to be calibrated</h2><div>At this point, you may be convinced that you want your model to be calibrated.  The question then becomes, how can you achieve calibration? The natural first step is checking whether you have already achieved calibration.  Practically speaking, we are interested in whether your model is calibrated enough.  We can check this by plotting your predicted probability against your empirical probability for some quantile buckets of your data.<br /><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-d5IfW5Jill8/YH33ycIJpNI/AAAAAAAAfNw/d5ztLjvAiQQIKHlXdDRH3SFZG5as8qwcwCPcBGAYYCw/s742/blog%2B1.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"742\" data-original-width=\"737\" height=\"640\" src=\"https://1.bp.blogspot.com/-d5IfW5Jill8/YH33ycIJpNI/AAAAAAAAfNw/d5ztLjvAiQQIKHlXdDRH3SFZG5as8qwcwCPcBGAYYCw/w636-h640/blog%2B1.png\" width=\"636\" /></a></div><br /><div>Miscalibration will be recognizable as a deviation from the diagonal line that represents perfect calibration. Usually an eye-test suffices to diagnose problems (see above) although you could check more formally with hypothesis testing or thresholding calibration specific metrics.<br /><br />If you discover that your classifier is miscalibrated, you might want to start fixing the classifier itself. We suggest a different approach: view your classifier as a black box and learn a&nbsp;<b>calibration function</b>&nbsp;which transforms your output to be calibrated. The reason you don’t want to adjust your classifier directly is it requires adaptation to your specific method. For instance, a random forest classifier will have different problems than a neural network.<br /><br />The model-as-black-box perspective assumes that fixing the model is intractable analytically. Instead, we just ignore the model’s internal structure and fix things with a method-agnostic approach. This is the same fruitful perspective taken by the jackknife,&nbsp;<a href=\"https://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf\">conformal prediction</a>, and&nbsp;<a href=\"https://www.unofficialgoogledatascience.com/2015/11/using-empirical-bayes-to-approximate.html\">second-order calibration</a>. This gives us the advantage that we can rapidly iterate on the model's structure and features and not have to worry about calibration every time. Of course it comes at the cost of maintaining a separate step, but we’ll show you that calibration functions are not particularly complicated to add/maintain.<br /><br />Finally, a frequent discussion topic is the relationship between calibration and slices. Currently we’re only talking about global calibration: $\\hat{p} = \\Pr(Y | \\hat{p})$. You can also have calibration for a particular slice: $\\hat{p} = \\Pr(Y | \\hat{p}, Z)$ for some covariates $Z$. Calibration for large $Z$ is unlikely, as it’s an even bigger ask than global calibration. In fact, the only model that’s perfectly calibrated across all slices is the true model. However, you might have good reasons for wanting calibration on a few select slices.<br /><br />Like with global calibration, you can calibrate your model on slices/subsets of data. But if you calibrate across too many slices, things can become as complicated as the original model. To keep things manageable, our recommendation is to calibrate globally, and to calibrate a small number of slices that affect important decisions as needed.</div><div><h2 style=\"text-align: left;\">How calibration functions work</h2>A calibration function takes as input the predicted probability $\\hat{p}$ and outputs a calibrated probability $p$.  In this way, you can view it as a single input probabilistic classifier: given $\\hat{p}$ as the sole covariate, can you predict the true probability $p$?<br /><br />Viewed this way, we can start imposing some conditions that will determine how our calibration function is constructed:<br /><ul style=\"text-align: left;\"><li><b>The calibration function should minimize a strictly <a href=\"https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\">proper scoring rule</a></b>.  Strictly proper scoring rules are loss functions such that the unique minimizer is the true probability distribution.  Log-loss and quadratic loss are two such examples.  This ensures that with enough data our calibration function converges to the true probabilities: $\\hat{p} = \\Pr(Y | \\hat{p})$.  </li><li><b>The calibration function should be strictly monotonic</b>. It doesn’t feel intuitive to flip predictions if the model suggests one is more likely. Additionally, a monotonic calibration function preserves the ranking of predictions: this means that AUC isn’t affected (indeed you can estimate AUC and train your calibration function on the same data: more on that later).</li><li><b>The calibration function should be flexible</b>. Miscalibration may not fit a specific parametric form so we need a non-parametric model.</li><li><b>The calibration function needs to be trained on independent data</b>. Otherwise you might be vulnerable to extreme overfitting: your model could be too confident predicting close to zero and one and then your calibration function makes it even more overconfident.</li><li><b>Calibration functions should be implemented in Tensorflow</b>. This is an extra Google requirement because many of our important models are implemented in Tensorflow.  Keeping tooling the same allows easy collaboration and coordination.  Also, implementing a calibration function into the same graph as the original model simplifies the training process: you can use <a href=\"https://www.tensorflow.org/api_docs/python/tf/stop_gradient\">stop-gradients</a> to only train the calibration function in stage two.  Similarly it simplifies the serving process as it’s only another layer in the original graph.</li></ul></div><br /><h2 style=\"text-align: left;\">Calibration Methods</h2>With these requirements in mind, let’s consider some traditional calibration methods.<br /><br />The first method is Platt’s scaling which uses a logistic regression as the calibration function.  This is easy to fit, but it violates our requirement for flexibility.  As a parametric function it doesn’t flexibly adapt to more complicated calibration curves.<br /><br />Isotonic regression solves this problem by switching from logistic regression to fully nonparametric regression.  This is almost everything we would want from a calibration method.  However, it does have two downsides.  First, it’s not strictly monotonic: the model will output piecewise continuous functions that lead to ties and thus affects AUC.  Second, isotonic regression is hard to fit into Tensorflow.<br /><br />With both Platt’s scaling and isotonic regression failing to satisfy all our requirements, we need another method. Taking a step back, it’s clear we simply need a strictly monotonic regression function that is easy to fit in Tensorflow. This gives us two potential candidates: <a href=\"https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html\">Tensorflow Lattice</a> and <a href=\"https://rpubs.com/deleeuw/268327\">I-Splines (ISplines)</a> (monotonic neural nets are another option, but they have not worked as well in our experience) . Tensorflow Lattice is described in the previous <a href=\"https://www.unofficialgoogledatascience.com/2020/11/adding-common-sense-to-machine-learning.html\">blog post</a> so we will focus on I-Splines here.<div><br /><h3 style=\"text-align: left;\">I-Spline Calibration</h3>Splines are piecewise polynomial functions which, amongst other applications, are used to learn nonparametric regression functions. Splines can be constructed as a linear combination of basis functions:$$<br /><br />\\Pr(y | x) = \\sum_{i = 1}^R \\beta_i \\phi_i(x)<br /><br />$$ where the basis functions $\\phi_i(x)$ are pre-specified. To fit splines, we learn the weights $\\beta_i$.</div><div><br /></div>The most common spline functions are B-Splines. B-splines are popular for computational purposes, since they can be defined recursively as:$$<br /><br />B_{i,1}(x) =<br />\\begin{cases}<br />1 &amp; t_i \\leq x &lt; t_{i+1} \\\\<br />0 &amp; \\mbox{otherwise}<br />\\end{cases}<br />\\\\<br />B_{i,k+1}(x) = <br />\\frac{x-t_i}{t_{i+k} - t_i} B_{i,k}(x) + <br />\\frac{t_{i+1+k}-x}{t_{i+1+k} - t_{i+1}} B_{i+1,k}(x)<br /><br /><br />$$where $k$ represents the order of the polynomial function, and $T = [t_1, t_2, …, t_k]$ is a set of non-decreasing knots where the piecewise polynomials connect.<div><br />B-Splines don't quite work for calibration functions, since they’re not monotonic. Fortunately, you can make splines monotonic with Integrated Splines, or <a href=\"https://en.wikipedia.org/wiki/I-spline\">I-Splines</a>. The idea behind I-Splines is that, since properly normalized B-Splines only take positive values, the integral of a B-Spline is always increasing. And if we pair increasing functions with positive weights, we achieve monotonicity.</div><div><br /></div>Less well known, though pointed out by <a href=\"https://rpubs.com/deleeuw/268327\">Jan de Leeuw</a> in an excellent article, is that I-Splines are also easy to compute. De Leeuw shows (section 4.1.2) that you can compute I-Splines directly from B-Splines:$$<br /><br />I_{j,m} = \\sum_{l=j}^R B_{l,m+1}(x)<br /><br /><br />$$where $R$ is the number of spline functions. This formula shows that we can compute I-Splines using reversed cumulative sums of B-Splines.<br /><br />Putting it all together:<br /><div><ol style=\"text-align: left;\"><li>We can evaluate the basis functions using the closed form expressions for I-splines given in this section.</li><li>We achieve monotonicity by restricting our weights to be positive. In practice, we enforce this by optimizing over $\\log(\\beta_i)$.</li><li>We can optimize the weights using a proper scoring rule, such as log loss, or MSE.</li></ol>All of this can be achieved in Tensorflow. And since I-Splines are flexible non-parametric functions, I-Spline calibration meets all of our requirements for a calibration function.<br /><br />Now that we understand how calibration functions work, and a few of our options, let’s look at a practical example.<br /><br /><br /><h2 style=\"text-align: left;\">Case Study:  Notifications Opt Out Model</h2>At Google, one way we interact with users is through notifications. We want to send users notifications they find valuable, but we don’t want to annoy users with too many notifications.<br /><br />To determine which notifications to send, we use models to predict whether or not a user will dismiss, opt out, or otherwise react to a notification in a negative way. Then, we use the predictions to decide whether or not to send the notification by assuring that the probability of negative reactions is minimal. This is sometimes done using a combination of ML model outputs, with coefficients that have been tuned through offline simulations.<br /><br />One model that’s particularly challenging is the $\\Pr(\\mbox{Opt Out})$ model. The $\\Pr(\\mbox{Opt Out})$&nbsp;model predicts whether a user will opt out of receiving notifications if a particular notification is sent. Since opt outs can be a clear negative signal from a user, large $\\Pr(\\mbox{Opt Out})$&nbsp;predictions prevent us from sending annoying notifications.<br /><br />A difficulty with predicting opt outs is that they’re rare, so the model suffers from class imbalance issues. To improve model performance in class imbalanced problems, a standard trick is re-weighting the data to emphasize examples from the minority class. But re-weighting based on the class changes the distribution of the predicted probabilities, which leads to miscalibration. In this case, calibration functions automatically compensate for scaling issues. An added bonus is that calibration functions work for any re-weighting method, so engineers can quickly iterate on new methods.<br /><br />Let’s start by looking at the reliability diagram for various calibration methods. The calibration methods we’ll try here are Platt Scaling, Isotonic Regression, and I-Splines. Also, since this model iteration re-weights the Opt Outs based on a set of features, our baseline method is to invert the weights computed from the features, which we include here as well.</div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-g7MF60ii-K4/YH33yWCN2PI/AAAAAAAAfN4/vLvEjw3ghtcPWRFIwxaDFhb5WjsS66ppwCPcBGAYYCw/s904/blog%2B2.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"904\" data-original-width=\"862\" height=\"640\" src=\"https://1.bp.blogspot.com/-g7MF60ii-K4/YH33yWCN2PI/AAAAAAAAfN4/vLvEjw3ghtcPWRFIwxaDFhb5WjsS66ppwCPcBGAYYCw/w610-h640/blog%2B2.png\" width=\"610\" /></a></div><br /><div>This plot shows the flaw in Platt Scaling. In this case, the empirical calibration curve was more complex than a sigmoid, and Platt scaling wasn’t flexible enough to adapt, and converged to essentially a constant function. We also observe that simply inverting the weights leads to miscalibrated predictions. In this case, the smaller predictions under predict, and the larger predictions over-predict. Fortunately, both I-Splines and Isotonic Regression are flexible enough to learn the more complex calibration curve, which leads to calibrated predictions.<br /><br />Let’s take a closer look at the calibration functions, which reveals a more about the differences between Isotonic Regression and I-Splines:<br /></div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9mQYhl4KR_M/YH33yQHvphI/AAAAAAAAfN0/qmhRa1HUtvYZqmIQmodNyia54X5piNtaACPcBGAYYCw/s904/blog%2B3.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"904\" data-original-width=\"862\" height=\"640\" src=\"https://1.bp.blogspot.com/-9mQYhl4KR_M/YH33yQHvphI/AAAAAAAAfN0/qmhRa1HUtvYZqmIQmodNyia54X5piNtaACPcBGAYYCw/w610-h640/blog%2B3.png\" width=\"610\" /></a></div><br /><div>You can see that I-Splines and Isotonic Regression learn essentially the same calibration function. The main difference is that I-Splines are smooth, and Isotonic Regression is piecewise constant.<br /><br />Finally, we mentioned earlier that when calibration functions are strictly monotonic, applying them leaves AUC unchanged. We can confirm this observation from the following table, which shows that AUC for Platt Scaling and I-Splines are identical up to 8 decimal places. In this case, Isotonic regression isn’t quite the same due to ties:</div><div><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-3MB3iNolhz0/YH8MML1yotI/AAAAAAAAfOA/K933xxQyIL0rg0_9h-vQGmki7_j3VazLwCNcBGAsYHQ/s596/blog%2B4.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"175\" data-original-width=\"596\" src=\"https://1.bp.blogspot.com/-3MB3iNolhz0/YH8MML1yotI/AAAAAAAAfOA/K933xxQyIL0rg0_9h-vQGmki7_j3VazLwCNcBGAsYHQ/s16000/blog%2B4.png\" /></a></div><div><span><span style=\"font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;\"><br /></span></span></div><div><span id=\"docs-internal-guid-873c2650-7fff-4c04-2017-25f4f1847545\"><span style=\"font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;\">In sum, this case study shows how you can use a calibration function to calibrate a tricky real-world problem. It also shows how flexible calibration functions are, since they work for arbitrary re-weighting schemes, extreme scales, and complicated shapes.<br /><br />We hope this example has convinced you that when your model gets too complicated, and you want to iterate on methods quickly, it’s ok to stop worrying and use a calibration function.</span></span></div><div><span><span style=\"font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;\"><br /></span></span><h2 style=\"text-align: left;\"><span><span style=\"font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;\">Conclusion</span></span></h2><span><span style=\"font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline;\">Calibration is an intuitively appealing property, and also has many practical benefits in complex ML systems and real-world environments. Despite its importance, there are many ways in which a model can end up not being calibrated.  Instead of focusing on fixing the model, we can treat the model as a blackbox and achieve calibration with a calibration function.  We can use any 1D monotonic regression function, but two that we’ve found to work well are I-Splines and piecewise linear functions from TF-Lattice.</span></span></div><div><br /></div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Amir Najmi",
    "uri": "http://www.blogger.com/profile/18174523203317227640",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}