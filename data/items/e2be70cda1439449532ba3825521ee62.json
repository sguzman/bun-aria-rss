{
  "title": "OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks. (arXiv:2210.12924v2 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2210.12924",
  "description": "<p>The size of deep neural networks has grown exponentially in recent years.\nUnfortunately, hardware devices have not kept pace with the rapidly increasing\nmemory requirements. To cope with this, researchers have turned to techniques\nsuch as spilling and recomputation, which increase training time, or reduced\nprecision and model pruning, which can affect model accuracy. We present OLLA,\nan algorithm that optimizes the lifetime and memory location of the tensors\nused to train neural networks. Our method reduces the memory usage of existing\nneural networks, without needing any modification to the models or their\ntraining procedures. We formulate the problem as a joint integer linear program\n(ILP). We present several techniques to simplify the encoding of the problem,\nand enable our approach to scale to the size of state-of-the-art neural\nnetworks using an off-the-shelf ILP solver. We experimentally demonstrate that\nOLLA only takes minutes if not seconds to allow the training of neural networks\nusing one-third less memory on average.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegarty_J/0/1/0/all/0/1\">James Hegarty</a>"
}