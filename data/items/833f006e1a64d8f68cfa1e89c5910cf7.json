{
  "title": "Pickle isn't slow, it's a protocol",
  "link": "",
  "updated": "2018-07-23T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/07/23/protocols-pickle",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc</a></em></p>\n\n<p><strong>tl;dr:</strong> <em>Pickle isn’t slow, it’s a protocol.  Protocols are important for\necosystems.</em></p>\n\n<p>A recent Dask issue showed that using Dask with PyTorch was\nslow because sending PyTorch models between Dask workers took a long time\n(<a href=\"https://github.com/dask/dask-ml/issues/281\">Dask GitHub issue</a>).</p>\n\n<p>This turned out to be because serializing PyTorch models with pickle was very\nslow (1 MB/s for GPU based models, 50 MB/s for CPU based models).  There is no\narchitectural reason why this needs to be this slow.  Every part of the\nhardware pipeline is much faster than this.</p>\n\n<p>We could have fixed this in Dask by special-casing PyTorch models (Dask has\nit’s own optional serialization system for performance), but being good\necosystem citizens, we decided to raise the performance problem in an issue\nupstream (<a href=\"https://github.com/pytorch/pytorch/issues/9168\">PyTorch Github\nissue</a>).  This resulted in a\nfive-line-fix to PyTorch that turned a 1-50 MB/s serialization bandwidth into a\n1 GB/s bandwidth, which is more than fast enough for many use cases (<a href=\"https://github.com/pytorch/pytorch/pull/9184\">PR to\nPyTorch</a>).</p>\n\n<div class=\"language-diff highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>     def __reduce__(self):\n<span class=\"gd\">-        return type(self), (self.tolist(),)\n</span><span class=\"gi\">+        b = io.BytesIO()\n+        torch.save(self, b)\n+        return (_load_from_bytes, (b.getvalue(),))\n</span>\n\n<span class=\"gi\">+def _load_from_bytes(b):\n+    return torch.load(io.BytesIO(b))\n</span></code></pre></div></div>\n\n<p>Thanks to the PyTorch maintainers this problem was solved pretty easily.\nPyTorch tensors and models now serialize efficiently in Dask or in <em>any other\nPython library</em> that might want to use them in distributed systems like\nPySpark, IPython parallel, Ray, or anything else without having to add\nspecial-case code or do anything special.  We didn’t solve a Dask problem, we\nsolved an ecosystem problem.</p>\n\n<p>However before we solved this problem we discussed things a bit.  This comment\nstuck with me:</p>\n\n<p><a href=\"https://github.com/pytorch/pytorch/issues/9168#issuecomment-402514019\">\n  <img src=\"https://mrocklin.github.io/blog/images/pytorch-pickle-is-slow-comment.png\" alt=\"Github Image of maintainer saying that PyTorch's pickle implementation is slow\" width=\"100%\" /></a></p>\n\n<p>This comment contains two beliefs that are both very common, and that I find\nsomewhat counter-productive:</p>\n\n<ol>\n  <li>Pickle is slow</li>\n  <li>You should use our specialized methods instead</li>\n</ol>\n\n<p>I’m sort of picking on the PyTorch maintainers here a bit (sorry!) but I’ve\nfound that they’re quite widespread, so I’d like to address them here.</p>\n\n<h2 id=\"pickle-is-slow\">Pickle is slow</h2>\n\n<p>Pickle is <em>not</em> slow.  Pickle is a protocol.  <em>We</em> implement pickle.  If it’s slow\nthen it is <em>our</em> fault, not Pickle’s.</p>\n\n<p>To be clear, there are many reasons not to use Pickle.</p>\n\n<ul>\n  <li>It’s not cross-language</li>\n  <li>It’s not very easy to parse</li>\n  <li>It doesn’t provide random access</li>\n  <li>It’s insecure</li>\n  <li>etc..</li>\n</ul>\n\n<p>So you shouldn’t store your data or create public services using Pickle, but\nfor things like moving data on a wire it’s a great default choice if you’re\nmoving strictly from Python processes to Python processes in a trusted and\nuniform environment.</p>\n\n<p>It’s great because it’s as fast as you can make it (up a a memory copy) and\nother libraries in the ecosystem can use it without needing to special case\nyour code into theirs.</p>\n\n<p>This is the change we did for PyTorch.</p>\n\n<div class=\"language-diff highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>     def __reduce__(self):\n<span class=\"gd\">-        return type(self), (self.tolist(),)\n</span><span class=\"gi\">+        b = io.BytesIO()\n+        torch.save(self, b)\n+        return (_load_from_bytes, (b.getvalue(),))\n</span>\n\n<span class=\"gi\">+def _load_from_bytes(b):\n+    return torch.load(io.BytesIO(b))\n</span></code></pre></div></div>\n\n<p>The slow part wasn’t Pickle, it was the <code class=\"language-plaintext highlighter-rouge\">.tolist()</code> call within <code class=\"language-plaintext highlighter-rouge\">__reduce__</code>\nthat converted a PyTorch tensor into a list of Python ints and floats.  I\nsuspect that the common belief of “Pickle is just slow” stopped anyone else\nfrom investigating the poor performance here.  I was surprised to learn that a\nproject as active and well maintained as PyTorch hadn’t fixed this already.</p>\n\n<p><em>As a reminder, you can implement the pickle protocol by providing the\n<code class=\"language-plaintext highlighter-rouge\">__reduce__</code> method on your class.  The <code class=\"language-plaintext highlighter-rouge\">__reduce__</code> function returns a\nloading function and sufficient arguments to reconstitute your object.  Here we\nused torch’s existing save/load functions to create a bytestring that we could\npass around.</em></p>\n\n<h2 id=\"just-use-our-specialized-option\">Just use our specialized option</h2>\n\n<p>Specialized options can be great.  They can have nice APIs with many options,\nthey can tune themselves to specialized communication hardware if it exists\n(like RDMA or NVLink), and so on.  But people need to learn about them first, and\nlearning about them can be hard in two ways.</p>\n\n<h3 id=\"hard-for-users\">Hard for users</h3>\n\n<p>Today we use a large and rapidly changing set of libraries. It’s hard\nfor users to become experts in all of them.  Increasingly we rely on new\nlibraries making it easy for us by adhering to standard APIs, providing\ninformative error messages that lead to good behavior, and so on..</p>\n\n<h3 id=\"hard-for-other-libraries\">Hard for other libraries</h3>\n\n<p>Other libraries that need to interact <em>definitely</em> won’t read the\ndocumentation, and even if they did it’s not sensible for every library to\nspecial case every other library’s favorite method to turn their objects into\nbytes.  Ecosystems of libraries depend strongly on the presence of protocols\nand a strong consensus around implementing them consistently and efficiently.</p>\n\n<h2 id=\"sometimes-specialized-options-are-appropriate\">Sometimes Specialized Options are Appropriate</h2>\n\n<p>There <em>are</em> good reasons to support specialized options.  Sometimes you need\nmore than 1GB/s bandwidth.  While this is rare in general (very few pipelines\nprocess faster than 1GB/s/node), it is true in the particular case of PyTorch\nwhen they are doing parallel training on a single machine with multiple\nprocesses.  Soumith (PyTorch maintainer) writes the following:</p>\n\n<p>When sending Tensors over multiprocessing, our custom serializer actually\nshortcuts them through shared memory, i.e. it moves the underlying Storages\nto shared memory and restores the Tensor in the other process to point to the\nshared memory. We did this for the following reasons:</p>\n\n<ul>\n  <li>\n    <p><strong>Speed:</strong> we save on memory copies, especially if we amortize the cost of\nmoving a Tensor to shared memory before sending it into the multiprocessing\nQueue. The total cost of actually moving a Tensor from one process to another\nends up being O(1), and independent of the Tensor’s size</p>\n  </li>\n  <li>\n    <p><strong>Sharing:</strong> If Tensor A and Tensor B are views of each other, once we\nserialize and send them, we want to preserve this property of them being\nviews. This is critical for neural-nets where it’s common to re-view the\nweights / biases and use them for another.  With the default pickle solution,\nthis property is actually lost.</p>\n  </li>\n</ul>"
}