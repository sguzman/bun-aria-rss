{
  "title": "Easily distributing a parallel IPython Notebook on a cluster",
  "link": "",
  "published": "2014-02-24T09:00:00-05:00",
  "updated": "2014-02-24T09:00:00-05:00",
  "author": {
    "name": "Thomas Wiecki"
  },
  "id": "tag:twiecki.io,2014-02-24:/blog/2014/02/24/ipython-nb-cluster/",
  "summary": "<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>Have you ever asked yourself: \"Do I want to spend 2 days adjusting this analysis to run on the cluster and wait 2 days for the jobs to finish or do I just run it locally with no extra work and just wait a week.\"</p>\n<p>If so, this blog post â€¦</p></div></div></div>",
  "content": "<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>Have you ever asked yourself: \"Do I want to spend 2 days adjusting this analysis to run on the cluster and wait 2 days for the jobs to finish or do I just run it locally with no extra work and just wait a week.\"</p>\n<p>If so, this blog post might be of interest to you. If not, it might still be of interest -- what do I know about your interests? ;) Below I outline my process of running my analyses on the cluster with minimal recoding/adaptation.</p>\n<h2 id=\"Problems-with-the-naive-(common)-approach\">Problems with the naive (common) approach<a class=\"anchor-link\" href=\"#Problems-with-the-naive-(common)-approach\">&#182;</a></h2><p>My main problems with moving my computations to the cluster have\nalways been:</p>\n<ol>\n<li><p><strong>Tedious recoding of my scripts to parallelize them:</strong> Most common cluster systems like SLURM or TORQUE require you to write\n a script that starts each parallel job in a single line. I saw many researchers then\n recode their analysis scripts to parse command line arguments and have hundreds of lines of various parameter values they want to run.</p>\n</li>\n<li><p><strong>Difficult debugging:</strong> I spent too many hours coding my clusterized code, copying them over, submitting a job, waiting for it to schedule, to only then find out that I had an obvious bug. The cycle of seeing the job to start is simply too long.</p>\n</li>\n<li><p><strong>Non-pickleable objects:</strong> I'm sure everyone experimenting with parallel programming in Python stumbled over the problem that everything you want to distribute to your workers has to be pickleable. Unfortunately, most things in Python are not pickleable (no functions, no classes, no objects etc). <a href=\"http://trac.mystic.cacr.caltech.edu/project/pathos/wiki/dill\"><code>dill</code></a> helps with that but in the end is also limited (see <a href=\"http://matthewrocklin.com/blog/work/2013/12/05/Parallelism-and-Serialization/\">here</a> for a blog post discussing this issue).</p>\n</li>\n</ol>\n<h2 id=\"Desiderata-of-setup\">Desiderata of setup<a class=\"anchor-link\" href=\"#Desiderata-of-setup\">&#182;</a></h2><p>If this sounds terrible, it's because it is! \nMy ideal setup thus has the following properties:</p>\n<ol>\n<li>I can make sure everything worked correctly on my local setup first before moving it to the cluster.</li>\n<li>Since I spend my time analysing data exclusively in the <a href=\"http://ipython.org/notebook\">IPython Notebook</a> these days I don't want to switch to a script-based setting just so I can run the analyses on the cluster.</li>\n<li>Finally, I don't want to be limited by what is pickable, so bringing my workers into the correct state should come with minimal extra-work.</li>\n</ol>\n<h2 id=\"Setting-up-the-IPython-Notebook\">Setting up the IPython Notebook<a class=\"anchor-link\" href=\"#Setting-up-the-IPython-Notebook\">&#182;</a></h2><p>We are going to use IPython parallel inside the IPython Notebook (IPyNB). The IPyNB allows me to launch multiple client workers (called engines) from the dashboard on my local machine (i.e. my laptop I do most of my work on). I first launched the engines on my laptop via the dashboard; I mainly need this for testing so I'm not concerned that I only have few cores available. Then, inside the IPython notebook, I can connect to the worker engines via:</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[&nbsp;]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">IPython</span> <span class=\"kn\">import</span> <span class=\"n\">parallel</span>\n<span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">parallel</span><span class=\"o\">.</span><span class=\"n\">Client</span><span class=\"p\">()</span>\n<span class=\"n\">view</span> <span class=\"o\">=</span> <span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">load_balanced_view</span><span class=\"p\">()</span>\n</pre></div>\n\n    </div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>This sets up a <code>view</code> object (see the <a href=\"http://ipython.org/ipython-doc/stable/parallel/parallel_intro.html\">IPython parallel docs</a> for more info) that allows us to distribute our jobs.</p>\n<p>The next issue we solve is that of bringing the workers into the correct state without relying on <code>pickle</code>. IPython has the nifty <code>%%px</code> magic operator for that. What it does is that it executes the cell on all connected clients. If we add <code>--local</code> it also executes it on the main node.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[&nbsp;]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span></span><span class=\"o\">%%</span><span class=\"k\">px</span> --local\n\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nclass MyLongAssComputation(object):\n    def compute(self, x):\n        return x**2\n    \ndef compute_func(x):\n    c = MyLongAssComputation()\n    return c.compute(x)\n</pre></div>\n\n    </div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>This makes sure everyone has access to the necessary modules and any classes or functions I need to execute a job. Obviously this code makes no sense but it's supposed to demonstrate that you can basically set up anything you want on the nodes and refer to them. This fulfills desiderata #3.</p>\n<p>Finally, we want to load any data and distribute it to the workers. This we execute only from the master node (so no magic prefix).</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing code_cell rendered\">\n<div class=\"input\">\n<div class=\"prompt input_prompt\">In&nbsp;[&nbsp;]:</div>\n<div class=\"inner_cell\">\n    <div class=\"input_area\">\n<div class=\" highlight hl-ipython3\"><pre><span></span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">)</span>\n<span class=\"n\">squared</span> <span class=\"o\">=</span> <span class=\"n\">view</span><span class=\"o\">.</span><span class=\"n\">map_sync</span><span class=\"p\">(</span><span class=\"n\">compute_func</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">squared</span><span class=\"p\">,</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;output.pickle&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;w&#39;</span><span class=\"p\">))</span>\n</pre></div>\n\n    </div>\n</div>\n</div>\n\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<p>In this local setup I can easily identify and debug my code. When I want to launch <code>pdb</code> I can easily call the parallel function directly on the master to have full control. I can also easily edit the above code, re-execute the cell to update the workers and retry to distribute the jobs, fulfilling desiderata #1.</p>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Executing-the-IPython-Notebook-on-the-cluster\">Executing the IPython Notebook on the cluster<a class=\"anchor-link\" href=\"#Executing-the-IPython-Notebook-on-the-cluster\">&#182;</a></h2><p>Once I made sure everything functions correctly on my local machine and perhaps a smaller set of data I want to run the full, long job on the cluster going massively parallel. For that I first copy (<code>scp</code>) the notebook to the cluster and then use <code>ssh</code> to connect to the cluster. From hereon out, everything will be happening on the cluster.</p>\n<p>Most clusters run some scheduler that queues and distributes your job. The <a href=\"https://www.ccv.brown.edu/technologies/computing\">Brown cluster</a> runs <a href=\"https://computing.llnl.gov/linux/slurm/\"><code>SLURM</code></a> but others are configured in a similar way so it shouldn't be hard to adapt these scripts.</p>\n<p>You have to write a script with some special comments that instruct the scheduler on how many cores you want (<code>-n</code>), how long the job is supposed to run (<code>--time</code>) etc:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"ch\">#!/bin/sh</span>\n<span class=\"c1\">#SBATCH -J ipython</span>\n<span class=\"c1\">#SBATCH -n 64</span>\n<span class=\"c1\">#SBATCH --time=48:00:00</span>\n</pre></div>\n<p>Next in our script we want to launch the IPython parallel cluster. First, we launch the IPython controller which distributes the jobs. We also specify that it should accept connection from external IPs (<code>--ip='*'</code>) instead of being restricted to <code>localhost</code>:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"nb\">echo</span> <span class=\"s2\">&quot;Launching controller&quot;</span>\nipcontroller --ip<span class=\"o\">=</span><span class=\"s1\">&#39;*&#39;</span> <span class=\"p\">&amp;</span>\nsleep <span class=\"m\">10</span>\n</pre></div>\n<p>We next need to launch our engines. Most schedulers have a command to distribute a shell command on all your reserved nodes. <code>SLURM</code> has <code>srun</code> which will execute the command 64 times (as specified above). You could also use <code>mpirun</code> here.</p>\n<div class=\"highlight\"><pre><span></span>srun ipengine <span class=\"p\">&amp;</span>\nsleep <span class=\"m\">25</span>\n</pre></div>\n<p>Finally, we want to run our IPyNB on the cluster. For this we'd like to just execute an IPyNB like a Python script. Luckily, there are some scripts that do exactly that. For our purposes, <a href=\"https://gist.github.com/minrk/2620876\"><code>checkipnb.py</code></a> does the job. You'll have to download it and save it to the directory.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"nb\">echo</span> <span class=\"s2\">&quot;Launching job&quot;</span>\npython checkipnb.py <span class=\"nv\">$1</span>\n\n<span class=\"nb\">echo</span> <span class=\"s2\">&quot;Done!&quot;</span>\n</pre></div>\n<p>The <code>$1</code> refers to the command line argument of the cluster script so that I can decide which IPyNB to run when I launch the job. That's the script, I now only need to copy the data and the IPyNB to the cluster and submit my job via:</p>\n<div class=\"highlight\"><pre><span></span>sbatch submit_ipython_notebook.sh MyIPyNB.ipynb\n</pre></div>\n<p>(Other schedulers have different commands to submit jobs like <code>qsub</code>).</p>\n<p>What happens next is that <code>SLURM</code> will schedule my job in the queue. Once it gets started with access to 64 cores, the shell script is executed and launches <code>ipcontroller</code> and 64 instances of <code>ipengine</code> which automatically know how to connect and register to the controller. <code>checkipnb.py</code> then executes each cell in <code>MyIPyNB.ipynb</code> consecutively. The <code>%%px</code> cells will bring all the engines into the correct state while the <code>map_sync()</code> call distributes the work. Once the computation is done, the results are saved to the pickle output file which I can then copy back (using <code>scp</code>) to my local setup to analyse.</p>\n<p>What is nice is that I never have to touch the cluster script again. I simply write an IPyNB that computes my analyses in parallel and if the computations become too big I can just copy them to the cluster and launch them there with minimal overhead.</p>\n<p>The full script is as follows (note that I'm running a local <a href=\"http://continuum.io/downloads\">Anaconda</a> install; this really helps as you normally don't have admin access on the cluster and don't want to have to keep asking them to install custom packages):</p>\n<div class=\"highlight\"><pre><span></span><span class=\"ch\">#!/bin/sh</span>\n<span class=\"c1\">#SBATCH -J ipython</span>\n<span class=\"c1\">#SBATCH -n 64</span>\n<span class=\"c1\">#SBATCH --time=48:00:00</span>\n\n<span class=\"nb\">echo</span> <span class=\"s2\">&quot;Launching controller&quot;</span>\n<span class=\"nv\">$HOME</span>/anaconda/bin/ipcontroller --ip<span class=\"o\">=</span><span class=\"s1\">&#39;*&#39;</span> <span class=\"p\">&amp;</span>\nsleep <span class=\"m\">10</span>\n\n<span class=\"nb\">echo</span> <span class=\"s2\">&quot;Launching engines&quot;</span>\nsrun <span class=\"nv\">$HOME</span>/anaconda/bin/ipengine <span class=\"p\">&amp;</span>\nsleep <span class=\"m\">25</span>\n\n<span class=\"nb\">echo</span> <span class=\"s2\">&quot;Launching job&quot;</span>\n<span class=\"nv\">$HOME</span>/anaconda/bin/python checkipnb.py <span class=\"nv\">$1</span>\n\n<span class=\"nb\">echo</span> <span class=\"s2\">&quot;Done!&quot;</span>\n</pre></div>\n\n</div>\n</div>\n</div>\n<div class=\"cell border-box-sizing text_cell rendered\"><div class=\"prompt input_prompt\">\n</div><div class=\"inner_cell\">\n<div class=\"text_cell_render border-box-sizing rendered_html\">\n<h2 id=\"Summary\">Summary<a class=\"anchor-link\" href=\"#Summary\">&#182;</a></h2><p>In this post I demonstrated and explained my cluster setup. The overhead of moving my computations to the cluster is greatly reduced which is what I care most about. I also don't have to route around the problem of pickling and I never have to leave the IPython Notebook.</p>\n<p>I uploaded the scripts to a <a href=\"https://github.com/twiecki/ipython_nb_cluster\">GitHub repository</a>. Perhaps others want to contribute launching scripts for other schedulers. You can find the IPyNB used to write this blog post <a href=\"https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/ipython_parallel_cluster.ipynb\">here</a>.</p>\n<p>Other interesting projects that make parallelization easier:</p>\n<ul>\n<li><a href=\"https://github.com/roryk/ipython-cluster-helper\">ipython-cluster-helper</a></li>\n<li><a href=\"https://github.com/uqfoundation/pyina\">pyina</a></li>\n<li><a href=\"https://github.com/uqfoundation/pathos\">pathos</a></li>\n</ul>\n<p>Thanks to <a href=\"http://research.clps.brown.edu/cchatham/\">Chris Chatham</a> for proof-reading and valuable input.</p>\n\n</div>\n</div>\n</div>",
  "category": ""
}