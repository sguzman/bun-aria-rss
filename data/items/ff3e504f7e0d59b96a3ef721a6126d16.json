{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-1370943694206193273",
  "published": "2022-10-14T11:47:00.009-07:00",
  "updated": "2022-10-21T10:24:04.673-07:00",
  "category": [
    "",
    "",
    ""
  ],
  "title": "UL2 20B: An Open Source Unified Language Learner",
  "content": "<span class=\"byline-author\">Posted by Yi Tay and Mostafa Dehghani, Research Scientists, Google Research, Brain Team</span><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif\" style=\"display: none;\" /><p>Building models that understand and generate natural language well is one the grand goals of machine learning (ML) research and has a direct impact on building smart systems for everyday applications. Improving the quality of language models is a key target for researchers to make progress toward such a goal.   </p><a name='more'></a><p>Most common paradigms to build and train language models use either autoregressive decoder-only architectures (e.g., <a href=\"https://arxiv.org/abs/2204.02311\">PaLM</a> or <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a>), where the model is trained to predict the next word for a given prefix phrase, or span corruption-based encoder-decoder architectures (e.g., <a href=\"https://arxiv.org/abs/1910.10683\">T5</a>, <a href=\"https://arxiv.org/abs/2202.08906\">ST-MoE</a>), where the training objective is to recover the subset of words masked out of the input. On the one hand, T5-like models perform well on <a href=\"https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">supervised fine-tuning</a> tasks, but struggle with few-shot in-context learning. On the other hand, autoregressive language models are great for <a href=\"https://arxiv.org/abs/2005.14165\">open-ended generation</a> (e.g., dialog generation with<a href=\"https://arxiv.org/abs/2201.08239\"> LaMDA</a>) and prompt-based learning (e.g., in-context learning with PaLM), but may perform suboptimally on fine-tuning tasks. Thus, there remains an opportunity to create an effective unified framework for pre-training models. </p><p>In “<a href=\"https://arxiv.org/abs/2205.05131\">Unifying Language Learning Paradigms</a>”, we present a novel language pre-training paradigm called Unified Language Learner (UL2) that improves the performance of language models universally across datasets and setups. UL2 frames different objective functions for training language models as <a href=\"https://arxiv.org/abs/1910.10683\">denoising</a> tasks, where the model has to recover missing sub-sequences of a given input. During pre-training it uses a novel <em>mixture-of-denoisers</em> that samples from a varied set of such objectives, each with different configurations. We demonstrate that models trained using the UL2 framework perform well in a variety of language domains, including prompt-based few-shot learning and models fine-tuned for down-stream tasks. Additionally, we show that UL2 excels in <a href=\"https://en.wikipedia.org/wiki/Natural_language_generation\">generation</a>, <a href=\"https://en.wikipedia.org/wiki/Natural-language_understanding\">language understanding</a>, <a href=\"https://en.wikipedia.org/wiki/Information_retrieval\">retrieval</a>, <a href=\"https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)\">long-text understanding</a> and <a href=\"https://en.wikipedia.org/wiki/Question_answering\">question answering</a> tasks. Finally, we are excited to publicly release the <a href=\"https://github.com/google-research/google-research/tree/master/ul2\">checkpoints</a> for our best performing UL2 20 billion parameter model. </p><div style=\"line-height:40%;\">    <br></div> <h2>Background: Language Modeling Objectives and Architectures</h2><p>Common objective functions for training language models can mostly be framed as learning data transformations that map inputs to targets. The model is conditioned on different forms of input to predict target tokens. To this end, different objectives utilize different properties of the inputs. </p><p>The standard <a href=\"https://arxiv.org/abs/2005.14165\">Causal Language</a> modeling objective (CausalLM) is trained to predict full sequence lengths and so, only recognizes tokens in the target output. The <a href=\"https://arxiv.org/abs/1910.10683\">prefix language modeling</a> objective (PrefixLM) modifies this process by randomly sampling a contiguous span of <em>k</em> tokens from the given tokenized text to form the input of the model, referred to as the “prefix”. The <a href=\"https://arxiv.org/abs/1910.10683\">span corruption</a> objective masks contiguous spans from the inputs and trains the model to predict these masked spans.  </p><p>In the table below, we list the common objectives on which state-of-the-art language models are trained along with different characteristics of the input, i.e., how it is presented to the model. Moreover, we characterize the example efficiency of each objective in terms of the ability of the model for exploiting supervision signals from a single input, e.g., how much of the input tokens contribute to the calculation of the loss. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: 5%; margin-right: 5%;\">      <colgroup>     <col style=\"width: 18%;\"></col>     <col style=\"width: 18%;\"></col>     <col style=\"width: 18%;\"></col>     <col style=\"width: 18%;\"></col>     <col style=\"width: 18%;\"></col>  </colgroup><tbody><tr>  <td style=\"text-align: left;\"><em><b>Objective<br />Function</b></em>   </td>  <td><em><b>Inputs<br />(Bi-directional)</b></em>   </td>  <td><em><b>Targets<br />(Causal)</b></em>   </td>   <td><em><b>Input<br />Properties</b></em>   </td>  <td><em><b>Example<br />Efficiency</b></em>   </td>  </tr>  <tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>  <tr>    <td style=\"text-align: left;\"><em>CausalLM</em>   </td>   <td>none    </td>   <td>text    </td>   <td>N/A    </td>   <td>full seq_len    </td>  </tr>  <tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>     <tr>    <td style=\"text-align: left;\"><em>PrefixLM</em>   </td>   <td>text<br>(up to position <em>k</em>)    </td>   <td>text<br>(after position <em>k</em>)    </td>   <td>contiguous    </td>   <td>seq_len - <em>k</em>   </td>  </tr>  <tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr>  <tr>    <td style=\"text-align: left;\"><em>Span corruption</em>   </td>   <td>masked text    </td>   <td>masked_tokens    </td>   <td><span style=\"font-size: small;\">non-contiguous, may be bi-directional</span></td>   <td><span style=\"font-size: small;\">typically lower than others</span></td>  </tr></tbody></table><br><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td class=\"tr-caption\" style=\"text-align: center;\">Common objectives used in today’s language models. Throughout, “text” indicates tokenized text.</td></tr></tbody></table><p>UL2 leverages the strengths of each of these objective functions through a framework that generalizes over each of them, which enables the ability to reason and unify common pre-training objectives. Based on this framework, the main task for training a language model is to learn the transformation of a sequence of input tokens to a sequence of target tokens. Then all the objective functions introduced above can be simply reduced to different ways of generating input and target tokens. For instance, the PrefixLM objective can be viewed as a transformation that moves a segment of <em>k</em> contiguous tokens from the inputs to the targets. Meanwhile, the span corruption objective is a data transformation that corrupts spans (a subsequence of tokens in the input), replacing them with mask tokens that are shifted to the targets. </p><p>It is worth noting that one can decouple the model architecture and the objective function with which it’s trained. Thus, it is possible to train different architectures, such as the common single stack decoder-only and two-stack encoder-decoder models, with any of these objectives. </p><div style=\"line-height:40%;\">    <br></div> <h2>Mixture of Denoisers</h2><p>The UL2 framework can be used to train a model on a mixture of pre-training objectives and supply it with capabilities and inductive bias benefits from different pre-training tasks. Training on the mixture helps the model leverage the strengths of different tasks and mitigates the weaknesses of others. For instance, the mixture-of-denoisers objective can strongly improve the prompt-based learning capability of the model as opposed to a span corruption-only T5 model.  </p><p>UL2 is trained using a mixture of three denoising tasks: (1) <em>R-denoising</em> (or regular span corruption), which emulates the standard T5 span corruption objective; (2) <em>X-denoising</em> (or extreme span corruption); and (3) <em>S-denoising</em> (or sequential PrefixLM). During pre-training, we sample from the available denoising tasks based on user-specified ratios (i.e., different combinations of the R, X, and S-denoisers) and prepare the input and target appropriately. Then, a paradigm token is appended to the input (one of <code>[R]</code>, <code>[X]</code>, or <code>[S]</code>) indicating the denoising task at hand. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s1300/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"500\" data-original-width=\"1300\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoRWMTOf1JUl345eb5BqKEPTRRxPvzPdzvspKtqlwNHqo4BVq98MJYkvEVPZAPdYmLaFMLQKAolOdzKD3uzbYTdYM8S9Z-y5BXgy6kotdukG8w9VCkrZt3Vb0H-BEDp8XC5bGIsA_OEQPWWll1vNRZbSBwJWowTCTf9cnW-7fDOXT8MmyH5s8KzieCQg/s16000/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An overview of the denoising objectives used in UL2’s mixture-of-denoisers.</td></tr></tbody></table> <h2>Improving Trade-Offs Across Learning Paradigms</h2><p>Many existing commonly used language learning paradigms typically excel at one type of task or application, such as fine-tuning performance or prompt-based in-context learning. In the plot below, we show baseline objective functions on different tasks compared to UL2: CausalLM (<em>referred to as GPT-like</em>), PrefixLM, Span Corrupt (<em>also referred to as T5 in the plot</em>), and a baseline objective function proposed by <a href=\"https://arxiv.org/abs/1905.03197\">UniLM</a>. We use these objectives for training decoder only architectures (green) and encoder-decoder architectures (blue) and evaluate different combinations of objective functions and architectures on two main sets of tasks:  </p><ol><li>Fine-tuning, by measuring performance on <a href=\"https://super.gluebenchmark.com/\">SuperGLUE</a> (y-axis of the plot below) </li>  <li>In-context learning, by measuring performance of the model on a suite of 1-shot <a href=\"https://gem-benchmark.com/\">GEM tasks</a> (e.g., <a href=\"https://arxiv.org/abs/1808.08745\">XSUM</a>, <a href=\"https://arxiv.org/abs/1909.05855\">SGD or Schema guided dialog</a> and <a href=\"https://arxiv.org/abs/2004.14373\">TOTTO</a>) (x-axis of the plot below).  </li></ol><p>For most of the existing language learning paradigms, there is a trade-off between the quality of the model on these two sets of tasks. We show that UL2 bridges this trade-off across in-context learning and fine-tuning.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/s1004/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"868\" data-original-width=\"1004\" height=\"346\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiozftwuxITX87OmCkAwkBouHRkjmpZHlfHCZYxRdp6_E5rLigiia3l1JlxvSnhih67iQ_CI1lQmtfffvuXNLGhuO5rFsrifmT1rk5wfLTCKcYK-6ngoendoOUzqUP1SENoQs9WvB-nsu7QDgha57NZXVMU6OpxOrbu9Mh4qKzsE3t6a0BGhlyMYhSLkw/w400-h346/image1.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">In both decoder-only and encoder-decoder setups, UL2 strikes a significantly improved balance in performance between fine-tuned discriminative tasks and prompt-based 1-shot open-ended text generation compared to previous methods. (All models are comparable in terms of computational costs, i.e., FLOPs (EncDec models are 300M and Dec models are 150M parameters).</td></tr></tbody></table> <h2>UL2 for Few-Shot Prompting and Chain-of-Thought Reasoning</h2><p>We scale up UL2 and train a 20 billion parameter encoder-decoder model on the public <a href=\"https://www.tensorflow.org/datasets/catalog/c4\">C4 corpus</a> and demonstrate some impressive capabilities of the UL2 20B model.  </p><p>UL2 is a powerful in-context learner that excels at both few-shot and <a href=\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\">chain-of-thought</a> (CoT) prompting. In the table below, we compare UL2 with other state-of-the-art models (e.g, <a href=\"https://arxiv.org/abs/1910.10683\">T5 XXL</a> and <a href=\"https://arxiv.org/abs/2204.02311\">PaLM</a>) for few-shot prompting on the XSUM summarization dataset. Our results show that UL2 20B outperforms PaLM and T5, both of which are in the same ballpark of compute cost. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: 17%; margin-right: 17%;\">  <colgroup>     <col style=\"width: 18%;\"></col>     <col style=\"width: 16%;\"></col>     <col style=\"width: 16%;\"></col>     <col style=\"width: 16%;\"></col>  </colgroup>  <tbody><tr>    <td style=\"text-align: left;\"><em><b>Model</b></em>   </td>   <td><em><b>ROUGE-1</b></em>   </td>   <td><em><b>ROUGE-2</b></em>   </td>   <td><em><b>ROUGE-L</b></em>   </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>LaMDA 137B</em>   </td>   <td>–    </td>   <td>5.4    </td>   <td>–    </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>PaLM 62B</em>     </td>   <td>–    </td>   <td>11.2    </td>   <td>–    </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>PaLM 540B</em>   </td>   <td>–    </td>   <td><strong>12.2</strong>   </td>   <td>–    </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>PaLM 8B</em>     </td>   <td>–     </td>   <td>4.5    </td>   <td>–     </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>T5 XXL 11B</em>   </td>   <td>0.6    </td>   <td>0.1    </td>   <td>0.6     </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>T5 XXL 11B + LM</em>   </td>   <td>13.3    </td>   <td> 2.3    </td>   <td> 10.7     </td>  </tr>  <tr>    <td style=\"text-align: left;\"><em>UL2 20B</em>   </td>   <td><strong>25.5</strong>   </td>   <td><strong>8.6</strong>   </td>   <td><strong>19.8</strong>   </td>  </tr></tbody></table><br><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Comparison of UL2 with <a href=\"https://arxiv.org/abs/1910.10683\">T5 XXL</a>, <a href=\"https://arxiv.org/abs/2204.02311\">PaLM</a> and <a href=\"https://arxiv.org/abs/2201.08239\">LamDA 137B</a> on 1-shot summarization (<a href=\"https://arxiv.org/abs/1808.08745\">XSUM</a>) in terms of <a href=\"https://arxiv.org/abs/1808.08745\">ROUGE-1/2/L</a> (higher is better), which captures the quality by comparing the generated summaries with the gold summaries as reference.</td></tr></tbody></table><p>Most CoT prompting results have been obtained using much larger language models, such as GPT-3 175B, PaLM 540B, or LaMDA 137B. We show that reasoning via CoT prompting can be achieved with UL2 20B, which is both publicly available and several times smaller than prior models that leverage chain-of-thought prompting. This enables an open avenue for researchers to conduct research on CoT prompting and reasoning at an accessible scale. In the table below, we show that for UL2, CoT prompting outperforms standard prompting on math word problems with a range of difficulties (<a href=\"http://go/arxiv/2110.14168\">GSM8K</a>, <a href=\"https://arxiv.org/abs/2103.07191\">SVAMP</a>, <a href=\"https://aclanthology.org/2020.acl-main.92/\">ASDiv</a>, <a href=\"https://aclanthology.org/P17-1015\">AQuA</a>, and <a href=\"https://aclanthology.org/N16-1136/\">MAWPS</a>). We also show that <a href=\"https://arxiv.org/abs/2203.11171\">self-consistency</a> further improves performance. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s1760/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"874\" data-original-width=\"1760\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikv4KU6NqBBwsQqHUvtvfBOFK9Tkly0AZzl0p-JQTtpVgWhqwtHguYYYB-jZdvB0zdsVZRKZkEStnNKHPqDE-U7wnJWXseLGaSmq48fwEN-eoN_1lmx5lFvTYBij9eVYNm0y62Hy1UXrLBs-lqN13dEXhBTI1Pg8oJWvGx03tHeQVUGKJ6YUjWAEgQMg/s16000/image2.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Chain-of-thought (CoT) prompting and <a href=\"https://arxiv.org/abs/2203.11171\">self-consistency</a> (SC) results on five arithmetic reasoning benchmarks.</td></tr></tbody></table>  <h2>Conclusion and Future Directions</h2><p>UL2 demonstrates superior performance on a plethora of fine-tuning and few-shot tasks. We <a href=\"https://github.com/google-research/google-research/tree/master/ul2\">publicly release</a> checkpoints of our best performing UL2 model with 20 billion parameters, which we hope will inspire faster progress in developing better language models in the machine learning community as a whole.  </p><div style=\"line-height:40%;\">    <br></div> <h2>Acknowledgements</h2><p><em>It was an honor and privilege to work on this with Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby and Donald Metzler. We further acknowledge Alexey Gritsenko, Andrew M. Dai, Jacob Devlin, Jai Gupta, William Fedus, Orhan Firat, Sebastian Gerhmann, Nan Du, Dave Uthus, Siamak Shakeri, Slav Petrov and Quoc Le for support and discussions. We thank the Jax and T5X team for building such wonderful infrastructure that made this research possible. </em></p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}