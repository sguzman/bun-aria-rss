{
  "title": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v5 [cs.LG] UPDATED)",
  "link": "http://arxiv.org/abs/2201.12433",
  "description": "<p>Methods for training models on graphs distributed across multiple clients\nhave recently grown in popularity, due to the size of these graphs as well as\nregulations on keeping data where it is generated, like GDPR in the EU.\nHowever, a single connected graph cannot be disjointly partitioned onto\nmultiple distributed clients due to the cross-client edges connecting graph\nnodes. Thus, distributed methods for training a model on a single graph incur\neither significant communication overhead between clients or a loss of\navailable information to the training. We introduce the Federated Graph\nConvolutional Network (FedGCN) algorithm, which uses federated learning to\ntrain GCN models for semi-supervised node classification on large graphs with\nfast convergence and little communication. Compared to prior methods that\nrequire communication among clients at each training round, FedGCN clients only\ncommunicate with the central server in one pre-training step, greatly reducing\ncommunication costs. We theoretically analyze the tradeoff between FedGCN's\nconvergence rate and communication cost under different data distributions and\nintroduce a general framework that can be used for analysis of all\nedge-completion-based GCN training algorithms. Experimental results show that\nour FedGCN algorithm achieves 51.7% faster convergence on average and at least\n100X less communication cost compared to prior work.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuhang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weizhao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Srivatsan Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1\">Carlee Joe-Wong</a>"
}