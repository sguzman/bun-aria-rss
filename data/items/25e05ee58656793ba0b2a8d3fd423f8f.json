{
  "title": "NBA Twitter, Emojis, and Word Embeddings",
  "description": "<!--kg-card-begin: markdown--><p>A few weeks ago I read this <a href=\"http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji\">blog post</a> from the Instagram engineering team on machine learning and emoji trends. The post talked about general emoji usage over time on Instagram and then used <a href=\"http://arxiv.org/pdf/1309.4168.pdf\">word2vec</a> an algorithm that uses a unsupervised learning process to read through a corpus of text</p>",
  "link": "https://www.danielforsyth.me/nba-twitter-emojis-and-word-embeddings/",
  "guid": "5d24af011e693a0017f5e5f9",
  "dc:creator": "Daniel Forsyth",
  "pubDate": "Wed, 17 Jun 2015 11:59:01 GMT",
  "content:encoded": "<!--kg-card-begin: markdown--><p>A few weeks ago I read this <a href=\"http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji\">blog post</a> from the Instagram engineering team on machine learning and emoji trends. The post talked about general emoji usage over time on Instagram and then used <a href=\"http://arxiv.org/pdf/1309.4168.pdf\">word2vec</a> an algorithm that uses a unsupervised learning process to read through a corpus of text and is then able to predict the context around a given word or emoji. The famous example of this word embedding method is vector['king'] - vector['man] + vector['woman'] = ['queen']. When I first read this post the NBA playoffs had recently started so I decided I would collect tweets pertaining to the playoffs and then try and use the same techniques Instagram used on the tweet dataset.</p>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/curry.jpg\" alt loading=\"lazy\"></p>\n<p>I used the same process I used to <a href=\"https://www.danielforsyth.me/analyzing-a-nhl-playoff-game-with-twitter/\">collect tweets about an NHL game</a> for this project however for the search terms I used the name of every starting player on each team that remained in the playoffs on May 11th. Over the last thirty six days I was able to collect <strong>8,520,786</strong> tweets. This was up until the beginning of game six of the finals. Once I had all of the tweets on my local machine I was able to start analyzing them.</p>\n<p>The first thing I wanted to do was create a pandas dataframe of all of the tweets and then drop all duplicate tweets, retweets, and tweets that contained a URL.</p>\n<pre><code>import pandas as pd\npd.options.display.max_colwidth  = 0\ndf = pd.read_csv('/Users/danielforsyth/desktop/tweets.csv', encoding='utf-8')\n\ntweets = df['text'].drop_duplicates().values.tolist()\n\ntweets  = [x for x in tweets if not x.startswith('RT')]\n\ntweets = [i for i in tweets if not ('http://' in i or 'https://' in i)]\n</code></pre>\n<p>Now that I had a filtered list of tweets I wanted to find all of the emojis used in the tweets. First I used a regex pattern to capture all of the emojis and then created a pandas series to find the top 25 most used emojis.</p>\n<pre><code>import re\ntry:\n    # UCS-4\n    e = re.compile(u'[\\U00010000-\\U0010ffff]')\nexcept re.error:\n    # UCS-2\n    e = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n\nemojis = []\nfor x in tweets:\n    match  = e.search(x)\n    if match:\n        emojis.append(match.group())\n\ndfe =  pd.DataFrame(emojis,columns=['text'])\npd.Series(' '.join(dfe['text']).lower().split()).value_counts()[:25]\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-16-07-PM-1.png\" alt loading=\"lazy\"></p>\n<p>The 'face with tears of joy' emoji came in first place being used 67,884 times. This is interesting because this emoji was also the most used in Instagram's results of more than 50 millions comments and captions.</p>\n<p>The final thing I wanted to do was create a word2vec model using the tweets and see what type of results I could find. I used the <a href=\"https://radimrehurek.com/gensim/index.html\">genism library</a> which makes setting up a model pretty trivial.</p>\n<pre><code>wt = [list(x.split()) for x in tweets]\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 400    # Word vector dimensionality                      \nmin_word_count = 5   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\nfrom gensim.models import word2vec\nprint \"Training model...\"\nmodel = word2vec.Word2Vec(wt, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\nmodel.init_sims(replace=True)\n</code></pre>\n<p>After creating the model, which could take some time depending on your machine and the size of the dataset, you can start performing some various syntactic/semantic NLP word tasks.</p>\n<pre><code>model.most_similar('&#x1F3C6;'.decode('utf-8'),topn=15)\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-35-14-PM.png\" alt loading=\"lazy\"></p>\n<pre><code>model.most_similar('dunk',topn =15)\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-40-41-PM.png\" alt loading=\"lazy\"></p>\n<p>You can see there is quite a few references to this Marreese Speights missed dunk:</p>\n<iframe src=\"http://streamable.com/e/ttq9\" width=\"560\" height=\"311\" frameborder=\"0\" allowfullscreen webkitallowfullscreen mozallowfullscreen scrolling=\"no\"></iframe>\n<pre><code>model.most_similar('chef',topn=15)\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-50-53-PM.png\" alt loading=\"lazy\"></p>\n<blockquote>\n<p>\"Been cookin' with the sauce, chef, curry with the pot, boy 360 with the wrist, boy.\" -Drake</p>\n</blockquote>\n<p>What I found to be the most interesting was what happened when I ran the <code>similarity</code> method with both MVP candidates and the word 'MVP'.</p>\n<pre><code>model.similarity('james', 'mvp')\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-48-36-PM-1.png\" alt loading=\"lazy\"></p>\n<pre><code>model.similarity('curry', 'mvp')\n</code></pre>\n<p><img src=\"https://www.danielforsyth.me/content/images/2015/06/Screen-Shot-2015-06-16-at-10-49-33-PM.png\" alt loading=\"lazy\"></p>\n<p>It looks like twitter believes Stephen Curry is more deserving of the finals MVP trophy.</p>\n<p>It was very interesting using word2vec for the first time as it was quite easy to setup and get results compared to other NLP techniques. In the future I would like to spend some more time working with word2vec particularly with a different and larger dataset.</p>\n<p>If you have any questions, feedback, advice, or corrections please get in touch with me on <a href=\"https://twitter.com/Daniel_Forsyth1\">Twitter</a> or email me at <a href=\"mailto:danforsyth1@gmail.com\">danforsyth1@gmail.com</a>.</p>\n<!--kg-card-end: markdown-->"
}