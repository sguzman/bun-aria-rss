{
  "title": "Finds in arxiv, october",
  "link": "https://mirror2image.wordpress.com/2013/10/29/finds-in-arxiv-october/",
  "dc:creator": "mirror2image",
  "pubDate": "Tue, 29 Oct 2013 09:42:43 +0000",
  "category": [
    "arxiv",
    "computer vision",
    "compressed sensing",
    "Deep Learning",
    "Math",
    "optimization"
  ],
  "guid": "http://mirror2image.wordpress.com/?p=1124",
  "description": "This is duplication of my ongoing G+ series of post on interesting for me papers in arxiv. Older posts are not here but in my G+ thread. Finds in #arxiv : *Optimization, numerical & convex, ML* The Linearized Bregman Method via Split Feasibility Problems: Analysis and Generalizations Reformulation of  Split Bregman/ ADMM as split feasibility [&#8230;]",
  "content:encoded": "<p>This is duplication of my ongoing <a title=\"googleplus\" href=\"https://plus.google.com/117183349381733551173/posts\">G</a>+ series of post on interesting for me papers in arxiv. Older posts are not here but in my <a title=\"google plus\" href=\"https://plus.google.com/117183349381733551173/posts\">G+</a> thread.</p>\n<p>Finds in #arxiv :<br />\n*Optimization, numerical & convex, ML*<br />\nThe Linearized Bregman Method via Split Feasibility Problems: Analysis and Generalizations<br />\nReformulation of  Split Bregman/ ADMM as split feasibility problem and algorithm/convergence for generalized split feasibility by Bregman projection. This general formulation include both Split Bregman and Kaczmarz (my comment &#8211; randomized Kaczmarz  seems could be here too)<br />\n<a href=\"http://arxiv.org/abs/1309.2094\" rel=\"nofollow\">http://arxiv.org/abs/1309.2094</a></p>\n<p>Stochastic gradient descent and the randomized Kaczmarz algorithm<br />\nHybrid of  randomized Kaczmarz  and stochastic gradient descent  &#8211; into my &#8220;to read&#8221; pile<br />\n<a href=\"http://arxiv.org/abs/1310.5715\" rel=\"nofollow\">http://arxiv.org/abs/1310.5715</a></p>\n<p>Trust&#8211;Region Problems with Linear Inequality Constraints: Exact SDP Relaxation, Global Optimality and Robust Optimization<br />\n&#8220;Extended&#8221; trust region for linear inequalities constrains<br />\n<a href=\"http://arxiv.org/abs/1309.3000\" rel=\"nofollow\">http://arxiv.org/abs/1309.3000</a></p>\n<p>Conic Geometric Programming<br />\nUnifing framwork for conic and geometric programming<br />\n<a href=\"http://arxiv.org/abs/1310.0899\" rel=\"nofollow\">http://arxiv.org/abs/1310.0899</a><br />\n<a href=\"http://en.wikipedia.org/wiki/Geometric_programming\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Geometric_programming</a><br />\n<a href=\"http://en.wikipedia.org/wiki/Conic_programming\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Conic_programming</a></p>\n<p>Gauge optimization, duality, and applications<br />\nAnother big paper about different, not Lagrange duality, introduced by Freund (1987)<br />\n<a href=\"http://arxiv.org/abs/1310.2639\" rel=\"nofollow\">http://arxiv.org/abs/1310.2639</a></p>\n<p>Color Bregman TV<br />\nmu parameters in split bregman made adaptive, to exploit coherence of edges in different color channels<br />\n<a href=\"http://arxiv.org/abs/1310.3146\" rel=\"nofollow\">http://arxiv.org/abs/1310.3146</a></p>\n<p>Iteration Complexity Analysis of Block Coordinate Descent Methods<br />\nSome convergence analysis for BCD and projected gradient BCD<br />\n<a href=\"http://arxiv.org/abs/1310.6957\" rel=\"nofollow\">http://arxiv.org/abs/1310.6957</a></p>\n<p>Successive Nonnegative Projection Algorithm for Robust Nonnegative Blind Source Separation<br />\nNonnegative matrix factorization<br />\n<a href=\"http://arxiv.org/abs/1310.7529\" rel=\"nofollow\">http://arxiv.org/abs/1310.7529</a></p>\n<p>Scaling SVM and Least Absolute Deviations via Exact Data Reduction<br />\nSVN for large-scale problems<br />\n<a href=\"http://arxiv.org/abs/1310.7048\" rel=\"nofollow\">http://arxiv.org/abs/1310.7048</a></p>\n<p>Image Restoration using Total Variation with Overlapping Group Sparsity<br />\nWhile title is promising I have doubt about that paper.  The method authors suggest is equivalent to adding averaging filter to TV-L1 under L1 norm. There is no comparison  to just applying TV-L1 and smoothing filter interchangeably.The method author suggest is very costly, and using median filter instead of averaging would cost the same while obviously more robust.<br />\n<a href=\"http://arxiv.org/abs/1310.3447\" rel=\"nofollow\">http://arxiv.org/abs/1310.3447</a></p>\n<p>*Deep learning*<br />\nDeep and Wide Multiscale Recursive Networks for Robust Image Labeling<br />\n_Open source_ matlab/c package coming soon(not yet available)<br />\n<a href=\"http://arxiv.org/abs/1310.0354\" rel=\"nofollow\">http://arxiv.org/abs/1310.0354</a></p>\n<p>Improvements to deep convolutional neural networks for LVCSR<br />\nconvolutional  networks, droput for speech recognition,<br />\n<a href=\"http://arxiv.org/abs/1309.1501v1\" rel=\"nofollow\">http://arxiv.org/abs/1309.1501v1</a></p>\n<p>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition<br />\nAlready discussed on G+  &#8211; open source framework in &#8220;learn one use everywhere&#8221; stile. Learning done off-line on GPU using ConvNet, and recognition is online in pure python.<br />\n<a href=\"http://arxiv.org/abs/1310.1531\" rel=\"nofollow\">http://arxiv.org/abs/1310.1531</a></p>\n<p>Statistical mechanics of complex neural systems and high dimensional data<br />\nBig textbook-like overview paper on statistical mechanics of learning. I&#8217;ve put it in my &#8220;to read&#8221; pile.<br />\n<a href=\"http://arxiv.org/abs/1301.7115\" rel=\"nofollow\">http://arxiv.org/abs/1301.7115</a></p>\n<p>Randomized co-training: from cortical neurons to machine learning and back again<br />\n&#8220;Selectron&#8221; instead of perception &#8211; neurons are &#8220;specializing&#8221; with weights.<br />\n<a href=\"http://arxiv.org/abs/1310.6536\" rel=\"nofollow\">http://arxiv.org/abs/1310.6536</a></p>\n<p>Provable Bounds for Learning Some Deep Representations<br />\n<a href=\"http://arxiv.org/abs/1310.6343\" rel=\"nofollow\">http://arxiv.org/abs/1310.6343</a><br />\nCitation:&#8221;The current paper presents both an interesting family of denoising autoencoders as</p>\n<div>well as new algorithms to provably learn almost all models in this family.&#8221;</div>\n<p>*Computer vision*<br />\nOnline Unsupervised Feature Learning for Visual Tracking<br />\nSparse representation, overcomplete dictionary<br />\n<a href=\"http://arxiv.org/abs/1310.1690\" rel=\"nofollow\">http://arxiv.org/abs/1310.1690</a></p>\n<p>From Shading to Local Shape<br />\nShape restoration from local shading &#8211; could be very useful in low-feature environment.<br />\n<a href=\"http://arxiv.org/abs/1310.2916\" rel=\"nofollow\">http://arxiv.org/abs/1310.2916</a></p>\n<p>Fast 3D Salient Region Detection in Medical Images using GPUs<br />\nFinding interest point in 3D images<br />\n<a href=\"http://arxiv.org/abs/1310.6736\" rel=\"nofollow\">http://arxiv.org/abs/1310.6736</a></p>\n<p>Object Recognition System Design in Computer Vision: a Universal Approach<br />\nGrid-based universal framework for object detection/classification<br />\n<a href=\"http://arxiv.org/abs/1310.7170\" rel=\"nofollow\">http://arxiv.org/abs/1310.7170</a></p>\n<p>Gaming :)<br />\nLyapunov-based Low-thrust Optimal Orbit Transfer: An approach in Cartesian coordinates<br />\nFor space sim enthusiast :)<br />\n<a href=\"http://arxiv.org/abs/1310.4201\" rel=\"nofollow\">http://arxiv.org/abs/1310.4201</a></p>\n",
  "media:content": {
    "media:title": "mirror2image"
  }
}