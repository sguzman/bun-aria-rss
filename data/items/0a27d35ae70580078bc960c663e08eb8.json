{
  "title": "Clean-label Backdoor Attack against Deep Hashing based Retrieval. (arXiv:2109.08868v2 [cs.CV] UPDATED)",
  "link": "http://arxiv.org/abs/2109.08868",
  "description": "<p>Deep hashing has become a popular method in large-scale image retrieval due\nto its computational and storage efficiency. However, recent works raise the\nsecurity concerns of deep hashing. Although existing works focus on the\nvulnerability of deep hashing in terms of adversarial perturbations, we\nidentify a more pressing threat, backdoor attack, when the attacker has access\nto the training data. A backdoored deep hashing model behaves normally on\noriginal query images, while returning the images with the target label when\nthe trigger presents, which makes the attack hard to be detected. In this\npaper, we uncover this security concern by utilizing clean-label data\npoisoning. To the best of our knowledge, this is the first attempt at the\nbackdoor attack against deep hashing models. To craft the poisoned images, we\nfirst generate the targeted adversarial patch as the backdoor trigger.\nFurthermore, we propose the confusing perturbations to disturb the hashing code\nlearning, such that the hashing model can learn more about the trigger. The\nconfusing perturbations are imperceptible and generated by dispersing the\nimages with the target label in the Hamming space. We have conducted extensive\nexperiments to verify the efficacy of our backdoor attack under various\nsettings. For instance, it can achieve 63% targeted mean average precision on\nImageNet under 48 bits code length with only 40 poisoned images.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kuofeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongxian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"
}