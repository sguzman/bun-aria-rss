{
  "id": "tag:blogger.com,1999:blog-8474926331452026626.post-7866944510509103804",
  "published": "2022-10-19T10:57:00.005-07:00",
  "updated": "2022-10-21T09:44:54.408-07:00",
  "title": "Do Modern ImageNet Classifiers Accurately Predict Perceptual Similarity?",
  "content": "<span class=\"byline-author\">Posted by Manoj Kumar, Research Engineer, and Ekin Dogus Cubuk, Research Scientist, Google Research</span> <p>The task of determining the similarity between images is an open problem in computer vision and is crucial for evaluating the realism of machine-generated images. Though there are a number of straightforward methods of estimating image similarity (e.g., low-level metrics that measure pixel differences, such as <a href=\"https://ieeexplore.ieee.org/document/5705575\">FSIM</a> and <a href=\"https://ieeexplore.ieee.org/document/1284395\">SSIM</a>), in many cases, the measured similarity differences do not match the differences perceived by a person. However, <a href=\"https://arxiv.org/abs/1801.03924\">more recent work</a> has demonstrated that intermediate representations of neural network classifiers, such as <a href=\"https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\">AlexNet</a>, <a href=\"https://arxiv.org/abs/1409.1556\">VGG</a> and <a href=\"https://arxiv.org/abs/1602.07360\">SqueezeNet</a> trained on <a href=\"https://image-net.org/index.php\">ImageNet</a>, exhibit perceptual similarity as an emergent property. That is, <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean distances</a> between encoded representations of images by ImageNet-trained models correlate much better with a person’s judgment of differences between images than estimating perceptual similarity directly from image pixels. </p><a name='more'></a><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s1379/image5.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"471\" data-original-width=\"1379\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguCNOD5qe7A72gZv0sl7QIgLv4HL06sCxEzVQLELH8Affzu_UzyEFJPj2nGU5PdEFO6h6XErHLFZqpFgcrJQZO0sif3zH70avGxMsop1IBQvfFKTBKfdJ-6OBYdIODCS9xopyjaIRRmcNR58EygavFwwxrGxqlOLidEkIvZxvHCIUdBsnczvy_EC7C/s16000/image5.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Two sets of sample images from the <a href=\"https://github.com/richzhang/PerceptualSimilarity\">BAPPS dataset</a>. Trained networks agree more with human judgements as compared to low-level metrics (<a href=\"https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\">PSNR</a>, SSIM, FSIM). Image source: <a href=\"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html\">Zhang et al. (2018)</a>.</td></tr></tbody></table><p>In “<a href=\"https://openreview.net/forum?id=qrGKGZZvH0\">Do better ImageNet classifiers assess perceptual similarity better?</a>” published in <em><a href=\"https://www.jmlr.org/tmlr/\">Transactions on Machine Learning Research</a></em>, we contribute an extensive experimental study on the relationship between the accuracy of ImageNet classifiers and their emergent ability to capture perceptual similarity. To evaluate this emergent ability, we follow <a href=\"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html\">previous work</a> in measuring the perceptual scores (PS), which is roughly the correlation between human preferences to that of a model for image similarity on the <a href=\"https://github.com/richzhang/PerceptualSimilarity\">BAPPS dataset</a>. While prior work studied the first generation of ImageNet classifiers, such as AlexNet, SqueezeNet and VGG, we significantly increase the scope of the analysis incorporating modern classifiers, such as <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">ResNets</a> and <a href=\"https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html\">Vision Transformers</a> (ViTs), across a wide range of hyper-parameters. </p><div style=\"line-height:40%;\">    <br></div><h2>Relationship Between Accuracy and Perceptual Similarity</h2><p>It is well established that features learned via training on ImageNet transfer well to a number of <a href=\"https://arxiv.org/abs/1910.04867\">downstream</a> <a href=\"https://arxiv.org/abs/1705.07750\">tasks</a>, making ImageNet pre-training a standard recipe. Further, better accuracy on ImageNet usually implies better performance on a diverse set of downstream tasks, such as <a href=\"https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html\">robustness to common corruptions</a>, <a href=\"https://arxiv.org/abs/1902.10811\">out-of-distribution</a> <a href=\"https://arxiv.org/abs/2107.04649\">generalization</a> and transfer learning on <a href=\"https://arxiv.org/abs/1805.08974\">smaller classification datasets</a>. Contrary to prevailing evidence that suggests models with high validation accuracies on ImageNet are likely to transfer better to other tasks, surprisingly, we find that representations from underfit ImageNet models with modest validation accuracies achieve the best perceptual scores. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/s800/image6.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"506\" data-original-width=\"800\" height=\"253\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ4bKam5PwhK_KhnWYVYdt4vPCrYTLF1_39bAR5Ofpn74gYD9dq3kqDQ8g_fYHioVBh2_w26_nKu0fEQN5WK5zI8hZO_Y--RZxGaSUHHTn2nzhvWeUedJubE2vd2CRRh59maGf8alD5jyRTy2p5OXuz-T-dsj6BKogLd7v4zcg-CqSMCRhtIV4l9FQ7A/w400-h253/image6.gif\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Plot of perceptual scores (PS) on the 64 × 64 <a href=\"https://github.com/richzhang/PerceptualSimilarity\">BAPPS dataset</a> (y-axis) against the <a href=\"https://image-net.org/index.php\">ImageNet</a> 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).</td></tr></tbody></table><!--<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/s530/image6.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"344\" data-original-width=\"530\" height=\"260\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKFHZ-d3xni61fU1_S2ZjAg66jPP_y3tYeZCuOpdsV7seOiMYhclYtG2fo0XmYKjwwjt720MJd3_uHmJ5LbgO-msPp5k9UWW5_Xlts_6qx-Botu2ZNWOkDERxYTpPGzed_xWSMWr8wRfEIA9ulQMwoG7nDwiMurPHodNVZ_S2tds-5MXe2FiHlyacq/w400-h260/image6.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Plot of perceptual scores (PS) on the 64 × 64 <a href=\"https://github.com/richzhang/PerceptualSimilarity\">BAPPS Dataset</a> (y-axis) against the <a href=\"https://image-net.org/index.php\">ImageNet</a> 64 × 64 validation accuracies (x-axis). Each blue dot represents an ImageNet classifier. Better ImageNet classifiers achieve better PS up to a certain point (dark blue), beyond which improving the accuracy lowers the PS. The best PS are attained by classifiers with moderate accuracy (20.0–40.0).</td></tr></tbody></table>-->  <p>We study the variation of perceptual scores as a function of neural network hyperparameters: width, depth, number of training steps, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\">weight decay</a>,<a href=\"https://ieeexplore.ieee.org/document/7780677\"> label smoothing</a> and <a href=\"https://en.wikipedia.org/wiki/Dilution_(neural_networks)\">dropout</a>. For each hyperparameter, there exists an optimal accuracy up to which improving accuracy improves PS. This optimum is fairly low and is attained quite early in the hyperparameter sweep. Beyond this point, improved classifier accuracy corresponds to worse PS. </p><p>As illustration, we present the variation of PS with respect to two hyperparameters: training steps in ResNets and width in ViTs. The PS of ResNet-50 and ResNet-200 peak very early at the first few epochs of training. After the peak, PS of better classifiers decrease more drastically. ResNets are trained with a learning rate schedule that causes a stepwise increase in accuracy as a function of training steps. Interestingly, after the peak, they also exhibit a step-wise decrease in PS that matches this step-wise accuracy increase.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/s423/image8.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"276\" data-original-width=\"423\" height=\"261\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg-BBJ6hr9O1Ixc77DP1usrpbUrYgst2OM88mU79csMXwfNv06H4dam1LHjZIZjNrSLOXPhG-_IEHWwoGadhW_kylZviGcZtRvNSm1VtcpLjhm3QCTKs1KnnFx_bYHWvt9yfnlO5DPK5lDjYf9t4QEHDY-i7t1StkcO7PEh-ushZpP56PBrK3L6Yx8/w400-h261/image8.png\" width=\"400\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/s423/image3.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"276\" data-original-width=\"423\" height=\"261\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8vyrykb9SDnaz1HH2mkFokJ9tzUnzBQYk8_SJ__HfAAzooORF5KYXXfOglAC2K3wk7zERnYJ4xnzkhj6v43kK49edBHK2IWP8uY-SZHLDLvFYbvI1v21pNO9mANR1RKnBDP_t7_5hbY2RNruKHoPLusUqCPNMkDRyesC-BhsS06lIsxQB4j0vfCJn/w400-h261/image3.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Early-stopped ResNets attain the best PS across different depths of 6, 50 and 200.</td></tr></tbody></table><p>ViTs consist of a stack of transformer blocks applied to the input image. The width of a ViT model is the number of output neurons of a single transformer block. Increasing its width is an effective way to improve its accuracy. Here, we vary the width of two ViT variants, B/8 and L/4 (i.e., Base and Large ViT models with patch sizes 4 and 8 respectively), and evaluate both the accuracy and PS. Similar to our observations with early-stopped ResNets, narrower ViTs with lower accuracies perform better than the default widths. Surprisingly, the optimal width of ViT-B/8 and ViT-L/4 are 6 and 12% of their default widths.&nbsp;For a more comprehensive list of experiments involving other hyperparameters such as width, depth, number of training steps, weight decay, label smoothing and dropout across both ResNets and ViTs, check out <a href=\"https://openreview.net/pdf?id=qrGKGZZvH0\">our paper</a>.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/s423/image7.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"276\" data-original-width=\"423\" height=\"261\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDl21gStyRmEcR-61uVBmHVyWVdd9dQxWyglkbBnz1EzTisb1h9kKxesv_0dw5uZpd6tw8s4YJ-eswj5OIEPO1DfvHbTOrfERnnwo4OwE3kmQewrZJHpgc57Bicye5eQCJYT9NQMkhP8SnCxk6xrJhM2DCrSeAz0nGfiPgHJUAgFnR52V_RShRq-yq/w400-h261/image7.png\" width=\"400\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/s423/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"276\" data-original-width=\"423\" height=\"261\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFhvSTKUw16K74BqmHea-iiWfUioxac-bM2zYcDAw-wkcNdYxvacibnZFkeva3hSvelhVCPd02YNYNyc4PT22Jo4M9Ek-xS0bJUQx2sggz3sJTySePP_HvkOPG_rHOyL4rQGMGplQMoKK15mq0KMBqsiAUHy7l5cIczHGfHhZumHwGbEfu62rcFwGK/w400-h261/image1.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Narrow ViTs attain the best PS.</td></tr></tbody></table> <h2>Scaling Down Models Improves Perceptual Scores</h2><p>Our results prescribe a simple strategy to improve an architecture’s PS: scale down the model to reduce its accuracy until it attains the optimal perceptual score. The table below summarizes the improvements in PS obtained by scaling down each model across every hyperparameter. Except for ViT-L/4, early stopping yields the highest improvement in PS, regardless of architecture. In addition, early stopping is the most efficient strategy as there is no need for an expensive grid search. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: 11%; margin-right: 11%;\">      <colgroup>     <col style=\"width: 15%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>     <col style=\"width: 9%;\"></col>  </colgroup>  <tbody>  <tr>   <td style=\"text-align: left;\"><b><em>Model</em></b>   </td>   <td><b><em>Default</em></b>   </td>   <td><b><em>Width</em></b>   </td>   <td><b><em>Depth</em></b>   </td>   <td><b><em>Weight<br />Decay</em></b>   </td>   <td><b><em>Central<br />Crop</em></b>   </td>   <td><b><em>Train<br />Steps</em></b>   </td>   <td><b><em>Best</em></b>   </td>  </tr>  <tr>   <td style=\"text-align: left;\"><em>ResNet-6</em>   </td>   <td>69.1    </td>   <td>+0.4    </td>   <td>-    </td>   <td>+0.3    </td>   <td>0.0    </td>   <td><b>+0.5</b>   </td>   <td>69.6    </td>  </tr>  <tr>   <td style=\"text-align: left;\"><em>ResNet-50</em>   </td>   <td>68.2    </td>   <td>+0.4    </td>   <td>-    </td>   <td>+0.7    </td>   <td>+0.7    </td>   <td><b>+1.5</b>   </td>   <td>69.7    </td>  </tr>  <tr>   <td style=\"text-align: left;\"><em>ResNet-200</em>   </td>   <td>67.6    </td>   <td>+0.2    </td>   <td>-    </td>   <td>+1.3    </td>   <td>+1.2    </td>   <td><b>+1.9</b>   </td>   <td>69.5    </td>  </tr>  <tr>   <td style=\"text-align: left;\"><em>ViT B/8</em>   </td>   <td>67.6    </td>   <td>+1.1    </td>   <td>+1.0    </td>   <td><b>+1.3</b>   </td>   <td>+0.9    </td>   <td>+1.1    </td>   <td>68.9    </td>  </tr>  <tr>   <td style=\"text-align: left;\"><em>ViT L/4</em>   </td>   <td>67.9    </td>   <td>+0.4    </td>   <td>+0.4    </td>   <td>-0.1    </td>   <td>-1.1    </td>   <td><b>+0.5</b>   </td>   <td>68.4    </td>  </tr>  </tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: 5%; margin-right: 5%;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Perceptual Score improves by scaling down ImageNet models. Each value denotes the improvement obtained by scaling down a model across a given hyperparameter over the model with default hyperparameters.</td></tr></tbody></table> <h2>Global Perceptual Functions</h2><p>In <a href=\"https://arxiv.org/abs/1801.03924\">prior work</a>, the perceptual similarity function was computed using Euclidean distances across the spatial dimensions of the image. This assumes a direct correspondence between pixels, which may not hold for warped, translated or rotated images. Instead, we adopt two perceptual functions that rely on global representations of images, namely the style-loss function from the <a href=\"https://arxiv.org/abs/1508.06576\">Neural Style Transfer</a> work that captures stylistic similarity between two images, and a normalized <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers\">mean pool</a> distance function. The style-loss function compares the inter-channel cross-correlation matrix between two images while the mean pool function compares the spatially averaged global representations. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/s423/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"277\" data-original-width=\"423\" height=\"263\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjHYIY3m7EyJOKW8znpmUVVTy7G1Bg-XSiOvkeDoXjhRcukUgWpJ5d3t0JOgXaV6vptIk30Qt1g9Yo4sEiBqHh56rIhlZWZ44pqfROujg4DDI_4f7gZBJwaywrOCxv2CXJkbpCRh7g5XU7QjeSZW8umduUPlHI7DA1ptPZbWBL0P7Coqr9XQnXHxjz/w400-h263/image2.png\" width=\"400\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/s423/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"277\" data-original-width=\"423\" height=\"263\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjleEhvKhCBu_6LCUxbbNN4KPxzD-abx_tAVwT1q8KLhLGDixECks5SvRDZyOs8kcm_QxkeaKX7Jz00P4ucN9S3XuHGAXqAud3UCAzN_XApUWEa23WvA89gD3E8Yqm1U-Gb-CwNlmwieClTG3eLKB91rDzITV9Xfj-q5x9G5eKNAZxoi5bm7h6YLZ9h/w400-h263/image4.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Global perceptual functions consistently improve PS across both networks trained with default hyperparameters (<b>top</b>) and ResNet-200 as a function of train epochs (<b>bottom</b>).</td></tr></tbody></table><p>We probe a number of hypotheses to explain the relationship between accuracy and PS and come away with a few additional insights. For example, the accuracy of models without commonly used skip-connections also inversely correlate with PS, and layers close to the input on average have lower PS as compared to layers close to the output. For further exploration involving distortion sensitivity, ImageNet class granularity, and spatial frequency sensitivity, check out <a href=\"https://openreview.net/pdf?id=qrGKGZZvH0\">our paper</a>. </p><div style=\"line-height:40%;\">    <br></div><h2>Conclusion</h2><p>In this paper, we explore the question of whether improving classification accuracy yields better perceptual metrics. We study the relationship between accuracy and PS on ResNets and ViTs across many different hyperparameters and observe that PS exhibits an inverse-U relationship with accuracy, where accuracy correlates with PS up to a certain point, and then exhibits an inverse-correlation. Finally, in our paper, we discuss in detail a number of explanations for the  observed relationship between accuracy and PS, involving skip connections, global similarity functions, distortion sensitivity, layerwise perceptual scores, spatial frequency sensitivity and ImageNet class granularity. While the exact explanation for the observed tradeoff between ImageNet accuracy and perceptual similarity is a mystery, we are excited that our paper opens the door for further research in this area. </p><div style=\"line-height:40%;\">    <br></div><h2>Acknowledgements</h2><p><em>This is joint work with Neil Houlsby and Nal Kalchbrenner. We would additionally like to thank Basil Mustafa, Kevin Swersky, Simon Kornblith, Johannes Balle, Mike Mozer, Mohammad Norouzi and Jascha Sohl-Dickstein for useful discussions.</em></p>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Google AI",
    "uri": "http://www.blogger.com/profile/12098626514775266161",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 0
}