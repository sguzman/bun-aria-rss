{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2018-08-02T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2018/08/02/dask-dev",
  "content": "<p><em>This work is supported by <a href=\"http://anaconda.com\">Anaconda Inc</a></em></p>\n\n<p>To increase transparency I’m trying to blog more often about the current work\ngoing on around Dask and related projects.  Nothing here is ready for\nproduction.  This blogpost is written in haste, so refined polish should not be\nexpected.</p>\n\n<p>Over the last two weeks we’ve seen activity in the following areas:</p>\n\n<ol>\n  <li>An experimental Actor solution for stateful processing</li>\n  <li>Machine learning experiments with hyper-parameter selection and parameter\nservers.</li>\n  <li>Development of more preprocessing transformers</li>\n  <li>Statistical profiling of the distributed scheduler’s internal event loop\nthread and internal optimizations</li>\n  <li>A new release of dask-yarn</li>\n  <li>A new narrative on dask-stories about modelling mobile networks</li>\n  <li>Support for LSF clusters in dask-jobqueue</li>\n  <li>Test suite cleanup for intermittent failures</li>\n</ol>\n\n<h3 id=\"stateful-processing-with-actors\">Stateful processing with Actors</h3>\n\n<p>Some advanced workloads want to directly manage and mutate state on workers.  A\ntask-based framework like Dask can be forced into this kind of workload using\nlong-running-tasks, but it’s an uncomfortable experience.  To address this\nwe’ve been adding an experimental Actors framework to Dask alongside the\nstandard task-scheduling system.  This provides reduced latencies, removes\nscheduling overhead, and provides the ability to directly mutate state on a\nworker, but loses niceties like resilience and diagnostics.</p>\n\n<p>The idea to adopt Actors was shamelessly stolen from the <a href=\"http://ray.readthedocs.io/en/latest/\">Ray Project</a> :)</p>\n\n<p>Work for Actors is happening in <a href=\"https://github.com/dask/distributed/pull/2133\">dask/distributed #2133</a>.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">Counter</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">increment</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n</span>\n\n<span class=\"n\">counter</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">Counter</span><span class=\"p\">,</span> <span class=\"n\">actor</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">).</span><span class=\"n\">result</span><span class=\"p\">()</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span> <span class=\"o\">=</span> <span class=\"n\">counter</span><span class=\"p\">.</span><span class=\"n\">increment</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">future</span><span class=\"p\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n<span class=\"mi\">1</span>\n</code></pre></div></div>\n\n<h2 id=\"machine-learning-experiments\">Machine learning experiments</h2>\n\n<h3 id=\"hyper-parameter-optimization-on-incrementally-trained-models\">Hyper parameter optimization on incrementally trained models</h3>\n\n<p>Many Scikit-Learn-style estimators feature a <code class=\"language-plaintext highlighter-rouge\">partial_fit</code> method that enables\nincremental training on batches of data.  This is particularly well suited for\nsystems like Dask array or Dask dataframe, that are built from many batches of\nNumpy arrays or Pandas dataframes.  It’s a nice fit because all of the\ncomputational algorithm work is already done in Scikit-Learn, Dask just has to\nadministratively move models around to data and call scikit-learn (or other\nmachine learning models that follow the fit/transform/predict/score API).  This\napproach provides a nice community interface between parallelism and machine\nlearning developers.</p>\n\n<p>However, this training is inherently sequential because the model only trains\non one batch of data at a time.  We’re leaving a lot of processing power on the\ntable.</p>\n\n<p>To address this we can combine incremental training with hyper-parameter\nselection and train several models on the same data at the same time.  This is\noften required anyway, and lets us be more efficient with our computation.</p>\n\n<p>However there are many ways to do incremental training with hyper-parameter\nselection, and the right algorithm likely depends on the problem at hand.\nThis is an active field of research and so it’s hard for a general project like\nDask to pick and implement a single method that works well for everyone.  There\nis probably a handful of methods that will be necessary with various options on\nthem.</p>\n\n<p>To help experimentation here we’ve been experimenting with some lower-level\ntooling that we think will be helpful in a variety of cases.  This accepts a\npolicy from the user as a Python function that gets scores from recent\nevaluations, and asks for how much further to progress on each set of\nhyper-parameters before checking in again.  This allows us to model a few\ncommon situations like random search with early stopping conditions, successive\nhalving, and variations of those easily without having to write any Dask code:</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/288\">dask/dask-ml #288</a></li>\n  <li><a href=\"https://gist.github.com/mrocklin/4c95bd26d15281d82e0bf2d27632e294\">Notebook showing a few approaches</a></li>\n  <li><a href=\"https://gist.github.com/stsievert/c675b3a237a60efbd01dcb112e29115b\">Another notebook showing convergence</a></li>\n</ul>\n\n<p>This work is done by <a href=\"http://github.com/stsievert\">Scott Sievert</a> and myself</p>\n\n<p><img src=\"https://user-images.githubusercontent.com/1320475/43540881-7184496a-95b8-11e8-975a-96c2f17ee269.png\" width=\"70%\" alt=\"Successive halving and random search\" /></p>\n\n<h3 id=\"parameter-servers\">Parameter Servers</h3>\n\n<p>To improve the speed of training large models <a href=\"https://github.com/stsievert\">Scott\nSievert</a> has been using Actors (mentioned above)\nto develop simple examples for parameter servers.  These are helping to\nidentify and motivate performance and diagnostic improvements improvements\nwithin Dask itself:</p>\n\n<script src=\"https://gist.github.com/ff8a1df9300a82f15a2704e913469522.js\"> </script>\n\n<p>These parameter servers manage the communication of models produced by\ndifferent workers, and leave the computation to the underlying deep learning\nlibrary. This is ongoing work.</p>\n\n<h3 id=\"dataframe-preprocessing-transformers\">Dataframe Preprocessing Transformers</h3>\n\n<p>We’ve started to orient some of the Dask-ML work around case studies.  Our\nfirst, written by <a href=\"https://github.com/stsievert\">Scott Sievert</a>, uses the\nCriteo dataset for ads.  It’s a good example of a combined dense/sparse dataset\nthat can be somewhat large (around 1TB).  The first challenge we’re running\ninto is preprocessing.  These have lead to a few preprocessing improvements:</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/310\">Label Encoder supports Pandas Categorical dask/dask-ml #310</a></li>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/11\">Add Imputer with mean and median strategies dask/dask-ml #11</a></li>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/313\">Ad OneHotEncoder dask/dask-ml #313</a></li>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/122\">Add Hashing Vectorizer dask/dask-ml #122</a></li>\n  <li><a href=\"https://github.com/dask/dask-ml/pull/315\">Add ColumnTransformer dask/dask-ml #315</a></li>\n</ul>\n\n<p>Some of these are also based off of improved dataframe handling features in the\nupcoming 0.20 release for Scikit-Learn.</p>\n\n<p>This work is done by\n<a href=\"https://github.com/dask/dask-ml/pull/122\">Roman Yurchak</a>,\n<a href=\"https://github.com/jrbourbeau\">James Bourbeau</a>,\n<a href=\"https://github.com/daniel-severo\">Daniel Severo</a>, and\n<a href=\"https://github.com/TomAugspurger\">Tom Augspurger</a>.</p>\n\n<h3 id=\"profiling-the-main-thread\">Profiling the main thread</h3>\n\n<p>Profiling concurrent code is hard.  Traditional profilers like CProfile become\nconfused by passing control between all of the different coroutines.  This\nmeans that we haven’t done a very comprehensive job of profiling and tuning the\ndistributed scheduler and workers.  Statistical profilers on the other hand\ntend to do a bit better.  We’ve taken the statistical profiler that we usually\nuse on Dask worker threads (available in the dashboard on the “Profile” tab)\nand have applied it to the central administrative threads running the Tornado\nevent loop as well.  This has highlighted a few issues that we weren’t able to\nspot before, and should hopefully result in reduced overhead in future\nreleases.</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/distributed/pull/2144\">dask/distributed #2144</a></li>\n  <li><a href=\"https://stackoverflow.com/questions/51582394/which-functions-are-free-when-profiling-tornado-asyncio\">stackoverflow.com/questions/51582394/which-functions-are-free-when-profiling-tornado-asyncio</a></li>\n</ul>\n\n<p><img src=\"https://user-images.githubusercontent.com/306380/43368136-4574f46c-930d-11e8-9d5b-6f4b4f6aeffe.png\" width=\"70%\" alt=\"Profile of event loop thread\" /></p>\n\n<h3 id=\"new-release-of-dask-yarn\">New release of Dask-Yarn</h3>\n\n<p>There is a new release of <a href=\"http://dask-yarn.readthedocs.io/en/latest\">Dask-Yarn</a>\nand the underlying library for managing Yarn jobs,\n<a href=\"https://jcrist.github.io/skein/\">Skein</a>.  These include a number of bug-fixes\nand improved concurrency primitives for YARN applications.  The new features are\ndocumented <a href=\"https://jcrist.github.io/skein/key-value-store.html\">here</a>, and were\nimplemented in <a href=\"https://github.com/jcrist/skein/pull/40\">jcrist/skein #40</a>.</p>\n\n<p>This work was done by <a href=\"https://jcrist.github.io/\">Jim Crist</a></p>\n\n<h3 id=\"support-for-lsf-clusters-in-dask-jobqueue\">Support for LSF clusters in Dask-Jobqueue</h3>\n\n<p><a href=\"http://dask-jobqueue.readthedocs.io/en/latest/\">Dask-jobqueue</a> supports Dask\nuse on traditional HPC cluster managers like SGE, SLURM, PBS, and others.\nWe’ve recently <a href=\"http://dask-jobqueue.readthedocs.io/en/latest/generated/dask_jobqueue.LSFCluster.html#dask_jobqueue.LSFCluster\">added support for LSF clusters</a></p>\n\n<p>Work was done in <a href=\"https://github.com/dask/dask-jobqueue/pull/78\">dask/dask-jobqueue #78</a> by <a href=\"https://github.com/raybellwaves\">Ray Bell</a>.</p>\n\n<h3 id=\"new-dask-story-on-mobile-networks\">New Dask Story on mobile networks</h3>\n\n<p>The <a href=\"http://dask-stories.readthedocs.io/en/latest/\">Dask Stories</a>\nrepository holds narrative about how people use Dask.\n<a href=\"https://www.linkedin.com/in/lalwanisameer/\">Sameer Lalwani</a>\nrecently added a story about using Dask to\n<a href=\"http://dask-stories.readthedocs.io/en/latest/network-modeling.html\">model mobile communication networks</a>.\nIt’s worth a read.</p>\n\n<h3 id=\"test-suite-cleanup\">Test suite cleanup</h3>\n\n<p>The dask.distributed test suite has been suffering from intermittent failures\nrecently.  These are tests that fail very infrequently, and so are hard to\ncatch when writing them, but show up when future unrelated PRs run the test\nsuite on continuous integration and get failures.  They add friction to the\ndevelopment process, but are expensive to track down (testing distributed\nsystems is hard).</p>\n\n<p>We’re taking a bit of time this week to track these down.  Progress here:</p>\n\n<ul>\n  <li><a href=\"https://github.com/dask/distributed/pull/2146\">dask/distributed #2146</a></li>\n  <li><a href=\"https://github.com/dask/distributed/pull/2152\">dask/distributed #2152</a></li>\n</ul>"
}