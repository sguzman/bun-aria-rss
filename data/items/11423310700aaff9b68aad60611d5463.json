{
  "title": "Random forest interpretation &#8211; conditional feature contributions",
  "link": "http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/",
  "comments": "http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/#comments",
  "dc:creator": "ando",
  "pubDate": "Mon, 24 Oct 2016 21:13:15 +0000",
  "category": "Random forest",
  "guid": "http://blog.datadive.net/?p=4511",
  "description": "In two of my previous blog posts, I explained how the black box of a random forest can be opened up by tracking decision paths along the trees and computing feature contributions. This way, any prediction can be decomposed into &#8230; <a href=\"http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/\">Continue reading <span class=\"meta-nav\">&#8594;</span></a>",
  "content:encoded": "<p>In two of my <a href=\"http://blog.datadive.net/interpreting-random-forests/\">previous</a> <a href=\"http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/\">blog posts</a>, I explained how the black box of a random forest can be opened up by tracking decision paths along the trees and computing feature contributions. This way, any prediction can be decomposed into contributions from features, such that \\(prediction = bias + feature_1contribution+..+feature_ncontribution\\).</p><p>However, this linear breakdown is inherently imperfect, since a linear combination of features cannot capture interactions between them. A classic example of a relation where a linear combination of inputs cannot capture the output is <a href=\"https://en.wikipedia.org/wiki/Exclusive_or\">exclusive or</a> (XOR), defined as</p>\n<div class=\"table-responsive\">\n<table  style=\"width:200px; \"  class=\"easy-table easy-table-default tablesorter  \" >\n<thead>\n<tr>\n<th class=' ' >X1</th>\n<th class=' ' >X2</th>\n<th class=' ' >OUT</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td >0</td>\n<td >0</td>\n<td >0</td>\n</tr>\n<tr>\n<td >0</td>\n<td >1</td>\n<td >1</td>\n</tr>\n<tr>\n<td >1</td>\n<td >0</td>\n<td >1</td>\n</tr>\n<tr>\n<td >1</td>\n<td >1</td>\n<td >0</td>\n</tr>\n</tbody>\n</table>\n</div><p>In this case, neither X1 nor X2 provide anything towards predicting the outcome in isolation. Their value only becomes predictive in conjunction with the the other input feature.</p><p>A decision tree can easily learn a function to classify the XOR data correctly via a two level tree (depicted below). However, if we consider feature contributions at each node, then at first step through the tree (when we have looked only at X1), we haven&#8217;t yet moved away from the bias, so the best we can predict at that stage is still &#8220;don&#8217;t know&#8221;, i.e. 0.5. And if we would have to write out the contribution from the feature at the root of the tree, we would (incorrectly) say that it is 0. After the next step down the tree, we would be able to make the correct prediction, at which stage we might say that the second feature provided all the predictive power, since we can move from a coin-flip (predicting 0.5), to a concrete and correct prediction, either 0 or 1. But of course attributing this to only the second level variable in the tree is clearly wrong, since the contribution comes from both features and should be equally attributed to both.</p><p>This information is of course available along the tree paths. We simply should gather together all conditions (and thus features) along the path that lead to a given node.</p>\n<div class=\"wpd3-1-0\"><script type=\"text/javascript\">function wpd3_1_0 () {}; wpd3_1_0();</script></div>\n\n    <style type=\"text/css\">\n\n\n.node circle {\n  cursor: pointer;\n  fill: #fff;\n  stroke: steelblue;\n  stroke-width: 1.5px;\n}\n\n.node text {\n  font-size: 15px;\n  \n}\n/*\ntable, th, td {\n   border: 1px solid black;\n} \ntable {\n    border-collapse: collapse;\n}\n\nth {\n    padding:5px\n}*/\npath.link {\n  fill: none;\n  stroke: #ccc;\n  stroke-width: 4px;\n}\n\n#equation {\nmargin-top:20px;\nmargin-bottom:20px;\nborder:1px solid;\npadding:3px;\nbackground-color:#eeffee;\n}\n\n.code {\n font-family: Consolas,Courier New,monospace;\n font-weight: bold;\n}\n\n    </style>\n    \n    <div id=\"d3\"></div>\n<div id =\"equation\">\n    \n    </div>\n\n\n    \n    <script type=\"text/javascript\">\n    \n    d3.selectAll(\"a.link\").on(\"click\", function(d,i){\n        var nodes = tree.nodes(root);\n        nodes.forEach(function(d) { d.highlight = 0;});\n        var key;\n        if (i == 0) {\n            key = 4;\n        } else if (i == 1) {\n            key = 6;\n        } else {\n            key = 10\n        }\n        \n        items = highlight_parents(nodes[key])\n\n        show_line(items)\n        upd(root)\n    });\nvar m = [0, 120, 0, 120],\n    w = 1280 - m[1] - m[3],\n    h = 500 - m[0] - m[2],\n    i = 0,\n    root;\n\nvar tree = d3.layout.tree()\n    .size([h, w]);\n\nvar diagonal = d3.svg.diagonal()\n    .projection(function(d) { return [d.y, d.x]; });\n\nvar vis = d3.select(\"#d3\").append(\"svg:svg\")\n    .attr(\"width\", w + m[1] + m[3])\n    .attr(\"height\", h + m[0] + m[2])\n  .append(\"svg:g\")\n    .attr(\"transform\", \"translate(\" + m[3] + \",\" + m[0] + \")\");\n\nvar root = {\"error\": 2, \"samples\": 3, \"value\": [0.5], \"label\": \"X1 == 0\", \"type\": \"split\", \n    \"children\": \n        [{\"error\": 2, \"samples\": 3, \"value\": [0.5], \"label\": \"X2 == 0\", \"type\": \"split\",\n            \"children\": [{\"error\": 2, \"samples\": 3, \"value\": [0], \"label\": \"Leaf - 3\", \"type\": \"leaf\"}\n                       , {\"error\": 2, \"samples\": 3, \"value\": [1], \"label\": \"Leaf - 4\", \"type\": \"leaf\"}\n                    ]}\n     , {\"error\": 2, \"samples\": 3, \"value\": [0.5], \"label\": \"X2 == 0\", \"type\": \"split\", \n            \"children\": [{\"error\": 2, \"samples\": 3, \"value\": [1], \"label\": \"Leaf - 6\", \"type\": \"leaf\"}\n                       , {\"error\": 2, \"samples\": 3, \"value\": [0], \"label\": \"Leaf - 7\", \"type\": \"leaf\"}\n]}\n]}\n\n\n\nvar leaf_id = 0;  \n\nvar nodes = tree.nodes(root).reverse();\nnodes.forEach(function(d) { d.highlight = 0; d.y = d.depth * 100; });\n  update(root);\n\n        \nitems = highlight_parents(nodes[3])\nshow_line(items)\nupd(root)\n        \n  \n  \nfunction highlight_parents(node) {\n    node.highlight = 1;\n    var accum2;\n    if (typeof node.parent !== \"undefined\") {\n        accum2 =  highlight_parents(node.parent)\n    } else {\n        accum2 = []\n    }\n\n    accum2.push(node)\n    return accum2;\n    \n}\n\nfunction show_line(items) {\n    var v = \"<span class='code'>\" + items[0].value[0].toFixed(2) + \"</span> (trainset mean)\"\n    var pred = items[items.length  - 1].value[0].toFixed(2) \n    var contr_list = \"\"\n    for (var i = 1; i < items.length; i++) {\n        add = (items[i].value[0] - items[i-1].value[0]).toFixed(2)\n        contr_list += items[i-1].label.split(\" \")[0] + \"  \"\n        if (add < 0) {\n            add = \" - \" + Math.abs(add)\n            word = \"decrease\"\n        } else {\n            add = \" + \" + add\n            word = \"increase\"\n        }\n        v += \"<span class='code'>\"+ add + \"</span> (\"+word+\" from \"+ contr_list+\")\"\n    }\n    d3.select('#equation').html(\"Prediction: <span class='code'><u>\" +pred +\"</u>  &#8776; </span>\"+ v)\n    \n}\n\nfunction upd(source) {\n\n  var link = vis.selectAll(\"path.link\")\n      .style(\"stroke\", function(d) {\n            if (d.target.highlight) {return '#ff0000'} else {return \"#00AA00\"}})\n      .style(\"stroke-width\", function(d) {if (d.target.highlight) {return '6px'} else {return \"4px\"}})\n   d3.selectAll('text').attr('font-weight', function(d){ if (d.highlight) {return 'bold'} else {return \"\"}})\n   \n}\n  \nfunction update(source) {\n  var duration = d3.event && d3.event.altKey ? 5000 : 500;\n\n  // Compute the new tree layout.\n  var nodes = tree.nodes(root).reverse();\n\n  nodes.forEach(function(d) { d.y = d.depth * 100; });\n  \n  // Normalize for fixed-depth.\n  //nodes.forEach(function(d) { d.y = d.depth * 100; });\n  \n\n  // Update the nodesâ€¦\n  var node = vis.selectAll(\"g.node\")\n      .data(nodes, function(d) { return d.id || (d.id = ++i); });\n  \n  // Enter any new nodes at the parent's previous position.\n  var nodeEnter = node.enter().append(\"svg:g\")\n      .attr(\"class\", \"node\")\n      .attr(\"transform\", function(d) { return \"translate(\" + source.y0 + \",\" + source.x0 + \")\"; })\n      .on(\"mouseover\", function(d){\n                    if (d.type == \"leaf\") {\n                        var nodes = tree.nodes(root);\n                        nodes.forEach(function(d) { d.highlight = 0;});\n                        items = highlight_parents(d)\n                        show_line(items)\n                        upd(root)\n                    }\n                    \n                    });\n\n  nodeEnter.append(\"svg:circle\")\n      .attr(\"r\", 1e-6)\n      .style(\"fill\", function(d) { return d._children ? \"lightsteelblue\" : \"#fff\"; });\n\n      \n  nodeEnter.append(\"svg:text\")\n      .attr(\"x\", function(d) { return d.children || d._children ? -10 : 10; })\n      //.attr(\"dy\", \".35em\")\n      .attr(\"text-anchor\", function(d) { return d.children || d._children ? \"end\" : \"start\"; })\n            .text(function(d) { \n                if (d.type != \"leaf\") {\n                    return d.label;\n                } else {\n                    return \"Value:\"+(d.value[0]).toFixed(2); \n                }})\n      .style(\"fill-opacity\", 1e-6)\n      .style(\"font-family\", \"Consolas,Courier New,monospace\")\n      .style(\"background-color\", \"yellow\")\n      \n      \n      \n  nodeEnter.append(\"svg:text\")\n      .attr(\"x\", function(d) { return d.children || d._children ? -10 : 10; })\n      .attr(\"y\", \"20\")\n      .attr(\"text-anchor\", function(d) { return d.children || d._children ? \"end\" : \"start\"; })\n      .text(function(d) { if (d.type != \"leaf\") {return \"Value:\"+(d.value[0]).toFixed(2);  } else {return \"\"}})\n      .style(\"font-family\", \"Consolas,Courier New,monospace\")\n      \n      \n  // Transition nodes to their new position.\n  var nodeUpdate = node.transition()\n      .duration(duration)\n      .attr(\"transform\", function(d) { return \"translate(\" + d.y + \",\" + d.x + \")\"; });\n\n  nodeUpdate.select(\"circle\")\n      .attr(\"r\", 8)\n      .style(\"fill\", function(d) { return d._children ? \"lightsteelblue\" : \"#fff\"; });\n\n  nodeUpdate.select(\"text\")\n      .style(\"fill-opacity\", 1);\n\n  \n  // Update the linksâ€¦\n  var link = vis.selectAll(\"path.link\")\n      .data(tree.links(nodes), function(d) { return d.target.id; });\n\n  // Enter any new links at the parent's previous position.\n  link.enter().insert(\"svg:path\", \"g\")\n      .attr(\"class\", \"link\")\n      .attr(\"d\", function(d) {\n        var o = {x: source.x0, y: source.y0};\n        return diagonal({source: o, target: o});\n      });\n  link.style(\"stroke\", function(d) {\n            if (d.target.highlight) {return '#ff0000'} else {return \"#00AA00\"}})\n      .style(\"stroke-width\", function(d) {if (d.target.highlight) {return '6px'} else {return \"4px\"}})\n      \n  \n  link.transition()\n      .duration(0)\n      .attr(\"d\", diagonal);\n\n\n}\n\n    </script>\n\n<p>As you can see, the contribution of the first feature at the root of the tree is 0 (<em>value</em> staying at 0.5), while observing the second feature gives the full information needed for the prediction. We can now combine the features along the decision path, and correctly state that X1 and X2 together create the contribution towards the prediction.</p><p>The joint contribution calculation is supported by v0.2 of the <a href=\"https://github.com/andosa/treeinterpreter/\">treeinterpreter</a> package (clone or install via pip). Joint contributions can be obtained by passing the <em>joint_contributions</em> argument to the <em>predict </em>method, returning the triple [prediction, contributions, bias], where contribution is a mapping from tuples of feature indices to absolute contributions.<br />\nHere&#8217;s an example, comparing two datasets of the Boston housing data, and calculating which feature combinations contribute to the difference in estimated prices</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\"> \nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom treeinterpreter import treeinterpreter as ti, utils\n\nboston = load_boston()\nrf = RandomForestRegressor()\n# We train a random forest model, ...\nrf.fit(boston.data[:300], boston.target[:300])\n# take two subsets from the data ...\nds1 = boston.data[300:400]\nds2 = boston.data[400:]\n# and check what the predicted average price is\nprint (np.mean(rf.predict(ds1)))\nprint (np.mean(rf.predict(ds2)))\n</pre>\n<blockquote><p><code><br />\n21.9329<br />\n17.8863207547<br />\n</code></p></blockquote><p>The average predicted price is different for the two datasets. We can break down why and check the joint feature contribution for both datasets.</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\"> \nprediction1, bias1, contributions1 = ti.predict(rf, ds1, joint_contribution=True)\nprediction2, bias2, contributions2 = ti.predict(rf, ds2, joint_contribution=True)\n</pre><p>Since biases are equal for both datasets (because the the model is the same), the difference between the average predicted values has to come only from (joint) feature contributions. In other words, the sum of the feature contribution differences should be equal to the difference in average prediction.<br />\nWe can make use of the <em>aggregated_contributions</em> convenience method which takes the contributions for individual predictions and aggregates them together for the whole dataset</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\"> \naggregated_contributions1 = utils.aggregated_contribution(contributions1)\naggregated_contributions2 = utils.aggregated_contribution(contributions2)\n\nprint (np.sum(list(aggregated_contributions1.values())) -  \n       np.sum(list(aggregated_contributions2.values())))\nprint (np.mean(prediction1) - np.mean(prediction2))\n</pre>\n<blockquote><p><code><br />\n4.04657924528<br />\n4.04657924528<br />\n</code></p></blockquote><p>Indeed we see that the contributions exactly match the difference, as they should.</p><p>Finally, we can check which feature combination contributed by how much to the difference of the predictions in the too datasets:</p>\n<pre class=\"brush: python; collapse: false; title: ; wrap-lines: false; notranslate\"> \nres = []\nfor k in set(aggregated_contributions1.keys()).union(\n              set(aggregated_contributions2.keys())):\n    res.append(([boston[\"feature_names\"][index] for index in k] , \n               aggregated_contributions1.get(k, 0) - aggregated_contributions2.get(k, 0)))   \n        \nfor lst, v in (sorted(res, key=lambda x:-abs(x[1])))[:10]:\n    print (lst, v)    \n</pre>\n<blockquote><p><code><br />\n(['RM', 'LSTAT'], 2.0317570671740883)<br />\n(['RM'], 0.69252072064203141)<br />\n(['CRIM', 'RM', 'LSTAT'], 0.37069750747155134)<br />\n(['RM', 'AGE'], 0.11572468903150034)<br />\n(['INDUS', 'RM', 'AGE', 'LSTAT'], 0.054158313631716165)<br />\n(['CRIM', 'RM', 'AGE', 'LSTAT'], -0.030778806073267474)<br />\n(['CRIM', 'RM', 'PTRATIO', 'LSTAT'], 0.022935961564662693)<br />\n(['CRIM', 'INDUS', 'RM', 'AGE', 'TAX', 'LSTAT'], 0.022200426774483421)<br />\n(['CRIM', 'RM', 'DIS', 'LSTAT'], 0.016906509656987388)<br />\n(['CRIM', 'INDUS', 'RM', 'AGE', 'LSTAT'], -0.016840238405056267)<br />\n</code></p></blockquote><p>The majority of the delta came from the feature for number of rooms (RM), in conjunction with demographics data (LSTAT).</p>\n<h2>Summary</h2><p>Making random forest predictions interpretable is pretty straightforward, leading to a similar level of interpretability as linear models. However, in some cases, tracking the feature interactions can be important, in which case representing the results as a linear combination of features can be misleading. By using the <em>joint_contributions</em> keyword for prediction in the <a href=\"https://github.com/andosa/treeinterpreter/\">treeinterpreter</a> package, one can trivially take into account feature interactions when breaking down the contributions.</p>\n<div style=\"padding-top:0px;\t\npadding-right:0px;\npadding-bottom:0px;\npadding-left:0px;\nmargin-top:0px;\nmargin-right:0px;\nmargin-bottom:0px;\nmargin-left:0px;\"><a href=\"https://twitter.com/crossentropy\" class=\"twitter-follow-button\" \n\t\t\t\t\t\tdata-show-count=\"false\"\n\t\t\t\t\t\tdata-lang=\"autoLANGauto\"\n\t\t\t\t\t\tdata-width=\"250px\"\n\t\t\t\t\t\tdata-align=\"left\"\n\t\t\t\t\t\tdata-show-screen-name=\"true\"\n\t\t\t\t\t\tdata-size=\"medium\"\n\t\t\t\t\t\tdata-dnt=\"false\">\n\t\t\t\t\t\tFollow @crossentropy </a> </div>\n\t\t\t\t\t\t<script>\n\t\t\t\t\t\t!function(d,s,id) {\n\t\t\t\t\t\t  var js,fjs=d.getElementsByTagName(s)[0];\n\t\t\t\t\t\t  if(!d.getElementById(id)) {\n\t\t\t\t\t\t   js=d.createElement(s);\n\t\t\t\t\t\t   js.id=id;js.src=\"//platform.twitter.com/widgets.js\";\n\t\t\t\t\t\t   fjs.parentNode.insertBefore(js,fjs);\n\t\t\t\t\t\t  }\n\t\t\t\t\t\t}\n\t\t\t\t\t\t(document,\"script\",\"twitter-wjs\");\n\t\t\t\t\t\t</script><div style=\"padding-top:0px;\t\npadding-right:0px;\npadding-bottom:0px;\npadding-left:0px;\nmargin-top:0px;\nmargin-right:0px;\nmargin-bottom:0px;\nmargin-left:0px;\"><a href=\"https://twitter.com/share\" class=\"twitter-share-button\" \n\t\t\t\t        data-url=\"http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/\" \n\t\t\t\t        data-via=\"crossentropy\"\n\t\t\t\t\t    data-text=\"Random forest interpretation - conditional feature contributions\"\n\t\t\t\t\t    data-related=\"\"\n\t\t\t\t\t    data-count=\"horizontal\"\n\t\t\t\t\t    data-hashtags=\"\"\n\t\t\t\t\t    data-lang=\"autoLANGauto\"\n\t\t\t\t\t    data-counturl=\"\"\n\t\t\t\t\t    data-size=\"medium\"\n\t\t\t\t\t    data-dnt=\"false\"\t> Tweet </a> </div>\n\t\t                <script>\n\t\t\t\t\t    !function(d,s,id) {\n\t\t\t\t\t      var js,fjs=d.getElementsByTagName(s)[0];\n\t\t\t\t\t      if(!d.getElementById(id)) {\n\t\t\t\t\t       js=d.createElement(s);js.id=id;js.src=\"https://platform.twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);\n\t\t\t\t\t      }\n\t\t\t\t\t    }\n\t\t\t\t\t   (document,\"script\",\"twitter-wjs\");\n\t\t\t\t\t    </script>",
  "wfw:commentRss": "http://blog.datadive.net/random-forest-interpretation-conditional-feature-contributions/feed/",
  "slash:comments": 25
}