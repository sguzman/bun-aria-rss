{
  "title": "Extreme IO performance with parallel Apache Parquet in Python",
  "link": "",
  "published": "2017-02-10T10:00:00-08:00",
  "updated": "2017-02-10T10:00:00-08:00",
  "author": {
    "name": "Wes McKinney"
  },
  "id": "tag:wesmckinney.com,2017-02-10:/blog/python-parquet-multithreading/",
  "summary": "<p>In this post, I show how Parquet can encode very large datasets in a small file\nfootprint, and how we can achieve data throughput significantly exceeding disk\nIO bandwidth by exploiting parallelism (multithreading).</p>",
  "content": "<p>In this post, I show how Parquet can encode very large datasets in a small file\nfootprint, and how we can achieve data throughput significantly exceeding disk\nIO bandwidth by exploiting parallelism (multithreading).</p>\n\n\n<h2>Apache Parquet: Top performer on low-entropy data</h2>\n<p>As you can read in the Apache Parquet <a href=\"https://github.com/apache/parquet-format\">format specification</a>, the format\nfeatures multiple layers of encoding to achieve small file size, among them:</p>\n<ul>\n<li>Dictionary encoding (similar to how <code>pandas.Categorical</code> represents data, but\n  they aren't equivalent concepts)</li>\n<li>Data page compression (Snappy, Gzip, LZO, or Brotli)</li>\n<li>Run-length encoding (for null indicators and dictionary indices) and integer\n  bit-packing</li>\n</ul>\n<p>To give you an idea of how this works, let's consider the dataset:</p>\n<div class=\"github\"><pre><span></span><code>[&#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;,\n &#39;banana&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;apple&#39;]\n</code></pre></div>\n\n<p>Almost all Parquet implementations dictionary encode by default. So the first\npass encoding becomes:</p>\n<div class=\"github\"><pre><span></span><code>dictionary: [&#39;banana&#39;, &#39;apple&#39;]\nindices: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n</code></pre></div>\n\n<p>The dictionary indices are further run-length encoded:</p>\n<div class=\"github\"><pre><span></span><code>dictionary: [&#39;banana&#39;, &#39;apple&#39;]\nindices (RLE): [(8, 0), (3, 1)]\n</code></pre></div>\n\n<p>Working backwards, you can easily reconstruct the original dense array of\nstrings.</p>\n<p>In my <a href=\"https://wesmckinney.com/blog/python-parquet-update/\">prior blog post</a>, I created a dataset that compresses very well with\nthis style of encoding. When writing with <code>pyarrow</code>, we can turn on and off\ndictionary encoding (which is on by default) to see how it impacts file size:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">pyarrow.parquet</span> <span class=\"k\">as</span> <span class=\"nn\">pq</span>\n\n<span class=\"n\">pq</span><span class=\"o\">.</span><span class=\"n\">write_table</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">out_path</span><span class=\"p\">,</span> <span class=\"n\">use_dictionary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n               <span class=\"n\">compression</span><span class=\"o\">=</span><span class=\"s1\">&#39;snappy)</span>\n</code></pre></div>\n\n<p>With a dataset that occupies 1 gigabyte (1024 MB) in a pandas.DataFrame, with\nSnappy compression and dictionary encoding, it occupies an amazing <strong>1.436\nMB</strong>, small enough to fit on an old-school floppy disk. Without dictionary\nencoding, it occupies <strong>44.4 MB</strong>.</p>\n<h2>Parallel reads in parquet-cpp via PyArrow</h2>\n<p>In <a href=\"https://github.com/apache/parquet-cpp\">parquet-cpp</a>, the C++ implementation of Apache Parquet, which we've made\navailable to Python in PyArrow, we recently added parallel column reads.</p>\n<p>To try this out, install PyArrow from conda-forge:</p>\n<div class=\"github\"><pre><span></span><code>conda install pyarrow -c conda-forge\n</code></pre></div>\n\n<p>Now, when reading a Parquet file, use the <code>nthreads</code> argument:</p>\n<div class=\"github\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">pyarrow.parquet</span> <span class=\"k\">as</span> <span class=\"nn\">pq</span>\n\n<span class=\"n\">table</span> <span class=\"o\">=</span> <span class=\"n\">pq</span><span class=\"o\">.</span><span class=\"n\">read_table</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">,</span> <span class=\"n\">nthreads</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n</code></pre></div>\n\n<p>For low entropy data, decompression and decoding becomes CPU-bound. Because we\nare doing all the work in C++, we are not burdened by the concurrency issues of\nthe GIL and thus can achieve a significant speed boost. See the results I\nachieved reading a 1 GB dataset to a pandas DataFrame on my quad-core laptop\n(Xeon E3-1505M, NVMe SSD):</p>\n<p><center>\n<img src=\"../../images/parquet_multithreaded_benchmarks.png\"\n     alt=\"Parallel Parquet Python performance\"/>\n</center></p>\n<p><a href=\"https://gist.github.com/wesm/2108100781481d342fa129b648fdc4ae\">Click here for full benchmarking script</a></p>\n<p>I included performance both for the dictionary-encoded and non-dictionary\nencoded cases. For low entropy data, even though the files are all small (~1.5\nMB with dictionaries and ~45 MB without), the impact of dictionary encoding on\nperformance is substantial. With 4 threads, the performance reading into pandas\nbreaks through an amazing <strong>4 GB/s</strong>. This is much faster than Feather format\nor other alternatives I've seen.</p>\n<h2>Conclusions</h2>\n<p>With the 1.0 release of parquet-cpp (Apache Parquet in C++) on the horizon,\nit's great to see this kind of IO performance made available to the Python user\nbase.</p>\n<p>Since all of the underlying machinery here is implemented in C++, other\nlanguages (such as R) can build interfaces to Apache Arrow (the common columnar\ndata structures) and parquet-cpp. The Python bindings are a lightweight wrapper\non top of the underlying <code>libarrow</code> and <code>libparquet</code> C++ libraries.</p>"
}