{
  "title": "Is your layer over-fit? (part 2)",
  "link": "https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/",
  "comments": "https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/#respond",
  "dc:creator": "Charles H Martin, PhD",
  "pubDate": "Tue, 14 Jun 2022 20:53:05 +0000",
  "category": "Uncategorized",
  "guid": "http://calculatedcontent.com/?p=14437",
  "description": "Say you are training a Deep Neural Network (DNN), and you see your model is over-trained. Or just not performing &#8230; <a class=\"more-link\" href=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/\">More</a>",
  "content:encoded": "\n<p>Say you are training a Deep Neural Network (DNN), and you see your model is over-trained. Or just not performing well.   Is there a way to detect which layer is actually over-trained?  (or over-fit, as some people call it)</p>\n\n\n\n<p>In this post, we will show how to use the open-source weightwatcher tool to answer this.</p>\n\n\n\n<p>WeightWatcher is an open-source, data-free diagnostic tool for analyzing (pre-)trained DNNs. It is based on my personal research into Why Deep Learning Works, in collaboration with UC Berkeley. It is based on ideas from the Statistical Mechanics of Learning (i.e theoretical physics and chemistry).</p>\n\n\n\n<p class=\"has-text-align-center\"><code>pip install weightwatcher</code></p>\n\n\n\n<p>WeightWatcher lets you inspect your layer weight matrices to see if they are converging properly. And in some cases, it can even tell you if the layer is over-trained.  The idea is simple. If you are training a model, and you over-regularize one of the layer, then you any observe the weightwatcher alpha metric drops below 2 (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 2\" class=\"latex\" />).  This is predicted by our HTSR theory of learning (although we have not published this specific result yet). And very unique as no other approach can do this.</p>\n\n\n\n<p>To see how this works, we will look at a very specific, carefully-designed experiment where the theory is known to work exactly as advertised.  </p>\n\n\n\n<p class=\"has-text-align-center\"><strong>BUT (and here&#8217;s the disclaimer)</strong></p>\n\n\n\n<p><em>Please be aware&#8211;training DNNs to State-of-the-Art (SOTA) is not easy, and applying the tool requires designing careful experiments that can isolate the problems you are trying to fix.  It does not work in every case, and you may see unusual results that are difficult to interpret.  In these cases, please feel free <a href=\"https://www.linkedin.com/in/charlesmartin14/\" target=\"_blank\" rel=\"noreferrer noopener\">to reach out to me directly</a>, and<a href=\"https://join.slack.com/t/weightwatcherai/shared_invite/zt-1511mk1d2-OvauYoot8_gm_YKIRT381Q\" target=\"_blank\" rel=\"noreferrer noopener\"> join our Slack channel </a>to get help.</em></p>\n\n\n\n<p class=\"has-text-align-center\"><strong>Having said that, let&#8217;s get started</strong></p>\n\n\n\n<p>First, <a rel=\"noreferrer noopener\" href=\"https://github.com/CalculatedContent/WeightWatcher/blob/master/WW_MLP3_BatchSizes.ipynb\" target=\"_blank\">here&#8217;s the Google </a><a href=\"https://github.com/CalculatedContent/WeightWatcher/blob/master/examples/WW_MLP3_BatchSizes.ipynb\" target=\"_blank\" rel=\"noreferrer noopener\">Colab</a><a rel=\"noreferrer noopener\" href=\"https://github.com/CalculatedContent/WeightWatcher/blob/master/WW_MLP3_BatchSizes.ipynb\" target=\"_blank\"> notebook</a> for reproducing this post; please try it yourself.</p>\n\n\n\n<h2>Experimental Design</h2>\n\n\n\n<p>We consider a very simple DNN, a 3-layer MLP (Multi-Layer Perceptron), trained on MNIST.   </p>\n\n\n\n<p>To induce the overtraining, we will train this model using different batch sizes, with <code>batch_size in [1,2,4,8,16,32]</code>.  </p>\n\n\n\n<p><strong>Why do we vary the batch sizes</strong> ?&#8230; and not a specific regularization hyper-parameter like Weight Decay or Dropout?  The batch size acts like a very strong regularizer, which can induce the Heavy-Tails we see in SOTA models even in this very small model and generally poorly performing model.  This is shown in Figure 25 of <a href=\"https://jmlr.org/papers/volume22/20-410/20-410.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">our JMLR paper describing our theory of Heavy-Tailed Self-Regularization (HT-SR)</a>, the theory behind weightwatcher.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png\"><img data-attachment-id=\"14455\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-10-38-14-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png\" data-orig-size=\"1262,898\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2022-06-14-at-10.38.14-am\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=1024\" alt=\"\" class=\"wp-image-14455\" srcset=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.38.14-am.png 1262w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure></div>\n\n\n<p>Moreover, with extremely small batch sizes, and a long number of epochs, we can even drive the model into a state of over-training.  Which is the goal here.So each model is trained for a very long number of epochs, and until the training loss stabilizes, using a Keras EarlyStopping Callback</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ntf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, verbose=0, min_delta=0.001, restore_best_weights=True)e()\n</pre></div>\n\n\n<p><strong>In your own models, the situation may be more complex</strong>.   </p>\n\n\n\n<p>The weightwatcher metrics work best when applied to SOTA models because this is when the layer weight matrics are best correlated, and the Power Law fits work the best.  It takes some work to design experiments on small models that can flush out these features.  So we choose to use the batch size to induce this effect.  But let me encourage you to try other approaches.</p>\n\n\n\n<p>The key to using the HTSR theory is to carefully control the training so that when you adjust some other knob (i.e Dropout, momentum, weight decay) that the training and test error change smoothly and systematically.  If, however,  the training accuracy or loss is unstable, and you are jumping all over the loss landscape, then HTSR theory, is more difficult to apply.  So, here, </p>\n\n\n\n<p class=\"has-text-align-center\"><em><strong>I follow the KISS mantra: &#8220;Keep It Super Simple!&#8221;</strong></em></p>\n\n\n\n<h3>Reproducibility</h3>\n\n\n\n<p>To compare 2 or more models to each other, with different batch sizes, for the purposes here, we need to ensure they have been trained with the exact same initial conditions.  To do this, we have to both set all the random seeds to a default value and tell the framework (here, Keras) to use deterministic options.  This also, nicely, makes the experiments 100% reproducible.  </p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\n%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n\nimport random\ndef reset_random_seeds(seed_value=42):\n   os.environ&#91;'PYTHONHASHSEED']=str(seed_value)\n   tf.random.set_seed(seed_value)\n   tf.keras.utils.set_random_seed(seed_value)\n   np.random.seed(seed_value)\n   random.seed(seed_value)\n   tf.config.experimental.enable_op_determinism()\n\n</pre></div>\n\n\n<p>Every time we build the model, we will first run <code>reset_random_seeds()</code>to ensure that every run, with different batch sizes, regularization, etc, is stated from the same spot and is reproducible.</p>\n\n\n\n<h3>Model Size and Shape: The Three (3) Layers</h3>\n\n\n\n<p>This model has 3 layers: input, hidden, and output. Note that each layer is initialized in the same way (i.e with GlorotNormalization, with the same seed).  Also, here, to <em>keep it super simple</em>,  no specific regularization is applied to the model (except for the changing of the batch size).</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\ninitializer = tf.keras.initializers.GlorotNormal(seed=1)\n  model = tf.keras.models.Sequential(&#91;\n      tf.keras.layers.Flatten(input_shape = &#91;28,28]),\n      tf.keras.layers.Dense(300, activation='relu', kernel_initializer=initializer),\n      tf.keras.layers.Dense(100, activation='relu', kernel_initializer=initializer),\n      tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer),\n  ])escribe()Also, \n</pre></div>\n\n\n<p>We can inspect the model using weightwatcher to see how the layers are labeled (layer_id), what kind of layer they are (DENSE, Conv2D, etc), and what their shapes are (N, M).  </p>\n\n\n\n<div class=\"is-layout-flow wp-block-group\"><div class=\"wp-block-group__inner-container\"><div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nimport weightwatcher as ww\nwatcher = ww.WeightWatcher(model=model)\nwatcher.describe()\n</pre></div></div></div>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png\"><img data-attachment-id=\"14444\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-10-08-35-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png\" data-orig-size=\"1096,288\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2022-06-14-at-10.08.35-am\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=1024\" alt=\"\" class=\"wp-image-14444\" srcset=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-10.08.35-am.png 1096w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\">WeightWatcher Descirption DataFrame</figcaption></figure></div>\n\n\n<p>In this experiment, we will analyze layer 1 (the Hidden Layer) and only layer 1.  This layer is a DENSE layer, which has a single weight matrix of dimension 100&#215;300.  It will have 100 eigenvalues, which is a large enough size for weightwatcher to analyze.  And for this super, simple experiment, this is the only later that is trainable; all other layers are held fixed.</p>\n\n\n\n<h3>Training the model (with different batch sizes)</h3>\n\n\n\n<p>Again, we will train the same model, with the same exact same initial conditions,  in a deterministic way, while changing the batch size.  For each fully trained model, we then compute the weighwatcher Power-Law capacity metric alpha (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />).  We will then compare the layer 1 alpha <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />)  to the model test accuracy for each run.</p>\n\n\n\n<p>Notice first that, however, when decreasing the batch size, both the training accuracy and the test accuracy improve both smoothly and systematically, and then drop off suddenly.  For example, below,  see that test accuracy increases from 89.0% at batch size 32 to 89.4% at batch size 4, and then drops off suddenly for batch size 2 down to 88.5%.   (The training accuracy behaves in a similar way when decreases the batch size, as can be seen in the notebook).</p>\n\n\n\n<p class=\"has-text-align-center\"><img data-attachment-id=\"14488\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-1-40-41-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.40.41-pm.png\" data-orig-size=\"830,570\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2022-06-14 at 1.40.41 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.40.41-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.40.41-pm.png?w=830\" class=\"wp-image-14488\" style=\"width:500px;\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.40.41-pm.png\" alt=\"\"></p>\n\n\n\n<p></p>\n\n\n\n<p>Likewise, the training loss is varying smoothly, and the optimizer is not jumping all over the energy landscape.  This indicates a clean experiment, amenable to analysis.</p>\n\n\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png\"><img data-attachment-id=\"14467\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-11-13-31-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png\" data-orig-size=\"1276,576\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2022-06-14-at-11.13.31-am\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=1024\" alt=\"\" class=\"wp-image-14467\" srcset=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.13.31-am.png 1276w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption class=\"wp-element-caption\">Training and test losses for a sample run training the 3-layer MLP</figcaption></figure></div>\n\n\n<p> </p>\n\n\n\n<p>(Notice that we apply early stopping to the training loss, not the validation loss.  That is because, in this experiment, we are trying to drive the model to a state of over-training by reducing the batch size, and going past the perhaps more common early stopping critera on the validation loss.  Also, since we are changing the batch size, we want to ensure each model runs with enough epochs to the runs can be compared to each other).</p>\n\n\n\n<h3>The WeightWatcher Layer Capacity Matric Alpha (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />)</h3>\n\n\n\n<p>To compute the weightwatcher metrics, at the end of every training cycle, just run</p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\">\nresults = watcher.analyze(layers=&#91;1])\n</pre></div>\n\n\n<p>The <code>watcher.analyze() </code>method will generate a pandas dataframe, with layer by layer metrics.</p>\n\n\n\n<p><strong>What does alpha mean?  </strong>Alpha (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha\" class=\"latex\" />) is a measure of how Heavy-Tailed the layer is.  It can be found, crudely, by simply plotting a histogram of the eigenvalue of the layer correlation matrix, <code><strong>X</strong>=np.dot(W.T,W)</code>, on a log-log scale, and calculating the slope of this plot in the tail region.  Here is an example where <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%3D2.410&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%3D2.410&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%3D2.410&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha=2.410\" class=\"latex\" />.</p>\n\n\n\n<p class=\"has-text-align-center\"><img data-attachment-id=\"14472\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-11-19-39-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.19.39-am.png\" data-orig-size=\"786,600\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2022-06-14 at 11.19.39 AM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.19.39-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.19.39-am.png?w=786\" class=\"wp-image-14472\" style=\"width:500px;\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-11.19.39-am.png\" alt=\"\"></p>\n\n\n\n<p>The smaller alpha is, the more Heavy-Tailed the layer matrix <strong>X</strong> is, and the better the layer performs for the model.  <em>But only upto a point.</em>  If the layer is too Heavy-Tailed, where <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 2\" class=\"latex\" /> (for simple models)  then it may be over-trained.</p>\n\n\n\n<h2>Results: detecting an over-trained layer</h2>\n\n\n\n<p>We can now plot the alpha vs the test accuracy for layer 1, and the result is quite amazing. </p>\n\n\n\n<p class=\"has-text-align-center\"><img data-attachment-id=\"14487\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-1-38-38-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.38.38-pm.png\" data-orig-size=\"816,580\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2022-06-14 at 1.38.38 PM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.38.38-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.38.38-pm.png?w=816\" class=\"wp-image-14487\" style=\"width:500px;\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-1.38.38-pm.png\" alt=\"\"></p>\n\n\n\n<p>Notice 2 key things</p>\n\n\n\n<ul>\n<li>as the test accuracy increases, the alpha metric decreases (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Crightarrow+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Crightarrow+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5Crightarrow+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;rightarrow 2\" class=\"latex\" />)</li>\n\n\n\n<li>as soon the test accuracy drops (with batch size = 1),  alpha drops below 2 (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C+2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha < 2\" class=\"latex\" />)</li>\n</ul>\n\n\n\n<p>For simple models like this 3-layer MLP, the weightwatcher approach can, remarkably, detect which layer is over-trained!  No other theory can do this.</p>\n\n\n\n<p>For more complex models, with lots of parameters varying, the situation may be more complex. </p>\n\n\n\n<p>Let me encourage you to try the weightwatcher tool for yourself, and join our Slack channel to discuss this and other aspects of training large models to SOTA.</p>\n\n\n\n<h3>Why does alpha < 2 mean the layer may be over-trained ?</h3>\n\n\n\n<p>The weightwatcher alpha $(latex \\alpha)$ metric is the exponent found when fitting the empirical spectral density (ESD), or a histogram of the eigenvalues, to a Power-Law distribution.   Moreover, when alpha is between roughly 2 and higher (theoretically 4, practically, upto 6, <img src=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha%5Cin%5B2%2C6%5D&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha&#92;in[2,6]\" class=\"latex\" />), <a href=\"https://jmlr.org/papers/volume22/20-410/20-410.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">as shown in our JMLR paper,</a> we can use our HTSR theory to characterize the layer weight matrix as being Moderately Heavy-Tailed. See Table 1:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><a href=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png\"><img data-attachment-id=\"14476\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-14-at-12-36-24-pm/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png\" data-orig-size=\"1418,780\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"screen-shot-2022-06-14-at-12.36.24-pm\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=1024\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=1024\" alt=\"\" class=\"wp-image-14476\" srcset=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=1024 1024w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=150 150w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=300 300w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png?w=768 768w, https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-14-at-12.36.24-pm.png 1418w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a></figure>\n\n\n\n<p>When a Power Law distribution is simply Moderately Heavy-Tailed,  this means that, in the limit, the variance may be unbounded, but the average (or mean) value is well defined.  So, for Deep Learning, this implies that the model has learned a wide variety of correlations, but, on average, the correlations are reasonably bounded, moreover, typical.  Being typical, the layer weight matrix model can be used to describe the information in the training and the test data, as long as they come from the same data distribution,</p>\n\n\n\n<p>But when the alpha is very small (<img src=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002\" srcset=\"https://s0.wp.com/latex.php?latex=%5Calpha+%3C2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Calpha+%3C2&#038;bg=%23ffffff&#038;fg=%23000000&#038;s=0&#038;c=20201002&#038;zoom=4.5 4x\" alt=\"&#92;alpha <2\" class=\"latex\" />), this means the layer weight matrix is Very Heavy-Tailed, and the layer weight matrix is <strong>atypical.</strong>  That is, the distributions of the correlations do not have a well-defined average of mean value, and the individual elements of W may even themselves be unbounded (ie. when you have <a rel=\"noreferrer noopener\" href=\"https://calculatedcontent.com/2021/04/04/are-your-models-overtrained/\" target=\"_blank\">a Correlation Trap</a>). Therefore, this layer weight matrix can not be used to describe any data except the training data.  </p>\n\n\n\n<p class=\"has-text-align-center\"><img data-attachment-id=\"14509\" data-permalink=\"https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/screen-shot-2022-06-24-at-8-18-10-am/\" data-orig-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-24-at-8.18.10-am.png\" data-orig-size=\"798,576\" data-comments-opened=\"1\" data-image-meta=\"{\"aperture\":\"0\",\"credit\":\"\",\"camera\":\"\",\"caption\":\"\",\"created_timestamp\":\"0\",\"copyright\":\"\",\"focal_length\":\"0\",\"iso\":\"0\",\"shutter_speed\":\"0\",\"title\":\"\",\"orientation\":\"0\"}\" data-image-title=\"Screen Shot 2022-06-24 at 8.18.10 AM\" data-image-description=\"\" data-image-caption=\"\" data-medium-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-24-at-8.18.10-am.png?w=300\" data-large-file=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-24-at-8.18.10-am.png?w=798\" class=\"wp-image-14509\" style=\"width:400px;\" src=\"https://charlesmartin14.files.wordpress.com/2022/06/screen-shot-2022-06-24-at-8.18.10-am.png\" alt=\"\"></p>\n\n\n\n<p class=\"has-text-align-center\"><strong>A Correlation Trap appears when the batch size = 1</strong></p>\n\n\n\n<p>Seeing this in practice is not necessarily easy, and interpreting it is harder.  As here, one may have to design a very careful experiment to flush this out.  Still, we encourage you to try the tool out, try to use it to identify and resolve such problems, and please give feedback.  </p>\n\n\n\n<h2>Final Plug</h2>\n\n\n\n<p>And if you need help with AI, ML, or just Data Science, please reach out. I provide strategy consulting, data science leadership, and hands-on, heads-down development. I will have availability in Q3 2022 for new projects. Reach out today.&nbsp;<a href=\"https://www.linkedin.com/feed/hashtag/?keywords=talktochuck&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6941808930264150016\">#talkToChuck</a>&nbsp;<a href=\"https://www.linkedin.com/feed/hashtag/?keywords=theaiguy&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6941808930264150016\">#theAIguy</a></p>\n",
  "wfw:commentRss": "https://calculatedcontent.com/2022/06/14/is-your-layer-over-trained-part-2/feed/",
  "slash:comments": 0,
  "media:thumbnail": "",
  "media:content": [
    {
      "media:title": "Screen Shot 2022-06-14 at 1.38.38 PM"
    },
    {
      "media:title": "charlesmartin14"
    },
    "",
    "",
    "",
    "",
    "",
    "",
    ""
  ]
}