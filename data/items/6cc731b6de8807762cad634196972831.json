{
  "title": "The LP-update policy for weakly coupled Markov decision processes. (arXiv:2211.01961v1 [math.OC])",
  "link": "http://arxiv.org/abs/2211.01961",
  "description": "<p>In this work we propose a novel policy called the LP-update policy for finite\nhorizon weakly coupled Markov decision processes. The latter can be seen as\nmulti-constraint multi-action bandits, and generalizes the classical restless\nbandit problems (that are single-constraint two-action bandits), widely studied\nin the literature. We consider a scaling model with $N$ statistically identical\narms. We show that our LP-update policy becomes asymptotically optimal at rate\n$O(1/\\sqrt{N})$ for any problem. This rate can be improved to $O(1/N)$ if the\nproblem is non-degenerate, and even to $e^{-\\Omega(N)}$ if in addition the\nproblem admits a perfect rounding. The definition of non-degeneracy extends the\nsame notion for the classical two-action restless bandits. By using this\nproperty, we also provide a more efficient implementation of the LP-update\npolicy. We illustrate the performance of our policy in a generalized applicant\nscreening problem.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/math/1/au:+Gast_N/0/1/0/all/0/1\">Nicolas Gast</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gaujal_B/0/1/0/all/0/1\">Bruno Gaujal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yan_C/0/1/0/all/0/1\">Chen Yan</a>"
}