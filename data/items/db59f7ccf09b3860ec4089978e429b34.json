{
  "title": "Spoken language identification with deep convolutional networks",
  "link": "",
  "updated": "2015-10-11T00:00:00+00:00",
  "id": "http://yerevann.github.io//2015/10/11/spoken-language-identification-with-deep-convolutional-networks",
  "content": "<p>By <a href=\"https://github.com/Harhro94\">Hrayr Harutyunyan</a></p>\n\n<p>Recently <a href=\"https://topcoder.com/\">TopCoder</a> announced a <a href=\"https://community.topcoder.com/longcontest/?module=ViewProblemStatement&amp;rd=16555&amp;compid=49304\">contest</a>\nto identify the spoken language in audio recordings. I decided to test how well \ndeep convolutional networks will perform on this kind of data. In short I managed to get \naround 95% accuracy and finished at the 10th place. This post reveals all the details.</p>\n\n<!--more-->\n\n<h2 class=\"no_toc\" id=\"contents\">Contents</h2>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#dataset-and-scoring\" id=\"markdown-toc-dataset-and-scoring\">Dataset and scoring</a></li>\n  <li><a href=\"#preprocessing\" id=\"markdown-toc-preprocessing\">Preprocessing</a></li>\n  <li><a href=\"#network-architecture\" id=\"markdown-toc-network-architecture\">Network architecture</a></li>\n  <li><a href=\"#data-augmentation\" id=\"markdown-toc-data-augmentation\">Data augmentation</a></li>\n  <li><a href=\"#ensembling\" id=\"markdown-toc-ensembling\">Ensembling</a></li>\n  <li><a href=\"#what-we-learned-from-this-contest\" id=\"markdown-toc-what-we-learned-from-this-contest\">What we learned from this contest</a></li>\n  <li><a href=\"#unexplored-options\" id=\"markdown-toc-unexplored-options\">Unexplored options</a></li>\n</ul>\n\n<h2 id=\"dataset-and-scoring\">Dataset and scoring</h2>\n\n<p>The recordings were in one of the 176 languages. Training set consisted of 66176 <code class=\"highlighter-rouge\">mp3</code> files, \n376 per language, from which I have separated 12320 recordings for validation \n(Python script is <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/choose_val_set.py\">available on GitHub</a>). \nTest set consisted of 12320 <code class=\"highlighter-rouge\">mp3</code> files. All recordings had the same length (~10 sec) \nand seemed to be noise-free (at least all the samples that I have checked).</p>\n\n<p>Score was calculated the following way: for every <code class=\"highlighter-rouge\">mp3</code> top 3 guesses were uploaded in a CSV file. \n1000 points were given if the first guess is correct,\n400 points if the second guess is correct and 160 points if the third guess is correct. \nDuring the contest the score was calculated only on 3520 recordings from the test set. \nAfter the contest the final score was calculated on the remaining 8800 recordings.</p>\n\n<h2 id=\"preprocessing\">Preprocessing</h2>\n\n<p>I entered the contest just 14 days before the deadline, so didn’t have much time to investigate\naudio specific techniques. But we had a deep convolutional network developed few months ago,\nand it seemed to be a good idea to test a pure CNN on this problem. \nSome Google search revealed that the idea is not new. The earliest attempt I could find was a \n<a href=\"http://research.microsoft.com/en-us/um/people/dongyu/nips2009/papers/montavon-paper.pdf\">paper by G. Montavon</a>\npresented in NIPS 2009 conference. The author used a network with 3 convolutional layers trained on \n<a href=\"https://en.wikipedia.org/wiki/Spectrogram\">spectrograms</a> of audio recordings, and \nthe output of convolutional/subsampling layers was given to a <a href=\"https://en.wikipedia.org/wiki/Time_delay_neural_network\">time-delay neural network</a>.</p>\n\n<p>I found a <a href=\"http://www.frank-zalkow.de/en/code-snippets/create-audio-spectrograms-with-python.html?ckattempt=1\">Python script</a> \nwhich creates a spectrogram of a <code class=\"highlighter-rouge\">wav</code> file. I used <a href=\"http://www.mpg123.de/index.shtml\"><code class=\"highlighter-rouge\">mpg123</code> library</a> \nto convert <code class=\"highlighter-rouge\">mp3</code> files to <code class=\"highlighter-rouge\">wav</code> format.</p>\n\n<p>The preprocessing script is available on <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py\">GitHub</a>.</p>\n\n<h2 id=\"network-architecture\">Network architecture</h2>\n\n<p>I took the network architecture designed for the Kaggle’s <a href=\"/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/\">diabetic retinopathy detection contest</a>. \nIt has 6 convolutional layers and 2 fully connected layers with 50% dropout. \nActivation function is always ReLU. Learning rates are set to be higher for\nthe first convolutional layers and lower for the top convolutional layers. \nThe last fully connected layer has 176 neurons and is trained using a softmax loss.</p>\n\n<p>It is important to note that this network does not take into account the sequential characteristics\nof the audio data. Although recurrent networks perform well on speech recognition tasks \n(one notable example is <a href=\"http://arxiv.org/abs/1303.5778\">this paper</a> \nby A. Graves, A. Mohamed and G. Hinton, cited by 272 papers according to the Google Scholar), \nI didn’t have time to learn how they work.</p>\n\n<p>I trained the CNN on <a href=\"http://caffe.berkeleyvision.org\">Caffe</a> with 32 images in a batch,\nits description in Caffe prototxt format is available <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/main_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024rd0.5-1024rd0.5_DLR.prototxt\">here</a>.</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td>Nr</td>\n      <td>Type</td>\n      <td>Batches</td>\n      <td>Channels</td>\n      <td>Width</td>\n      <td>Height</td>\n      <td>Kernel size / stride</td>\n    </tr>\n    <tr>\n      <td>0</td>\n      <td>Input</td>\n      <td>32</td>\n      <td>1</td>\n      <td>858</td>\n      <td>256</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>32</td>\n      <td>852</td>\n      <td>250</td>\n      <td>7x7 / 1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>32</td>\n      <td>852</td>\n      <td>250</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>32</td>\n      <td>426</td>\n      <td>125</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>64</td>\n      <td>422</td>\n      <td>121</td>\n      <td>5x5 / 1</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>64</td>\n      <td>422</td>\n      <td>121</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>64</td>\n      <td>211</td>\n      <td>60</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>64</td>\n      <td>209</td>\n      <td>58</td>\n      <td>3x3 / 1</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>64</td>\n      <td>209</td>\n      <td>58</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>64</td>\n      <td>104</td>\n      <td>29</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>128</td>\n      <td>102</td>\n      <td>27</td>\n      <td>3x3 / 1</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>128</td>\n      <td>102</td>\n      <td>27</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>128</td>\n      <td>51</td>\n      <td>13</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>128</td>\n      <td>49</td>\n      <td>11</td>\n      <td>3x3 / 1</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>128</td>\n      <td>49</td>\n      <td>11</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>128</td>\n      <td>24</td>\n      <td>5</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>Conv</td>\n      <td>32</td>\n      <td>256</td>\n      <td>22</td>\n      <td>3</td>\n      <td>3x3 / 1</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>ReLU</td>\n      <td>32</td>\n      <td>256</td>\n      <td>22</td>\n      <td>3</td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>MaxPool</td>\n      <td>32</td>\n      <td>256</td>\n      <td>11</td>\n      <td>1</td>\n      <td>3x3 / 2</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>Fully connected</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>ReLU</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>Dropout</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>Fully connected</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>ReLU</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>Dropout</td>\n      <td>20</td>\n      <td>1024</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>Fully connected</td>\n      <td>20</td>\n      <td>176</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>Softmax Loss</td>\n      <td>1</td>\n      <td>176</td>\n      <td> </td>\n      <td> </td>\n      <td> </td>\n    </tr>\n  </tbody>\n</table>\n\n<p><a href=\"https://github.com/Hrant-Khachatrian\">Hrant</a> suggested to try the <a href=\"http://arxiv.org/abs/1212.5701\"><code class=\"highlighter-rouge\">ADADELTA</code> solver</a>.\nIt is a method which dynamically calculates learning rate for every network parameter, and the \ntraining process is said to be independent of the initial choice of learning rate. Recently it\nwas <a href=\"https://github.com/BVLC/caffe/pull/2782\">implemented in Caffe</a>.</p>\n\n<p>In practice, the base learning rate set in the Caffe solver did matter. At first I tried to use <code class=\"highlighter-rouge\">1.0</code> \nlearning rate, and the network didn’t learn at all. Setting the base learning rate to <code class=\"highlighter-rouge\">0.01</code>\nhelped a lot and I trained the network for 90 000 iterations (more than 50 epochs). \nThen I switched to <code class=\"highlighter-rouge\">0.001</code> base learning rate for another 60 000\niterations. The solver is available <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/solver.main.adadelta.prototxt\">here</a>. \nNot sure why the base learning rate mattered so much at the early stages of the training.\nOne possible reason could be the large learning rate coefficients on the lower convolutional layers.\nBoth tricks (dynamically updating the learning rates in <code class=\"highlighter-rouge\">ADADELTA</code> and large learning rate coefficients)\naim to fight the gradient vanishing problem, and maybe their combination is not a very good idea. \nThis should be carefully analysed.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th><img src=\"/public/2015-10-11/no-augm-loss.jpg\" alt=\"Training (blue) and validation (red) loss\" title=\"Training (blue) and validation (red) loss\" /></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Training (blue) and validation (red) loss over the 150 000 iterations on the non-augmented dataset. The sudden drop of training loss corresponds to the point when the base learning rate was changed from <code class=\"highlighter-rouge\">0.01</code> to <code class=\"highlighter-rouge\">0.001</code>. Plotted using <a href=\"https://github.com/YerevaNN/Caffe-python-tools/blob/master/plot_loss.py\">this script</a>.</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The signs of overfitting were getting more and more visible and I stopped at 150 000 iterations. \nThe softmax loss got to 0.43 and it corresponded to 3 180 000 score \n(out of 3 520 000 possible). Some ensembling with other models of the same network allowed to\nget a bit higher score (3 220 000), but it was obvious that data augmentation is needed to overcome the \noverfitting problem.</p>\n\n<h2 id=\"data-augmentation\">Data augmentation</h2>\n\n<p>The most important weakness of our team in the <a href=\"/2015/08/17/diabetic-retinopathy-detection-contest-what-we-did-wrong/\">previous contest</a>\nwas that we didn’t augment the dataset well enough. So I was looking for ways to augment the \nset of spectrograms. One obvious idea was to crop random, say, 9 second intervals of the recordings.\nHrant suggested another idea: to warp the frequency axis of the spectrogram. This process is known as\n<em>vocal tract length perturbation</em>, and is generally used for speaker normalization at least \n<a href=\"http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=650310&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F89%2F14168%2F00650310\">since 1998</a>.\nIn 2013 <a href=\"https://www.cs.toronto.edu/~hinton/absps/perturb.pdf\">N. Jaitly and G. Hinton</a>\nused this technique to augment the audio dataset. I <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L32\">used this formula</a>\nto linearly scale the frequency bins during spectrogram generation:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th><img src=\"/public/2015-10-11/frequency-warp-formula.png\" alt=\"Frequency warping formula\" title=\"Frequency warping formula\" /></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Frequency warping formula from the <a href=\"http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=650310&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F89%2F14168%2F00650310\">paper by L. Lee and R. Rose</a>. α is the scaling factor. Following Jaitly and Hinton I <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L92\">chose it uniformly</a> between 0.9 and 1.1</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>I also <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/augment_data.py#L77\">randomly cropped</a>\nthe spectrograms so they had <code class=\"highlighter-rouge\">768x256</code> size. Here are the results:</p>\n\n<table>\n  <tbody>\n    <tr>\n      <td><img src=\"/public/2015-10-11/spectrogram.jpg\" alt=\"Spectrogram without modifications\" title=\"Spectrogram without modifications\" /></td>\n    </tr>\n    <tr>\n      <td>Spectrogram of one of the recordings</td>\n    </tr>\n    <tr>\n      <td><img src=\"/public/2015-10-11/spectrogram-warped-cropped.jpg\" alt=\"Cropped spectrogram with warped frequency axis\" title=\"Cropped spectrogram with warped frequency axis\" /></td>\n    </tr>\n    <tr>\n      <td>Cropped spectrogram of the same recording with warped frequency axis</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>For each <code class=\"highlighter-rouge\">mp3</code> I have created 20 random spectrograms, but trained the network on 10 of them. \nIt took more than 2 days to create the augmented dataset and convert it to LevelDB format (the format Caffe suggests). \nBut training the network proved to be even harder. For 3 days I couldn’t significantly decrease\nthe train loss. After removing the dropout layers the loss started to decrease but it would take weeks \nto reach reasonable levels. Finally, Hrant suggested to try to reuse the weights of the \nmodel trained on the non-augmented dataset. The problem was that due to the cropping,\nthe image sizes in the two datasets were different. But it turned out that convolutional \nand pooling layers in Caffe <a href=\"https://github.com/BVLC/caffe/issues/189#issuecomment-36754479\">work with images of variable sizes</a>, \nonly the fully connected layers couldn’t reuse the weights from the first model. \nSo I just <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/augm_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024r-1024r_DLR_nolrcoef.prototxt#L292\">renamed the FC layers</a>\nin the prototxt file and <a href=\"http://caffe.berkeleyvision.org/tutorial/interfaces.html#command-line\">initialized</a>\nthe network (convolution filters) by the weights of the first model:</p>\n\n<figure class=\"highlight\"><pre><code class=\"language-bash\" data-lang=\"bash\">./build/tools/caffe train <span class=\"nt\">--solver</span><span class=\"o\">=</span>solver.prototxt <span class=\"nt\">--weights</span><span class=\"o\">=</span>models/main_32r-2-64r-2-64r-2-128r-2-128r-2-256r-2-1024rd0.5-1024rd0.5_DLR_72K-adadelta0.01_iter_153000.caffemodel</code></pre></figure>\n\n<p>This helped a lot. I used standard stochastic gradient descent (inverse decay learning rate policy)\nwith base learning rate <code class=\"highlighter-rouge\">0.001</code> for 36 000 iterations (less than 2 epochs), then increased \nthe base learning rate to <code class=\"highlighter-rouge\">0.01</code> for another 48 000 iterations (due to the inverse decay policy\nthe rate decreased seemingly too much). \nThese trainings were done without any regularization techniques,\nweight decay or dropout layers, and there were clear signs of overfitting. I tried to add 50%\ndropout layers on fully connected layers, but the training was extremely slow. To improve the \nspeed I used 30% dropout, and trained the network for 120 000 more iterations using <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/prototxt/solver.augm.nolrcoef.prototxt\">this solver</a>.\nSoftmax loss on the validation set reached 0.21 which corresponded to 3 390 000 score. \nThe score was calculated by averaging softmax outputs over 20 spectrograms of each recording.</p>\n\n<h2 id=\"ensembling\">Ensembling</h2>\n\n<p>30 hours before the deadline I had several models from the same network. And even simple\nensembling (just the sum of softmax activations of different models) performed better than\nany individual model. Hrant suggested to use <a href=\"https://github.com/dmlc/xgboost\">XGBoost</a>, \nwhich is a fast implementation of <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">gradient boosting</a> \nalgorithm and is very popular among Kagglers. XGBoost has a good documentation and \nall parameters are <a href=\"https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\">well explained</a>.</p>\n\n<p>To perform the ensembling I was creating a CSV file containing softmax activations \n(or the average of softmax activations among <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/get_output_layers.py#L40\">20</a> \naugmented versions of the same recording) using <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/get_output_layers.py\">this script</a>.\nThen I was running XGBoost on these CSV files. The submission file (which was requested by TopCoder)\nwas generated using <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/make_submission.py\">this script</a>.</p>\n\n<p>I also tried to train a <a href=\"https://github.com/YerevaNN/Spoken-language-identification-CNN/blob/master/ensembling/ensemble.theano.py\">simple neural network</a>\nwith one hidden layer on the same CSV files. The results were significantly better than\nwith XGBoost.</p>\n\n<p>The best result was obtained by ensembling the following two models: snapshots of the last \nnetwork (the one with 30% dropout) after 90 000 iterations and 105 000 iterations. Final \nscore was 3 401 840 and it was the <a href=\"http://community.topcoder.com/longcontest/stats/?module=ViewOverview&amp;rd=16555\">10th result</a>\nof the contest.</p>\n\n<h2 id=\"what-we-learned-from-this-contest\">What we learned from this contest</h2>\n\n<p>This was a quite interesting contest, although too short when compared with Kaggle’s contests.</p>\n\n<ul>\n  <li>Plain, <a href=\"http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\">AlexNet</a>-like \nconvolutional networks work quite well for fixed length audio recordings</li>\n  <li>Vocal tract length perturbation works well as an augmentation technique</li>\n  <li>Caffe supports sharing weights between convolutional networks having different input sizes</li>\n  <li>Single layer neural network sometimes performs better than XGBoost for ensembling (although \nI had just one day to test the both)</li>\n</ul>\n\n<h2 id=\"unexplored-options\">Unexplored options</h2>\n\n<ul>\n  <li>It is interesting to see if a network with 50% dropout layers will improve the accuracy</li>\n  <li>Maybe larger convolutional networks, like <em>OxfordNet</em> will perform better. \nThey require much more memory, and it was risky to play with them under a tough deadline</li>\n  <li><a href=\"http://www.cs.toronto.edu/~asamir/papers/icassp12_cnn.pdf\">Hybrid methods</a>\ncombining CNN and Hidden Markov Models should work better</li>\n  <li>We believe it is possible to squeeze more from these models with better ensembling methods</li>\n  <li><a href=\"https://apps.topcoder.com/forums/?module=Thread&amp;threadID=866734&amp;start=0&amp;mc=4\">Other contestants report</a>\nbetter results based on careful mixing of the results of more traditional techniques, \nincluding <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-gram</a>\nand <a href=\"https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model\">Gaussian Mixture Models</a>.\nWe believe the combination of these techniques with the deep models will provide very \ngood results on this dataset</li>\n</ul>\n\n<p>One important issue is that the organizers of this contest <a href=\"http://apps.topcoder.com/forums//?module=Thread&amp;threadID=866217&amp;start=0&amp;mc=3\">do not allow</a>\nto use the dataset outside the contest. We hope this decision will be changed eventually.</p>"
}