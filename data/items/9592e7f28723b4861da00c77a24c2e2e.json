{
  "title": "Neuroevolution of Self-Interpretable Agents",
  "link": "https://blog.otoro.net/2020/3/18/attention/",
  "guid": "https://blog.otoro.net/2020/3/18/attention/",
  "description": "<center>\n<!--<img src=\"/assets/20171109/biped/bipedcover.gif\" width=\"100%\"/><br/>-->\n<!--<img src=\"/assets/20171109/minitaur/duck_normal_small.gif\" width=\"100%\"/><br/>-->\n<video class=\"b-lazy\" autoplay=\"\" muted=\"\" playsinline=\"\" loop=\"\" style=\"display: block; margin: auto; width: 100%;\"><source src=\"/assets/20200318/carracing_doom_stages.mp4\" type=\"video/mp4\" /></video><br />\n<!--<img src=\"/assets/20171109/biped/biped_cma.gif\" width=\"100%\"/><br/>-->\n<!--<i>Evolved Biped Walker.</i><br/>-->\n<p></p>\n<i>Agents with a self-attention “bottleneck” not only can solve these tasks from pixel inputs with only 4000 parameters, but they are also better at generalization.</i><br />\n<!--<code>\n<a href=\"https://github.com/worldmodels/\">GitHub</a>\n</code>-->\n</center>\n<p><br /></p>\n\n<p>Redirecting to <a href=\"https://attentionagent.github.io/\">attentionagent.github.io</a>, where the article resides.</p>\n\n<script>\nconsole.log('redirect.');\nwindow.location.href = \"https://attentionagent.github.io/\";\n</script>",
  "pubDate": "Wed, 18 Mar 2020 00:00:00 -0500"
}