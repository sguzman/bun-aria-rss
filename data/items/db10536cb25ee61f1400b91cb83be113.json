{
  "title": "Dask Development Log",
  "link": "",
  "updated": "2016-12-24T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2016/12/24/dask-dev-4",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nthe <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nand the Data Driven Discovery Initiative from the <a href=\"https://www.moore.org/\">Moore\nFoundation</a></em></p>\n\n<p>To increase transparency I’m blogging weekly about the work done on Dask and\nrelated projects during the previous week.  This log covers work done between\n2016-12-11 and 2016-12-18.  Nothing here is ready for production.  This\nblogpost is written in haste, so refined polish should not be expected.</p>\n\n<p>Themes of last week:</p>\n\n<ol>\n  <li>Cleanup of load balancing</li>\n  <li>Found cause of worker lag</li>\n  <li>Initial Spark/Dask Dataframe comparisons</li>\n  <li>Benchmarks with asv</li>\n</ol>\n\n<h3 id=\"load-balancing-cleanup\">Load Balancing Cleanup</h3>\n\n<p>The last two weeks saw several disruptive changes to the scheduler and workers.\nThis resulted in an overall performance degradation on messy workloads when\ncompared to the most recent release, which stopped bleeding-edge users from\nusing recent dev builds.  This has been resolved, and bleeding-edge git-master\nis back up to the old speed and then some.</p>\n\n<p>As a visual aid, this is what bad (or in this case random) load balancing looks\nlike:</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/bad-work-stealing.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/bad-work-stealing.png\" alt=\"bad work stealing\" width=\"70%\" /></a></p>\n\n<h3 id=\"identified-and-removed-worker-lag\">Identified and removed worker lag</h3>\n\n<p>For a while there have been significant gaps of 100ms or more between successive\ntasks in workers, especially when using Pandas.  This was particularly odd\nbecause the workers had lots of backed up work to keep them busy (thanks to the\nnice load balancing from before).  The culprit here was the calculation of the\nsize of the intermediate on object dtype dataframes.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/task-stream-pandas-lag.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/task-stream-pandas-lag.png\" alt=\"lag between tasks\" width=\"70%\" /></a></p>\n\n<p>Explaining this in greater depth, recall that to schedule intelligently, the\nworkers calculate the size in bytes of every intermediate result they produce.\nOften this is quite fast, for example for numpy arrays we can just multiply the\nnumber of elements by the dtype itemsize.  However for object dtype arrays or\ndataframes (which are commonly used for text) it can take a long while to\ncalculate an accurate result here.  Now we no longer calculuate an accurate\nresult, but instead take a fairly pessimistic guess.  The gaps between tasks\nshrink considerably.</p>\n\n<p><a href=\"https://mrocklin.github.io/blog/images/task-stream-pandas-no-lag.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/task-stream-pandas-no-lag.png\" alt=\"no lag between tasks\" width=\"40%\" /></a>\n<a href=\"https://mrocklin.github.io/blog/images/task-stream-pandas-no-lag-zoomed.png\">\n    <img src=\"https://mrocklin.github.io/blog/images/task-stream-pandas-no-lag-zoomed.png\" alt=\"no lag between tasks zoomed\" width=\"40%\" /></a></p>\n\n<p>Although there is still a significant bit of lag around 10ms long between tasks\non these workloads (see zoomed version on the right).  On other workloads we’re\nable to get inter-task lag down to the tens of microseconds scale.  While 10ms\nmay not sound like a long time, when we perform very many very short tasks this\ncan quickly become a bottleneck.</p>\n\n<p>Anyway, this change reduced shuffle overhead by a factor of two.  Things are\nstarting to look pretty snappy for many-small-task workloads.</p>\n\n<h3 id=\"initial-sparkdask-dataframe-comparisons\">Initial Spark/Dask Dataframe Comparisons</h3>\n\n<p>I would like to run a small benchmark comparing Dask and Spark DataFrames.  I\nspent a bit of the last couple of days using Spark locally on the NYC Taxi data\nand futzing with cluster deployment tools to set up Spark clusters on EC2 for\nbasic benchmarking.  I ran across\n<a href=\"https://github.com/nchammas/flintrock\">flintrock</a>, which has been highly\nrecommended to me a few times.</p>\n\n<p>I’ve been thinking about how to do benchmarks in an unbiased way.  Comparative\nbenchmarks are useful to have around to motivate projects to grow and learn\nfrom each other.  However in today’s climate where open source software\ndevelopers have a vested interest, benchmarks often focus on a projects’\nstrengths and hide their deficiencies.  Even with the best of intentions and\npractices, a developer is likely to correct for deficiencies on the fly.\nThey’re much more able to do this for their own project than for others’.\nBenchmarks end up looking more like sales documents than trustworthy research.</p>\n\n<p>My tentative plan is to reach out to a few Spark devs and see if we can\ncollaborate on a problem set and hardware before running computations and\ncomparing results.</p>\n\n<h3 id=\"benchmarks-with-airspeed-velocity\">Benchmarks with airspeed velocity</h3>\n\n<p><a href=\"https://github.com/postelrich\">Rich Postelnik</a> is building on work from\n<a href=\"https://github.com/TomAugspurger\">Tom Augspurger</a> to build out benchmarks for\nDask using <a href=\"https://github.com/spacetelescope/asv\">airspeed velocity</a> at\n<a href=\"https://github.com/dask/dask-benchmarks\">dask-benchmarks</a>.  Building out\nbenchmarks is a great way to get involved if anyone is interested.</p>\n\n<h3 id=\"pre-pre-release\">Pre-pre-release</h3>\n\n<p>I intend to publish a pre-release for a 0.X.0 version bump of dask/dask and\ndask/distributed sometime next week.</p>"
}