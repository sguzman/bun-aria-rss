{
  "title": "Dask is one year old",
  "link": "",
  "updated": "2015-12-21T00:00:00+00:00",
  "id": "https://mrocklin.github.io/blog/work/2015/12/21/dask-year",
  "content": "<p><em>This work is supported by <a href=\"http://continuum.io\">Continuum Analytics</a>\nand the <a href=\"http://www.darpa.mil/program/XDATA\">XDATA Program</a>\nas part of the <a href=\"http://blaze.pydata.org\">Blaze Project</a></em></p>\n\n<p><strong>tl;dr: Dask turned one yesterday.  We discuss success and failures.</strong></p>\n\n<p>Dask began one year ago yesterday with the <a href=\"https://github.com/blaze/dask/commit/05488db498c1561d266c7b676b8a89021c03a9e7\">following\ncommit</a>\n(with slight edits here for clarity’s sake).</p>\n\n<figure class=\"highlight\">\n  <pre><code class=\"language-python\" data-lang=\"python\"><span class=\"k\">def</span> <span class=\"nf\">istask</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"n\">x</span> <span class=\"ow\">and</span> <span class=\"nb\">callable</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">):</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">d</span><span class=\"p\">[</span><span class=\"n\">key</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"n\">istask</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">):</span>\n        <span class=\"n\">func</span><span class=\"p\">,</span> <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]</span>\n        <span class=\"k\">return</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">[</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">arg</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">arg</span> <span class=\"ow\">in</span> <span class=\"n\">args</span><span class=\"p\">])</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">v</span>\n\n <span class=\"p\">...</span> <span class=\"p\">(</span><span class=\"ow\">and</span> <span class=\"n\">around</span> <span class=\"mi\">50</span> <span class=\"n\">lines</span> <span class=\"n\">of</span> <span class=\"n\">tests</span><span class=\"p\">)</span></code></pre>\n</figure>\n\n<p><em>this is a very inefficient scheduler</em></p>\n\n<p>Since then dask has matured, expanded to new domains, gathered <a href=\"https://raw.githubusercontent.com/blaze/dask/master/AUTHORS.md\">excellent\ndevelopers</a>,\nand spawned other open source projects.  I thought it’d be a good time to look\nback on what worked, what didn’t, and what we should work on in the future.</p>\n\n<h2 id=\"collections\">Collections</h2>\n\n<p>Most users experience dask through the high-level collections of\n<code class=\"language-plaintext highlighter-rouge\">dask.array/bag/dataframe/imperative</code>.  Each of these evolve as projects of\ntheir own with different user groups and different levels of maturity.</p>\n\n<h3 id=\"daskarray\">dask.array</h3>\n\n<p>The parallel larger-than-memory array module dask.array has seen the most\nsuccess of the dask components.  It is the oldest, most mature, and most\nsophisticated subproject.  Much of dask.array’s use comes from downstream\nprojects, notably <a href=\"http://xray.readthedocs.org/en/stable/\">xray</a> which seems to\nhave taken off in climate science.  Dask.array also sees a fair amount of use\nin imaging, genomics, and numerical algorithms research.</p>\n\n<p>People that I don’t know now use dask.array to do scientific research.  From my\nperspective that’s mission accomplished.</p>\n\n<p>There are still tweaks to make to algorithms, particularly as we scale out to\ndistributed systems (see far below).</p>\n\n<h3 id=\"daskbag\">dask.bag</h3>\n\n<p>Dask.bag started out as a weekend project and didn’t evolve much beyond that.\nFortunately there wasn’t much to do and this submodule probably has the highest\nvalue/effort ratio .</p>\n\n<p>Bag doesn’t get as much attention as its older sibling array though.  It’s\nhandy but not as well used and so not as robust.</p>\n\n<h3 id=\"daskdataframe\">dask.dataframe</h3>\n\n<p>Dataframe is an interesting case, it’s both pretty sophisticated, pretty\nmature, and yet also probably generates the most user frustration.</p>\n\n<p>Dask.dataframe gains a lot of value by leveraging Pandas both under the hood\n(one dask DataFrame is many pandas DataFrames) and by copying its API (Pandas\nusers can use dask.dataframe without learning a new API.)  However, because\ndask.dataframe only implements a core subset of Pandas, users end up tripping\nup on the missing functionality.</p>\n\n<p>This can be decomposed into to issues:</p>\n\n<ol>\n  <li>It’s not clear that there exists a core subset of Pandas that would handle most\nuse cases.  Users touch many diffuse parts of Pandas in a single workflow.\nWhat one user considers core another user considers fringe.  It’s not clear\nhow to agree on a sufficient subset to implement.</li>\n  <li>Once you implement this subset (and we’ve done our best) it’s hard to\nconvey expectations to the user about what is and is not available.</li>\n</ol>\n\n<p>That being said, dask.dataframe is pretty solid.  It’s very fast, expressive,\nand handles common use cases well.  It probably generates the most\nStackOverflow questions.  This signals both confusion and active use.</p>\n\n<p>Special thanks here go out to Jeff Reback, for making Pandas release the GIL\nand to Masaaki Horikoshi (@sinhrks) for greatly improving the maturity of\ndask.dataframe.</p>\n\n<h3 id=\"daskimperative\">dask.imperative</h3>\n\n<p>Also known as <code class=\"language-plaintext highlighter-rouge\">dask.do</code> this little backend remains one of the most powerful\nand one of the least used (outside of myself.)  We should rethink the API here\nand improve learning materials.</p>\n\n<h3 id=\"general-thoughts-on-collections\">General thoughts on collections</h3>\n\n<p><em>Warning: this section is pretty subjective</em></p>\n\n<p>Big data collections are cool but perhaps less useful than people expect.\nParallel applications are often more complex than can be easily described by a\nbig array or a big dataframe.  Many real-world parallel computations end up\nbeing more particular in their parallelism needs.  That’s not to say that the\narray and dataframe abstractions aren’t central to parallel computing, it’s\njust that we should not restrict ourselves to them.  The world is more complex.</p>\n\n<p>However, it’s reasonable to break this “world is complex” rule within\nparticular domains.  NDArrays seem to work well in climate science.\nSpecialized large dataframes like Dato’s SFrame seem to be effective for a\nparticular class of machine learning algorithms.  The SQL table is inarguably\nan effective abstraction in business intelligence.  Large collections are\nuseful in specific contexts, but they are perhaps the focus of too much\nattention.  The big dataframe in particular is over-hyped.</p>\n\n<p>Most of the really novel and impressive work I’ve seen with dask has been done\neither with custom graphs or with the dask.imperative API.  I think we should\nconsider APIs that enable users to more easily express custom algorithms.</p>\n\n<h2 id=\"avoid-parallelism\">Avoid Parallelism</h2>\n\n<p>When giving talks on parallelism I’ve started to give a brief “avoid\nparallelism” section.  From the problems I see on stack overflow and from\ngeneral interactions when people run into performance challenges their first\nsolution seems to be to parallelize.  This is sub-optimal.  It’s often far\ncheaper to improve storage formats, use better algorithms, or use C/Numba\naccelerated code than it is to parallelize.  Unfortunately storage formats\nand C aren’t as sexy as big data parallelism, so they’re not in the forefront\nof people’s minds.  We should change this.</p>\n\n<p>I’ll proudly buy a beer for anyone that helps to make storage formats a sexier\ntopic.</p>\n\n<h2 id=\"scheduling\">Scheduling</h2>\n\n<h3 id=\"single-machine\">Single Machine</h3>\n\n<p>The single machine dynamic task scheduler is very very solid.  It has roughly\ntwo objectives:</p>\n\n<ol>\n  <li>Use all the cores of a machine</li>\n  <li>Choose tasks that allow the release of intermediate results</li>\n</ol>\n\n<p>This is what allows us to quickly execute complex workflows in small space.\nThis scheduler underlies all execution within dask.  I’m very happy with it.  I\nwould like to find ways to expose it more broadly to other libraries.\nSuggestions are very welcome here.</p>\n\n<p>We still run into cases where it doesn’t perform optimally\n(see <a href=\"https://github.com/blaze/dask/issues/874\">issue 874</a>),\nbut so far we’ve always been able to enhance the scheduler whenever these cases\narise.</p>\n\n<h3 id=\"distributed-cluster\">Distributed Cluster</h3>\n\n<p>Over the last few months we’ve been working on another scheduler for\ndistributed memory computation.  It should be a nice extension to the existing\ndask collections out to “big data” systems.  It’s experimental but usable now\nwith documentation at the follow links:</p>\n\n<ul>\n  <li>http://distributed.readthedocs.org/en/latest/</li>\n  <li>https://github.com/blaze/distributed</li>\n  <li>http://matthewrocklin.com/distributed-by-example/</li>\n</ul>\n\n<p>Feedback is welcome.  I recommend waiting for a month or two if you prefer\nclean and reliable software.  It will undergo a name-change to something less\ngeneric.</p>"
}