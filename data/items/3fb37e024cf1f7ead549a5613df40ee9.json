{
  "title": "From Gaussian Algebra to Gaussian Processes, Part 2",
  "link": "",
  "published": "2018-06-12T08:00:00-04:00",
  "updated": "2018-06-12T08:00:00-04:00",
  "author": {
    "name": "Will Wolf"
  },
  "id": "tag:willwolf.io,2018-06-12:/2018/06/12/gaussian-algebra-to-gaussian-processes-part-2/",
  "summary": "<p>Introducing the RBF kernel, and motivating its ubiquitous use in Gaussian processes.</p>",
  "content": "<p>In the previous post, we covered the following topics:</p>\n<ul>\n<li>A Gaussian process (GP) defines a distribution over functions (i.e. function evaluations)</li>\n<li>Marginalizing a Gaussian over a subset of its elements gives another Gaussian (just pluck out the pieces of interest)</li>\n<li>Conditioning a subset of the elements of a Gaussian on another subset gives another Gaussian (a simple algebraic formula)</li>\n<li>Posterior over functions (the linear map of the posterior over weights onto some matrix <span class=\"math\">\\(A = \\phi(X_{*})^T\\)</span>)</li>\n<li>Covariances (the second thing we need in order to specify a multivariate Gaussian)</li>\n</ul>\n<p><strong>If any of the above is still not clear, please look no further, and re-visit the <a href=\"https://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/\">previous post</a>.</strong></p>\n<p>Conversely, we did not directly cover:</p>\n<ul>\n<li>Kernels</li>\n<li>Squared-exponentials</li>\n</ul>\n<p>Here, we'll explain these two.</p>\n<h2>The more features we use, the more expressive our model</h2>\n<p>We concluded the previous post by plotting posteriors over function evaluations given various <code>phi_func</code>s, i.e. a function that creates \"features\" <span class=\"math\">\\(\\phi(X)\\)</span> given an input <span class=\"math\">\\(X\\)</span>.</p>\n<p>For example:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>  <span class=\"c1\"># 5 inputs</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">])</span>  <span class=\"c1\"># 5 corresponding outputs, which we'll use later on</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">phi_func</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">cos</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">abs</span><span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"mi\">3</span><span class=\"p\">))])</span>  <span class=\"c1\"># makes D=2 features for each input</span>\n\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">phi_func</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n<span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre></div>\n<p>One common such set of features are those given by \"radial basis functions\", a.k.a. the \"squared exponential\" function, defined as:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">phi_func</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">D</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-.</span><span class=\"mi\">5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">d</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">d</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">D</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">D</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">))])</span>\n\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">phi_func</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n<span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n</pre></div>\n<p>Again, the choice of which features to use is ultimately arbitrary, i.e. a choice left to the modeler.</p>\n<p>Throughout the exercise, we saw that the larger the dimensionality <span class=\"math\">\\(d\\)</span> of our feature function <code>phi_func</code>, the more expressive, i.e. less endemically prone to overfitting, our model became.</p>\n<p><strong>So, how far can we take this?</strong></p>\n<h2>Computing features is expensive</h2>\n<p>Ideally, we'd compute as many features as possible for each input element, i.e. employ <code>phi_func(x, D=some_huge_number)</code>. Unfortunately, the cost of doing so adds up, and ultimately becomes intractable past meaningfully-large values of <span class=\"math\">\\(d\\)</span>.</p>\n<p><strong>Perhaps there's a better way?</strong></p>\n<h2>How are these things used?</h2>\n<p>Let's bring back our GP equations, and prepare ourselves to <em>squint</em>! In the previous post, we outlined the following modeling process:</p>\n<ol>\n<li>Define prior distribution over weights and function evaluations, <span class=\"math\">\\(P(w, y)\\)</span></li>\n<li>Marginalizing <span class=\"math\">\\(P(w, y)\\)</span> over <span class=\"math\">\\(y\\)</span>, i.e. <span class=\"math\">\\(\\int P(w, y)dy\\)</span>, and given some observed function evaluations <span class=\"math\">\\(y\\)</span>, compute the posterior distribution over weights, <span class=\"math\">\\(P(w\\vert y)\\)</span></li>\n<li>After linear-mapping <span class=\"math\">\\(P(w\\vert y)\\)</span> onto some new, transformed test input <span class=\"math\">\\(\\phi(X_*)^T\\)</span>, compute the posterior distribution over function evaluations, <span class=\"math\">\\(P(y_*\\ \\vert\\ y) = P(\\phi(X_{*})^Tw\\ \\vert\\ y)\\)</span></li>\n</ol>\n<p>Now, let's unpack #2 and #3.</p>\n<h3><span class=\"math\">\\(P(w\\vert y)\\)</span></h3>\n<ul>\n<li>First, the mathematical equation:</li>\n</ul>\n<div class=\"math\">$$\n\\begin{align*}\nP(w\\vert y)\n    &amp;= \\mathcal{N}(\\mu_w + \\Sigma_{wy}\\Sigma_y^{-1}(y - \\mu_y), \\Sigma_w - \\Sigma_{wy}\\Sigma_y^{-1}\\Sigma_{wy}^T)\\\\\n    \\\\\n    &amp;= \\mathcal{N}(\\mu_w + \\Sigma_{wy}(\\phi(X)^T\\Sigma_w \\phi(X))^{-1}(y - \\mu_w^T \\phi(X)), \\Sigma_w - \\Sigma_{wy}(\\phi(X)^T\\Sigma_w \\phi(X))^{-1}\\Sigma_{wy}^T)\n\\end{align*}\n$$</div>\n<ul>\n<li>Next, this equation in code:</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># Define initial parameters</span>\n<span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"o\">...</span>  <span class=\"c1\"># dimensionality of `phi_func`</span>\n\n<span class=\"n\">mu_w</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">)</span>  <span class=\"c1\"># often a vector of zeros, though it doesn't have to be</span>\n<span class=\"n\">cov_w</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">)</span>  <span class=\"c1\"># often the identity matrix, though it doesn't have to be</span>\n\n<span class=\"c1\"># Featurize `X_train`</span>\n<span class=\"n\">phi_x</span> <span class=\"o\">=</span> <span class=\"n\">phi_func</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">D</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Params of prior distribution over function evals</span>\n<span class=\"n\">mu_y</span> <span class=\"o\">=</span> <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">mu_w</span>\n     <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">)</span>\n<span class=\"n\">cov_y</span> <span class=\"o\">=</span> <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span>\n\n<span class=\"c1\"># Params of posterior distribution over weights</span>\n<span class=\"n\">mu_w_post</span> <span class=\"o\">=</span> <span class=\"n\">mu_w</span> <span class=\"o\">+</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">cov_y</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"p\">(</span><span class=\"n\">y_train</span> <span class=\"o\">-</span> <span class=\"n\">mu_y</span><span class=\"p\">)</span>\n          <span class=\"o\">=</span> <span class=\"n\">mu_w</span> <span class=\"o\">+</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">cov_y</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>\n<span class=\"n\">cov_w_post</span> <span class=\"o\">=</span> <span class=\"n\">cov_w</span> <span class=\"o\">-</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">cov_y</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span>\n           <span class=\"o\">=</span> <span class=\"n\">cov_w</span> <span class=\"o\">-</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span>\n</pre></div>\n<h3><span class=\"math\">\\(P(y_*\\ \\vert\\ y) = P(\\phi(X_{*})^Tw\\ \\vert\\ y)\\)</span></h3>\n<p>Here, <span class=\"math\">\\(X_*\\)</span> is a set of test points, e.g. <code>np.linspace(-10, 10, 200)</code>.</p>\n<p>In addition, let's call <span class=\"math\">\\(X_* \\rightarrow\\)</span> <code>X_test</code> and <span class=\"math\">\\(y_* \\rightarrow\\)</span> <code>y_test</code>.</p>\n<ul>\n<li>The mathematical equations in code:</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># Featurize `X_test`</span>\n<span class=\"n\">phi_x_test</span> <span class=\"o\">=</span> <span class=\"n\">phi_func</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"o\">=</span><span class=\"n\">D</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># The following two equations were defined above</span>\n<span class=\"n\">mu_w_post</span> <span class=\"o\">=</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>\n<span class=\"n\">cov_w_post</span> <span class=\"o\">=</span> <span class=\"n\">cov_w</span> <span class=\"o\">-</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span>\n\n<span class=\"c1\"># The mean of the posterior distribution over function evaluations</span>\n<span class=\"n\">mu_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">mu_w_post</span>\n               <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>\n\n<span class=\"c1\"># The covariance of the posterior distribution over function evaluations</span>\n<span class=\"n\">cov_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w_post</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n                <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span> <span class=\"o\">-</span> \\\n                  <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> \\\n                  <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n</pre></div>\n<h2>Never alone</h2>\n<p>Squinting at the equations for <code>mu_y_test_post</code> and <code>cov_y_test_post</code>, we see that <code>phi_x</code> and <code>phi_x_test</code> appear <strong>only in the presence of another <code>phi_x</code>, or <code>phi_x_test</code>.</strong></p>\n<p>These four distinct such terms are:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n<span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span>\n<span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span>\n<span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n</pre></div>\n<p>In mathematical notation, they are (respectively):</p>\n<ul>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X_*)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X_*)\\)</span></li>\n</ul>\n<h2>Simplifying further</h2>\n<p>These are nothing more than <em>scaled</em> (via the <span class=\"math\">\\(\\Sigma_w\\)</span> term) dot products in some expanded feature space <span class=\"math\">\\(\\phi\\)</span>.</p>\n<p><em>Until now, we've explicitly chosen what this <span class=\"math\">\\(\\phi\\)</span> function is.</em></p>\n<p>If the scaling matrix <span class=\"math\">\\(\\Sigma_w\\)</span> is <a href=\"https://en.wikipedia.org/wiki/Positive-definite_matrix\">positive definite</a>, we can state the following, using <span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X)\\)</span>, i.e. <code>phi_x.T @ cov_w @ phi_x</code>, as an example:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\Sigma_w = (\\sqrt{\\Sigma_w})^2\n\\end{align*}\n$$</div>\n<div class=\"math\">$$\n\\begin{align*}\n\\phi(X)^T \\Sigma_w \\phi(X)\n    &amp;= \\big(\\sqrt{\\Sigma_w}\\phi(X)\\big)^T\\big(\\sqrt{\\Sigma_w}\\phi(X)\\big)\\\\\n    &amp;= \\varphi(X)^T\\varphi(X)\\\\\n    &amp;= \\varphi(X) \\cdot \\varphi(X)\\\\\n\\end{align*}\n$$</div>\n<p>As such, our four distinct scaled-dot-product terms can be rewritten as:</p>\n<ul>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X_*) = \\varphi(X_*) \\cdot \\varphi(X_*)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X) = \\varphi(X_*) \\cdot \\varphi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X) = \\varphi(X) \\cdot \\varphi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X_*) = \\varphi(X) \\cdot \\varphi(X_*)\\)</span></li>\n</ul>\n<p><strong>In other words, these terms can be equivalently written as dot-products in some space <span class=\"math\">\\(\\varphi\\)</span>.</strong></p>\n<p><em>NB: we have <strong>not</strong> explicitly chosen what this <span class=\"math\">\\(\\varphi\\)</span> function is.</em></p>\n<h2>Kernels</h2>\n<p>A \"kernel\" is a function which gives the similarity between individual elements in two sets, i.e. a Gram matrix.</p>\n<p>For instance, imagine we have two sets of countries, <span class=\"math\">\\(\\{\\text{France}, \\text{Germany}, \\text{Iceland}\\}\\)</span> and <span class=\"math\">\\(\\{\\text{Morocco}, \\text{Denmark}\\}\\)</span>, and that similarity is given by an integer value in <span class=\"math\">\\([1, 5]\\)</span>, where 1 is the least similar, and 5 is the most. Applying a kernel to these sets might give a Gram matrix such as:</p>\n<div>\n<style scoped=\"\">\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe table table-striped table-hover\">\n<thead>\n<tr style=\"text-align: right;\">\n<th></th>\n<th>France</th>\n<th>Germany</th>\n<th>Iceland</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<th>Morocco</th>\n<td>4</td>\n<td>2</td>\n<td>1</td>\n</tr>\n<tr>\n<th>Denmark</th>\n<td>3</td>\n<td>3</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>When you hear the term \"kernel\" in the context of machine learning, think \"similarity between things in lists.\"</strong></p>\n<p>NB: A \"list\" could be a list of vectors, i.e. a matrix. A vector, or a matrix, are the canonical inputs to a kernel.</p>\n<h2>Mercer's Theorem</h2>\n<p>Mercer's Theorem has as a key result that any kernel function can be expressed as a dot product, i.e.</p>\n<div class=\"math\">$$\nK(X, X') = \\varphi(X) \\cdot \\varphi (X')\n$$</div>\n<p>where <span class=\"math\">\\(\\varphi\\)</span> is some function that creates <span class=\"math\">\\(d\\)</span> features out of <span class=\"math\">\\(X\\)</span> (in the same vein as <code>phi_func</code> from above).<sup id=\"fnref:2\"><a class=\"footnote-ref\" href=\"#fn:2\">2</a></sup></p>\n<h3>Example</h3>\n<p>To illustrate, I'll borrow an example from <a href=\"https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is\">CrossValidated</a>:</p>\n<p>\"For example, consider a simple polynomial kernel <span class=\"math\">\\(K(\\mathbf x, \\mathbf y) = (1 + \\mathbf x^T \\mathbf y)^2\\)</span> with <span class=\"math\">\\(\\mathbf x, \\mathbf y \\in \\mathbb R^2\\)</span>. This doesn't seem to correspond to any mapping function <span class=\"math\">\\(\\varphi\\)</span>, it's just a function that returns a real number. Assuming that <span class=\"math\">\\(\\mathbf x = (x_1, x_2)\\)</span> and <span class=\"math\">\\(\\mathbf y = (y_1, y_2)\\)</span>, let's expand this expression:</p>\n<div class=\"math\">$$\n\\begin{align}\nK(\\mathbf x, \\mathbf y)\n    &amp;= (1 + \\mathbf x^T \\mathbf y)^2\\\\\n    &amp;= (1 + x_1 \\, y_1  + x_2 \\, y_2)^2\\\\\n    &amp;= 1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 y_1 + 2 x_2 y_2 + 2 x_1 x_2 y_1 y_2\n\\end{align}\n$$</div>\n<p>Note that this is nothing else but a dot product between two vectors <span class=\"math\">\\((1, x_1^2, x_2^2, \\sqrt{2} x_1, \\sqrt{2} x_2, \\sqrt{2} x_1 x_2)\\)</span> and <span class=\"math\">\\((1, y_1^2, y_2^2, \\sqrt{2} y_1, \\sqrt{2} y_2, \\sqrt{2} y_1 y_2)\\)</span>, and <span class=\"math\">\\(\\varphi(\\mathbf x) = \\varphi(x_1, x_2) = (1, x_1^2, x_2^2, \\sqrt{2} x_1, \\sqrt{2} x_2, \\sqrt{2} x_1 x_2)\\)</span>. So the kernel <span class=\"math\">\\(K(\\mathbf x, \\mathbf y) = (1 + \\mathbf x^T \\mathbf y)^2 = \\varphi(\\mathbf x) \\cdot \\varphi(\\mathbf y)\\)</span> computes a dot product in 6-dimensional space without explicitly visiting this space.\"</p>\n<h3>What this means</h3>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/kernels-for-gaussian-processes.svg\"/></p>\n<ul>\n<li>We start with inputs <span class=\"math\">\\(X\\)</span> and <span class=\"math\">\\(Y\\)</span>.</li>\n<li>Our goal is to compute the similarity between then, <span class=\"math\">\\(\\text{Sim}(X, Y)\\)</span>.</li>\n</ul>\n<h4>Bottom path</h4>\n<ul>\n<li>Lifting these inputs into some feature space, then computing their dot-product in that space, i.e. <span class=\"math\">\\(\\varphi(X) \\cdot \\varphi (Y)\\)</span> (where <span class=\"math\">\\(F = \\varphi\\)</span>, since I couldn't figure out how to draw a <span class=\"math\">\\(\\varphi\\)</span> in <a href=\"http://draw.io\">draw.io</a>), is one strategy for computing this similarity.</li>\n<li>Unfortunately, this robustness comes at a cost: <strong>the computation is extremely expensive.</strong></li>\n</ul>\n<h4>Top Path</h4>\n<ul>\n<li>A valid kernel computes similarity between inputs. The function it employs might be extremely simple, e.g. <span class=\"math\">\\((X - Y)^{2}\\)</span>; <strong>the computation is extremely cheap.</strong></li>\n</ul>\n<h4>Mercer!</h4>\n<ul>\n<li>Mercer's Theorem tells us that every valid kernel, i.e. the top path, is <em>implicitly traversing the bottom path.</em> <strong>In other words, kernels allow us to directly compute the result of an extremely expensive computation, extremely cheaply.</strong></li>\n</ul>\n<h2>How does this help?</h2>\n<p>Once more, the Gaussian process equations are littered with the following terms:</p>\n<ul>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X_*) = \\varphi(X_*) \\cdot \\varphi(X_*)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X_*)^T\\Sigma_w \\phi(X) = \\varphi(X_*) \\cdot \\varphi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X) = \\varphi(X) \\cdot \\varphi(X)\\)</span></li>\n<li><span class=\"math\">\\(\\phi(X)^T\\Sigma_w \\phi(X_*) = \\varphi(X) \\cdot \\varphi(X_*)\\)</span></li>\n</ul>\n<p>In addition, we previously established that the more we increase the dimensionality <span class=\"math\">\\(d\\)</span> of our given feature function, the more flexible our model becomes.</p>\n<p>Finally, past any meaningfully large value of <span class=\"math\">\\(d\\)</span>, and irrespective of what <span class=\"math\">\\(\\varphi\\)</span> actually is, <strong>this computation becomes intractably expensive.</strong></p>\n<h3>Kernels!</h3>\n<p>You know where this is going.</p>\n<p>Given Mercer's theorem, we can state the following equalities:</p>\n<ul>\n<li><span class=\"math\">\\(\\varphi(X_*) \\cdot \\varphi(X_*) = K(X_*, X_*)\\)</span></li>\n<li><span class=\"math\">\\(\\varphi(X_*) \\cdot \\varphi(X) = K(X_*, X)\\)</span></li>\n<li><span class=\"math\">\\(\\varphi(X) \\cdot \\varphi(X) = K(X, X)\\)</span></li>\n<li><span class=\"math\">\\(\\varphi(X) \\cdot \\varphi(X_*) = K(X, X_*)\\)</span></li>\n</ul>\n<h2>Which kernels to choose?</h2>\n<p>At the outset, we stated that our primary goal was to increase <span class=\"math\">\\(d\\)</span>. As such, <strong>let's pick the kernel whose implicit <span class=\"math\">\\(\\varphi\\)</span> has the largest dimensionality possible.</strong></p>\n<p>In the example above, we saw that the kernel <span class=\"math\">\\(k(\\mathbf x, \\mathbf y)\\)</span> was implicitly computing a <span class=\"math\">\\(d=6\\)</span>-dimensional dot-product. Which kernels compute a <span class=\"math\">\\(d=100\\)</span>-dimensional dot-product? <span class=\"math\">\\(d=1000\\)</span>?</p>\n<p><strong>How about <span class=\"math\">\\(d=\\infty\\)</span>?</strong></p>\n<h2>Radial basis function, a.k.a. the \"squared-exponential\"</h2>\n<p>This kernel is implicitly computing a <span class=\"math\">\\(d=\\infty\\)</span>-dimensional dot-product. That's it. <strong>That's why it's so ubiquitous in Gaussian processes.</strong></p>\n<h2>Rewriting our equations</h2>\n<p>With all of the above in mind, let's rewrite the equations for the parameters of our posterior distribution over function evaluations.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># The mean of the posterior distribution over function evaluations</span>\n<span class=\"n\">mu_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">mu_w_post</span>\n               <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>\n\n               <span class=\"c1\"># Now, substituting in our kernels</span>\n               <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">))</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>\n\n<span class=\"c1\"># The covariance of the posterior distribution over function evaluations</span>\n<span class=\"n\">cov_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w_post</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n                <span class=\"o\">=</span> <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span> <span class=\"o\">-</span> \\\n                  <span class=\"n\">phi_x_test</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x</span><span class=\"p\">)</span> <span class=\"o\">@</span> \\\n                  <span class=\"n\">phi_x</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">cov_w</span> <span class=\"o\">@</span> <span class=\"n\">phi_x_test</span>\n\n                <span class=\"c1\"># Now, substituting in our kernels</span>\n                <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"o\">-</span> \\\n                  <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">))</span> <span class=\"o\">@</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">)</span>\n</pre></div>\n<h2>Defining the kernel in code</h2>\n<p>Mathematically, the RBF kernel is defined as follows:</p>\n<div class=\"math\">$$\nK(X, Y) = \\exp(-\\frac{1}{2}\\vert X - Y \\vert ^2)\n$$</div>\n<p>To conclude, let's define a Python function for the parameters of our posterior over function evaluations, using this RBF kernel as <code>k</code>, then plot the resulting distribution.</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>  <span class=\"c1\"># 5 inputs</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">])</span>  <span class=\"c1\"># 5 corresponding outputs, which we'll use below</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">200</span><span class=\"p\">)</span>  <span class=\"c1\"># vector of test inputs</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">rbf_kernel</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># shape: (len(x), 1)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>  <span class=\"c1\"># shape: (1, len(y))</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-.</span><span class=\"mi\">5</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># shape: (len(x), len(y))</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">k</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">rbf_kernel</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\"># The following quantity is used in both `mu_y_test_post` and `cov_y_test_post`;</span>\n<span class=\"c1\"># we extract it into a separate variable for readability</span>\n<span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">)</span> <span class=\"o\">@</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">inv</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"p\">))</span>\n\n<span class=\"n\">mu_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">A</span> <span class=\"o\">@</span> <span class=\"n\">y_train</span>            \n<span class=\"n\">cov_y_test_post</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">A</span> <span class=\"o\">@</span> <span class=\"n\">k</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">)</span>\n</pre></div>\n<h2>Visualizing results</h2>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_17_0.png\"/></p>\n<p>And for good measure, with some samples from the posterior:</p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/gaussian-algebra-to-gaussian-processes-part-2/output_19_0.png\"/></p>\n<h2>In summary</h2>\n<p>In this post, we've unpacked the notion of a kernel, and its ubiquitous use in Gaussian Processes.</p>\n<p>In addition, we've introduced the RBF kernel, i.e. \"squared exponential\" kernel, and motivated its widespread application in these models.</p>\n<h2>Code</h2>\n<p>The <a href=\"https://github.com/cavaunpeu/gaussian-processes\">repository</a> and <a href=\"https://nbviewer.jupyter.org/github/cavaunpeu/gaussian-processes/blob/master/gaussian-processes-part-2.ipynb\">rendered notebook</a> for this project can be found at their respective links.</p>\n<h2>References</h2>\n<div class=\"footnote\">\n<hr/>\n<ol>\n<li id=\"fn:1\">\n<p><a href=\"http://gaussianprocess.org/gpml/?\">Gaussian Processes for Machine Learning</a>. Carl Edward Rasmussen and Christopher K. I. Williams\nThe MIT Press, 2006. ISBN 0-262-18253-X. <a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">↩</a></p>\n</li>\n<li id=\"fn:2\">\n<p><a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-Mercers-Theorem\">What is an intuitive explanation of Mercer's Theorem?</a> <a class=\"footnote-backref\" href=\"#fnref:2\" title=\"Jump back to footnote 2 in the text\">↩</a></p>\n</li>\n</ol>\n</div>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}