{
  "title": "Adversarial Machines",
  "link": "https://medium.com/@samim/adversarial-machines-998d8362e996?source=rss-f3c8148878e1------2",
  "guid": "https://medium.com/p/998d8362e996",
  "category": [
    "machine-learning",
    "copyright",
    "artificial-intelligence"
  ],
  "dc:creator": "samim",
  "pubDate": "Mon, 07 Dec 2015 15:12:17 GMT",
  "atom:updated": "2015-12-17T20:23:16.416Z",
  "content:encoded": "<h4>Fooling A.Is (and turn everyone into a Manga)</h4><p><strong>Adversarial A.Is</strong> are a common sci-fi theme: Robot VS Robot. In recent years, real adversarial examples have emerged. This experiment explores how to <strong>generate images to fool A.Is <em>(and turn everyone into manga).</em></strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-degb_Avfq5TR0_0CJhn9A.png\" /></figure><h3>Convolutional Neural Networks</h3><p>At the heart of many modern computer vision systems are <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\"><strong>Convolutional Neural Networks</strong></a>. On <em>some</em> vision tasks, <strong>CNNs</strong> have surpassed human performance. Industries such as Web-Services, Research, Transport, Medical, Manufacturing, Defence and Intelligence rely on them every day.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1018/1*kVm3hjja6tUAh9tiSoSC0A.png\" /></figure><p><strong>Convolutional Nets </strong>are commonly used to classify images. The network is shown an image of a <em>pipe</em> and classifies it as “<em>pipe</em>”. Generalist networks are able to classify 1000+ classes of objects with amazing precision and speed.</p><h3><strong>Fooling Neural Networks</strong></h3><p>A series of published research papers has produced evidence that <strong>Convolutional Neural Networks can be fooled. </strong>Images can be manipulated, so that image recognition networks are likely to miss-classify them. These manipulations look like noise, almost invisible to humans.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Vp2hctXzXImYq6BHDpbr-g.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0R7EasNbUPLk9M-w4Rf5iQ.png\" /><figcaption>Image by Christian Szegedy (Google) et al. NOTE: “Noise” is used for imagination. “Imperceptible changes” more fitting.</figcaption></figure><p>This problem has stirred controversy in the Machine Learning community, with some hailing it as a “deep flaw” of deep neural networks and others promoting a more cautious interpretation. Researchers are actively exploring the reasons for adversarial examples. <strong>Ian Goodfellow</strong> gives a great overview in his recent talk:<strong> </strong><a href=\"https://www.youtube.com/watch?v=Pq4A2mPCB0Y\"><strong>‘Do Statistical Models Understand the World? (Video)’</strong></a></p><blockquote><strong>Research Papers: </strong><a href=\"http://arxiv.org/pdf/1412.1897v4.pdf\">Deep Neural Networks are Easily Fooled</a> (<a href=\"http://www.evolvingai.org/fooling\"><strong>code</strong></a><strong> / </strong><a href=\"https://www.youtube.com/watch?v=M2IebCN9Ht4\"><strong>video</strong></a>): <a href=\"http://arxiv.org/pdf/1510.05328v3.pdf\">Exploring the space of Adversarial Images</a> : <a href=\"http://arxiv.org/pdf/1511.05122v3.pdf\">Adversarial manipulation of deep representations </a>: <a href=\"http://arxiv.org/pdf/1412.6572v3.pdf\">Explaining and harnessing adversarial examples</a> : <a href=\"http://arxiv.org/pdf/1312.6199v4.pdf\">Intriguing properties of neural networks</a> : <a href=\"https://karpathy.github.io/2015/03/30/breaking-convnets/\">Breaking Linear Classifiers on ImageNet</a> : <a href=\"http://arxiv.org/abs/1511.04508\">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</a> : <a href=\"http://arxiv.org/abs/1511.07528\">The Limitations of Deep Learning in Adversarial Settings</a></blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*-9LAwwDPElBj3qs7VAHoYg.png\" /></figure><h3>Experiment: Generating Adversarial Images</h3><p>This experiment started with an exploration of the recently published paper <a href=\"http://arxiv.org/abs/1510.05328\"><strong>Exploring the space of adversarial images</strong></a> by <a href=\"https://github.com/tabacof\">Pedro Tabacof</a> & <a href=\"http://eduardovalle.com/\">Eduardo Valle</a> of University of Campinas in Brazil. The paper investigates adversarial examples and hints that <strong>most current</strong> <strong>CNN</strong> <strong>classifiers are vulnerable</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dN_GB6jQqOtSH-rMkKO0cw.png\" /><figcaption>Adversarial Noise examples. (Image by Pedro Tabacof, Eduardo Valle)</figcaption></figure><p>Alongside the paper, they released <a href=\"https://github.com/tabacof/adversarial\"><strong>open-source code</strong></a><strong> </strong>that enables anyone to generate adversarial images easily.</p><p><strong>The experiments aim was to find a way to demo this library. All explored scenarios were rejected, as outcomes are highly uncertain. Here is a sample of rejected ideas:</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*uMdKkq7qQSBZ9MMLJVp2dg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*oJGKkbvc7psJaPY3tUs7WQ.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EVbpB3jGnjYEj49Mz-YVuw.png\" /><figcaption>Selection of rejected ideas. WARNING: COMEDY INTENDED</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*9SOcfunKZmCoKvmNgSZ7JQ.gif\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*Zn5bz6vqE1GnaVqsMCo80Q.gif\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*1HB8iuvoqCdEgoLmvNkMSA.gif\" /><figcaption>Rejected Cartoon / Don Hertzfeldt</figcaption></figure><h3>Experiment: Generating Adversarial Mangas</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*_n3GueGSrWcPN7_RWCt54Q.gif\" /><figcaption>Generative Manga</figcaption></figure><p>Recently a new <strong>pre-trained CNN model</strong> was <a href=\"https://github.com/rezoo/illustration2vec\">released</a>: <a href=\"http://illustration2vec.net/papers/illustration2vec-main.pdf\"><strong>Illustration2Vec: </strong>A Semantic Vector Representation of Illustrations</a>.<strong> </strong>Masaki Saito(Tohoku University) and Yusuke Matsui (University of Tokyo) trained a model on large amounts of Manga Images.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3oMsHHTChV8BWaFd9VLj5w.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fMiI_vCbfcoX04t1RCFRxw.png\" /><figcaption>Selection of training data (<em>presumably copyrighted content</em>) / Tag Prediction</figcaption></figure><p>The <a href=\"http://illustration2vec.net/\">Illustration2Vec model</a> has the ability to <a href=\"http://demo.illustration2vec.net/\"><strong>predict copyright</strong></a> tags. One could say, <em>It has memories of copyrighted content</em>. A fascinating way to explore convolutional networks is <strong>deepdream</strong>. This experiment <em>dreams</em> with the Illustration2Vec model and <strong>turns everyone into a Manga.</strong></p><p><strong>Questions raised: </strong>Are the generated images copyright infringing? Can the copyright detection bots of large manga sites (or disney) be fooled easily?</p><h4>A selection of generated manga</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/863/1*2LHLqsDjly4JTo1j3Dj21w.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/648/1*2kJAJRWth5hUyTWjmWZgAw.jpeg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*N-a4Sz2Eo9-lKspWWS5Q9w.jpeg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*oVXpeGPWCO62uzoDKoVE8Q.jpeg\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qOHKP76r8w44_PUqVF3i5Q.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/955/1*UyHk2GWZgQALMITXlF7SQQ.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/660/1*R4-jVgk3-tNgEHBUs9lbSA.gif\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1002/1*tJ9-P5ENc68qiFBiKp0hDg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*HnK1KondBJOJKmJ5TVwbrA.gif\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-3W6Qbt2uyQXH6MSYHsXwA.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*ZA6IblUAv0qxbkiqVIw93A.jpeg\" /></figure><h3>Final Thoughts</h3><p>Adversarial Examples are a fascinating area of ongoing research. They highlight limitations of current systems and raise a number of interesting questions. While industries are racing to include visual intelligence systems in mission-critical infrastructure, looking at <em>edge-cases</em> and exploring solutions is a productive path. As the <a href=\"https://en.wikipedia.org/wiki/Surrealist\">surrealist</a> Belgian painter <a href=\"https://en.wikipedia.org/wiki/Ren%C3%A9_Magritte\"><strong>René Magritte</strong></a> said in “<a href=\"https://en.wikipedia.org/wiki/The_Treachery_of_Images\"><em>The Treachery of Images</em></a>”: <strong>“<em>Ceci n’est pas une pipe</em>”</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/500/1*-9LAwwDPElBj3qs7VAHoYg.png\" /></figure><h3>Interview</h3><p>The following is a interview with <a href=\"http://www.papernot.fr/\"><strong>Nicolas Papernot</strong></a><strong>, </strong>a machine learning researcher who recently published <a href=\"http://arxiv.org/pdf/1511.04508v1.pdf\">two</a> <a href=\"http://arxiv.org/pdf/1511.07528v1.pdf\">papers</a> on adversarial examples:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*KlaRASm4ndC8pDdWmFf5XA.jpeg\" /><figcaption>Nicolas Papernot</figcaption></figure><p><strong>Q: What is your core research interest in the “adversarial examples” space?</strong></p><p><em>I do research in a lab focused on security. Our end-goal is to identify vulnerabilities in deep neural networks to better understand their attack surface and defend them. Our first paper explores attacks while the second one defenses. Our algorithms were designed to reduce the number of input features that we perturb so that they can be applied on various datasets (spam, authentication, etc.).</em></p><p><strong>Q: In your recent publication you introduce a “a defensive mechanism to reduce the effectiveness of adversarial samples on DNNs”. Could you explain your approach in simple terms?</strong></p><p><em>Our paper proposes to provide additional information at training on samples. This Information takes the form of probabilities and gives us insight on the various classes. To extract probabilities we do a 1st training of the network. Then we do a 2nd training with the same architecture, including these probabilities. This gives more robustness and smoothness.</em></p><p><em>The consequence are: </em><strong><em>1.</em></strong><em> Derivatives very small (amplitude of jacobian components). </em><strong><em>2.</em></strong><em> average minimum perturbation (number of input dimensions) to leave source class and achieve target class increases by 500–800% in tests.</em></p><p><strong>Q: Do you think we will “solve” adversarial problems in the near future or are the problems deeper?</strong></p><p><em>It is a tough problem because it is closely linked to how we train our networks. </em><a href=\"http://arxiv.org/pdf/1511.04508v1.pdf\"><em>Distillation</em></a><em> as a defense is a good first step. New defenses will probably involve additional tricks at training.</em></p><p><strong>Q: Part of your research was “sponsored by the Army Research Laboratory”. How serious do you think the implications/risks of adversarial examples are for society at the current stage?</strong></p><blockquote><em>“</em><strong><em>Note that my opinions are mine and not the opinion of the Army Research Laboratory (but I acknowledge their generous support).</em></strong><em> The implications of adversarial examples are very serious for any company in the industry. If someone has potentially an incentive to benefit from the misbehaviors (e.g. classification) then there is a risk. The consequences can be bad: cars, spam, authentication, malware, network intrusion, fraud detection come to mind.”</em></blockquote><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/265/1*xfef1Z1l_dmzSzrVrBC0-Q.png\" /><figcaption>Generated Fingerprint with DeepTexture</figcaption></figure><blockquote>Get in touch here: <a href=\"https://twitter.com/samim\">twitter.com/samim</a> |<a href=\"http://samim.io/\">http://samim.io</a></blockquote><blockquote><a href=\"https://tinyletter.com/samim\">Sign up for the Newsletter for more experiments like this!</a></blockquote><h3>Follow Up Discussion</h3><h3>Zachary Lipton on Twitter</h3><p>@samim Nice bit on adversarial examples. One nit: Misleading to call perturbations noise as they're deliberately & deterministically chosen</p><h3>samim on Twitter</h3><p>@zacharylipton good point! Struggled abit between accuracy and accessibility. Would you be able to give me a clarifying quote to include?</p><h3>Zachary Lipton on Twitter</h3><p>@samim I think you could reasonably point out that these changes do look like noise.</p><h3>samim on Twitter</h3><p>@zacharylipton will tweak the post now based on your advice! Interview with researcher of 2 new papers on the topic will be added nxt week</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=998d8362e996\" width=\"1\" height=\"1\" alt=\"\">"
}