{
  "title": "Neural Variational Inference: Scaling Up",
  "link": "http://artem.sobolev.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html",
  "description": "<p>In the <a href=\"/posts/2016-07-01-neural-variational-inference-classical-theory.html\">previous post</a> I covered well-established classical theory developed in early 2000-s. Since then technology has made huge progress: now we have much more data, and a great need to process it and process it fast. In big data era we have huge datasets, and can not afford too many full passes over it, which might render classical VI methods impractical. Recently M. Hoffman et al. dissected classical Mean-Field VI to introduce stochasticity right into its heart, which resulted in <a href=\"https://arxiv.org/abs/1206.7051\">Stochastic Variational Inference</a>.</p>\n<!--more-->\n<h3>\nStochastic Variational Inference\n</h3>\n<p>We start with model assumptions: we have 2 types of latent variables, the global latent variable <span class=\"math inline\">\\(\\beta\\)</span> and a bunch of local variables <span class=\"math inline\">\\(z_n\\)</span> for each observation <span class=\"math inline\">\\(x_n\\)</span>. Recalling our GMM example, <span class=\"math inline\">\\(\\beta\\)</span> can be thought of as a mixture weights <span class=\"math inline\">\\(\\pi\\)</span>, and <span class=\"math inline\">\\(z_n\\)</span> are membership indicators, as previously. These variables are assumed to come from some exponential family distribution:</p>\n<p><span class=\"math display\">\\[\np(x_n, z_n \\mid \\beta) = h(x_n, z_n) \\exp \\left( \\beta^T t(x_n, z_n) - a_l(\\beta) \\right) \\\\\n\\\\\np(\\beta) = h(\\beta) \\exp(\\alpha^T t(\\beta) - a_g(\\alpha))\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(t(\\cdot)\\)</span> and <span class=\"math inline\">\\(h(\\cdot)\\)</span> are overloaded by their argument, so <span class=\"math inline\">\\(t(\\beta)\\)</span> and <span class=\"math inline\">\\(t(z_{nj})\\)</span> correspond to two different functions. <span class=\"math inline\">\\(t(\\cdot)\\)</span> gives a <strong>natural parameter</strong> and also <strong>sufficient statistics</strong>. <span class=\"math inline\">\\(a_g\\)</span> and <span class=\"math inline\">\\(a_l\\)</span> are log-normalizing constants which for exponential family distributions have an interesting property, namely, the gradient of the log-normalizing constant is the expectation of sufficient statistics: <span class=\"math inline\">\\(\\nabla_\\alpha a_g(\\alpha) = \\mathbb{E} t(\\beta)\\)</span>.</p>\n<p>From these assumptions we can derive <em>complete conditionals</em> (conditional distribution given all other hidden variables and observables) for <span class=\"math inline\">\\(\\beta\\)</span> and <span class=\"math inline\">\\(z_{nj}\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\np(\\beta \\mid x, z, \\alpha)\n&\\propto \\prod_{n=1}^N p(x_n, z_n \\mid \\beta) p(\\beta \\mid \\alpha) \\\\\n&= h(\\beta) \\prod_{n=1}^N h(x_n, z_n) \\exp \\left( \\beta^T \\sum_{n=1}^N t(x_n, z_n) - N a_l(\\beta) + \\alpha^T t(\\beta) - a_g(\\alpha) \\right) \\\\\n&\\propto h(\\beta) \\exp \\left( \\eta_g(x, z, \\alpha)^T t(\\beta) \\right)\n\\end{align}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(t(\\beta) = (\\beta, -a_l(\\beta))\\)</span>, <span class=\"math inline\">\\(\\eta_g(x, z, \\alpha) = (\\alpha_1 + \\sum_{n=1}^N t(x_n, z_n), \\alpha_2 + N)\\)</span>. We see that the (unnormalized) posterior distribution for <span class=\"math inline\">\\(\\beta\\)</span> has the same functional form as the (unnormalized) prior <span class=\"math inline\">\\(p(\\beta)\\)</span>, therefore after normalization it’d be</p>\n<p><span class=\"math display\">\\[\np(\\beta \\mid x, z, \\alpha)\n= h(\\beta) \\exp \\left( \\eta_g(x, z, \\alpha)^T t(\\beta) - a_g(\\eta_g(x, z, \\alpha)) \\right)\n\\]</span></p>\n<p>The same applies to local variables <span class=\"math inline\">\\(z_{nj}\\)</span>:</p>\n<p><span class=\"math display\">\\[\np(z_{nj} \\mid x_n, z_{n,-j}, \\beta)\n\\propto h(z_{nj}) \\exp \\left( \\eta_l(x_n, z_{n,-j}, \\beta)^T t(z_{nj}) \\right)\n\\]</span> Hence <span class=\"math display\">\\[\np(z_{nj} \\mid x_n, z_{n,-j}, \\beta)\n= h(z_{nj}) \\exp \\left( \\eta_l(x_n, z_{n,-j}, \\beta)^T t(z_{nj}) - a_m(\\eta_l(x_n, z_{n,-j}, \\beta)) \\right)\n\\]</span></p>\n<p>Even though we’ve managed to find the complete conditional for <span class=\"math inline\">\\(\\beta\\)</span>, it might be intractable to find the posterior for all latent variables <span class=\"math inline\">\\(p(\\beta, z \\mid x, \\alpha)\\)</span>. We therefore turn to the mean field approximation:</p>\n<p><span class=\"math display\">\\[\nq(z, \\beta \\mid \\Lambda) = q(\\beta \\mid \\lambda) \\prod_{n=1}^N \\prod_{j=1}^J q(z_{nj} \\mid \\phi_{nj})\n\\]</span></p>\n<p>We assume these marginal distributions come from the exponential family:</p>\n<p><span class=\"math display\">\\[\nq(\\beta \\mid \\lambda) = h(\\beta) \\exp(\\lambda^T t(\\beta) - a_g(\\lambda)) \\\\\nq(z_{nj} \\mid \\phi_{nj}) = h(z_{nj}) \\exp(\\phi_{nj}^T t(z_{nj}) - a_m(\\phi_{nj}))\n\\]</span></p>\n<p>Let’s find the optimal variational parameters now by optimizing the ELBO <span class=\"math inline\">\\(\\mathcal{L}(\\Theta, \\Lambda)\\)</span> (<span class=\"math inline\">\\(\\Theta\\)</span> is model parameters, <span class=\"math inline\">\\(\\alpha\\)</span>, and <span class=\"math inline\">\\(\\Lambda\\)</span> contains variational parameters <span class=\"math inline\">\\(\\phi\\)</span> and <span class=\"math inline\">\\(\\lambda\\)</span>) by <span class=\"math inline\">\\(\\lambda\\)</span> and <span class=\"math inline\">\\(\\phi_{nj}\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathcal{L}(\\lambda)\n&= \\mathbb{E}_{q} \\left( \\log p(x, z, \\beta) - \\log q(\\beta) - \\log q(z) \\right)\n= \\mathbb{E}_{q} \\left( \\log p(\\beta \\mid x, z) - \\log q(\\beta) \\right) + \\text{const} \\\\\n&= \\mathbb{E}_{q} \\left( \\eta_g(x, z, \\alpha)^T t(\\beta) - \\lambda^T t(\\beta) + a_g(\\lambda) \\right) + \\text{const} \\\\\n&= \\left(\\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha) - \\lambda \\right)^T \\mathbb{E}_{q(\\beta)} t(\\beta) + a_g(\\lambda) + \\text{const} \\\\\n&= \\left(\\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha) - \\lambda \\right)^T \\nabla_\\lambda a_g(\\lambda) t(\\beta) + a_g(\\lambda) + \\text{const}\n\\end{align}\n\\]</span></p>\n<p>Where we used aforementioned property of exponential family distributions: <span class=\"math inline\">\\(\\nabla_\\lambda a_g(\\lambda) = \\mathbb{E}_{q(\\beta)} t(\\beta)\\)</span>. The gradient then is <span class=\"math display\">\\[\n\\nabla_\\lambda \\mathcal{L}(\\lambda)\n= \\nabla_\\lambda^2 a_g(\\lambda) \\left(\\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha) - \\lambda \\right)\n\\]</span></p>\n<p>After setting it to zero we get an update for global latent variables: <span class=\"math inline\">\\(\\lambda = \\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha)\\)</span>. Following the same reasoning we derive the optimal update for <span class=\"math inline\">\\(\\phi_{nj}\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathcal{L}(\\phi_{nj})\n&= \\mathbb{E}_{q} \\left( \\log p(z_{nj} \\mid x_n, z_{n,-j}, \\beta) - \\log q(z_{nj}) \\right) + \\text{const} \\\\\n&= \\mathbb{E}_{q} \\left( \\eta_l(x_n, z_{n,-j}, \\beta)^T t(z_{nj}) - \\phi_{nj}^T t(z_{nj}) + a_m(\\phi_{nj})\\right) + \\text{const} \\\\\n&= \\left(\\mathbb{E}_{q(\\beta) q(z_{n,-j})} \\eta_l(x_n, z_{n,-j}, \\beta) - \\phi_{nj} \\right)^T \\mathbb{E}_{q(z_{nj})} t(z_{nj}) + a_m(\\phi_{nj}) + \\text{const} \\\\\n\\end{align}\n\\]</span></p>\n<p>The gradient then is <span class=\"math inline\">\\(\\nabla_{\\phi_{nj}} \\mathcal{L}(\\phi) = \\nabla_{\\phi_{nj}}^2 a_m(\\phi_{nj}) \\left(\\mathbb{E}_{q(\\beta) q(z_{n,-j})} \\eta_l(x_n, z_{n,-j}, \\beta) - \\phi_{nj} \\right)\\)</span>, and the update is <span class=\"math inline\">\\(\\phi_{nj} = \\mathbb{E}_{q(\\beta) q(z_{n,-j})} \\eta_l(x_n, z_{n,-j}, \\beta)\\)</span>.</p>\n<p>So far we found mean-field updates, as well as corresponding gradients of the ELBO for variational parameters <span class=\"math inline\">\\(\\lambda\\)</span> and <span class=\"math inline\">\\(\\phi_{nj}\\)</span>. Next step is to transform these gradients into <strong>natural gradients</strong>. Intuitively, classical gradient defines local linear approximation, where the notion of locality comes from the Euclidean space. However, parameters influence the ELBO only through distributions <span class=\"math inline\">\\(q\\)</span>, so we might like to alter our idea of locality based on how much the distributions change. This is what natural gradient does: it defines local linear approximation where locality means small distance (symmetrized KL-divergence) between distributions. There’s great formal explanation in the paper, and if you want to read more on that matter, I refer you to a great post by Roger Grosse, <a href=\"http://www.metacademy.org/roadmaps/rgrosse/dgml\">Differential geometry for machine learning</a>.</p>\n<p>The natural gradient can be obtained from the usual gradient using a simple linear transformation:</p>\n<p><span class=\"math display\">\\[\n\\nabla_\\lambda^\\text{N} f(\\lambda) = \\mathcal{I}(\\lambda)^{-1} \\nabla_{\\lambda} f(\\lambda)\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(\\mathcal{I}(\\lambda) := \\mathbb{E}_{q(\\beta \\mid \\lambda)} \\left[ \\nabla_\\lambda \\log q(\\beta \\mid \\lambda) (\\nabla_\\lambda \\log q(\\beta \\mid \\lambda))^T \\right]\\)</span> is Fisher Information Matrix. Here I considered parameter <span class=\"math inline\">\\(\\lambda\\)</span> of the distribution <span class=\"math inline\">\\(q(\\beta \\mid \\lambda)\\)</span>, you got the idea. For the exponential family distribution this Information Matrix takes an especially simple form:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathcal{I}(\\lambda)\n&= \\mathbb{E}_q (t(\\beta) - \\nabla_\\lambda a_g(\\lambda)) (t(\\beta) - \\nabla_\\lambda a_g(\\lambda))^T\n= \\mathbb{E}_q (t(\\beta) - \\mathbb{E}_q t(\\beta)) (t(\\beta) - \\mathbb{E}_q t(\\beta))^T \\\\\n&= \\text{Cov}_q (t(\\beta))\n = \\nabla_\\lambda^2 a_g(\\lambda)\n\\end{align}\n\\]</span></p>\n<p>Where we’ve used another <a href=\"https://en.wikipedia.org/wiki/Exponential_family#Differential_identities_for_cumulants\">differential identity for exponential family</a>. All these calculations lead us to the natural gradients of ELBO for variational parameters:</p>\n<p><span class=\"math display\">\\[\n\\nabla_\\lambda^\\text{N} \\mathcal{L}(\\lambda) = \\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha) - \\lambda \\\\\n\\nabla_{\\phi_{nj}}^\\text{N} \\mathcal{L}(\\lambda) = \\mathbb{E}_{q(\\beta) q(z_{n,-j})} \\eta_l(x_n, z_{n,-j}, \\beta) - \\phi_{nj}\n\\]</span></p>\n<p>Surprisingly, computation-wise calculating natural gradients is even simpler that calculating classical gradients! There’s an interesting connection between the mean-field update and a natural gradient step. In particular, if we make a step along the natural gradient with step size equal 1, we’d get <span class=\"math inline\">\\(\\lambda^{\\text{new}} = \\lambda^{\\text{old}} + (\\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha) - \\lambda^{\\text{old}}) = \\mathbb{E}_{q(z)} \\eta_g(x, z, \\alpha)\\)</span>. The same applies to parameters <span class=\"math inline\">\\(\\phi\\)</span>. This means that the mean field updates are exactly natural gradient steps, and vice versa.</p>\n<p>Recall, we have derived mean field updates by finding a minima of KL-divergence with the true posterior, that is in just one step (one update) we arrive at minimum. Obviously, we have the same in the natural gradient formulation, when just one step brings us to the optimum.</p>\n<p>Now, the last component is stochasticity itself. So far we have only played a little with mean-field update scheme, and discovered its connection to the natural gradient optimization. We note that we have 2 parameters: local <span class=\"math inline\">\\(\\phi_{nj}\\)</span> and global parameter <span class=\"math inline\">\\(\\lambda\\)</span>. The first one is easy to optimize over as it depends only on one, <span class=\"math inline\">\\(n\\)</span>th sample <span class=\"math inline\">\\(x_n\\)</span>. The second one, though, needs to incorporate information from all the samples, which is computationally prohibitive in large scale regime. Luckily, now once we know the equivalence between mean-field update and natural gradient step, we can borrow ideas from stochastic optimization to make this process more scalable.</p>\n<p>Let’s first reformulate the ELBO to include the sum over samples <span class=\"math inline\">\\(x_n\\)</span>:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathcal{L}(\\Theta, \\Lambda)\n&= \\mathbb{E}_{q} \\left[ \\log p(\\beta \\mid \\alpha) - \\log q(\\beta \\mid \\lambda) + \\sum_{n=1}^N \\left(\\log p(x_n, z_n \\mid \\beta) - \\log q(z_n \\mid \\phi_n) \\right) \\right] \\\\\n& = \\mathbb{E}_{q} \\left[ \\log p(\\beta \\mid \\alpha) - \\log q(\\beta \\mid \\lambda) + N \\mathbb{E}_{I} \\left(\\log p(x_I, z_I \\mid \\beta) - \\log q(z_I \\mid \\phi_I) \\right) \\right]\n\\end{align}\n\\]</span></p>\n<p>Where <span class=\"math inline\">\\(I \\sim \\text{Unif}\\{1, \\dots, N\\}\\)</span> — uniformly distribution index of a sample. Now let’s estimate <span class=\"math inline\">\\(\\mathcal{L}\\)</span> using a sample <span class=\"math inline\">\\(S\\)</span> (assume <span class=\"math inline\">\\(N\\)</span> divides by sample size <span class=\"math inline\">\\(|S|\\)</span>) of uniformly chosen indices, this’d result in an unbiased estimator (it’s gradient would also be unbiased, so we can maximize the true ELBO by maximizing the estimate). Author of the paper start with single-sample derivation and then extend it to minibatches, but I decided I’d go straight to the minibatch case:</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathcal{L}_S(\\Theta, \\Lambda)\n& := \\mathbb{E}_{q} \\left[ \\log p(\\beta \\mid \\alpha) - \\log q(\\beta \\mid \\lambda) + \\frac{N}{|S|} \\sum_{i \\in S} \\left(\\log p(x_i, z_i \\mid \\beta) - \\log q(z_i \\mid \\phi_i) \\right) \\right] \\\\\n& = \\mathbb{E}_{q} \\left[ \\log p(\\beta \\mid \\alpha) - \\log q(\\beta \\mid \\lambda) + \\sum_{n=1}^{N / |S|} \\sum_{i \\in S} \\left(\\log p(x_i, z_i \\mid \\beta) - \\log q(z_i \\mid \\phi_i) \\right) \\right]\n\\end{align}\n\\]</span></p>\n<p>This estimate is exactly <span class=\"math inline\">\\(\\mathcal{L}(\\Theta, \\Lambda)\\)</span> calculated on sample consisting of <span class=\"math inline\">\\(\\{x_i, z_i\\}_{i \\in S}\\)</span> repeated <span class=\"math inline\">\\(N / |S|\\)</span> times. Hence its natural gradient w.r.t. <span class=\"math inline\">\\(\\lambda\\)</span> is</p>\n<p><span class=\"math display\">\\[\n\\nabla_\\lambda^\\text{N} \\mathcal{L}_S(\\lambda) = \\mathbb{E}_{q(z)} \\eta_g(\\{x_S\\}_{n=1}^{N/|S|}, \\{z_S\\}_{n=1}^{N/|S|}, \\alpha) - \\lambda \\\\\n\\]</span></p>\n<p>One important note: for stochastic optimization we can’t use constant step size. As Robbins-Monro conditions suggest, we need to use schedule <span class=\"math inline\">\\(\\rho_t\\)</span> such that <span class=\"math inline\">\\(\\sum \\rho_t = \\infty\\)</span> and <span class=\"math inline\">\\(\\sum \\rho_t^2 < \\infty\\)</span>. Then the update <span class=\"math inline\">\\(\\lambda^{\\text{new}} = \\lambda^{\\text{old}} + \\rho_t \\nabla_\\lambda^\\text{N} \\mathcal{L}_S(\\lambda) = (1 - \\rho_t) \\lambda^{\\text{old}} + \\rho_t \\mathbb{E}_{q(z)} \\eta_g(\\{x_S\\}_{n=1}^{N/|S|}, \\{z_S\\}_{n=1}^{N/|S|}, \\alpha)\\)</span></p>\nFinally we have the following optimization scheme:\n<ul>\n<li>\nStart with random initialization for <span class=\"math inline\">\\(\\lambda^{(0)}\\)</span>\n</li>\n<li>\nFor <span class=\"math inline\">\\(t\\)</span> from 0 to MAX_ITER\n<ol>\n<li>\nSample <span class=\"math inline\">\\(S \\sim \\text{Unif}\\{1, \\dots, N\\}^{|S|}\\)</span>\n</li>\n<li>\nFor each sample <span class=\"math inline\">\\(i \\in S\\)</span> update the local variational parameter <span class=\"math inline\">\\(\\phi_{i,j} = \\mathbb{E}_{q(\\beta) q(z_{i,-j})} \\eta_l(x_i, z_{i,-j}, \\beta)\\)</span>\n</li>\n<li>\nReplicate the sample <span class=\"math inline\">\\(N / |S|\\)</span> times and compute the global update <span class=\"math inline\">\\(\\hat \\lambda = \\mathbb{E}_{q(z)} \\eta_g(\\{x_S\\}_{n=1}^{N/|S|}, \\{z_S\\}_{n=1}^{N/|S|}, \\alpha)\\)</span>\n</li>\n<li>\nUpdate the global update <span class=\"math inline\">\\(\\lambda^{(t+1)} = (1-\\rho_t) \\lambda^{(t)} + \\rho_t \\hat \\lambda\\)</span>\n</li>\n</ol>\n</li>\n</ul>",
  "pubDate": "Mon, 04 Jul 2016 00:00:00 UT",
  "guid": "http://artem.sobolev.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html",
  "dc:creator": "Artem"
}