{
  "title": "Rethinking Hierarchicies in Pre-trained Plain Vision Transformer. (arXiv:2211.01785v1 [cs.CV])",
  "link": "http://arxiv.org/abs/2211.01785",
  "description": "<p>Self-supervised pre-training vision transformer (ViT) via masked image\nmodeling (MIM) has been proven very effective. However, customized algorithms\nshould be carefully designed for the hierarchical ViTs, e.g., GreenMIM, instead\nof using the vanilla and simple MAE for the plain ViT. More importantly, since\nthese hierarchical ViTs cannot reuse the off-the-shelf pre-trained weights of\nthe plain ViTs, the requirement of pre-training them leads to a massive amount\nof computational cost, thereby incurring both algorithmic and computational\ncomplexity. In this paper, we address this problem by proposing a novel idea of\ndisentangling the hierarchical architecture design from the self-supervised\npre-training. We transform the plain ViT into a hierarchical one with minimal\nchanges. Technically, we change the stride of linear embedding layer from 16 to\n4 and add convolution (or simple average) pooling layers between the\ntransformer blocks, thereby reducing the feature size from 1/4 to 1/32\nsequentially. Despite its simplicity, it outperforms the plain ViT baseline in\nclassification, detection, and segmentation tasks on ImageNet, MS COCO,\nCityscapes, and ADE20K benchmarks, respectively. We hope this preliminary study\ncould draw more attention from the community on developing effective\n(hierarchical) ViTs while avoiding the pre-training cost by leveraging the\noff-the-shelf checkpoints. The code and models will be released at\nhttps://github.com/ViTAE-Transformer/HPViT.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"
}