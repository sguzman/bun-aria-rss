{
  "title": "Deriving Mean-Field Variational Bayes",
  "link": "",
  "published": "2018-11-23T10:00:00-05:00",
  "updated": "2018-11-23T10:00:00-05:00",
  "author": {
    "name": "Will Wolf"
  },
  "id": "tag:willwolf.io,2018-11-23:/2018/11/23/mean-field-variational-bayes/",
  "summary": "<p>A detailed derivation of Mean-Field Variational Bayes, its connection to Expectation-Maximization, and its implicit motivation for the \"black-box variational inference\" methods born in recent years.</p>",
  "content": "<p>\"Mean-Field Variational Bayes\" (MFVB), is similar to <a href=\"https://willwolf.io/2018/11/11/em-for-lda/\">expectation-maximization</a> (EM) yet distinct in two key ways:</p>\n<ol>\n<li>We do not minimize <span class=\"math\">\\(\\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\)</span>, i.e. perform the E-step, as [in the problems in which we employ mean-field] the posterior distribution <span class=\"math\">\\(p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\)</span> \"is too complex to work with,\"â„¢ i.e. it has no analytical form.</li>\n<li>Our variational distribution <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> is a <em>factorized distribution</em>, i.e.</li>\n</ol>\n<div class=\"math\">$$\nq(\\mathbf{Z}) = \\prod\\limits_i^{M} q_i(\\mathbf{Z}_i)\n$$</div>\n<p>for all latent variables <span class=\"math\">\\(\\mathbf{Z}_i\\)</span>.</p>\n<p>Briefly, factorized distributions are cheap to compute: if each <span class=\"math\">\\(q_i(\\mathbf{Z}_i)\\)</span> is Gaussian, <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> requires optimization of <span class=\"math\">\\(2M\\)</span> parameters (a mean and a variance for each factor); conversely, a non-factorized <span class=\"math\">\\(q(\\mathbf{Z}) = \\text{Normal}(\\mu, \\Sigma)\\)</span> would require optimization of <span class=\"math\">\\(M\\)</span> parameters for the mean and <span class=\"math\">\\(\\frac{M^2 + M}{2}\\)</span> parameters for the covariance. Following intuition, this gain in computational efficiency comes at the cost of decreased accuracy in approximating the true posterior over latent variables.</p>\n<h2>So, what is it?</h2>\n<p>Mean-field Variational Bayes is an iterative maximization of the ELBO. More precisely, it is an iterative M-step with respect to the variational factors <span class=\"math\">\\(q_i(\\mathbf{Z}_i)\\)</span>.</p>\n<p>In the simplest case, we posit a variational factor over every latent variable, <em>as well as every parameter</em>. In other words, as compared to the log-marginal decomposition in EM, <span class=\"math\">\\(\\theta\\)</span> is absorbed into <span class=\"math\">\\(\\mathbf{Z}\\)</span>.</p>\n<div class=\"math\">$$\n\\log{p(\\mathbf{X}\\vert\\theta)} = \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z}\\vert\\theta)}{q(\\mathbf{Z})}}\\bigg] + \\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X}, \\theta)\\big)\\quad \\text{(EM)}\n$$</div>\n<p>becomes</p>\n<div class=\"math\">$$\n\\log{p(\\mathbf{X})} = \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}\\bigg] + \\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X})\\big)\\quad \\text{(MFVB)}\n$$</div>\n<p>From there, we simply maximize the ELBO, i.e. <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}\\bigg]\\)</span>, by <em>iteratively maximizing with respect to each variational factor <span class=\"math\">\\(q_i(\\mathbf{Z}_i)\\)</span></em> in turn.</p>\n<h2>What's this do?</h2>\n<p>Curiously, we note that <span class=\"math\">\\(\\log{p(\\mathbf{X})}\\)</span> is a <em>fixed quantity</em> with respect to <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>: updating our variational factors <em>will not change</em> the marginal log-likelihood of our data.</p>\n<p>This said, we note that the ELBO and <span class=\"math\">\\(\\text{KL}\\big(q(\\mathbf{Z})\\Vert p(\\mathbf{Z}\\vert\\mathbf{X})\\big)\\)</span> trade off linearly: when one goes up by <span class=\"math\">\\(\\Delta\\)</span>, the other goes down by <span class=\"math\">\\(\\Delta\\)</span>.</p>\n<p>As such, (iteratively) maximizing the ELBO in MFVB is akin to minimizing the divergence between the true posterior over the latent variables given data and our factorized variational approximation thereof.</p>\n<h2>Derivation</h2>\n<p>So, what do these updates look like?</p>\n<p>First, let's break the ELBO into its two main components:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}\\bigg]\n&amp;= \\int{q(\\mathbf{Z})\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}}d\\mathbf{Z}\\\\\n&amp;= \\int{q(\\mathbf{Z})\\log{p(\\mathbf{X, Z})}}d\\mathbf{Z} - \\int{q(\\mathbf{Z})\\log{q(\\mathbf{Z})}}d\\mathbf{Z}\\\\\n&amp;= A + B\n\\end{align*}\n$$</div>\n<p>Next, rewrite this expression in a way that isolates a single variational factor <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span>, i.e. the factor with respect to which we'd like to maximize the ELBO in a given iteration.</p>\n<h2>Expanding the first term</h2>\n<div class=\"math\">$$\n\\begin{align*}\nA\n&amp;= \\int{q(\\mathbf{Z})\\log{p(\\mathbf{X, Z})}d\\mathbf{Z}}\\\\\n&amp;= \\int{\\prod\\limits_{i}q_i(\\mathbf{Z}_i)\\log{p(\\mathbf{X, Z})}d\\mathbf{Z}_i}\\\\\n&amp;= \\int{q_j(\\mathbf{Z}_j)\\bigg[\\int{\\prod\\limits_{i \\neq j}q_i(\\mathbf{Z}_{i})\\log{p(\\mathbf{X, Z})}}d\\mathbf{Z}_i\\bigg]}d\\mathbf{Z}_j\\\\\n&amp;= \\int{q_j(\\mathbf{Z}_j){ \\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}] }d\\mathbf{Z}_j}\\\\\n\\end{align*}\n$$</div>\n<p>Following Bishop<sup id=\"fnref:1\"><a class=\"footnote-ref\" href=\"#fn:1\">1</a></sup>'s derivation, we've introduced the notation:</p>\n<div class=\"math\">$$\n\\int{\\prod\\limits_{i \\neq j}q_i(\\mathbf{Z}_{i})\\log{p(\\mathbf{X, Z})}}d\\mathbf{Z}_i = \\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\n$$</div>\n<p>A few things to note, and in case this looks strange:</p>\n<ul>\n<li>Were the left-hand side to read <span class=\"math\">\\(\\int{q(\\mathbf{Z})\\log{p(\\mathbf{X, Z})}}d\\mathbf{Z}\\)</span>, this would look like the perfectly vanilla expectation <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}[\\log{p(\\mathbf{X, Z})}]\\)</span>.</li>\n<li>An expectation maps a function <span class=\"math\">\\(f\\)</span>, e.g. <span class=\"math\">\\(\\log{p(\\mathbf{X, Z})}\\)</span>, to a single real number. As our expression reads <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\)</span> as opposed to <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}[\\log{p(\\mathbf{X, Z})}]\\)</span>, we're conspicuously unable to integrate over the remaining factor <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span></li>\n<li><strong>As such, <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\)</span> gives a function of the value of <span class=\"math\">\\(\\mathbf{Z}_j\\)</span></strong> which itself maps to the aforementioned real number.</li>\n</ul>\n<p>To further illustrate, let's employ some toy Python code:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"c1\"># Suppose `Z = [Z_0, Z_1, Z_2]`, with corresponding (discrete) variational distributions `q_0`, `q_1`, `q_2`</span>\n\n<span class=\"n\">q_0</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">2</span><span class=\"p\">),</span>  <span class=\"c1\"># q_0(1) = .2</span>\n    <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">q_1</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">3</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">q_2</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">2</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"o\">.</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">q_0</span><span class=\"p\">,</span> <span class=\"n\">q_1</span><span class=\"p\">,</span> <span class=\"n\">q_2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Next, suppose we'd like to isolate Z_2</span>\n<span class=\"n\">j</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n</pre></div>\n<p><span class=\"math\">\\(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\)</span>, written <code>E_i_neq_j_log_p_X_Z</code> below, can be computed as:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">E_i_neq_j_log_p_X_Z</span><span class=\"p\">(</span><span class=\"n\">Z_j</span><span class=\"p\">):</span>\n\n    <span class=\"n\">E</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"n\">Z_i_neq_j_dists</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">dist</span> <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">dist</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">!=</span> <span class=\"n\">j</span><span class=\"p\">]</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">comb</span> <span class=\"ow\">in</span> <span class=\"n\">product</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">Z_i_neq_j_dists</span><span class=\"p\">):</span>\n        <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">comb</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">comb</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">)):</span>\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"n\">j</span><span class=\"p\">:</span>\n                <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">Z_j</span><span class=\"p\">)</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">Z_i</span><span class=\"p\">,</span> <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">comb</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n                <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">Z_i</span><span class=\"p\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">prob</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                    <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"n\">p</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"n\">prob</span> <span class=\"o\">*=</span> <span class=\"n\">p</span>\n        <span class=\"n\">E</span> <span class=\"o\">+=</span> <span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"n\">ln_p_X_Z</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">E</span>\n</pre></div>\n<ul>\n<li>Continuing with our notes, it was not immediately obvious to me how and why we're able to introduce a second integral sign on line 3 of the derivation above. Notwithstanding, the reason is quite simple; a simple exercise of nested for-loops is illustrative.</li>\n</ul>\n<p>Before beginning, we remind the definition of an integral in code. In its simplest example, <span class=\"math\">\\(\\int{ydx}\\)</span> can be written as:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"n\">lower_lim</span><span class=\"p\">,</span> <span class=\"n\">upper_lim</span><span class=\"p\">,</span> <span class=\"n\">n_ticks</span><span class=\"p\">)</span>\n\n<span class=\"n\">integral</span> <span class=\"o\">=</span> <span class=\"nb\">sum</span><span class=\"p\">([</span><span class=\"n\">y</span> <span class=\"o\">*</span> <span class=\"n\">dx</span> <span class=\"k\">for</span> <span class=\"n\">dx</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># ...where `n_ticks` approaches infinity.</span>\n</pre></div>\n<p>With this in mind, the following confirms the self-evidence of the second integral sign:</p>\n<div class=\"highlight\"><pre><span></span><span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">])</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">ln_p_X_Z</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">Z</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>  <span class=\"c1\"># some dummy expression</span>\n\n\n<span class=\"c1\"># Line 2 of `Expanding the first term`</span>\n<span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"k\">for</span> <span class=\"n\">Z_0</span> <span class=\"ow\">in</span> <span class=\"n\">q_0</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">Z_1</span> <span class=\"ow\">in</span> <span class=\"n\">q_1</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">Z_2</span> <span class=\"ow\">in</span> <span class=\"n\">q_2</span><span class=\"p\">:</span>\n            <span class=\"n\">val_z_0</span><span class=\"p\">,</span> <span class=\"n\">prob_z_0</span> <span class=\"o\">=</span> <span class=\"n\">Z_0</span>\n            <span class=\"n\">val_z_1</span><span class=\"p\">,</span> <span class=\"n\">prob_z_1</span> <span class=\"o\">=</span> <span class=\"n\">Z_1</span>\n            <span class=\"n\">val_z_2</span><span class=\"p\">,</span> <span class=\"n\">prob_z_2</span> <span class=\"o\">=</span> <span class=\"n\">Z_2</span>\n            <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">val_z_0</span><span class=\"p\">,</span> <span class=\"n\">val_z_1</span><span class=\"p\">,</span> <span class=\"n\">val_z_2</span><span class=\"p\">])</span>\n            <span class=\"n\">total</span> <span class=\"o\">+=</span> <span class=\"n\">prob_z_0</span> <span class=\"o\">*</span> <span class=\"n\">prob_z_1</span> <span class=\"o\">*</span> <span class=\"n\">prob_z_2</span> <span class=\"o\">*</span> <span class=\"n\">ln_p_X_Z</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">)</span>\n\n<span class=\"n\">TOTAL</span> <span class=\"o\">=</span> <span class=\"n\">total</span>\n\n\n<span class=\"c1\"># Line 3 of `Expanding the first term`</span>\n<span class=\"n\">total</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"k\">for</span> <span class=\"n\">Z_0</span> <span class=\"ow\">in</span> <span class=\"n\">q_0</span><span class=\"p\">:</span>\n    <span class=\"n\">_total</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"n\">val_z_0</span><span class=\"p\">,</span> <span class=\"n\">prob_z_0</span> <span class=\"o\">=</span> <span class=\"n\">Z_0</span>\n    <span class=\"k\">for</span> <span class=\"n\">Z_1</span> <span class=\"ow\">in</span> <span class=\"n\">q_1</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">Z_2</span> <span class=\"ow\">in</span> <span class=\"n\">q_2</span><span class=\"p\">:</span>\n            <span class=\"n\">val_z_1</span><span class=\"p\">,</span> <span class=\"n\">prob_z_1</span> <span class=\"o\">=</span> <span class=\"n\">Z_1</span>\n            <span class=\"n\">val_z_2</span><span class=\"p\">,</span> <span class=\"n\">prob_z_2</span> <span class=\"o\">=</span> <span class=\"n\">Z_2</span>\n            <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">val_z_0</span><span class=\"p\">,</span> <span class=\"n\">val_z_1</span><span class=\"p\">,</span> <span class=\"n\">val_z_2</span><span class=\"p\">])</span>\n            <span class=\"n\">_total</span> <span class=\"o\">+=</span> <span class=\"n\">prob_z_1</span> <span class=\"o\">*</span> <span class=\"n\">prob_z_2</span> <span class=\"o\">*</span> <span class=\"n\">ln_p_X_Z</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">)</span>\n    <span class=\"n\">total</span> <span class=\"o\">+=</span> <span class=\"n\">prob_z_0</span> <span class=\"o\">*</span> <span class=\"n\">_total</span>\n\n\n<span class=\"k\">assert</span> <span class=\"n\">total</span> <span class=\"o\">==</span> <span class=\"n\">TOTAL</span>\n</pre></div>\n<p>In effect, isolating <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span> is akin to the penultimate line <code>total += prob_z_0 * _total</code>, i.e. multiplying <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span> by an intermediate summation <code>_total</code>.  Therefore, the second integral sign is akin to <code>_total += prob_z_1 * prob_z_2 * ln_p_X_Z(X, Z)</code>, i.e. the computation of this intermediate summation itself.</p>\n<p>More succinctly, a multi-dimensional integral can be thought of as a nested-for-loop which commutes a global sum. Herein, we are free to compute intermediate sums at will.</p>\n<h2>Expanding the second term</h2>\n<p>Next, let's expand <span class=\"math\">\\(B\\)</span>. We note that this is the entropy of the full variational distribution <span class=\"math\">\\(q(\\mathbf{Z})\\)</span>.</p>\n<div class=\"math\">$$\n\\begin{align*}\nB\n&amp;= - \\int{q(\\mathbf{Z})\\log{q(\\mathbf{Z})}}d\\mathbf{Z}\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{q(\\mathbf{Z})}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\prod\\limits_{i}q_i(\\mathbf{Z}_i)}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\sum\\limits_{i}\\log{q_i(\\mathbf{Z}_i)}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{q_j(\\mathbf{Z}_j)} + \\sum\\limits_{i \\neq j}\\log{q_i(\\mathbf{Z}_i)}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{q_j(\\mathbf{Z}_j)}\\bigg] - \\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\sum\\limits_{i \\neq j}\\log{q_i(\\mathbf{Z}_i)}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q_j(\\mathbf{Z}_j)}\\bigg[\\log{q_j(\\mathbf{Z}_j)}\\bigg] - \\mathop{\\mathbb{E}}_{q_{i \\neq j}(\\mathbf{Z}_i)}\\bigg[\\sum\\limits_{i \\neq j}\\log{q_i(\\mathbf{Z}_i)}\\bigg]\\\\\n&amp;= - \\mathop{\\mathbb{E}}_{q_j(\\mathbf{Z}_j)}\\bigg[\\log{q_j(\\mathbf{Z}_j)}\\bigg] + \\text{const}\\\\\n&amp;= - \\int{q_j(\\mathbf{Z}_j)\\log{q_j(\\mathbf{Z}_j)}}d\\mathbf{Z}_j + \\text{const}\\\\\n\\end{align*}\n$$</div>\n<p>As we'll be maximizing w.r.t. just <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span>, we can set all terms that don't include this factor to constants.</p>\n<h2>Putting it back together</h2>\n<div class=\"math\">$$\n\\begin{align*}\n\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}\\bigg]\n&amp;= A + B\\\\\n&amp;= \\int{q_j(\\mathbf{Z}_j){ \\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}] }d\\mathbf{Z}_j} - \\int{q_j(\\mathbf{Z}_j)\\log{q_j(\\mathbf{Z}_j)}}d\\mathbf{Z}_j + \\text{const}\\\\\n\\end{align*}\n$$</div>\n<h2>One final pseudonym</h2>\n<p>Were we able to replace the expectation in <span class=\"math\">\\(A\\)</span> with the <span class=\"math\">\\(\\log\\)</span> of some density <span class=\"math\">\\(D\\)</span>, i.e.</p>\n<div class=\"math\">$$\n= \\int{q_j(\\mathbf{Z}_j){ \\log{D} }\\ d\\mathbf{Z}_j} - \\int{q_j(\\mathbf{Z}_j)\\log{q_j(\\mathbf{Z}_j)}}d\\mathbf{Z}_j + \\text{const}\n$$</div>\n<p><span class=\"math\">\\(A + B\\)</span> could be rewritten as <span class=\"math\">\\(-\\text{KL}(q_j(\\mathbf{Z}_j)\\Vert D)\\)</span>.</p>\n<p>Acknowledging that <span class=\"math\">\\(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\)</span> is an unnormalized log-likelihood written as a function of <span class=\"math\">\\(\\mathbf{Z}_j\\)</span>, we temporarily rewrite it as:</p>\n<div class=\"math\">$$\n\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}] = \\log{\\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j})\n$$</div>\n<p>As such:</p>\n<div class=\"math\">$$\n\\begin{align*}\n\\mathop{\\mathbb{E}}_{q(\\mathbf{Z})}\\bigg[\\log{\\frac{p(\\mathbf{X, Z})}{q(\\mathbf{Z})}}\\bigg]\n&amp;= \\int{q_j(\\mathbf{Z}_j){ \\log{\\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j}) }d\\mathbf{Z}_j} - \\int{q_j(\\mathbf{Z}_j)\\log{q_j(\\mathbf{Z}_j)}}d\\mathbf{Z}_j + \\text{const}\\\\\n&amp;= \\int{q_j(\\mathbf{Z}_j){ \\log{\\frac{\\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j)}{q_j(\\mathbf{Z}_j)}} }d\\mathbf{Z}_j} + \\text{const}\\\\\n&amp;= - \\text{KL}\\big(q_j(\\mathbf{Z}_j)\\Vert \\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j)\\big) + \\text{const}\\\\\n\\end{align*}\n$$</div>\n<p>Finally, per this expression, the ELBO reaches its minimum when:</p>\n<div class=\"math\">$$\n\\begin{align*}\nq_j(\\mathbf{Z}_j)\n&amp;= \\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j)\\\\\n&amp;= \\exp{\\bigg(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\bigg)}\n\\end{align*}\n$$</div>\n<p>Or equivalently:</p>\n<div class=\"math\">$$\n\\log{q_j(\\mathbf{Z}_j)} = \\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\n$$</div>\n<p><strong>Summing up:</strong></p>\n<ul>\n<li>Iteratively minimizing the divergence between <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span> and <span class=\"math\">\\(\\tilde{p}(\\mathbf{X}, \\mathbf{Z}_j)\\)</span> for all factors <span class=\"math\">\\(j\\)</span> is our mechanism for maximizing the ELBO</li>\n<li>In turn, maximizing the ELBO is our mechanism for minimizing the KL divergence between the full factorized posterior <span class=\"math\">\\(q(\\mathbf{Z})\\)</span> and the true posterior <span class=\"math\">\\(p(\\mathbf{Z}\\vert\\mathbf{X})\\)</span>.</li>\n</ul>\n<p>Finally, as the optimal density <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span> relies on those of <span class=\"math\">\\(q_{i \\neq j}(\\mathbf{Z}_{i})\\)</span>, this optimization algorithm is necessarily <em>iterative</em>.</p>\n<h2>Normalization constant</h2>\n<p>Nearing the end, we note that <span class=\"math\">\\(q_j(\\mathbf{Z}_j) = \\exp{\\bigg(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\bigg)}\\)</span> is not necessarily a normalized density (over <span class=\"math\">\\(\\mathbf{Z}_j\\)</span>). \"By inspection,\" we compute:</p>\n<div class=\"math\">$$\n\\begin{align*}\nq_j(\\mathbf{Z}_j)\n&amp;= \\frac{\\exp{\\bigg(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\bigg)}}{\\int{\\exp{\\bigg(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\bigg)}d\\mathbf{Z}_j}}\\\\\n&amp;= \\exp{\\bigg(\\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\\bigg)} + \\text{const}\\\\\n\\end{align*}\n$$</div>\n<h2>How to actually employ this thing</h2>\n<p>First, plug in values for the right-hand side of:</p>\n<div class=\"math\">$$\n\\log{q_j(\\mathbf{Z}_j)} = \\mathop{\\mathbb{E}}_{i \\neq j}[\\log{p(\\mathbf{X, Z})}]\n$$</div>\n<p>Then, attempt to rearrange this expression such that:</p>\n<p>Once exponentiated, giving <span class=\"math\">\\(\\exp{\\big(\\log{q_j(\\mathbf{Z}_j)}\\big)} = q_j(\\mathbf{Z}_j)\\)</span>, we are left with something that, once normalized (by inspection), resembles a known density function (e.g. a Gaussian, a Gamma, etc.).</p>\n<p>NB: This may require significant computation.</p>\n<h1>Approximating a Gaussian</h1>\n<p>Here, we'll approximate a 2D multivariate Gaussian with a factorized mean-field approximation.</p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-1.png\"/></p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-2.png\"/></p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-3.png\"/></p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-4.png\"/></p>\n<p><img alt=\"png\" class=\"img-responsive\" src=\"https://willwolf.io/figures/mean-field-variational-bayes/mv-gaussian-approx-5.png\"/></p>\n<h1>Summing up</h1>\n<p>Mean-Field Variational Bayes is an iterative optimization algorithm for maximizing a lower-bound of the marginal likelihood of some data <span class=\"math\">\\(\\mathbf{X}\\)</span> under a given model with latent variables <span class=\"math\">\\(\\mathbf{Z}\\)</span>. It accomplishes this task by positing a factorized variational distribution over all latent variables <span class=\"math\">\\(\\mathbf{Z}\\)</span> and parameters <span class=\"math\">\\(\\theta\\)</span>, then computes, <em>analytically</em>, the algebraic forms and parameters of each factor which maximize this bound.</p>\n<p>In practice, this process can be cumbersome and labor-intensive. As such, in recent years, \"black-box variational inference\" techniques were born, which <em>fix</em> the forms of each factor <span class=\"math\">\\(q_j(\\mathbf{Z}_j)\\)</span>, then optimize its parameters via gradient descent.</p>\n<h2>References</h2>\n<div class=\"footnote\">\n<hr/>\n<ol>\n<li id=\"fn:1\">\n<p>C. M. Bishop. Pattern recognition and machine learning,\npage 229. Springer-Verlag New York, 2006.Â <a class=\"footnote-backref\" href=\"#fnref:1\" title=\"Jump back to footnote 1 in the text\">â†©</a></p>\n</li>\n</ol>\n</div>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}