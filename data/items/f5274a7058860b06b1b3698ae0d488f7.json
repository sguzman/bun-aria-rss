{
  "title": "Modern Computational Methods for Bayesian Inference — A Reading List",
  "link": "",
  "id": "https://www.georgeho.org/bayesian-inference-reading/",
  "updated": "2019-01-02T00:00:00Z",
  "published": "2019-01-02T00:00:00Z",
  "content": "<p>Lately I&rsquo;ve been troubled by how little I actually knew about how Bayesian\ninference <em>really worked</em>. I could explain to you <a href=\"https://maria-antoniak.github.io/2018/11/19/data-science-crash-course.html\">many other machine learning\ntechniques</a>,\nbut with Bayesian modelling&hellip; well, there&rsquo;s a model (which is basically the\nlikelihood, I think?), and then there&rsquo;s a prior, and then, um&hellip;</p>\n<p>What actually happens when you run a sampler? What makes inference\n&ldquo;variational&rdquo;? And what is this automatic differentiation doing in my\nvariational inference? <em>Cue long sleepless nights, contemplating my own\nignorance.</em></p>\n<p>So to celebrate the new year<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>, I compiled a list of things to read — blog\nposts, journal papers, books, anything that would help me understand (or at\nleast, appreciate) the math and computation that happens when I press the <em>Magic\nInference Button™</em>. Again, this reading list isn&rsquo;t focused on how to use\nBayesian modelling for a <em>specific</em> use case<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup>; it’s focused on how modern\ncomputational methods for Bayesian inference work <em>in general</em>.</p>\n<p>So without further ado&hellip;</p>\n<div>\n<h2>Contents</h2>\n<nav id=\"TableOfContents\">\n<ul>\n<li><a href=\"#markov-chain-monte-carlo\">Markov-Chain Monte Carlo</a>\n<ul>\n<li><a href=\"#for-the-uninitiated\">For the uninitiated</a></li>\n<li><a href=\"#hamiltonian-monte-carlo-and-the-no-u-turn-sampler\">Hamiltonian Monte Carlo and the No-U-Turn Sampler</a></li>\n<li><a href=\"#sequential-monte-carlo-and-other-sampling-methods\">Sequential Monte Carlo and other sampling methods</a></li>\n</ul>\n</li>\n<li><a href=\"#variational-inference\">Variational Inference</a>\n<ul>\n<li><a href=\"#for-the-uninitiated-1\">For the uninitiated</a></li>\n<li><a href=\"#automatic-differentiation-variational-inference-advi\">Automatic differentiation variational inference (ADVI)</a></li>\n</ul>\n</li>\n<li><a href=\"#open-source-software-for-bayesian-inference\">Open-Source Software for Bayesian Inference</a></li>\n<li><a href=\"#further-topics\">Further Topics</a>\n<ul>\n<li><a href=\"#approximate-bayesian-computation-abc-and-likelihood-free-methods\">Approximate Bayesian computation (ABC) and likelihood-free methods</a></li>\n<li><a href=\"#expectation-propagation\">Expectation propagation</a></li>\n<li><a href=\"#operator-variational-inference-opvi\">Operator variational inference (OPVI)</a></li>\n</ul>\n</li>\n</ul>\n</nav>\n</div>\n<h2 id=\"markov-chain-monte-carlo\">Markov-Chain Monte Carlo</h2>\n<h3 id=\"for-the-uninitiated\">For the uninitiated</h3>\n<ol>\n<li><a href=\"https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/\">MCMC Sampling for\nDummies</a> by Thomas\nWiecki. A basic introduction to MCMC with accompanying Python snippets. The\nMetropolis sampler is used an introduction to sampling.</li>\n<li><a href=\"http://www.mcmchandbook.net/HandbookChapter1.pdf\">Introduction to Markov Chain Monte\nCarlo</a> by Charles Geyer.\nThe first chapter of the aptly-named <a href=\"http://www.mcmchandbook.net/\"><em>Handbook of Markov Chain Monte\nCarlo</em></a>.</li>\n<li><a href=\"https://arxiv.org/abs/2001.06249\">Markov Chain Monte Carlo Methods, a survey with some frequent\nmisunderstandings</a> is an instructive\ncollection of Cross-Validated questions that clear up common\nmisunderstandings of MCMC.</li>\n</ol>\n<h3 id=\"hamiltonian-monte-carlo-and-the-no-u-turn-sampler\">Hamiltonian Monte Carlo and the No-U-Turn Sampler</h3>\n<ol>\n<li><a href=\"https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html\">Hamiltonian Monte Carlo\nexplained</a>.\nA visual and intuitive explanation of HMC: great for starters.</li>\n<li><a href=\"https://arxiv.org/abs/1701.02434\">A Conceptual Introduction to Hamiltonian Monte\nCarlo</a> by Michael Betancourt. An excellent\npaper for a solid conceptual understanding and principled intuition for HMC.</li>\n<li><a href=\"https://colindcarroll.com/2019/04/06/exercises-in-automatic-differentiation-using-autograd-and-jax/\">Exercises in Automatic Differentiation using <code>autograd</code> and\n<code>jax</code></a>\nby Colin Carroll. This is the first in a series of blog posts that explain\nHMC from the very beginning. See also <a href=\"https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/\">Hamiltonian Monte Carlo from\nScratch</a>,\n<a href=\"https://colindcarroll.com/2019/04/21/step-size-adaptation-in-hamiltonian-monte-carlo/\">Step Size Adaptation in Hamiltonian Monte\nCarlo</a>,\nand <a href=\"https://colindcarroll.com/2019/04/28/choice-of-symplectic-integrator-in-hamiltonian-monte-carlo/\">Choice of Symplectic Integrator in Hamiltonian Monte\nCarlo</a>.</li>\n<li><a href=\"https://arxiv.org/abs/1111.4246\">The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte\nCarlo</a> by Matthew Hoffman and Andrew Gelman.\nThe original NUTS paper.</li>\n<li><a href=\"http://www.mcmchandbook.net/HandbookChapter5.pdf\">MCMC Using Hamiltonian\nDynamics</a> by Radford Neal.</li>\n<li><a href=\"https://colindcarroll.com/talk/hamiltonian-monte-carlo/\">Hamiltonian Monte Carlo in\nPyMC3</a> by Colin\nCarroll.</li>\n</ol>\n<h3 id=\"sequential-monte-carlo-and-other-sampling-methods\">Sequential Monte Carlo and other sampling methods</h3>\n<ol>\n<li>Chapter 11 (Sampling Methods) of <a href=\"https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\">Pattern Recognition and Machine\nLearning</a>\nby Christopher Bishop. Covers rejection, importance, Metropolis-Hastings,\nGibbs and slice sampling. Perhaps not as rampantly useful as NUTS, but good\nto know nevertheless.</li>\n<li><a href=\"https://chi-feng.github.io/mcmc-demo/\">The Markov-chain Monte Carlo Interactive\nGallery</a> by Chi Feng. A fantastic\nlibrary of visualizations of various MCMC samplers.</li>\n<li>For non-Markov chain based Monte Carlo methods, there is <a href=\"https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf\">An Introdution to\nSequential Monte Carlo\nMethods</a>\nby Arnaud Doucet, Nando de Freitas and Neil Gordon. This chapter from <a href=\"https://www.springer.com/us/book/9780387951461\">the\nauthors&rsquo; textbook on SMC</a>\nprovides motivation for using SMC methods, and gives a brief introduction to\na basic particle filter.</li>\n<li><a href=\"http://www.stats.ox.ac.uk/~doucet/smc_resources.html\">Sequential Monte Carlo Methods &amp; Particle Filters\nResources</a> by Arnaud\nDoucet. A list of resources on SMC and particle filters: way more than you\nprobably ever need to know about them.</li>\n</ol>\n<h2 id=\"variational-inference\">Variational Inference</h2>\n<h3 id=\"for-the-uninitiated-1\">For the uninitiated</h3>\n<ol>\n<li><a href=\"http://willwolf.io/2018/11/11/em-for-lda/\">Deriving\nExpectation-Maximization</a> by Will\nWolf. The first blog post in a series that builds from EM all the way to VI.\nAlso check out <a href=\"http://willwolf.io/2018/11/23/mean-field-variational-bayes/\">Deriving Mean-Field Variational\nBayes</a>.</li>\n<li><a href=\"https://arxiv.org/abs/1601.00670\">Variational Inference: A Review for\nStatisticians</a> by David Blei, Alp\nKucukelbir and Jon McAuliffe. An high-level overview of variational\ninference: the authors go over one example (performing VI on GMMs) in depth.</li>\n<li>Chapter 10 (Approximate Inference) of <a href=\"https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\">Pattern Recognition and Machine\nLearning</a>\nby Christopher Bishop.</li>\n</ol>\n<h3 id=\"automatic-differentiation-variational-inference-advi\">Automatic differentiation variational inference (ADVI)</h3>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1603.00788\">Automatic Differentiation Variational\nInference</a> by Alp Kucukelbir, Dustin Tran\net al. The original ADVI paper.</li>\n<li><a href=\"https://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan\">Automatic Variational Inference in\nStan</a>\nby Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman and David Blei.</li>\n</ol>\n<h2 id=\"open-source-software-for-bayesian-inference\">Open-Source Software for Bayesian Inference</h2>\n<p>There are many open-source software libraries for Bayesian modelling and\ninference, and it is instructive to look into the inference methods that they do\n(or do not!) implement.</p>\n<ol>\n<li><a href=\"http://mc-stan.org/\">Stan</a></li>\n<li><a href=\"http://docs.pymc.io/\">PyMC3</a></li>\n<li><a href=\"http://pyro.ai/\">Pyro</a></li>\n<li><a href=\"https://www.tensorflow.org/probability/\">Tensorflow Probability</a></li>\n<li><a href=\"http://edwardlib.org/\">Edward</a></li>\n<li><a href=\"https://greta-stats.org/\">Greta</a></li>\n<li><a href=\"https://dotnet.github.io/infer/\">Infer.NET</a></li>\n<li><a href=\"https://www.mrc-bsu.cam.ac.uk/software/bugs/\">BUGS</a></li>\n<li><a href=\"http://mcmc-jags.sourceforge.net/\">JAGS</a></li>\n</ol>\n<h2 id=\"further-topics\">Further Topics</h2>\n<p>Bayesian inference doesn&rsquo;t stop at MCMC and VI: there is bleeding-edge research\nbeing done on other methods of inference. While they aren&rsquo;t ready for real-world\nuse, it is interesting to see what they are.</p>\n<h3 id=\"approximate-bayesian-computation-abc-and-likelihood-free-methods\">Approximate Bayesian computation (ABC) and likelihood-free methods</h3>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1001.2058\">Likelihood-free Monte Carlo</a> by Scott\nSisson and Yanan Fan.</li>\n</ol>\n<h3 id=\"expectation-propagation\">Expectation propagation</h3>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1412.4869\">Expectation propagation as a way of life: A framework for Bayesian inference\non partitioned data</a> by Aki Vehtari, Andrew\nGelman, et al.</li>\n</ol>\n<h3 id=\"operator-variational-inference-opvi\">Operator variational inference (OPVI)</h3>\n<ol>\n<li><a href=\"https://arxiv.org/abs/1610.09033\">Operator Variational Inference</a> by Rajesh\nRanganath, Jaan Altosaar, Dustin Tran and David Blei. The original OPVI\npaper.</li>\n</ol>\n<p>(I&rsquo;ve tried to include as many relevant and helpful resources as I could find,\nbut if you feel like I&rsquo;ve missed something, <a href=\"https://twitter.com/@_eigenfoo\">drop me a\nline</a>!)</p>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p><a href=\"https://twitter.com/year_progress/status/1079889949871300608\">Relevant tweet\nhere.</a>&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>If that’s what you’re looking for, check out my <a href=\"https://www.georgeho.org/bayesian-modelling-cookbook\">Bayesian modelling\ncookbook</a> or <a href=\"https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\">Michael\nBetancourt’s excellent essay on a principles Bayesian\nworkflow</a>.&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}