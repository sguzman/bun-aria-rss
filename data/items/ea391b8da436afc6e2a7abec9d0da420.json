{
  "title": "Prototyping Long Term Time Series Storage with Kafka and Parquet",
  "description": "<p><em>Another attempt to find better storage for time series data, this time it looks quite promising</em></p>\n\n<p>Last time I tried to switch from <a href=\"http://graphite.readthedocs.org/en/latest/\">Graphite</a> time series storage to <a href=\"https://mabrek.github.io/blog/spark-cassandra-timeseries/\">Cyanite/Cassandra</a> but the attempt failed and I stayed with <a href=\"http://graphite.readthedocs.org/en/latest/whisper.html\">Whisper</a> files. After struggling with keeping disk IOPS sane while ingesting hi-resolution performance data I ended up putting <a href=\"http://graphite.readthedocs.org/en/latest/whisper.html\">Whisper</a> files into <a href=\"https://en.wikipedia.org/wiki/Tmpfs\">tmpfs</a> and shortening data retention interval to just one day because my load tests usually don’t last more than several hours. Then I export data into <a href=\"http://r-project.org/\">R</a> and do analysis. Large scale projects like <a href=\"http://www.vldb.org/pvldb/vol8/p1816-teller.pdf\">Gorilla(pdf)</a> and <a href=\"http://techblog.netflix.com/2014/12/introducing-atlas-netflixs-primary.html\">Atlas</a> do similar things. They store recent data in RAM for dashboards and real-time analytics and then dump it to slow long term storage for offline analysis.</p>\n\n<p><a href=\"http://graphite.readthedocs.org/en/latest/whisper.html#database-format\">Whisper file format</a> is relatively good in terms of storage size (12 bytes per datapoint). It’s columnar because it saves each metric in its own file. It contains redundant data because it saves a timestamp with each value and many values from different metrics share the same timestamp. There is no compression. I need to find something better than Whisper.</p>\n\n<p><a href=\"http://www.vldb.org/pvldb/vol8/p1816-teller.pdf\">Gorilla(pdf)</a> paper inspired me to look into column storage formats with efficient encoding of repeated data. I decided to try <a href=\"https://parquet.apache.org/\">Parquet</a>. Unfortunately floating point compression is <a href=\"https://github.com/Parquet/parquet-mr/issues/306\">not there yet</a> but my values have type <code class=\"language-plaintext highlighter-rouge\">double</code> (as a side note <a href=\"http://orc.apache.org/\">ORCFile</a> <a href=\"https://issues.apache.org/jira/browse/ORC-15\">lacks it</a> too). Many metrics (counters) actually have integer type but there is no information about their types upfront.</p>\n\n<p>To achieve good compression data needs to be written in large chunks so I needed something to buffer data. The way <a href=\"http://kafka.apache.org/\">Kafka</a> works with streaming writes and read offsets made me think that it’s a good fit for storing data until it’s picked up by periodical job. That job would start, read all the data available from the last read offset, compress and store it, and sleep until the next cycle.</p>\n\n<p>My data comes in <a href=\"http://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol\">Graphite plaintext protocol</a> which is quite verbose:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>&lt;metric_name&gt; &lt;value&gt; &lt;timestamp&gt;\n</code></pre></div></div>\n\n<p>Metric names and timestamps are repeating. Metrics are sent periodically at the same time so many lines share the same timestamp. The same set of metrics is being sent each time. Values for many metrics are not changing a lot over time (like disk usage) which makes it a good target for <a href=\"https://en.wikipedia.org/wiki/Dictionary_coder\">dictionary</a> or <a href=\"https://en.wikipedia.org/wiki/Delta_encoding\">delta encoding</a>.</p>\n\n<h3 id=\"feeding-graphite-data-into-kafka\">Feeding graphite data into Kafka</h3>\n\n<p>I set up single node Kafka as described in <a href=\"http://kafka.apache.org/documentation.html#quickstart\">manual</a></p>\n\n<p>Feeding graphite data into Kafka turned out to be one-liner with <a href=\"http://netcat.sourceforge.net/\">nc</a> and <a href=\"https://github.com/edenhill/kafkacat\">kafkacat</a>:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>nc -4l localhost 2003 | kafkacat -P -b localhost -t metrics -K ' '\n</code></pre></div></div>\n\n<p>Metric name is used as message key in kafka and <code class=\"language-plaintext highlighter-rouge\">value timestamp</code> is a payload.</p>\n\n<p>Then I started <a href=\"https://collectd.org/\">collectd</a> with <a href=\"https://collectd.org/wiki/index.php/Plugin:Write_Graphite\">write graphite plugin</a> pointing to localhost and reporting interval of 1 second. After several hours I got 1.8 Gb queue in kafka.</p>\n\n<p>Dumping the data back into text format is a one-liner too:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kafka-console-consumer.sh --zookeeper localhost:2181 --topic metrics \\\n  --from-beginning --property print.key=true\n</code></pre></div></div>\n\n<p>Text file had size of 1.4Gb which means kafka has some overhead for storing uncompressed data. There were ~19000000 lines in the file.</p>\n\n<h3 id=\"fetching-data-from-kafka\">Fetching data from Kafka</h3>\n\n<p>I needed to handle read offsets manually so I chose <a href=\"http://kafka.apache.org/documentation.html#simpleconsumerapi\">SimpleConsumer</a>. Its API turned out to be quite confusing and not that simple. It doesn’t talk to Zookeeper and allows to specify offsets. Handling all corner cases requires lots of <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example\">code</a> but simple prototype turned out to be quite short in Scala:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>val consumer = new SimpleConsumer(\"localhost\", 9092, 5000,\n    BlockingChannel.UseDefaultBufferSize, name)\nval fetchRequest = new FetchRequestBuilder().clientId(name)\n    .addFetch(topic, partition, offset, fetchSize).build()\nval fetchResponse = consumer.fetch(fetchRequest)\nval messages = fetchResponse.messageSet(topic, partition)\n</code></pre></div></div>\n\n<p>It just reads all messages from specified <code class=\"language-plaintext highlighter-rouge\">offset</code> up to <code class=\"language-plaintext highlighter-rouge\">fetchSize</code>.</p>\n\n<h3 id=\"saving-data-into-parquet\">Saving data into Parquet</h3>\n\n<p><a href=\"https://parquet.apache.org/\">Parquet</a> API documentation doesn’t seem to be published anywhere. Javadoc for <a href=\"https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/schema/Types.java#L30\">org.apache.parquet.schema.Types</a> contains several schema examples. Writing local files from standalone application is not described anywhere but <code class=\"language-plaintext highlighter-rouge\">parquet-benchmarks</code> module contains class <a href=\"https://github.com/apache/parquet-mr/blob/master/parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/DataGenerator.java#L68\">org.apache.parquet.benchmarks.DataGenerator</a> which writes several variants of local files. It depends on Hadoop classes so you’ll need it as a project dependency (<a href=\"https://github.com/Parquet/parquet-mr/issues/305\">issue</a>)</p>\n\n<p>I decided to use ‘wide’ schema when each metric has its own column to make delta encoding possible and a single column for timestamps (like <a href=\"https://cran.r-project.org/web/packages/xts/index.html\">xts</a> does for multivariate series):</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>val types = mutable.Set[Type]()\n...\n// for each message collect unique keys as types\ntypes += Types.optional(DOUBLE).named(key)\n...\nval schema = new MessageType(\"GraphiteLine\",\n    (types + Types.required(INT64).named(\"timestamp\")).toList) \n</code></pre></div></div>\n\n<p>Boilerplate to create <a href=\"https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\">ParquetWriter</a> object:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>val configuration = new Configuration\nGroupWriteSupport.setSchema(schema, configuration)\nval gf = new SimpleGroupFactory(schema)\nval outFile = new Path(\"data-file.parquet\")\nval writer = new ParquetWriter[Group](outFile, \n    new GroupWriteSupport, UNCOMPRESSED, DEFAULT_BLOCK_SIZE, \n    DEFAULT_PAGE_SIZE, 512, true, false, PARQUET_2_0, \n    configuration)\n</code></pre></div></div>\n\n<p>For each unique timestamp a row (called group in Parquet) is added which contains values for all metrics (columns) at that time:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>for (timestamp &lt;- timestamps) {\n    val group = gf.newGroup().append(\"timestamp\", timestamp)\n        for ((metric, column) &lt;- columns) {\n            column.get(timestamp).foreach(group.append(metric, _))\n        }\n    writer.write(group)\n}\nwriter.close()\n</code></pre></div></div>\n\n<p>And that’s it.</p>\n\n<h3 id=\"effect-of-compression\">Effect of compression</h3>\n\n<p>Enabling gzip compression in Parquet reduced file size 3 times compared to uncompressed. The result took 12Mb for ~19000000 input lines which is quite impressive. Storing the same data in whisper format would take at least 230Mb (actually more because it reserves space for whole retention interval).</p>\n\n<p>I tried enabling Snappy compression for Kafka publisher:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kafkacat -P -b localhost -t metricz -K ' ' -z snappy &lt; metrics.txt\n</code></pre></div></div>\n\n<p>and got ~ 500Mb queue size for 1.4Gb original data. The only problem is that log compaction is not yet compatible with compressed topics.</p>\n\n<p>The result looks quite good: temporal buffer in Kafka needs 1/3 size of original data  and long term storage takes ~ 0.6 bytes per datapoint (while whisper takes 12 bytes).</p>\n\n<h3 id=\"open-questions\">Open questions</h3>\n\n<p>Can Parquet handle very wide schema with 100k columns and more?</p>\n\n<p>How to guess effective metric type (double or integer) from incoming graphite data?</p>\n\n<p>How to get data from Parquet into R without Spark? (<a href=\"https://github.com/Parquet/parquet-format/issues/72\">issue</a>)</p>\n\n<h3 id=\"source-code\">Source code</h3>\n\n<p>Available on github <a href=\"https://github.com/mabrek/kafka-timeseries\">kafka-timeseries</a> with build instructions. Actually it’s just a <a href=\"https://github.com/mabrek/kafka-timeseries/blob/master/src/main/scala/KafkaTimeseries.scala\">single Scala file</a> with 100 lines of code 20 of which are import statements.</p>",
  "pubDate": "Sun, 25 Oct 2015 00:00:00 +0000",
  "link": "https://mabrek.github.io/blog/kafka-parquet-timeseries/",
  "guid": "https://mabrek.github.io/blog/kafka-parquet-timeseries/"
}