{
  "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. (arXiv:2211.02001v1 [cs.LG])",
  "link": "http://arxiv.org/abs/2211.02001",
  "description": "<p>Progress in machine learning (ML) comes with a cost to the environment, given\nthat training ML models requires significant computational resources, energy\nand materials. In the present article, we aim to quantify the carbon footprint\nof BLOOM, a 176-billion parameter language model, across its life cycle. We\nestimate that BLOOM's final training emitted approximately 24.7 tonnes\nof~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes\nif we account for all processes ranging from equipment manufacturing to\nenergy-based operational consumption. We also study the energy requirements and\ncarbon emissions of its deployment for inference via an API endpoint receiving\nuser queries in real-time. We conclude with a discussion regarding the\ndifficulty of precisely estimating the carbon footprint of ML models and future\nresearch directions that can contribute towards improving carbon emissions\nreporting.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viguier_S/0/1/0/all/0/1\">Sylvain Viguier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ligozat_A/0/1/0/all/0/1\">Anne-Laure Ligozat</a>"
}