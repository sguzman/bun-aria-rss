{
  "title": "CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking. (arXiv:2112.08766v3 [cs.IR] UPDATED)",
  "link": "http://arxiv.org/abs/2112.08766",
  "description": "<p>Contrastive learning has been the dominant approach to training dense\nretrieval models. In this work, we investigate the impact of ranking context -\nan often overlooked aspect of learning dense retrieval models. In particular,\nwe examine the effect of its constituent parts: jointly scoring a large number\nof negatives per query, using retrieved (query-specific) instead of random\nnegatives, and a fully list-wise loss. To incorporate these factors into\ntraining, we introduce Contextual Document Embedding Reranking (CODER), a\nhighly efficient retrieval framework. When reranking, it incurs only a\nnegligible computational overhead on top of a first-stage method at run time\n(delay per query in the order of milliseconds), allowing it to be easily\ncombined with any state-of-the-art dual encoder method. After fine-tuning\nthrough CODER, which is a lightweight and fast process, models can also be used\nas stand-alone retrievers. Evaluating CODER in a large set of experiments on\nthe MS~MARCO and TripClick collections, we show that the contextual reranking\nof precomputed document embeddings leads to a significant improvement in\nretrieval performance. This improvement becomes even more pronounced when more\nrelevance information per query is available, shown in the TripClick\ncollection, where we establish new state-of-the-art results by a large margin.\n</p>",
  "dc:creator": "<a href=\"http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1\">George Zerveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1\">Daniel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"
}