{
  "title": "Hyperparameter sample-size dependence",
  "link": "",
  "published": "2016-08-21T00:00:00-07:00",
  "updated": "2016-08-21T00:00:00-07:00",
  "author": {
    "name": "Jonathan Landy"
  },
  "id": "tag:efavdb.com,2016-08-21:/model-selection",
  "summary": "<p>Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, <span class=\"math\">\\(N.\\)</span> To illustrate this point, we derive expressions for the optimal strength for both <span class=\"math\">\\(L_1\\)</span> and <span class=\"math\">\\(L_2\\)</span> regularization in single-variable models. We find that …</p>",
  "content": "<p>Here, we briefly review a subtlety associated with machine-learning model selection: the fact that the optimal hyperparameters for a model can vary with training set size, <span class=\"math\">\\(N.\\)</span> To illustrate this point, we derive expressions for the optimal strength for both <span class=\"math\">\\(L_1\\)</span> and <span class=\"math\">\\(L_2\\)</span> regularization in single-variable models. We find that the optimal <span class=\"math\">\\(L_2\\)</span> approaches a finite constant as <span class=\"math\">\\(N\\)</span> increases, but that the optimal <span class=\"math\">\\(L_1\\)</span> decays exponentially fast with <span class=\"math\">\\(N.\\)</span> Sensitive dependence on <span class=\"math\">\\(N\\)</span> such as this should be carefully extrapolated out when optimizing mission-critical&nbsp;models.</p>\n<h3>Introduction</h3>\n<p>There are two steps one must carry out to fit a machine-learning model. First, a specific model form and cost function must be selected, and second the model must be fit to the data. The first of these steps is often treated by making use of a training-test data split: One trains a set of candidate models to a fraction of the available data and then validates their performance using a hold-out, test set. The model that performs best on the latter is then selected for&nbsp;production.</p>\n<p>Our purpose here is to highlight a subtlety to watch out for when carrying out an optimization as above: the fact that the optimal model can depend sensitively on training set size <span class=\"math\">\\(N\\)</span>. This observation suggests that the training-test split paradigm must sometimes be applied with care: Because a subsample is used for training in the first, selection step, the model identified as optimal there may not be best when training on the full data&nbsp;set.</p>\n<p>To illustrate the above points, our main effort here is to present some toy examples where the optimal hyperparameters can be characterized exactly: We derive the optimal <span class=\"math\">\\(L_1\\)</span> and <span class=\"math\">\\(L_2\\)</span> regularization strength for models having only a single variable. These examples illustrate two opposite limits: The latter approaches a finite constant as <span class=\"math\">\\(N\\)</span> increases, but the former varies exponentially with <span class=\"math\">\\(N\\)</span>. This shows that strong <span class=\"math\">\\(N\\)</span>-dependence can sometimes occur, but is not necessarily always an issue. In practice, a simple way to check for sensitivity is to vary the size of your training set during model selection: If a strong dependence is observed, care should be taken during the final&nbsp;extrapolation.</p>\n<p>We now walk through our two&nbsp;examples.</p>\n<h3><span class=\"math\">\\(L_2\\)</span>&nbsp;optimization</h3>\n<p>We start off by positing that we have a method for generating a Bayesian posterior for a parameter <span class=\"math\">\\(\\theta\\)</span> that is a function of a vector of <span class=\"math\">\\(N\\)</span> random samples <span class=\"math\">\\(\\textbf{x}\\)</span>. To simplify our discussion, we assume that &#8212; given a flat prior &#8212; this is unbiased and normal with variance <span class=\"math\">\\(\\sigma^2\\)</span>. We write <span class=\"math\">\\(\\theta_0 \\equiv \\theta_0(\\textbf{x})\\)</span> for the maximum a posteriori (<span class=\"caps\">MAP</span>) value under the flat prior. With the introduction of an <span class=\"math\">\\(L_2\\)</span> prior, the posterior for <span class=\"math\">\\(\\theta\\)</span> is&nbsp;then\n</p>\n<div class=\"math\">$$\\tag{1}\nP\\left(\\theta \\vert \\theta_0(\\textbf{x})\\right) \\propto \\exp\\left( - \\frac{(\\theta - \\theta_0)^2}{2 \\sigma^2} - \\Lambda \\theta^2 \\right).\n$$</div>\n<p>\nSetting the derivative of the above to zero, the point-estimate, <span class=\"caps\">MAP</span> is given&nbsp;by\n</p>\n<div class=\"math\">$$\\tag{2}\n\\hat{\\theta} = \\frac{\\theta_0}{1 + 2 \\Lambda \\sigma^2}.\n$$</div>\n<p>\nThe average squared error of this estimate is obtained by averaging over the possible <span class=\"math\">\\(\\theta_0\\)</span> values. Our assumptions above imply that <span class=\"math\">\\(\\theta_0\\)</span> is normal about the true parameter value, <span class=\"math\">\\(\\theta_*\\)</span>, so we&nbsp;have\n</p>\n<div class=\"math\">\\begin{eqnarray}\n\\langle (\\hat{\\theta} - \\theta_*)^2 \\rangle &amp;\\equiv&amp; \\int_{\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\ne^{ - \\frac{(\\theta_0 - \\theta_*)^2}{2 \\sigma^2}} \\left ( \\frac{\\theta_0}{1 + 2 \\Lambda \\sigma^2} - \\theta_* \\right)^2 d \\theta_0 \\\\\n&amp;=&amp; \\frac{ 4 \\Lambda^2 \\sigma^4 \\theta_*^2 }{(1 + 2 \\Lambda \\sigma^2 )^2} + \\frac{\\sigma^2}{\\left( 1 + 2 \\Lambda \\sigma^2 \\right)^2}. \\tag{3} \\label{error}\n\\end{eqnarray}</div>\n<p>\nThe optimal <span class=\"math\">\\(\\Lambda\\)</span> is readily obtained by minimizing this average error. This&nbsp;gives,\n</p>\n<div class=\"math\">$$ \\label{result}\n\\Lambda = \\frac{1}{2 \\theta_*^2}, \\tag{4}\n$$</div>\n<p>\na constant, independent of sample size. The mean squared error with this choice is obtained by plugging (\\ref{result}) into (\\ref{error}). This&nbsp;gives\n</p>\n<div class=\"math\">$$\n\\langle (\\hat{\\theta} - \\theta_*)^2 \\rangle = \\frac{\\sigma^2}{1 + \\sigma^2 / \\theta_*^2}. \\tag{5}\n$$</div>\n<p>\nNotice that this is strictly less than <span class=\"math\">\\(\\sigma^2\\)</span> &#8212; the variance one would get without regularization &#8212; and that the benefit is largest when <span class=\"math\">\\(\\sigma^2 \\gg \\theta_*^2\\)</span>. That is, <span class=\"math\">\\(L_2\\)</span> regularization is most effective when <span class=\"math\">\\(\\theta_*\\)</span> is hard to differentiate from zero &#8212; an intuitive&nbsp;result!</p>\n<h3><span class=\"math\">\\(L_1\\)</span>&nbsp;optimization</h3>\n<p>The analysis for <span class=\"math\">\\(L_1\\)</span> optimization is similar to the above, but slightly more involved. We go through it quickly. The posterior with an <span class=\"math\">\\(L_1\\)</span> prior is given&nbsp;by\n</p>\n<div class=\"math\">$$ \\tag{6}\nP\\left(\\theta \\vert \\theta_0(\\textbf{x})\\right) \\propto \\exp\\left( - \\frac{(\\theta - \\theta_0)^2}{2 \\sigma^2} - \\Lambda \\vert \\theta \\vert \\right).\n$$</div>\n<p>\nAssuming for simplicity that <span class=\"math\">\\(\\hat{\\theta} &gt; 0\\)</span>, the <span class=\"caps\">MAP</span> value is&nbsp;now\n</p>\n<div class=\"math\">$$ \\tag{7}\n\\hat{\\theta} = \\begin{cases}\n\\theta_0 - \\Lambda \\sigma^2 &amp; \\text{if } \\theta_0 - \\Lambda \\sigma^2 &gt; 0 \\\\\n0 &amp; \\text{else}.\n\\end{cases}\n$$</div>\n<p>\nThe mean squared error of the estimator&nbsp;is\n</p>\n<div class=\"math\">$$ \\tag{8}\n\\langle (\\hat{\\theta} - \\theta_*)^2 \\rangle \\equiv \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\ne^{ - \\frac{(\\theta_0 - \\theta_*)^2}{2 \\sigma^2}} \\left ( \\hat{\\theta} - \\theta_* \\right)^2 d \\theta_0.\n$$</div>\n<p>\nThis can be evaluated in terms of error functions. The optimal value of <span class=\"math\">\\(\\Lambda\\)</span> is obtained by differentiating the above. Doing this, one finds that it satisfies the&nbsp;equation\n</p>\n<div class=\"math\">$$ \\tag{9}\ne^{ - \\frac{(\\tilde{\\Lambda}- \\tilde{\\theta_*})^2}{2} } + \\sqrt{\\frac{\\pi}{2}} \\tilde{\\Lambda} \\ \\text{erfc}\\left( \\frac{\\tilde{\\Lambda} - \\tilde{\\theta_*}}{\\sqrt{2}} \\right ) = 0,\n$$</div>\n<p>\nwhere <span class=\"math\">\\(\\tilde{\\Lambda} \\equiv \\sigma \\Lambda\\)</span> and <span class=\"math\">\\(\\tilde{\\theta_*} \\equiv \\theta_* / \\sigma\\)</span>. In general, the equation above must be solved numerically. However, in the case where <span class=\"math\">\\(\\theta_* \\gg \\sigma\\)</span> &#8212; relevant when <span class=\"math\">\\(N\\)</span> is large &#8212; we can obtain a clean asymptotic solution. In this case, we have <span class=\"math\">\\(\\tilde{\\theta_*} \\gg 1\\)</span> and we expect <span class=\"math\">\\(\\Lambda\\)</span> small. This implies that the above equation can be approximated&nbsp;as\n</p>\n<div class=\"math\">$$ \\tag{10}\ne^{ - \\frac{\\tilde{\\theta_*}^2}{2} } - \\sqrt{2 \\pi} \\tilde{\\Lambda} \\sim 0.\n$$</div>\n<p>\nSolving&nbsp;gives\n</p>\n<div class=\"math\">\\begin{eqnarray} \\tag{11}\n\\Lambda \\sim \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{ - \\frac{\\theta_*^2}{2 \\sigma^2}} \\sim \\frac{\\sqrt{N}}{\\sqrt{2 \\pi \\sigma_1^2}} e^{ - \\frac{N \\theta_*^2}{2 \\sigma_1^2}}.\n\\end{eqnarray}</div>\n<p>\nHere, in the last line we have made the <span class=\"math\">\\(N\\)</span>-dependence explicit, writing <span class=\"math\">\\(\\sigma^2 = \\sigma_1^2 / N\\)</span> &#8212; a form that follows when our samples <span class=\"math\">\\(\\textbf{x}\\)</span> are independent. Whereas the optimal <span class=\"math\">\\(L_2\\)</span> regularization strength approaches a constant, our result here shows that the optimal <span class=\"math\">\\(L_1\\)</span> strength decays exponentially to zero as <span class=\"math\">\\(N\\)</span>&nbsp;increases.</p>\n<h3>Summary</h3>\n<p>The subtlety that we have discussed here is likely already familiar to those with significant applied modeling experience: optimal model hyperparameters can vary with training set size. However, the two toy examples we have presented are interesting in that they allow for this <span class=\"math\">\\(N\\)</span> dependence to be derived explicitly. Interestingly, we have found that the <span class=\"caps\">MSE</span>-minimizing <span class=\"math\">\\(L_2\\)</span> regularization remains finite, even at large training set size, but the optimal <span class=\"math\">\\(L_1\\)</span> regularization goes to zero in this same limit. For small and medium <span class=\"math\">\\(N\\)</span>, this exponential dependence represents a strong sensitivity to <span class=\"math\">\\(N\\)</span> &#8212; one that must be carefully taken into account when extrapolating to the full training&nbsp;set.</p>\n<p>One can imagine many other situations where hyperparameters vary strongly with <span class=\"math\">\\(N\\)</span>. For example, very complex systems may allow for ever-increasing model complexity as more data becomes available. Again, in practice, the most straightforward method to check for this is to vary the size of the training set during model selection. If strong dependence is observed, this should be extrapolated out to obtain the truly optimal model for&nbsp;production.</p>\n<script type=\"text/javascript\">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\n    var align = \"center\",\n        indent = \"0em\",\n        linebreak = \"false\";\n\n    if (false) {\n        align = (screen.width < 768) ? \"left\" : align;\n        indent = (screen.width < 768) ? \"0em\" : indent;\n        linebreak = (screen.width < 768) ? 'true' : linebreak;\n    }\n\n    var mathjaxscript = document.createElement('script');\n    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\n    mathjaxscript.type = 'text/javascript';\n    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';\n\n    var configscript = document.createElement('script');\n    configscript.type = 'text/x-mathjax-config';\n    configscript[(window.opera ? \"innerHTML\" : \"text\")] =\n        \"MathJax.Hub.Config({\" +\n        \"    config: ['MMLorHTML.js'],\" +\n        \"    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" +\n        \"    jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" +\n        \"    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" +\n        \"    displayAlign: '\"+ align +\"',\" +\n        \"    displayIndent: '\"+ indent +\"',\" +\n        \"    showMathMenu: true,\" +\n        \"    messageStyle: 'normal',\" +\n        \"    tex2jax: { \" +\n        \"        inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" +\n        \"        displayMath: [ ['$$','$$'] ],\" +\n        \"        processEscapes: true,\" +\n        \"        preview: 'TeX',\" +\n        \"    }, \" +\n        \"    'HTML-CSS': { \" +\n        \"        availableFonts: ['STIX', 'TeX'],\" +\n        \"        preferredFont: 'STIX',\" +\n        \"        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" +\n        \"        linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" +\n        \"    }, \" +\n        \"}); \" +\n        \"if ('default' !== 'default') {\" +\n            \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n            \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" +\n                \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" +\n                \"VARIANT['normal'].fonts.unshift('MathJax_default');\" +\n                \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" +\n                \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" +\n                \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" +\n            \"});\" +\n        \"}\";\n\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);\n    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\n}\n</script>",
  "category": ""
}