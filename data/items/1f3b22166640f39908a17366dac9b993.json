{
  "id": "tag:blogger.com,1999:blog-19803222.post-3373191565627655987",
  "published": "2017-08-15T07:50:00.000-06:00",
  "updated": "2017-08-15T09:47:13.154-06:00",
  "title": "Column squishing for multiclass updates",
  "content": "Score-based multiclass classifiers typically have the following form: <i>x</i> is a d-dimensional input vector (perhaps engineered features, perhaps learned features), <i>A</i> is a d*k matrix, where k is the number of classes, and the prediction is given by computing a score vector <i>y=Ax.</i> (This is a blog post, not an arxiv paper, so I'm going to be a bit fast a loose with dimension ordering.) The predicted class is then taken as <i>argmax<sub>i</sub> y<sub>i</sub></i>.<br /><br />(Advanced thanks to my former Ph.D. student <a href=\"http://inductivebias.ml/\">Abhishek Kumar</a> for help with this!) <br /><br />The question I wanted to answer, which I felt <i>must</i> have a known answer though I'd never seen it, is the following. Suppose (e.g., at training time), I know that the correct label is <i>i</i>, but the model is (perhaps) not predicting <i>i</i>. I'd like to change <i>A</i> as little as possible so that it predicts <i>i</i>, perhaps with an added margin of 1. If I measure \"as little as possible\" by l2 norm, and assume wlog that <i>i=1</i>, then I get:<br /><br /><i>&nbsp; min<sub>B</sub> ||B-A||<sup>2</sup> st (xB)<sub>1</sub>≥(xB)<sub>i</sub>+1 for all i ≥ 2</i><br /><br />This problem arises most specifically in Crammer et al., 2006, \"<a href=\"http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf\">Online Passive-Aggressive Algorithms</a>\", though appears elsewhere too.<br /><br />I'll show below (including python code using just numpy), a very efficient solution to this problem. (If this appears elsewhere and I simply was unable to find it, please let me know.)<br /><br />First, I'll make the following unproven assertion, though I'm pretty sure it'll go through (famous last words). The assertion is that any difference between <i>A</i> and <i>B</i> will be in the direction of <i>x</i>. In other words, the first row of <i>A</i> will likely move in the direction of <i>x</i> and the other rows of <i>A</i> will move away. Hand-wavy reason: because otherwise you increase the norm <i>||B-A||</i> without helping satisfy the constraints.<br /><br />In particular, I'll assume that <i>b<sub>i</sub>=a<sub>i</sub>+d<sub>i</sub>x</i>, where the dis are scalars.<br /><br />Given this, we can do a bit of algebra:<br /><br /><i>&nbsp; ||B-A||<sup>2</sup> = Σ<sub>i</sub> (b<sub>i</sub> - a<sub>i</sub>)<sup>2</sup> = Σ<sub>i</sub> (a<sub>i</sub> + d<sub>i</sub> x - a<sub>i</sub>)<sup>2</sup>= ||x||<sup>2</sup> Σ<sub>i</sub> d<sub>i</sub><sup>2</sup></i><br /><br />Since x is a constant, we really only care about minimizing the norm of the deltas.<br /><br />We can similarly rewrite the constraints to just say:<br /><br />&nbsp; <i>xb</i><i><i><sub>1</sub></i> ≥ xb</i><i><i><sub>i</sub></i> + 1 for all i ≥ 2</i><br /><i>iff&nbsp;&nbsp;&nbsp; x(a</i><i><i><i><sub>1</sub></i></i> + d</i><i><i><i><sub>1</sub></i></i>x) ≥ x(a</i><i><i><sub>i</sub></i> + d</i><i><i><sub>i</sub></i>x) + 1 for all i ≥ 2</i><br /><i>iff &nbsp;&nbsp; xa</i><i><i><i><sub>1</sub></i></i> + d</i><i><i><i><sub>1</sub></i></i> ||x||</i><i><i><sup>2</sup></i> ≥ xa</i><i><i><sub>i</sub></i> + d</i><i><i><sub>i</sub></i>||x</i><i><i>||<sup>2</sup></i> + 1 for all i ≥ 2</i><br /><i><i>iff&nbsp;&nbsp;&nbsp; </i>d</i><i><i><i><sub>1</sub></i></i> ≥ d</i><i><i><sub>i</sub></i> + C</i><i><i><sub>i</sub></i> for all i ≥ 2</i><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><br /></div>where <i>C</i><i><i><i><sub>i</sub></i></i> = [x(a</i><i><i><sub>i</sub></i>-a</i><i><i><i><sub>1</sub></i></i>)+1]/||x||</i><i><sup>2</sup></i> is independent of <i>d.</i><br /><br /><i></i>Now, we have a plausibly simpler optimization problem just over the <i>d</i> vector:<br /><br /><i>&nbsp; min<sub>d</sub> Σ<sub>i</sub> d<sub>i</sub><sup>2</sup> st d<sub>1</sub> ≥ d<sub>i</sub> + C<sub>i</sub> for all i ≥ 2</i><br /><i><br /></i>This was the place I got stuck. I felt like there would be some algorithm for solving this that involves sorting and projecting and whatever, but couldn't figure it out for a few days. I then asked current and former advisees, at which point <a href=\"http://inductivebias.ml/\">Abhishek Kumar</a> came to my rescue :). He pointed me to the paper \"<a href=\"http://pages.cs.wisc.edu/~brecht/papers/12.Bit.EtAl.HOTT.pdf\">Factoring Non-negative Matrices with Linear Programs</a>\" by Bittorf et al., 2012. It's maybe not obvious that this is all connect from the title, but they solve a very similar problem in Algorithm 5. All of the following is due to Abhishek:<br /><br />In particular, their Equation 11 has the form:<br /><br /><i>&nbsp; min<sub>x</sub> ||z-x||<sup>2</sup> st 0 ≤ x<sub>i</sub> ≤ x<sub>1</sub> for all i, x<sub>1</sub> ≤ 1</i><br /><br />My problem can be happed to this by a change of variables: <i>z=d+D</i>, where <i>D=[0, C</i><i><i><sub>2</sub></i>, C</i><i><i><sub>3</sub></i>, ..., C</i><i><i><sub>k</sub></i>]</i>. We also need to remove the lower and upper bounds. This means that their Algorithm 5 can be used to solve my problem, but with all of the [0,1] projection steps removed. For completeness, here is their algorithm:<br /><br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://4.bp.blogspot.com/-SOcpXmLZscM/WZL6FIjM7PI/AAAAAAAAAho/WGCYGspAgzARbXSNn1f4p-7PZRp1lx8aACLcBGAs/s1600/column_squishing.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"598\" data-original-width=\"1161\" height=\"205\" src=\"https://4.bp.blogspot.com/-SOcpXmLZscM/WZL6FIjM7PI/AAAAAAAAAho/WGCYGspAgzARbXSNn1f4p-7PZRp1lx8aACLcBGAs/s400/column_squishing.png\" width=\"400\" /></a></div><br /><br />Putting this all together, we arrive at some python code in <a href=\"http://hal3.name/column_squishing.py\">column_squishing.py</a> for solving my multiclass problem. Here's an example of running it:<br /><br /><br /><pre>≫ A = np.random.randn(3,5)<br />≫ x = np.random.randn(5)<br />≫ A.dot(x)<br />array([ 0.90085352,  2.25573249,  0.25974194])<br /></pre><br />So currently label \"1\" is winning by a big margin. Let's make each label win by a margin of one, one at a time: <br /><br /><pre>≫ multiclass_update(A, x, 0).dot(x)<br />array([ 2.078293  ,  1.078293  ,  0.25974194])</pre><pre>&nbsp;</pre><pre>≫ multiclass_update(A, x, 1).dot(x)&nbsp;</pre><pre>array([ 0.90085352,  2.25573249,  0.25974194])</pre><pre>&nbsp;</pre><pre>≫ multiclass_update(A, x, 2).dot(x)&nbsp;</pre><pre>array([ 0.80544265,  0.80544265,  1.80544265])<br />&nbsp;</pre><pre>&nbsp;</pre>Hopefully you find this helpful. If you end up using it, please make some sort of acknowledgement to this blog post and definitely please credit Abhishek.",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "hal",
    "uri": "http://www.blogger.com/profile/02162908373916390369",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "media:thumbnail": "",
  "thr:total": 5
}