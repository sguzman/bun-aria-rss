{
  "id": "tag:blogger.com,1999:blog-5547907074344788039.post-5633805100742730363",
  "published": "2013-01-06T14:31:00.001-08:00",
  "updated": "2013-01-06T14:51:36.468-08:00",
  "category": "",
  "title": "What's the significance of 0.05 significance?",
  "content": "<br /><div class=\"p1\">Why do we tend to use a statistical significance level of 0.05? When I teach statistics or mentor colleagues brushing up, I often get the sense that a statistical significance level of α = 0.05 is viewed as some hard and fast threshold, a publishable / not publishable step function. I've seen grad students finish up an empirical experiment and groan to find that p = 0.052. Depressed, they head for the pub. I've seen the same grad students extend their experiment just long enough for statistical variation to swing in their favor to obtain p = 0.049. Happy, they head for the pub.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">Clearly, 0.05 is not the only significance level used. 0.1, 0.01 and some smaller values are common too. This is partly related to field. In my experience, the ecological literature and other fields that are often plagued by small sample sizes are more likely to use 0.1. Engineering and manufacturing where larger samples are easier to obtain tend to use 0.01. Most people in most fields, however, use 0.05. It is indeed the default value in most statistical software applications.</div><div class=\"p2\"><br /></div><div class=\"p1\">This \"standard\" 0.05 level is typically associated with <a href=\"http://en.wikipedia.org/wiki/Ronald_Fisher\">Sir R. A. Fisher</a>, a brilliant biologist and statistician that pioneered many areas of statistics, including ANOVA and experimental design. However, the true origins make for a much richer story.</div><div class=\"p2\"><br /></div><div class=\"p1\">Let's start, however, with Fisher's contribution.&nbsp;In <i><a href=\"http://psychclassics.yorku.ca/Fisher/Methods/chap3.htm\">Statistical Methods for Research Workers</a></i> (1925),&nbsp;he states</div><blockquote class=\"tr_bq\">The value for which P=0.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation ought to be considered significant or not. Deviations exceeding twice the standard deviation are thus formally regarded as significant. Using this criterion we should be led to follow up a false indication only once in 22 trials, even if the statistics were the only guide available. Small effects will still escape notice if the data are insufficiently numerous to bring them out, but no lowering of the standard of significance would meet this difficulty.</blockquote><div class=\"p1\">The next year he states, somewhat loosely,</div><blockquote class=\"tr_bq\">... it is convenient to draw the line at about the level at which we can say: \"Either there is something in the treatment, or a coincidence has occurred such as does not occur more than once in twenty trials.\"...&nbsp;</blockquote><blockquote class=\"tr_bq\">If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 per cent point), or one in a hundred (the 1 per cent point). Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.</blockquote><div class=\"p1\">(See&nbsp;<a href=\"http://www.jerrydallal.com/LHSP/p05.htm\">http://www.jerrydallal.com/LHSP/p05.htm</a>)</div><div class=\"p1\"><br /></div><div class=\"p1\">And there you have it. With no theoretical justification, these few sentences drove the standard significance level that we use to this day.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">Fisher was not the first to think about this but he was the first to reframe it as a probability in this manner and the first to state this 0.05 value explicitly.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">Those two z-values in the first quote, however, hint at a longer history and basis of the different significance levels that we know and love.&nbsp;Cowles &amp; Davis (1982)&nbsp;<a href=\"http://www.radford.edu/~jaspelme/611/Spring-2007/Cowles-n-Davis_Am-Psyc_orignis-of-05-level.pdf\">On the Origins of the .05 level of statistical significance</a>&nbsp;describe a fascinating extended history which reads like a <i>Whos Whos</i> of statistical luminaries: De Moivre, Pearson, Gossett (Student), Laplace, Gauss and others.&nbsp;</div><div class=\"p2\"><br /></div><div class=\"p1\">Our story really begins in 1818 with Bessel who coined the term \"probable error\" (well, at least the equivalent in German). Probable error is the semi-interquartle range. That is, ±1PE contains the central 50% of values and is roughly 2/3 of a standard deviation. So, for a uniform distribution ±2PE contains all values but for a standard normal it contains only the central 82% of values. Finally, and crucially to our story,</div><div class=\"p1\"></div><ul><li>±3PE contains the central ~95% of values. 1 - 0.95 = 0.05</li><li>People like Quetelet and Galton had tended to express variation or errors outside some typical range in terms of&nbsp;±3PE, even after&nbsp;Pearson coined the term standard deviation.&nbsp;</li></ul><br /><div class=\"p1\">There you have the basis of&nbsp;0.05 significance:&nbsp;±3PE was in common use in the late 1890s and this translates to 0.05.&nbsp;1 in 20 is easier to interpret for most people than a z value of 2 or in terms of PE (<a href=\"http://www.radford.edu/~jaspelme/611/Spring-2007/Cowles-n-Davis_Am-Psyc_orignis-of-05-level.pdf\">Cowles &amp; Davis, 1982</a>) and thus explains why 0.05 became more popular.&nbsp;</div><div class=\"p1\"><br /></div><div class=\"p1\">In one paper from the 1890s, Pearson remarks on different p-values obtained as</div><div class=\"p2\"><br /></div><div class=\"p1\">p = 0.5586 --- \"thus we may consider the fit remarkably good\"</div><div class=\"p1\">p = 0.28 --- \"fairly represented\"</div><div class=\"p1\">p = 0.1 --- \"not very improbable that the observed frequencies are compatible with a random sampling\"</div><div class=\"p1\">p = 0.01 --- \"this very improbable result\"</div><div class=\"p2\"><br /></div><div class=\"p1\">and here we see the start of different significance levels. 0.1 is a little probable and 0.01 very improbable. 0.05 rests between the two.</div><div class=\"p2\"><br /></div><div class=\"p1\">Despite this, ±3PE continued to be used as the primary criterion up to the 1920s and is still used in some fields today, <a href=\"http://en.wikipedia.org/wiki/Significance_level\">especially in physics</a>. It was Fisher that rounded off the probability to 0.05 which in turn, switched from a clean ±2<span style=\"background-color: #f9f7f4; color: #666666; font-family: arial, helvetica; line-height: 21.266666412353516px;\">σ</span>&nbsp;to ±1.96<span style=\"background-color: #f9f7f4; color: #666666; font-family: arial, helvetica; line-height: 21.266666412353516px;\">σ</span>.</div><div class=\"p2\"><br /></div><div class=\"p1\">In summary,&nbsp;±3PE --&gt; ±2<span style=\"background-color: #f9f7f4; color: #666666; font-family: arial, helvetica; line-height: 21.266666412353516px;\">σ</span>&nbsp;--&gt; ±1.96<span style=\"background-color: #f9f7f4; color: #666666; font-family: arial, helvetica; line-height: 21.266666412353516px;\">σ</span>&nbsp;--&gt; α = 0.05 more accurately describes the evolution of statistical significance.</div>",
  "link": [
    "",
    "",
    "",
    "",
    ""
  ],
  "author": {
    "name": "Carl Anderson",
    "uri": "http://www.blogger.com/profile/11930448254473684406",
    "email": "noreply@blogger.com",
    "gd:image": ""
  },
  "thr:total": 14
}