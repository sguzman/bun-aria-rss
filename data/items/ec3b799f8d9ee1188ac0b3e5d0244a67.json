{
  "title": "An Analysis of the World's Leading robots.txt Files",
  "link": "",
  "updated": "2017-10-18T00:00:00-07:00",
  "id": "http://www.benfrederickson.com/robots-txt-analysis/",
  "content": "\n        \n            <img src=\"http://www.benfrederickson.com/images/robots_txt_analysis/image.png\" width=\"100%\" style=\"max-width:500px\">\n        \n        <p>A site’s robots.txt file advises the web crawlers of the worlds what files they\ncan and can’t download. It acts as the first gatekeeper of the internet,\nunlike blocking the response - it lets you stop requests to your site before\nit happens. The interesting thing about these files is that it lays out how\nwebmasters intend automated processes should access their websites. While it’s easy\nfor a bot to just ignore this file, it specifies an idealized behaviour of how they should act.</p>\n\n<p>As such these files are kind of important. So I thought I’d download the robots.txt file \nfrom each of the top million websites on the planet and see what kind of patterns I could find.</p>\n\n<p>I got the list of <a href=\"http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\">the top 1 million sites from Alexa</a>\nand wrote a <a href=\"https://github.com/benfred/bens-blog-code/tree/master/robots.txt-analysis\">small program</a>\nto download the robots.txt file from each domain. With the data all downloaded, I ran each\nfile through pythons\n<a href=\"https://docs.python.org/3.0/library/urllib.robotparser.html\">urllib.robotparser</a> package and \nstarted looking at the results.</p>\n\n<p class='more'><a href='http://www.benfrederickson.com/robots-txt-analysis/'>Read more ...</a></p>\n     "
}