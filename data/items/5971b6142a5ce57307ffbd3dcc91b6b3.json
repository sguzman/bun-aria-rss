{
  "title": "NLP for Log Analysis &#8211; Tokenization",
  "link": "https://streamhacker.com/2018/11/13/nlp-log-analysis-tokenization/#utm_source=feed&#038;utm_medium=feed&#038;utm_campaign=feed",
  "comments": "https://streamhacker.com/2018/11/13/nlp-log-analysis-tokenization/#respond",
  "dc:creator": "Jacob",
  "pubDate": "Tue, 13 Nov 2018 19:00:34 +0000",
  "category": [
    "insight-engines",
    "programming",
    "talks",
    "logs",
    "nlp",
    "tokenization"
  ],
  "guid": "https://streamhacker.com/?p=2029",
  "description": "Tokenizing logs using natural language tokenizers, with examples from NLTK.",
  "content:encoded": "<p><em>This is part 1 of a series of posts based on a <a href=\"https://www.slideshare.net/japerk/nlp-techniques-for-log-analysis\" target=\"_blank\" rel=\"noopener\">presentation</a> I gave at the <a href=\"https://www.meetup.com/siliconvalleysecurity/events/254164494/\" target=\"_blank\" rel=\"noopener\">Silicon Valley Cyber Security Meetup</a> on behalf of my company, <a href=\"https://insightengines.com/\" target=\"_blank\" rel=\"noopener\">Insight Engines</a>. Some of the ideas are speculative and I do not know if they are used in practice. If you have any experience applying these techniques on logs, please share in the comments below.</em></p>\n<p>Natural language processing is the art of applying software algorithms to human language. However, the techniques operate on text, and there’s a lot of text that is not natural language. These techniques have been applied to <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.423.5913&rep=rep1&type=pdf\" target=\"_blank\" rel=\"noopener\">code authorship classification</a>, so why not apply them to <a href=\"https://en.wikipedia.org/wiki/Log_analysis\" target=\"_blank\" rel=\"noopener\">log analysis</a>?</p>\n<h2>Tokenization</h2>\n<p>To process any kind of text, you need to tokenize it. For natural language, this means splitting the text into sentences and words. But for logs, the tokens are different. Some tokens may be words, but other tokens could be symbols, timestamps, numbers, and more.</p>\n<p>Another difference is punctuation. For many human languages, punctuation is mostly regular and predictable, although social media & short text writing has been challenging this assumption.</p>\n<p>Logs come in a whole variety of formats. If you only have 1 type of log, then you may be able to tokenize it with a regular expression, like <a href=\"https://httpd.apache.org/docs/current/logs.html#common\" target=\"_blank\" rel=\"noopener\">apache access logs</a> for example. But when you have multiple types of logs, regular expressions can become overwhelming or even unusable. Many logs are written by humans, and there’s few rules or conventions when it comes to formatting or use of punctuation. A generic tokenizer could be a useful first pass at parsing arbitrary logs.</p>\n<h2>Whitespace Tokenizer</h2>\n<p>Tokenizing on whitespace is an obvious thing to try first. Here’s an example log and the result when run through my <a href=\"http://text-processing.com/demo/tokenize/\" target=\"_blank\" rel=\"noopener\">NLTK tokenization demo</a>.</p>\n<pre>Sep 19 19:18:40 acmepayroll syslog: 04/30/10 12:18:51 39480627 wksh: HANDLING TELNET CALL (User: root, Branch: ABCDE, Client: 10101) pid=9644</pre>\n<p><a href=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?ssl=1\"><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-2035 size-large\" title=\"NLTK Whitespace Tokenizer log example\" src=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?resize=474%2C151&#038;ssl=1\" alt=\"NLTK Whitespace Tokenizer log example\" width=\"474\" height=\"151\" srcset=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?resize=1024%2C327&ssl=1 1024w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?resize=300%2C96&ssl=1 300w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?resize=768%2C245&ssl=1 768w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?w=1442&ssl=1 1442w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WhitespaceTokenizer.png?w=948&ssl=1 948w\" sizes=\"(max-width: 474px) 100vw, 474px\" data-recalc-dims=\"1\" /></a>As you can see, it does get some tokens, but there’s punctuation in weird places that would have to be cleaned up later.</p>\n<h2>Wordpunct Tokenizer</h2>\n<p>My preferred <a href=\"http://www.nltk.org/api/nltk.tokenize.html\" target=\"_blank\" rel=\"noopener\">NLTK tokenizer</a> is the <code>WordPunctTokenizer</code>, since it’s fast and the behavior is predictable: split on whitespace and punctuation. But this is a terrible choice for log tokenization.</p>\n<p><a href=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?ssl=1\"><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-2036 size-large\" title=\"NLTK WordPunct Tokenizer log example\" src=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?resize=474%2C194&#038;ssl=1\" alt=\"NLTK WordPunct Tokenizer log example\" width=\"474\" height=\"194\" srcset=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?resize=1024%2C420&ssl=1 1024w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?resize=300%2C123&ssl=1 300w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?resize=768%2C315&ssl=1 768w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?w=1428&ssl=1 1428w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/WordPunctTokenizer.png?w=948&ssl=1 948w\" sizes=\"(max-width: 474px) 100vw, 474px\" data-recalc-dims=\"1\" /></a>Logs are filled with punctuation, so this produces far too many tokens.</p>\n<h2>Treebank Tokenizer</h2>\n<p>Out of curiosity, I tried the <code>TreebankWordTokenizer</code> on the log example. This tokenizer uses a statistical model trained on news text, and it does surprisingly well.</p>\n<p><a href=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?ssl=1\"><img decoding=\"async\" loading=\"lazy\" class=\"aligncenter wp-image-2037 size-large\" title=\"NLTK Treebank Word Tokenizer log example\" src=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?resize=474%2C161&#038;ssl=1\" alt=\"NLTK Treebank Word Tokenizer log example\" width=\"474\" height=\"161\" srcset=\"https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?resize=1024%2C347&ssl=1 1024w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?resize=300%2C102&ssl=1 300w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?resize=768%2C260&ssl=1 768w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?w=1394&ssl=1 1394w, https://i0.wp.com/streamhacker.com/wp-content/uploads/2018/11/TreebankWordTokenizer.png?w=948&ssl=1 948w\" sizes=\"(max-width: 474px) 100vw, 474px\" data-recalc-dims=\"1\" /></a>There’s no weird punctuation in the tokens, some is separated out and other punctuation is within tokens. It all looks pretty logical & useful. This was an unexpected result, and indicates that perhaps logs are often closer to natural language text than one might think.</p>\n<h2>Next Steps</h2>\n<p>After tokenization, you’ll want to do something with the log tokens. Maybe extract certain features, and then cluster or classify the log. These topics will be covered in a future post.</p>\n",
  "wfw:commentRss": "https://streamhacker.com/2018/11/13/nlp-log-analysis-tokenization/feed/",
  "slash:comments": 0,
  "post-id": 2029
}