{
  "title": "Decaying Evidence and Contextual Bandits â€” Bayesian Reinforcement Learning (Part 2)",
  "link": "",
  "id": "https://www.georgeho.org/bayesian-bandits-2/",
  "updated": "2019-06-02T00:00:00Z",
  "published": "2019-06-02T00:00:00Z",
  "content": "<blockquote>\n<p>This is the second of a two-part series about Bayesian bandit algorithms.\nCheck out the first post <a href=\"https://www.georgeho.org/bayesian-bandits/\">here</a>.</p>\n</blockquote>\n<p><a href=\"https://www.georgeho.org/bayesian-bandits/\">Previously</a>, I introduced the\nmulti-armed bandit problem, and a Bayesian approach to solving/modelling it\n(Thompson sampling). We saw that conjugate models made it possible to run the\nbandit algorithm online: the same is even true for non-conjugate models, so long\nas the rewards are bounded.</p>\n<p>In this follow-up blog post, we&rsquo;ll take a look at two extensions to the\nmulti-armed bandit. The first allows the bandit to model nonstationary rewards\ndistributions, whereas the second allows the bandit to model context. Jump in!</p>\n<figure>\n<a href=\"https://www.georgeho.org/assets/images/multi-armed-bandit.jpg\"><img src=\"https://www.georgeho.org/assets/images/multi-armed-bandit.jpg\" alt=\"Cartoon of a multi-armed bandit\"></a>\n<figcaption>An example of a multi-armed bandit situation. Source: <a href=\"https://www.inverse.com/article/13762-how-the-multi-armed-bandit-determines-what-ads-and-stories-you-see-online\">Inverse</a>.</figcaption>\n</figure>\n<h2 id=\"nonstationary-bandits\">Nonstationary Bandits</h2>\n<p>Up until now, we&rsquo;ve concerned ourselves with stationary bandits: in other words,\nwe assumed that the rewards distribution for each arm did not change over time.\nIn the real world though, rewards distributions need not be stationary: customer\npreferences change, trading algorithms deteriorate, and news articles rise and\nfall in relevance.</p>\n<p>Nonstationarity could mean one of two things for us:</p>\n<ol>\n<li>either we are lucky enough to know that rewards are similarly distributed\nthroughout all time (e.g. the rewards are always normally distributed, or\nalways binomially distributed), and that it is merely the parameters of these\ndistributions that are liable to change,</li>\n<li>or we aren&rsquo;t so unlucky, and the rewards distributions are not only changing,\nbut don&rsquo;t even have a nice parametric form.</li>\n</ol>\n<p>Good news, though: there is a neat trick to deal with both forms of\nnonstationarity!</p>\n<h3 id=\"decaying-evidence-and-posteriors\">Decaying evidence and posteriors</h3>\n<p>But first, some notation. Suppose we have a model with parameters $\\theta$. We\nplace a prior $\\color{purple}{\\pi_0(\\theta)}$ on it<sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\">1</a></sup>, and at the $t$&lsquo;th\ntime step, we observe data $D_t$, compute the likelihood $\\color{blue}{P(D_t\n| \\theta)}$ and update the posterior from $\\color{red}{\\pi_t(\\theta |\nD_{1:t})}$ to $\\color{green}{\\pi_{t+1}(\\theta | D_{1:t+1})}$.</p>\n<p>This is a quintessential application of Bayes&rsquo; Theorem. Mathematically:</p>\n<p>$$ \\color{green}{\\pi_{t+1}(\\theta | D_{1:t+1})} \\propto \\color{blue}{P(D_{t+1} |\n\\theta)} \\cdot \\color{red}{\\pi_t (\\theta | D_{1:t})} \\tag{1} \\label{1} $$</p>\n<p>However, for problems with nonstationary rewards distributions, we would like\ndata points observed a long time ago to have less weight than data points\nobserved recently. This is only prudent: in the absence of recent data, we would\nlike to adopt a more conservative &ldquo;no-data&rdquo; prior, rather than allow our\nposterior to be informed by outdated data. This can be achieved by modifying the\nBayesian update to:</p>\n<p>$$ \\color{green}{\\pi_{t+1}(\\theta | D_{1:t+1})} \\propto \\color{magenta}{[}\n\\color{blue}{P(D_{t+1} | \\theta)} \\cdot \\color{red}{\\pi_t (\\theta | D_{1:t})}\n{\\color{magenta}{]^{1-\\epsilon}}} \\cdot\n\\color{purple}{\\pi_0(\\theta)}^\\color{magenta}{\\epsilon} \\tag{2} \\label{2} $$</p>\n<p>for some $0 &lt; \\color{magenta}{\\epsilon} \\ll 1$. We can think of\n$\\color{magenta}{\\epsilon}$ as controlling the rate of decay of the\nevidence/posterior (i.e. how quickly we should distrust past data points).\nNotice that if we stop observing data points at time $T$, then\n$\\color{red}{\\pi_t(\\theta | D_{1:T})} \\rightarrow\n\\color{purple}{\\pi_0(\\theta)}$ as $t \\rightarrow \\infty$.</p>\n<p>Decaying the evidence (and therefore the posterior) can be used to address both\ntypes of nonstationarity identified above. Simply use $(\\ref{2})$ as a drop-in\nreplacement for $(\\ref{1})$ when updating the hyperparameters. Whether you&rsquo;re\nusing a conjugate model or the algorithm by <a href=\"https://arxiv.org/abs/1111.1797\">Agarwal and\nGoyal</a> (introduced in <a href=\"https://www.georgeho.org/bayesian-bandits\">the previous blog\npost</a>), using $(\\ref{2})$ will decay\nthe evidence and posterior, as desired.</p>\n<p>For more information (and a worked example for the Beta-Binomial model!), check\nout <a href=\"https://austinrochford.com/resources/talks/boston-bayesians-2017-bayes-bandits.slides.html#/3\">Austin Rochford&rsquo;s talk for Boston\nBayesians</a>\nabout Bayesian bandit algorithms for e-commerce.</p>\n<h2 id=\"contextual-bandits\">Contextual Bandits</h2>\n<p>We can think of the multi-armed bandit problem as follows<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\">2</a></sup>:</p>\n<ol>\n<li>A policy chooses an arm $a$ from $k$ arms.</li>\n<li>The world reveals the reward $R_a$ of the chosen arm.</li>\n</ol>\n<p>However, this formulation fails to capture an important phenomenon: there is\nalmost always extra information that is available when making each decision.\nFor instance, online ads occur in the context of the web page in which they\nappear, and online store recommendations are given in the context of the user&rsquo;s\ncurrent cart contents (among other things).</p>\n<p>To take advantage of this information, we might think of a different formulation\nwhere, on each round:</p>\n<ol>\n<li>The world announces some context information $x$.</li>\n<li>A policy chooses an arm $a$ from $k$ arms.</li>\n<li>The world reveals the reward $R_a$ of the chosen arm.</li>\n</ol>\n<p>In other words, contextual bandits call for some way of taking context as input\nand producing arms/actions as output.</p>\n<p>Alternatively, if you think of regular multi-armed bandits as taking no input\nwhatsoever (but still producing outputs, the arms to pull), you can think of\ncontextual bandits as algorithms that both take inputs and produce outputs.</p>\n<h3 id=\"bayesian-contextual-bandits\">Bayesian contextual bandits</h3>\n<p>Contextual bandits give us a very general framework for thinking about\nsequential decision making (and reinforcement learning). Clearly, there are many\nways to make a bandit algorithm take context into account. Linear regression is\na straightforward and classic example: simply assume that the rewards depend\nlinearly on the context.</p>\n<p>For a refresher on the details of Bayesian linear regression, refer to <a href=\"https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book\"><em>Pattern\nRecognition and Machine\nLearning</em></a>\nby Christopher Bishop: specifically, section 3.3 on Bayesian linear regression\nand exercises 3.12 and 3.13<sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\">3</a></sup>. Briefly though, if we place a Gaussian prior on\nthe regression weights and an inverse gamma prior on the noise parameter (i.e.,\nthe noise of the observations), then their joint prior will be conjugate to a\nGaussian likelihood, and the posterior predictive distribution for the rewards\nwill be a Student&rsquo;s $t$.</p>\n<p>Since we need to maintain posteriors of the rewards for each arm (so that we can\ndo Thompson sampling), we need to run a separate Bayesian linear regression for\neach arm. At every iteration we then Thompson sample from each Student&rsquo;s $t$\nposterior, and select the arm with the highest sample.</p>\n<p>However, Bayesian linear regression is a textbook example of a model that lacks\nexpressiveness: in most circumstances, we want something that can model\nnonlinear functions as well. One (perfectly valid) way of doing this would be to\nhand-engineer some nonlinear features and/or basis functions before feeding them\ninto a Bayesian linear regression. However, in the 21st century, the trendier\nthing to do is to have a neural network learn those features for you. This is\nexactly what is proposed in a <a href=\"https://arxiv.org/abs/1802.09127\">ICLR 2018 paper from Google\nBrain</a>. They find that this model â€” which they\ncall <code>NeuralLinear</code> â€” performs decently well across a variety of tasks, even\ncompared to other bandit algorithms. In the words of the authors:</p>\n<blockquote>\n<p>We believe [<code>NeuralLinear</code>&rsquo;s] main strength is that it is able to\n<em>simultaneously</em> learn a data representation that greatly simplifies the task\nat hand, and to accurately quantify the uncertainty over linear models that\nexplain the observed rewards in terms of the proposed representation.</p>\n</blockquote>\n<p>For more information, be sure to check out the <a href=\"https://arxiv.org/abs/1802.09127\">Google Brain\npaper</a> and the accompanying <a href=\"https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits\">TensorFlow\ncode</a>.</p>\n<h2 id=\"further-reading\">Further Reading</h2>\n<p>For non-Bayesian approaches to contextual bandits, <a href=\"https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Contextual-Bandit-algorithms\">Vowpal\nWabbit</a>\nis a great resource: <a href=\"http://hunch.net/~jl/\">John Langford</a> and the team at\n<a href=\"https://www.microsoft.com/research/\">Microsoft Research</a> has <a href=\"https://arxiv.org/abs/1402.0555v2\">extensively\nresearched</a> contextual bandit algorithms.\nThey&rsquo;ve provided blazingly fast implementations of recent algorithms and written\ngood documentation for them.</p>\n<p>For the theory and math behind bandit algorithms, <a href=\"https://banditalgs.com/\">Tor Lattimore and Csaba\nSzepesvÃ¡ri&rsquo;s book</a> covers a breathtaking amount of\nground.</p>\n<blockquote>\n<p>This is the second of a two-part series about Bayesian bandit algorithms.\nCheck out the first post <a href=\"https://www.georgeho.org/bayesian-bandits/\">here</a>.</p>\n</blockquote>\n<div class=\"footnotes\" role=\"doc-endnotes\">\n<hr>\n<ol>\n<li id=\"fn:1\">\n<p>Did you know you can make <a href=\"http://adereth.github.io/blog/2013/11/29/colorful-equations/\">colored equations with\nMathJax</a>?\nTechnology frightens me sometimes.&#160;<a href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:2\">\n<p>This explanation is largely drawn from <a href=\"http://hunch.net/?p=298\">from John Langford&rsquo;s\n<code>hunch.net</code></a>.&#160;<a href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n<li id=\"fn:3\">\n<p>If you don&rsquo;t want to do Bishop&rsquo;s exercises, there&rsquo;s a partially complete\nsolutions manual <a href=\"https://github.com/GoldenCheese/PRML-Solution-Manual/\">on\nGitHub</a> ðŸ˜‰&#160;<a href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\">&#x21a9;&#xfe0e;</a></p>\n</li>\n</ol>\n</div>"
}