{
  "title": "My favourite ICML 2015 papers - part two",
  "link": "",
  "updated": "2015-07-08T00:00:00+00:00",
  "id": "http://daoudclarke.github.com/machine%20learning%20in%20practice/2015/07/08/icml2015-favourite-papers-day2",
  "content": "<p>Yesterday I <a href=\"/machine%20learning%20in%20practice/2015/07/07/icml2015-favourite-papers-day1\">posted</a>\non my favourite papers from the beginning of ICML (some of those\npapers were actually presented today, although the posters were\ndisplayed yesterday). Here’s today’s update, which includes some\npapers to be presented tomorrow, because the posters were on display\ntoday…</p>\n\n<h2 id=\"neural-nets\">Neural Nets</h2>\n\n<h3 id=\"unsupervised-domain-adaptation-by-backpropagation\"><a href=\"http://jmlr.org/proceedings/papers/v37/ganin15.pdf\">Unsupervised Domain Adaptation by Backpropagation</a></h3>\n\n<p><em>Yaroslav Ganin, Victor Lempitsky</em></p>\n\n<p>Imagine you have a small amount of labelled training data and a lot of\nunlabelled data from a different domain. This technique will allow you\nto build a neural network model that fits the unlabelled domain. The\nkey idea is super cool and really simple to implement. You build a\nnetwork that optimises features such that it is difficult to\ndistinguish which domain the data came from.</p>\n\n<h3 id=\"weight-uncertainty-in-neural-networks\"><a href=\"http://jmlr.org/proceedings/papers/v37/blundell15.pdf\">Weight Uncertainty in Neural Networks</a></h3>\n\n<p><em>Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra</em></p>\n\n<h3 id=\"probabilistic-backpropagation-for-scalable-learning-of-bayesian-neural-networks\"><a href=\"http://jmlr.org/proceedings/papers/v37/hernandez-lobatoc15.pdf\">Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks</a></h3>\n\n<p><em>Jose Miguel Hernandez-Lobato, Ryan Adams</em></p>\n\n<p>These papers have a very similar goal, namely making neural networks\nprobabilistic. This is cool because it allows you to not only make a\ndecision, but know <em>how sure you are about the decision</em>. There are a\nbunch of other benefits: you don’t need to worry about regularisation,\nhyperparameter tuning is easier etc.</p>\n\n<p>Anyway, the two papers achieve this in two different ways. The first\nuses Gaussian scale mixtures together with a clever trick to\nbackpropagate expectations. The second one computes the distribution\nafter rectifying and then approximates this with a Gaussian\ndistribution. Either way, this is an exciting development for neural\nnetworks.</p>\n\n<h3 id=\"training-deep-convolutional-neural-networks-to-play-go\"><a href=\"http://jmlr.org/proceedings/papers/v37/clark15.pdf\">Training Deep Convolutional Neural Networks to Play Go</a></h3>\n\n<p><em>Christopher Clark, Amos Storkey</em></p>\n\n<p>Although I’ve never actually played the game, I have an interest in AI\nGo players, because it’s such a hard game for computers, which still\ncan’t reach the level of human players. The current state of the art\nuses <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\">Monte Carlo tree search</a>\nwhich is a really cool technique. The authors of this paper use neural\nnetworks to play the game but don’t quite achieve the same level of\nperformance. I asked the author whether the two approaches could be\ncombined, and they think they can! Watch this space for a new state of\nthe art Go player.</p>\n\n<h2 id=\"natural-language-processing\">Natural Language Processing</h2>\n\n<h3 id=\"phrase-based-image-captioning\"><a href=\"http://jmlr.org/proceedings/papers/v37/lebret15.pdf\">Phrase-based Image Captioning</a></h3>\n\n<p><em>Remi Lebret, Pedro Pinheiro, Ronan Collobert</em></p>\n\n<p>This is a new state of the art in this very interesting task of\nlabelling images with phrases. The clever bit is in the syntactic\nanalysis of the phrases in the training set, which often follow a\nsimilar pattern. The authors use this to their advantage: the model is\ntrained on the individual sub-phrases that are extracted, which allows\nit to behave compositionally. This means that it can describe, for\nexample, both the fact that a plate is on a table, and that there is\npizza on the plate. Unlike previous approaches, the sentences that are\ngenerated are not often found in the training set, which shows\nthat it is doing real generation and not retrieval. Exciting stuff!</p>\n\n<h3 id=\"bimodal-modelling-of-source-code-and-natural-language\"><a href=\"http://jmlr.org/proceedings/papers/v37/allamanis15.pdf\">Bimodal Modelling of Source Code and Natural Language</a></h3>\n\n<p><em>Miltos Allamanis, Daniel Tarlow, Andrew Gordon, Yi Wei</em></p>\n\n<p>Another fun paper; this one tries to generate source code given a\nnatural language query, quite an ambitious task! It is trained on\nsnippets of code extracted from StackOverflow.</p>\n\n<h2 id=\"optimisation\">Optimisation</h2>\n\n<h3 id=\"gradient-based-hyperparameter-optimization-through-reversible-learning\"><a href=\"http://jmlr.org/proceedings/papers/v37/maclaurin15.pdf\">Gradient-based Hyperparameter Optimization through Reversible Learning</a></h3>\n\n<p><em>Dougal Maclaurin, David Duvenaud, Ryan Adams</em></p>\n\n<p>Hyperparameter optimisation is important when training neural networks\nbecause there are so many of the things floating around. How do you\nknow what to set them to? Normally you have to perform some kind of\nsearch on the space of possible parameters, and Bayesian techniques\nhave been very helpful at doing this. This paper suggests something\nentirely different and completely audacious. The authors are able to\ncompute gradients for hyperparameters using automatic differentiation\n<em>after going through a whole round of stochastic gradient descent\nlearning</em>. That’s quite a feat. What this means is that we can answer\nquestions about what the optimal hyperparameter settings look like in\ndifferent settings - and makes a whole set of things that was\npreviously a “black art” a lot more scientific and\nunderstandable.</p>\n\n<h2 id=\"and-more\">And more…</h2>\n\n<p>There were many more interesting papers - too many to write up\nhere. Take a look at the <a href=\"http://icml.cc/2015/?page_id=825\">schedule</a>\nand find your favourite! Let me know on <a href=\"https://twitter.com/daarkecloud\">Twitter</a>.</p>"
}